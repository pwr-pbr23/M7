[{"number": 6965, "title": "Cherrypicks into r1.0", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 6964, "title": "Fix nasm URL", "body": "See #6956", "comments": []}, {"number": 6963, "title": "Fix nasm URL in r0.12", "body": "See #6956 ", "comments": ["Mr. Jenkins: test this please", "I think the issues are unrelated.\r\nI will merge the change."]}, {"number": 6962, "title": "inception_v4 checkpoint has no convolution biases", "body": "I've been trying to export the trained model weights and biases from inception_v4 to be used in a Keras implementation of the model. However, it does not look like any of the convolution layers have biases when they (I'm assuming) should. Is there some other way to get the biases other than from the ckpt file?\r\n\r\n**Other model checkpoints DO have biases for the convolution layers though such as vgg_16:**\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcurand.so.8.0 locally\r\n('tensor_name: ', 'global_step')\r\n('tensor_name: ', 'vgg_16/conv1/conv1_1/biases')\r\n('tensor_name: ', 'vgg_16/conv1/conv1_1/weights')\r\n('tensor_name: ', 'vgg_16/conv1/conv1_2/biases')\r\n('tensor_name: ', 'vgg_16/conv1/conv1_2/weights')\r\n('tensor_name: ', 'vgg_16/conv2/conv2_1/biases')\r\n('tensor_name: ', 'vgg_16/conv2/conv2_1/weights')\r\n('tensor_name: ', 'vgg_16/conv2/conv2_2/biases')\r\n('tensor_name: ', 'vgg_16/conv2/conv2_2/weights')\r\n('tensor_name: ', 'vgg_16/conv3/conv3_1/biases')\r\n('tensor_name: ', 'vgg_16/conv3/conv3_1/weights')\r\n('tensor_name: ', 'vgg_16/conv3/conv3_2/biases')\r\n('tensor_name: ', 'vgg_16/conv3/conv3_2/weights')\r\n('tensor_name: ', 'vgg_16/conv3/conv3_3/biases')\r\n('tensor_name: ', 'vgg_16/conv3/conv3_3/weights')\r\n('tensor_name: ', 'vgg_16/conv4/conv4_1/biases')\r\n('tensor_name: ', 'vgg_16/conv4/conv4_1/weights')\r\n('tensor_name: ', 'vgg_16/conv4/conv4_2/biases')\r\n('tensor_name: ', 'vgg_16/conv4/conv4_2/weights')\r\n('tensor_name: ', 'vgg_16/conv4/conv4_3/biases')\r\n('tensor_name: ', 'vgg_16/conv4/conv4_3/weights')\r\n('tensor_name: ', 'vgg_16/conv5/conv5_1/biases')\r\n('tensor_name: ', 'vgg_16/conv5/conv5_1/weights')\r\n('tensor_name: ', 'vgg_16/conv5/conv5_2/biases')\r\n('tensor_name: ', 'vgg_16/conv5/conv5_2/weights')\r\n('tensor_name: ', 'vgg_16/conv5/conv5_3/biases')\r\n('tensor_name: ', 'vgg_16/conv5/conv5_3/weights')\r\n('tensor_name: ', 'vgg_16/fc6/biases')\r\n('tensor_name: ', 'vgg_16/fc6/weights')\r\n('tensor_name: ', 'vgg_16/fc7/biases')\r\n('tensor_name: ', 'vgg_16/fc7/weights')\r\n('tensor_name: ', 'vgg_16/fc8/biases')\r\n('tensor_name: ', 'vgg_16/fc8/weights')\r\n('tensor_name: ', 'vgg_16/mean_rgb')\r\n```\r\n\r\n# Using a minimal version of the inspect checkpoint tool:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef print_layers(file_name):\r\n\ttry:\r\n\t\treader = tf.train.NewCheckpointReader(file_name)\r\n\t\tvar_to_shape_map = reader.get_variable_to_shape_map()\r\n\t\tvar_to_shape_map = sorted(var_to_shape_map)\r\n\t\tfor key in var_to_shape_map:\r\n\t\t\tprint(\"tensor_name: \", key)\r\n\texcept Exception as e:\r\n\t\tprint(str(e))\r\n\t\tif \"corrupted compressed block contents\" in str(e):\r\n\t\t\tprint(\"It's likely that your checkpoint file has been compressed \"\r\n\t\t\t\t\t\"with SNAPPY.\")\r\n\r\nif __name__ == \"__main__\":\r\n\tprint_layers(\"./inception_v4.ckpt\")\r\n```\r\n\r\n**The output is as follows (notice that only Logits and AuxLogits have biases):**\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:120] successfully opened CUDA library libcurand.so.8.0 locally\r\n('tensor_name: ', 'InceptionV4/AuxLogits/Aux_logits/biases')\r\n('tensor_name: ', 'InceptionV4/AuxLogits/Aux_logits/weights')\r\n('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_1b_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_1b_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_1b_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_1b_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_2a/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_2a/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_2a/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/AuxLogits/Conv2d_2a/weights')\r\n('tensor_name: ', 'InceptionV4/Conv2d_1a_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Conv2d_1a_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Conv2d_1a_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Conv2d_1a_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Conv2d_2a_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Conv2d_2a_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Conv2d_2a_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Conv2d_2a_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Conv2d_2b_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Conv2d_2b_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Conv2d_2b_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Conv2d_2b_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Logits/Logits/biases')\r\n('tensor_name: ', 'InceptionV4/Logits/Logits/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_3a/Branch_1/Conv2d_0a_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_3a/Branch_1/Conv2d_0a_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_3a/Branch_1/Conv2d_0a_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_3a/Branch_1/Conv2d_0a_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_0/Conv2d_1a_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0b_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_0c_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5a/Branch_0/Conv2d_1a_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0b_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_1/Conv2d_0b_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0b_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0b_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_1/Conv2d_0b_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0b_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0b_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0c_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_2/Conv2d_0c_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_5e/Branch_3/Conv2d_0b_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_0/Conv2d_1a_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6a/Branch_1/Conv2d_1a_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_1/Conv2d_0c_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0c_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0b_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_2/Conv2d_0e_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0b_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0c_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0d_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6e/Branch_3/Conv2d_0b_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0b_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_1/Conv2d_0c_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0b_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0b_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0c_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0c_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0d_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0d_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0e_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_2/Conv2d_0e_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6f/Branch_3/Conv2d_0b_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0b_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_1/Conv2d_0c_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0b_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0b_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0c_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0c_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0d_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0d_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0e_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_2/Conv2d_0e_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6g/Branch_3/Conv2d_0b_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0b_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_1/Conv2d_0c_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0b_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0b_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0c_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0c_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0d_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0d_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0e_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_2/Conv2d_0e_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_6h/Branch_3/Conv2d_0b_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0b_1x7/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_0c_7x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0c_3x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_1/Conv2d_0c_3x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0b_3x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0b_3x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0b_3x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0b_3x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0d_1x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0d_1x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0d_1x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0d_1x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0e_3x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0e_3x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0e_3x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_2/Conv2d_0e_3x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7b/Branch_3/Conv2d_0b_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0b_3x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0b_3x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0b_3x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0b_3x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0d_1x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0d_1x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0d_1x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0d_1x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0e_3x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0e_3x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0e_3x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_2/Conv2d_0e_3x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7c/Branch_3/Conv2d_0b_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_0/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0b_1x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0b_1x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0c_3x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_1/Conv2d_0c_3x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0a_1x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0b_3x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0b_3x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0b_3x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0b_3x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0c_1x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0c_1x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0d_1x3/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0d_1x3/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0d_1x3/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0d_1x3/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0e_3x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0e_3x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0e_3x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_2/Conv2d_0e_3x1/weights')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance')\r\n('tensor_name: ', 'InceptionV4/Mixed_7d/Branch_3/Conv2d_0b_1x1/weights')\r\n('tensor_name: ', 'global_step')\r\n```", "comments": ["Closing this as I believe it should be posted in Tensorflow/Models instead."]}, {"number": 6961, "title": "Prevent/warn user setting up infinite loops with Defun/symbolic_gradient", "body": "After session.run, the node `dx` seems to be evaluated in an infinite loop. Tensorflow version 12.1 and also in HEAD.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.framework import function\r\nfrom tensorflow.python.ops import functional_ops\r\n\r\n\r\n@function.Defun(tf.float32, tf.float32)\r\ndef XSquarePlusOneGrad(x, dy):\r\n    message = tf.Print(1, [1], \"back prop\")\r\n    with tf.control_dependencies([message]):\r\n        dx = functional_ops._symbolic_gradient(input=[x, dy], Tout=[tf.float32], f=\"XSquarePlusOneFn\", name=\"dx\")\r\n    return dx\r\n\r\n@function.Defun(tf.float32, func_name=\"XSquarePlusOneFn\", grad_func=XSquarePlusOneGrad)\r\ndef XSquarePlusOne(x):\r\n    message = tf.Print(1, [1], \"forward prop\")\r\n    with tf.control_dependencies([message]):\r\n        y = tf.square(x, name=\"fprop_square\")\r\n    return y\r\n\r\n\r\nx = tf.placeholder(tf.float32, shape=(), name=\"x_input\")\r\nfprop = XSquarePlusOne(x)\r\ngrad = tf.gradients(fprop, [x])[0]\r\nsess = tf.Session()\r\nsess.run([fprop, grad], feed_dict={x: 4})\r\n```\r\n\r\nYou see\r\n\r\n```\r\nI tensorflow/core/kernels/logging_ops.cc:79] back prop[1]\r\nI tensorflow/core/kernels/logging_ops.cc:79] back prop[1]\r\nI tensorflow/core/kernels/logging_ops.cc:79] back prop[1]\r\nI tensorflow/core/kernels/logging_ops.cc:79] back prop[1]\r\nI tensorflow/core/kernels/logging_ops.cc:79] back prop[1]\r\nI tensorflow/core/kernels/logging_ops.cc:79] back prop[1]\r\nI tensorflow/core/kernels/logging_ops.cc:79] back prop[1]\r\n...\r\n```", "comments": ["@zffchen78 any idea what's going on here?", "OK, looks like the following construct sets up infinite computation loop in TensorFlow\r\n```\r\n\r\n@function.Defun(...)\r\ndef B:\r\n  _symbolic_gradient(input=..., f=\"A\")\r\n\r\n@function.Defun(..., func_name=\"A\", grad_func=B)\r\ndef A:\r\n  ...\r\n\r\nsess.run(tf.gradients(A(x), [x]), ...\r\n```\r\n\r\n<img src=https://cloud.githubusercontent.com/assets/23068/22128319/c4e260b4-de54-11e6-8b0c-4adab7a861f3.png width=272 height=175>\r\n", "a way to break the loop while capturing backprop graph in a function is to call `symbolic_gradient` on a copy of `A` instead of `A` directly. \r\n\r\n<img width=307 height=202\r\nsrc=https://cloud.githubusercontent.com/assets/23068/22128144/156da6fc-de54-11e6-8c90-7259b4587343.png>\r\n\r\nNot sure how easy it is to detect this kind of computation loops, but a warning from TF runtime would help debugging.", "It seems that it's working as intended. Note that one can construct recursive TF functions as in other programming environment. AFAIK, it lies on the programmer to make sure the recursion is finite.", "TensorFlow already has checks for cycles, but cycles caused by `Defun/_symbolic_gradient` are not caught\r\n\r\nIE, for the following code `TF_ExtendGraph` returns bad status with message `3 nodes in a cycle` \r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.graph_editor as ge\r\n\r\ndef run_after(a, b):\r\n  a = a.op\r\n  b = b.op\r\n  already_after = (b in a.control_inputs) or (b in [i.op for i in a.inputs])\r\n\r\n  if already_after:\r\n    return 0\r\n  ge.reroute.add_control_inputs(a, [b])\r\n  return 1\r\n\r\ntf.reset_default_graph()\r\na = tf.ones(())\r\nb = tf.ones(())\r\nc = tf.ones(())\r\nrun_after(a, b)\r\nrun_after(b, c)\r\nrun_after(c, a)\r\n\r\nsess = tf.Session()\r\nsess.run(a)\r\n```\r\nerror\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/Users/yaroslav/anaconda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1029     try:\r\n-> 1030       return fn(*args)\r\n   1031     except errors.OpError as e:\r\n\r\n/Users/yaroslav/anaconda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1007       # Ensure any changes to the graph are reflected in the runtime.\r\n-> 1008       self._extend_graph()\r\n   1009       with errors.raise_exception_on_not_ok_status() as status:\r\n\r\n/Users/yaroslav/anaconda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py in _extend_graph(self)\r\n   1056           tf_session.TF_ExtendGraph(\r\n-> 1057               self._session, graph_def.SerializeToString(), status)\r\n   1058         self._opened = True\r\n\r\n/Users/yaroslav/anaconda/envs/tim-jan17/lib/python3.5/contextlib.py in __exit__(self, type, value, traceback)\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n\r\n/Users/yaroslav/anaconda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()\r\n    468           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 469           pywrap_tensorflow.TF_GetCode(status))\r\n    470   finally:\r\n\r\nInvalidArgumentError: 3 nodes in a cycle\r\n\r\n```", "> OK, looks like the following construct sets up infinite computation loop in TensorFlow\r\n> \r\n> ```\r\n> \r\n> @function.Defun(...)\r\n> def B:\r\n>   _symbolic_gradient(input=..., f=\"A\")\r\n> \r\n> @function.Defun(..., func_name=\"A\", grad_func=B)\r\n> def A:\r\n>   ...\r\n> \r\n> sess.run(tf.gradients(A(x), [x]), ...\r\n> ```\r\n> <img alt=\"\" width=\"272\" height=\"175\" src=\"https://cloud.githubusercontent.com/assets/23068/22128319/c4e260b4-de54-11e6-8b0c-4adab7a861f3.png\">\r\n\r\n"]}, {"number": 6960, "title": "Improved support for libxsmm", "body": "", "comments": []}, {"number": 6959, "title": "Passing config to MonitoredSession when using Estimator Class", "body": "While implementing my own Estimator class I noticed that although I supplied a RunConfig with `gpu_memory_fraction < 1`  all memory was allocated. After some investigation I think this is because the ConfigProto contained in the RunConfig is not passed to the MonitoredTrainingSession. \r\nChanging the argument in line 981 from `config=None` to `config=self.config.tf_config` did the trick for me.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Please in the future merge all such fixes to the master branch first, then merge to release branch.\r\n"]}, {"number": 6958, "title": "Branch 144951784", "body": "", "comments": []}, {"number": 6957, "title": "Inception3 retraining - attach text/labels to individual images", "body": "Hi\r\n\r\nI am using the inception v3 model to retrain my own dataset. I have few folder which represent the classes which contain images for each class. What i would like to do is to 'attach' some text ids to these images so when they are retrained and used to run classification/similarity-detection those ids are retrieved too. (basically its image similarity detection)\r\n\r\nFor instance, Image X is of class 'Teachers' and it belongs to John. When i retrain the model, and run a classification on the new model, i would like to get the Teachers class, but in addition to this i would like to know who is teacher (John).\r\n\r\nAny ideas how to go for it?\r\n\r\nRegards", "comments": ["This question is better suited for stackoverflow and I see you have [posted there](http://stackoverflow.com/questions/41745022/tensorflow-inception-v3-retraining-attach-text-labels-to-individual-images). Thanks for doing that.\r\n\r\nPlease do not cross post in github as we try to keep the github issues focused on bugs and feature requests."]}, {"number": 6956, "title": "Error downloading nasm", "body": "As noted in issue #6950 nasm-2.12.02.tar.bz2 is currently unavailable. (www.nasm.us does not accept connections.) @trsaunders observed this for head of r0.12 and I can confirm this for v0.12.0.\r\n\r\nWorkaround: Google the file to get it from another source and update URL in tensorflow/workspace.bzl (if you download a .gz file re-compress to bz2 and put it on a webserver) - sha256 fingerprinting should check that the file contents are correct; if you don't trust ./configure to check this, check the sha256sum yourself against the sha256 given in the .bzl file.\r\n", "comments": ["pasting error message for people googling the problem (shortening local paths)\r\n``ERROR: /tmp/bazel/external/jpeg/BUILD:162:1: no such package '@nasm//': Error downloading [http://www.nasm.us/pub/nasm/releasebuilds/2.12.02/nasm-2.12.02.tar.bz2] to /tmp/bazel/external/nasm/nasm-2.12.02.tar.bz2: GET returned 503 Service Unavailable and referenced by '@jpeg//:simd_x86_64_assemblage23'.\r\nERROR: /tmp/bazel/external/jpeg/BUILD:162:1: no such package '@nasm//': Error downloading [http://www.nasm.us/pub/nasm/releasebuilds/2.12.02/nasm-2.12.02.tar.bz2] to /tmp/bazel/external/nasm/nasm-2.12.02.tar.bz2: GET returned 503 Service Unavailable and referenced by '@jpeg//:simd_x86_64_assemblage23'.\r\nERROR: Evaluation of query \"deps((//tensorflow/... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\r\n``", "I meet the same issue.", "It looks like the nasm website is totally down for the count. Still.\r\n\r\nThe good news is that, at HEAD, this isn't a problem because we have the redundant GCS mirror URL. However I've got two PRs out to fix it anyway at HEAD and in the r0.12 branch. Earlier release branches do not need to be fixed because \u2264r0.11 don't build nasm.\r\n\r\nThank you for bringing this to our attention.", "This is fixed at HEAD so I'm marking as resolved. The fix for r0.12 should be merged probably tonight or tomorrow. It just seems like the tests aren't being cooperative.", "I am unable to build tensorflow on Ubuntu Linux 16.04. This issue with the nasm website hasn't been fixed yet. Copying and pasting the URL works fine but the build process still says that the site is inaccessible.", "@case540 Last comment raises questions re: international viability of mirror.bazel.build. Fedora appears to have deleted old NASM source of truth link. We need to find a working failover URL for that tarball that isn't pkgs.fedoraproject.org.\r\n\r\ncc: @gunan @martinwicke ", "Most external mirrors seem to have tar.xz instead of tar.bz2.\r\n\r\nOption 1: Download one of these tar.xz mirrors, uncompress it to .tar, compare to a .tar obtained from the currently used .tar.bz2, run sha256 on the tar.xz file and update workspace.bzl to use the tar.xz file from a few suitable mirrors. (This assumes that Bazel knows how to extract a tar.xz archive. The docs only mention gz and bz2.)\r\n\r\nOption 2: Set up another build mirror, for example on google drive, not just for nasm but for all dependencies. According to https://stackoverflow.com/questions/39925649/is-it-possible-to-get-the-download-link-with-sharing-option-from-gdrive-api-in-c it is fairly easy to automatically obtain the download URLs for uploaded files on google drive. I'm suggesting google drive as google is connected to tensorflow.", "I am able to download the pkgs.fedoraproject.org NASM mirror. Just to clarify, is the reason we want to replace it because it is not reliable (and not that is doesnt work at all)?\r\n\r\nAnd Option 1 should be work. Looks like some deepmind project has Bazel handle a tar.xz archive...\r\nhttps://github.com/deepmind/lab/blob/master/WORKSPACE#L28", "Bazel can absolutely handle .tar.xz.\r\n\r\nmirror.bazel.build and the canonical authoritative URL is the minimum requirement.\r\n\r\nLong term might be nice if more trusted orgs wanted to help us diversify our mirroring of public artifacts.", "More specifically, would it be possible for us to get nasm from here?\r\n\r\nhttp://www.nasm.us/pub/nasm/releasebuilds/2.13.02/nasm-2.13.02.tar.gz", "The source used to be http://www.nasm.us/pub/nasm/releasebuilds/2.12.02/nasm-2.12.02.tar.bz2 (not .gz) but it was changed because www.nasm.us did not accept connections on the 19th of January 2017. It would increase reliability if this URL was added as a 3rd source. I can confirm that the sha256sum is still right, i.e. is is the same tar file and the same bzip2 compression parameters were used (presumably the defaults).\r\n\r\nWhat traffic can a mirror site expect that hosts the 0.9 MiB nasm packages from bazel clients? In other words, how many fresh builds are made worldwide per month?", "Even GitHub itself had 98.7% availability, back when I helped rewrite [Bazel's downloader](https://github.com/bazelbuild/bazel/commit/ed7ced0018dc5c5ebd6fc8afc7158037ac1df00d). If we start adding additional mirrors (e.g. AWS, Alibaba, etc.) then I'd ideally like to see it be systemic, i.e. more than just nasm. That would elevate Bazel to carrier grade status.", "jart's comment points to 2 different approaches:\r\n\r\n1. Ask (publicly) for a volunteer to mirror all of https://mirror.bazel.build/ -- This call should include some statistics what monthly traffic, peak hourly traffic, number of concurrent connections etc. to expect.\r\n\r\n2. Hand-picking individual low priority fall back mirror sites for individual packages where the 2nd URL is not github.com. If not confident that the site can deal with high traffic we should ask for permission. Google search finds the following candidates for nasm:\r\n\r\n    * https://ftp.osuosl.org/pub/blfs/conglomeration/nasm/\r\n    * http://ftp.oregonstate.edu/.1/blfs/conglomeration/nasm/ (this probably is the same server as the above)\r\n    * https://mirror.sobukus.de/files/src/nasm/\r\n    * http://ftp.lfs-matrix.net/pub/blfs/conglomeration/nasm/\r\n\r\n", "Those are important concerns. It's part of why we designed Bazel downloader to do failover. We don't want our CI systems hammering anyone's web server. We also want to protect user privacy with opt-in trust while respecting the decisions of their policymakers.\r\n\r\nWe could hand-pick mirrors, but I'd recommend we hold off and focus on just making sure our URLs aren't broken. Expanding our mirror ecosystem in a delicate way that engenders trust with the community is something that goes way above my paygrade. I'm just an ordinary engineer.", "While we are waiting for the grand solution, can somebody with Google Contributor License Agreement add http://www.nasm.us/pub/nasm/releasebuilds/2.12.02/nasm-2.12.02.tar.bz2 as a third URL (after mirror.bazel.build and pkgs.fedoraproject.org) in tensorflow/tensorflow/workspace.bzl?\r\n\r\nA 3rd source, even one with mediocre uptime, is better than no 3rd source. No need to ask nasm.us for permission as they used to be the primary source.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Pull request #17234 added the URL to libjpeg-turbo rather than nasm."]}, {"number": 6955, "title": "Better way to transfer data from memory to tensor in C++ API", "body": "This is more of a general feature request for a better way to load data into a tensor in the C++ API, but I'll take our specific case as a reference.\r\n\r\nWe currently train a model in Python, freeze it, and load it in a C++ production pipeline. This works fine, but it seems the only way of loading data into a tensor is by looping through every single element in the data and copying it to a tensor. In our case, we're dealing with a 1920x1080 ~30 fps video input signal (each frame coming as an OpenCV matrix) making it infeasible to do if we want to process a video within a reasonable amount of time.\r\n\r\nIt seems there is a way of creating a tensor from a pointer in the C API (see http://stackoverflow.com/questions/39379747/import-opencv-mat-into-c-tensorflow-without-copying), which we will test next, but it would be nice to also have this functionality in the C++ API.\r\n\r\nFor reference, it takes approximately 900 ms to copy from an 1920x1080x3 OpenCV matrix to a tensor while it takes 315 ms to do session.run (which I assume includes transferring between CPU and GPU memory). \r\n\r\nAn issue related to this is that we will probably already have the data on the GPU from some earlier preprocessing, so we would also be very interested in not having to transfer between CPU and GPU unnecessarily.\r\n\r\nSo I guess it boils down to:\r\n\r\n1. Are there any plans for making it easier to load data already in memory to a tensor?\r\n2. How can we contribute?", "comments": ["Perhaps I'm misunderstanding, but could you use `std::copy_n` on `Tensor::flat`? For example, see [`FillValues` in `tensor_testutil.h`](https://github.com/tensorflow/tensorflow/blob/dcc414587f50673271a31ab767909ec89c956324/tensorflow/core/framework/tensor_testutil.h#L57).\r\n\r\nThat will avoid the element-by-element copy, but perhaps you were looking for ways to avoid a copy altogether?", "Yes, the idea is to avoid copying at all, since we're dealing with approximately 360 mb/s that are already in CPU memory. Avoiding a 1:1 copy (memcpy or otherwise) would be ideal. \r\n\r\nI've been looking into creating a custom allocator for tens or creation, to point to existing data instead - would this be possible? Or are we blind to the obvious solution here? ", "BTW, memcpy does around 5GB/sec on a single core, so 360 MB/s seems on the low side", "Preliminary tests with memcpy confirms that it does indeed run *much* faster; however, having a way of pointing to data instead of copying would still be very nice \u2013 especially if we could point to data already on the GPU in some way.", "There's a discussion of sharing GPU memory in https://github.com/tensorflow/tensorflow/issues/2210#event-706768999 and it seems hard. \r\n\r\nFor sharing main memory, possibly related issue to get rid of copy [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/tensor_coding.cc#L199) . I suspect that Allocator interface would need to be modified to allow keeping pointers to memory that it doesn't own", "> however, having a way of pointing to data instead of copying would still be very nice \u2013 especially if we could point to data already on the GPU in some way.\r\n\r\nAs @yaroslavvb notes here: https://github.com/tensorflow/tensorflow/issues/2919#issuecomment-226660045 there's also a [memcpy ](https://github.com/tensorflow/tensorflow/blob/1084748efa3234c7daa824718aeb7df7b9252def/tensorflow/python/client/tf_session_helper.cc#L460-L470) for copying numpy array contents into python `Session.run()`. I'm mentioning this because I think there are similar issues across languages.\r\n\r\nA similar python solution may involve creating a Tensor that wraps the numpy array but does not own the data.\r\n\r\nAnother possible solution involves explicitly allowing the user to pass array data by move or reference (C++ style). Move would assign ownership for the data to tensorflow, while by ref would leave ownership to the calling python/C++ code.\r\n\r\n```python\r\nwith tf.Session() as S:\r\n  S.run([ops], feed_dict={x: ...}, move_feed_dict={y:...}, ref_feed_dict={z: ...}\r\n```\r\n\r\nIt would be nice for this to be consistent across languages.", "CC @mrry, @hawkinsp  as FYI.\r\n\r\nI can sort of see how that would work on CPU only, but with GPU devices, and also XLA I can see that we would need to think carefully. A shallow tensor sounds like a great idea!", "Thoughts:\r\n\r\nI'm not sure that Eigen has the same concept of a [numpy strided indexing](https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html#internal-memory-layout-of-an-ndarray) for a Tensor, but similar concepts are expressed in [TensorStriding](https://eigen.tuxfamily.org/dox-devel/unsupported/TensorStriding_8h_source.html) and [TensorPadding](https://eigen.tuxfamily.org/dox-devel/unsupported/TensorPadding_8h_source.html). It would be useful to map the numpy stride concept onto a Tensor, to avoid copies.\r\n\r\nOtherwise, a call to [np.ascontiguousarray](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ascontiguousarray.html) will copy a non-contiguous array into a contiguous array, otherwise it will return a view of the original (contiguous) array.", "@yaroslavvb Thanks for the reference you pointed to! Really helpful to what I am looking for. \r\n\r\nBut to this issue per se, I guess parsing in a multi-node context is not exactly what the owner of this issue referred to. \r\n\r\nSo currently there is no way to construct a tensor by moving an existing pointer to a memory buffer? How about constructing a tensor with empty underlying storage and take that storage for your own use, and give it back to TF? \r\n\r\n`Allocator` is such a monolithic runtime so I guess it has to take control over the details of memory allocation, for example, alignment, similar to what `malloc`/`free` does. As it is simply not a good idea to glibc `free` a chunk of memory returned by another memory allocation library, say `jemalloc`.", "cc @michaelisard \r\n\r\n@byronyi I'm not away of any way of doing that.\r\n\r\nHaving TensorFlow allocate the memory buffer and provide the pointer seems safer because TensorFlow could make sure memory is aligned.\r\n\r\nThis would be useful because one could initialize numpy array based on pre-allocated memory as described [here](http://blog.enthought.com/python/numpy/simplified-creation-of-numpy-arrays-from-pre-allocated-memory/#.WMwNwRIrJE4) and this would reduce memory transfers. (ie like in PyTorch which reuses memory between Tensor and numpy array objects)", "@byronyi there is a class [TF_ManagedBuffer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L119) derived from `TensorBuffer`, the `data_` field is public, you can directly construct a `TF_ManagedBuffer`, then use [TensorCApi::MakeTensor](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L475) to construct a Tensor. `TF_ManagedBuffer` need a deallocator function pointer for destruction use. There are also two functions [allocate_tensor](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L143) and [deallocate_buffer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L154) which can give you a memory buffer from tensorflow's cpu allocator.", "@suiyuan2009 That piece of information is very helpful. Thank you!", "TensorCApi::MakeTensor doesn't seem visible from client code today. However, you can probably avoid copying pre-allocated buffers to tensors with a customized Alocator. \r\n```\r\nclass FakeAllocator {\r\n public:\r\n  FakeAllocator(void* buffer): buffer_(buffer) {}\r\n  void* AllocateRaw(size_t alignment, size_t num_bytes) override { return buffer_; }\r\n  void DeallocateRaw(void*) override {}\r\n\r\n private:\r\n  void* buffer_ = nullptr;\r\n};\r\n```", "I notice that `TF_ManagedBuffer` is only visible within `c_api.cc` file, using a `FakeAllocator` to construct a `TensorBuffer` is a good approach.", "Caveat: AllocateRaw should check the alignment; otherwise, internal TensorFlow code may crash if the allocated memory is misaligned. ", "@wujingyue Do you know how one should check for the alignment in Eigen::Tensor?", "@bnurbekov https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/allocator.h#L70\r\n\r\nEigen::Tensor has a larger alignment (maybe 512), but it doesn't matter as long as TF allocator is happy with the alignment of your buffer. ", "@ppries  Have you found a better way or a workaround for this ? If so, I can close this issue.", "@harshini-gadige \r\n\r\nwe are using memcpy for now, but might look into alternative ways of doing this \u2013 at some point we will have a need for pointing at data already on the gpu, but other problems are prioritized for now.", "Ok. Closing this issue for now. Feel free to reopen if there any new suggestions.", "Hi @ppries , I have the same needs that pointing to data already on the gpu, do you have a solution?", "> Hi @ppries , I have the same needs that pointing to data already on the gpu, do you have a solution?\r\n\r\nI have the same needs as well. Is there any solution? Thanks! ", "Hi all, I solved this problem in my team, I am sorry that I can not post my code here now, but I can provide some information that may help you guys.\r\n\r\nAs T4 GPU only have about 15G device memory, in my team, each model is about 500M, we can only load 12-15 models per GPU for serving . To solve this, we want to make model hold in host memory , only when requests come, the model copy from host to deivce for serving. \r\n\r\nTo get there, I  replace ConstantOp before loading the model and I create a global singleton object to get gpu address for each memory tensor before session run . MutableOp get tensor device address from this global singleton object in construct function .\r\n\r\n`\r\n// pseudo code for serving as follow:\r\ndef init():\r\n   1. copy model tensor to a host memory\r\n   2. replace ConstantOp to MutableOp\r\n   3.  load tensorflow model\r\n\r\ndef process():\r\n  1. if model is not in device:\r\n      1.1 copy model from host to device\r\n      1.2 write device address for each weights tensor to singleton object\r\n  2. session run\r\n\r\n// pseudo code for MutableOp \r\nMutableOp::Compute:\r\n   get tensor device address from singleton object\r\n   set tensor data address\r\n    // same as ConstantOp::Compute\r\n`\r\n", "@duduscript I think everybody here would love to see the code for the \"set tensor data address\" pseudo line. please post a snippet of code.", "> @duduscript I think everybody here would love to see the code for the \"set tensor data address\" pseudo line. please post a snippet of code.\r\n\r\nHi @dirktheeng , I make this custom op(MutableOp) a friend of Tensor, so I just set tensor data member in MutableOp::Compute\r\n\r\n```C++\r\n// mutable_op.cc\r\nMutableOp::MutableOp(OpKernelConstruction* ctx)\r\n    : OpKernel(ctx, StripTensorDataFromNodeDef(ctx)),\r\n      tensor_(ctx->output_type(0)),\r\n      // set a key for each model,  to solve multi model concurrent warmup problem\r\n      signature_(ctx->def().attr().at(\"signature\").s()) {\r\n  const TensorProto* proto = nullptr;\r\n  OP_REQUIRES_OK(ctx, ctx->GetAttr(\"value\", &proto));\r\n  type_ = proto->dtype();\r\n  tensor_ = Tensor(type_, proto->tensor_shape(), nullptr);\r\n  int64 elem_num = tensor_.shape().num_elements();\r\n  CASES(type_, { auto* buffer = new Buffer<T>();\r\n                 buffer->set_elem(elem_num);\r\n                 tensor_.buf_ = buffer;\r\n              });\r\n}\r\n\r\nvoid MutableOp::Compute(OpKernelContext* ctx) {\r\n  auto& signature2model_info =\r\n      ModelSwapInfo::singleton()->signature2model_info_;\r\n  void* dptr = signature2model_info[signature_]->at(name());\r\n  CASES(type_, {  auto* buffer = dynamic_cast<Buffer<T>*>(tensor_.buf_);\r\n                  CHECK_NOTNULL(buffer);\r\n                  buffer->set_data(dptr);\r\n               });\r\n  ctx->set_output(0, tensor_);\r\n}\r\n\r\n\r\n// tensor.h\r\nclass Tensor {\r\n...\r\n  friend class MutableOp;             // For access to buf_\r\n...\r\n};\r\n```\r\n\r\n\r\n", "@duduscript Thanks much.  I will give this a try.", "By looking at the code here in TF repo: https://github.com/tensorflow/tensorflow/blob/72d8f7ba27c1593e17a5fdc03dbe171bca23a88b/tensorflow/compiler/jit/xla_launch_util.h#L190\r\n\r\nLooks like I could also implement a subclass of `TensorBuffer`, which only holds the piece of memory you want to pass to TF."]}, {"number": 6954, "title": "only publish pre-trained model's checkpoint files are useless.", "body": "@nathansilberman @sguada\r\nI can see there are some pre-trained models in the link below for download.\r\n\r\nhttps://github.com/tensorflow/models/tree/master/slim\r\n\r\nHowever, these are only checkpoint files. We can't use them at all. Because without .pbtxt file, which is the graph define, we can't restore the any model only base on .ckpt file. There are no example about how to use these checkpoint file, such as for **a single image prediction** or feature extraction. I can only see one example about how to use V3 .pb file for prediction, but that one is .pb file, not .ckpt file. Without .pbtxt file, we can't freeze the graph at all.\r\n\r\nI want to use Inception V2 model, which is smaller than V3 and better than V1. However, I can do nothing...\r\n\r\neven if you fine tune the model, you can only evaluate it base on a batch file, but not to predict single image. in the section of **Evaluating performance of a model** of the page, we can only get accuracy. However, we need to test single image to see the result, but not a accuracy number for a batch file.", "comments": ["The graph definitions reside in https://github.com/tensorflow/models/tree/master/slim/nets. You should be able to create the graphs and load the appropriate checkpoint file using the code in that folder.", "While .py is different from .pbtxt. These pre-trained models use Imagenet dataset, which has 1000 classes and 1.2 Million images and more than 100+GB in size.\r\n\r\nIf I want ONLY want to get .pbtxt file for each model ( since .ckpt file is already there),  I need download that 1.2 million images and create data records to train at my local machine? \r\n\r\nTo get a text graph define .pbtxt file for the model need to download 100+GB file?\r\n\r\nNo one find this issue before?", "@civilman628 You don't need to have the training dataset to get the structure of a graph. They are separated by design.", "I did not find any way to get model's .pbtxt file only from .py file. If we do not have .pbtxt, these pre-trained .ckpt can do nothing. We can't use .ckpt file for prediction or feature extraction.", "@civilman628 this sounds like it could be a documentation issue. Can you specify which model you are trying to restore, and where you expected to see instructions on restoring it?\r\n\r\n`.ckpt` files are indeed insufficient on their own, usually you need a  `.py` file and instructions that tells you how to run it", "any model. base on this link:\r\n\r\nhttps://www.tensorflow.org/versions/master/tutorials/image_recognition/   it is for V3 only.\r\n\r\nYou can only predict an image base .pb file, This .pb file is from .ckpt and .pbtxt, by freeze graph right? If we only have pre-trained model .ckpt file, we can do nothing.\r\n\r\nFurthermore, we can only get tensor name from .pb file, but not .ckpt file. Why there are so many different formats and there is no easy way to convert?\r\n\r\n\r\n\r\n", "There is no example about how to use .ckpt file for prediction and feature extraction. if we can't do prediction or feature extraction, what these .ckpt files use for?", "if no one report this issue before, then that's mean no one use these .ckpt files, since if anyone want to use them, then they will meet the same problem as mine.", "Quick example:\r\n```python\r\nimport tensorflow as tf\r\nfrom nets import lenet\r\n\r\nx = tf.placeholder(shape=[64,64,64,3], dtype=tf.float32)\r\nlenet = lenet.lenet(x)\r\n\r\nsess = tf.Session()\r\ntf.train.write_graph(sess.graph_def, '/tmp/my-model', 'train.pbtxt')\r\n```\r\n\r\nAnd that's your .pbtxt file.", "@civilman628 I'm guessing those `.ckpt` files\u00a0are read by some `.py` script that sets up the model, and loads the checkpoint. They are not meant to be used directly.", "OK, I'm assuming that's working as intended, `.ckpt` files are useless on their own, but are intended to be used by a script that builds a model, sets up saver, and loads them. This seems like a request for more documentation, but it's missing details of which/where documentation is missing. I'll close it for now, but if you can provide more information I'll reopen", "the above approach, which gets .pbtxt and then freeze into .pb does not work, see the defect:\r\n\r\n#7172 ", "Actually, the .ckpt files are sufficient with the model code. Fore details, please see my answer here #7172 ", "vgg_19 and inception .pb files that we get from the pretrained models link are not frozen graphs. Without having a checkpoint, I cannot use freeze_graph.py. How can I generate a frozen graph from the pretrained model here? It would have been nice if the pretrained models link provided the .pb file along with the checkpoint, so we could freeze it easily.", "I am trying to use a pretrained nasnet from the tf slim and I see only model.ckpt.data-00000-of-00001 and model.ckpt.index. I am unsure if I can do anything with just these 2 files without the meta. Any way to convert it to .pb ? Also wondering why would tensorflow not release a .meta file! ", "Hi, You can download its weights from here:\r\nhttps://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\r\n\r\nAfter downloading this, you can get values of its convolution  filters in a sequence using 'h5py'\r\ne.g.\r\nh5 = h5py('Path/to/.h5')\r\nconv_name = 'conv2d_1'\r\nconv2d_1 = h5[conv_name][conv_name]['kernel:0']\r\n\r\nNow, conv2d_1 contains filter values of its first convolution filter in a model.\r\n"]}, {"number": 6953, "title": "Error when generating Python tools", "body": "I attempted to generate the python tools with the following code from commit b03beb0:\r\n```\r\nbazel build tensorflow/python/tools:freeze_graph && \\\r\nbazel-bin/tensorflow/python/tools/freeze_graph \\\r\n--input_graph=some_graph_def.pb \\\r\n--input_checkpoint=model.ckpt-8361242 \\\r\n--output_graph=/tmp/frozen_graph.pb --output_node_names=softmax\r\n```\r\nThe result was the following:\r\n```\r\nMacBook-Air-2:tensorflow kevin$ bazel build tensorflow/python/tools:freeze_graph && bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=some_graph_def.pb --input_checkpoint=model.ckpt-8361242 --output_graph=/tmp/frozen_graph.pb --output_node_names=softmax\r\nWARNING: Bazel Android NDK crosstools are based on Android NDK revision 12. The revision of the Android NDK given in android_ndk_repository rule 'androidndk' is '13.1.3345770'.\r\nERROR: /Users/kevin/tensorflow/tensorflow/core/BUILD:1207:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /Users/kevin/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /Users/kevin/tensorflow/tensorflow/core/BUILD:1207:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /Users/kevin/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /Users/kevin/tensorflow/tensorflow/core/BUILD:1207:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /Users/kevin/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: Analysis of target '//tensorflow/python/tools:freeze_graph' failed; build aborted.\r\nINFO: Elapsed time: 31.740s\r\n```\r\nRunning on MacOS 10.12.2.", "comments": ["I suspect you did not run the `./configure` script before running `bazel build` as mentioned in [building from sources](https://www.tensorflow.org/get_started/os_setup#configure_the_installation) instructions.\r\n\r\nLet us know if running it fixes your build.", "Thank you.  Yes, that fixed it.\r\nFollowing the instructions, I used \r\n```\r\n./configure\r\n```\r\nthen\r\n```\r\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nand then used the following to install to python:\r\n```\r\nbash bazel-bin/tensorflow/tools/pip_package/build_pip_package /Users/kevin/anaconda2/lib/python2.7/site-packages\r\n```"]}, {"number": 6952, "title": "Update zlib URL in r0.9 branch", "body": "I don't know if anyone still uses 0.9 from source, but if they do, this PR is going to preemptively make them happy <('.'<)\r\n\r\nSee also: #6950 and #6865 ", "comments": ["Some of our tests didn't exist back then, so failures of e.g. Makefile are expected and harmless.", "I see this error in sanity build:\r\n```\r\nERROR: no such package '@protobuf//': Error cloning repository: https://github.com/google/protobuf: cannot open git-upload-pack caused by https://github.com/google/protobuf: cannot open git-upload-pack caused by https://github.com/google/protobuf: cannot open git-upload-pack caused by https://github.com/google/protobuf: cannot open git-upload-pack caused by https://github.com/google/protobuf:\r\n```\r\n\r\nAre we ok with leaving this one in?\r\nI think this change is OK. But if the build is in a broken state, it wont help, I think.", "`git_repository` is evil and jgit is buggy, so maybe that's just a weird flake. Also I'm not sure if we officially maintain this branch anymore, so the way I see it, this PR makes it less obviously broken than before."]}, {"number": 6951, "title": "Update zlib URL in r0.10 branch", "body": "This change fixes the broken zlib URL in yet another branch. Fixes #6950.", "comments": ["Jenkins, test this please. Don't give up!", "Jenkins sanity tests are failing with java certificate issues.\r\nThat in turn aborts everything else.\r\nNot sure how we can make this pass without making more changes into the branch."]}, {"number": 6950, "title": "Error downloading zlib 1.2.8 in TensorFlow r0.10", "body": "While building TensorFlow r0.10, we faced below error:\r\n`zlib_archive: Error downloading http://zlib.net/zlib-1.2.8.tar.gz`\r\n\r\nIt looks like the issue has been fixed in r0.12 through [commit](https://github.com/tensorflow/tensorflow/commit/1e317b1f7dc5ccf04fc51ac96d97f5bdaefa9af9).\r\n\r\nWill it be possible to include same change for r0.10?\r\n\r\n\r\n\r\n", "comments": ["Oh my goodness. It looks like I should just cherry pick https://github.com/tensorflow/tensorflow/commit/96e3e6f70ad9c441be46fcf70449e810a4b36896 on all the branches.", "You're going to be so happy to hear that this is never going to happen again in the future, thanks to the incredible improvements we made to the Bazel downloader. Policy now dictates we have redundant mirrors for everything. It's going to do wonders for preserving the archival integrity of this software. https://github.com/bazelbuild/bazel/commit/ed7ced0018dc5c5ebd6fc8afc7158037ac1df00d", "It looks like redundant mirrors are needed for nasm:\r\n\r\n```\r\nERROR: /home/tom/.cache/bazel/_bazel_tom/0eaf2b6b7a8618527eb96256b130e46f/external/jpeg/BUILD:162:1: no such package '@nasm//': Error downloading [http://www.nasm.us/pub/nasm/releasebuilds/2.12.02/nasm-2.12.02.tar.bz2] to /home/tom/.cache/bazel/_bazel_tom/0eaf2b6b7a8618527eb96256b130e46f/external/nasm/nasm-2.12.02.tar.bz2: Connection refused (Connection refused) and referenced by '@jpeg//:simd_x86_64_assemblage23'.\r\nERROR: /home/tom/.cache/bazel/_bazel_tom/0eaf2b6b7a8618527eb96256b130e46f/external/jpeg/BUILD:162:1: no such package '@nasm//': Error downloading [http://www.nasm.us/pub/nasm/releasebuilds/2.12.02/nasm-2.12.02.tar.bz2] to /home/tom/.cache/bazel/_bazel_tom/0eaf2b6b7a8618527eb96256b130e46f/external/nasm/nasm-2.12.02.tar.bz2: Connection refused (Connection refused) and referenced by '@jpeg//:simd_x86_64_assemblage23'.\r\nERROR: Evaluation of query \"deps((//tensorflow/... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\r\n```\r\n\r\n(on the head of r0.12)", "created issue #6956 for the nasm error", "Closing this to tidy up issues. Feel free to re-open another issue with a different title if you want to track."]}, {"number": 6949, "title": "Bazel building error : proxy address 127.0.0.1:8118 is not a valid URL", "body": "I try to use bazel to build an Android APK by following https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android\r\n\r\nbut I meet download error like this after use this commend **bazel build -c opt //tensorflow/examples/android:tensorflow_demo**:\r\n\r\n> _/home/gehen/tensorflow/tensorflow/examples/android/BUILD:58:1: no such package '@inception5h//': Error downloading [https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip] to /home/gehen/.cache/bazel/_bazel_gehen/9dc623b792acc42ff703ada723a44292/external/inception5h/inception5h.zip: Proxy address 127.0.0.1:8118 is not a valid URL and referenced by '//tensorflow/examples/android:tensorflow_demo'._\r\n\r\nI use these attempt to fix this error ,but all failed.\r\n1.**unset http_proxy**\r\n     this report the same error\r\n2.corret the http_proxy.\r\n    Actually I use the http://localhost:1080 as my proxy , but I don't known why it change into 127.0.0.1:8118, so I use export **http_proxy=http://localhost:1080** and **https_proxy=http://localhost:1080**, it can not download\r\n\r\n _Timeout connecting to https://storage.googleapis.com/download.tensorflow\\\r\n.org/models/mobile_multibox_v1.zip_\r\n\r\n3.I find I can wget the *.zip seperately , so why can't I get the sources from bazel , what might the resons .\r\n\r\nThank you for your reply\r\n\r\nI'm almost crazy about it\r\n\r\n", "comments": ["This doesnt sound like a TF issue -- you'll have better luck asking at http://github.Com/bazelbuild or on stackoverflow. In the meantime there's the manual download at least."]}, {"number": 6948, "title": "fix typos in tensor constructor docs", "body": "A fix for a minor typo in the tensor.h constructor docs. This was causing \"Copy constructor\" to appear under the move constructor declaration, and was hiding the move constructor description in the generated docs.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 6947, "title": "fix few signed/unsigned warnings", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 6946, "title": "How do I use mkl's matmul in MatMulOp::Compute()?", "body": "I've been trying to modify core/kernels/matmul_op.cc to use mkl's matmul api. Although I can link mkl without any problem to compile my simple standalone c++ file, things don't work after adding the linking option to matmul_op's rule.\r\ncopts = [\"-Wl,--start-group /opt/intel/compilers_and_libraries_2017.0.098/linux/mkl/lib/intel64/libmkl_intel_lp64.a /opt/intel/compilers_and_libraries_2017.     0.098/linux/mkl/lib/intel64/libmkl_gnu_thread.a /opt/intel/compilers_and_libraries_2017.0.098/linux/mkl/lib/intel64/libmkl_core.a -Wl,--end-group -lgomp -lpthread -lm -ldl\"]\r\n\r\nI also used the mkl link option when compiling a test that uses the matmul op.\r\n\r\nLike this:\r\n\r\nbazel build -c opt tensorflow/tools/benchmark:2mklmatmul_test --linkopt=\"-L${MKLROOT}/lib/intel64/libmkl_intel_lp64 -lmkl_gnu_thread -lmkl_core -lgomp -lpthread -lm -ldl\"\r\n\r\nHowever, tensorflow can't find the definition of mkl's matmul function during linking.\r\n\r\nPlease note that I understand that I should add a new op. I did this just to quickly see whether I can link MKL in tensorflow.", "comments": ["Did you see this PR? https://github.com/tensorflow/tensorflow/pull/6921/commits", "The changes to ./configure in change above give the steps needed to link MKL against TensorFlow, let us know if it doesn't work", "Closing due to lack of recent activity. Please update the issue if it persists and we will reopen."]}, {"number": 6945, "title": "Gradle builds for Tensorflow Android does not automatically fetch the models", "body": "Builds work fine using bazel.\r\nHowever when the project is imported into android studio using gradle, the assets directory remains empty. The app crashes on startup. When the models are manually downloaded and placed into the asset folder, the gradle builds work. Not sure if this was intended. ", "comments": ["Yes, this is intended, though perhaps the [example's README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md) could make this clearer.\r\n\r\nIf you'd like to send in a pull request that makes gradle download/copy the asset, we'd welcome that. Thanks!\r\n\r\nFYI @andrewharp ", "@ggfan has a solution in his cmake work: https://github.com/ggfan/tensorflow/blob/android-studio-port/tensorflow/examples/android/cmake/app/build.gradle\r\n\r\nWe should be able share the same solution in examples/android/build.gradle once this is ready."]}, {"number": 6944, "title": "Fix: Fix timing flakiness by mocking time object in", "body": "basic_session_run_hooks_test.py", "comments": []}, {"number": 6943, "title": "Adding new vocab words to seq2seq machine translation model during training?", "body": "I am using the seq2seq tutorial to play with machine translation. Say I have trained the model for some time and determine that I want to supplement the original vocab with new words to enhance the quality of the model. Is there a way to pause training, add words to the vocabulary, and then resume training from the most recent checkpoint? I attempted to do so but when I began training again I got this error:\r\n\r\nTraceback (most recent call last):\r\nFile \"execute.py\", line 405, in <module>\r\ntrain()\r\nFile \"execute.py\", line 127, in train\r\nmodel = create_model(sess, False)\r\nFile \"execute.py\", line 108, in create_model\r\nmodel.saver.restore(session, ckpt.model_checkpoint_path)\r\nFile \"/home/jrthom18/.local/lib/python2.7/site-    packages/tensorflow/python/training/saver.py\", line 1388, in restore\r\n{self.saver_def.filename_tensor_name: save_path})\r\nFile \"/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\nrun_metadata_ptr)\r\nFile \"/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\nfeed_dict_string, options, run_metadata)\r\nFile \"/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\ntarget_list, options, run_metadata)\r\nFile \"/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\nraise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Assign   requires shapes of both tensors to match. lhs shape= [384633] rhs shape=   [384617]\r\n [[Node: save/Assign_82 = Assign[T=DT_FLOAT, _class=[\"loc:@proj_b\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](proj_b, save/RestoreV2_82)]]\r\n\r\nCaused by op u'save/Assign_82', defined at:\r\nFile \"execute.py\", line 405, in <module>\r\ntrain()\r\nFile \"execute.py\", line 127, in train\r\nmodel = create_model(sess, False)\r\nFile \"execute.py\", line 99, in create_model\r\nmodel = seq2seq_model.Seq2SeqModel( gConfig['enc_vocab_size'],  gConfig['dec_vocab_size'], _buckets, gConfig['layer_size'], gConfig['num_layers'], gConfig['max_gradient_norm'], gConfig['batch_size'], gConfig['learning_rate'], gConfig['learning_rate_decay_factor'], forward_only=forward_only)\r\nFile \"/home/jrthom18/data/3x256_bs32/easy_seq2seq/seq2seq_model.py\", line 166, in __init__\r\nself.saver = tf.train.Saver(tf.global_variables(), keep_checkpoint_every_n_hours=2.0)\r\nFile \"/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\r\nself.build()\r\nFile \"/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1030, in build\r\nrestore_sequentially=self._restore_sequentially)\r\nFile \"/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 624, in build\r\nrestore_sequentially, reshape)\r\nFile \"/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 373, in _AddRestoreOps\r\nassign_ops.append(saveable.restore(tensors, shapes))\r\nFile \"/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 130, in restore\r\nself.op.get_shape().is_fully_defined())\r\nFile \"/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\r\nuse_locking=use_locking, name=name)\r\nFile \"/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\nop_def=op_def)\r\nFile \"/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\noriginal_op=self._default_original_op, op_def=op_def)\r\nFile \"/home/jrthom18/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\nself._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [384633] rhs shape= [384617]\r\n [[Node: save/Assign_82 = Assign[T=DT_FLOAT, _class=[\"loc:@proj_b\"],   use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](proj_b, save/RestoreV2_82)]]\r\n\r\nObviously the new vocab is larger and so the tensor sizes do not match. Is there some way around this?", "comments": ["This question is probably better posed on [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) as we try to keep github issues focused on bugs and feature requests"]}, {"number": 6942, "title": "TensorFlow for Python3.4::The specified key does not exist", "body": "I am trying to install TensorFlow for GPU and I have CUDA8 and CUDNN in Ubuntu 14.04.\r\nHere's the errors I received. Please suggest solution:\r\n```\r\n\r\n$ sudo pip3 install --upgrade $TF_BINARY_URL\r\nDownloading/unpacking https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp34\r\n  HTTP error 404 while getting https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp34\r\nCleaning up...\r\nException:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/pip/basecommand.py\", line 122, in main\r\n    status = self.run(options, args)\r\n  File \"/usr/lib/python3/dist-packages/pip/commands/install.py\", line 278, in run\r\n    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)\r\n  File \"/usr/lib/python3/dist-packages/pip/req.py\", line 1198, in prepare_files\r\n    do_download,\r\n  File \"/usr/lib/python3/dist-packages/pip/req.py\", line 1376, in unpack_url\r\n    self.session,\r\n  File \"/usr/lib/python3/dist-packages/pip/download.py\", line 547, in unpack_http_url\r\n    resp.raise_for_status()\r\n  File \"/usr/share/python-wheels/requests-2.2.1-py2.py3-none-any.whl/requests/models.py\", line 773, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 404 Client Error: Not Found\r\n\r\nStoring debug log for failure in /home/mona/.pip/pip.log\r\n```\r\n\r\nHere's the log:\r\n```\r\n  1 ------------------------------------------------------------\r\n  2 /usr/bin/pip3 run on Wed Jan 18 16:30:31 2017\r\n  3 Downloading/unpacking https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp34\r\n  4   HTTP error 404 while getting https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp34\r\n  5 Cleaning up...\r\n  6   Removing temporary dir /tmp/pip_build_root...\r\n  7 Exception:\r\n  8 Traceback (most recent call last):\r\n  9   File \"/usr/lib/python3/dist-packages/pip/basecommand.py\", line 122, in main\r\n 10     status = self.run(options, args)\r\n 11   File \"/usr/lib/python3/dist-packages/pip/commands/install.py\", line 278, in run\r\n 12     requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)\r\n 13   File \"/usr/lib/python3/dist-packages/pip/req.py\", line 1198, in prepare_files\r\n 14     do_download,\r\n 15   File \"/usr/lib/python3/dist-packages/pip/req.py\", line 1376, in unpack_url\r\n 16     self.session,\r\n 17   File \"/usr/lib/python3/dist-packages/pip/download.py\", line 547, in unpack_http_url\r\n 18     resp.raise_for_status()\r\n 19   File \"/usr/share/python-wheels/requests-2.2.1-py2.py3-none-any.whl/requests/models.py\", line 773, in raise_for_status\r\n 20     raise HTTPError(http_error_msg, response=self)\r\n 21 requests.exceptions.HTTPError: 404 Client Error: Not Found\r\n\r\n```\r\n\r\n![screenshot from 2017-01-18 16-29-24](https://cloud.githubusercontent.com/assets/1892917/22085884/aafbcb72-dd9b-11e6-99e9-6acea8f25526.png)\r\n", "comments": ["I guess you used incomplete link:\r\n\r\nfor python 2.7 try : \r\n[https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl](https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl)\r\n\r\nand for python 3.4 try:\r\n[https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp34-cp34m-linux_x86_64.whl](https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp34-cp34m-linux_x86_64.whl)\r\n\r\ninstead of \r\n[https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp34](https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp34)", "Thank you! I see what happened! I'm so embarrassed thanks to that horizontal scroll \r\n![screenshot from 2017-01-18 16-59-44](https://cloud.githubusercontent.com/assets/1892917/22086699/979c4ddc-dd9f-11e6-8948-2335c908aa56.png)\r\n", "No worries, glad it helped :)"]}, {"number": 6941, "title": "Add pre-compilation of Pascal binaries (CC 6.0/6.1)", "body": "See issue #6914.\r\n\r\nMentioning @vrv since he gave the go-ahead for this.", "comments": ["Can one of the admins verify this patch?", "@gunan let us know if this is okay, shouldn't need to run a bunch of tests...", "@vrv @gunan This should be fine as long as you have CUDA 8.0 (or an equivalent version of Google-CUDA) available on the CI and Docker build server, so as to support Pascal CCs. Since we're not changing sass-based codes here (same CUBLAS and CUDNN versions), this should be fine, IMHO.", "@mkolod agreed, just want to make sure there aren't any unexpected configs we have to update first :)", "@vrv @gunan Also, since this is a PR to master (rather than 0.12.1 that gcr.io/tensorflow/tensorflow:latest-devel-gpu is currently based on), I assume this won't affect containers until at least 1.0, unless an update is made to 0.12.1, which I assume is probably not advised at this point, but of course it's your decision :)", "There is another place where capability versions are specified in the codebase:\r\ntensorflow/contrib/cmake/CMakeLists.txt\r\nLine 166\r\n\r\nIs it necessary to change that as well?", "not in this change: the goal is to only fix the development gpu docker images.  expanding the releases to do more will involve changing a lot of other things.", "Change looks good to me, @caisq, any objections?", "LGTM", "@vrv @caisq @gunan I was also wondering if this change to master could be cherry-picked for the final 1.0 release, or maybe even now. From what I understand, the concern is not the Pascal cubins in the Docker containers, but the pip package size limitations. So, maybe it would be OK to add this change to r1.0 as well for the gpu-devel container, once \"latest\" is released with r1.0? I know of several public users who would appreciate that.", "It doesn't change the binary, just the docker at the branch, so it seems okay to me.  @martinwicke  wdyt?", "@vrv @martinwicke For the r0.12 users though, would it be possible to also update the shippable compute capabilities so that users get the Pascal cubins in the nightlies even now?", "we're probably going to release 1.0 imminently, and then the default pip install / nightlies will use 1.0, so I'd rather just wait for that.", "Sounds good.", "@vrv Also, I think I have a solution to the pip package size limitation problem. There is an nvcc flag (not sure about Google's CUDA compiler) `-Xfatbin -compress-all` which compresses the cubins for storage, but uncompresses at runtime. This would enable pip packages to ship with Pascal too, while using less storage (it would also reduce the size of existing pip packages and Docker containers). Unfortunately, while I know how to pass gcc flags to Bazel, I'm not sure how to pass the nvcc ones - otherwise I would have issued a PR for that as well.", "@vrv @martinwicke @gunan @zheng-xq The nightly is still shipping without Pascal support, so this change hasn't been cherry-picked into the 1.0 branch, and hence the 1.0 gpu-devel Docker image. The way I tested it was via cuobjdump, but it's clear from the 1.0 branch as well (see [here](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/tools/docker/Dockerfile.devel-gpu#L92)).\r\n\r\n```\r\n$ nvidia-docker run --rm -it gcr.io/tensorflow/tensorflow:latest-devel-gpu /bin/bash\r\nroot@ee4171f66f39:~# cuobjdump /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so | grep sm_ | sort | uniq\r\narch = sm_30\r\narch = sm_35\r\narch = sm_52\r\n\r\n```"]}, {"number": 6940, "title": "use context to cleanly close the file descriptor", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "By the way, the reason `open('a','w').write('x')` works is because `close` is run by `__del__`, which is called immediately on dereference"]}, {"number": 6939, "title": "Leave old binaries up?", "body": "I've noticed that the binaries for older versions have been taken down. In particular, I need to install 0.11.rc0 and both \r\n\r\n`https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.11.rc0-cp27-none-linux_x86_64.whl`\r\n\r\nand\r\n\r\n`https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.11.0-cp27-none-linux_x86_64.whl`\r\n\r\nare 404s. Pretty sure they used to work when 0.11.rc0 was the latest though. \r\n\r\nWere they intentionally taken down to prevent people from unwittingly installing older versions? That would be understandable, but it would be really nice if there was an easy way to access older binaries. Greatly looking forward to the release of 1.0, but for the time being there's a nest of dependency conflicts to manage and that's difficult when it's not clear how to install old versions!\r\n", "comments": ["I believe the naming convention changed (CCing @yifeif @gunan  to confirm) between 0.11 and 0.12, but the URLs of the earlier releases are still accessible. For example:\r\n\r\n- 0.11.0rc0: `https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl`\r\n- 0.11.0:`\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl`\r\n\r\nClosing this out as the URLs are still accessible. Let me know if I misunderstood. \r\n\r\n", "That's correct, as @asimshankar mentioned, we changed the naming convention so we could upload both cpu and gpu binaries to pypi. @rueberger you can find the full list of binary urls for r0.11 at https://www.tensorflow.org/versions/r0.11/get_started/os_setup#using_pip", "Wonderful, thanks!"]}, {"number": 6938, "title": "Make XLA command-line option to mnist_softmax_xla.py actually work.", "body": "The --xla command-line argument to mnist_softmax_xla.py is effectively always true, so the example on https://www.tensorflow.org/versions/master/experimental/xla/jit (i.e. `python mnist_softmax_xla.py --xla=false`) still runs XLA.  This PR gives the script --xla and --no-xla optional arguments, with --xla as the default.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "cc @hawkinsp in case he wants to fix this in another way", "@ndronen did you sign the CLA? If so, please reply to the thread as advised. Thanks.", "Hi, @drpngx, it's going to be a while before the legal department will get around to approving it.  Not sure if they're even going to act on it.  My apologies!", "Got it, thanks for letting us know!", "I'm going to close this PR so we keep the number of active PRs low  -- feel free to send this to us again when you can sign the CLA -- we really appreciate it!"]}, {"number": 6937, "title": "Confusing arg name for sequence_loss", "body": "Sorry if this is noise, but it's just a minor suggestion to make the API better (before the release of TensorFlow 1.0 where it will be much more difficult to change)\r\n\r\nFor the function [sequence_loss](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py) on the master branch, the name of the parameter `softmax_loss_function` seems to imply that only softmax based loss are compatible which is not true (for instance `tf.nn.sigmoid_cross_entropy_with_logits` works too). Maybe the name was that just to indicate the default behavior but I think it is misleading.\r\n\r\nWhy not simply call the parameter `loss_function` ? It's more representative of what the parameter really does.", "comments": ["`contrib` and its submodules are not covered by API stability guarantees, so we do have an opportunity to change them. This was introduced in PR #4382 \r\n\r\n@lukaszkaiser @ebrevdo : Any opinions here?", "I think all that code belonged in seq2seq_legacy, it doesn't seem to really\nbe a good fit for the new seq2seq API.\n\nOn Jan 18, 2017 11:08 AM, \"Asim Shankar\" <notifications@github.com> wrote:\n\n> contrib and its submodules are not covered by API stability guarantees,\n> so we do have an opportunity to change them. This was introduced in PR\n> #4382 <https://github.com/tensorflow/tensorflow/pull/4382>\n>\n> @lukaszkaiser <https://github.com/lukaszkaiser> @ebrevdo\n> <https://github.com/ebrevdo> : Any opinions here?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6937#issuecomment-273569807>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-K6AZzFImUtWSt78An5Fitmd6ukks5rTmM-gaJpZM4LnQBq>\n> .\n>\n", "Automatically closing due to lack of recent activity.\r\nplease feel free to reopen if this is still an issue, or better yet, send a pull request with the proposed fix."]}, {"number": 6936, "title": "Change combiner default for crossed_column", "body": "", "comments": ["There are test failures. Can you please take a look, @terrytangyuan ", "Sorry, it appears we got ahead of ourselves with that deprecation. It's not clear the new combiner is superior, and changing it does cause a lot of trouble for people with existing models. @ispirmustafa, this is fallout from 31439963. We should remove the deprecation notice.", "That's fine. Thanks for letting me know. Just randomly came across it and edited on Github."]}]