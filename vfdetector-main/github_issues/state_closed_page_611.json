[{"number": 35331, "title": "Fix bug in documentation of  tf.while_loop.parallel_iterations", "body": "This PR fixes a documentation bug mentioned in #18257 and also raised in one stack overflow [question](https://stackoverflow.com/questions/59299060/tf-2-0-while-loop-and-parallel-iterations). Since this is my first PR in tensorflow, I've initially just fixed this bug in documentation, please let me know if I should also do some related follow up work. Thanks", "comments": []}, {"number": 35330, "title": "Keras backend functions not working as intended?", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: 4.4.0-18362-Microsoft\r\n- TensorFlow installed from: Anaconda default source\r\n- TensorFlow version: 1.15\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GeForce RTX 2060\r\n\r\n**Describe the current behavior**\r\n\r\nI'm trying to implement a custom loss function based on a custom accuracy function that I'm already using to evaluate my model predictions on the test dataset. The conversion can't be 1:1 because I use numpy \"greater\" and \"equal\" functions that are not differentiable. I created thefore custom functions that approximate the latters but their behavior has some problems\r\n\r\n**Describe the expected behavior**\r\n\r\nIn particular I can test if everything is fine by comparing the results obtained by my original custom accuracy f. and the new loss f. given the same input (my input are tensorflow predictions, I just inglobe them inside K.constant to convert them in tensors). What I noticed is that this line of code\r\n\r\n```\r\neps = sys.float_info.epsilon\r\nreturn 0.5*(y + 5 + K.sqrt(K.pow(y-5,2) + eps))\r\n```\r\n\r\nis problematic. In particular y is an array of float32 values in [1:10] range and the returned array, let's call it 'ret', should have ret[i]=max(5,y[i]) but sometimes the value of **5** becomes **4.9999995** instead. The next portion of my code is based on how many **5** are present and therefore I can't ignore this problem.\r\n\r\nThe fact is that, let's say a problematic index is 'w' so that ret[w]=4.9999995 instead of 5, if I use the same code with y now equal to only y[w] the returned array is correctly 5. This means that somehow if y is a batch of predictions and not just one something isn't working. This should not be the case because both K.sqrt and K.pow works element wise, it should not matter if y is an array of 1 or multiple values \r\n\r\nOut of almost 20k predictions, around 1k have this same problem and it is deterministic (always the same are problematic). I also tried to use:\r\n\r\n```\r\neps = sys.float_info.epsilon\r\nreturn 0.5*(y + 5 + np.sqrt(np.pow(y-5,2) + eps))\r\n```\r\n\r\nand the problem is gone thefore it is related to Keras backend.\r\n\r\nLast info, I tried to use also:\r\n\r\n```\r\neps = sys.float_info.epsilon\r\nreturn tf.math.ceil(0.5*(y + 5 + K.sqrt(K.pow(y-5,2) + eps)))\r\n```\r\n\r\nbut this completely ruins the returned value, sometimes real numbers such as 4.5 are rounded to 6 instead of 5\r\n\r\nIf more informations are needed I can provide them", "comments": ["@korr4k,\r\nIn order to expedite the trouble-shooting process, please provide the complete code snippet to reproduce the issue reported here. Thanks!", "```Python\r\nfrom tensorflow.compat.v1.keras import backend as K\r\nimport numpy as np\r\nimport sys\r\neps = sys.float_info.epsilon\r\n\r\ndef compute_wms_keras(y):\r\n    # Numpy vector of weights\r\n    w = np.arange(1,11).astype('float32')\r\n    w = np.reshape(w, (10,1))\r\n    w = tf.convert_to_tensor(w, dtype=tf.float32)\r\n\r\n    # wms of y by w\r\n    return K.dot(y, w)\r\n\r\ndef greater_approx(y, threshold):    \r\n    return 0.5*(y + threshold + K.sqrt(K.pow(y-threshold,2) + eps))\r\n\r\ndef gaussian(x, mu):\r\n    sig=eps\r\n    return K.exp(-K.pow((x - mu),2) / (2.0 * sig*sig))\r\n\r\ndef custom_accuracy_keras(y, y_first):\r\n\r\n    # Compute the wms of both y and y_first\r\n    y_wms = compute_wms_keras(y)\r\n    y_first_wms = compute_wms_keras(y_first)\r\n\r\n    # greater_approx and gaussian are the approximation functions of \"greater\"\r\n    # and \"equal\" respectively\r\n    greater_approx_y_wms = greater_approx(y_wms, 5.0)\r\n    greater_approx_y_first_wms = greater_approx(y_first_wms, 5.0)\r\n    \r\n    gaussian_y_wsm = gaussian(greater_approx_y_wms, 5.0)\r\n    gaussian_y_first_wms = gaussian(greater_approx_y_first_wms, 5.0)\r\n    \r\n    matches = gaussian(gaussian_y_wsm, gaussian_y_first_wms)\r\n    \r\n    # matches at this point is an array of 0 and 1, 0 when the accuracy test\r\n    # failed and 1 otherwise\r\n    return y_wms, y_first_wms, greater_approx_y_wms, greater_approx_y_first_wms, gaussian_y_wsm, gaussian_y_first_wms, matches, K.mean(matches)\r\n```\r\n\r\nBoth input y and y_first are array of shape (x,10), where x is the number of elements in the dataset, with \"compute_wms\" they are converted to shape (x,1). Note that \"custom_accuracy_keras\" should return only K.mean(matches) but to identify the problem it is temporary changed\r\n\r\nThe orginal numpy accuracy function, the one correct, is:\r\n\r\n```Python\r\nimport numpy as np\r\n\r\ndef compute_wms(y):\r\n    # Numpy vector of weights\r\n    w = np.arange(1,11).astype('float32')\r\n    w = np.reshape(w, (10,1))\r\n\r\n    # wms of y by w\r\n    return np.dot(y, w)\r\n\r\ndef custom_accuracy(y, y_first):\r\n\r\n    # Compute the wms of both y and y_first\r\n    y_wms = compute_wms(y)\r\n    y_first_wms = compute_wms(y_first)\r\n\r\n    # Are the obtained values greater than 5?\r\n    y_true_bool = np.greater(y_wms, 5)\r\n    y_pred_bool = np.greater(y_first_wms, 5)\r\n\r\n    # The results of the previous tests are compared\r\n    matches = np.equal(y_true_bool, y_pred_bool).astype('float32')\r\n\r\n    # The mean of the previous test is returned\r\n    return y_wms, y_first_wms, matches, np.mean(matches)\r\n```\r\n\r\nNote same as above for the returned value.\r\n\r\nTo test the code, after loading y and y_first numpy array, I use this code:\r\n\r\n```Python\r\nfrom tensorflow.compat.v1.keras import backend as K\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\nimport numpy as np\r\n\r\ncorrect_results = custom_accuracy(y,y_first)\r\n\r\n# Originally I used K.constant instead of tf.convert_to_tensor but I think it is the same\r\ny_wms, y_first_wms, greater_approx_y_wms, greater_approx_y_first_wms, gaussian_y_wsm, gaussian_y_first_wms, matches, meatches_mean = custom_accuracy_keras(tf.convert_to_tensor(y), tf.convert_to_tensor(y_first))\r\n\r\ny_wms = y_wms.eval(session=sess)\r\ny_first_wms = y_first_wms.eval(session=sess)\r\ngreater_approx_y_wms = greater_approx_y_wms.eval(session=sess)\r\ngreater_approx_y_first_wms = greater_approx_y_first_wms.eval(session=sess)\r\ngaussian_y_wsm = gaussian_y_wsm.eval(session=sess)\r\ngaussian_y_first_wms = gaussian_y_first_wms.eval(session=sess)\r\nmatches = matches.eval(session=sess)\r\nmeatches_mean = meatches_mean.eval(session=sess)                                                                                                                                            \r\n\r\nerrors = []\r\nindexes= []\r\n        \r\nfor i in range(matches.size):            \r\n    if(matches[i][0]!=correct_results[2][i]):\r\n        errors.append((y_wms[i][0],\r\n                       y_first_wms[i][0],\r\n                       greater_approx_y_wms[i][0],\r\n                       greater_approx_y_first_wms[i][0],\r\n                       gaussian_y_wsm[i][0],\r\n                       gaussian_y_first_wms[i][0],\r\n                       matches[i][0]))\r\n        indexes.append(i)\r\n```\r\nAt this point in \"errors\" I have all the data relative to input with problems and in \"indexes\" their relative position\r\n\r\nTo test what I reported the code is:\r\n\r\n```Python\r\ntest = []\r\n        \r\nfor i in indexes[:10]:\r\n    y_wms, y_first_wms, greater_approx_y_wms, greater_approx_y_first_wms, gaussian_y_wsm, gaussian_y_first_wms, matches, meatches_mean = custom_accuracy_keras(tf.convert_to_tensor(y[i].reshape((1, 10)), dtype=tf.float32), tf.convert_to_tensor(y_first[i].reshape((1, 10)), dtype=tf.float32))\r\n            \r\n    test.append((y_wms[0].eval(session=sess)[0],\r\n                 y_first_wms[0].eval(session=sess)[0],\r\n                 greater_approx_y_wms[0].eval(session=sess)[0],\r\n                 greater_approx_y_first_wms[0].eval(session=sess)[0],\r\n                 gaussian_y_wsm[0].eval(session=sess)[0],\r\n                 gaussian_y_first_wms[0].eval(session=sess)[0],\r\n                 matches[0].eval(session=sess)[0]))\r\n```\r\nWhat I did here is just extracting the singol inputs with \"problems\" from the original batch and process them alone\r\n\r\nNow the first 10 (I use only the first 10 for simplicity) tuples of \"errors\" should be the same as \"test\" but they aren't and most importantly the latter has the correct/expected results.\r\nNote the colomuns where the **5** becomes **4,9999995** (4th or 5th) but also the first two don't have the exact same values! Maybe the two things are related\r\n\r\n[Input.zip](https://github.com/tensorflow/tensorflow/files/3994594/Input.zip): this is a zip with both the numpy array y and y_first ", "Was able to reproduce the issue. Please find the [Gist](https://colab.sandbox.google.com/gist/amahendrakar/090aeeccec85f1a475bfd90f3f241d83/35330.ipynb) here. Thanks!", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35330\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35330\">No</a>\n"]}, {"number": 35329, "title": "Error: Cannot convert 'auto' to EagerTensor of dtype float", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/Reduction?version=stable\r\n\r\n## Description of issue (what needs changing):\r\n\r\nI intend to build up a custom loss function as follows:\r\n\r\n\r\n`\tfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\timport functools\r\n\r\n\timport numpy as np\r\n\timport tensorflow as tf\r\n\r\n\r\n\tclass GeneralDiceLoss(tf.keras.losses.Loss):\r\n\t\tdef __init__(self, reduction=tf.keras.losses.Reduction.AUTO, name='GeneralDiceLoss'):\r\n\t\t\tsuper().__init__(reduction=reduction, name=name)\r\n\t\t\tself.epsilon = 1e-16 \r\n\t\t\r\n\t\t\r\n\t\tdef get_config(self):\r\n\t\t\tconfig = super(GeneralDiceLoss, self).get_config()\r\n\t\t\treturn config\r\n\t\t\r\n\t\tdef call(self, yPred, yTrue):\r\n\t\t\t#yTrue =tf.dtypes.cast(yTrue, dtype=yPred.dtype)\r\n\t\t\t# Dot product yPred and yTrue and sum them up for each datum and class\r\n\t\t\tcrossProd=tf.multiply(yPred, yTrue)\r\n\t\t\tcrossProdSum=tf.math.reduce_sum(crossProd, axis=np.arange(2, yTrue.ndim))\r\n\t\t\t# Calculate weight for each datum and class \r\n\t\t\tweight = tf.math.reduce_sum(yTrue, axis=np.arange(2, yTrue.ndim))\r\n\t\t\tweight = tf.math.divide(1, tf.math.square(weight)+self.epsilon)\r\n\t\t\t# Weighted sum over classes\r\n\t\t\tnumerator = 2*tf.math.reduce_sum(tf.multiply(crossProdSum, weight), axis=1)\r\n\t\t\t# Saquared summation \r\n\t\t\tyySum = tf.math.reduce_sum(tf.math.square(yPred) + tf.math.square(yTrue), axis=np.arange(2, yTrue.ndim))\r\n\t\t\t# Weighted sum over classes\r\n\t\t\tdenominator = tf.math.reduce_sum(tf.multiply(weight, yySum), axis=1)\r\n\t\t\tloss = 1 - tf.math.divide(numerator, denominator+self.epsilon)\r\n\t\t\t#loss = tf.math.reduce_mean(1 - tf.math.divide(numerator, denominator+self.epsilon))\r\n\t\t\t\r\n\t\t\treturn loss\r\n`\r\n\r\nThen I create variables to have it test\r\n`\r\n\r\n\tGeneralDiceLoss()\r\n\tyPred = tf.random.uniform(shape=(16, 3, 4, 4, 4))\r\n\tyTrue = tf.round(tf.random.uniform(shape=(16, 3, 4, 4, 4)))\r\n\r\n\tloss=GeneralDiceLoss(yPred, yTrue)\r\n`\r\nBut I got an error\r\n`\r\n\r\n\t  File \"...\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 96, in convert_to_eager_tensor\r\n\t\treturn ops.EagerTensor(value, ctx.device_name, dtype)\r\n\r\n\tTypeError: Cannot convert 'auto' to EagerTensor of dtype float\r\n`\r\n\r\nIn the doc above, \r\n1) there is NO clear indication or warning about conversion issue, not to mention there is NO dtype conversion in my code at all. \r\n2) there is NO clear example indicating which option, AUTO or SUM_OVER_BATCH_SIZE, should be adopted in one's minbatch size is greater than 1. In my case, assume my batch is 16 as exhibted in yPred and yTrue above, shall I use\r\n\r\n`\r\n\t\t\tloss = 1 - tf.math.divide(numerator, denominator+self.epsilon)\r\n`\r\nor \r\n`\r\n\t\t\tloss = tf.math.reduce_mean(1 - tf.math.divide(numerator, denominator+self.epsilon))\r\n`\r\nAnd for which option?\r\n\r\nBuilding up a custom layer/loss function is already a tough task for many practitioners, so could the doc provide more detailed explanations and examples so as to make users' life a little bit easier? Many thanks.", "comments": ["Was able to reproduce the issue. Please find the [Gist](https://colab.sandbox.google.com/gist/amahendrakar/ed9cea3d43ac8815969a2988edec2d61/35329.ipynb) here. Thanks!", "I see. So may I ask what the solution is or where I made mistakes in\nmy code? I cannot see any incompatible shape of tensors. I am quite\nnew to tf.\nThanks.\n\nOn 23/12/2019, amahendrakar <notifications@github.com> wrote:\n> Was able to reproduce the issue. Please find the\n> [Gist](https://colab.sandbox.google.com/gist/amahendrakar/ed9cea3d43ac8815969a2988edec2d61/35329.ipynb)\n> here. Thanks!\n>\n> --\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub:\n> https://github.com/tensorflow/tensorflow/issues/35329#issuecomment-568415113\n", "@amahendrakar So how to change the code and make it work? From the link you gave, I am afraid that I did not see any change. Now, even even a dummy class below raises the same error. \r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass GeneralDiceLoss(tf.keras.losses.Loss):\r\n\tdef __init__(self, reduction=tf.keras.losses.Reduction.AUTO, name='GeneralDiceLoss'):\r\n\t\tsuper().__init__(reduction=reduction, name=name)\r\n\t\r\n\t\r\n\tdef get_config(self):\r\n\t\tconfig = super(GeneralDiceLoss, self).get_config()\r\n\t\treturn config\r\n\r\n\tdef call(self, yPred, yTrue):\r\n            loss = 0\r\n\t\t\r\n\t    return loss\r\n    \r\nGeneralDiceLoss()\r\nyPred = tf.random.uniform(shape=(16, 3, 4, 4, 4))\r\nyTrue = tf.round(tf.random.uniform(shape=(16, 3, 4, 4, 4)))\r\n\r\nloss=GeneralDiceLoss(yPred, yTrue)\r\n```", "Good job!", "@tavco09 Sorry, but I have not seen any solution, yet. Did I miss anything?", "Hello. I would like to work on this issue. Could you please give me some guidance as to how I should proceed?", "@amukh18 Hi, thanks for the message. Please see my code above\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass GeneralDiceLoss(tf.keras.losses.Loss):\r\n\tdef __init__(self, reduction=tf.keras.losses.Reduction.AUTO, name='GeneralDiceLoss'):\r\n\t\tsuper().__init__(reduction=reduction, name=name)\r\n\t\r\n\t\r\n\tdef get_config(self):\r\n\t\tconfig = super(GeneralDiceLoss, self).get_config()\r\n\t\treturn config\r\n\r\n\tdef call(self, yPred, yTrue):\r\n            loss = 0\r\n\t\t\r\n\t    return loss\r\n    \r\nGeneralDiceLoss()\r\nyPred = tf.random.uniform(shape=(16, 3, 4, 4, 4))\r\nyTrue = tf.round(tf.random.uniform(shape=(16, 3, 4, 4, 4)))\r\n\r\nloss=GeneralDiceLoss(yPred, yTrue)\r\n```\r\nEven a dummy class can reproduce the issue. ", "Could you please tell me what 'auto' is in the given error (I'm a bit new to Tensorflow)? I have tried searching this keyword but haven't got any answers.", "I am sorry I do not have a clue, which is partly why I asked the question.", "@pavithrasv When I tried the code on Colab, the following error popped out\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-465b8423adac> in <module>()\r\n      1 import tensorflow as tf\r\n      2 from tensorflow.keras import layers\r\n----> 3 class GeneralDiceLoss(tf.keras.losses.Loss):\r\n      4     def __init__(self, wPow, reduction=tf.keras.losses.Reduction.AUTO, name='GeneralDiceLoss'):\r\n      5         super().__init__(reduction=reduction, name=name)\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/module_wrapper.py in __getattr__(self, name)\r\n    191   def __getattr__(self, name):\r\n    192     try:\r\n--> 193       attr = getattr(self._tfmw_wrapped_module, name)\r\n    194     except AttributeError:\r\n    195       if not self._tfmw_public_apis:\r\n\r\nAttributeError: module 'tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction'\r\n```\r\nBut if I removed `reduction', it showed no error\r\n```\r\nclass GeneralDiceLoss(tf.keras.losses.Loss):\r\n    def __init__(self, name='GeneralDiceLoss):\r\n        super().__init__(name=name)\r\n        self.epsilon = 1e-16 \r\n```\r\nSo can I work that way around? And because I manually let the function to average over classes and batch, will it be working properly?", "You need to create an instance of your loss class before you call it.\r\n\r\n```\r\nloss_func = GeneralDiceLoss()\r\nloss_val = loss_func(yPred, yTrue)\r\n```", "\nThanks buddy!\u00a0\n\n\u041e\u0442\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043e \u0438\u0437 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u043e\u0439 \u041f\u043e\u0447\u0442\u044b Mail.Ru\n\n\u0447\u0435\u0442\u0432\u0435\u0440\u0433, 9 \u0430\u043f\u0440\u0435\u043b\u044f 2020 \u0433., 00:30 +0300 \u043e\u0442 notifications@github.com  <notifications@github.com>:\n>You need to create an instance of your loss class before you call it.\n>loss_func = GeneralDiceLoss()\n>loss_val = loss_func(yPred, yTrue)\n>\n>\u2014\n>You are receiving this because you were mentioned.\n>Reply to this email directly,  view it on GitHub , or  unsubscribe .", "> Could you please tell me what 'auto' is in the given error (I'm a bit new to Tensorflow)? I have tried searching this keyword but haven't got any answers.\r\n\r\n@amukh18 AUTO reduction is default when losses are computed in multiple GPU's, basically you can get an output there but if you have 3 GPU's and you allocate a batch size of 30(GLOBAL BATCH SIZE) then the replicas of computation is 3 with each getting 10 batches, so at the end when mean or sum is computed in batches it shouldn't be at 10 but 30 which is your GLOBAL BATCH SIZE, so setting AUTO will produce wrong values, so when strategy is called we should be careful in declaring the aggregation reduction method.", "@yourtheron,\r\nCode works fine if we create an instance of the Class as per [dli7319's Comment](https://github.com/tensorflow/tensorflow/issues/35329#issuecomment-611205264). Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/7814fe7ceb1fa32164595328609771a2/35329.ipynb) of the working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 35328, "title": "query replaced with value", "body": "Issue #34696 #34283 #35301 ", "comments": []}, {"number": 35327, "title": "TF1.15 fails to dropout tf.Tensor", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): False\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS (using dockerfile from nvidia/cuda:10.0-cudnn7-devel)\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: CUDA 10.0, cuDNN7\r\n- GPU model and memory: GTX 1080 Ti\r\n\r\n**Describe the current behavior**\r\nWhen I dropout a `tf.Tensor` object by using `tf.layers.dropout`, it fails.\r\n\r\n**Describe the expected behavior**\r\nSuccessfully dropout input object as in tf 1.14.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ne = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\nh = tf.layers.dropout(e, noise_shape=tf.shape(e), rate=0.5, training=True)\r\n```\r\nThe code above works in tensorflow-gpu==1.14\r\n\r\n**Other info / logs**\r\n```\r\n    layers/core.py:226 call\r\n        return super(Dropout, self).call(inputs, training=training)\r\n    keras/layers/core.py:166 call\r\n        lambda: array_ops.identity(inputs))\r\n    keras/utils/tf_utils.py:59 smart_cond\r\n        pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n    framework/smart_cond.py:54 smart_cond\r\n        return true_fn()\r\n    keras/layers/core.py:160 dropped_inputs\r\n        noise_shape=self._get_noise_shape(inputs),\r\n    keras/layers/core.py:149 _get_noise_shape\r\n        for i, value in enumerate(self.noise_shape):\r\n    framework/ops.py:547 __iter__\r\n        self._disallow_iteration()\r\n    framework/ops.py:543 _disallow_iteration\r\n        self._disallow_in_graph_mode(\"iterating over `tf.Tensor`\")\r\n    framework/ops.py:523 _disallow_in_graph_mode\r\n        \" this function with @tf.function.\".format(task))\r\n\r\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n```", "comments": ["@toshohirasawa \r\n\r\nCan you enable eager execution (`tf.enable_eager_execution() )`and check. I am not seeing any issue. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/bbd2f832ba2c8e49bff8b33fb7fc8987/untitled494.ipynb). Thanks!", "It works! \r\nWhile my question is why `tf.layers.dropout` in TF 1.5 works differently from TF 1.4. Any suggestion?", "@toshohirasawa The TF 1.14 and 1.15 exhibit same behavior with `tf.layers.dropout` . Both require you to enable eager execution. Thanks!"]}, {"number": 35326, "title": "dataset_ops.py   'NoneType' object has no attribute 'device'", "body": "**System information**\r\n- Ubuntu 16.04\r\n- TensorFlow and Tensorflow_Datasets installed from conda\r\n- TensorFlow version 2.0\r\n- Python version: 3.7\r\n- GPU : None\r\n\r\n**Describe the current behavior**\r\nwhen I run the code form [segmentation](https://www.tensorflow.org/tutorials/images/segmentation?hl=en)\r\nafter plt.show()\r\nit run well in jupyter but this error occurred in *.py\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/cirno/anaconda3/envs/pytf/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 3009, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'device'\r\nException ignored in: <function _MemoryCacheDeleter.__del__ at 0x7f839a9344d0>\r\nTraceback (most recent call last):\r\n  File \"/home/cirno/anaconda3/envs/pytf/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 2944, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'device'\r\nException ignored in: <function _RandomSeedGeneratorDeleter.__del__ at 0x7f839a9347a0>\r\nTraceback (most recent call last):\r\n  File \"/home/cirno/anaconda3/envs/pytf/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 3009, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'device'\r\n```\r\n\r\n**Code to reproduce the issue**\r\n[segmentation](https://www.tensorflow.org/tutorials/images/segmentation?hl=en)\r\n\r\n**Other info **\r\nin dataset_ops.py\r\nadd `import tensorflow as tf`\r\nand\r\nline: 2944 and 3009   `with ops.device(self._device):` modify to  `with tf.device(self._device):`\r\nIt looks like it's solved", "comments": ["@kuzen \r\n\r\nI have tried in TF 2.0 and i am not seeing any issue.Please, find the log file. \r\n[log.tar.gz](https://github.com/tensorflow/tensorflow/files/3994521/log.tar.gz). Please, correct me if i miss something.Thanks!\r\n\r\n", "Sorry, i can't reproduce this.", "Hello,\r\nI am getting this same issue while following\r\n[this time series tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series?hl=en)\r\non windows in VS Code, when trying to use the `simple_lstm_model` `predict` or `fit` methods.\r\n```\r\nException ignored in: <function _MemoryCacheDeleter.__del__ at 0x000001BA6BB2B288>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\MichalVrska\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 2945, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'device'\r\nException ignored in: <function _RandomSeedGeneratorDeleter.__del__ at 0x000001BA6BB2B558>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\MichalVrska\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 3010, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'device'\r\n```\r\n", "@kuzen @ravikyram  I encountered this issue today again after a lenghty model compilation, so I checked my RAM and found that I was completely exhausting it (16GB) and I suspect that this might be the root of the issue. Can you confirm if you have enough RAM or also running out?\r\n\r\n", "Here is some test code that I could repro the same error with:\r\n```\r\nimport os\r\nimport random\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nseed = 0\r\nos.environ[\"PYTHONHASHSEED\"] = str(seed)\r\nrandom.seed(seed)\r\nnp.random.seed(seed)\r\ntf.random.set_seed(seed)\r\nlayer = keras.layers.Input(shape=(32, 32, 3))\r\nmodel = keras.models.Model(inputs=layer, outputs=layer)\r\nmodel.compile(optimizer=\"adam\", loss=\"mse\")\r\ndata = np.empty((5, 32, 32, 3))\r\ndata = tf.data.Dataset.from_tensor_slices((data, data))\r\ndata = data.shuffle(buffer_size=5).batch(2)\r\nmodel.fit(x=data, verbose=1, epochs=10)\r\nprint(\"Done.\")\r\n```\r\n\r\nThe output ends in\r\n```\r\nEpoch 9/10\r\n3/3 [==============================] - 0s 6ms/step - loss: 0.0000e+00\r\nEpoch 10/10\r\n3/3 [==============================] - 0s 6ms/step - loss: 0.0000e+00\r\nDone.\r\nException ignored in: <function _RandomSeedGeneratorDeleter.__del__ at 0x00000185EAEC2EE8>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\bersbersbers\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 3462, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'device'****\r\n```\r\n\r\nThis error appears when remote-debugging in VS Code through VS Code Remote SSH, but not when running the same file locally (neither on the client nor the server). The server has 256 GB RAM, of which 8 are in use, so I doubt RAM is an issue, @mvrska.\r\n\r\nThis is TF 2.1.0rc2 on Python 3.7.6 on a Windows 10 server.\r\n\r\nAnd this is the most minimal version of the code I could come up with:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nlayer = keras.layers.Input(shape=(1,))\r\nmodel = keras.models.Model(inputs=layer, outputs=layer)\r\nmodel.compile(optimizer=\"adam\", loss=\"mse\")\r\ndata = [[1]]\r\ndata = tf.data.Dataset.from_tensor_slices((data, data))\r\ndata = data.shuffle(buffer_size=5)\r\nmodel.fit(x=data)\r\nprint(\"Done.\")\r\n```", "Can replicate this: Windows 10, Nvidia RTX 2080S. 16GB ram, 8 used.\r\nRunning locally on miniconda3 and tensorflow-gpu.\r\nVSCode python debug", "Hi.\r\n\r\nI don't think this is:\r\n\r\n 1) something to worry about, or \r\n2) an error that can be simply fixed. \r\n\r\nThese are all labeled \"Exception ignored\" is likely because the object is being deleted during python exit. See this answer:\r\n\r\nhttps://stackoverflow.com/a/16620987/997378\r\n\r\nThis isn't interrupting a running program, right?\r\n\r\n> add import `tensorflow as tf` and line: 2944 and 3009 with ops.device(self._device): modify to with tf.device(self._device): It looks like it's solved\r\n\r\nThanks for the suggestion @kuzen. This might look like a fix but it isn't, really. This has something to do with object deletion order, I think during python exit.\r\n\r\nTensorflow files never `import tensorflow  as tf` directly, they import the file that the object they want is implemented in.  `tf.device` is implemented in `/tensorflow/python/framework/ops.py`.", "Hi,\r\nyou are absolutely correct, thanks for the detailed answer.\r\n\r\nIt's not a running program that is being interrupted, this just gets written to the output at the end of the execution.\r\n\r\nThanks again,\r\nPeter"]}, {"number": 35325, "title": "Publish presubmit scripts.", "body": "Change 1: 2052be4c79e772ca40367a9848b7fa5731a60d9a\r\nPiperOrigin-RevId: 286669248\r\nChange-Id: I687e242e69784a804e477fce909b9a091c8f43ad", "comments": ["This cannot work like this as it first needs #33982.\r\n\r\nInstead, cherry-picking onto that, as #33982 also requires these changes to work."]}, {"number": 35324, "title": "Update RELEASE.md ", "body": "", "comments": []}, {"number": 35323, "title": "Use std::vector instead of C arrays for TestParams", "body": "", "comments": []}, {"number": 35322, "title": "Fix segfault when attempting to convert string to float16.", "body": "To make sure this gets fixed, add test for converting string to any numeric type.\r\n\r\nPiperOrigin-RevId: 286650886\r\nChange-Id: I81f770ec2bbd33a863e8057ce198c679912fa8e0", "comments": []}, {"number": 35321, "title": "Super small update for CONTRIBUTING.md tensorflow/tensorflow", "body": "Changing \"1\" to \"2\" in \"TensorFlow has reached version 2\". It's so small but kind of important because \"1\" sticks out. But because this super small PR would likely consume hours CI testing on CPUs and GPUs, do you think we put this PR on hold and think of the ways to beef it up to make it worth all the compute? Cheers all and happy holidays \ud83c\udf84", "comments": ["If you can also combine it with a small typo fix I'd take it :D", "> If you can also combine it with a small typo fix I'd take it :D\r\n\r\nDone. Reviewing a PR about a paragraph that mentions reviewing a PR must be a bit like inception:\r\n\r\n![](https://media0.giphy.com/media/jXlB0pg6zbx3G/source.gif)"]}, {"number": 35320, "title": "Missing file when using the hexagon delegate, solved now with instructions.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution :16.04 :\r\n- Mobile device: Samsung Galaxy S9:\r\n- TensorFlow installed from source:\r\n- TensorFlow version: TF mainline\r\n- Python version: 3.6\r\n- Installed using virtualenv? N/A\r\n- Bazel version (if compiling from source): 1.2.1\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 1080TI\r\n\r\n\r\n\r\n**Describe the problem**\r\nMissing file when using the hexagon delegate. Solved now with the instructions.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nrun the hexagon delegate example.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Saw the other issue has no problem to run hexagon delegate:\r\nhttps://github.com/tensorflow/tensorflow/issues/35221\r\n", "@karimnosseir, Can you help to comment on this?", "Hi @edgedlai \r\nIt is part of the third_party/hexagon package which bazel automatically downloads\r\n\r\nYou can fetch it like this\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/delegates/hexagon/java/BUILD#L29\r\n\r\nI am assuming you're trying to use the C API and not using the Java AAR, correct ?", "@karimnosseir , got it, thanks, it's working now.", "You're welcome.\r\n\r\nThanks", "> Hi @edgedlai\r\n> It is part of the third_party/hexagon package which bazel automatically downloads\r\n> \r\n> You can fetch it like this\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/delegates/hexagon/java/BUILD#L29\r\n> \r\n> I am assuming you're trying to use the C API and not using the Java AAR, correct ?\r\n\r\nHey, I am not able to reach that link, can you please share?", "@harshgrovr The link now should be [1] as hexagon delegate moved outside experimental directory\r\n\r\n[1] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/java/BUILD#L29"]}, {"number": 35319, "title": "TF2.0 fails post-training uint8 quantization", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): installed TF binary 2.0 with Conda\r\n- TensorFlow version (or github SHA if from source): TF 2.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('model_mnist.hd5')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\nimages = tf.cast(X_train, tf.float32)\r\nmnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\r\ndef representative_data_gen():\r\n    for input_value in mnist_ds.take(100):\r\n        yield[input_value]\r\nconverter.representative_dataset = representative_data_gen\r\n\r\ntflite_quant_model = converter.convert()\r\nwith open('model_mnist_quant_uint8.tflite', 'wb') as f:\r\n    f.write(tflite_quant_model)\r\n\r\n\r\ninterpreter = tf.lite.Interpreter(model_path='model_mnist_quant_uint8.tflite')\r\ninterpreter.allocate_tensors()\r\n\r\nimg = X_train[0] * 255\r\nimg = img.astype('uint8')\r\nprint(interpreter.get_input_details())\r\ninterpreter.set_tensor(interpreter.get_input_details()[0]['index'], np.expand_dims(img, axis=0))\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n[{'name': 'flatten_input', 'index': 11, 'shape': array([ 1, 28, 28]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-12-5a2fa86c9de2> in <module>\r\n      5 img = img.astype('uint8')\r\n      6 print(interpreter.get_input_details())\r\n----> 7 interpreter.set_tensor(interpreter.get_input_details()[0]['index'], np.expand_dims(img, axis=0))\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\lite\\python\\interpreter.py in set_tensor(self, tensor_index, value)\r\n    344       ValueError: If the interpreter could not set the tensor.\r\n    345     \"\"\"\r\n--> 346     self._interpreter.SetTensor(tensor_index, value)\r\n    347 \r\n    348   def resize_tensor_input(self, input_index, tensor_size):\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\lite\\python\\interpreter_wrapper\\tensorflow_wrap_interpreter_wrapper.py in SetTensor(self, i, value)\r\n    134 \r\n    135     def SetTensor(self, i, value):\r\n--> 136         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_SetTensor(self, i, value)\r\n    137 \r\n    138     def GetTensor(self, i):\r\n\r\nValueError: Cannot set tensor: Got tensor of type UINT8 but expected type FLOAT32 for input 11, name: flatten_input \r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nThe conversion is successful, but the generated model is wrong. The input tensor dtype should be uint8, but is still float32.\r\n\r\nI tried the same thing with TF 1.15.0. In this case, every things works as expected. Here is the result with TF 1.15.0\r\n```\r\n[{'name': 'flatten_input', 'index': 11, 'shape': array([ 1, 28, 28], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.003921568859368563, 0)}]\r\n```\r\n\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@dojipkim Can you please provide a standalone code to reproduce the issue? Thanks!", "@jvishnuvardhan Yes. Here is the code.\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\n\r\nfrom tensorflow.keras.datasets import mnist\r\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\r\nX_train = X_train.astype('float32') / 255.\r\nX_test = X_test.astype('float32') / 255.\r\ny_train = keras.utils.to_categorical(y_train)\r\ny_test = keras.utils.to_categorical(y_test)\r\n\r\nmodel = Sequential()\r\nmodel.add(layers.Flatten(input_shape=X_train.shape[1:]))\r\nmodel.add(layers.Dense(512, activation='relu'))\r\nmodel.add(layers.Dense(10, activation='softmax'))\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['acc'])\r\nhistory = model.fit(X_train, y_train, epochs=2, batch_size=32, validation_split=0.2)\r\n\r\nmodel.save('model_mnist')\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('model_mnist')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\nimages = tf.cast(X_train, tf.float32)\r\nmnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\r\ndef representative_data_gen():\r\n    for input_value in mnist_ds.take(100):\r\n        yield[input_value]\r\nconverter.representative_dataset = representative_data_gen\r\n\r\ntflite_quant_model = converter.convert()\r\nwith open('model_mnist_quant_uint8.tflite', 'wb') as f:\r\n    f.write(tflite_quant_model)\r\n\r\ninterpreter = tf.lite.Interpreter(model_path='model_mnist_quant_uint8.tflite')\r\ninterpreter.allocate_tensors()\r\n\r\nimg = X_train[0] * 255\r\nimg = img.astype('uint8')\r\nprint(interpreter.get_input_details())\r\ninterpreter.set_tensor(interpreter.get_input_details()[0]['index'], np.expand_dims(img, axis=0))\r\n```\r\n\r\nThanks.", "@dojipkim,\r\nLooks like the issue has been resolved with TensorFlow v2.3. I was able to run the code the code without any error. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/0651dda764b54bf618dfc380c7fb3f3c/35319.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 35318, "title": "TensorFlow Lite Micro fully connected int8 test passes illegal filter offset", "body": "An external developer pointed out that the test for the quantized fully connected operation passes in a non-zero weight offset to the kernel for int8 tests:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/fully_connected_test.cc#L118-L119\r\n\r\nThe quantization specification promises that int8 kernels will always receive zero weight offsets:\r\nhttps://www.tensorflow.org/lite/performance/quantization_spec\r\n\r\nThis failing test is preventing an optimized kernel for a hardware platform from being accepted.", "comments": ["ARM with CMSIS-NN optimization seems to be also affected by this issue. At least the int8 tests fail and once the same workaround as above is added [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/cmsis-nn/fully_connected.cc), all tests pass.", "This issue has been fixed by PR #38040", "@petewarden  Can I close this issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35318\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35318\">No</a>\n"]}, {"number": 35317, "title": "Slicing tensor within a keras.utils.Sequence with multiprocessing=True hangs fit_generator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):  binary\r\n- TensorFlow version (use command below):  2.0.0\r\n- Python version:  3.6.8\r\n- CUDA/cuDNN version:  10.2 / 7.6.5.32-1+cuda10.2\r\n- GPU model and memory:  NVidia Titan RTX 24218 MiB\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I attempt to slice a tensor inside a keras.util.Sequence from model.fit_generator with multiprocessing=True, TensorFlow hangs forever without reporting any error or using any CPU or GPU cycles.  It works as expected when multiprocessing=False.\r\n\r\n**Describe the expected behavior**\r\n\r\nTensorFlow should correctly fit the model just as it does with multiprocessing=False\r\n\r\n**Code to reproduce the issue**\r\n\r\nIn order to reproduce, substitute my_jpeg for some jpeg on your computer (hopefully with dimension greater than 224px).  Note that if you set use_multiprocessing=False in the example below, then this will correctly train the model.\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n# In order to reproduce, just use whatever random JPEG you have handy here.\r\n# It should be larger than my_crop in the x and y dimension.\r\nmy_jpeg = \"/home/ben/my_jpeg.jpg\"\r\nmy_crop = 224\r\n\r\n# Generates a single crop for TensorFlow.\r\nclass DataGenerator(keras.utils.Sequence):\r\n  def __init__(\r\n      self,\r\n      image_location,\r\n      crop_size=224):\r\n    self._image_location = image_location\r\n    self._crop_size = crop_size\r\n\r\n  # Just one single batch will be returned, of just one single image.\r\n  def __len__(self):\r\n    return 1\r\n\r\n  # Generate one batch of data.\r\n  def __getitem__(self, index):\r\n    # Where the tensors will be stored.\r\n    X = []\r\n    y = [1]\r\n\r\n    # Read it.\r\n    image = tf.io.read_file(self._image_location)\r\n\r\n    # Load it.\r\n    image = tf.image.decode_jpeg(image, channels=3)\r\n\r\n    assert image.shape[2] == 3  # MUST be RGB.\r\n    height = image.shape[0]\r\n    width = image.shape[1]\r\n\r\n    # Just take a trivial crop of the image.\r\n    # This is the offending line operation which hangs forever.\r\n    image = image[0:self._crop_size, 0:self._crop_size, :]\r\n\r\n    # This line is equivalent to above, and it also hangs with multiprocessing enabled.\r\n    # image = tf.slice(image, [0, 0, 0], [self._crop_size, self._crop_size, 3])\r\n\r\n    X.append(tf.dtypes.cast(image, tf.float32))\r\n\r\n    # Tensors are not generally assignable, but we can create them from a number of existing ones.\r\n    X = tf.stack(X)\r\n    y = tf.stack(y)\r\n\r\n    # Preprocess it.\r\n    X /= 255.0  # Normalize to [0, 1] range.\r\n\r\n    return X, y\r\n\r\ngenerator = DataGenerator(my_jpeg, my_crop)\r\n\r\nmodel = tf.keras.applications.ResNet50(input_shape=(my_crop, my_crop, 3))\r\n\r\nmodel.compile(loss='mse')\r\n\r\n# use_multiprocessing=False works.\r\n# use_multiprocessing=True hangs.\r\nmodel.fit_generator(generator, use_multiprocessing=True, workers=2)\r\n```\r\n\r\n", "comments": ["I could reproduce the issue with Tf 2.0.\r\nPlease take a look at the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/92da8cad7821d73e50e8940f0592c158/untitled318.ipynb). Thanks!", "I noticed when testing this same code with TensorFlow 2.1 instead of 2.0, a helpful warning is generated:\r\n\r\nWARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\r\n\r\nThis warning is absolutely correct.  You can probably close this bug, since this is a known issue and as of TF 2.1 the user is properly warned.", "@theGOTOguy \r\n\r\nPlease,let us know is this still an issue?Please, close this thread if you feel your issue was resolved.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35317\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35317\">No</a>\n"]}, {"number": 35316, "title": "Where is the pip package for TF 1.15?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu (or similar)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version: 1.15\r\n- Python version: python3\r\n- Installed using virtualenv? pip? conda?: virtualenv and pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\nVersion 1.15 of TF does not seem to exist on pip, but the documentation [states that it does](https://www.tensorflow.org/install/pip?lang=python3#3.-install-the-tensorflow-pip-package)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\npip3 install tensorflow==1.15\r\nCollecting tensorflow==1.15\r\n  Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)\r\nNo matching distribution found for tensorflow==1.15\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nVersion 1.15 is required for einsum gradients.\r\n", "comments": ["Here is a [gist](https://gist.github.com/0x0539/abfd4071a7740dfaa53bdc1beaeb1c54) showing the full log output, with verbose mode.\r\nThere are tensorflow 1.15 pip modules available, but not for python3.\r\nNext, I'll test whether I can get this working in a python2 env.", "I verified that this works with a python2 environment.\r\nAre there plans to release a python3 pip module for TF 1.15? This would be really useful for library maintainers, as it makes it easier to support both versions of python.", "Also, it would make the docs consistent with reality, which is always a plus :) https://www.tensorflow.org/install/pip?lang=python3#3.-install-the-tensorflow-pip-package", "Please upgrade `pip` version. The pip exists already, see for example https://pypi.org/project/tensorflow/1.15.0/#files", "In particular, you missed the `pip install --upgrade pip` part of the documentation, just slightly above the link you provided", "Oh, that fixed it! Thanks!"]}, {"number": 35315, "title": "Revert \"Revert \"Set --incompatible_remove_legacy_whole_archive to False\"\"", "body": "Reverts tensorflow/tensorflow#35313\r\n\r\nTesting triggered, we can revert", "comments": []}, {"number": 35314, "title": "Lack of dataset length or cardinality causes `BaseCollectiveExecutor::StartAbort Out of range` issues", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I borrowed someone's code for this particular issue.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source install through Conda.\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: python 3.7.\r\n- Bazel version (if compiling from source): NA.\r\n- GCC/Compiler version (if compiling from source): Cuda 10.1\r\n- CUDA/cuDNN version: 7.4\r\n- GPU model and memory: Nvidia GTX 1080\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nThere is an existing error https://github.com/tensorflow/tensorflow/issues/32817 the references \r\nan issue that I had raised before. The problem is that when the batch size does not divide evenly into the total number of examples, you can an error `BaseCollectiveExecutor::StartAbort Out of range.` This is actually a pretty confusing and un-intuitive error message. The code to reproduce this error is below. \r\n\r\nThe problem is that this error message relates to a closed issue that I had raised before (https://github.com/tensorflow/tensorflow/issues/26966). There seems to be no good way to compute the cardinality or number of examples in a dataset. There is a function `tf.data.experimental.cardinality` that will find the cardinality of a dataset if say the dataset comes from a pandas dataframe or something, but generally not from some large CSV file or text file. \r\n\r\nAlso, say I actually calculate the cardinality of the dataset by manually counting over the batches of examples. The problem is that there is no where to record this information in the Dataset API itself. So I have to either manually hardcode this in a file, or recompute the length each time I run a training loop, which takes a lot of time. \r\n\r\nIs there a better strategy for this? Like is there a way to encode some metadata into a tensorflow Dataset API? Or perhaps a better and more recommended way is to create a Tensorflow Dataset in the style of the https://www.tensorflow.org/datasets , i.e., `tfds`. \r\n\r\n**Describe the expected behavior**\r\n\r\nI should not get a warning message for the `BaseCollectiveExecutor::StartAbort Out of range` message because the total number of examples in the dataset does not divide evenly into the batch size. Alternatively, I should be able to compute the total dataset size using the `tf.data.experimental.cardinality` function for csv files. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nCode taken from: https://github.com/tensorflow/tensorflow/issues/32817#issuecomment-539200561\r\n\r\ndata = tf.random.normal((60000,30,4))\r\nground_truth = tf.ones((60000,1))\r\ndataset = tf.data.Dataset.from_tensor_slices((data, ground_truth)).batch(64)\r\n\r\n#predefined model here: input: [?, 30,4] output: [?,1]\r\nmodel.fit(dataset, epochs=5)\r\n\r\n'''\r\n    938/Unknown - 16s 17ms/step - loss: 0.02172019-10-07 14:49:49.928619: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Shape/_2]]\r\n2019-10-07 14:49:49.928619: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n938/938 [==============================] - 16s 17ms/step - loss: 0.0217\r\nEpoch 2/5\r\n935/938 [============================>.] - ETA: 0s - loss: 2.2229e-062019-10-07 14:49:59.722216: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n2019-10-07 14:49:59.722218: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[Shape/_2]]\r\n'''\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@dubey @omalleyt12 why is the collective executor involved here?\r\n\r\n@rxsang @tomerk what is the recommendation for Keras users? My expectation would be that Keras can handle this scenario (`fit` with dataset whose cardinality is not divisible by batch size) correctly.", "@qqfish \r\n\r\nHi Krishna,\r\n\r\nIt seems you are not using any DistributionStrategy, correct? Are you able to continue training even after you get the OutOfRange error log?\r\n\r\nMy suspicion is because currently the OutOfRange error is raised from C++, then propagated and caught in python. So C++ side logs are always printed regardless whether user catches the exception in python, which causes the confusion.", "The collective executor warning comes from:\r\nhttps://github.com/tensorflow/tensorflow/blob/bb45024ae9d3df0127d1c1056b08f25e60ba601c/tensorflow/core/common_runtime/base_collective_executor.cc#L217\r\nwhich is called from:\r\nhttps://github.com/tensorflow/tensorflow/blob/bb45024ae9d3df0127d1c1056b08f25e60ba601c/tensorflow/core/common_runtime/executor.cc#L2289\r\n\r\nWe can lower the logging level if it causes confusion for users.", "@rxsang Sorry I did not see your message until today. I understand what you mean that the C++ warnings are propagated upward. I agree that this is a source of confusion because I could not really tell if training was happening or if some batches were being aborted or what. \r\n\r\nSo just to clarify, should I not worry about this warning, or is there some better way to handle it? Should I adjust my own settings to lower the logging level, or is that something that has to be done in the interface between python and C++? Seems like https://github.com/tensorflow/tensorflow/commit/4ab663e27c230a99e2e67f0609932d6151c54a32 should have fixed this, so would I just need to upgrade my installation, or do I have to wait for the next release. \r\n\r\nThanks so much for your help. Yeah, running the counter over all the batches was certainly taking a lot of time. So if I don't have to do that, it will be great. \r\nKrishna\r\n", "Yes, you can ignore the warning.  The fix I submitted is available in nightly and should be a part of the next release.", "Thanks so much @dubey ", "> Yes, you can ignore the warning. The fix I submitted is available in nightly and should be a part of the next release.\r\n\r\nI can confirm that the warning disappeared for me when using tf-nightly."]}, {"number": 35313, "title": "Revert \"Set --incompatible_remove_legacy_whole_archive to False\"", "body": "Reverts tensorflow/tensorflow#34764\r\n\r\nTemporarily, for testing", "comments": []}, {"number": 35312, "title": "Revert \"Revert \"<release 2.1>-<rc1> cherry-pick request: update tflite op versions\"\"", "body": "Reverts tensorflow/tensorflow#35284", "comments": []}, {"number": 35311, "title": "Tensorflow build from source - choose patch version", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: using conda\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 7.4\r\n- CUDA/cuDNN version: CUDA 10.0, CUDNN 7\r\n- GPU model and memory: GeForce RTX 2080 8 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI'm building tensorflow 1.14 from source because I'm working with an old cpu.\r\n\r\nDue to compatibility issues I *MUST* use version 1.14.0, however I always end up with the pip package for version 1.14.1. I read in the documentation that it was related to the bazel version installed so I downgraded to 0.24.1 (as pointed in the docs) but still, i get version 1.14.1", "comments": ["@FrancescoTerrosi, Please provide the error log and exact sequence of commands/steps that you executed before running into the problem. Thanks!", "You need to do `git checkout v1.14.0` if you want to build the code at version 1.14.0. Checking out `r1.14` gets you the latest code on the branch, which was supposed to be for the release of 1.14.1.\r\n\r\nAs this is not really a TF issue but more a git usage one, I'm going to go ahead and close the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35311\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35311\">No</a>\n"]}, {"number": 35310, "title": "Execution hangs after particular step in CUDA 10.1 TF 1.14.0", "body": "### System information\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: 18.04.3 LTS Ubuntu\r\n- TensorFlow installed : pre installed container\r\n- TensorFlow version : 1.14.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): - \r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Tesla V100-SXM3-32GB\r\n- Exact command to reproduce: Running the command python train_blstm.py\r\n\r\n### Describe the problem\r\nI am working on an Bi-LSTM + CTC Loss + WordBeamSearch Architecture for online handwriting recognition. The problem is that the code hangs/stalls after a particular point everytime. I have also let it run for 2 days now but it's still the same. Any help in this regard would be appreciated.\r\n\r\n### Source code / logs\r\nHere is the traceback of the issue attached below:\r\n```\r\n\r\n2019-12-20 14:18:51.291556: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\nW1220 14:18:54.480478 139982147983168 deprecation_wrapper.py:119] From train_blstm_dec_19.py:295: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\r\n\r\nW1220 14:18:54.481328 139982147983168 deprecation_wrapper.py:119] From train_blstm_dec_19.py:155: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\r\n\r\ndata_dir: ./\r\ncheckpoints_dir: ../checkpoints/\r\nlog_dir1: ../train_log/\r\nrestore_path: None\r\nbatch_size: 128\r\ntotal_epoches: 300\r\nhidden_size: 128\r\nnum_layers: 2\r\ninput_dims: 10\r\nnum_classes: 80\r\nsave_freq: 5\r\nlearning_rate: 0.001\r\ndecay_rate: 0.99\r\nmomentum: 0.9\r\nmax_length: 1940.0\r\nlabel_pad: 63\r\nif_valid_vr: False\r\nW1220 14:18:59.086257 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:30: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n\r\nW1220 14:18:59.098966 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:31: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nW1220 14:18:59.106423 139982147983168 deprecation.py:323] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:36: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nW1220 14:18:59.112254 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:113: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n\r\nW1220 14:18:59.112922 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:116: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\r\n\r\nW1220 14:18:59.113215 139982147983168 deprecation.py:323] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:116: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\r\nW1220 14:18:59.115090 139982147983168 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/rnn/python/ops/rnn.py:239: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\r\nW1220 14:18:59.115478 139982147983168 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `keras.layers.RNN(cell)`, which is equivalent to this API\r\nW1220 14:18:59.192415 139982147983168 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW1220 14:18:59.200585 139982147983168 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nstack_bidirectional_dynamic_rnn: Tensor(\"blstm/Reshape:0\", shape=(?, 1940, 2, 128), dtype=float32)\r\nW1220 14:19:00.570831 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:122: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\r\n\r\nTensor(\"blstm/unstack:0\", shape=(?, 2, 128), dtype=float32)\r\nTensor(\"blstm/add:0\", shape=(?, 128), dtype=float32)\r\nTensor(\"blstm/stack:0\", shape=(1940, ?, 80), dtype=float32)\r\nTensor(\"blstm/stack:0\", shape=(1940, ?, 80), dtype=float32)\r\nW1220 14:19:08.754076 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:87: The name tf.nn.ctc_loss is deprecated. Please use tf.compat.v1.nn.ctc_loss instead.\r\n\r\nW1220 14:19:08.757078 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:89: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\r\n\r\nTensor(\"ctc_loss/Mean:0\", shape=(), dtype=float32)\r\nW1220 14:19:08.758585 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:93: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\r\n\r\nW1220 14:20:39.822741 139982147983168 deprecation.py:506] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:101: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ndim is deprecated, use axis instead\r\nW1220 14:20:39.824999 139982147983168 deprecation_wrapper.py:119] From /workspace/data/src_hit_word_beam_edition/model_blstm_dec_19.py:41: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\r\n\r\nW1220 14:20:39.827233 139982147983168 deprecation_wrapper.py:119] From train_blstm_dec_19.py:203: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\r\n\r\n2019-12-20 14:20:39.892933: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-12-20 14:20:40.039163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla V100-SXM3-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.597\r\npciBusID: 0000:be:00.0\r\n2019-12-20 14:20:40.039210: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-20 14:20:40.041126: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n2019-12-20 14:20:40.042838: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10\r\n2019-12-20 14:20:40.043132: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10\r\n2019-12-20 14:20:40.044996: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10\r\n2019-12-20 14:20:40.046033: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10\r\n2019-12-20 14:20:40.049918: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-20 14:20:40.056223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-12-20 14:20:40.096725: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2700000000 Hz\r\n2019-12-20 14:20:40.146344: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xf2c8a20 executing computations on platform Host. Devices:\r\n2019-12-20 14:20:40.146400: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-12-20 14:20:40.580239: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xcd331c0 executing computations on platform CUDA. Devices:\r\n2019-12-20 14:20:40.580287: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-SXM3-32GB, Compute Capability 7.0\r\n2019-12-20 14:20:40.583875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla V100-SXM3-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.597\r\npciBusID: 0000:be:00.0\r\n2019-12-20 14:20:40.583911: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-20 14:20:40.583946: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n2019-12-20 14:20:40.583968: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10\r\n2019-12-20 14:20:40.583978: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10\r\n2019-12-20 14:20:40.583987: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10\r\n2019-12-20 14:20:40.583997: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10\r\n2019-12-20 14:20:40.584007: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-20 14:20:40.591236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-12-20 14:20:40.591303: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-20 14:20:41.297763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-20 14:20:41.297816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-12-20 14:20:41.297824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-12-20 14:20:41.305712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30466 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM3-32GB, pci bus id: 0000:be:00.0, compute capability: 7.0)\r\n2019-12-20 14:27:51.088076: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n\r\n```", "comments": ["@sakshambanga, Could you provide the standalone code to reproduce the reported issue. Thanks!", "@sakshambanga, Any update on code snippet!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 35309, "title": "tf.math.sigmoid precision issues on GPU", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: `2.1.0.dev20191219`\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.3\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n**Describe the current behavior**\r\n\r\nWe compared TensorFlow versions `2.1.0.dev20191203` and `2.1.0.dev20191219` and found some precision differences when using `tf.math.sigmoid`. Is that expected and what is the related commit?\r\n\r\nSome results are improved (see last section) but we also find some inconsistent values on GPU when the tensor size is changing.\r\n\r\n**Describe the expected behavior**\r\n\r\nSigmoid results should not depend on the tensor size.\r\n\r\n**Code to reproduce the issue**\r\n\r\nOn GPU, going from 3 to 4 elements changes the result:\r\n\r\n```python\r\n>>> tf.sigmoid([34.0, 0.0, 0.0])\r\n<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1. , 0.5, 0.5], dtype=float32)>\r\n>>> tf.sigmoid([34.0, 0.0, 0.0, 0.0])\r\n<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.99999994, 0.5       , 0.5       , 0.5       ], dtype=float32)\r\n```\r\n\r\nThis is especially problematic when taking the log of the sigmoid output. For reference, this is not an issue on CPU:\r\n\r\n```python\r\n>>> tf.sigmoid([34.0, 0.0, 0.0])\r\n<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1. , 0.5, 0.5], dtype=float32)>\r\n>>> tf.sigmoid([34.0, 0.0, 0.0, 0.0])\r\n<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1. , 0.5, 0.5, 0.5], dtype=float32)>\r\n```\r\n\r\n**Other info / logs**\r\n\r\nHere is an example of improved precision:\r\n\r\nIn `2.1.0.dev20191203`:\r\n\r\n```python\r\n>>> tf.sigmoid(-20.0)\r\n<tf.Tensor: shape=(), dtype=float32, numpy=0.>\r\n```\r\n\r\nIn `2.1.0.dev20191219`:\r\n\r\n```python\r\n>>> tf.sigmoid(-20.0)\r\n<tf.Tensor: shape=(), dtype=float32, numpy=2.0611537e-09>\r\n```", "comments": ["A related issue might be https://github.com/tensorflow/tensorflow/issues/33878, thus related to eigen\r\nThere are several commits from Rasmus Larsen in the eigen repository in December, so I guess this is likely to explain the difference between `1203` and `1219` in tensorflow.", "I could no longer reproduce the issue with 2.1 and 2.2.0.dev20200212.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35309\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35309\">No</a>\n"]}, {"number": 35308, "title": "Why does the cpu bias_op support only up to five dims?", "body": "I'm currently using tf.1.14 on Windows 10, but it doesn't seem to matter too much.\r\n\r\nI have a high-dimensional tensor object which I try to process with a Keras Dense layer. On the GPU everything works perfectly fine, but if I want to run the network on the CPU I get the following error \"Only ranks up to 5 supported:.... \".  A quick look into the C code shows that the dimensionality handling is, indeed, hardcoded:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/bias_op.cc#L102\r\n\r\nMy question is only why? It seems like it might have something to do with the way `GetBiasValueDims()` is written, but not necessarily.", "comments": ["This was done originally to reduce the number of instantiated templates, however this could be always computed with tensors:\r\n\r\n(1) Rank 2 for NHWC -> fuse all dimensions before last\r\n(2) Rank 3 for NCHW -> fuse all dimensions after dim 1\r\n\r\nI'm not sure when I'll have cycles to add this fix myself, but it should be rather trivial. Would gladly review external contribution.\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35308\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35308\">No</a>\n"]}, {"number": 35307, "title": "I got nan value when I to predict in cpu,this model that I trained it in gpu", "body": "when using a trained pix2pix model to predict in cpu ,but I got a nan value .if I use the model that trained in CPU\uff0cits work right. \r\n**this is trained by cup\uff1a**\r\n`[[[0.4903388  0.50368387 0.49226668]\r\n  [0.50147295 0.5219532  0.5668285 ]\r\n  [0.5083101  0.56787324 0.45141172]\r\n  ...\r\n  [0.56986064 0.52189714 0.5279328 ]\r\n  [0.5258558  0.5492905  0.435853  ]\r\n  [0.48710588 0.4718867  0.4559199 ]]\r\n\r\n [[0.4601045  0.45254436 0.45107806]\r\n  [0.50491637 0.48829857 0.5550411 ]\r\n  [0.5183163  0.5770218  0.5410818 ]\r\n  ...\r\n  [0.42861402 0.48687014 0.50826234]\r\n  [0.50582343 0.47616833 0.49303797]\r\n  [0.45225543 0.47339994 0.4373474 ]]\r\n\r\n [[0.42919013 0.4317314  0.4555189 ]\r\n  [0.49468514 0.5151671  0.4483238 ]\r\n  [0.50465244 0.48404706 0.4806079 ]\r\n  ...\r\n  [0.4833022  0.54902816 0.40051547]\r\n  [0.4632537  0.45243093 0.49023414]\r\n  [0.48761165 0.45556885 0.46273252]]\r\n\r\n ...\r\n\r\n [[0.43931952 0.4372336  0.4398279 ]\r\n  [0.37421006 0.44697812 0.45707896]\r\n  [0.43218714 0.42247662 0.4385087 ]\r\n  ...\r\n  [0.47185218 0.5173737  0.45031384]\r\n  [0.48594344 0.43559843 0.4615797 ]\r\n  [0.45971525 0.44595918 0.48696217]]\r\n\r\n [[0.45223635 0.41471708 0.40041572]\r\n  [0.46029133 0.4413036  0.41087914]\r\n  [0.44727382 0.42518058 0.3782655 ]\r\n  ...\r\n  [0.44791928 0.49392545 0.5107697 ]\r\n  [0.46841094 0.449826   0.4900933 ]\r\n  [0.44273183 0.4620495  0.4870268 ]]\r\n\r\n [[0.47757632 0.4741067  0.39477357]\r\n  [0.39080092 0.4166201  0.4097267 ]\r\n  [0.43215263 0.43118754 0.40830547]\r\n  ...\r\n  [0.45276994 0.4164047  0.45256376]\r\n  [0.3767779  0.43754327 0.44750926]\r\n  [0.50948536 0.4650667  0.49479547]]`\r\n\r\nthis is trained by gpu:\r\n`[[[nan nan nan]\r\n  [nan nan nan]\r\n  [nan nan nan]\r\n  ...\r\n  [nan nan nan]\r\n  [nan nan nan]\r\n  [nan nan nan]]\r\n\r\n [[nan nan nan]\r\n  [nan nan nan]\r\n  [nan nan nan]\r\n  ...\r\n  [nan nan nan]\r\n  [nan nan nan]\r\n  [nan nan nan]]\r\n\r\n [[nan nan nan]\r\n  [nan nan nan]\r\n  [nan nan nan]\r\n  ...\r\n  [nan nan nan]\r\n  [nan nan nan]\r\n  [nan nan nan]]\r\n\r\n ...\r\n\r\n [[nan nan nan]\r\n  [nan nan nan]\r\n  [nan nan nan]\r\n  ...\r\n  [nan nan nan]\r\n  [nan nan nan]\r\n  [nan nan nan]]\r\n\r\n [[nan nan nan]\r\n  [nan nan nan]\r\n  [nan nan nan]\r\n  ...\r\n  [nan nan nan]\r\n  [nan nan nan]\r\n  [nan nan nan]]\r\n\r\n [[nan nan nan]\r\n  [nan nan nan]\r\n  [nan nan nan]\r\n  ...\r\n  [nan nan nan]\r\n  [nan nan nan]\r\n  [nan nan nan]]]`", "comments": ["Whether the model trained by GPU must load prediction in GPU environment\uff1f", "ps:The model I saved and used is in .h5 format", "When you save a model, it records the device on which it has been trained. The model is restored on this device by default. In your case, I suppose your net has been trained on GPU.\r\n\r\nTo remove this information you need to specify clear_devices=True when restoring the meta graph. You can then place the net where it suits you", "@JonLeeCSDN did the above comment help you in resolving the issue. Thanks!", "Closing this issue as it has been inactive for more than 2 weeks. Please add additional comments and we can open this issue again. Thanks!", "@gowthamkpr `clear_devices=True ` is not in tf2."]}, {"number": 35306, "title": "A possible bug in ConvRNN2D __call__", "body": "Referring to\r\n[ConvRNN2D.__call__](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional_recurrent.py#L294-L341)\r\n\r\nL308 ` kwargs['initial_state'] = initial_state` and L317 `kwargs['constants'] = constants` should be added in the else block at L340. The current situation contradicts with the use of `full_input` at L337.\r\n\r\nI am not sure if we can simply replicate the code from its parent [RNN.__call__](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent.py#L640-L700).\r\n\r\nI went ahead and tried it, but after loading the saved model weights in a complete new python session the results on validation data doesn't match at all.\r\n\r\nI would appreciate a quick fix for local edit at least.\r\n\r\nThanks\r\n\r\n", "comments": ["Can you please  help us with simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Which TensorFlow version you are using?.Thanks!", "Please refer the linked issue. That support is never resolved, people tried their own ways.\r\nSo while I was skimming through the code I noticed the bug and suggested the line numbers.\r\n\r\nIt seems that feature is never tested. After fixing the code it works but has another problem of non reproducible results when re loading the weights (in a new run). I think somehow it behaves like **stateful** model, which is not.\r\n\r\nI'm using tf 2.0, but the code in master branch is same.\r\n\r\nSince you asked, an example model:\r\n`\r\nfrom tensorflow.keras import layers\r\n import tensorflow.keras as keras\r\nencoder_inputs = layers.Input((None,32,32,3))\r\n encoder = layers.ConvLSTM2D(filters=32, kernel_size=(3, 3), padding=\"same\", return_sequences=False, return_state=True)\r\n encoder_outputs, state_h, state_c = encoder(encoder_inputs)\r\n encoder_states = [state_h, state_c]\r\n\r\n decoder_inputs = layers.Input((None,32,32,4))\r\n decoder_lstm = layers.ConvLSTM2D(filters=32, kernel_size=(3, 3), padding=\"same\", return_sequences=False, return_state=True)\r\n decoder_outputs, _, _ = decoder_lstm([decoder_inputs,state_h, state_c])\r\n output = layers.Conv2D(1, (3,3), padding=\"same\", activation=\"relu\")(decoder_outputs)\r\n model = keras.Model([encoder_inputs, decoder_inputs], output)\r\n`", "@gmrhub \r\nI have tried on colab with TF version 2.0,2.1.0-rc1 and i am seeing `AssertionError: `.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/47bd873e15b693985657da79121bc541/untitled498.ipynb).Is this the expected behavior?\r\n", "Yes, that error is due to the lines I mentioned above.\n\nOn Tue, 24 Dec 2019 at 12:19 PM, ravikyram <notifications@github.com> wrote:\n\n> @gmrhub <https://github.com/gmrhub>\n> I have tried on colab with TF version 2.0,2.1.0-rc1 and i am seeing AssertionError:\n> .Please, find the gist here\n> <https://colab.sandbox.google.com/gist/ravikyram/47bd873e15b693985657da79121bc541/untitled498.ipynb>.Is\n> this the expected behavior?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35306?email_source=notifications&email_token=ACSHLKBXQLZHJZ7ZEJWO5DLQ2GWIPA5CNFSM4J54YP5KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHSUCTI#issuecomment-568672589>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACSHLKDD5JLODB467XTV62LQ2GWIPANCNFSM4J54YP5A>\n> .\n>\n", "Related stackoverflow with a workaround that allowed me to train successfully (tf 1.14): [https://stackoverflow.com/questions/50253138/convlstm2d-initial-state-assertion-error](https://stackoverflow.com/questions/50253138/convlstm2d-initial-state-assertion-error)\r\n\r\nIn line 331 in ` ConvRNN2D.__call__()` : change\r\n`    if K.is_keras_tensor(additional_inputs[0]):\r\n`\r\nto \r\n`  if False and K.is_keras_tensor(additional_inputs[0]):`\r\npreventing the if statement from executing.", "Sorry for the very late reply. I will take a look and fix the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35306\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35306\">No</a>\n", "tf.__version__=1.15.0\r\ntf.keras.__version__ = 2.2.4-tf\r\n\r\nI encounter a similar problem when using tf.keras.layers.convLSTM2D with initial_state being input. The code raises the problem is\r\n```\r\nde_convlstm = ConvLSTM2D(filters=64, kernel_size=(5, 5), name='de_convlstm',\r\n                             return_state=True, padding='same', return_sequences=True)\r\ndecoder_convlstm, h, c = de_convlstm(inputs=[decoder_conv2d_2, h, c])\r\n```\r\nFollowing a possible solution in https://github.com/keras-team/keras/issues/9761#issuecomment-381058540, I remove \r\n```\r\ninputs, initial_state, constants = self._standardize_args(\r\n            inputs, initial_state, constants)\r\n```\r\nfrom tensorflow_core/python/keras/layers/convolutional_recurrent.py. However, the following ValueError is reported\r\n\r\n_ValueError: Variable <tf.Variable 'en3_trans/kernel:0' shape=(1, 1, 5, 8) dtype=float32> has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval_\r\n\r\nBTW, I am really sure that my code runs well without initial_state being assigned, such as  \r\n```\r\nde_convlstm = ConvLSTM2D(filters=64, kernel_size=(5, 5), name='de_convlstm',\r\n                             return_state=True, padding='same', return_sequences=True)\r\ndecoder_convlstm, h, c = de_convlstm(inputs=decoder_conv2d_2)\r\n```\r\n\r\n"]}, {"number": 35305, "title": "runnin magic_wand on efr/efm32 microcontroller", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 ( Windows SL)\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): target : efr32/efm32 (tried to compile with mbed) \r\n\r\n**Describe the problem**\r\nI'm trying to run one of the provided projects on efr/efm32 microcontroller, (i choose magic_wanda example because i have an accelero & it seem's good example to start with). \r\nI wanted just to generate a binary file first, then make changes to adapt code with external accelerometer, itried this command:  _make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS efm32pg_stk3401\" magic_wand_bin_ but errors appear.\r\nShould i do some big modifications on project to run it on efr32/efm32 micro ?\r\nThank's for your answers\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n _make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS efm32pg_stk3401\" magic_wand_bin\r\n\r\n", "comments": ["@KarimelQ,\r\nCould you please provide the error log and also the TensorFlow version you are using. Thanks!", "@amahendrakar This is the error log: \r\nfatal error: mbed.h: No such file or directory\r\n #include <mbed.h>\r\n          ^~~~~~~~\r\ncompilation terminated.\r\ntensorflow/lite/micro/tools/make/Makefile:258: recipe for target 'tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/obj/tensorflow/lite/micro/mbed/debug_log.o' failed\r\nmake: *** [tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/obj/tensorflow/lite/micro/mbed/debug_log.o] Error 1\r\nthere is no error related to mbed with other projects.\r\ni used different versions of tf, i'm using the last version actually.\r\n\r\nThanks ", "Hi @KarimelQ !\r\nWe are checking to see whether you still need help in this issue . Have you checked [documentation](https://www.tensorflow.org/lite/microcontrollers) from latest versions yet? Please post this query in [TF forum](https://discuss.tensorflow.org/)/Stackoverflow for further assistance as there will be larger community to address this issue.. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35305\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35305\">No</a>\n"]}, {"number": 35304, "title": "How to use tflite c++ api to load a mat input and decode the output?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Demo code**\r\n\r\n```\r\nstd::unique_ptr<tflite::FlatBufferModel> model =\r\n\t\t\ttflite::FlatBufferModel::BuildFromFile(\"detect.tflite\");\r\n// Build the interpreter\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\ntflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n\r\n// Resize input tensors, if desired.\r\ninterpreter->AllocateTensors();\r\nTfLiteTensor* output_locations = nullptr;\r\nTfLiteTensor* output_classes = nullptr;\r\nTfLiteTensor* num_detections = nullptr;\r\nauto cam = cv::VideoCapture(0);\r\nwhile (true) {\r\n\tcv::Mat image;\r\n\tauto success = cam.read(image);\r\n\tif (!success) {\r\n\t\tstd::cout << \"cam fail\" << std::endl;\r\n\t\tbreak;\r\n\t}\r\n\t// resize(image, image, Size(300,300));\r\n\r\n\tuchar* input = interpreter->typed_input_tensor<uchar>(0);\r\n\r\n\t// feed input\r\n\r\n\r\n\tmemcpy(interpreter->typed_input_tensor<uchar>(0), image.data, image.total() *image.elemSize());\r\n\r\n\tinterpreter->SetNumThreads(1);\r\n\r\n\tinterpreter->Invoke();\r\n\r\n\tauto output = interpreter->typed_output_tensor<uchar>(0);\r\n\r\n        output_locations = interpreter->tensor(interpreter->outputs()[0]);\r\n\toutput_classes = interpreter->tensor(interpreter->outputs()[1]);\r\n\r\n\tconst float* detection_locations = TensorData<float>(output_locations, batch_index);\r\n\tconst float* detection_classes =  TensorData<float>(output_classes, batch_index);\r\n\tconst int num_detections = output_classes->dims->data[1];\r\n\tconst int num_classes = output_classes->dims->data[2];\r\n\t\t\t\r\n        std::cout << \"output: \"<< num_detections << std::endl;\r\n```\r\nThe tflite model is mobilenet-ssd.\r\nThe output 'num_detections' is always 10, no matter  whether there are something or not in front of the camera. \r\nSo I guess the problem is due to the wrong input.\r\nAnd the function 'Tensordata' is from [here](https://github.com/YijinLiu/tf-cpu/blob/master/benchmark/obj_detect_lite.cc) \r\n\r\nIs there anyone know how to feed tflite model with opencv mat and decode the output?\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 35303, "title": "When Pass experimental_relax_shapes to instance methods decorated with `tf.function`, it can't work.", "body": "### System information\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Stretch\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0rc1\r\n- Python version: 3.7.1\r\n\r\n### Describe the current behavior\r\nConsider the following source:\r\n```\r\nimport tensorflow as tf\r\n@tf.function(experimental_relax_shapes=True)\r\ndef fn(x):\r\n    print(tf.shape(x))\r\n    return x * x\r\n```\r\n1:\r\n```\r\nfn(tf.constant([2]))\r\n```\r\nOutput:\r\nTensor(\"Shape:0\", shape=(1,), dtype=int32)\r\nOut[9]: <tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>\r\n\r\n2:\r\n```\r\nfn(tf.constant([2, 3]))\r\n```\r\nOutput:\r\nTensor(\"Shape:0\", shape=(1,), dtype=int32)\r\nOut[10]: <tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 9], dtype=int32)>\r\n\r\n### Describe the expected behavior\r\nThe experimental_relax_shapes should work also on instance method.", "comments": ["I am not sure what problem you are reporting; but the problem of `experimental_relax_shapes` not working on instance methods has been solved in #35021.", "Was able to reproduce the issue. Please find the [Gist](https://colab.sandbox.google.com/gist/amahendrakar/24e22fe7e91e68780650634f2cad2aa6/35303.ipynb) here. Thanks!", "Note that `tf.shape` is dynamic and always returns a `Tensor` that will contain the actual shape at runtime. What you want in this case is the static shape: `print(x.shape)`.\r\n\r\nThe logic that `experimental_relax_shapes` uses is a bit convoluted and [hard to follow](https://cs.corp.google.com/aosp-master/external/tensorflow/tensorflow/python/eager/function.py?type=cs&q=_experimental_relax_shapes&g=0&l=2131), but roughly it will only attempt to relax shapes on the third call. In general, if you enable it you should expect the function to be still called a few times times and the shapes of tensors to be anything.\r\n\r\n1:\r\n```\r\nfn(tf.constant([2]))\r\n```\r\n\r\nOutput:\r\n(1,)\r\n<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>\r\n\r\n2:\r\n```\r\nfn(tf.constant([2, 3]))\r\n```\r\nOutput:\r\n(2,)\r\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 9], dtype=int32)>\r\n\r\n3:\r\n```\r\nfn(tf.constant([2, 3, 4]))\r\n```\r\nOutput:\r\n(None,)\r\n<tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 4,  9, 16], dtype=int32)>", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35303\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35303\">No</a>\n"]}, {"number": 35302, "title": "lite/kernels/concatenation: use tflite::TfLiteRound instead of std::round", "body": "I recently started to port tensorflow-lite on RIOT, an operating system for microcontrollers, (see https://github.com/RIOT-OS/RIOT/pull/12847 to follow this work) and had a build issue with the RISC-V toolchain where `std::round` is not defined (I'm using `riscv-none-embed-gcc` version 8.2.0).\r\n\r\nThis PR replaces a call to `std::round` with the global `tflite::TfLiteRound` function that internally handles this miss in case `TF_LITE_USE_GLOBAL_ROUND` is defined.\r\n\r\n`git grep` is reporting other uses of `std::round` in the `lite` directory but they doesn't break the build on RISC-V. I'm not sure if they should be updated as well.\r\n\r\n<details><summary>git grep -n \"std::round\" tensorflow/lite/</summary>\r\n\r\n```\r\ntensorflow/lite/experimental/ruy/test.h:1414:  auto q_fixed = static_cast<std::int64_t>(std::round(q * (1ll << 31)));\r\ntensorflow/lite/kernels/activations.cc:123:    const float rescaled = std::round(transformed * inverse_scale);\r\ntensorflow/lite/kernels/activations.cc:373:                        std::round(input->params.zero_point +\r\ntensorflow/lite/kernels/activations_test.cc:361:      std::round(std::numeric_limits<QuantizedType>::min() +\r\ntensorflow/lite/kernels/activations_test.cc:364:      std::round(std::numeric_limits<QuantizedType>::min() +\r\ntensorflow/lite/kernels/internal/logsoftmax_quantized_test.cc:62:               255 + std::round(16.0f * reference_output_float_data[i])));\r\ntensorflow/lite/kernels/internal/logsoftmax_quantized_test.cc:96:                  127 + std::round(16.0f * reference_output_float_data[i])));\r\ntensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc:2163:    //    std::round(*scaling_factor * values[i]));\r\ntensorflow/lite/kernels/internal/optimized/optimized_ops.h:4025:  const int32_t prob_rnd = static_cast<int32_t>(std::round(prob_rescaled));\r\ntensorflow/lite/kernels/internal/optimized/optimized_ops.h:4184:      // Use std::rint over std::round (which is used in\r\ntensorflow/lite/kernels/internal/optimized/optimized_ops.h:6474:    const int int_exponent = static_cast<int>(std::round(exponent));\r\ntensorflow/lite/kernels/internal/quantization_util.cc:374:  // Using TfLiteRound instead of std::round and std::log instead of\r\ntensorflow/lite/kernels/internal/reference/reduce.h:371:            static_cast<U>(std::round(temp_sum[idx] * scale + bias)) +\r\ntensorflow/lite/kernels/internal/reference/reduce.h:381:            std::min(std::round(float_mean * scale + bias) + output_zero_point,\r\ntensorflow/lite/kernels/internal/reference/reference_ops.h:1096:              static_cast<int32_t>(std::round(input_ptr[j] * scale + bias)) +\r\ntensorflow/lite/kernels/internal/round.h:34:  return std::round(x);\r\ntensorflow/lite/kernels/internal/softmax_quantized_test.cc:62:        static_cast<int>(std::round(256.0f * reference_output_float_data[i])));\r\ntensorflow/lite/kernels/test_util.h:54:                        std::round(zero_point + (f / scale))))));\r\ntensorflow/lite/kernels/test_util.h:443:      nudged_zero_point = static_cast<T>(std::round(zero_point_double));\r\ntensorflow/lite/toco/graph_transformations/quantization_util.cc:152:    auto integer_val = tflite::SafeCast<DataType<A>>(std::round(scaled_val));\r\ntensorflow/lite/toco/graph_transformations/quantize.cc:308:      scaled_value - std::round(scaled_value);\r\ntensorflow/lite/toco/graph_transformations/quantize.cc:312:  const double rounded_scaled_value = std::round(scaled_value);\r\ntensorflow/lite/tools/optimize/quantization_utils.cc:80:    zero_point = static_cast<int64_t>(std::round(zero_point_from_min));\r\n```\r\n\r\n</details>", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35302) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35302) for more info**.\n\n<!-- ok -->", "Hi there.\r\nAny chance to get a review on this small PR ? Thanks!", "cc'ing @njeffrie. Maybe I'll get a review.", "Any news here ? Thanks!", "Still awaiting review here, any chance to get one ?"]}]