[{"number": 6633, "title": "CudnnRnnSequenceTensorDescriptor should support different sequence lengths", "body": "According to the cuDNN docs, the functions `cudnnRNNForwardInference` / `cudnnRNNForwardTraining` get the argument `cudnnTensorDescriptor_t* xDesc`, where:\r\n\r\nxDesc: Array of tensor descriptors. Each must have the same second dimension. The first dimension may decrease from element n to element n + 1 but may not increase.\r\n\r\nThe usage of `xDesc` is a bit non-straight-forward. I wrote about that in more detail [here](http://stackoverflow.com/questions/41461670/cudnnrnnforwardtraining-seqlength-xdesc-usage).\r\nAccording to a [comment in the CNTK code](https://github.com/Microsoft/CNTK/blob/7c5fb2d7d806148b5cbd795407f7c7b6a1a64520/Source/Math/CuDnnRNN.cpp) about the dimensions of each `xDesc[t]`:\r\n\r\n> these dimensions are what CUDNN expects: (the minibatch dimension, the data dimension, and the number 1 (because each descriptor describes one frame of data)\r\n\r\nTensorFlow sets the same minibatch dimension for each `xDesc[t]` in `CudnnRnnSequenceTensorDescriptor`:\r\n\r\n    int dims[] = {batch_size, data_size, 1};\r\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\r\n    status = dynload::cudnnSetTensorNdDescriptor(\r\n        parent, handle /*tensorDesc*/, data_type /*dataType*/,\r\n        sizeof(dims) / sizeof(dims[0]) /*nbDims*/, dims /*dimA*/,\r\n        strides /*strideA*/);\r\n    CUDNN_RETURN_IF_FAIL(status, \"Failed to update tensor descriptor\");\r\n    // Replicate handle across the number of steps.\r\n    handles_.assign(seq_length, handle);\r\n\r\nAlso `createRnnSequenceTensorDescriptor` needs a new API to allow for that.\r\n\r\nAnd I'm not sure if there are ways to prepare the input `x` for `cudnnRNNForwardTraining` so that it has all sequences contiguously behind each other, and the sequences are sorted by sequences length.\r\nSimilar as [`PackSequencesForCuDNN` in CNTK](https://github.com/Microsoft/CNTK/blob/4472649412929543d4dfe553f50be5d9b3102521/Source/ComputationNetworkLib/RNNNodes.cpp#L265).\r\n", "comments": ["Are there any updates on this? And how does it relate to this [issue](https://groups.google.com/a/tensorflow.org/forum/#!searchin/discuss/alquraishi/discuss/Q4egUFzG_3I/IZ5NMQIxAQAJ)?", "Assigning James who works on cudnn-rnn extensively. ", "@albertz \r\nI think it's actually more of a question to Nvidia than to TF.\r\n1. I'm not sure why we chose 3 dimension, for the TensorNdDescriptor -- but from Cudnn 7.0 it actually suggests at least 4 dims:\r\n> The total size of a tensor including the potential padding between dimensions is\r\nlimited to 2 Giga-elements of type datatype . Tensors are restricted to having at least\r\n4 dimensions, and at most CUDNN_DIM_MAX dimensions (defined in cudnn.h). When\r\nworking with lower dimensional data, it is recommended that the user create a 4D\r\ntensor, and set the size along unused dimensions to 1.\r\n\r\n2. We don't yet have functions to help doing that. The sorting and packing (and later unsorting/unpacking) is very expensive.", "@protoget it appears that Keras does support this however, according to this [post](https://groups.google.com/a/tensorflow.org/forum/#!searchin/discuss/alquraishi/discuss/Q4egUFzG_3I/IZ5NMQIxAQAJ)?\r\n\r\nIrrespective of the padding issue, if the input tensor changes in size (say due to bucketing so that from batch to batch the size of the input tensor is different), would the ops and layers defined in `cudnn_rnn.py` function correctly or not? I understand that they don't currently account for different sequence lengths within a batch, but are they at least able to deal with dynamically sized input tensors?", "You can use [`tf.reverse_sequence`](https://www.tensorflow.org/api_docs/python/tf/reverse_sequence) to reverse it for the backward direction. Then you can reverse it afterwards again. You have to do it separately for the forward and backward direction, and also separately for each layer. I implemented that in our framework [Returnn](https://github.com/rwth-i6/returnn). Of course, that degrades the performance. I did a benchmark [here](http://returnn.readthedocs.io/en/latest/tf_lstm_benchmark.html).", "@alquraishi\r\n\r\nKeras has support of padding with a performance hit. I don't know if it's better/worst than if implemented using CuDNN's native support (which is also expensive).\r\n\r\ninputs to cudnn_rnn classes have 3 dims: [time_steps, batch_size, input_dim]. The 3rd dim is a constant which doesn't change across batches. It's a requirement for all LSTM impl, regardless of what devices they are.", "@protoget thanks for the clarification about the 3 dims. My question was with regards to time_steps. Is it ok if that changes from batch to batch? I have it running and it's not throwing up an error, but I want to make sure the resulting semantics are correct (modulo padding issue--I understand that the padded time steps are treated as actual entries.)", "It's ok if the sequence_length varies from batch to batch.", "We'll probably support variable sequence length the same way Keras does. I don't seem to have the bandwidth in recent 2-3 weeks however. Will follow up after that.", "One question / comment about supporting the correct zero-padding semantics for variable lengths within a batch. It appears to me that if one assumes all zeros for the initial hidden and cell states, which is the default behavior (the user must explicitly pass in an initial state tensor to change this), then the fix to get the correct semantics is rather simple. That's because, looking at the math for LSTMs, if both the inputs and previous states are zero, then the output of an LSTM cell is also zero. And so as long as the initial state is zero for the backward direction, then all the padding zeros encountered before the real sequence begins will simply be ignored. The only issue is that the forward direction will carry on some residual signal even after it encounters the zero padding. Thus, the solution would be to zero out all the outputs of the LSTM that correspond to the zero padding regions, before feeding these outputs to the next layer in a multi-layer stack. This precludes using the native cuDNN functionality for multiple layers, but AFAICT there's no speed difference for _bidirectional_ LSTMs between manually constructing multiple layers, and using the native functionality in cuDNN. So this seems like a much simpler and cheaper solution than using `tf.reverse_sequence`, etc. The only limitation is that it would prevent one from having an initial state other than all zeros.", "@alquraishi \r\n\r\nThe following conclusion isn't correct:\r\n> if both the inputs and previous states are zero, then the output of an LSTM cell is also zero\r\n\r\nEach gate in CuDNN LSTM (which isn't the same as LSTM from wiki) has two bias terms -- which means with zero input and state, you get zero output/new state only if the biases are zero.\r\n\r\nAs for:\r\n>  but AFAICT there's no speed difference for bidirectional LSTMs between manually constructing multiple layers, and using the native functionality in cuDNN.\r\n\r\nI don't think so. CuDNN fused bidirectional multi-layer impl is much faster than if manually stacking concatenated outputs from each layer.", "Yes you're right, I didn't think about the bias terms. I did some numerical tests and the outputs came out identical, but that must be because the bias terms were small enough that things cancelled out. That's too bad :(\r\n\r\nRegarding speed, in the tests I've done I did not find a detectable difference between manual stacking and CuDNN fused layers. It's possible that other parts of the model are adding sufficient overhead to mask the difference, but it also makes sense that it wouldn't make a huge difference, since both directions have to be integrated before the next layer can begin to be computed, and so there's no real extra parallelism that can be exploited, and presumably whatever overhead is added due to taking outputs from one layer and feeding them into the next is negligible for reasonably large LSTMs. I'm running with ~1000 units per direction and hundreds of time steps, and the slowdown, if any, is less than 1%.\r\n\r\nGiven that my idea is wrong... any chance this'll be implemented soon or should I look into splitting directions and using `tf.reverse_sequence`?\r\n\r\nSorry about the false alarm.\r\n", "I went ahead and implemented a corrected (hopefully for real this time) version by splitting the two directions and then using `tf.reverse_sequence` on the inputs and outputs of the backward direction. This appears to work correctly with more sensitive tests that I now have that fail with the previous incorrect approach. Surprisingly, the performance penalty appears non-existent! On a fairly bare bones model with two bidirectional layers with 800 units for each direction, and a maximum sequence length of around 1,000, I'm seeing absolutely no difference in speed between the fused bidirectional cuDNN op and this split approach (in fact the latter is 5% faster, but that's likely noise). This is on a system with Titan X Pascal, running on TF 1.4, Ubuntu 14.04, CUDA 8, and cuDNN 7 (compiled from source.) The tests were done on a fully loaded system, and are an average of separate 10 runs, each of which consumed 100 batches, so the numbers should be trustworthy.\r\n\r\nUPDATE: With other model configurations, I am seeing around 20% speed penalty for using this approach. This is not because the multilayer stacking is being done in TF--I still don't see any appreciable speed loss there. The slowdown is due to splitting the two directions and then doing the reversal before and after.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think @alquraishi 's suggestion solves the problem. Please reopen if it doesn't.", "No, it does not. It describes a slow workaround which everyone is doing now because this is not correctly implemented.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Any updates on this?\r\n\r\nThe workaround works fine with a little hit in performance, but a clean solution would be much appreciated.", "Hi everybody! Just implemented variable sequence length CuDNN RNNs, which touches on this subject. PR here: https://github.com/tensorflow/tensorflow/pull/22308\r\n\r\nCheers", "I think this was fixed by #23588?", "Closing this issue, as the capability was introduced in PR #23588."]}, {"number": 6632, "title": "Add adaptive softmax implemtation in nn_impl.py", "body": "Add a implementation of AdaptiveSoftmax, see [Efficient softmax approximation for GPUs](https://arxiv.org/pdf/1609.04309v2.pdf) for detail.\r\n\r\nThe adaptive softmax is a faster way to train a softmax classifier over a huge number of classes, and can be used for both training and prediction. For example, it can be used for training a language model with a very huge vocabulary, and the trained languaed model can be used in speech recognition, text generation, and machine translation very efficiently .\r\n\r\nIt has been tested for language modeling on PTB (Penn Treebank) and GBW (Google Billion Word) corpus, and achieved a better result than the [Torch implementation](https://github.com/facebookresearch/adaptive-softmax). \r\n\r\nOn GBW corpus, we achived a perplexcity of 43.24 after 5 epochs, taking about two days to train on 2 GPUs with synchronous gradient updates. Detail experiment result and usage demo can be found here: [TencentAILab/tf-adaptive-softmax-lstm-lm](https://github.com/TencentAILab/tf-adaptive-softmax-lstm-lm).\r\n\r\nFurther more, it has been used in the ASR system developed by Tencent AI Lab, and achieved about **20x speed up** than the full sotfmax in the second pass for rescoing.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed a CLA!", "CLAs look good, thanks!\n\n<!-- ok -->", "Please provide a link to the paper from the code.", "@ebrevdo want to take a look?", "Steve, thanks for the contribution!  We'll check internally that we have\nsomeone who has cycles to maintain this function on our side.  If we do, we\ncan accept it.\n\nKeep in mind that code like this should be added to tf.contrib first, and\nnot core.  If we decide that we can accept it, you'll have to move it to\nsomewhere in contrib (perhaps tf.contrib.losses is the appropriate module)\n\nOn Mon, Jan 9, 2017 at 10:23 AM, drpngx <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> want to take a look?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6632#issuecomment-271362773>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8Nd8WGfQymMCzgugzKzNNFIZzhDks5rQns2gaJpZM4LaeuU>\n> .\n>\n", "To follow up:\n\n1. Let's get your patch submitted.\n2. Please refactor to move the functionality to tf.contrib.layers; follow\nall conventions of the code therein (python cleanliness, op and variable\nnames, etc).\n\nPing me when ready for review.\n\nOn Mon, Jan 9, 2017 at 1:41 PM, Martin Wicke <notifications@github.com>\nwrote:\n\n> *@martinwicke* commented on this pull request.\n>\n> Since this function creates variables, I believe this functionality should\n> probably go into layers.\n>\n> We're trying to keep tf.nn to pure functions, so that we can more easily\n> see which changes will lead to checkpoint incompatibilities.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6632#pullrequestreview-15802088>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyU72F0KjkqokWWysZacZfthbWoXks5rQqmkgaJpZM4LaeuU>\n> .\n>\n", "@ebrevdo thanks for your help. I have moved the adaptive_softma_loss to tf.contrib.layers, could you please review the code.", "@SteveSYYang could you address the reviewers comments, please?", "Closing due to lack of update to keep our list of active PRs small -- feel free to update this PR or send a new one once the comments have been addressed."]}, {"number": 6631, "title": "tf-master-win-bzl is failing due to dependency on //tensorflow/contrib/framework:framework_py", "body": "http://ci.tensorflow.org/job/tf-master-win-bzl/245/\r\n\r\nMany targets, including `//tensorflow/contrib/framework:framework_py`, in `//tensorflow/contrib:contrib_py` currently don't build on Windows with Bazel, it is [excluded from `//tensorflow/python:python`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/BUILD#L72)\r\n\r\ncc0de74ff0fbaee205826f7cef23e91892fb5db2 introduced the dependency on `//tensorflow/contrib/framework:framework_py` which caused the failure.\r\n```\r\n$ bazel query 'somepath(//tensorflow/tools/pip_package:build_pip_package, //tensorflow/contrib/framework:python/ops/_variable_ops.so)'\r\n//tensorflow/tools/pip_package:build_pip_package\r\n//tensorflow/examples/tutorials/mnist:package\r\n//tensorflow/examples/tutorials/mnist:input_data\r\n//tensorflow/contrib/learn/python/learn/datasets:datasets\r\n//tensorflow/contrib/framework:framework_py\r\n//tensorflow/contrib/framework:python/ops/_variable_ops.so\r\n```\r\n//cc @jart @gunan ", "comments": ["https://github.com/tensorflow/tensorflow/pull/6621 already out for temporary fix.\r\nIt is waiting for approval."]}, {"number": 6630, "title": "[Docs] A mismatch bug in api_docs ", "body": "There is a mismatch bug in api_docs about [tf.graph](https://www.tensorflow.org/api_docs/python/framework/core_graph_data_structures#Graph). It's easy to find that **tf.Graph.__init__() {:#Graph.init}** is a error. And I check the original script [ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L2003), it seems like the `{:#Graph.init}` is added by other script. \r\nAt the same time, it causes the index `URL#Graph.init` ineffectively.", "comments": ["@mrry @drpngx Could you have a look? And re-pin the corresponding reviewer?", "Right, it's incorrectly parsed by the markdown. Looping internally. We have a new version of the docs coming soon so we might not fix this one.", "Yes, I think the {} anchors will go away entirely.", "@drpngx Yep, I do agree with you. It's not a separate problem, but a more general issue of markdown parser. Look forward to the new version of G3docs."]}, {"number": 6629, "title": "Feature request: configuration files for tensorflow", "body": "Matplotlib and theano provide [`matplotlibrc`](http://matplotlib.org/users/customizing.html) and [`theanorc`](http://deeplearning.net/software/theano/library/config.html) files to configure the libraries, respectively. It would be great if we could add such functionality to tensorflow. \r\n\r\nIn particular, it would be great to \r\n\r\n* expose a `floatX` variable such that it is straightforward to switch between 16, 32, and 64 bit representations without having to modify code\r\n* expose the [`config.proto`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto)\r\n\r\nA JSON representation of the configuration may be more suitable than the formats used by matplotlib and theano because it can more easily capture the hierarchical structure of the configuration.", "comments": ["Can you give the use-case you need this for? In this specific case, the `floatX` option doesn't seem to be very useful -- `float32` support is usually there, but `float16/float64` is spotty. For non-trivial calculation, switching away from `float32` type will usually give \"kernel not registered\" error. Also .theanorc has \"device\" setting, but that doesn't seem very applicable to TensorFlow. It was designed to use multiple devices, and the most common case is two devices -- cpu+(1 or more GPUs), the latter being configurable with CUDA_VISIBLE_DEVICES.\r\n\r\nWhen using distributed TensorFlow, you could have more than one session and use different configs for local vs. grpc session worker vs grpc session ps. The raises the question of global default -- which session they would apply to", "I often have the need to switch between 32 and 64 bits because 32 bits do not always suffice for the models I am interested in. If possible, I use 32 bits because I can train on the GPU. If that doesn't work I'd like to be able to switch to 64 bit. But maybe I can just define a `floatX` in my environment conditional on env variables.\r\n\r\nRegarding the `config.proto`: we often want to use different default arguments than the default arguments defined by tensorflow. It would be nice to keep these defaults in a central location which each individual developer using our repository can customise to their needs by adding a `.tensorflowrc` to their `$HOME`, for example.", "Regarding \"float\" type. Right now \"float32\" default is hardcoded in almost every op (github [search](https://github.com/tensorflow/tensorflow/search?p=3&q=%22dtype%3Dtf.float32%22&type=Code&utf8=%E2%9C%93).\r\n\r\nOne could potentially factor it out. But right now so few ops support non-float32 type, I wonder if other types make sense as a default. For instance, right now convolution doesn't support float64.\r\n\r\nRegarding having global session settings/run options, I think more details are needed of how the semantics of it would work. ", "Unfortunately, I think this is a huge support nightmare, since most people forget what settings they have in their home rc files.  I think it's unlikely we'll want to do this any time soon, though you could build a library to do this on your own (read your own .tensorflowrc file to populate the right fields in a ConfigProto). ", "Good idea. Maybe we'll just do that. Would `tf.contrib.rc` be a sensible place to put this if we end up sharing it?", "For now, I think it would be best to have it in an external repo which you can advertise here.\r\n\r\nIf you're willing to write code to read a file from $HOME to populate a ConfigProto, you could also just write a default ConfigProto that you use in your own code..."]}, {"number": 6628, "title": "Remove unused TF_NEED_SYCL from ./configure.", "body": "TF_NEED_SYCL seems to be an old alias for TF_NEED_OPENCL. It does not appear anywhere else in the repository. This change replaces it with TF_NEED_OPENCL, using the same structure as TF_NEED_CUDA.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@benoitsteiner @lukeiwanski \r\nI took a quick look, change looks OK.\r\nBut wanted to double check with you.", "That looks good, thanks!", "Thank you very much for the quick turnaround!"]}, {"number": 6627, "title": "TypeError: strided_slice() takes at least 4 arguments (3 given)", "body": "I have check the error.  And I find that the bug has been fixed in github code.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py\r\n\r\n\r\nlastest github version:\r\n`def strided_slice(input_,\r\n                  begin,\r\n                  end,\r\n                  strides=None,`\r\n\r\nlastest whl version:\r\n`def strided_slice(input_,\r\n                  begin,\r\n                  end,\r\n                  strides,`\r\n\r\nHowever tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl doesn't  synchronize with github.\r\n\r\nHow can I upgrade tensorflow from lastest github code?", "comments": ["Same error here with OSX..", "Please see the [instructions on building from sources](https://www.tensorflow.org/get_started/os_setup#installing_from_sources).", "thanks for the link. Though I've found building for Mac with GPU support extemely hard (didn't work for me at the end)", "try use commands as follows:\r\npip uninstall tensorflow\r\npip install tensorflow"]}, {"number": 6626, "title": "Add adaptive softmax implemtation in nn_impl.py", "body": "Add a implementation of AdaptiveSoftmax, see [Efficient softmax approximation for GPUs](https://arxiv.org/pdf/1609.04309v2.pdf) for detail.\r\n\r\nThis is a faster way to train a softmax classifier over a huge number of  classes, and can be used for both training and prediction.\r\n\r\nIt has been tested for language modeling on PTB (Penn Treebank) and GBW (Google Billion Word) corpus, and achieved a good result.\r\n\r\nSee detail experiment result: [tf-adaptive-softmax-lstm-lm](https://github.com/yangsaiyong/tf-adaptive-softmax-lstm-lm).", "comments": ["Can one of the admins verify this patch?"]}, {"number": 6625, "title": "Add AdaptiveSoftmax implemtation in nn_impl.py", "body": "Add a implementation of AdaptiveSoftmax, see [Efficient softmax approximation for GPUs](https://arxiv.org/pdf/1609.04309v2.pdf) for detail.\r\n\r\nThis is a faster way to train a softmax classifier over a huge number of  classes, and can be used for both training and prediction.\r\n\r\nIt has been tested for language modeling on PTB (Penn Treebank) and GBW (Google Billion Word) corpus, and achieved a good result.\r\n\r\nSee detail experiment result: [tf-adaptive-softmax-lstm-lm](https://github.com/stevesyang/tf-adaptive-softmax-lstm-lm).", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 6624, "title": "Fail with error (rather than hanging) when init_op requires starting queues (ManagedSesssion etc)", "body": "In 0.12 and in master\r\n\r\n`MonitoredSession` wraps the original `SessionCreator` in a `_CoordinatedSessionCreator` in an attempt to ensure queue runners are started before the session is run, however the original session has to be created first (as it is an argument to start_queue_runner). In the case of `ChiefSessionCreator`, `ChiefSessionCreator.create_session` calls `ChiefSessionCreator._get_session_manager`\r\nwhich instantiates a new `SessionManager` and returns it. Then `ChiefSessionCreator.create_session` calls `SessionManager.prepare_session` which calls `sess.run(init_op)`. Thus resulting in `sess.run(init_op)` being run before `start_queue_runner` is called. \r\n\r\nThis makes it impossible to initialize variables from queues. Even if this is intended behavior, it causes the initialization of `MonitoredSession` to stall without any logging output, and not respond to SIGTERM.\r\n\r\nIt seems that `prepare_session` may need to be split into two calls for this use case. ", "comments": ["Unless I'm mistaken, it's quite important for initialization to happen before starting the queue runners, because [`input_producer()`](https://github.com/tensorflow/tensorflow/blob/7c36309c37b04843030664cdc64aca2bb7d6ecaa/tensorflow/python/training/input.py#L101) calls [`limit_epochs()`](https://github.com/tensorflow/tensorflow/blob/7c36309c37b04843030664cdc64aca2bb7d6ecaa/tensorflow/python/training/input.py#L69), which sometimes creates a variable... that needs to be initialized before starting the queue runner (for the input producer).\r\n\r\nWhat's the use case for initializing a variable from a queue? It sounds quite exotic, so perhaps it would be possible to achieve with a workaround in the user program.", "Ahh shoot. You're right. I was just testing my code with constants upstream from the enqueue_ops (and initializing from a queue worked fine, because I could start the queue before running the init_op) but if I put variables upstream from the enqueue ops it throws a failed precondition. \r\n\r\nGoing to rescope this bug to failing more noisily when unstarted queues are inputs to ops. FYI my exotic use case was just a regular old rolling window pattern: https://gist.github.com/elibixby/cedd3be67020169ff3efc47dbb603af6 \r\n\r\nI'll try to find another way to do it.", "I had the same issue when I wanted to use data dependent initialization in my neural net. I general, data dependent initialization is not an exotic thing to do, is it? Is there a better workaround than using assign ops after initialization is completed?", "@elibixby What's the status of this issue?", "Not sure there is any fix in the works. But I believe there may be a workaround, where you can create a separate GraphKey for variables that are downstream of Queues, and run initialization of those variables after queue initialization. Have to test it but believe it should work.", "Actually, did https://github.com/tensorflow/tensorflow/commit/b25d1c7d3e9f30925aa132ba62e79d281191a3dc fix this?  It makes `Variable.initialized_value` not reinitialize already initialized variables, which should make data dependent initialization quite a bit easier.", "It's possible, busy today but I can test tomorrow if no-one beats me to it.", "@girving Yep! Good catch! This use case is now possible, if you remember to start queue runners, *however*, it still hangs indefinitely if you do not start queue runners. It should be possible for evaluation of graph nodes to determine if there are upstream queue nodes, and queues have not been started, which would help immensely in debugging this common error (whether it's worth the effort given `tf.contrib.data` is coming, is another question).\r\n\r\nTo be clear with the following graph:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n   queue = tf.train.string_input_producer(['foo', 'bar', 'baz'])\r\n   var = tf.Variable(queue.dequeue_many(3))\r\n```\r\n\r\nThe following code works:\r\n\r\n```\r\nwith tf.Session(graph=graph) as sess:\r\n   tf.train.start_queue_runners(sess=sess)\r\n   tf.global_variables_initializer().run()\r\n   print(sess.run(var))\r\n```\r\n\r\nBut the easy mistake of reversing two lines:\r\n\r\n```\r\nwith tf.Session(graph=graph) as sess:\r\n   tf.global_variables_initializer().run()\r\n   tf.train.start_queue_runners(sess=sess)\r\n   print(sess.run(var))\r\n```\r\n\r\nResults in a program that hangs indefinitely\r\n\r\nEDIT: This is in TF 1.2\r\n", "@mrry Do you think it's feasible to programmatically warn about `QueueRunners` not being started?  The warning could even point people to `tf.contrib.data`. :)", "@girving I once hacked up something that added to each `tf.Tensor` the set of queue runners on which it depended, and printed a warning when you tried to evaluate a tensor that depended on a queue runner that hadn't been started. I think it foundered because people ignore warnings and want errors, but there was a concern about false positives (e.g. perhaps you could feed a value to avoid depending on the queue, or it could be control-flow sensitive, and it wasn't clear how much metadata we should track to avoid the problem).", "At the very least, I find it concerning that running of the initialization op doesn't respond to `SIGTERM` and only responds to `SIGKILL` when it's waiting for a queue dependency. What gives there?", "[This Stack Overflow answer](https://stackoverflow.com/q/14707049/3574081) goes into some of the details. Short answer: invoking native code on the main thread of a Python process can prevent the signal from being delivered.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Given that I think queues are now mostly deprecated, should we mark this wontfix? If no response after a while, we will close this.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I think this is no longer relevant in the world of tf.data, so closing. Please reopen if it's still an issue."]}, {"number": 6623, "title": "Where is the skflow examples ??", "body": "https://github.com/tensorflow/skflow/tree/master/examples\r\n\r\nIt says skflow examples has moved to https://github.com/tensorflow/skflow/tree/master/examples, but this  folder is empty.", "comments": ["Looks like they are moved under `tensorflow/examples/learn` by https://github.com/tensorflow/tensorflow/commit/f9b6d55ffd630082efd088fca927f7d991fdf3fa#diff-4b9849eed2f5253aaddd26fbf7afdef2\r\n\r\nWe might need to update the pointers from the old repo.", "Indeed, skflow is renamed to learn [tf.learn] so examples have moved.\r\nI've updated pointer and we probably should remove that repository soon."]}, {"number": 6622, "title": "Minor doc issue - Param order in nce_loss in word2vec tutorial", "body": "In section 'Building the graph' of g3doc/tutorials/word2vec/index.md:\r\n\r\n```python\r\n# Compute the NCE loss, using a sample of the negative labels each time.\r\nloss = tf.reduce_mean(\r\n  tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\r\n                 num_sampled, vocabulary_size))\r\n```\r\n\r\nWhereas the actual order of parameters is, as in python/ops/nn_impl.py, is defined as:\r\n\r\n```python\r\ndef nce_loss(weights,\r\n             biases,\r\n             labels,\r\n             inputs,\r\n             ....\r\n```\r\n\r\nSo the `embed` and the `train_labels` parameters in the code excerpt in the word2vec tutorial page should be swapped to match the definition of `tf.nn.nce_loss()`.\r\n\r\n", "comments": ["Good catch.  I have fixed this internally, but unclear when it will propagate out to the tensorflow website and to Github (we are changing some directory structures for docs so things may be weird for a little while)"]}, {"number": 6621, "title": "Break indirect dependency on contrib for windows.", "body": "This fixes the tf-master-win-bzl build breakage.", "comments": ["PTAL, updated the changes as we discussed.", "Ping!\r\nThis fix is ready to go in."]}, {"number": 6620, "title": "CudnnLSTM doesn't work with AdamOptimizer", "body": "I am testing how to use CudnnLSTM, there is not a lot of documentation on this. In my own experiment, I found when use AdamOptimizer with CudnnLSTM, it always raises the following Exception. \r\n\r\nI also found another repository using CudnnLSTM, and uploaded it here: https://github.com/boche/LM-PTB-CUDNNLSTM. It also raises the same exception.\r\n\r\n> File \"ptb_word_lm.py\", line 465, in main\r\n>     m = PTBModel(is_training=True, config=config, debug=FLAGS.debug)\r\n>   File \"ptb_word_lm.py\", line 254, in __init__\r\n>     self._train_op = optimizer.apply_gradients(zip(allgrads, allvars))\r\n>   File \"/data/ASR1/ramons/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 409, in apply_gradients\r\n>     self._create_slots(var_list)\r\n>   File \"/data/ASR1/ramons/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/adam.py\", line 119, in _create_slots\r\n>     self._zeros_slot(v, \"m\", self._name)\r\n>   File \"/data/ASR1/ramons/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 609, in _zeros_slot\r\n>     named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\r\n>   File \"/data/ASR1/ramons/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 121, in create_zeros_slot\r\n>     val = array_ops.zeros(primary.get_shape().as_list(), dtype=dtype)\r\n>   File \"/data/ASR1/ramons/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 782, in as_list\r\n>     raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\n> ValueError: as_list() is not defined on an unknown TensorShape.\r\n\r\nFor now, using GradientDescentOptimizer is okay, but it seems all other fancy optimizers all have similar problems. I have looked up this problem on stackoverflow, and found a related thread: http://stackoverflow.com/questions/40698821/tensorflow-adamoptimizer-throws-error-when-variable-has-validate-shape-false . \r\n\r\nIt seems the problem is that parameter buffer used in CudnnLSTM doesn't have fixed shape (validate_shape=False, defined in line 145 of https://github.com/boche/LM-PTB-CUDNNLSTM/blob/master/ptb_word_lm.py ) , which is required by AdamOptimizer.\r\n \r\nCudnn seems to be faster (less time per epoch), but if we can't use better learning algorithms with it (meaning more epochs), then the total running time may not be improved so much.", "comments": ["I also met the problem. Now I am using a method like this: Compute the params size first , and build the cudnn_lstm model with fixed shape. It can work, but looks ugly. Hope to get a better solution.", "@boche, yeah, you are right that CudnnLSTM doesn't have a shape known at static time, which is required by AdamOptimizer. As CuDNN manages the internal storage structure for weight and bias parameters, this shape is hardware-and-CuDNN dependent and only available at runtime. I think the solution by @robotnc is a right, natural thing to do, given the this requirement of a known shape by AdamOptimizer, and the fact that it could only be provided at runtime by CuDNN.", "Looks like a serious issue; @zheng-xq @zhangyaobit - can we add static shape info to the parameters created by cudnn-rnn?  Perhaps by exposing the cudnn library helper functions via swig?  Is there a document describing how the shape varies as a fn of hardware and version?\r\n\r\nAlternatively can we modify the Adam optimizer to not require static shapes?", "Following up: looking at the slot_creator code, looks like we no longer require static shapes.  Can you check to see if the error still exists in the TF nightlies?", "I just tried it with the latest nightly build and the problem is still there.\r\n\r\nNote that what's needed is not really the shape, but the _size_ of the parameter buffer, since what's being passed to CudnnLSTM is just a flat 1D vector anyway. And the size should not vary from HW iteration to another (otherwise there cannot be a 'canonical' version and the whole premise of `RNNParamsSaveable` would be moot). Given that, it seems like a very simple solution would be to just calculate the size and return that for `model.params_size()`. For an LSTM it's literally just:\r\n\r\n```\r\nparams_size = ((input_size * layer_size * 4) + (layer_size * layer_size * 4) + (layer_size * 2 * 4)) * dir_count\r\n```\r\n\r\nfor one layer. Multiple layers would need to modify the input_size for layers > 1 to be the size of the previous layer.", "Any updates on this?", "A related issue is #5972, to be able to optimize variables with dynamic/unknown shape.\r\n", "@robotnc Could you elaborate more on your workaround? Thanks! ", "@dchaws: When you create the variable, use the static size `params_size` like @alquraishi describes. That's all. ", "could somebody give a code example for the workaround, please?", "any updates on this or a workaround ?", "AdamOptimizer should work with the new Cudnn **layer API** in cudnn_rnn.py\r\n\r\nWe have a CL to switch tf.contrib.cudnn_rnn.CudnnLSTM to point to classes in cudnn_rnn.py instead of cudnn_rnn_ops.py, it's running a series of tests, should be submitted soon.\r\n\r\nThe following should work with the new layer API.\r\n```py\r\n  num_layers = 4\r\n  num_units = 2\r\n  batch_size = 8\r\n  dir_count = 1\r\n\r\n  inputs = tf.random_uniform([\r\n      num_layers * dir_count, batch_size, num_units], dtype=tf.float32)\r\n\r\n  lstm = cudnn_rnn.CudnnLSTM(num_layers, num_units,\r\n                             name=\"awesome_lstm\")\r\n\r\n  outputs, _ = lstm(inputs)\r\n  loss = tf.reduce_sum(outputs)\r\n  var = lstm.trainable_variables[0]\r\n\r\n  grad = tf.gradients(loss, var)[0]\r\n  print('grad.shape: %s' % grad.shape)\r\n  print('var.shape: %s' % var.shape)\r\n\r\n  opt = tf.train.AdamOptimizer()\r\n  train_op = opt.apply_gradients([(grad, lstm.trainable_variables[0])])\r\n\r\n  with tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    print(sess.run(outputs))\r\n    sess.run(train_op)\r\n    print(sess.run(outputs))\r\n```", "@protoget what does \"CL\" stand for?", "It means \"PR\"\n\nOn Fri, Nov 3, 2017 at 1:31 AM, stefbraun <notifications@github.com> wrote:\n\n> @protoget <https://github.com/protoget> what does \"CL\" stand for?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6620#issuecomment-341644841>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyHlSvu5hKFY6pkM6qc961PBCKmMks5sys9xgaJpZM4LaA_V>\n> .\n>\n", "TypeError: __call__() takes at least 5 arguments (2 given)\r\nAlso , I am having difficulty figuring out the param sizes for bidirectional Cudnns.  So If my expected output for a single layer bi-directional cudnnLSTM is 2*dim  and input is of size [batch_size,seq_len,dim] how should i set up the h,c matrices . Is the following method correct in terms of usage. (Basically i cannot figure out the link between num_units,input_size in CudnnLSTM and in h and c matrix.\r\n\r\n> def createCudnnLayer(batch_size, input, input_length, dim, dropout_rate,scope_name, reuse, is_training) :\r\n> \r\n>     with tf.variable_scope(scope_name, reuse=reuse):\r\n>         inputs = tf.transpose(input, [1, 0, 2])\r\n>         cell = tf.contrib.cudnn_rnn.CudnnLSTM(direction='bidirectional',\r\n>                                               num_layers=1,\r\n>                                               num_units=dim*2,\r\n>                                               input_size=dim*2,\r\n>                                               dropout=dropout_rate if is_training else 0)\r\n>         params_size_t = cell.params_size()\r\n>         rnn_params = tf.get_variable(\r\n>             \"cudnn_params\",\r\n>             initializer=tf.random_uniform(\r\n>                 [params_size_t], -0.1, 0.1),\r\n>             validate_shape=False)\r\n>         c = tf.zeros([2, batch_size, dim], tf.float32)\r\n>         h = tf.zeros([2, batch_size, dim],tf.float32)\r\n>         outputs, h, c = cell(input_data=inputs, input_h=h, input_c=c, params=rnn_params, is_training=is_training)\r\n>         outputs = tf.transpose(outputs, [1, 0, 2])\r\n>         f_rep = outputs[:, :, 0:dim]\r\n>         b_rep = outputs[:, :, dim:2*dim]\r\n>         return (f_rep,b_rep) ", "You might be using the old cudnn_rnn **\"ops\"** API instead of the **\"layers\"** API.\r\nThe new nightly build (tmr) should now point ```tf.contrib.cudnn_rnn.CuDNNLSTM``` to the layers API.\r\n\r\nWith the layer API, you don't need to figure out the size yourself, the layer creates and owns the variables.\r\n", "excellent !! Thanks it works now. ", "will this change be included in tf-1.5? any examples for multi gpu usage of the new CudnnLSTM?", "yes it'd be included in 1.5.\r\nYou can check ```cudnn_rnn_test.py```. \r\n```ptb_word_lm.py``` is updated but not released to the public yet.", "Thanks again. Could you please advise on how to use CudnnLSTM trained models on cpu's at test time ?  i,e how to convert Bidirectional CudnnLSTM to say Bidirectional BasicLSTM that can be used on cpus.", "I'm no longer seeing problems with Adam, but both RMSprop and Adagrad don't appear to work with cudnnLSTM. This is the error I'm getting with RMSprop:\r\n\r\n`Shape of a new variable (geomnet/geomnet/layer0/fw/cudnn_lstm/opaque_kernel/RMSProp/) must be fully defined, but instead was <unknown>.`\r\n\r\nAnd Adagrad:\r\n\r\n`ValueError: If initializer is a constant, do not specify shape.`\r\n\r\nI tried steepest, momentum, adam, and adadelta and they all work.", "Re @alquraishi I have a fix pending review and hopefully submit in 2 dasy.", "@protoget  It does need to give state_h and state_c variable for CudnnLSTM in TF 1.5?"]}, {"number": 6619, "title": "Branch 143464290", "body": "", "comments": []}, {"number": 6618, "title": "Could not resolve github.com", "body": "I am trying for tensorflow for poets and while doing the following in docker image\r\n`cd /tensorflow`\r\n`git pull`\r\n\r\nerror:\r\n`fatal: unable to access 'https://github.com/tensorflow/tensorflow.git/': Could not resolve host: github.com`\r\n\r\n### attempted solutions \r\nfollowed the issue discussion `https://github.com/discourse/discourse_docker/issues/68`\r\nAfter editing `/etc/default/docker`, uncommenting the DOCKER_OPTS line makes no difference.", "comments": ["This one looks like it is an internet/github connection issue.\r\nThere is nothing we can do for this on TF side, as the problem happens before you even connect to github.", "but I am able to connect and reach github.com on browser.", "Still, docker, git and github are tools/resources we use but we do not manage.\r\nI do not see how we can help with this.", "As for some people, the issue solved by editing `/etc/default/docker` in the discourse_docker issue `https://github.com/discourse/discourse_docker/issues/68`, I hope there is something you can suggest.\r\n@gunan ", "I solved this by changing my DNS from the one it automatically gets to the OpenDNS one (208.67.222.222 for main and 208.67.220.220 for the alternative)."]}, {"number": 6617, "title": "Make *args in sv.loop example an iterable", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 6616, "title": "fake_quant_with_min_max_args has odd behavior", "body": "### Description\r\n\r\nOn a simple linear regression example, `fake_quant_with_min_max_args` is not working. If I change to `fake_quant_with_min_max_vars` with trainable quantization min/max ranges, it works just fine. \r\nThe min/max values are the same in both approaches. \r\nReproducer included below.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04.5, Python 2.7\r\n\r\nInstalled version of CUDA and cuDNN: Cuda 8, CuDNN 5.1\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 0.12.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nBelow is a small reproducer, adapted from the example at [https://www.tensorflow.org/get_started/](https://www.tensorflow.org/get_started/)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Create 10000 phony x, y data points in NumPy, y = x * 123.456 + 78.9\r\nx_data = np.random.rand(10000).astype(np.float32)\r\ny_data = x_data * 123.456 + 78.9\r\n\r\n# Try to find values for W and b that compute y_data = W * x_data + b\r\n# (We know that W should be 123.456 and b 78.9, but TensorFlow will\r\n# figure that out for us.)\r\nW = tf.Variable(tf.random_uniform([1], 0.0, 255.0))\r\nb = tf.Variable(tf.zeros([1]))\r\n\r\n# Now we quantize the weights and bias \r\n# The expected result after training should be \r\n# y = x * 123 + 79\r\nW = tf.fake_quant_with_min_max_args(W, min=0.0, max=255.0)\r\nb = tf.fake_quant_with_min_max_args(b, min=0.0, max=255.0)\r\n\r\ny = W * x_data + b # Model\r\n\r\nloss = tf.reduce_mean(tf.square(y - y_data))\r\noptimizer = tf.train.GradientDescentOptimizer(0.5)\r\ntrain = optimizer.minimize(loss)\r\n\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init)\r\n\r\n# Fit the line.\r\nfor step in range(201):\r\n    sess.run(train)\r\n    if step % 20 == 0:\r\n        print(step, sess.run(loss), sess.run(W), sess.run(b))\r\n```\r\nOutput is a random rounded values for W and b, depending on run, but not the expected W=123, b=79, and the loss is large.\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nIf I instead use `fake_quant_with_min_max_vars` as illustrated below, it works fine (by printing, we have verified that the quantization ranges are [0,255] for each training iteration). The loss decreases and the values for W and b are as expected. The example is adapted from [https://www.tensorflow.org/get_started/](https://www.tensorflow.org/get_started/)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Create 10000 phony x, y data points in NumPy, y = x * 123.456 + 78.9\r\nx_data = np.random.rand(10000).astype(np.float32)\r\ny_data = x_data * 123.456 + 78.9\r\n\r\nW = tf.Variable(tf.random_uniform([1], 0.0, 255.0))\r\nb = tf.Variable(tf.zeros([1]))\r\n\r\n# Now we quantize the weights and bias\r\n# The expected result after training should be \r\n# y = x * 123 + 79   Note that we train the quantization ranges\r\nqmin = tf.Variable(0.0)\r\nqmax = tf.Variable(255.0)\r\nW = tf.fake_quant_with_min_max_vars(W, min=qmin, max=qmax)\r\n\r\nqminb = tf.Variable(0.0)\r\nqmaxb = tf.Variable(255.0)\r\nb = tf.fake_quant_with_min_max_vars(b, min=qminb, max=qmaxb)\r\n\r\ny = W * x_data + b # Model\r\n\r\nloss = tf.reduce_mean(tf.square(y - y_data))\r\noptimizer = tf.train.GradientDescentOptimizer(0.5)\r\ntrain = optimizer.minimize(loss)\r\n\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init)\r\n\r\n# Fit the line.\r\nfor step in range(201):\r\n    sess.run(train)\r\n    if step % 20 == 0:\r\n        print(step, sess.run(loss), sess.run(W), sess.run(b))\r\n\r\nprint('Quantization ranges',  sess.run(qmin), sess.run(qmax), sess.run(qminb), sess.run(qmaxb))\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).", "comments": ["It looks like the gradient function for fake_quant_with_min_max_args forgets to pass the min max range.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L1877\r\nI'll send a fix.", "Hi, @yoslber   @suharshs \r\nin the prcess of  quantification \uff0c how to decide the value of  min and max  in  tf.fake_quant_with_min_max_vars(input, min, max)\r\n\r\n"]}, {"number": 6615, "title": "Make placement of constants follow consumers if they are all on the same device", "body": "A generator node (for instant a constant) will now go to the same device as its consumers, when those consumers are all on the same device. Previously a constant would only follow its consumer when there was only a single consumer.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "ok - i think we are going to need some help with this.   Is it that the same person who has made all of the commits that constitute a pull request must also make that pull request.  In which case, how do multiple people work on a feature in a fork, which is then included in a single pull request to the master repo?\r\n", "The two 'x's by both commits suggest that neither of the emails are being detected by the CLA.  The CLAbot would give a different message if all authors had signed the CLA (and that would be okay).\r\n\r\nIf your graphcore.ai is part of the googlegroup, can you also check to see that you've added that email to your GitHub account (public) ?  I think that's the only other requirement.", "(You should also generally squash your commits to avoid the merge commit in this case -- having a series of individual commits is typically only useful during the course of the review process, before merging, or for big feature additions, which typically should be sent as smaller incremental PRs anyway).", "Ok.  Will merge in recent head changes and squash all commits. \r\n\r\nI see the problem with the emails.  I guess I am commiting using an email that is not recognized by GitHub as an account.", "CLAs look good, thanks!\n\n<!-- ok -->", "The email thing worked.  \r\n\r\nThe 2 merge commits are just where I was pulling the main tensorflow master to the repo to verify that the changes still work.  i wasn't expecting the change to take so long to get into the repo, so i didn't isolate the commit onto a branch originally.", "(We can update the name later, let's get this in)\r\n\r\n@tensorflow-jenkins test this please", "hmm.. I didn't run any of the tests but my own one.  I'll run them all now and see if i can track down these failures.  I guess there is a higher level knock on effect maybe.  Seems odd that the CPU only tests would change though.  I would assume that the placement is always on the same device.", "the failing test does relate to the placement of constants.  it is strange that it should fail, because it explicitly places the constants, and I was pretty sure that explicitly placed things are not affected by my change.  apparently not", "I'm not sure i fully understand what is going on in the placer.  I have traced some of the state.\r\n\r\n```\r\nGraphdef : versions = producer: 20;\r\nconst1 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 1>, _device=\"/device:CPU:0\"]();\r\nconst2 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 2>, _device=\"/device:CPU:1\"]();\r\nadd = Add[T=DT_FLOAT, _device=\"/device:CPU:2\"](const1, const2);\r\nmul = Mul[T=DT_FLOAT, _device=\"/device:CPU:2\"](const1, const2);\r\nadd_1 = Add[T=DT_FLOAT, _device=\"/device:CPU:2\"](add, mul);\r\n\r\nNode const2 has assigned name \r\nNode const1 has assigned name \r\nNode mul has assigned name \r\nNode add has assigned name \r\nNode add_1 has assigned name \r\n.[u'/job:localhost/replica:0/task:0/cpu:1', u'/job:localhost/replica:0/task:0/cpu:0', u'/job:localhost/replica:0/task:0/cpu:2']\r\n```\r\n\r\nthe first batch is the graphdef showing that the nodes in the testAnalysisAndAllocations test (of //tensorflow/python:timeline_test) know the explicit placements that have been made for them.  The 'Node const2 has assigned name' is saying that node->assigned_device_name() in simple_placer.cc has no value (is skipped in the test on line ~720).\r\n\r\nWhy isn't the assigned name that is present in the graphdef present in the placer where it is deliberately skipping over things that are pre-placed?\r\n", "Ok.  Now I have dumped the Node*.  It is:\r\n\r\n```\r\nNode {name:'const2' id:2 op device:{} def:{const2 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 2>, _device=\"/device:CPU:0\"]()}}\r\n```\r\n\r\nThis shows that the 'op device' is {}, meaning there is no assigned device, but the NodeDef has _device set correctly.  I shall see if I can find out where one is assigned to the other (if this is what happens at all)", "So... I think what is happening is that the placement algorithm goes something like this:\r\n\r\n```\r\npass A\r\n1) for each node check if it has already been placed (not by the user but by the algorithm)\r\n2) defer 'generator nodes' to a later pass\r\n3) place nodes according to possible placements (according to GetDevicesForNodes - which is where the explict user-specified location comes in)\r\n\r\npass B\r\n1) place generator nodes with their consumers\r\n```\r\n\r\nso generator nodes are not guided by the user's explicit placement commands.  the test 'timeline_test' is only valid when the definition of generator nodes does not include nodes which feed more than one consumer.  I have changed the definition to include multiple consumer nodes, so the test doesn't work any more.\r\n\r\nnot sure what to do about this.  I could doctor up the test to avoid using generator nodes. \r\n\r\n\r\ndo you have any advice?\r\n", "Hm, that might be a bug that the generator nodes are not getting their explicit device placements -- let me test a change to see.", "Okay, there are likely two issues here:\r\n\r\n1) That skip to check for already assigned devices is bogus (it's a no-op right now, must have been due to a change).\r\n\r\n2) Heuristics should only apply if there are no device specifications, even partial ones (at least, that is the intention right now).\r\n\r\nI'll send a fix and a test for this.", "Okay, after talking with mrry@, we found out my statement above is wrong.\r\n\r\n1) The check for already assigned devices is necessary for stateful placements (they get set in simple_graph_execution_state.cc, and must be skipped).\r\n\r\n2) The heuristic applies only after checking that to-be-assigned device is in the list of the devices (potentially determined by a partial or full device spec), so there's no violation going on here.\r\n\r\nI'll have to dig in more to figure out what's going n.", "indeed it did seem like the check for existing device placement was a no-op, but i was expecting there to be an outside case that I wasn't aware of.\r\n\r\nit seems that the generator placement doesn't follow the explicit placement though.   the explicit placement comes in through the GetDevicesForNode.  In the case of a generator node, even if there is an explicit placement, the assigned node is determined by the location of the consumer.   This would have applied when a generator node had only one consumer, too.\r\n\r\nI could generate a simple python test that demonstrates the problem, if you would like.\r\n", "Actually - with reference to your point 1 - i would be tempted to put a comment against the pre- assigned_name test referencing that.   otherwise it seems a bit opaque when someone is running stuff through the debugger.", "Indeed -- I think I have a better fix coming (including a test for the need for the stateful placements and a comment).", "nice one :)", "Okay, about to submit a fix and some more tests -- should be available later today if our pushing process goes well.\r\n\r\nBasically, CanAssignToDevice was only checking that the device type was valid, not that the device was in the list generated by the placer (which uses the user's device spec).  The change I am submitting, makes CanAssignToDevice check that the consumer's device is in the list of the potential devices.\r\n\r\nThe reason timeline_test failed is that the consumer was on /cpu:1, the producer was on /cpu:2 (explicit), and because the device types were the same, it moved producer to /cpu:1, which causes the test to fail.  With this change, the producer will always remain on /cpu:2.\r\n\r\nLastly, the check for assigned_devices / continuing could be removed without breaking any test, but is an optimization (there is no need to do more work to check for metadata / generators when you know the device has been assigned already), so I just added a comment to make it more clear.\r\n\r\nAfter it gets pushed, we can merge this PR and things should get better.", "I can see that your commit is in, and I like it. \r\n\r\nPerhaps this will cause the buildbot to check the code:\r\n\r\n@tensorflow-jenkins test this please", "i can also see that a local run of timeline test is ok.   I can also see that XLA is in the repo, which is good.  I think I shall have to spend some time browsing that code base now.  \r\n\r\nHopefully you don't think that this change has been a big time hole for you.  It has been very useful for me, especially to get a feeling for the process of pushing code into the public repo.  I guess I will have to do more of this in the future.\r\n\r\nIt is possible that XLA eliminates the problem that I was trying to solve with this change, although I suspect that it will be useful in general (until the placer is re-written), and also it helped to expose the thing where an explicitly placed constant would still follow its consumer.\r\n\r\nI will, soon, do another merge request for the fix for my other issue (where a tree with a constant value remains on a device even when replaced by a single constant, due to the presence of control edges).\r\n", "@tensorflow-jenkins test this please\r\n\r\nThanks!", "oh wait, you have to merge to master to get my changes.  the tests are going to fail again.", "and no, no problem at all - you helped us find a subtle bug, and since not all devices will end up wanting to use XLA, these heuristics are still useful (if even as examples of the types of heuristics that we will want to maintain in the future),  so thank you for pushing this change forward! :)", "it needed a little change to the test to make it compatible with the recent changes to the test device names.\r\n", "hopefully it will be ok now.\r\n\r\nif this works, I may push through my one and only other 'fix', which is a fix for:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/5543\r\n\r\n\r\nIncidentally - this change is the fix for:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/5745\r\n", "Jenkins, test this please.", "@vrv if this doesn't look right, feel free to comment on this thread, o/w I'll merge.", "cheers.  "]}, {"number": 6614, "title": "zlib has disappeared", "body": "Obviously this isn't your doing, but http://zlib.net/zlib-1.2.8.tar.gz and http://zlib.net/zlib-1.2.9.tar.gz are no longer on that server. The new http://zlib.net/zlib-1.2.10.tar.gz released yesterday is there. \r\n\r\nBecause dependencies are downloaded as needed this is breaking the build.", "comments": ["Duplicate to https://github.com/tensorflow/tensorflow/issues/6594"]}, {"number": 6613, "title": "tensorflow cannot download packages when run ./configure", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nwhen I run ./configure, below comes out:\r\nERROR: /home/louis/Documents/dep-tensorflow/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@font_roboto//': Error downloading from https://github.com/polymerelements/font-roboto/archive/v1.0.1.tar.gz to /home/louis/.cache/bazel/_bazel_louis/d4f28d83a590525a3c3f2c81d1c44f93/external/font_roboto: Error downloading https://github.com/polymerelements/font-roboto/archive/v1.0.1.tar.gz to /home/louis/.cache/bazel/_bazel_louis/d4f28d83a590525a3c3f2c81d1c44f93/external/font_roboto/v1.0.1.tar.gz: Timed out connecting to https://github.com/polymerelements/font-roboto/archive/v1.0.1.tar.gz : connect timed out and referenced by '//tensorflow/tensorboard/bower:bower'.\r\n\r\nERROR: /home/louis/Documents/dep-tensorflow/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@graphlib//': Error downloading from https://github.com/cpettitt/graphlib/archive/v1.0.7.tar.gz to /home/louis/.cache/bazel/_bazel_louis/d4f28d83a590525a3c3f2c81d1c44f93/external/graphlib: Error downloading https://github.com/cpettitt/graphlib/archive/v1.0.7.tar.gz to /home/louis/.cache/bazel/_bazel_louis/d4f28d83a590525a3c3f2c81d1c44f93/external/graphlib/v1.0.7.tar.gz: Timed out connecting to https://github.com/cpettitt/graphlib/archive/v1.0.7.tar.gz : connect timed out and referenced by '//tensorflow/tensorboard/bower:bower'.\r\n\r\nERROR: /home/louis/Documents/dep-tensorflow/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_a11y_announcer//': Error downloading from https://github.com/polymerelements/iron-a11y-announcer/archive/v1.0.4.tar.gz to /home/louis/.cache/bazel/_bazel_louis/d4f28d83a590525a3c3f2c81d1c44f93/external/iron_a11y_announcer: Error downloading https://github.com/polymerelements/iron-a11y-announcer/archive/v1.0.4.tar.gz to /home/louis/.cache/bazel/_bazel_louis/d4f28d83a590525a3c3f2c81d1c44f93/external/iron_a11y_announcer/v1.0.4.tar.gz: Timed out connecting to https://github.com/polymerelements/iron-a11y-announcer/archive/v1.0.4.tar.gz : connect timed out and referenced by '//tensorflow/tensorboard/bower:bower'.\r\n\r\nINFO: Downloading from https://github.com/polymerelements/iron-a11y-keys-behavior/archive/v1.1.2.tar.gz: 0B\r\n\r\nJust as it shows in INFO, it seems it can not download the package that causes the error, and i am sure the internet is ok ,because i could download the package in the web.\r\n\r\n### Environment info\r\nOperating System:\r\nUBUNTU 14.04\r\n\r\nBazel Version:\r\nbazel 0.3.2\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\ncuda8.0\r\ncuDNN:5.0\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\ngit clone -b v0.11.0 https://github.com.................\r\n2. The output of `bazel version`\r\n\r\nBuild label: 0.3.2- (@non-git)\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Sun Jan 1 22:19:10 2017 (1483309150)\r\nBuild timestamp: 1483309150\r\nBuild timestamp as int: 1483309150\r\n", "comments": ["I manually tried all the packages in the error messages above, and they all download ok for me.\r\nAlso, our CI is able to download all of these packages OK.\r\n\r\nCould you try again? maybe it is a transient connection issue on your end.", "I had similar troubles because my `bazel` was outdated. It works fine for me on `bazel` 0.4.3.", "@louisway did a newer bazel fix it?", "This was probably due to an ephemeral GitHub outage. At HEAD we've got a new system for redundant downloading where this will never happen again thanks to https://github.com/bazelbuild/bazel/commit/4c67807964e37cfd55bbcda4c6374fcc480bcecc and https://github.com/bazelbuild/bazel/commit/ed7ced0018dc5c5ebd6fc8afc7158037ac1df00d. However we haven't applied it to the TensorBoard downloads yet since that code is generated. It will be soon enough.", "sudo /var/lib/dpkg/info/ca-certificates-java.postinst configure"]}, {"number": 6612, "title": "worspace.bzl uses zlib permalink", "body": "zlib permalinks including the current version are always in http://zlib.net/fossils/zlib-*.*.*.tar.gz\r\nlinks in http://zlib.net/zlib-*.*.*.tar.gz are removed when no longer current.\r\n\r\nShould resolve #6594 if merged into r0.12 branch.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please\r\n\r\nI forgot that github now allows us to make these edits, so I went ahead and made it.  Will submit after merging.\r\n\r\ncc @yaroslavvb "]}, {"number": 6611, "title": "Word2Vec Number of Steps in Example", "body": "In the word2vec basic example (master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py) the number of steps is set to 100001. Is this an arbitrary number? \r\n\r\nShould there not be a len(data) // (2*skip_window + 1  + (batch_size // num_skips)) number of steps to go through the data set?", "comments": ["This type of question is better asked on [Stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow). Github issues are for bug reports and installation issues.", "I believe that this is a bug in the code. Why is this better asked on stack overflow? \r\n\r\nSee line 185 in the word2vec_basic code and notice that the number of steps is set seemingly arbitrarily. The model does not iterate through even half of the dataset like this.\r\n\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\r\n\r\n@michaelisard ", "You are right this isn't a stackoverflow question: my apologies!\r\n\r\n@sherrym I don't know if this is a bug or intentional for tutorial purposes but even if so I guess it needs a comment?", "Shoudnt it be only : len(data) // (batch_size // num_skips) ?", "My intuition was from looking at the generate_batch function and seeing when data_index was incremented. \r\n\r\nThe first loop increments data_index 2 * skip_window + 1 times.\r\n\r\nThe next for loop increments data_index (batch_size // num_skips) times.\r\nSo I use this to calculate the number of steps (2*skip_window + 1  + (batch_size // num_skips)).\r\n\r\nThe formula  len(data) // (batch_size // num_skips) allows the data index to be reset to 0 and we begin training at the beginning of the data again for a few steps.\r\n\r\nWith a batch size of 128, and a data_index that has a max of 4251300:\r\n\r\nFor the formula len(data) // (batch_size // num_skips):\r\n('Est. Avg loss at step ', 60000, ': ', 13.664521800994873, ' data index 4020067')\r\n('Est. Avg loss at step ', 61000, ': ', 12.748281367778779, ' data index 4087067')\r\n('Est. Avg loss at step ', 62000, ': ', 13.45276182770729, ' data index 4154067')\r\n('Est. Avg loss at step ', 63000, ': ', 12.419636116027831, ' data index 4221067')\r\n('Est. Avg loss at step ', 64000, ': ', 12.700695453643799, ' data index 36766')\r\n('Est. Avg loss at step ', 65000, ': ', 11.144913824081421, ' data index 103766')\r\n('Est. Avg loss at step ', 66000, ': ', 11.105838320016861, ' data index 170766')\r\n\r\n\r\nFor the formula (2*skip_window + 1  + (batch_size // num_skips)):\r\n('Est. Avg loss at step ', 59000, ': ', 13.633916213512421, ' data index 3953067')\r\n('Est. Avg loss at step ', 60000, ': ', 14.011484805822372, ' data index 4020067')\r\n('Est. Avg loss at step ', 61000, ': ', 12.926126554250716, ' data index 4087067')\r\n('Est. Avg loss at step ', 62000, ': ', 13.53601130437851, ' data index 4154067')\r\n('Est. Avg loss at step ', 63000, ': ', 12.231826926708221, ' data index 4221067')\r\n", "Actually only looking at it after posting that comment it seems both are getting cut short on the whole data set and I am not sure why. I'll have to make sure I haven't messed something up.\r\n\r\nSorry one issue is that I am only printing loss every 1000 steps sorry for that - will run more carefully.", "With finer grained output: \r\n\r\nlen(data) // (batch_size // num_skips):\r\n\r\n('Est. Avg loss at step ', 63449, ': ', 0.015983373641967773, ' data index 4251150')\r\n('Est. Avg loss at step ', 63450, ': ', 0.011031746864318848, ' data index 4251217')\r\n('Est. Avg loss at step ', 63451, ': ', 0.013918094635009766, ' data index 4251284')\r\n('Est. Avg loss at step ', 63452, ': ', 0.014092603683471679, ' data index 50')\r\n('Est. Avg loss at step ', 63453, ': ', 0.0071760320663452146, ' data index 117')\r\n('Est. Avg loss at step ', 63454, ': ', 0.014572011947631835, ' data index 184')\r\n('Est. Avg loss at step ', 63455, ': ', 0.0071507630348205562, ' data index 251')\r\n\r\n(2*skip_window + 1 + (batch_size // num_skips)):\r\n('Est. Avg loss at step ', 63446, ': ', 0.025812118530273438, ' data index 4250949')\r\n('Est. Avg loss at step ', 63447, ': ', 0.028827564239501953, ' data index 4251016')\r\n('Est. Avg loss at step ', 63448, ': ', 0.014959125518798829, ' data index 4251083')\r\n('Est. Avg loss at step ', 63449, ': ', 0.017365852355957032, ' data index 4251150')\r\n('Est. Avg loss at step ', 63450, ': ', 0.010945444107055663, ' data index 4251217')\r\n('Est. Avg loss at step ', 63451, ': ', 0.014031584739685059, ' data index 4251284')", "Unfortunately I don't think a simple formula for the number of steps is sufficient to cover all cases.  The proposed formula does only one epoch, which is usually not what one wants.  Since this is a tutorial, I think a hardcoded number of steps is fine."]}, {"number": 6610, "title": "whether we have another function that have the same function as tf.select in the latest version\uff1f", "body": "I found there is no \"tf.select\" in the latest version\uff08tensorflow\uff09 ,so I want to know if we have another function that have the same function as tf.select", "comments": ["From [RELEASE.md for 0.12](https://github.com/tensorflow/tensorflow/blob/c72d891418c326794a913cc13382c104255fe1f2/RELEASE.md#breaking-changes-to-the-api-1)\r\n\r\n`Deprecated tf.select op. tf.where should be used instead.`"]}, {"number": 6609, "title": "Fix capitalization of \"TensorFlow\".", "body": "Fix capitalization of \"TensorFlow\" in MNIST For ML Beginners tutorial.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Closing this PR due to inactivity for five days. @mcsmart76 If you'd like to work on it, please reopen it."]}, {"number": 6608, "title": "Detect and match against full cuda and cudnn versions.", "body": "Currently, if a user specifies the full cuda or cudnn versions (with\r\nmajor, minor, and patch numbers), cuda_configure will fail because we\r\nonly detect the version numbers we use (major and minor for cuda and\r\nmajor only for cudnn). This change detects full versions and matches the\r\nuser-provided versions against the full versions. If the user provides\r\nonly partial version numbers (e.g. without patch version), then the\r\nmatch will still succeed if the provided numbers match the detected\r\nnumbers.\r\n\r\nThis fixes the issue raised in this [StackOverflow post](http://stackoverflow.com/questions/41117980/cant-install-cudnn-5-1-5-with-cuda-8-0-and-tensorflow-0-12-on-g2-2xlarge-instan).", "comments": []}, {"number": 6607, "title": "tf.contrib.learn yields error message \u201cmodule has no attribute 'learn' \u201d", "body": "Here is a snippet of my code taken directly from the tf.contrib.learn tutorial \r\n\r\n`# Load Data Sets\r\ntraining_set = tf.contrib.learn.datasets.base.load_csv_with_header(\r\n    filename = IRIS_TRAINING,\r\n    target_dtype = np.int,\r\n    features_dtype = `np.float32)`\r\n\r\nHere is the error message:\r\n\r\n`AttributeError                            Traceback (most recent call last)\r\n<ipython-input-14-7122d1244c55> in <module>()\r\n     11 \r\n     12 # Load Data Sets\r\n---> 13 training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\r\n     14     filename = IRIS_TRAINING,\r\n     15     target_dtype = np.int,\r\n\r\nAttributeError: 'module' object has no attribute 'learn'`\r\n\r\nClearly the module has the attribute learn since tensorflow has a section on learning tf.contrib.learn. What am I doing wrong? All guidance is appreciated.", "comments": ["Please make sure to fill out all of the template when filing bugs.\r\nWhich operating system are you running on?\r\nWhich version of tensorflow are you using?\r\n\r\nIn our setups we cannot reproduce this problem. We need much more information than you provided if we are to help you with this problem.", "Closing due to lack of recent activity. We will reopen when additional information becomes available. Thanks!"]}, {"number": 6606, "title": "could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM Check failed: stream->parent()-GetConvolveAlgorithms(&algorithms) ```", "body": "\r\nI'm trying to use tensorflow for this project: https://github.com/ibab/tensorflow-wavenet\r\n\r\nI've gotten to the point where when I import tensorflow, I get the messages that all the CUDA libraries are successfully opened locally.\r\n\r\nI can run the following python code from https://www.tensorflow.org/get_started/os_setup#run_tensorflow_from_the_command_line and it works fine.\r\n\r\n> import tensorflow as tf\r\n> hello = tf.constant('Hello, TensorFlow!')\r\n> sess = tf.Session()\r\n> print(sess.run(hello))\r\nHello, TensorFlow!\r\n> a = tf.constant(10)\r\n> b = tf.constant(32)\r\n> print(sess.run(a + b))\r\n42\r\n>\r\n\r\nHowever when I run the wavenet project, I get the following error messages and then python crashes.\r\n\r\n```\r\nc:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 950\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.19\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.65GiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950, pci bus id: 0000:01:00.0)\r\nWARNING:tensorflow:From train.py:249 in main.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\nInstructions for updating:\r\nUse `tf.global_variables_initializer` instead.\r\nTrying to restore saved checkpoints from ./logdir\\train\\2017-01-02T16-17-15 ... No checkpoint found.\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:532] Check failed: stream->parent()-GetConvolveAlgorithms(&algorithms)\r\n```\r\n\r\n\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/4251\r\n\r\n\r\n### Environment info\r\nOperating System:\r\nWindows\r\n\r\nInstalled version of CUDA and cuDNN: \r\ncuDNN v5.1 (August 10, 2016), for CUDA 8.0\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nNameError: name 'tensor' is not defined\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nI have tried reinstalling Cuda, different versions of cudnn. Looked at different issues with same error messages but nothing seemed to help.\r\n\r\n", "comments": ["Hi, @aclaussen1 , I got the same issue when running https://github.com/ibab/tensorflow-wavenet, and I also tried many solutions, but all failed.\r\n\r\nHave you fixed the issue?", "I'm using TensorFlow on Windows Server 2012, TensorFlow version is 0.12.1\r\nI've install CUDA (CUDA Version 8.0.44), and install cuDnn (cuDNN v5.1 (August 10, 2016), for CUDA 8.0).\r\n\r\nthat cuDnn is build for windows 10.\r\n\r\nBut when I run my tensorflow code is get some error about:\r\n```\r\nam_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_IN\r\nTERNAL_ERROR\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_B\r\nAD_PARAM\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\r\n\\kernels\\conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(\r\n&algorithms)\r\n```\r\n\r\nIt seems the cuDNN version is not correct, but I tried cuDNN v5.0, also got this exception.\r\n\r\ndoes anyone know how to fix this issue?\r\nThanks very much.", "This seems to be same as #6509 ", "Closing duplicate issue. Please follow #6509 for updates.", "I gave up on Windows and got it to work with Ubuntu @weixsong ", "I am in Ubuntu, I have this three errors and I found it is because insufficient memory on the GPU I specified. That GPU is occupied by some other programs. I changed to another GPU with more memory available. The problem is gone.", "today I get the same issue,but not \u5e72\u6389it!", "config = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.7\r\nsess = tf.Session(config=config)\r\nsess.run(init)", "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333) ", "I have the same error, I found that @cliffzwj 's solution does work! thanks~ "]}, {"number": 6605, "title": "Commit to enable true fully convolutional application of network", "body": "Hello,\r\n\r\nI have created a pull request to tensorflow/models long time ago\r\nand just want to point out that it will make using TF-Slim easier for\r\nImage Segmentation:\r\nhttps://github.com/tensorflow/models/pull/684\r\n\r\nIs there a chance someone can review it?\r\n\r\nThank you.", "comments": ["I have added reviewers to that PR, thanks."]}, {"number": 6604, "title": "add summary_feed_dict support to tf.train.Supervisor", "body": "Example use case:\r\n\r\n> We want to use `tf.summary.image` on a (computed) tensor which requires a `feed_dict` of some input tensor.\r\n\r\nCurrent work around:\r\n\r\n> Manage `summary_op` manually (inside training loop). Cannot use any higher level training helper.\r\n\r\nI could also provide a PR on interest.", "comments": ["One possible solution I see with `tf.train.MonitoredTrainingSession`:\r\n\r\n```python\r\nwith tf.train.MonitoredTrainingSession(hooks=hooks) as session:\r\n    while not session.should_stop():\r\n        session.run(train, feed_dict={\r\n            x: batch[0].eval(session=session),\r\n            y: batch[1].eval(session=session),\r\n        })\r\n```\r\n\r\nthat we take the `feed_dict` argument of `session.run` and pass it down to every `hook` where we add that `feed_dict` as default to `tf.train.SessionRunArgs`.", "BTW, you can use supervisor `loop` construct to add a modified summary writer:\r\n\r\n```\r\nsv = tf.train.Supervisor(..., summary_op=None)\r\nwith sv.managed_session() as sess:\r\n    sv.loop(60, lambda: sv.summary_computed(sess, sess.run(summary_op, feed_dict=...)))\r\n    <your training loop>\r\n```", "This problem is not limited to `TensorLoggingHook` and `SummarySaverHook` but also to custom `SessionRunHook`s which at the moment provide the only modular interface to organize such things around your training / testing evaluation.\r\n\r\nAs we already have support for `init_dict` and `init_fn` on many interfaces (e.g. `tf.train.Supervisor`, `tf.train.Scaffold`) I guess it would be consistent to add something similar for evaluation.\r\n\r\nAt the moment calls to `monitored_session.run(fetches, feed_dict=...)` forward this information so I can do:\r\n\r\n```python\r\nclass LoggerHook(tf.train.LoggingTensorHook):\r\n\r\n    def before_run(self, run_context):\r\n        run_args = super().before_run(run_context)\r\n        feed_dict = run_context.original_args.feed_dict\r\n\r\n       # extract arguments from `monitored_session.run(...)` out of `feed_dict`\r\n\r\n        return run_args\r\n```\r\n\r\n**update**\r\nThis approach fails if a tensor in `tensors` of `tf.train.LoggingTensorHook` depends on a `tf.placeholder` with [Same tensor is fed by a SessionRunHook and user.'](https://github.com/tensorflow/tensorflow/blob/f1d60927dddd1599135ccb31ec9a8b439e2eaa37/tensorflow/python/training/monitored_session.py#L916-L918).\r\n\r\n**update2**\r\nIf I use the `batch` tensors instead of placeholders then everything works fine. Does this mean that we only require placeholder if we want to use \"external\" data (not handled by tensorflow directly)? In this case this feature may only benefit a small target group as most people will use an input queue at this scale.", "Related: http://stackoverflow.com/questions/41366248/using-tf-train-supervisors-default-summary-op-on-validation-data", "@bodokaiser yes, `feed_dict` is only needed for data external to TensorFlow. So doing `feed_dict={x: batch[0].eval(session=session),` is a bit odd, since you are taking data out of TF runtime, and putting it back-in without modification", "@mrry do you have any thoughts? Or reassign...", "@michaelisard My understanding was that we're deprecating `tf.train.Supervisor` and encouraging people to move to the `tf.train.FooSession` classes, so any features would be added here. Sherry and/or Mustafa are the respective owners for these.", "@mrry are you referring to [training hooks](https://www.tensorflow.org/versions/master/api_docs/python/train/training_hooks) with `tf.train.MonitoredSession`?", "@carlthome Hooks are the main way to add features to a `tf.train.MonitoredSession`, yes. It's possible that such a feature would require changes elsewhere, but I'll let @ispirmustafa comment on that.", "Closing since `tf.train.Supervisor` is deprecated in favor of `tf.train.MonitoredSession`.", "Hi there, and excuse me for posting on a closed Issue. Thus, `Supervisor` will be deprecated soon and `MonitoredTrainingSession` is the way future way to load/save models and write summaries? Cheers!", "@girving , @mrry - if `tf.train.Supervisor` will be deprecated soon, can someone edit the programming guide [page](https://www.tensorflow.org/programmers_guide/supervisor) to let people like myself know?\r\n\r\nIdeally, I would appreciate a replacement for that page based on `tf.train.MonitoredSession` etc.", "We're in the process of updating those programming guides.", "@ispirmustafa , awesome! Do you have a rough idea of when those guides will be updated?"]}]