[{"number": 1750, "title": "NotImplementedError: grad(Digamma) == Polygamma(1) is not implemented", "body": "when my graph has the digamma function\n", "comments": ["Duplicate of #1741 \n"]}, {"number": 1749, "title": "Request: Better documentation on TFRecords and their use", "body": "I am using TFRecords to use the sharding/queuing machinery. For simple problems for which the examples can be cloned, it is easy to use. But recently I tried to do something more complex: train examples where there are multiple labels. I posted a question on Stackoverflow about the specific issue I have: http://stackoverflow.com/questions/36365118/how-do-you-write-and-retrieve-tfrecord-features-that-are-lists\n\nIt would be nice to have clearer guidance on the use of TFRecords. I've looked through the code, have a pretty good understanding of Protocol Buffers, etc. In spite of this it is difficult to understand how the machinery works.\n", "comments": ["@robwell I saw your stackoverflow question was resolved. Is this still relevant? Thanks!\n", "@alextp The issue here is not a question on SO. It's more a request for better documentation for TFRecords. This Issue here is the _4th hit_ on Google Search if you look for 'TFRecords'. \nhttps://www.tensorflow.org/versions/r0.11/how_tos/reading_data/index.html has a exact grand total of **8  sentences** on the topic of TFRecords.\nIt literally states it is The [Tensorflow] Standard Data Format, it would make sense to me to elaborate more on this topic with examples and good usage practices. And sure, the well documented API itself provides the tools, but does not touch upon the actual usage and power of TFRecords.\n", "it would also be awesome if there would be some function which easily writes examples in the given tfrecords file. A function you give a tensor of the for example a image and the label tensor for this image (one or multidim label) and it writes this to the tfrecord file. \n", "We revamped our docs. Feel free to open a new issue if this is still missing from our new docs.", "I've tried to explain the basics of TFRecords in this post: [TFRecords for Humans](http://planspace.org/20170323-tfrecords_for_humans/). Maybe it's helpful for at least some aspects?\r\n\r\nI'm thinking of continuing to write, specifically on working with images, and perhaps also on reading in-graph.", "Thanks @ajschumacher, that's really neat!", "I've written a bit more about working with images specifically: [Images and TFRecords](http://planspace.org/20170403-images_and_tfrecords/). Also includes a bit of background on how image formats work. Hope it's helpful!", "Ideally we don't have to scrape people's blogs for documentation but have it in the documentation. @drpngx closed the issue saying the docs are revamped. But really, on the topic of TFRecords usage there is still exactly that same single one: paragraph.https://www.tensorflow.org/programmers_guide/reading_data#reading_from_files.\r\n\r\nOne sentence reads`The recommended format for TensorFlow is a TFRecords file (..) `; given the fact that **the paragraph on the topic consists of exactly 9 sentences**, that's pretty weird.\r\n\r\nTF is obviously great, and we could contribute ourselves here, but don't close the issue when it's obviously open. @ajschumacher how about contributing to the docs instead of the blog?", "As well as including information on how to create inference programs with a model that was trained using queue runners and tfrecords.", "Exactly, this is not about one question or doubt or a blog post. Tensorflow, in general, has very vague documentation. It seems totally confusing for beginners like me. For eg. https://www.tensorflow.org/api_docs/python/tf/train/Features\r\n", "This needs more attention... All we're left to do is peruse source code or random blogs. Official docs are greatly needed!", "Yes, please add more documentation. TFRecords seem to greatly increase performance; it would be helpful to have an official guide on their usage", "I'm not able to write more than 1.3 million tensors at once. Is there a way I can boost this to say 2 or 3 million tensors? I'm using this code to write tensors\r\n\r\nwith tf.python_io.TFRecordWriter(tfrecords_path) as writer:\r\n            for index, image in enumerate(images):\r\n                features = tf.train.Features(feature={\r\n                    'labels': self.int64_feature(labels[index]),\r\n                    'images': self.bytes_feature(image),\r\n                    'imagenames': self.bytes_feature(imagenames[index])\r\n                })\r\n                example = tf.train.Example(features=features)\r\n                writer.write(example.SerializeToString())\r\n                sys.stdout.write('\\r>>Writing {:d}/{:d} {:s} tfrecords'.format(index+1, len(images), imagenames[index]))\r\n                sys.stdout.flush()\r\n            sys.stdout.write('\\n')\r\nsys.stdout.flush()"]}, {"number": 1748, "title": "Optimizers incompatible with sampling -- missing docs?", "body": "Hi,\n\nperhaps Tensorflow docs should mention that 5 out of 7 available optimizers will not work with sampling losses? For now, they fail with mysterious messages.\n\nThe following script will fail for Momentum, AdaGrad, AdaDelta, RMSProp and FTRL.\n\nAlso, where should I look if I'd like to implement my own optimizers for GPU?\n\n``` python\nimport tensorflow as tf\nimport numpy.random as nr\nimport numpy as np\n\n# config \nnum_classes = 10000\nnum_sampled = 512\nnum_true = 32\nactivation_dim = 512\nbatch_sz = 16\n\n# \"model\" setup\nactivations = tf.placeholder(tf.float32, shape=(None, activation_dim))\nlabels = tf.placeholder(tf.int64, shape=(None, num_true))\n\nnce_W = tf.Variable(tf.truncated_normal((num_classes, activation_dim)))\nnce_b = tf.Variable(tf.truncated_normal((num_classes, )))\n\nnce_loss = tf.reduce_mean(\n                tf.nn.nce_loss(nce_W, nce_b, \n                               activations, labels, \n                               num_sampled, num_classes, num_true))\n\n# optimizer setup\nglobal_step   = tf.Variable(1)\ninitial_alpha = tf.Variable(0.1)\nalpha         = tf.Variable(0.01)\n\noptimizer = tf.train.FtrlOptimizer(alpha)\n\n# fetches\nstep = optimizer.minimize(nce_loss, global_step=global_step, name='sgd_step') \ninit = tf.initialize_all_variables()\n\n# synthetic data\nX = nr.randn(batch_sz, activation_dim).astype(np.float32)\ny = nr.randint(0, num_classes, (batch_sz, num_true)).astype(np.int64)\n\nwith tf.Session() as sess:\n    sess.run(init)\n    sess.run(step, feed_dict={activations:X, labels:y})\n```\n\nThe error message is:\n\n```\n---------------------------------------------------------------------------\nStatusNotOK                               Traceback (most recent call last)\nStatusNotOK: Invalid argument: Cannot assign a device to node 'Variable_1/read': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\n     [[Node: Variable_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_1\"]](Variable_1)]]\n```\n", "comments": ["Does pinning the variables to CPU make this work? i.e.\n\n``` python\nwith tf.device(\"/cpu:0\"):\n  nce_W = \u2026\n  nce_b = \u2026\n```\n", "@mrry I assume this is fixed now, in that we don't do improper placement, but I'm not sure all sparse optimizers are supported on GPU, which is perhaps another bug which I believe we already have an issue for.\n"]}, {"number": 1747, "title": "Support OpenMP and SIMD?", "body": "char foo(char *A, int n){\nint i;\nchar x = 0;\n# pragma omp parallel for simd\n\nfor(i = 0; i < n; i++)\nx = x + A[i];\nreturn 0;\n}\n\n$ icc test.c -c -vec-report2 -c -openmp-report2 -openmp\ntest.c(4): (col. 1) remark: OpenMP DEFINED LOOP WAS PARALLELIZED\ntest.c(5): (col. 1) remark: OpenMP SIMD LOOP WAS VECTORIZED\n", "comments": ["Most of our in-op parallel currently comes through Eigen, which includes SIMD optimizations. We're not planning on using OpenMP right now, see also a discussion at #22. \n\nI'll close this issue for now. We may revisit it depending on how the OpenCL work goes.\n"]}, {"number": 1746, "title": "Upstream changes for april fools", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "So that this is an april fool's prank is an april fool's prank? :p\n", "android demo app seems to have had an infrastructure failure (groupadd problem).  Trying again.\n@tensorflow-jenkins: test this please\n\nSadly we did not prepare anything for April Fools this year :(.\n", "Looks persistent -- not sure what we can do -- any ideas @jendap or @caisq ?  This has happened once before, it would be good to root cause why the issue comes back.\n\n```\nRunning '/tmp/tf_build.sh' inside jenkins-tensorflow-pull-requests-android-549.android...\nAdding group `jenkins' (GID 120) ...\ngroupadd: failure while writing changes to /etc/group\naddgroup: `/usr/sbin/groupadd -g 120 jenkins' returned error code 10. Exiting.\n```\n", "@vrv @jendap On the android slave, the file systems inside docker container have become read-only. This is the reason why addgroup fails in with_the_same_user.sh. This is not specific to to the android tf containers. I'm looking into why docker locks the file system. \n", "@tensorflow-jenkins test this please\n", "@vrv @jendap OK, I re-created /var/lib/docker on the android slave and restarted the docker service. The problem seems to be gone for now. Why the file systems became read-only in docker containers is still unclear to me. If you have any thoughts or clues, please let me know. We'll keep an eye on the android builds.\n", "Using a merge commit here instead of squashing.  I wish we could not squash but just rebase.\n"]}, {"number": 1745, "title": "install fails on ubuntu ", "body": "install per recomendatoin from github fails on ubuntu \n\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\nDownloading/unpacking https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n\nsandy@sandy-laptop:~$ sudo apt-get install python-pip python-dev\n[sudo] password for sandy: \nReading package lists... Done\nBuilding dependency tree  \nReading state information... Done\npython-pip is already the newest version.\npython-dev is already the newest version.\n0 upgraded, 0 newly installed, 0 to remove and 268 not upgraded.\nsandy@sandy-laptop:~$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\nDownloading/unpacking https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n  Downloading tensorflow-0.7.1-cp27-none-linux_x86_64.whl (13.8Mb): 13.8Mb downloaded\n  Running setup.py egg_info for package from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n    Traceback (most recent call last):\n      File \"<string>\", line 14, in <module>\n    IOError: [Errno 2] No such file or directory: '/tmp/pip-8L3Eho-build/setup.py'\n    Complete output from command python setup.py egg_info:\n    Traceback (most recent call last):\n\n  File \"<string>\", line 14, in <module>\n\nIOError: [Errno 2] No such file or directory: '/tmp/pip-8L3Eho-build/setup.py'\n\n---\n\nCommand python setup.py egg_info failed with error code 1\nStoring complete log in /home/sandy/.pip/pip.log\nsandy@sandy-laptop:~$ \n\nGitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["Try https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#pip-installation-issues and leave a comment if that doesn't solve that issue.\n"]}, {"number": 1744, "title": "Update README.md", "body": "", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1743, "title": "Retrain Inception Image", "body": "I'm running Tensorflow on Ubuntu 14.04\nnewest version 0.7.1\n\nwhen I try to retrain by using this command\n\npython retrain.py --image_dir ~/flower\n\nAfter it extract bottleneck, it shows that errors\n\nTraceback (most recent call last):\n  File \"retrain.py\", line 824, in <module>\n    tf.app.run()\n  File \"/home/trungdn/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"retrain.py\", line 751, in main\n    bottleneck_tensor)\n  File \"retrain.py\", line 673, in add_final_training_ops\n    bottleneck_input = tf.placeholder_with_default(\n**AttributeError: 'module' object has no attribute 'placeholder_with_default'**\n\nName: tensorflow\nVersion: 0.7.1\nLocation: /home/trungdn/tensorflow/lib/python2.7/site-packages\nRequires: numpy, protobuf, wheel, six\n\nI tried to git pull with update repo but it doesn't help. Thanks\n", "comments": ["retrain.py at HEAD now uses code that is only available after 0.7.1.  Try installing the pip nightly or use retrain.py at the r0.7 branch.\n"]}, {"number": 1742, "title": "CTC: Add support for dense label matrix", "body": "I'm working on integrating the new CTC operation into Keras and the SparseTensor used to pass the labels is causing a lot challenges due internal Keras requirements about the shape of the data going into the layers. What would be make integration trivial is an alternative dense label input format:\n\n``` python\ndef ctc_loss_dense(inputs, labels, sequence_length, label_length,\n             preprocess_collapse_repeated=False, ctc_merge_repeated=True):\n  \"\"\"Computes the CTC (Connectionist Temporal Classification) Loss.\n  Requires:\n       max_label_length <= sequence_length\n  If ctc_merge_repeated is set False, then *during* CTC calculation\n  repeated non-blank labels will not be merged and are interpreted\n  as individual labels.  This is a simplified version of CTC.\n  Args:\n    inputs: 3-D `float` `Tensor` sized\n      `[max_time x batch_size x num_classes]`.  The logits.\n    labels: 2-D `int` `Tensor` sized\n       `[max_label_length x batch_size]\n    sequence_length: 1-D `int32` vector, size `[batch_size]`.\n      The sequence lengths.\n    label_length: 1-D `int32` vector, size `[batch_size]`.\n      The label lengths.\n    preprocess_collapse_repeated: Boolean.  Default: False.\n      If True, repeated labels are collapsed prior to the CTC calculation.\n    ctc_merge_repeated: Boolean.  Default: True.\n  Returns:\n    A 1-D `float` `Tensor`, size `[batch]`, containing logits.\n  Raises:\n    TypeError: if labels is not a `SparseTensor`.\n  \"\"\"\n```\n\nThis dense input format would also be consistent with Baidu's Warp-CTC and various Theano CTC implementations, so projects using cross-library platforms like Keras would be a lot cleaner.  Also, for common CTC uses like OCR and speech processing, the sequence lengths are often a lot smaller than the input lengths, so I don't think the increased bandwidth from not having a spare tensor would be too noticeable. \n", "comments": ["This is method is simple to write using the sparse_tensor_to_dense op.  Contributions are welcome.\n", "I was thinking it would need the reverse of that (Dense to sparse) which doesn't seem to exist.\n", "Sorry right. I added gather_nd which does what you need.\n", "Thanks for pointing me to gather_nd.  It did take a bit more effort to generate the index list from the label_length arrays, but I got it working.  Below is the code.\n\n``` python\ndef ctc_label_dense_to_sparse( labels, label_lengths ):\n    label_shape = tf.shape( labels )\n    num_batches_tns = tf.pack( [label_shape[0]] )\n    max_num_labels_tns = tf.pack( [label_shape[1]] )\n    def range_less_than(previous_state, current_input):\n        return tf.expand_dims( tf.range( label_shape[1] ), 0 ) < current_input\n\n    init = tf.cast( tf.fill( max_num_labels_tns, 0 ), tf.bool )\n    dense_mask = functional_ops.scan(range_less_than, label_lengths , initializer=init, \n         parallel_iterations=1)\n    dense_mask = dense_mask[ :, 0, : ]\n\n    label_array = tf.reshape( tf.tile( tf.range( 0, label_shape[1] ), num_batches_tns ),\n          label_shape )\n    label_ind = tf.boolean_mask( label_array, dense_mask )\n\n    batch_array = tf.transpose( tf.reshape( tf.tile( tf.range( 0,  label_shape[0] ), max_num_labels_tns ),\n          tf.reverse( label_shape,[True]) ) )\n    batch_ind = tf.boolean_mask( batch_array, dense_mask )\n\n    indices = tf.transpose( tf.reshape( tf.concat( 0, [batch_ind, label_ind] ), [2,-1] ) )\n    vals_sparse = tf.gather_nd( labels, indices )\n    return tf.SparseTensor( tf.to_int64(indices), vals_sparse, tf.to_int64( label_shape ) )\n```\n\nFor now I may put this in a Keras PR, unless you think it has a place in the TensorFlow API. \n", "@ebrevdo \n\nHi ebrevdo ,\nrecently I am trying to use ctc in a OCR project, but I found that the \"label\" parameter in ctc_loss function is sparse tensor, but it seams that I can't feed a sparse tensor to placeholder. Is there any way to work around it?\n", "Create 3 tensors (indices, values, shape), wrap them in a SparseTensor\nobject, and feed the 3 tensors together.\nOn May 3, 2016 11:37 PM, \"Yongliang Wang\" notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo\n> \n> Hi ebrevdo ,\n> recently I am trying to use ctc in a OCR project, but I found that the\n> \"label\" parameter in ctc_loss function is sparse tensor, but it seams that\n> I can't feed a sparse tensor to placeholder. Is there any way to work\n> around it?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1742#issuecomment-216758508\n", "Maybe a stupid question, but why use a SparseTensor at all since it is never sparse? You have a label for each timestep right?\n", "No, for CTC you have a different number of output time steps than input\ntime steps.  That's the point.\n\nOn Fri, May 13, 2016 at 7:13 AM, Erik Rehn notifications@github.com wrote:\n\n> Maybe a stupid question, but why use a SparseTensor at all since it is\n> never sparse? You have a label for each timestep right?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1742#issuecomment-219054493\n", "Yes of course, the target/output sequence is shorter than the input sequence. But for every \"target timestep\" you have a label, right?\nIm probably misunderstanding what `labels` represents.  In the docs it says:\n`labels.indices[i, :] == [b, t]` means `labels.values[i]` stores the id for (batch b, time t). \nIs not t here in the output time scale?\n", "Ah, now i get it, you want to use a SparseTensor since the target sequences might have different length?\n", "Yes - the target sequences almost always have different length, from one\nbatch entry to the next.\n\nOn Fri, May 13, 2016 at 9:38 AM, Erik Rehn notifications@github.com wrote:\n\n> Ah, now i get it, you want to use a SparseTensor since the target\n> sequences might have different length?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1742#issuecomment-219095155\n", "@ebrevdo @raindeer What's the status of this issue?\n", "Marked as resolved.  It's easy to create a SparseTensor if you have fixed length target sequences.\n", "PR https://github.com/tensorflow/tensorflow/pull/16119 addresses this issue."]}, {"number": 1741, "title": "Polygamma and zeta function for tensorflow", "body": "The polygamma function is necessary to evaluate the derivative of the digamma function which is already supported by tensorflow. As suggested by @ebrevdo in #291, I have ported the code for the two-parameter zeta function from cephes which allows us to evaluate the polygamma function. A pull request for eigen is [here](https://bitbucket.org/eigen/eigen/pull-requests/173/added-zeta-function-of-two-arguments-and). \n\nWhat is the best way to proceed integrating the new functionality into tensorflow? I'm happy to contribute the code with a bit of input regarding the right procedure. In particular, how do you update the eigen sources within the tensorflow repo?\n", "comments": ["Let's get that code through review on bitbucket, then we can update the references and I'll push my changes adding igamma/igammac to TensorFlow.  You'll be able to use those as reference for the TF implementations of polygamma/zeta.\n", "The zeta and polygamma functions are now in eigen. Happy to have a look at implementing them in tensorflow whenever you have time to update the references. \n", "Great!  I need a couple of days to import your changes on our end.  In the meantime look at my recent push of igamma/igammac as a reference for wrapping zeta/polygamma.\n", "(note the unit tests I added: they require scipy and cause a soft failure if it's not found.  this is fine - just make sure you have scipy installed when testing locally)\n", "That sounds good. I have implemented the wrappers locally. Just let me know when you've integrated the changes and I'll submit a PR.\n", "The Eigen implementations of the polygamma and zeta functions are now available in TensorFlow\n", "@benoitsteiner is the build containing these functions already available ? thanks \n", "@akashgit, the eigen code in TensorFlow supports the functions but they won't be available from python until the PR above is merged. You can have a look there if you need them urgently.\n"]}, {"number": 1740, "title": "Tensorflow Adam Multigpu Gradient", "body": "I am trying to implement a network multiple gpu on tensorflow using ADAM Optimization.\n\nI am coping the code from the Cifar10_multigpu, but it looks that when the gradient calls the second tower it calls the gradient of the first and generated error on the average of the two towers. the code of the two tower is this\n\n```\n for d in devs:\n        with tf.device(d):\n            with tf.name_scope('%s_%d' % (tf_model.TOWER_NAME, i)) as scope:\n                loss = tower_loss(scope)\n                tf.get_variable_scope().reuse_variables()\n                summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n                grads = opt.compute_gradients(loss)\n                print('\\n'.join('{}: {}'.format(*k) for k in enumerate(grads)))\n                tower_grads.append(grads)\n        i +=1\n```\n\nand this generates each tower:\n\n```\nstream, target= placeholder_inputs(FLAGS.batch_size*tf_model.ANGLES/FLAGS.num_gpus)\n    logits = tf_model.inference_noisy_simulate(stream)\n    _ = tf_model.loss(logits, target)\n    losses = tf.get_collection('losses', scope)\n    total_loss = tf.add_n(losses, name='total_loss')\n```\n\nlooking at the gradients the first tower generate this:\n\n```\n0: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca11d0ae10>)\n1: (<tf.Tensor 'tower_0/gradients/tower_0/conv1/Conv2D_grad/tuple/control_dependency_1:0' shape=(1, 1, 8, 16) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c351b10>)\n2: (<tf.Tensor 'tower_0/gradients/tower_0/conv1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(16,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c380dd0>)\n3: (<tf.Tensor 'tower_0/gradients/tower_0/conv2/Conv2D_grad/tuple/control_dependency_1:0' shape=(45, 4, 16, 16) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c351a10>)\n4: (<tf.Tensor 'tower_0/gradients/tower_0/conv2/BiasAdd_grad/tuple/control_dependency_1:0' shape=(16,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c3a6dd0>)\n5: (<tf.Tensor 'tower_0/gradients/tower_0/conv3/Conv2D_grad/tuple/control_dependency_1:0' shape=(45, 4, 16, 32) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c3a6490>)\n6: (<tf.Tensor 'tower_0/gradients/tower_0/conv3/BiasAdd_grad/tuple/control_dependency_1:0' shape=(32,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c351990>)\n7: (<tf.Tensor 'tower_0/gradients/tower_0/conv4/Conv2D_grad/tuple/control_dependency_1:0' shape=(45, 4, 32, 64) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c351890>)\n8: (<tf.Tensor 'tower_0/gradients/tower_0/conv4/BiasAdd_grad/tuple/control_dependency_1:0' shape=(64,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c3b7790>)\n9: (<tf.Tensor 'tower_0/gradients/tower_0/conv5/Conv2D_grad/tuple/control_dependency_1:0' shape=(45, 4, 64, 128) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c2d9110>)\n10: (<tf.Tensor 'tower_0/gradients/tower_0/conv5/BiasAdd_grad/tuple/control_dependency_1:0' shape=(128,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c2849d0>)\n11: (<tf.Tensor 'tower_0/gradients/tower_0/conv6/Conv2D_grad/tuple/control_dependency_1:0' shape=(45, 4, 128, 256) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c2e6f10>)\n12: (<tf.Tensor 'tower_0/gradients/tower_0/conv6/BiasAdd_grad/tuple/control_dependency_1:0' shape=(256,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c2afed0>)\n13: (<tf.Tensor 'tower_0/gradients/tower_0/fc1/MatMul_grad/tuple/control_dependency_1:0' shape=(18944, 4096) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c1f9550>)\n14: (<tf.Tensor 'tower_0/gradients/tower_0/fc1/add_grad/tuple/control_dependency_1:0' shape=(4096,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c214a10>)\n15: (<tf.Tensor 'tower_0/gradients/tower_0/fc1_1/MatMul_grad/tuple/control_dependency_1:0' shape=(4096, 1024) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c23dfd0>)\n16: (<tf.Tensor 'tower_0/gradients/tower_0/fc1_1/add_grad/tuple/control_dependency_1:0' shape=(1024,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c269bd0>)\n17: (<tf.Tensor 'tower_0/gradients/tower_0/softmax_linear/MatMul_grad/tuple/control_dependency_1:0' shape=(1024, 360) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c1d1a50>)\n18: (<tf.Tensor 'tower_0/gradients/tower_0/softmax_linear/softmax_linear_grad/tuple/control_dependency_1:0' shape=(360,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c1def50>)\n```\n\nand the second generates this;\n\n```\n0: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca11d0ae10>)\n1: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c351b10>)\n2: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c380dd0>)\n3: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c351a10>)\n4: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c3a6dd0>)\n5: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c3a6490>)\n6: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c351990>)\n7: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c351890>)\n8: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c3b7790>)\n9: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c2d9110>)\n10: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c2849d0>)\n11: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c2e6f10>)\n12: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c2afed0>)\n13: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c1f9550>)\n14: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c214a10>)\n15: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c23dfd0>)\n16: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c269bd0>)\n17: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c1d1a50>)\n18: (None, <tensorflow.python.ops.variables.Variable object at 0x7fca0c1def50>)\n19: (<tf.Tensor 'tower_1/gradients/tower_1/conv1/Conv2D_grad/tuple/control_dependency_1:0' shape=(1, 1, 8, 16) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0c178c50>)\n20: (<tf.Tensor 'tower_1/gradients/tower_1/conv1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(16,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0bfbb490>)\n21: (<tf.Tensor 'tower_1/gradients/tower_1/conv2/Conv2D_grad/tuple/control_dependency_1:0' shape=(45, 4, 16, 16) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0bfda950>)\n22: (<tf.Tensor 'tower_1/gradients/tower_1/conv2/BiasAdd_grad/tuple/control_dependency_1:0' shape=(16,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0bf91bd0>)\n23: (<tf.Tensor 'tower_1/gradients/tower_1/conv3/Conv2D_grad/tuple/control_dependency_1:0' shape=(45, 4, 16, 32) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0bfcb590>)\n24: (<tf.Tensor 'tower_1/gradients/tower_1/conv3/BiasAdd_grad/tuple/control_dependency_1:0' shape=(32,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0bf39e90>)\n25: (<tf.Tensor 'tower_1/gradients/tower_1/conv4/Conv2D_grad/tuple/control_dependency_1:0' shape=(45, 4, 32, 64) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0bf499d0>)\n26: (<tf.Tensor 'tower_1/gradients/tower_1/conv4/BiasAdd_grad/tuple/control_dependency_1:0' shape=(64,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0bf14fd0>)\n27: (<tf.Tensor 'tower_1/gradients/tower_1/conv5/Conv2D_grad/tuple/control_dependency_1:0' shape=(45, 4, 64, 128) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0bf39150>)\n28: (<tf.Tensor 'tower_1/gradients/tower_1/conv5/BiasAdd_grad/tuple/control_dependency_1:0' shape=(128,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0bebd8d0>)\n29: (<tf.Tensor 'tower_1/gradients/tower_1/conv6/Conv2D_grad/tuple/control_dependency_1:0' shape=(45, 4, 128, 256) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0bf23110>)\n30: (<tf.Tensor 'tower_1/gradients/tower_1/conv6/BiasAdd_grad/tuple/control_dependency_1:0' shape=(256,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0bf04610>)\n31: (<tf.Tensor 'tower_1/gradients/tower_1/fc1/MatMul_grad/tuple/control_dependency_1:0' shape=(18944, 4096) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0bebdc50>)\n32: (<tf.Tensor 'tower_1/gradients/tower_1/fc1/add_grad/tuple/control_dependency_1:0' shape=(4096,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0bebd310>)\n33: (<tf.Tensor 'tower_1/gradients/tower_1/fc1_1/MatMul_grad/tuple/control_dependency_1:0' shape=(4096, 1024) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0be96e10>)\n34: (<tf.Tensor 'tower_1/gradients/tower_1/fc1_1/add_grad/tuple/control_dependency_1:0' shape=(1024,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0be96990>)\n35: (<tf.Tensor 'tower_1/gradients/tower_1/softmax_linear/MatMul_grad/tuple/control_dependency_1:0' shape=(1024, 360) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0be52c90>)\n36: (<tf.Tensor 'tower_1/gradients/tower_1/softmax_linear/softmax_linear_grad/tuple/control_dependency_1:0' shape=(360,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7fca0bf56f50>)\n```\n\nI am wondering how to remove from the second the first None, but no targeting indexes so i can make for more towers.\n", "comments": ["I already find the error. I was using a trainable variable for the learning rate(I wanted to trak the lr but it does not look possible), and also add the list of variables to compute by the op at adam. I am not sure if this is a correct way but it looks like it works.\n\n```\n\n`   with tf.Graph().as_default(), tf.device('/cpu:0'):\n        devs = ['/job:prs/task:0/gpu:0','/job:worker/task:0/gpu:0'] #\n        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n        num_batches_per_epoch = dt_fdr.FLS_PER_ANGLE/ FLAGS.batch_size\n        #lr = tf.Variable(tf.constant(FLAGS.learning_rate, dtype=tf.float32))\n        opt = tf.train.AdamOptimizer(FLAGS.learning_rate)\n        tower_grads = []\n        for i in xrange(FLAGS.num_gpus):\n            with tf.device(devs[i]):\n                with tf.name_scope('%s_%d' % (tf_model.TOWER_NAME, i)) as scope:\n                    loss = tower_loss(scope)\n                    tf.get_variable_scope().reuse_variables()\n                    summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n                    #\"print('\\n'.join('{}: {}'.format(*k) for k in enumerate(summaries)))\n                    grads = opt.compute_gradients(loss, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope))\n                    #print('\\n'.join('{}: {}'.format(*k) for k in enumerate(grads)))\n                    tower_grads.append(grads)\n        grads = average_gradients(tower_grads)\n        #summaries.append(tf.scalar_summary('learning_rate', lr))\n        for grad, var in grads:\n            if grad:\n                summaries.append(\n                    tf.histogram_summary(var.op.name + '/gradients', grad))\n        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step) \n        for var in tf.trainable_variables():\n            summaries.append(tf.histogram_summary(var.op.name, var))\n\n        train_op = apply_gradient_op\n\n        saver = tf.train.Saver(tf.all_variables())\n\n        summary_op = tf.merge_summary(summaries)\n\n        init = tf.initialize_all_variables()\n\n        sess = tf.Session(\"grpc://nelson-lab:2500\",config=tf.ConfigProto(\n            allow_soft_placement=True,\n            log_device_placement=FLAGS.log_device_placement))\n        sess.run(init)`\n\n\n```\n\nI wonder if someone also tried to do some double gpu training using adam.\n\nRegards\n", "This is probably a question better suited to StackOverflow, since there doesn't appear to be a bug.\n"]}, {"number": 1739, "title": "Fix links to nightly gpu build for python 3", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n", "Merged. Thanks.\n"]}, {"number": 1738, "title": "Fix Linux CPU Tests CMAKE", "body": "Changes:\n- target_compile_features require CMake 3.1.0\n- add install_proto3.sh for protobuf3, which required by the CMake\n- add ${CMAKE_DL_LIBS} for -ldl\n- \"make all test\" to \"make all\" since there's no add_test() in tensorflow/contrib/cmake/\n\nRef:\nhttps://cmake.org/cmake/help/v3.1/release/3.1.0.html\n\nRelated PR: #1721 \n", "comments": ["Can one of the admins verify this patch?\n", "Alternative of protobuf3 installation is setup --recurse-submodules in Jenkins to clone/update google/protobuf. However that can not be done here.\n", "@tensorflow-jenkins test this please.\n", "There is still a failure in the cmake PR build: http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-cmake/9/console\n", "I've merged #1721 in this PR.\n", "@tensorflow-jenkins test this please\n", "The renaming \"tensorflow/core/ops/compat/update_ops.cc \u2192 ...orflow/core/ops/compat/update_ops_main.cc\" has broken CPU builds. \n", "Just fixed it. Can you please give a try? I may also need to setup another jenkins task for it. :-/\n", "Will cmake work for gpu builds? If so, the install_proto3.sh needs to be added to tensorflow/tools/ci_build/Dockerfile.gpu as well.\n", "@tensorflow-jenkins test this please\n", "I'll need to grab a AWS AMI to setup Jenkins with GPU support to test it. How about separate gpu-specifics to another PR?\n", "@clsung Let me help you test it later. We have GPU machines available. How much change do you expect to be needed? If the test passes, we'll just merge it right away. BTW, the commits need to be squashed before the merge. \n", "Since nvidia/cuda:7.5-cudnn4-devel is from ubuntu:14.04, we should only need to copy one line from Dockerfile.cpu to Dockerfile.gpu:\n\n`RUN /install/install_proto3.sh`\n\nWhere we already install autoconf, automake and cmake 3.x in install_deb_packages.sh.\n", "@clsung if cmake 3.0 would be enough then please put it into your next PR.\n\n@vrv I will make PR to move the ppa repositories.\n\nBoth... 48 lines in 17 commits? Should we squash it next time? :-)\n", "https://github.com/tensorflow/tensorflow/commit/2c2f422a628d83a591fb5d20d46deecc47655ead it was squashed.\n", "Right! That's why I had to look for the PR and could not just click through :-)\n\nGithub should really make squash+rebase a feature! (even a default option)\n", "It is a feature -- it's what we used here.  I guess it doesn't modify the original pull request, that's all.\n"]}, {"number": 1737, "title": "android:tensorflow_demo issue..", "body": "`poporo@poporo-All-Series://tensorflow/tensorflow/examples/android$ bazel build //tensorflow/examples/android:tensorflow_demo\n\nWARNING: /tensorflow/tensorflow/core/BUILD:638:9: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\n.\n.\n.\nINFO: Found 1 target...\nWARNING: failed to create one or more convenience symlinks for prefix 'bazel-':\n  cannot create symbolic link bazel-out -> /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out:  /tensorflow/bazel-out (Permission denied)\n  cannot create symbolic link bazel-tensorflow -> /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow:  /tensorflow/bazel-tensorflow (Permission denied)\n  cannot create symbolic link bazel-bin -> /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-opt/bin:  /tensorflow/bazel-bin (Permission denied)\n  cannot create symbolic link bazel-testlogs -> /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-opt/testlogs:  /tensorflow/bazel-testlogs (Permission denied)\n  cannot create symbolic link bazel-genfiles -> /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/local_linux-opt/genfiles:  /tensorflow/bazel-genfiles (Permission denied).\nERROR: /tensorflow/tensorflow/examples/android/BUILD:63:1: Processing resources failed: resources_processor failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/android/resources_processor --buildToolsVersion 23.0.1 --aapt bazel-out/host/bin/external/androidsdk/aapt_binary --annotationJar ... (remaining 13 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nError: bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\n.\n.\n.\n.\nApr 01, 2016 10:48:05 AM com.google.devtools.build.android.AndroidResourceProcessingAction main\nSEVERE: Error during merging resources\nError: Failed to run command:\n    bazel-out/host/bin/external/androidsdk/aapt_binary s -i /tmp/android_resources_tmp1623410823562764165/tmp-deduplicated/tensorflow/examples/android/res/drawable-hdpi/ic_launcher.png -o /tmp/android_resources_tmp1623410823562764165/merged_resources/drawable-hdpi-v4/ic_launcher.png\nError Code:\n    1\nOutput:\n    bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\n\n```\nat com.android.ide.common.res2.MergeWriter.end(MergeWriter.java:54)\nat com.android.ide.common.res2.MergedResourceWriter.end(MergedResourceWriter.java:113)\nat com.android.ide.common.res2.DataMerger.mergeData(DataMerger.java:291)\nat com.android.ide.common.res2.ResourceMerger.mergeData(ResourceMerger.java:48)\nat com.google.devtools.build.android.AndroidResourceProcessor.mergeData(AndroidResourceProcessor.java:390)\nat com.google.devtools.build.android.AndroidResourceProcessingAction.main(AndroidResourceProcessingAction.java:321)\n```\n\nCaused by: com.android.ide.common.internal.LoggedErrorException: Failed to run command:\n    bazel-out/host/bin/external/androidsdk/aapt_binary s -i /tmp/android_resources_tmp1623410823562764165/tmp-deduplicated/tensorflow/examples/android/res/drawable-hdpi/ic_launcher.png -o /tmp/android_resources_tmp1623410823562764165/merged_resources/drawable-hdpi-v4/ic_launcher.png\nError Code:\n    1\nOutput:\n    bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /home/poporo/.cache/bazel/_bazel_poporo/68a62076e91007a7908bc42a32e4cff9/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\n\n```\nat com.android.ide.common.internal.CommandLineRunner.runCmdLine(CommandLineRunner.java:123)\nat com.android.ide.common.internal.CommandLineRunner.runCmdLine(CommandLineRunner.java:96)\nat com.android.ide.common.internal.AaptCruncher.crunchPng(AaptCruncher.java:58)\nat com.android.ide.common.res2.MergedResourceWriter$1.call(MergedResourceWriter.java:188)\nat com.android.ide.common.res2.MergedResourceWriter$1.call(MergedResourceWriter.java:139)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\n```\n\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 7.359s, Critical Path: 4.34s\n`\n\nhelp me..\n", "comments": ["You may need to run sdk_path/tools/android and ensure that you have the 23.0.1 Android build tools installed. Alternatively, if you already have a version >= 21, you can try changing the setting in WORKSPACE and point to that.\n", "popo@popo-All-Series:~/android-sdk-linux/build-tools/23.0.1$ ls\nNOTICE.txt  aidl                      bcc_compat  dx                     jack.jar  lib         mainDexClasses        mipsel-linux-android-ld  runtime.properties  split-select\naapt        arm-linux-androideabi-ld  dexdump     i686-linux-android-ld  jill.jar  llvm-rs-cc  mainDexClasses.rules  renderscript             source.properties   zipalign\n\n---\n\nWORKSPACE\n\nandroid_sdk_repository(\n    name = \"androidsdk\",\n    api_level = 23,\n    build_tools_version = \"23.0.1\",\n    # Replace with path to Android SDK on your system\n    path = \"/home/poporo/android-sdk-linux\",\n)\n\nandroid_ndk_repository(\n    name=\"androidndk\",\n    path=\"/home/poporo/android-ndk-r10e\",\n    api_level=21)\n\nI was already doing. help me..\n", "Is your username popo or poporo? I notice it changes in the two printouts you've provided.\n", "@andrewharp\n I'm also having the same issue. but I'm retraining in Google compute engine on Ubuntu 14.04\nfor this command  `bazel build -c opt --copt=-mavx tensorflow/examples/image_retraining:retrain`\n\n```\n cannot create symbolic link bazel-genfiles -> /home/developersriharsha/.cache/bazel/_bazel_developersriharsha/622942cbf\n9b89fb76bd82d9602a4d614/tensorflow/bazel-out/local_linux-opt/genfiles:  /home/developersriharsha/tensorflow/tensorflow/ba\nzel-genfiles (Permission denied).\n```\n", "If you can't create a bazel-genfiles link I'd suspect it's a basic Bazel installation problem. You could try doublechecking the permissions on the relevant directories and the users you installed Bazel as/are building as.\n\n@damienmg Does this seem familiar to you? thanks\n", "it says it cannot write the convenience symlink. What does `ls -l /home/developersriharsha/tensorflow/tensorflow` says?\n", "I'm also having the same issue.So I try to put \"ulimit -SHn 65535\" into /etc/profile and run by root.But:\n\nINFO: Found 1 target...\nINFO: From Executing genrule @androidsdk//:zipalign_runner [for host]:\nsrc/main/tools/namespace-sandbox.c:460: mount(opt->sandbox_root, opt->sandbox_root, NULL, MS_BIND | MS_NOSUID, NULL): Permission denied\nERROR: /home/eli/.cache/bazel/_bazel_root/e86f6fce5559de9e3e13fb6adb66b858/external/androidsdk/BUILD:144:2: Executing genrule @androidsdk//:zipalign_runner failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped).\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 3.985s, Critical Path: 1.41s\n", "This is a different issue which should not happens in tensorflow. Did you run ./configure?\n", "@damienmg \nThese are the files\n\n```\ndevelopersriharsha@instancenew:~/tensorflow/tensorflow$ ls\nACKNOWLEDGMENTS  configure        google             LICENSE    README.md   tensorflow   util\nAUTHORS          CONTRIBUTING.md  ISSUE_TEMPLATE.md  navbar.md  RELEASE.md  third_party  WORKSPACE\nbower.BUILD      eigen.BUILD      jpeg.BUILD         png.BUILD  six.BUILD   tools\n```\n\ndevelopersriharsha@instancenew:~/tensorflow/tensorflow$ \n", "I would like to get the output of `ls -l /home/developersriharsha/tensorflow/tensorflow` precisely (with -l) to see the permisssions\n", "@damienmg \n\n```\ndevelopersriharsha@instancenew:~/tensorflow/tensorflow$ ls -l\ntotal 120\n-rw-r--r--  1 root root  2219 Apr  1 07:29 ACKNOWLEDGMENTS\n-rw-r--r--  1 root root   313 Apr  1 07:29 AUTHORS\n-rw-r--r--  1 root root 11318 Apr  1 07:29 bower.BUILD\n-rwxr-xr-x  1 root root  9040 Apr  1 07:29 configure\n-rw-r--r--  1 root root  1749 Apr  1 07:29 CONTRIBUTING.md\n-rw-r--r--  1 root root   313 Apr  1 07:29 eigen.BUILD\ndrwxr-xr-x  3 root root  4096 Apr  1 07:29 google\n-rw-r--r--  1 root root   940 Apr  1 07:29 ISSUE_TEMPLATE.md\n-rw-r--r--  1 root root  1646 Apr  1 07:29 jpeg.BUILD\n-rw-r--r--  1 root root 11416 Apr  1 07:29 LICENSE\n-rw-r--r--  1 root root   396 Apr  1 07:29 navbar.md\n-rw-r--r--  1 root root   999 Apr  1 07:29 png.BUILD\n-rw-r--r--  1 root root  6322 Apr  1 07:29 README.md\n-rw-r--r--  1 root root  6960 Apr  1 07:29 RELEASE.md\n-rw-r--r--  1 root root   243 Apr  1 07:29 six.BUILD\ndrwxr-xr-x 13 root root  4096 Apr  1 07:29 tensorflow\ndrwxr-xr-x  5 root root  4096 Apr  1 07:29 third_party\ndrwxr-xr-x  2 root root  4096 Apr  1 07:34 tools\ndrwxr-xr-x  3 root root  4096 Apr  1 07:29 util\n-rw-r--r--  1 root root  9631 Apr  1 07:29 WORKSPACE\n```\n", "Here you are, only root can write here. \"sudo chown -R developersriharsha /home/developersriharsha/tensorflow/tensorflow\" should fix your issue \n", "@damienmg Thanks ,It resolved \n", "I have run ./configure and \"sudo chown -R eli /home/eli/tensorflow\", but nothing is changed.\n", "@qemb1 What target are you trying to build, and what version of Bazel are you using? Can you rerun with --verbose_failures, and then check that there are no ownership/permission/existence issues with the Android SDK binary in question?\n", "version of Bazel :bazel-0.1.4\n\neli@eli-VirtualBox:~/tensorflow$ bazel build //tensorflow/examples/android:tensorflow_demo\n\nINFO: Found 1 target...\n\nINFO: Building...\n\n[0 / 1] BazelWorkspaceStatusAction stable-status.txt\n\n[1 / 100] Linking google/protobuf/protoc [for host]\n\nINFO: From Processing resources:\n\nError: /home/eli/.cache/bazel/_bazel_eli/9d5da1ed772a83016cc571955005289d/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /home/eli/.cache/bazel/_bazel_eli/9d5da1ed772a83016cc571955005289d/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\nError: /home/eli/.cache/bazel/_bazel_eli/9d5da1ed772a83016cc571955005289d/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /home/eli/.cache/bazel/_bazel_eli/9d5da1ed772a83016cc571955005289d/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\nApr 07, 2016 10:47:53 AM com.google.devtools.build.android.AndroidResourceProcessingAction main\nSEVERE: Error during merging resources\nError: Failed to run command:\n/home/eli/.cache/bazel/_bazel_eli/9d5da1ed772a83016cc571955005289d/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary s -i /tmp/tmp-deduplicated941045509851047210/tensorflow/examples/android/res/drawable-mdpi/ic_launcher.png -o /home/eli/.cache/bazel/_bazel_eli/9d5da1ed772a83016cc571955005289d/tensorflow/merged_resources/drawable-mdpi-v4/ic_launcher.png\nError Code:\n1\nOutput:\n/home/eli/.cache/bazel/_bazel_eli/9d5da1ed772a83016cc571955005289d/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /home/eli/.cache/bazel/_bazel_eli/9d5da1ed772a83016cc571955005289d/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\n\nat com.android.ide.common.res2.MergeWriter.end(MergeWriter.java:54)\nat com.android.ide.common.res2.MergedResourceWriter.end(MergedResourceWriter.java:113)\nat com.android.ide.common.res2.DataMerger.mergeData(DataMerger.java:291)\nat com.android.ide.common.res2.ResourceMerger.mergeData(ResourceMerger.java:48)\nat com.google.devtools.build.android.AndroidResourceProcessor.mergeData(AndroidResourceProcessor.java:239)\nat com.google.devtools.build.android.AndroidResourceProcessingAction.main(AndroidResourceProcessingAction.java:322)\nCaused by: com.android.ide.common.internal.LoggedErrorException: Failed to run command:\n/home/eli/.cache/bazel/_bazel_eli/9d5da1ed772a83016cc571955005289d/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary s -i /tmp/tmp-deduplicated941045509851047210/tensorflow/examples/android/res/drawable-mdpi/ic_launcher.png -o /home/eli/.cache/bazel/_bazel_eli/9d5da1ed772a83016cc571955005289d/tensorflow/merged_resources/drawable-mdpi-v4/ic_launcher.png\nError Code:\n1\nOutput:\n/home/eli/.cache/bazel/_bazel_eli/9d5da1ed772a83016cc571955005289d/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /home/eli/.cache/bazel/_bazel_eli/9d5da1ed772a83016cc571955005289d/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\n\nat com.android.ide.common.internal.CommandLineRunner.runCmdLine(CommandLineRunner.java:123)\nat com.android.ide.common.internal.CommandLineRunner.runCmdLine(CommandLineRunner.java:96)\nat com.android.ide.common.internal.AaptCruncher.crunchPng(AaptCruncher.java:58)\nat com.android.ide.common.res2.MergedResourceWriter$1.call(MergedResourceWriter.java:188)\nat com.android.ide.common.res2.MergedResourceWriter$1.call(MergedResourceWriter.java:139)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\n\nERROR: /home/eli/tensorflow/tensorflow/examples/android/BUILD:65:1: Processing resources failed: resources_processor failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/android/resources_processor --aapt bazel-out/host/bin/external/androidsdk/aapt_binary --annotationJar external/androidsdk/tools/support/annotations.jar ... (remaining 13 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n\nINFO: Building complete.\n\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 10.953s, Critical Path: 6.29s\n", "@qemb01 Do you have the sdk build tools 23.0.1 installed? If not, you'll need to update your WORKSPACE file to point to a version you do have. If it is already installed, check your SDK directory for ownership conflicts.\n", "@andrewharp It is useful.Thank you for the help!\n", "I have the same problem with bazel build either.", "@brucelau-github What is the exact error you're seeing? Host OS/bazel version? Have you tried the suggestions from this thread without success?"]}, {"number": 1736, "title": "Batch normalization for RNNs", "body": "I realize that this is hot off the press, but since batch normalization for feed forward layers is being added, any chance we can see it for RNNs? The performance improvements seem substantial.\n\nhttp://arxiv.org/abs/1603.09025\n", "comments": ["@alquraishi, that definitely seems like would be a great addition to the existing RNN library in TF. Marking as 'contributions welcome', feel free to send a PR.\n", "+1\n", "FYI there's a torch implementation here: https://github.com/iassael/torch-bnlstm\n", "Yes I agree -- I'm trying to work on an implementation of it right now. If I get it to work, I'll submit a PR but the problem is that they are going to update LSTM soon so I don't know if it will conflict:\n\nhttps://github.com/tensorflow/tensorflow/pull/2002\n", "there is an implementation here of batch normalized lstm: https://github.com/OlavHN/bnlstm/blob/master/lstm.py\nexplained in the article http://olavnymoen.com/2016/07/07/rnn-batch-normalization\ninspired from the paper https://arxiv.org/abs/1603.09025\n", " +1\n", "That's something I would be really interested in! :+1: ", "I believe one of the important points made in @cooijmanstim's paper is that you're supposed to collect mean/variances __per timestep__ during training (to let the LSTM state stabilize properly at the start of a new sequence), and then in test time you step through them (and if a test sequence is longer, you just keep with the last mean/var for the remainder of the sequence). @OlavHN's implementation doesn't do this, last I checked.\r\n\r\nI have a version that includes this in (but for ConvLSTMs, however it would be easy to copy+paste into FC-LSTMs): https://github.com/carlthome/tensorflow-convlstm-cell/blob/a647de6b86eff4c4bb502205d0ff9c1262869f19/ConvLSTMCell.py", "Good day,\r\n\r\nIs there any progress on this?\r\n\r\nAs pointed out by carlthome the implementation of @OlavHN does not compute distinct population statistics per timestep. To my understanding, @carlthome's implementation neither. To compute the timestep he uses:\r\n```python\r\nnew_sequences = tf.placeholder(tf.bool)\r\n...\r\nincrement_op = tf.cond(self._new_sequences,\r\n                               lambda: tf.assign(timestep, 0),\r\n                               lambda: tf.add(timestep, 1))\r\n```\r\nThis basically means that `new_sequences` is constant per run and `timestep` will either be 0 or grow indefinitely.\r\n\r\nI've tried to implement it myself but getting a variable to reflect the RNN-timestep in the `call()`  is tricky. Does anyone have idea on how to overcome this issue? \r\nIs there maybe a tensor from the outside one could access by name? Or to re-initialize a variable for every run? Or to modify the value of a placeholder from within the graph?\r\nI'd be happy to take it from there...\r\n\r\nThanks in advance! :-)", "If it helps, Sonnet has a TF implementation of a batch-normalized LSTM here: https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py", "I will give it a try and make a pull request by Monday", "I have made the pull request\r\n#14106\r\n\r\nIt follows the quoted paper, even in the initializer for the scaling variable, except that the statistics are taken \"globally\" rather than for each time step as recommended. The reason is that it uses tensorflow.python.layers.normalization.BatchNormalization to perform the actual normalization, and it would require that the variable creation/reuse be controlled from outside, and it seemed a strong reworking of it, so I decided to leave the pull request without any modification of BatchNormalization. If desired it could be implemented as a second step.", "Closing as the pull request is closed."]}, {"number": 1735, "title": "Upstream changes from internal for March 31", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 1734, "title": "Random functions always return 0 when using CUDA 7.5 ", "body": "### Environment info\n\nOperating System:\nUbuntu 14.04.3 LTS\n\nIf installed from sources, provide the commit hash: 48e6165fc864d0a222571507613c63c77ec6bf61\n### Steps to reproduce\n1. Compile from source and during `./configure` process specify CUDA 7.5\n2. Run the following script\n\n```\nimport tensorflow as tf\nv = tf.Variable(tf.truncated_normal([3,2], mean = 12.0, stddev = 0.1))\nsess = tf.InteractiveSession()\nsess.run(tf.initialize_all_variables())\nsess.run(v)\n```\n1. The result is 3x2 array of zeros rather than random value centered at 12.0. The same happens for all random variables\n### To solve it\n\nUse CUDA 7.0 solved the problem for me.\n", "comments": ["I cannot reproduce the problem at commit 3c8780451007c27b69f10925d43d1f3501d94106, Cuda 7.5. \n\nFor both CPU and GPU, this is what I got\n\n[[ 11.89173698  11.91889286]\n [ 11.91601562  11.93181419]\n [ 11.83213997  12.00035954]]\n", "Closing this issue for now. We will reopen it if more people hit into the same issue. \n"]}, {"number": 1733, "title": "rnn.dynamic_rnn() causes gradients graph building error", "body": "I found rnn.dynamic_rnn() that seems to do what I want, but when I modified the following lines \nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L114-L116 into \n\n```\nfrom tensorflow.models.rnn import rnn\noutputs, state = rnn.dynamic_rnn(cell, inputs, initial_state=self._initial_state)\noutput = tf.reshape(outputs, [-1, size])\n```\n\nIt throws out error at the line containing `tf.gradients(cost, tvars)`\n\n```\n  File \"/data/lisatmp3/yaoli/anaconda/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"ptb_word_lm.py\", line 304, in main\n    m = PTBModel(is_training=True, config=config)\n  File \"ptb_word_lm.py\", line 148, in __init__\n    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n  File \"/data/lisatmp3/yaoli/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 477, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/data/lisatmp3/yaoli/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_grad.py\", line 137, in _TensorArrayPackGrad\n    grad_source = _GetGradSource(grad)\n  File \"/data/lisatmp3/yaoli/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_grad.py\", line 62, in _GetGradSource\n    % op_or_tensor.name)\nValueError: Expected op/tensor name to start with gradients, got: model/gradients/model/RNN/transpose_grad/transpose:0\nUncaught exception. Entering post mortem debugging\nRunning 'cont' or 'step' will restart the program\n```\n\nI couldn't find any examples in the repo of using dynamic_rnn(), and hope someone could point out where it went wrong. The complete file is attached below (adapted based on `tensorflow/tensorflow/models/rnn/ptb/ptb_word_lm.py`). LOOP_VERSION=0 and 1 run all right, 2 raises the issue.  \n\n```\n# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Example / benchmark for building a PTB LSTM model.\n\nTrains the model described in:\n(Zaremba, et. al.) Recurrent Neural Network Regularization\nhttp://arxiv.org/abs/1409.2329\n\nThere are 3 supported model configurations:\n===========================================\n| config | epochs | train | valid  | test\n===========================================\n| small  | 13     | 37.99 | 121.39 | 115.91\n| medium | 39     | 48.45 |  86.16 |  82.07\n| large  | 55     | 37.87 |  82.62 |  78.29\nThe exact results may vary depending on the random initialization.\n\nThe hyperparameters used in the model:\n- init_scale - the initial scale of the weights\n- learning_rate - the initial value of the learning rate\n- max_grad_norm - the maximum permissible norm of the gradient\n- num_layers - the number of LSTM layers\n- num_steps - the number of unrolled steps of LSTM\n- hidden_size - the number of LSTM units\n- max_epoch - the number of epochs trained with the initial learning rate\n- max_max_epoch - the total number of epochs for training\n- keep_prob - the probability of keeping weights in the dropout layer\n- lr_decay - the decay of the learning rate for each epoch after \"max_epoch\"\n- batch_size - the batch size\n\nThe data required for this example is in the data/ dir of the\nPTB dataset from Tomas Mikolov's webpage:\n\n$ wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n$ tar xvf simple-examples.tgz\n\nTo run:\n\n$ python ptb_word_lm.py --data_path=simple-examples/data/\n\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.models.rnn.ptb import reader\n\nflags = tf.flags\nlogging = tf.logging\n\nflags.DEFINE_string(\n    \"model\", \"small\",\n    \"A type of model. Possible options are: small, medium, large.\")\nflags.DEFINE_string(\"data_path\", None, \"data_path\")\n\nFLAGS = flags.FLAGS\n\nLOOP_VERSION = 2\n\nclass PTBModel(object):\n  \"\"\"The PTB model.\"\"\"\n\n  def __init__(self, is_training, config):\n    self.batch_size = batch_size = config.batch_size\n    self.num_steps = num_steps = config.num_steps\n    size = config.hidden_size\n    vocab_size = config.vocab_size\n\n    self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n    self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n\n    # Slightly better results can be obtained with forget gate biases\n    # initialized to 1 but the hyperparameters of the model would need to be\n    # different than reported in the paper.\n    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(size, forget_bias=0.0)\n    if is_training and config.keep_prob < 1:\n      lstm_cell = tf.nn.rnn_cell.DropoutWrapper(\n          lstm_cell, output_keep_prob=config.keep_prob)\n    cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers)\n    self._initial_state = cell.zero_state(batch_size, tf.float32)\n\n    with tf.device(\"/cpu:0\"):\n      embedding = tf.get_variable(\"embedding\", [vocab_size, size])\n      inputs = tf.nn.embedding_lookup(embedding, self._input_data) # (b, t, 200)\n\n    if is_training and config.keep_prob < 1:\n      inputs = tf.nn.dropout(inputs, config.keep_prob)\n\n    # Simplified version of tensorflow.models.rnn.rnn.py's rnn().\n    # This builds an unrolled LSTM for tutorial purposes only.\n    # In general, use the rnn() or state_saving_rnn() from rnn.py.\n    #\n    # The alternative version of the code below is:\n    if LOOP_VERSION == 0:\n      from tensorflow.models.rnn import rnn\n      inputs = [tf.squeeze(input_, [1])\n               for input_ in tf.split(1, num_steps, inputs)]\n      outputs, state = rnn.rnn(cell, inputs, initial_state=self._initial_state)\n      output = tf.reshape(tf.concat(1, outputs), [-1, size])\n    if LOOP_VERSION == 1:\n      outputs = []\n      state = self._initial_state\n      with tf.variable_scope(\"RNN\"):\n        for time_step in range(num_steps):\n          if time_step > 0: tf.get_variable_scope().reuse_variables()\n          (cell_output, state) = cell(inputs[:, time_step, :], state)\n          outputs.append(cell_output)\n      output = tf.reshape(tf.concat(1, outputs), [-1, size])\n    if LOOP_VERSION == 2:\n      from tensorflow.models.rnn import rnn\n      # inputs: (b,t,d)\n      outputs, state = rnn.dynamic_rnn(cell, inputs, initial_state=self._initial_state)\n      output = tf.reshape(outputs, [-1, size])\n\n    softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size])\n    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n    logits = tf.matmul(output, softmax_w) + softmax_b\n    loss = tf.nn.seq2seq.sequence_loss_by_example(\n        [logits],\n        [tf.reshape(self._targets, [-1])],\n        [tf.ones([batch_size * num_steps])])\n    self._cost = cost = tf.reduce_sum(loss) / batch_size\n    self._final_state = state\n\n    if not is_training:\n      return\n\n    self._lr = tf.Variable(0.0, trainable=False)\n    tvars = tf.trainable_variables()\n    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n                                      config.max_grad_norm)\n\n    optimizer = tf.train.GradientDescentOptimizer(self.lr)\n    self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n\n  def assign_lr(self, session, lr_value):\n    session.run(tf.assign(self.lr, lr_value))\n\n  @property\n  def input_data(self):\n    return self._input_data\n\n  @property\n  def targets(self):\n    return self._targets\n\n  @property\n  def initial_state(self):\n    return self._initial_state\n\n  @property\n  def cost(self):\n    return self._cost\n\n  @property\n  def final_state(self):\n    return self._final_state\n\n  @property\n  def lr(self):\n    return self._lr\n\n  @property\n  def train_op(self):\n    return self._train_op\n\n\nclass SmallConfig(object):\n  \"\"\"Small config.\"\"\"\n  init_scale = 0.1\n  learning_rate = 1.0\n  max_grad_norm = 5\n  num_layers = 2\n  num_steps = 20\n  hidden_size = 200\n  max_epoch = 4\n  max_max_epoch = 13\n  keep_prob = 1.0\n  lr_decay = 0.5\n  batch_size = 20\n  vocab_size = 10000\n\n\nclass MediumConfig(object):\n  \"\"\"Medium config.\"\"\"\n  init_scale = 0.05\n  learning_rate = 1.0\n  max_grad_norm = 5\n  num_layers = 2\n  num_steps = 35\n  hidden_size = 650\n  max_epoch = 6\n  max_max_epoch = 39\n  keep_prob = 0.5\n  lr_decay = 0.8\n  batch_size = 20\n  vocab_size = 10000\n\n\nclass LargeConfig(object):\n  \"\"\"Large config.\"\"\"\n  init_scale = 0.04\n  learning_rate = 1.0\n  max_grad_norm = 10\n  num_layers = 2\n  num_steps = 35\n  hidden_size = 1500\n  max_epoch = 14\n  max_max_epoch = 55\n  keep_prob = 0.35\n  lr_decay = 1 / 1.15\n  batch_size = 20\n  vocab_size = 10000\n\n\nclass TestConfig(object):\n  \"\"\"Tiny config, for testing.\"\"\"\n  init_scale = 0.1\n  learning_rate = 1.0\n  max_grad_norm = 1\n  num_layers = 1\n  num_steps = 2\n  hidden_size = 2\n  max_epoch = 1\n  max_max_epoch = 1\n  keep_prob = 1.0\n  lr_decay = 0.5\n  batch_size = 20\n  vocab_size = 10000\n\n\ndef run_epoch(session, m, data, eval_op, verbose=False):\n  \"\"\"Runs the model on the given data.\"\"\"\n  epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n  start_time = time.time()\n  costs = 0.0\n  iters = 0\n  state = m.initial_state.eval()\n  for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size,\n                                                    m.num_steps)):\n    cost, state, _ = session.run([m.cost, m.final_state, eval_op],\n                                 {m.input_data: x,\n                                  m.targets: y,\n                                  m.initial_state: state})\n    costs += cost\n    iters += m.num_steps\n\n    if verbose and step % (epoch_size // 10) == 10:\n      print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n            (step * 1.0 / epoch_size, np.exp(costs / iters),\n             iters * m.batch_size / (time.time() - start_time)))\n\n  return np.exp(costs / iters)\n\n\ndef get_config():\n  if FLAGS.model == \"small\":\n    return SmallConfig()\n  elif FLAGS.model == \"medium\":\n    return MediumConfig()\n  elif FLAGS.model == \"large\":\n    return LargeConfig()\n  elif FLAGS.model == \"test\":\n    return TestConfig()\n  else:\n    raise ValueError(\"Invalid model: %s\", FLAGS.model)\n\n\ndef main(_):\n  if not FLAGS.data_path:\n    raise ValueError(\"Must set --data_path to PTB data directory\")\n\n  raw_data = reader.ptb_raw_data(FLAGS.data_path)\n  # train: 929589, valid: 73760, test: 82430\n  train_data, valid_data, test_data, _ = raw_data\n\n  config = get_config()\n  eval_config = get_config()\n  eval_config.batch_size = 1\n  eval_config.num_steps = 1\n\n  with tf.Graph().as_default(), tf.Session() as session:\n    initializer = tf.random_uniform_initializer(-config.init_scale,\n                                                config.init_scale)\n    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n      m = PTBModel(is_training=True, config=config)\n    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n      mvalid = PTBModel(is_training=False, config=config)\n      mtest = PTBModel(is_training=False, config=eval_config)\n\n    tf.initialize_all_variables().run()\n\n    for i in range(config.max_max_epoch):\n      lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n      m.assign_lr(session, config.learning_rate * lr_decay)\n\n      print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n      train_perplexity = run_epoch(session, m, train_data, m.train_op,\n                                   verbose=True)\n      print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n      valid_perplexity = run_epoch(session, mvalid, valid_data, tf.no_op())\n      print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n\n    test_perplexity = run_epoch(session, mtest, test_data, tf.no_op())\n    print(\"Test Perplexity: %.3f\" % test_perplexity)\n\n\nif __name__ == \"__main__\":\n  tf.app.run()\n\n\n```\n", "comments": ["This has been fixed internally and is waiting for the next push to nightly.\n", "Thanks for the response. What does mean by 'waiting for the next push to\nnightly'?\n\nOn Thu, Mar 31, 2016 at 7:22 PM, ebrevdo notifications@github.com wrote:\n\n> This has been fixed internally and is waiting for the next push to nightly.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1733#issuecomment-204170366\n", "It means it'll show up on github in a day or two; and in our pip nightly builds the same night.\n", "Good to know. Could you perhaps inform this thread when it's finally merged into the master branch? Thanks again. \n", "Should be fixed at head. Can you confirm?\n", "Fixed! Thank you very much. \n"]}, {"number": 1732, "title": "[skflow] sklearn dep version check fix", "body": "cc: @ilblackdragon \n", "comments": ["Can one of the admins verify this patch?\n", "Updated\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 1731, "title": "grammar update in TensorFlow mechanics 101 index", "body": "", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1730, "title": "update description for Tensorflow Mechanics 101 Tutorial", "body": "I changed: \"We use again MNIST as the example.\" to \"We again use MNIST as the example.\" as this seemed to make more sense. \n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "will submit again with signed git username\n"]}, {"number": 1729, "title": "Add getting started doc for skflow", "body": "This is a slight revision of the previous version of the skflow readme, which is still in the [skflow repository](https://github.com/tensorflow/skflow/blob/master/g3doc/get_started/index.md)\n\nThe changes between that version and this one are\n1) The examples link now points to the new location of the examples (within the tensorflow repository)\n2) A couple of minor markdown formatting changes.\n", "comments": ["Can one of the admins verify this patch?\n", "@martinwicke Should we move all skflow documentation into g3doc? Or should we update the generation file and keep it in the contrib?\n\nOptionally we can call it `learn` documentation to consolidate all things there.\n", "We should keep it in contrib and update the website generation. It's not the only project with this issue now and separating the documentation seems like a bad idea.\n", "@dansbecker Closing this one, as we moved documentation to https://github.com/tensorflow/tensorflow/tree/master/tensorflow/g3doc/contrib/learn \n\nFeel free to add more documentation there - it will be showing up on tensorflow.org website over time.\n"]}, {"number": 1728, "title": "TensorFlow: fix python3 with GPU test failures.", "body": "python3 compatibility is so annoying.\n\nFixes #1722 \n", "comments": ["FYI @caisq.\n\nCan we get a python 3 GPU test suite?  Would have caught these bugs (which only show up on the GPU build).\n"]}, {"number": 1727, "title": "GPU resources not released when session is closed", "body": "As I understand from the documentation, running `sess.close()` is supposed to release the resources, but it doesn't. I have been running the following test:\n\n``` python\nwith tf.Session() as sess:\n    with tf.device('/gpu:0'):\n        matrix1 = tf.constant([[3., 3]])\n        matrix2 = tf.constant([[2.], [2.]])\n        product = tf.matmul(matrix1, matrix2)\n        result = sess.run(product)\n        print(result)\n```\n\nThis allocates all the free memory of gpu0, but it is not released when `sess` is closed (both using a context manager as in the code above, but also when calling `sess.close()` manually). The memory usage persists until that Python process is terminated. The way I have been checking memory usage is through `nvidia-smi`, but I have also confirmed that other processes can't allocate that GPU memory until the process terminates (not the session closes). I would like to be able to free the resources and still keep the Python process running.\n### Environment info\n\nI am running a 64-bit Linux (CentOS) with a computer that has two Tesla K40c (driver 346.46, CUDA 7.0). I installed the 0.7.1 tensorflow for Linux and Python 3.4 through pip. The output of `tf.__version__` is 0.7.1.\n### Steps to reproduce\n\nSimply running the code above should according to the document allocate and then release the memory. However, the GPU memory is still allocated and thus unusable by other processes. However, it can be re-used by the same Python process, meaning that I can re-run the snippet over and over as long as I do it from the same Python process.\n### Logs or other output that would be helpful\n\nHere is a log of the session. At the end, the memory is still allocated. Note that another user is connected to both GPUs through Torch7, and is actively using gpu0.\n\n``` python\nIn [1]: import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n\nIn [2]: with tf.Session() as sess:\n    with tf.device('/gpu:1'):\n        matrix1 = tf.constant([[3., 3]])\n        matrix2 = tf.constant([[2.], [2.]])\n        product = tf.matmul(matrix1, matrix2)\n        result = sess.run(product)\n        print(result)\n   ...:\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: Tesla K40c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:03:00.0\nTotal memory: 11.25GiB\nFree memory: 3.27GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties:\nname: Tesla K40c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:82:00.0\nTotal memory: 11.25GiB\nFree memory: 11.05GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40c, pci bus id: 0000:82:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.50GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 1 memory begins at 0x4208f40000 extends to 0x44a8b030cd\n[[ 12.]]\n\nIn [3]: \n```\n", "comments": ["I just wanted to add that I have also tested this on the most recent master (https://github.com/tensorflow/tensorflow/commit/de5101da4638ac469041575dadb4921ebb33eb6a) now and it is still a problem.\n", "I'm having this issue as well.\n\n```\nimport tensorflow as tf\nsess = tf.Session()\nsess.close()\nsess._closed\nsess._opened\n```\n\nBoth _opened and _closed come back as false\n", "This is a bug.\n", "I am experiencing the same issue... thought I missed something...\nPlease look into this ASAP if it is a bug.\n", "Same problem here.\n", "I have the same problem here with version 0.8.0.\n\nThis is really anoying, because I have to kill the python kernel to free the resources!\n", "Same issue here with version 0.8.0rc0. In fact, the GPU memory isn't even released after shutting down the Python kernel. Running 64-bit Linux (CentOS) with 4 nVidia GRID K520 GPUs, Python 2.7.\n\nIn lieue of fixing the issue, a quick workaround could be to allow the user to free up the memory explicitly (of course fixing the issue would be preferable)\n\nEDIT: this seems to only happen on GPU with ID 0. If I mask available GPUs through the env var CUDA_VISIBLE_DEVICES to not include 0, then all appears to go fine\n", "A negative side-effect of this is that you can't run all the tests with \n`bazel test -c opt --config=cuda tensorflow/...`\n\nA fraction of the tests (10-30%) fail with `CUDA_ERROR_OUT_OF_MEMORY` on my 4GB GTX 980.\nBut then if I rerun any of the failing tests using separate `blaze test`, it works.\n", "Each bazel test invocation is a separate process, so when the process exits, it does release the memory.  In this case, bazel is running multiple tests in parallel.  Use bazel test -j 1 to only run one at a time.\n", "Thanks @vrv, that fixed all the out-of-memory errors I had\n", "As for the original problem, currently the Allocator in the GPUDevice belongs to the ProcessState, which is essentially a global singleton. The first session using GPU initializes it, and frees itself when the process shuts down. Even if a second session chooses a different GPUOptions, it would not take effect. \n\nChanging this would be a fairly large change. We need to rethink how the device is initialized and how it interacts with the session, and therefore modify the current API. It is unlikely the TensorFlow team can get to this in the short term. \n\nMarking it as contribution welcome. If anyone is interested, a design proposal could be discussed here, before proceeding to implementations. \n", "Is it possible to provide a dedicated subroutine we can call at the end of our sessions that will free up GPU resources and release GPU control back to the OS?\r\nOn Windows 7, no GPU-required programs can run after TensorFlow until the PC is restarted.", "@recursionbane it's supposed to release it all, although on Linux I've seen cases that GPU was rendered unusable until restart, and this was caused by NVidia driver problems", "Yes, certain NVIDIA drivers had problems with releasing the memory.\n\nOn Fri, Mar 10, 2017 at 3:38 PM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> @recursionbane <https://github.com/recursionbane> it's supposed to\n> release it all, although on Linux I've seen cases that GPU was rendered\n> unusable until restart, and this was caused by NVidia driver problems\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/1727#issuecomment-285814765>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APAgTlGNS-czunIHwsNxjXqGmUgsCu_lks5rkd7hgaJpZM4H890g>\n> .\n>\n", "I am on v369.30 for my Quadro M1000M.\r\nI will upgrade to v376.84 to check if the problem is resolved.", "I am using Ubuntu 16.04 with tensorflow 1.0 and NVIDIA Tesla K20Xm GPU (CUDA 8.0) . I am facing similar problems. Memory is not released after the session is over", "As mentioned here, the best idea anyone has is: https://github.com/tensorflow/tensorflow/issues/1727#issuecomment-285815312, saying to upgrade your NVIDIA drivers.  Closing this and locking to make sure the current conclusion is easily found; if there is evidence that upgrading drivers does not solve the problem, we can open up a new bug!"]}, {"number": 1726, "title": "Add support for development version of Bazel when detecting the version", "body": "Development version of Bazel (especially built on ci.bazel.io) are setting their version to empty, making the test fails with error.\n\n/cc @vrv \n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 1725, "title": "scan / TensorArray bug", "body": "### Environment info\n\nOperating System: CentOS 6.7\nIf installed from sources, provide the commit hash: b4b276e\n### Issue\n\nAccording to https://github.com/tensorflow/tensorflow/issues/208, `scan` will soon be part of the public API.\n\nThe snippet of code\n\n```\nfrom __future__ import division, print_function\nimport tensorflow as tf\nfrom tensorflow.python.ops import functional_ops\n\ndef fn(previous_state, current_input):\n    return previous_state + current_input\n\nx = tf.Variable([0.0, 1.0, 2.0, 3.0, 4.0])\ny = functional_ops.scan(fn, x, parallel_iterations=1)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(y))\n```\n\nresults in the error below.\n\nI get the same error\n- whether or not I use a REPL\n- whether or not I define `x` and `y` inside of a `with tf.device('/cpu:0')` block\n- whether or not `swap_memory` is `True`\n- whether or not I use soft device placement\n\n```\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-1-73ddd8c32a4f> in <module>()\n     10 \n     11 with tf.Session() as sess:\n---> 12     sess.run(tf.initialize_all_variables())\n     13     print(sess.run(y))\n\n/home-4/rdipiet2@jhu.edu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\n    331     try:\n    332       result = self._run(None, fetches, feed_dict, options_ptr,\n--> 333                          run_metadata_ptr)\n    334       if run_metadata:\n    335         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/home-4/rdipiet2@jhu.edu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\n    536     # Run request and get response.\n    537     results = self._do_run(handle, target_list, unique_fetches,\n--> 538                            feed_dict_string, options, run_metadata)\n    539 \n    540     # User may have fetched the same tensor multiple times, but we\n\n/home-4/rdipiet2@jhu.edu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\n    599     if handle is None:\n    600       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n--> 601                            target_list, options, run_metadata)\n    602     else:\n    603       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n\n/home-4/rdipiet2@jhu.edu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\n    606   def _do_call(self, fn, *args):\n    607     try:\n--> 608       return fn(*args)\n    609     except tf_session.StatusNotOK as e:\n    610       error_message = compat.as_text(e.error_message)\n\n/home-4/rdipiet2@jhu.edu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\n    583                 run_metadata):\n    584       # Ensure any changes to the graph are reflected in the runtime.\n--> 585       self._extend_graph()\n    586       if options:\n    587         return tf_session.TF_Run(session, options,\n\n/home-4/rdipiet2@jhu.edu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _extend_graph(self)\n    636               self._session, graph_def.SerializeToString(), status)\n    637           if tf_session.TF_GetCode(status) != 0:\n--> 638             raise RuntimeError(compat.as_text(tf_session.TF_Message(status)))\n    639           self._opened = True\n    640         finally:\n\nRuntimeError: AttrValue must not have reference type value of float_ref\n     for attr 'dtype'\n    ; NodeDef: scan/TensorArray = TensorArray[dtype=DT_FLOAT_REF, dynamic_size=false, tensor_array_name=\"\"](scan/Squeeze); Op<name=TensorArray; signature=size:int32 -> handle:Ref(string); attr=dtype:type; attr=dynamic_size:bool,default=false; attr=tensor_array_name:string,default=\"\"; is_stateful=true>\n```\n", "comments": ["Try:\nx = tf.Variable(...)\nx = tf.identity(x)\n\ndoes the code run then?\n", "Yes that runs\n", "Perhaps it's fine to always require that the input is not a ref. I'll double check to see why it's not being converted for you. In the meantime use variable.value instead of variable directly.\n", "I am trying to use scan in tensorflow 0.8 version to implement a theano like rnn, while getting an error like this \n![screenshot from 2016-05-09 16 35 28](https://cloud.githubusercontent.com/assets/3108838/15107733/19526442-1604-11e6-9eb7-5fbbc6ae0fc4.png)\nmy code is like this\n![screenshot from 2016-05-09 16 36 24](https://cloud.githubusercontent.com/assets/3108838/15107751/32d2b7f0-1604-11e6-8527-0195a95e5aad.png)\ncan anyone tell me what's going on? and is there any possibility that tensorflow will make rnn easier to write with a symbolic loop like scan in theano with multiple parameters in the  fn function and maybe multiple return values?\n", "See tf.nn.dynamic_rnn.\nOn May 9, 2016 1:39 AM, \"\u674e\u709c\" notifications@github.com wrote:\n\n> I am trying to use scan in tensorflow 0.8 version to implement a theano\n> like rnn, while getting an error like this\n> [image: screenshot from 2016-05-09 16 35 28]\n> https://cloud.githubusercontent.com/assets/3108838/15107733/19526442-1604-11e6-9eb7-5fbbc6ae0fc4.png\n> my code is like this\n> [image: screenshot from 2016-05-09 16 36 24]\n> https://cloud.githubusercontent.com/assets/3108838/15107751/32d2b7f0-1604-11e6-8527-0195a95e5aad.png\n> can anyone tell me what's going on? and is there any possibility that\n> tensorflow will make rnn easier to write with a symbolic loop like scan in\n> theano with multiple parameters in the fn function and maybe multiple\n> return values?\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1725#issuecomment-217807428\n", "@ebrevdo sorry to bother you, I just transferred to tensorflow from theano, I am having trouble finding tf.nn.dynamic_rnn and its api, where can I find it? Because I can't find it in the api page.\nthanks for your kind reply.\n", "it's \"undocumented\" as a still-changing API, but it's pretty stable now;\nshould be \"officially released\" into the documentation soon.  for now, you\ncan use ipython and type:\n\nimport tf\ntf.nn.dynamic_rnn?\ntf.nn.rnn_cell.LSTMCell?\ntf.nn.rnn_cell.GRUCell?\n\nto get some documentation\n\nOn Mon, May 9, 2016 at 7:43 AM, \u674e\u709c notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo sorry to bother you, I just\n> transferred to tensorflow from theano, I am having trouble finding\n> tf.nn.dynamic_rnn and its api, where can I find it? Because I can't find it\n> in the api page.\n> thanks for your kind reply.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1725#issuecomment-217884239\n", "@ebrevdo thanks again for your reply, I am trying the dynamic_rnn, but got confused with the parameter sen_len, if I send in a 1-dim tensor of lengths of sentences in the batch, What shape would the output of dynamic_rnn be? will the last time_step of this output still be right? and why output[:,sen_len,:] couldnt' work? Thank you.\n", "Anyone with pending questions - please open new issues for your individual questions - the initial question was answered.\n"]}, {"number": 1724, "title": "Updating ExponentialMovingAverage based on a condition", "body": "Using tensorflow 0.7.1.\n\nI have followed issue #804 to make use of batch normalization with tensorflow. However, I struggle to get the snippet to work as expected and have nailed down the problem to something more concise.\n\nLet us assume we want to perform a moving average over incoming scalars; sometimes we want to update the statistics of that moving average, sometimes we don't. We will model that with a placeholder `do_update`, which we can set to `True` or `False` in the `feed_dict` passed to `sess.run(...)`. Basically, this is the code from #804 but greatly simplified.\n\n```\nimport tensorflow as tf\nimport numpy as np\n\ninpt = tf.Variable(np.array([1.]))\ndo_update = tf.placeholder(tf.bool)\n\nema = tf.train.ExponentialMovingAverage(.9)\nema_assign = ema.apply([inpt])\n\n\ndef update():\n    with tf.control_dependencies([ema_assign]):\n        return tf.identity(ema.average(inpt))      # note the identity.\n\ndef no_update():\n    return ema.average(inpt)\n\nrun = tf.python.control_flow_ops.cond(do_update, update, no_update)\n```\n\nHowever, when I execute `run` updating will happen. It does not matter what the value of `do_update` is.\n\n```\nprint run.eval({inpt: np.array([2.]), do_update: True})\nprint run.eval({inpt: np.array([2.]), do_update: True})\nprint run.eval({inpt: np.array([2.]), do_update: True})\n\n# prints:\n# [ 1.10000002]\n# [ 1.19000004]\n# [ 1.27100006]\n\nprint run.eval({inpt: np.array([2.]), do_update: False})\nprint run.eval({inpt: np.array([2.]), do_update: False})\nprint run.eval({inpt: np.array([2.]), do_update: False})\n\n# prints:\n# [ 1.34390007]\n# [ 1.40951008]\n# [ 1.46855908]\n```\n\nCuriously, if I remove the `tf.identity` above in the definition of `update`, neither of them performs an update after starting a new session.\n\n```\nprint run.eval({inpt: np.array([2.]), do_update: True})\nprint run.eval({inpt: np.array([2.]), do_update: True})\nprint run.eval({inpt: np.array([2.]), do_update: True})\n\n# prints:\n# [ 1.]\n# [ 1.]\n# [ 1.]\n\nprint run.eval({inpt: np.array([2.]), do_update: False})\nprint run.eval({inpt: np.array([2.]), do_update: False})\nprint run.eval({inpt: np.array([2.]), do_update: False})\n\n# prints:\n# [ 1.]\n# [ 1.]\n# [ 1.]\n```\n\nThis seems as uninteded behaviour to me, but maybe I am missing something.\n", "comments": ["Reproduced this. I'm compiling from source, commit https://github.com/tensorflow/tensorflow/commit/b4b276eb785f77beb226189ca1d32ba36717a588.\n\nEven more confusing to me:\n\n```\nimport tensorflow as tf\nimport numpy as np\n\nx = tf.Variable(1.)\nupdate_x = tf.assign_add(x, 1.0)\n\ndo_update = tf.placeholder(tf.bool)\n\nema = tf.train.ExponentialMovingAverage(.9)\nema_assign = ema.apply([x])\n\navg_without_update = ema.average(x)\n\nwith tf.control_dependencies([ema_assign]):\n    avg_with_update = tf.identity(avg_without_update)\n\navg = tf.python.control_flow_ops.cond(do_update, lambda: avg_with_update, lambda: avg_without_update)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run([update_x, avg], {do_update: False}))\n    print(sess.run([update_x, avg], {do_update: False}))\n    print(sess.run([update_x, avg], {do_update: False}))\n    print(sess.run([update_x, avg], {do_update: False}))\n    print(sess.run([update_x, avg], {do_update: False}))\n```\n\nresults in\n\n```\n[2.0, 1.0]\n[3.0, 1.1]\n[4.0, 1.3900001]\n[5.0, 1.7510002]\n[6.0, 2.0759003]\n```\n", "I think the dependencies set in a tf.cond() function are always getting executed.   For example:\n\n```\ndummy = tf.Print(1, ['dummy'])\n\ndef true_fn():\n  with tf.control_dependencies([dummy]):\n    return tf.Print(True,['in true_fn'])\n\ndef false_fn():\n  return tf.Print(False,['in false_fn'])\n\nrun = tf.python.control_flow_ops.cond(tf.constant(False), true_fn, false_fn)\n\nwith tf.Session() as sess:\n  print run.eval()\n```\n\nwill output:\n\n```\nI tensorflow/core/kernels/logging_ops.cc:79] [in false_fn]\nI tensorflow/core/kernels/logging_ops.cc:79] [dummy]\nFalse\n```\n", "Interesting. This might imply that the `sequence_length` complications in `python.ops.rnn._rnn_step` are actually saving no computation time.\n", "Is there any update regarding this issue?\n", "@mikowals is correct, tf.cond() seems to be executing both fn1 and fn2 regardless of the condition. I have figured out a work around without using tf.train.ExponentialMovingAverage\n\n```\nimport tensorflow as tf\nimport numpy as np\n\ninpt = tf.Variable(np.array([1.0]))\ndo_update = tf.placeholder(tf.bool)\n\nprev_ema = tf.Variable(np.array([0.0]))\ndecay = 0.9\n\nnew_ema = (1-decay)*inpt + decay*prev_ema\n\nema = tf.cond(do_update, lambda: new_ema, lambda: prev_ema)\n\nassign_op = prev_ema.assign(ema)\n\nwith tf.control_dependencies([assign_op]):\n    cur_ema = tf.identity(prev_ema)\n```\n\nNotice that now update becomes an assign_op that executes each time, however with a different value. Now I can execute the following with desired behavior\n\n```\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    sess.run(cur_ema, {inpt:np.array([2.]), do_update: True})\n    print prev_ema.eval()\n    sess.run(cur_ema, {inpt:np.array([2.]), do_update: True})\n    print prev_ema.eval()\n    sess.run(cur_ema, {inpt:np.array([2.]), do_update: True})\n    print prev_ema.eval()\n\n    sess.run(cur_ema, {inpt:np.array([2.]), do_update: False})\n    print prev_ema.eval()\n    sess.run(cur_ema, {inpt:np.array([2.]), do_update: False})\n    print prev_ema.eval()\n    sess.run(cur_ema, {inpt:np.array([2.]), do_update: False})\n    print prev_ema.eval()\n\n    sess.run(cur_ema, {inpt:np.array([2.]), do_update: True})\n    print prev_ema.eval()\n    sess.run(cur_ema, {inpt:np.array([2.]), do_update: True})\n    print prev_ema.eval()\n```\n\nWhich results in\n\n```\n[ 0.2]\n[ 0.38]\n[ 0.542]\n[ 0.542]\n[ 0.542]\n[ 0.542]\n[ 0.6878]\n[ 0.81902]\n```\n", "There was an explanation / workaround posted to a issue #2062 a couple of days ago.  The dependency node needs to be moved inside the conditional function.  \n\nSo an EMA update from this issues works as expected if the condition is done like this:\n\n```\nimport tensorflow as tf\nimport numpy as np\n\ninpt = tf.Variable(np.array([1.]))\ndo_update = tf.placeholder(tf.bool)\n\nema = tf.train.ExponentialMovingAverage(.9)\n\ndef update():\n  ema_assign = ema.apply([inpt])\n  with tf.control_dependencies([ema_assign]):\n      return tf.identity(ema.average(inpt))      # note the identity.\n\ndef no_update():\n  return ema.average(inpt)\n\nrun = tf.python.control_flow_ops.cond(do_update, update, no_update)\n```\n", "Thanks for adding a reference to issue #2062!  I meant but forgot to do that. \n"]}, {"number": 1723, "title": "tf.concat(tensorA, tensorB) should throw an error", "body": "### Environment info\n\nOperating System: Ubuntu 15.10\nCPU-only pip package, version 0.7.1\n### Steps to reproduce\n\n```\nimport tensorflow as tf\na = tf.Variable(tf.constant(1.0, shape=[10]))\nb = tf.Variable(tf.constant(2.0, shape=[10]))\nprint tf.concat(a, b) # <tf.Tensor 'concat:0' shape=(10,) dtype=float32>\n```\n\nSince the second argument to concat can be converted into a list of length 1, tensorflow takes a fast-path and immediately returns it, before type-checking the first argument. I would have expected an error that enabled me to find my incorrect API usage.\n", "comments": []}, {"number": 1722, "title": "The nightly gpu builds for python 3 are failing consistently", "body": "The nightly gpu builds for python 3 are failing some tests consistently.\n\nIn the [most recent console output log](http://ci.tensorflow.org/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-working/50/consoleFull), the following 4 tests seem to fail: (each item links to the relevant test output log)\n- [(66 / 224) Python test-on-install FAILED (): tensorflow/python/kernel_tests/bias_op_test.py](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-working/ws/pip_test/tests/logs/tensorflow/python/kernel_tests/bias_op_test.py.log)\n- [(75 / 224) Python test-on-install FAILED (): tensorflow/python/kernel_tests/conv_ops_test.py](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-working/ws/pip_test/tests/logs/tensorflow/python/kernel_tests/conv_ops_test.py.log)\n- [(93 / 224) Python test-on-install FAILED (): tensorflow/python/kernel_tests/fft_ops_test.py](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-working/ws/pip_test/tests/logs/tensorflow/python/kernel_tests/fft_ops_test.py.log)\n- [(120 / 224) Python test-on-install FAILED (): tensorflow/python/kernel_tests/pooling_ops_test.py](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-working/ws/pip_test/tests/logs/tensorflow/python/kernel_tests/pooling_ops_test.py.log)\n\n(Also: The link in the [readme file](https://github.com/tensorflow/tensorflow/blob/master/README.md) to the nightly gpu builds for python 3 seems to pointing to some old build that hasn't been updated for a week?)\n", "comments": ["@martinwicke @vrv Any plan to get this fixed soon?\n", "To me, that's gating the next release, so probably :)\n", "I filed bugs for these and assigned people, and some were fixed. It looks\nlike the failures are still the same though :(\nOn Thu, Mar 31, 2016 at 09:15 Vijay Vasudevan notifications@github.com\nwrote:\n\n> To me, that's gating the next release, so probably :)\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1722#issuecomment-204004713\n", "I'm fixing some of them.\n"]}, {"number": 1721, "title": "[bugfix] chase version to tensorflow/workspace.bzl", "body": "Fix #1720 . With original version we will have errors like:\n\n> /Users/clsung/git/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:10: fatal error:\n>       'eigen-eigen-3d9f227afae2/unsupported/Eigen/CXX11/Tensor' file not found\n> # include \"eigen-eigen-3d9f227afae2/unsupported/Eigen/CXX11/Tensor\"\n> \n> ```\n>      ^\n> ```\n> \n> 1 error generated.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n", "Hm, looks like a cmake failure -- not sure how to resolve this.  Is this working for you, and maybe  our cmake test suite isn't working?\n", "Linux CPU Tests CMAKE has been executed 8 times, none of them has a green light. I think that's a different issue and will look into it. However it seems like a protobuf setting(did not pre-installed?) problem.\n", "@jendap, @caisq  to help validate, if possible\n", "It seems we need an install_proto3.sh (sort of) in the tensorflow/tools/ci_build/install/\n\nTry\n`\n$ grep -ri proto tensorflow/tools/ci_build/install/ \n`\n", "I believe 2c2f422a628d83a591fb5d20d46deecc47655ead fixed this, right?  Let me know if I'm wrong and I'll reopen.\n"]}]