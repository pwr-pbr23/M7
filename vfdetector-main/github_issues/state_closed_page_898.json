[{"number": 26537, "title": "[TF 2.0 API Docs] tf.zeros", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:  2.0\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/zeros\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\n- **Visuals, if Applicable**\r\n  No visuals are included.\r\n\r\n- **Raises listed and defined**\r\n  No raises listed\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["Closing this issue as I combined this issue with another similar issue (#26532). Thanks!"]}, {"number": 26536, "title": "tf.keras.layers.Dense can't set attribute when subclassing tf.keras.Model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nUsing the Keras model subclassing API, Dense layer cannot be instantiated at any point in `__init__` after instantiation of Conv2D layer. \r\n\r\nDefining the same model using the Sequential API gives no errors. Similarly, Functional API gives no error message. Reverting to TF 1.13.1 with eager execution enabled shows the same results.\r\n\r\n**Describe the expected behavior**\r\n`tf.keras.layers.Dense` should be correctly instantiated when defining a model using Keras model subclassing API, just like how Sequential and Functional APIs behave.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n# Model subclassing, error\r\nclass Discriminator(tf.keras.Model):\r\n  def __init__(self):\r\n    super(Discriminator, self).__init__(name='Discriminator')\r\n    self.conv1 = layers.Conv2D(64, kernel_size=(5, 5), strides=(2, 2),\r\n                              padding='same')\r\n    self.conv1_bn = layers.BatchNormalization()\r\n    self.conv1_out = layers.LeakyReLU()\r\n    \r\n    self.conv2 = layers.Conv2D(128, kernel_size=(5, 5), strides=(2, 2),\r\n                              padding='same')\r\n    self.conv2_bn = layers.BatchNormalization()\r\n    self.conv2_out = layers.LeakyReLU()\r\n    \r\n    self.conv3 = layers.Conv2D(256, kernel_size=(5, 5), strides=(1, 1),\r\n                               padding='same')\r\n    self.conv3_bn = layers.BatchNormalization()\r\n    self.conv3_out = layers.LeakyReLU()\r\n\r\n    self.flatten = layers.Flatten()\r\n    # Error occurs here, code does run without dense layer\r\n    self.output = layers.Dense(1)\r\n    \r\n  def call(self, input, training=True):\r\n    conv1 = self.conv1(input)\r\n    conv1_bn = self.conv1_bn(conv1)\r\n    conv1 = self.conv1_out(conv1_bn)\r\n\r\n    conv2 = self.conv2(conv1)\r\n    conv2_bn = self.conv2_bn(conv2)\r\n    conv2 = self.conv2_out(conv2_bn)\r\n    \r\n    conv3 = self.conv3(conv2)\r\n    conv3_bn = self.conv3_bn(conv3)\r\n    conv3 = self.conv3_out(conv3_bn)\r\n    \r\n    flatten = self.flatten(conv3)\r\n    output = self.output(flatten)\r\n    \r\n    return output\r\n```\r\n```python\r\n# Sequential API, no error\r\ndef make_discriminator_sequentialmodel():\r\n    model = tf.keras.Sequential()\r\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', \r\n                                     input_shape=[28, 28, 1]))\r\n    model.add(layers.LeakyReLU())\r\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\r\n    model.add(layers.LeakyReLU())\r\n    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\r\n    model.add(layers.LeakyReLU())\r\n    model.add(layers.Flatten())\r\n    model.add(layers.Dense(1))\r\n     \r\n    return model\r\n  \r\ndiscriminator_sequential = make_discriminator_sequentialmodel()\r\nfake_input = np.ones((32, 28, 28, 1), dtype=np.float32)\r\nprint(discriminator_sequential(fake_input))\r\n```\r\n\r\n```python\r\n# Functional API, no error\r\ndef make_discriminator_functionalmodel():\r\n  inputs = tf.keras.Input(shape=(28, 28, 1))\r\n  \r\n  x = layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(inputs)\r\n  x = layers.LeakyReLU()(x)\r\n  x = layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(x)\r\n  x = layers.LeakyReLU()(x)\r\n  x = layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same')(x)\r\n  x = layers.LeakyReLU()(x)\r\n  x = layers.Flatten()(x)\r\n  x = layers.Dense(1)(x)\r\n  \r\n  model = tf.keras.Model(inputs=inputs, outputs=x)\r\n  \r\n  return model\r\n\r\ndiscriminator_functional = make_discriminator_functionalmodel()\r\nfake_input = np.ones((32, 28, 28, 1), dtype=np.float32)\r\nprint(discriminator_functional(fake_input))\r\n```\r\n**Other info / logs**\r\nTraceback from model subclassing code:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-12-1e75b671d09c> in <module>()\r\n     48     return logits\r\n     49 \r\n---> 50 D = Discriminator()\r\n     51 fake_input = np.ones((32, 28, 28, 1), dtype=np.float32)\r\n     52 d_o = D.call(fake_input, training=True)\r\n\r\n<ipython-input-12-1e75b671d09c> in __init__(self)\r\n     25 \r\n     26     self.flatten = layers.Flatten()\r\n---> 27     self.output = layers.Dense(1)\r\n     28 \r\n     29   def call(self, input, training=True):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in __setattr__(self, name, value)\r\n    410                            ' Always start with this line.')\r\n    411 \r\n--> 412     super(Network, self).__setattr__(name, value)\r\n    413 \r\n    414     # Keep track of metric instance created in subclassed model/layer.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __setattr__(self, name, value)\r\n   1778         # Exclude @property.setters from tracking\r\n   1779         hasattr(self.__class__, name)):\r\n-> 1780       super(Layer, self).__setattr__(name, value)\r\n   1781       return\r\n   1782 \r\n\r\nAttributeError: can't set attribute\r\n```\r\n", "comments": ["This is because \"output\" is a `@property` of `Layer` without a setter. You'll need to pick a different name."]}, {"number": 26535, "title": "[TF 2.0 API Docs] tf.unique", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:  2.0\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/unique\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\n- **Visuals, if Applicable**\r\n  No visuals are included.\r\n\r\n- **Raises listed and defined**\r\n  No raises listed\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["Closing this issue as I combined this issue with another similar issue (#26532). Thanks!"]}, {"number": 26534, "title": "[TF 2.0 API Docs] tf.unstack", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:  2.0\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/unstack\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\n- **Visuals, if Applicable**\r\n  No visuals are included.\r\n\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["Closing this issue as I combined this issue with another similar issue (#26532). Thanks!"]}, {"number": 26533, "title": "[TF 2.0 API Docs] tf.argsort", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:  2.0\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/argsort\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\n- **Usage example**\r\n  No usage example is provided.\r\n\r\n- **Visuals, if Applicable**\r\n  No visuals are included.\r\n\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["It has a usage example now. I don't think it needs a visual. Closing because it seems fixed: https://www.tensorflow.org/api_docs/python/tf/argsort"]}, {"number": 26532, "title": "[TF 2.0 API Docs] tf.math.argmin", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:  2.0\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/argmin\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\n- **Usage example**\r\n  No usage example is provided.\r\n\r\n- **Visuals, if Applicable**\r\n  No visuals are included.\r\n\r\n- **Raises listed and defined**\r\n  No raises listed\r\n\r\nMuch like #26530, documentation for tf.math.argmin is created from a generated file ``python/ops/gen_math_ops.py``; a link to the file that generates ``python/ops/gen_math_ops.py`` would be handy for users.\r\n\r\nRelated files to be updated: ``tensorflow/core/api_def/base_api/api_def_ArgMin.pbtxt``, ``/tensorflow/core/ops/math_ops.cc``\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["@mandroid6 Could you combine This and six similar issues (you created) into one issue? We can close all those issues to reduce overhead for the developers. Thanks!", "Here I am combining four more similar issues (#26534, #26535, #26537, and #26538) with this issue. I am closing the similar issue to reduce overhead on triaging similar issues. Thanks!\r\n\r\n1. System information (#26534)\r\n\r\nTensorFlow version: 2.0\r\nDoc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/unstack\r\n\r\n2. System information (#26535)\r\n\r\nTensorFlow version: 2.0\r\nDoc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/unique\r\n\r\n3. System information (#26537)\r\n\r\nTensorFlow version: 2.0\r\nDoc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/zeros\r\n\r\n4. System information (#26538)\r\n\r\nTensorFlow version: 2.0\r\nDoc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/zeros_initializer\r\n\r\nThanks!", "A part of the work on this enlarged issue has been done in #26540. @mandroid6 do you want to make a single PR with the rest of the documentation (`tf.unstack`, `tf.unique`, `tf.zeros`, `tf.zeros_initializer`) and then we can close this? Just to make sure we're not forgetting anything.", "@mihaimaruseac yes will make a single PR for these documentation issues `` tf.unstack``,`` tf.unique``, ``tf.zeros``, ``tf.zeros_initializer`` and also ``tf.zeros_like``.", "@mandroid6 any new PRs for the rest of the issues?", "I am closing this issue as docs for the four ops are updated in the TF website. Please feel free to reopen if I am mistaken. Thanks!"]}, {"number": 26531, "title": "TensorFlow Official website tutorials error", "body": "\r\n**System information**\r\n- TensorFlow version:\r\n- Doc Link: https://tensorflow.google.cn/tutorials/keras/basic_classification#preprocess_the_data\r\n\r\n**Describe the documentation issue**\r\nplt.show()  missing\r\n\r\n\r\n", "comments": ["@flyingsalmon I can see the following\r\nplt.figure()\r\nplt.imshow(train_images[0])\r\nplt.colorbar()\r\nplt.grid(False)\r\nplt.show()\r\n\r\nCould you remove cache and open again? Thanks!", "@jvishnuvardhan I'm visiting the Chinese version, and can't see it.\r\n", "Hi @flyingsalmon, \r\n\r\nThanks for the report.\r\n\r\nWe try to keep these up to date.\r\nBut there is some lag on translation.\r\n\r\nThere's no extra action anyone needs to take for this.\r\nThis will be fixed when the translation is through the pipeline."]}, {"number": 26530, "title": "[TF 2.0 API Docs] tf.math.argmax", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:  2.0\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/argmax\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\n- **Usage example**\r\n  No usage example is provided.\r\n\r\n- **Visuals, if Applicable**\r\n  No visuals are included.\r\n\r\n- **Raises listed and defined**\r\n  No raises listed\r\n\r\nMuch like #25802, documentation for tf.math.argmax is created from a generated file ``python/ops/gen_math_ops.py``; a link to the file that generates ``python/ops/gen_math_ops.py`` would be handy for users.\r\n\r\nRelated files to be updated: ``tensorflow/core/api_def/base_api/api_def_ArgMax.pbtxt``, ``/tensorflow/core/ops/math_ops.cc``\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["@mandroid6 Can you create single issue with all the similar modifications? It is too much overhead to work on 8 similar issues and it will be better for everyone (including you). Could you combine rest of the seven similar issues into one issue? Thanks! ", "@jvishnuvardhan Yes sure,my understanding was to create individual issues so that new GSoC students could easily pick them up as their first task \ud83d\ude03. \r\nWill combine them into a single issue by tomorrow!", "@mandroid6 Are you going to create single issue? please let me know so that i can close other 8 similar issues. Thanks!", "@jvishnuvardhan still waiting to combine them as new students have already started working on individual issues. Please wait till tomorrow. Thanks", "@mandroid6 Thanks for your support.", "@Mandar-Shinde Can I take reference from [here](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.argmax.html?highlight=argmax#numpy.argmax) for adding examples just like it is mentioned in this Issue - https://github.com/tensorflow/tensorflow/issues/25802", "Closing this in favor of #26532 as work has been done on #26540 for both this and that"]}, {"number": 26529, "title": "Added input verification for tf.image.grayscale_to_rgb", "body": "At the moment there is no verification of the input's shape in `tf.image.grayscale_to_rgb`. This means that if someone uses this operation on an input with invalid shape, e.g. only a `2D` tensor (this is how `matplotlib` defines grayscale images), an error message will be raised that might be hard to understand for users. This pull requests added some assertion operations inside the operation to give the user better feedback _why_ the operation failed. This might help preventing further confusion for users, e.g. [issue 26324](https://github.com/tensorflow/tensorflow/issues/26324)", "comments": ["Are you still available to review this PR @drpngx ?", "@alextp I cannot see why the required builds have failed as I cannot access the logs. On my local setup, this does not happen, therefore, I cannot reproduce this. Can you trigger the builds again?", "Can we merge this now?", "@rthadur Is there something left for me to do or can this be merged into `master` now?"]}, {"number": 26528, "title": "Trivial Fixes in the files", "body": "Fixed few issues in the files.", "comments": []}, {"number": 26527, "title": "Tensorflow segfaults with simple convolutional network", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 18.04 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Pip\r\n- TensorFlow version (use command below): \r\ntensorflow                     1.13.1       \r\ntensorflow-estimator     1.13.0       \r\nb'v1.13.1-0-g6612da8951' 1.13.1\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: 1070 TI\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nReceiving a segmentation fault when I run this code\r\n\r\n**Describe the expected behavior**\r\nExpected model to be filled\r\n\r\n**Code to reproduce the issue**\r\n```\r\ndef load_image_into_numpy_array(image_path):\r\n  image = Image.open(image_path)\r\n  (im_width, im_height) = image.size\r\n  return np.array(image.getdata()).reshape(\r\n      (im_height, im_width, 3)).astype(np.uint8)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    TEST_IMAGE_PATHS = 'train'\r\n    output = {}\r\n    model = keras.Sequential([\r\n        keras.layers.Conv2D(64, kernel_size=3, activation=tf.nn.relu, input_shape=(360, 640,  3)),\r\n        keras.layers.Flatten(),\r\n        keras.layers.Dense(128, activation=tf.nn.relu),\r\n        keras.layers.Dense(2, activation=tf.nn.softmax)\r\n    ])\r\n    model.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n    labels = {}\r\n    print(\"Loaded labels\")\r\n    with open(\"labels.json\") as label_f:\r\n        labels = json.loads(''.join(label_f.readlines()))\r\n    train_images = []\r\n    train_labels = []\r\n    count = 0\r\n    for (dirpath, dirnames, filenames) in os.walk(TEST_IMAGE_PATHS):\r\n        for filename in filenames:\r\n            if filename in labels:\r\n                img = load_image_into_numpy_array(os.path.join(dirpath, filename))\r\n                train_images.append(img)\r\n                train_labels.append([labels[filename]])\r\n        else:\r\n            continue\r\n        break\r\n    train_images = np.array(train_images)\r\n    train_labels = np.array(train_labels)\r\n    model.fit(train_images, train_labels, epochs=5, batch_size=396)\r\n```\r\n\r\nAll images in TEST_IMAGE_PATHS are 360x640 \r\n\r\n**Other info / logs**\r\nHere's the gdb traceback\r\n\r\n```\r\n\r\nThread 1 \"python3\" received signal SIGSEGV, Segmentation fault.\r\n__memmove_avx_unaligned_erms () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:490\r\n490\t../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S: No such file or directory.\r\n(gdb) bt\r\n#0  __memmove_avx_unaligned_erms () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:490\r\n#1  0x00007fffcdd033bd in TF_NewTensor () from /home/cjds/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fffcdc5b8d5 in tensorflow::PyArrayToTF_Tensor(_object*, std::unique_ptr<TF_Tensor, tensorflow::detail::TFTensorDeleter>*) ()\r\n   from /home/cjds/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fffcdc5c5e5 in tensorflow::NdarrayToTensor(_object*, tensorflow::Tensor*) () from /home/cjds/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fffcdb06ad3 in tensorflow::(anonymous namespace)::RunCallableHelper(tensorflow::Session*, long, _object*, TF_Status*, absl::InlinedVector<_object*, 8ul, std::allocator<_object*> >*, TF_Buffer*) ()\r\n   from /home/cjds/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fffcdb0765c in tensorflow::TF_SessionRunCallable(TF_Session*, long, _object*, TF_Status*, absl::InlinedVector<_object*, 8ul, std::allocator<_object*> >*, TF_Buffer*) ()\r\n   from /home/cjds/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fffcdab59e0 in _wrap_TF_SessionRunCallable () from /home/cjds/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n```", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26527\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26527\">No</a>\n"]}, {"number": 26526, "title": "importing scope with map_fn twice causes internal bug", "body": "**System information**\r\n- Code is custom written\r\n- System OS Ubuntu 16.04\r\n- Mobile version N/A\r\n- Installed from binary\r\n- Tested on TF versions 1.12.0 and 1.13.1\r\n- Python version 3.5\r\n- Bazel version N/A\r\n- Compiler version N/A\r\n- CUDA/CUDNN 9/7.5\r\n- Graphics card GTX 1080 12GB\r\n\r\n**CURRENT BEHAVIOUR**: \r\nA graph is used that holds multiple map_fn functions inside it, trying to load the graph multiple times into different scopes as \"towers\" throws an internal bug error. The error is as follows:\r\n```\r\n\r\nInvalid loop structure: Loop \"create_t_fn/while/while_context\" has more than one LoopCond node: \r\n{{node tower_1/create_t_fn/while/LoopCond}} and {{node tower_0/create_t_fn/while/LoopCond}}. This is an internal bug, please file a bug report with instructions on how to reproduce the error.\r\n\r\n```\r\n\r\n**EXPECTED BEHAVIOUR**\r\n\r\nI should be able to load multiple graphs into different scopes, I have used this method of loading multiple graphs before, but none of those graphs used a map_fn function\r\n\r\n**CODE TO REPRODUCE ERROR**\r\n\r\n```\r\nwith self.graph.as_default():\r\n            for i, device in enumerate(gpu_device):\r\n                with tf.device('/device:GPU:%d'%device):\r\n                    tf.import_graph_def(graph_def, name='tower_%d' % i)\r\n                    with tf.name_scope('tower_%d' % i):\r\n                        print('...... Placing  graph into tower_%d on /device:GPU:%d' %(i, device))\r\n\r\n                        input_image_tensor = self.graph.get_tensor_by_name('tower_%d/input_img:0' % i)\r\n                        input_boxes_tensor = self.graph.get_tensor_by_name('tower_%d/bbox:0' % i)\r\n\r\n                        keypoints_tensor = self.graph.get_tensor_by_name('tower_%d/kpt:0' % i)\r\n\r\n                        self.tower_input_image_tensors.append(input_image_tensor)\r\n                        self.tower_input_boxes_tensors.append(input_boxes_tensor)\r\n                        self.tower_ouput_kpt_tensors.append(keypoints_tensor)\r\n```\r\n\r\nThe above code is used to load the graphs, the error is incurred only when I try to execute it using the code below:\r\n\r\n```\r\nfor t_idx, tower_data in enumerate(tower_data_splits):\r\n            if len(tower_data) > 0:\r\n\r\n                feed_dict[self.tower_input_boxes_tensors[t_idx]] = tower_data\r\n                feed_dict[self.tower_input_image_tensors[t_idx]] = image.copy()\r\n                print(\"Tower\",t_idx,\" gets data of shape \",tower_data.shape)\r\n                \r\n                out_tensors.append(self.tower_ouput_kpt_tensors[t_idx])\r\n\r\n        with self.sess.graph.as_default():\r\n            tower_outs = self.sess.run(out_tensors,feed_dict)\r\n\r\n```\r\n**Other info / logs**\r\n\r\nThe graph is custom created in TF, usage of the map_fn function can be seen below\r\n\r\n```\r\nT = tf.identity(tf.map_fn(lambda x: tf.convert_to_tensor([\r\n            [x[0,0], x[0,1],0],\r\n            [x[1,0], x[1,1],0],\r\n            [  0,      0,   1]], name = \"convert_t\"),set_val_udv, name = \"create_t_fn\"),name=\"T\")\r\n        \r\n```\r\n\r\n```\r\nscale_map = tf.identity(tf.map_fn(lambda x: tf.convert_to_tensor([\r\n            [x, x, 1],\r\n            [x, x, 1],\r\n            [1, 1, 1]], dtype=tf.float32),scale, name = \"create_scale_fn\"),name=\"scale_map\")\r\n\r\n```\r\n\r\nThe issues states that there's an issue in the first map_fn, but it will exist for any and all map_fn\r\n\r\nI have tried setting the `TF_ENABLE_WHILE_V2` and `TF_ENABLE_COND_V2` variables both via \r\n```\r\nos.environ['TF_ENABLE_WHILE_V2'] = '1'\r\nos.environ['TF_ENABLE_COND_V2'] = '1'\r\n```\r\nand via\r\n```\r\nexport TF_ENABLE_WHILE_V2 = '1'\r\nexport TF_ENABLE_COND_V2 = '1'\r\n```\r\n\r\nas  suggested by another issue, but this does not change anything, another implied quick fix was to add a name to all map_fn calls, but even that does not fix it", "comments": ["Hi @Imtinan1996 , thanks for reporting! Would it be possible for you to simplify the code you shared above into a single code snippet we can run to reproduce this problem? I don't fully understand how the map_fn calls fit into the tower construction code you shared.", "Hey @skye, let me elaborate, map_fn is not a part of the tower construction code. I used map_fn in a graph, that graph is then loaded into a graph_def, that is then used to create multiple towers. Anyways, I have simplified the code and compiled it into a notebook, I can upload the notebook if required, but here is the simplified code its outputs.\r\n\r\n**What's going on, in the code**\r\n\r\n- I create a sample graph that uses a list of 2x2 matrices and a list of scales\r\n- The graph uses map_fn to iterate through the 2x2 matrices and creates T\r\n- The graph then uses map_fn again to create a scale matrix, that is responsible for only multiplying specific values\r\n- The scale matrix is then multiplied with T, to give the output \"out\"\r\n- This graph is saved to disk\r\n- Default graph is reset\r\n- Saved graph is read as a graph_def\r\n- Graphs are loaded into specific GPUs\r\n- Input is distributed to each tower\r\n- Towers are run\r\n\r\n**THE CODE**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'\r\n\r\nprint(\"TENSORFLOW VERSION:\", tf.__version__)\r\n\r\nprint(\"\\n\\n------------ CREATING TF-GRPAH USING MULTIPLE MAP_FN ------------\\n\\n\")\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess:\r\n        \r\n        assign_val = tf.placeholder(tf.float32, name='input_matrix', shape=(None,2,2))\r\n        scale_val = tf.placeholder(tf.float32, name='scales', shape=(None))\r\n        \r\n        T = tf.identity(tf.map_fn(lambda x: tf.convert_to_tensor([\r\n            [x[0,0], x[0,1],0],\r\n            [x[1,0], x[1,1],0],\r\n            [  0,      0,   1]], name = \"convert_t\"),assign_val, name = \"create_t_fn\"),name=\"T\")\r\n        \r\n        scale_map = tf.identity(tf.map_fn(lambda x: tf.convert_to_tensor([\r\n            [x, x, 1],\r\n            [x, x, 1],\r\n            [1, 1, 1]], dtype=tf.float32),scale_val, name = \"create_scale_fn\"),name=\"scale_map\")\r\n        \r\n        out = T*scale_map\r\n        tf.identity(out,\"out\")\r\n        \r\n        out = sess.run([\"out:0\"],{\"input_matrix:0\": np.ones((10,2,2)),\r\n                                  \"scales:0\": np.arange(10)\r\n                                  })\r\n \r\n        print(\"\\n\\n------------ FREEZING AND SAVING GRAPH ------------\\n\\n\")\r\n\r\n        frozen_graph = tf.graph_util.convert_variables_to_constants(\r\n            sess,\r\n            tf.get_default_graph().as_graph_def(),\r\n            output_node_names=[\"out\"])\r\n        \r\n        with open('test_graph.pb', 'wb') as f:\r\n            f.write(frozen_graph.SerializeToString())\r\n\r\nprint(\"\\n\\n------------ LAODING GRAPH DEF ------------\\n\\n\")\r\n\r\ntf.reset_default_graph()\r\ngpu_device = [0,1,2]\r\n\r\nwith tf.gfile.GFile('test_graph.pb','rb') as model_file:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(model_file.read())\r\n\r\ngraph = tf.Graph()\r\ntower_input_matrix_tensors = list()\r\ntower_input_sclaes_tensors = list()\r\ntower_ouput_tensors = list()\r\n\r\nprint(\"\\n\\n------------ CREATING TOWERS ON MULTIPLE GPU ------------\\n\\n\")\r\n\r\nwith graph.as_default():\r\n    for i, device in enumerate(gpu_device):\r\n        with tf.device('/device:GPU:%d'%device):\r\n            tf.import_graph_def(graph_def, name='tower_%d' % i)\r\n            with tf.variable_scope('tower_%d' % i):\r\n                print('               Placing tower_%d on /device:GPU:%d' %(i, device))\r\n\r\n                input_matrix_tensor = graph.get_tensor_by_name('tower_%d/input_matrix:0' % i)\r\n                input_scale_tensor = graph.get_tensor_by_name('tower_%d/scales:0' % i)\r\n                out_tensor = graph.get_tensor_by_name('tower_%d/out:0' % i)\r\n\r\n                tower_input_matrix_tensors.append(input_matrix_tensor)\r\n                tower_input_sclaes_tensors.append(input_scale_tensor)\r\n                tower_ouput_tensors.append(out_tensor)\r\n\r\ntf_config = tf.ConfigProto(allow_soft_placement=True, \r\n                            gpu_options=tf.GPUOptions(allow_growth=True))\r\nsess = tf.Session(graph=graph, config=tf_config)\r\nn_towers = len(gpu_device)\r\n\r\nprint(\"\\n\\n------------ PREPARING INPUTS TO EXECUTE TOWERS ------------\\n\\n\")\r\n\r\ninput_matrix = np.ones((10,2,2))\r\nscales = np.arange(10)\r\n\r\nmatrices_splits = np.array_split(input_matrix, n_towers)\r\nscales_split = np.array_split(scales, n_towers)\r\n\r\nfeed_dict = {}\r\nout_tensors = []\r\n\r\nfor t_idx, tower_data in enumerate(matrices_splits):\r\n    if len(tower_data) > 0:\r\n        feed_dict[tower_input_matrix_tensors[t_idx]] = tower_data\r\n        feed_dict[tower_input_sclaes_tensors[t_idx]] = scales_split[t_idx]\r\n        out_tensors.append(tower_ouput_tensors[t_idx])\r\n\r\n\r\nprint(\"\\n\\n------------ EXECUTING TOWERS ------------\\n\\n\")        \r\n        \r\nwith sess.graph.as_default():\r\n    tower_outs = sess.run(out_tensors,feed_dict)\r\n\r\nprint(np.concatenate(tower_outs))\r\n```\r\n**The output of the code**\r\n\r\n```\r\nTENSORFLOW VERSION: 1.12.0\r\n\r\n\r\n------------ CREATING TF-GRPAH USING MULTIPLE MAP_FN ------------\r\n\r\n\r\n\r\n\r\n------------ FREEZING AND SAVING GRAPH ------------\r\n\r\n\r\nINFO:tensorflow:Froze 0 variables.\r\nINFO:tensorflow:Converted 0 variables to const ops.\r\n\r\n\r\n------------ LAODING GRAPH DEF ------------\r\n\r\n\r\n\r\n\r\n------------ CREATING TOWERS ON MULTIPLE GPU ------------\r\n\r\n\r\n               Placing tower_0 on /device:GPU:0\r\n               Placing tower_1 on /device:GPU:1\r\n               Placing tower_2 on /device:GPU:2\r\n\r\n\r\n------------ PREPARING INPUTS TO EXECUTE TOWERS ------------\r\n\r\n\r\n\r\n\r\n------------ EXECUTING TOWERS ------------\r\n\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/home/sibt/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1333     try:\r\n-> 1334       return fn(*args)\r\n   1335     except errors.OpError as e:\r\n\r\n/home/sibt/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1318       return self._call_tf_sessionrun(\r\n-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1320 \r\n\r\n/home/sibt/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1406         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1407         run_metadata)\r\n   1408 \r\n\r\nInvalidArgumentError: Invalid loop structure: Loop \"create_t_fn/while/while_context\" has more than one LoopCond node: {{node tower_1/create_t_fn/while/LoopCond}} and {{node tower_0/create_t_fn/while/LoopCond}}. This is an internal bug, please file a bug report with instructions on how to reproduce the error.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-19-293f067b0900> in <module>\r\n     98 \r\n     99 with sess.graph.as_default():\r\n--> 100     tower_outs = sess.run(out_tensors,feed_dict)\r\n    101 \r\n    102 print(np.concatenate(tower_outs))\r\n\r\n/home/sibt/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    927     try:\r\n    928       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 929                          run_metadata_ptr)\r\n    930       if run_metadata:\r\n    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/home/sibt/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1151       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1152                              feed_dict_tensor, options, run_metadata)\r\n   1153     else:\r\n   1154       results = []\r\n\r\n/home/sibt/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1326     if handle is None:\r\n   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1328                            run_metadata)\r\n   1329     else:\r\n   1330       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/home/sibt/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1346           pass\r\n   1347       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1348       raise type(e)(node_def, op, message)\r\n   1349 \r\n   1350   def _extend_graph(self):\r\n\r\nInvalidArgumentError: Invalid loop structure: Loop \"create_t_fn/while/while_context\" has more than one LoopCond node: node tower_1/create_t_fn/while/LoopCond (defined at <ipython-input-19-293f067b0900>:62)  and node tower_0/create_t_fn/while/LoopCond (defined at <ipython-input-19-293f067b0900>:62) . This is an internal bug, please file a bug report with instructions on how to reproduce the error.\r\n```\r\n\r\n**NOTE**\r\n\r\nThe sample contains some redundant code, like saving and loading the graph again, but i didnt want to disrupt the workflow that leads to the error. \r\n\r\nAlso the sample graph is used to illustrate the map_fn issue, i am using a similar structure to append data pre processing into my ResNet50 model.", "Awesome, thank you for compiling this into a single repro script! I am able to reproduce the error, and will keep you updated.", "That's great, i will be on a lookout for your feedback", "I believe I've identified the problem. The issue is around importing a while loop (which is generated by tf.map_fn) via import_graph_def multiple times. We don't propagate the import prefix to the while loop correctly.\r\n\r\nWe're working on a new implementation of tf.while_loop which doesn't have this problem (see the [design doc](https://github.com/tensorflow/community/blob/master/rfcs/20180821-differentiable-functional-while.md) for more details). As a workaround for now, you can enable the new while implementation by setting the environment variable `TF_ENABLE_CONTROL_FLOW_V2=1`. If you do this via sys.env in your code, make sure to do it _before_ importing tf.", "That's great, I worked around the problem by eliminating the use of map_fn, but still wanted to bring this to your attention, I have tried setting TF_ENABLE_CONTROL_FLOW_V2=1 just to test but the error persists, could be due to me using tensorflow version 1.12.1. If you think that this is fine then you may go ahead and close this issue. Thank you for your quick response and solution.", "Ah yeah, the TF_ENABLE_CONTROL_FLOW_V2 env var was only added in 1.13. In 1.12 there was an earlier version, TF_ENABLE_WHILE_V2=1, that you can try. However, I still recommend upgrading, since we've improved the new while loop quite a bit between 1.12 and 1.13.\r\n\r\nI'm gonna keep this issue open for now, since we should probably fix the issue without requiring a workaround. Please let me know if run into any issues with the new while loop implementation, if you end up using it.", "I tested the new while loop implementation in 1.13 with TF_ENABLE_CONTROL_FLOW_V2 set as 1, and it seems to work, thanks alot for your help", "I got the same error, and set TF_ENABLE_CONTROL_FLOW_V2 = 1 still do not work. My tensorflow version is 1.13.1.\r\n@skye Could you help to look into it?\r\n\r\n\r\nHere is my code:\r\n```\r\nimport os\r\nos.environ['TF_ENABLE_CONTROL_FLOW_V2'] = \"1\"\r\nimport tensorflow as tf\r\nimport time\r\nfrom tensorflow.python.platform import gfile\r\n\r\npb_graph_path = \"./models/ssd_mobilenet_v2_bootstrap_20190329_300_600_330k.pb\"\r\n\r\ndef LoadGraphDef(graph_path):\r\n    graph_def = tf.GraphDef()\r\n    with gfile.FastGFile(graph_path, 'rb') as f:\r\n        graph_def.ParseFromString(f.read())\r\n    return graph_def\r\n\r\nmerged_graph = tf.Graph()\r\nwith merged_graph.as_default():\r\n    graph_def1 = LoadGraphDef(pb_graph_path)\r\n    tf.import_graph_def(graph_def1, name=\"TrafficLight_1\")\r\n\r\n    graph_def2 = LoadGraphDef(pb_graph_path)\r\n    tf.import_graph_def(graph_def2, name=\"TrafficLight_2\")\r\n\r\n    image_tensor1 = merged_graph.get_tensor_by_name(\"TrafficLight_1/image_tensor:0\")\r\n    image_tensor2 = merged_graph.get_tensor_by_name(\"TrafficLight_2/image_tensor:0\")\r\n\r\n    detect_boxes_1   = merged_graph.get_tensor_by_name(\"TrafficLight_1/detection_boxes:0\")\r\n    detect_scores_1  = merged_graph.get_tensor_by_name(\"TrafficLight_1/detection_scores:0\")\r\n    detect_class_1   = merged_graph.get_tensor_by_name(\"TrafficLight_1/detection_classes:0\")\r\n    num_detections_1 = merged_graph.get_tensor_by_name(\"TrafficLight_1/num_detections:0\")\r\n\r\n    detect_boxes_2   = merged_graph.get_tensor_by_name(\"TrafficLight_2/detection_boxes:0\")\r\n    detect_scores_2  = merged_graph.get_tensor_by_name(\"TrafficLight_2/detection_scores:0\")\r\n    detect_class_2   = merged_graph.get_tensor_by_name(\"TrafficLight_2/detection_classes:0\")\r\n    num_detections_2 = merged_graph.get_tensor_by_name(\"TrafficLight_2/num_detections:0\")\r\n\r\n\r\nwith tf.Session(graph=merged_graph) as sess:\r\n    with tf.device(\"/device:GPU:0\"):\r\n        input_image_1 = tf.random_normal(shape=[1,300,600,3],mean=0.0, stddev=1.0, dtype=tf.float32)\r\n        input_image_2 = tf.random_normal(shape=[1,300,600,3],mean=0.0, stddev=1.0, dtype=tf.float32)\r\n\r\n    sess.run(tf.global_variables_initializer())\r\n    feed_image_1, feed_image_2 = sess.run([input_image_1, input_image_2])\r\n\r\n    sess.run([detect_boxes_1,detect_scores_1,detect_class_1,num_detections_1,\r\n              detect_boxes_2,detect_scores_2,detect_class_2,num_detections_2],\r\n             feed_dict={image_tensor1: feed_image_1, image_tensor2: feed_image_2})\r\n```\r\n\r\n\r\nHere is the error:\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid loop structure: Loop \"Preprocessor/map/while/while_context\" has more than one LoopCond node: {{node TrafficLight_2/Preprocessor/map/while/LoopCond}} and {{node TrafficLight_1/Preprocessor/map/while/LoopCond}}. This is an internal bug, please file a bug report with instructions on how to reproduce the error.\r\n```\r\n", "@dcmrlee I have met with the same problem. You need to use the pb file frozen with TF_ENABLE_CONTROL_FLOW_V2=1 to make this work", "> @dcmrlee I have met with the same problem. You need to use the pb file frozen with TF_ENABLE_CONTROL_FLOW_V2=1 to make this work\r\n\r\nIn my script, I have already set the this env by \" os.environ['TF_ENABLE_CONTROL_FLOW_V2'] = \"1\" \".\r\n\r\nDo you mean set TF_ENABLE_CONTROL_FLOW_V2=1 when generate the frozen graph (pb file) ?", "Yes. I used this var when I was exporting the graph and importing the graph.", "> Yes. I used this var when I was exporting the graph and importing the graph.\r\n\r\nI tried your suggestion, but still not working... Could you share your code? @JunweiLiang ", "You can check out my repo where I have sucessfully used frozen graph for multi-gpu inferencing on tf 1.13.1: https://github.com/JunweiLiang/Object_Detection_Tracking", "> You can check out my repo where I have sucessfully used frozen graph for multi-gpu inferencing on tf 1.13.1: https://github.com/JunweiLiang/Object_Detection_Tracking\r\n\r\nThanks a lot. ", "Could you please try on latest stable version of TF 2.5.0 and let us know if this is still an issue.Thanks!\r\n\r\n", "I tested this issue with the following code from above (Tf Version 2.6.0) on Google Colab and also on Ubuntu 20.04. It didn't show any error. I replaced some methods with the tf.compat.v1 Version.\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'\r\n\r\nprint(\"TENSORFLOW VERSION:\", tf.__version__)\r\n\r\nprint(\"\\n\\n------------ CREATING TF-GRPAH USING MULTIPLE MAP_FN ------------\\n\\n\")\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=tf.compat.v1.GPUOptions(allow_growth=True))) as sess:\r\n        \r\n        assign_val = tf.compat.v1.placeholder(tf.float32, name='input_matrix', shape=(None,2,2))\r\n        scale_val = tf.compat.v1.placeholder(tf.float32, name='scales', shape=(None))\r\n        \r\n        T = tf.identity(tf.map_fn(lambda x: tf.convert_to_tensor([\r\n            [x[0,0], x[0,1],0],\r\n            [x[1,0], x[1,1],0],\r\n            [  0,      0,   1]], name = \"convert_t\"),assign_val, name = \"create_t_fn\"),name=\"T\")\r\n        \r\n        scale_map = tf.identity(tf.map_fn(lambda x: tf.convert_to_tensor([\r\n            [x, x, 1],\r\n            [x, x, 1],\r\n            [1, 1, 1]], dtype=tf.float32),scale_val, name = \"create_scale_fn\"),name=\"scale_map\")\r\n        \r\n        out = T*scale_map\r\n        tf.identity(out,\"out\")\r\n        \r\n        out = sess.run([\"out:0\"],{\"input_matrix:0\": np.ones((10,2,2)),\r\n                                  \"scales:0\": np.arange(10)\r\n                                  })\r\n \r\n        print(\"\\n\\n------------ FREEZING AND SAVING GRAPH ------------\\n\\n\")\r\n\r\n        frozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(\r\n            sess,\r\n            tf.compat.v1.get_default_graph().as_graph_def(),\r\n            output_node_names=[\"out\"])\r\n        \r\n        with open('test_graph.pb', 'wb') as f:\r\n            f.write(frozen_graph.SerializeToString())\r\n\r\nprint(\"\\n\\n------------ LAODING GRAPH DEF ------------\\n\\n\")\r\n\r\ntf.compat.v1.reset_default_graph()\r\ngpu_device = [0,1,2]\r\n\r\nwith tf.compat.v1.gfile.GFile('test_graph.pb','rb') as model_file:\r\n    graph_def = tf.compat.v1.GraphDef()\r\n    graph_def.ParseFromString(model_file.read())\r\n\r\ngraph = tf.Graph()\r\ntower_input_matrix_tensors = list()\r\ntower_input_sclaes_tensors = list()\r\ntower_ouput_tensors = list()\r\n\r\nprint(\"\\n\\n------------ CREATING TOWERS ON MULTIPLE GPU ------------\\n\\n\")\r\n\r\nwith graph.as_default():\r\n    for i, device in enumerate(gpu_device):\r\n        with tf.device('/device:GPU:%d'%device):\r\n            tf.import_graph_def(graph_def, name='tower_%d' % i)\r\n            with tf.compat.v1.variable_scope('tower_%d' % i):\r\n                print('               Placing tower_%d on /device:GPU:%d' %(i, device))\r\n\r\n                input_matrix_tensor = graph.get_tensor_by_name('tower_%d/input_matrix:0' % i)\r\n                input_scale_tensor = graph.get_tensor_by_name('tower_%d/scales:0' % i)\r\n                out_tensor = graph.get_tensor_by_name('tower_%d/out:0' % i)\r\n\r\n                tower_input_matrix_tensors.append(input_matrix_tensor)\r\n                tower_input_sclaes_tensors.append(input_scale_tensor)\r\n                tower_ouput_tensors.append(out_tensor)\r\n\r\ntf_config = tf.compat.v1.ConfigProto(allow_soft_placement=True, \r\n                            gpu_options=tf.compat.v1.GPUOptions(allow_growth=True))\r\nsess = tf.compat.v1.Session(graph=graph, config=tf_config)\r\nn_towers = len(gpu_device)\r\n\r\nprint(\"\\n\\n------------ PREPARING INPUTS TO EXECUTE TOWERS ------------\\n\\n\")\r\n\r\ninput_matrix = np.ones((10,2,2))\r\nscales = np.arange(10)\r\n\r\nmatrices_splits = np.array_split(input_matrix, n_towers)\r\nscales_split = np.array_split(scales, n_towers)\r\n\r\nfeed_dict = {}\r\nout_tensors = []\r\n\r\nfor t_idx, tower_data in enumerate(matrices_splits):\r\n    if len(tower_data) > 0:\r\n        feed_dict[tower_input_matrix_tensors[t_idx]] = tower_data\r\n        feed_dict[tower_input_sclaes_tensors[t_idx]] = scales_split[t_idx]\r\n        out_tensors.append(tower_ouput_tensors[t_idx])\r\n\r\n\r\nprint(\"\\n\\n------------ EXECUTING TOWERS ------------\\n\\n\")        \r\n        \r\nwith sess.graph.as_default():\r\n    tower_outs = sess.run(out_tensors,feed_dict)\r\n\r\nprint(np.concatenate(tower_outs))\r\n```", "@Imtinan1996 Could you please update as per the above[ comment](https://github.com/tensorflow/tensorflow/issues/26526#issuecomment-943278146) by using TF v2.6.0  and let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26526\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26526\">No</a>\n"]}, {"number": 26525, "title": "Failed to import TRTEngineOp", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.3\r\n- CUDA/cuDNN version:9.0\r\n- GPU model and memory:Tesla P4\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nFailed to load a tensorrt trt_converted saved model. \r\n```\r\n    meta_graph_def = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], model_path)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 269, in load\r\n    return loader.load(sess, tags, import_scope, **saver_kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 420, in load\r\n    **saver_kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 350, in load_graph\r\n    meta_graph_def, import_scope=import_scope, **saver_kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1457, in _import_meta_graph_with_return_elements\r\n    **kwargs))\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\r\n    return_elements=return_elements)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 399, in import_graph_def\r\n    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 159, in _RemoveDefaultAttrs\r\n    op_def = op_dict[node.op]\r\nKeyError: 'TRTEngineOp'\r\n\r\n```\r\nBut when I add extra import statements, the original code works. I must import tensorflow.contrib.tensorrt as trt, but this is a unused import for my code.\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n`tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], tensorrt_saved_mode_path)`\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@lu814478913 Could you provide more details on the bug and the steps leading to the bug? It would be helpful If you can share a code to reproduce the bug. Thanks! ", "Thanks  for your reply. I just use tf.saved_model.loader.load to load the tftrt converted model. But it does not work. I have to explicitly import tftrt to load model successfully.", "Hi @lu814478913,\r\n\r\nThat is the intended behavior. Because TRT support is optional when using TF GPU pip package, meaning you don't need to install TRT before you can use TF GPU. So, to avoid loading possibly non-existing TRT library we don't import tftrt when `import tensorflow` is called.\r\n\r\nIn the future when we switch to dynamic loading of TRT library this problem will be solved, but for now if we want to load/run a tftrt converted model we'll need to import the tftrt module in addition to `import tensorflow`.\r\n\r\nThanks.", "I got the same issue, how to solve it?\r\n\r\n> but for now if we want to load/run a tftrt converted model we'll need to import the tftrt module in addition to import tensorflow.\r\n\r\n@aaroey \r\nWhat should be done after importing tftrt module?", "@Eloring That should be the only extra step in your python script.", "I'll close this issue for now, please let me know if it still doesn't work.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26525\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26525\">No</a>\n", "add import: \r\n`from tensorflow.python.compiler.tensorrt import trt_convert as trt`\r\n\r\n"]}, {"number": 26524, "title": "Can't import tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Not sure (pip install tensorflow-gpu in conda prompt)\r\n- TensorFlow version: Latest stable release with GPU support \r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: conda with a self-created virtual environment\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 1070-Ti \r\n\r\n\r\n\r\n**Describe the problem**\r\nSuccessfully installed tensorflow and keras through pip install in conda prompt. But then failed to import tensorflow.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nimport tensorflow as tf\r\n\r\n**Any other info / logs**\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\.conda\\envs\\tensorflow\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\.conda\\envs\\tensorflow\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u7d44\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\n\r\n~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 from tensorflow._api.v1 import app\r\n\r\n~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\user\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\user\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\user\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\user\\.conda\\envs\\tensorflow\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\user\\.conda\\envs\\tensorflow\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u7d44\u3002\r\n\r\n", "comments": ["@HowardHChuang Please uninstall python and tensorflow and then install both using the instructions [here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12). Please let me know how it progresses. thanks!", "Hi, thanks for your prompt response. I fixed the issue by myself and can use tensorflow with GPU now. However, I get the following messages\r\nWARNING:tensorflow:From C:\\Users\\user\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n\r\nThis pops up after compiling\r\n#These lines establish the feed-forward part of the network used to choose actions\r\ninputs1 = tf.placeholder(shape=[1,16],dtype=tf.float32)\r\nW = tf.Variable(tf.random_uniform([16,4],0,0.01))\r\nQout = tf.matmul(inputs1,W)\r\npredict = tf.argmax(Qout,1)\r\n\r\nCan you please shed some lights on this?", "@HowardHChuang Check these resources [1](https://stackoverflow.com/questions/54685134/warning-from-tensorflow-when-creating-vgg16) and [2](https://github.com/tensorflow/tensorflow/issues/25578#issuecomment-462997000) to understand the warning. \r\n\r\nI am closing the issue. If you see any similar issue, please open a new issue. thanks!"]}, {"number": 26523, "title": "pointer checked before assigning", "body": "", "comments": []}, {"number": 26522, "title": "Update deprecation notice to tf.contrib.ffmpeg", "body": "This fix is related to #21549.\r\n\r\nThe tf.contrib.ffmpeg will be removed from 2.0 and support for audio and video formats are not in tensorflow-io. This PR updates the deprecation notice so that users could be redirected to related project.\r\n\r\nThis fix fixes #21549 .\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 26521, "title": "Add activation function", "body": "Added Swish activation function, along with tests. I'd love to know if I should make any changes or if it is good to merge\r\n\r\nReview pleasee @rthadur ????", "comments": ["What's the update on this? @rthadur @nataliaponomareva ", "Could you assign someone else to this @rthadur ???", "Gentle ping to inquire why there is no activity here @rthadur ???", "Up to @fchollet, but I think it should live in add-on for now and merge into core if it receives wide attention.", "Yes I agree, we should have this implementation in add ons.", "Please create pull requests through tensorflow addons. We would need to consolidate the API before moving it to tensorflow core."]}, {"number": 26520, "title": "Modification to tf.keras.activations.tanh().", "body": "learing", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26520) for more info**.\n\n<!-- need_sender_cla -->", "@aenjon please sign CLA", "@aenjon please rebase your branch", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Closing this PR as similar changes have been pushed here #26603"]}, {"number": 26519, "title": "Build Tensorflow 2.0 Alpha From Source", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0 alpha\r\n- Python version: 3.7.2\r\n- Bazel version (if compiling from source): 0.20\r\n- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.10.44.4)\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuilded Tensorflow 2.0 Alpha from source but tf does not enable eager by default.\r\nWhat should I do to enable eager by default\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel build --config=opt --config=noaws --config=nohdfs --config=noignite --config=nokafka --config=nonccl --config=nogcp //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n**Any other info / logs**\r\n\r\nimport tensorflow as tf\r\nIn[3]: tf.__version__\r\nOut[3]: '2.0.0-alpha0'\r\ntf.executing_eagerly()\r\nOut[4]: False\r\na = tf.ones(shape=(3, 2))\r\na\r\nOut[6]: <tf.Tensor 'ones:0' shape=(3, 2) dtype=float32>\r\n", "comments": ["I haven't tried the 2.0 alpha version of the source code, but I remember you need to pass `--config=v2` to enable 2.0 for master branch.  Maybe you could pass `--config=v2` to give it a try?", "Thanks, it worked", "Using `--config=v2` from master branch is building `1.13.1` version  `tensorflow-1.13.1-xxx.whl`. \r\nBuild command: \r\n`bazel --output_base=$tf_deps/ \\\r\n --config=v2 \\\r\n--copt=\"-O3\" --copt=\"-g\" -s -c opt \\\r\n//tensorflow/tools/pip_package:build_pip_package`\r\n\r\nAfter installing it also shows 1.13.1 when using. `tf.__version__`\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.__version__\r\n1.13.1\r\n```\r\n"]}, {"number": 26518, "title": "Enable int8 support for tile", "body": "While working on tf.repeat, noticed that tf.tile does not support tf.int8 yet. Since almost all other\r\ndtypes have been supported already (even with uint8/int16/int32/int64), think it makes sense to add int8 support for tile as well. (so that tf.repeat could use it).\r\n\r\nThis fix adds int8 support for tile on CPU.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Nagging Reviewer : You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 26517, "title": "Add tf.repeat support equivalent to numpy.repeat", "body": "(Most of the credit goes to RaggedTensor author, couldn't find the GitHub handle but the related commit is bad5d1a)\r\n\r\nThis PR tries to address the issue raised in #8246 to have tf.repeat equivalent to numpy.repeat.\r\n\r\nMultiple attempts have been made in the past. The previous PR #15224 add the support with C++ which was closed, as a python implementation relying on existing ops is more desirable.\r\n\r\nNow repeat has been added in ragged_util.py for RaggedTensor (the axis=None is not covered but this is fairly straight ward). The related commit is bad5d1a.\r\n\r\nThis PR\r\n- moves the repeat implementation in ragged_util.py to array_ops.py so that it is possible to get exposed.\r\n- adds additional logic to handle numpy.repeat with axis=None\r\n- expose repeat_with_axis in ragged_util.py again so that existing logic in ragged tensor does not break\r\n- expose `tf.repeat` in v1 and v2.\r\n\r\nThis fix fixes #8264.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["I would not be the right reviewer for this.\r\nYou also need to have a TF API owner review this change, I think", "@seanpmorgan I think this belongs in addons for now. WDYT?", "Initial Thoughts:\r\n1) Looks like a great contribution thanks @yongtang !\r\n2) I'm struggling to find a location for this in addons. A key aspect that we're trying to push is addons must match an API pattern that we support (e.g. layers, optimizers, etc.) or it belongs to a sub-package that a reputable contributor or organization maintains (e.g. seq2seq). Is there a sub-package we could imagine this living under? It seems like a primative tensor transformation... so unless we make a tfa.transform or something it might be a little tough.\r\n\r\n@karmel @facaiy thoughts?", "@seanpmorgan should this be tfa.manip.repeat so it can conceivably end up in tf.manip.repeat?\r\n", "> Looks like a great contribution\r\n\r\n+1, good job!\r\n\r\n> Is there a sub-package we could imagine this living under? It seems like a primative tensor transformation... so unless we make a tfa.transform or something it might be a little tough.\r\n\r\nYes, we need to create a new sub-package if we think addons should accept the contribution. But I'm not sure if it's appropriate to put primitive ops in addons, because the requirements in https://github.com/tensorflow/addons are:\r\n>  contributions that conform to **well-established API patterns**, but implement new functionality not available in core TensorFlow.   \r\n\r\n@karmel  @dynamicwebpaige  What do you think?", "Yeah, my high level thought is that `tfa.manip` may not be an easily maintainable subpackage as it could be hard to define that expected API structure. Not impossible though and I'd be open to a subpackage submission (Currently working on the process of doing such a thing).\r\n\r\nApologies for the tepid answer, but obviously Addons will be in a forever battle with scope creep.", "Maybe tf-text? I agree that it's not obvious that this fits inside the addons paradigm. Although, if it were an op for repeating with a python endpoint rather than python for the same, I guess that would be allowed.", "@yongtang gentle ping to check review comments.", "@yongtang could you please rebase?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26517) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26517) for more info**.\n\n<!-- ok -->", "Sorry for so many people getting pinned for review. I think GitHub was confused by the rebase and push. I have remove unneeded reviews and pushed again.", "Thanks @rmlarsen. The PR has been rebased with merge conflict addressed. Please take a look.", "@alextp This seems like a natural companion to tf.tile, so why not accept the PR as is?", "@rmlarsen no strong reason just trying to keep the API small and we couldn't determine quickly whether this was redundant. If you think it's useful I'll bring it up in the next API owners meeting to approve.", "@alextp I'm not 100% sure, so I'll defer to the tf-api owners. But since it doesn't involve any new kernels it seems like a handy utility compared to setting up, say, a convolution to achieve the same.", "@edloper Thanks for the review. The PR has been updated. Please take a look.", "@rthadur @alextp Previously, Kokoro CI test failed because of some API change in `test_util.TensorFlowTestCase`. \r\n\r\nI have fixed the build failure and all tests passes now, please take a look.\r\n\r\nSorry for not catching the test failure earlier.", "Merged internally , waiting for auto merge to happen.", "Can one of the admins verify this patch?", "Looks like the PR has been merged into master so I will close this PR. Thanks everyone for the help to make this happen \ud83d\udc4d \u2764\ufe0f \ud83c\udf89 !"]}, {"number": 26516, "title": "Session crashed for an unknown reason", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n \r\n import tensorflow as tf\r\n\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nAfter installing tensorflow 2.0 in colab,  on executing command 'import tensorflow as tf', I get and error message \"**Your session crashed for an unknown reason**.\"\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n    The code below is from the Colab tutorials for TF2.0 in Boosted Tree Models.\r\n\r\nfrom __future__ import absolute_import, division, print_function\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom IPython.display import clear_output\r\n!pip install tensorflow==2.0.0-alpha0\r\nimport tensorflow as tf\r\ntf.random.set_seed(123)\r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["<img width=\"1680\" alt=\"Screenshot 2019-03-10 at 5 21 16 PM\" src=\"https://user-images.githubusercontent.com/31190928/54084615-ee8e5880-4358-11e9-98e9-224b1cda667d.png\">\r\n@sta017 The code you provided works correctly when I tried executing it.", "As @Ayush517 mentioned, I don't see any issue? Only first line need to be updated as (from __future__ import absolute_import, division, print_function) shown in @Ayush517 screenshot. Thanks!\r\n\r\nI am closing this issue as it was not related to build/Installation or Bug/performance. Please post this kind of support questions in Stackoverflow. Thanks!"]}, {"number": 26515, "title": "RuntimeError: You must compile your model before using it.", "body": "System information\r\n\r\nOS Platform and Distribution : Windows 10\r\nTensorFlow installed from (source or binary): anaconda promp\r\nTensorFlow version (use command below): '1.8.0'\r\nPython version: 3.6\r\nBazel version (if compiling from source): na\r\nGCC/Compiler version (if compiling from source): na\r\nCUDA/cuDNN version: 9/na\r\nGPU model and memory: gtx1060\r\nExact command to reproduce: na\r\nAnaconda platform:yes\r\n\r\n\r\nI am trying to run the following model in eager mode  but I get and error that I need to compile my model although I have model.compile in my code\r\n\r\n```\r\n\r\n\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nfrom keras.datasets import cifar10\r\n\r\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n\r\n#one hot encoding\r\ny_train_one_hot=tf.keras.utils.to_categorical(y_train,num_classes=10)\r\ny_test_one_hot=tf.keras.utils.to_categorical(y_test,num_classes=10)\r\n\r\n#Transofrm them to a float32 type\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\n\r\n#normalize the images\r\nx_train /= 255\r\nx_test /= 255\r\n\r\n\r\ndef base_model_v2():\r\n\r\n    #input_layer = tf.keras.layers.Input(shape=(32, 32, 3), name=\"input_layer\")\r\n    num_classes=10\r\n    weight_decay = 1e-4\r\n    input_shape=x_train.shape[1:]\r\n    #\r\n    model = tf.keras.Sequential()\r\n    model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), \r\n                                     input_shape=input_shape))\r\n    model.add(tf.keras.layers.Activation('elu'))\r\n    model.add(tf.keras.layers.BatchNormalization())\r\n    model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\r\n    model.add(tf.keras.layers.Activation('elu'))\r\n    model.add(tf.keras.layers.BatchNormalization())\r\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\r\n    model.add(tf.keras.layers.Dropout(0.2))\r\n \r\n    model.add(tf.keras.layers.Conv2D(64, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\r\n    model.add(tf.keras.layers.Activation('elu'))\r\n    model.add(tf.keras.layers.BatchNormalization())\r\n    model.add(tf.keras.layers.Conv2D(64, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\r\n    model.add(tf.keras.layers.Activation('elu'))\r\n    model.add(tf.keras.layers.BatchNormalization())\r\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\r\n    model.add(tf.keras.layers.Dropout(0.3))\r\n \r\n    model.add(tf.keras.layers.Conv2D(128, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\r\n    model.add(tf.keras.layers.Activation('elu'))\r\n    model.add(tf.keras.layers.BatchNormalization())\r\n    model.add(tf.keras.layers.Conv2D(256, (3,3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\r\n    model.add(tf.keras.layers.Activation('elu'))\r\n    model.add(tf.keras.layers.BatchNormalization())\r\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\r\n    model.add(tf.keras.layers.Dropout(0.4))\r\n \r\n    model.add(tf.keras.layers.Flatten())\r\n    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\r\n    \r\n    \r\n    \r\n    return model\r\n \r\n\r\n    \r\nbatch_size = 256\r\n\r\nepochs = 50\r\ndatagen = tf.keras.preprocessing.image.ImageDataGenerator(\r\n    rotation_range=15,\r\n    width_shift_range=0.1,\r\n    height_shift_range=0.1,\r\n    horizontal_flip=True)\r\n\r\ndatagen.fit(x_train)\r\n\r\n\r\ncif10=base_model_v2()\r\n\r\n#opt_rms=tf.keras.optimizers.RMSprop(lr=0.001,decay=1e-6)\r\n\r\nopt=tf.train.AdamOptimizer(learning_rate=0.0001)\r\ncif10.compile(optimizer=opt,loss=\"categorical_crossentropy\", metrics=['accuracy'])\r\n\r\n\r\n    \r\n\r\n#Fit the model on the batches generated by datagen.flow().\r\nhistory2 = cif10.fit_generator(datagen.flow(x_train, y_train_one_hot,batch_size=batch_size),\r\n                               validation_data=(x_test,y_test_one_hot),epochs=epochs )\r\n```\r\n\r\n \r\n", "comments": ["The code snippet you provided is incomplete. Please provide bare minimal code snippet to reproduce the issue reported here. Thanks!", "@ymodak I have now updated the code snippet.Thanks for pointing out my incomplete code. ", "Thanks for the updated code snippet. I was able to execute your code successfully in TF 1.13.1 using eager mode. Apparently this issue is fixed in later TF versions.", "@ymodak I will update my version.Thanks for the solution"]}, {"number": 26514, "title": "Added description for softmax and sigmoid", "body": "It's my first contribution so I'm not sure whether everything is right the way i did it. Also I am not a native english speaker so maybe there are some spelling errors.", "comments": ["Thanks for the feedback @kyscg! I updated the descriptions with your suggestions.", "@aweers please resolve conflicts", "Hope, everything is ok now! ;)", "> Hope, everything is ok now! ;)\r\n\r\n@alextp can you please review ", "How to fix this error? Can't even see any details when clicking on 'Details'", "Thanks @alextp, I hope now everything is right... ;)"]}, {"number": 26513, "title": "Could not find any TPU devices on Colab using TF 2.0 Alpha", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab \r\n- TensorFlow installed from (source or binary): source using `pip`\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n\r\nError occurred when try to run the colab notebook shown in [TF 2.0 Alpha: Distributed Training in TensorFlow](https://www.tensorflow.org/alpha/guide/distribute_strategy) for TPUStrategy:\r\n\r\n```python\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\ntpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\n# output: RuntimeError: Could not find any TPU devices\r\n# (Detailed Error message shown below)\r\n```\r\n\r\nI had enabled Colab runtime to `TPU`, and even checked there indeed is a TPU available:\r\n\r\n```python\r\ndef check_tpu_statue():\r\n    import os\r\n    \r\n    if 'COLAB_TPU_ADDR' not in os.environ:\r\n      print('ERROR: Not connected to a TPU runtime')\r\n    else:\r\n      tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\n      print ('TPU address is', tpu_address)\r\n\r\ncheck_tpu_statue()\r\n# output: TPU address is grpc://10.70.191.234:8470\r\n``` \r\n\r\n**Describe the expected behavior**\r\n\r\nTPU devices can be found on Colab when runtime is changed to `TPU` and using:\r\n- `tf.tpu.experimental.initialize_tpu_system(resolver)`\r\n- `tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)`\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\n!pip install tensorflow-gpu==2.0.0-alpha0\r\nimport tensorflow as tf\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\ntpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```text\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-15-9ec182bf3b8d> in <module>()\r\n      1 resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n----> 2 tf.tpu.experimental.initialize_tpu_system(resolver)\r\n      3 tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system(cluster_resolver)\r\n     89     # pylint: enable=protected-access\r\n     90 \r\n---> 91     with ops.device(get_first_tpu_host_device(cluster_resolver)):\r\n     92       output = tpu_functional_ops.TPUPartitionedCall(\r\n     93           args=[], device_ordinal=0, Tout=[dtypes.string], f=func_name)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py in get_first_tpu_host_device(cluster_resolver)\r\n     41         [x for x in context.list_devices() if \"device:TPU:\" in x])\r\n     42     if not tpu_devices:\r\n---> 43       raise RuntimeError(\"Could not find any TPU devices\")\r\n     44     spec = tf_device.DeviceSpec.from_string(tpu_devices[0])\r\n     45     task_id = spec.task\r\n\r\nRuntimeError: Could not find any TPU devices\r\n```", "comments": ["@leemengtaiwan \r\n\r\n1. You'll need to connect to remote tpu host when using eager mode so something like\r\n```\r\ntf.config.experimental_connect_to_host(TPU_ADDRESS)\r\n```\r\nthis needs to happen before you initialize the device. \r\n\r\nPS: TPU support in 2.0 is still WIP but we're actively working on that right now. ", "Related to #24412"]}, {"number": 26512, "title": "Broken link in TF 2.0 Alpha: Distributed Training in TensorFlow documentation", "body": "**System information**\r\n- TensorFlow version: TF 2.0 Alpha\r\n- Doc Link: https://www.tensorflow.org/alpha/guide/distribute_strategy#examples_and_tutorials\r\n\r\n**Describe the documentation issue**\r\n\r\nThe second link (the `Tutorial` text shown below) in `Examples and Tutorials` section is missing:\r\n\r\n```text\r\n2. Tutorial to train Fashion MNIST with TPUStrategy (currently uses disable_eager_execution)\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/3454980/54072195-bdf7e180-42ba-11e9-9c09-3de5687cc016.png)\r\n\r\n", "comments": ["Still in progress, and thank you for letting us know! We will have a version of the TPU guide when we have complete support for TPUs in TF 2.0 (see #24412).", "@leemengtaiwan Looks like its fixed.\r\nCan you please let us know if you are happy to close if no issue persists. Thanks!", "These all work now: https://www.tensorflow.org/guide/distributed_training#examples_and_tutorials"]}, {"number": 26511, "title": "FailedPreconditionError ", "body": "Please help me out \r\n\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-27-72245a7797cb> in <module>()\r\n      6     #session.run(adapt_policy_mean, feed_dict=rl.feed_dict)\r\n      7     sess = tf.Session()\r\n----> 8     sess.run(adapt_policy_mean, feed_dict=rl.feed_dict)\r\n\r\n/home/ayan/slearn/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 26510, "title": "tf.print() support of `sep` and `end` arguments", "body": "**System information**\r\n- TensorFlow version (you are using):\r\ntf.version.GIT_VERSION: 'v1.12.0-9807-ga86f6286b4'\r\ntf.version.VERSION: '2.0.0-dev20190308'\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nRight now `tf.print()` does not support the `sep` and `end` arguments of Python's `print()` function.\r\n\r\n**Will this change the current api? How?**\r\nYes, it will add two more optional arguments to the `tf.print()` function.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who wants to use `tf.print()` just as naturally as Python's `print()` function.\r\n\r\n**Any Other info.**\r\nThe following code demonstrates how you could use this new feature to display progress during training:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Uncomment these lines to see the expected output\r\n#tf.print = print\r\n#tf.function = lambda f: f\r\n#tf.range = range\r\n\r\n@tf.function\r\ndef train(n_epochs, n_steps):\r\n    for epoch in tf.range(1, n_epochs + 1):\r\n        tf.print(\"Epoch \", epoch, \"/\", n_epochs, sep=\"\")\r\n        for step in tf.range(1, n_steps + 1):\r\n            tf.print(\"\\rStep \", step, \"/\", n_steps, sep=\"\", end=\"\")\r\n            # ... training loop\r\n        tf.print()\r\n\r\ntrain(n_epochs=10, n_steps=10000)\r\n```\r\n\r\nThe expected output looks like this:\r\n\r\n```\r\nEpoch 1/10\r\nStep 10000/10000\r\nEpoch 2/10\r\nStep 10000/10000\r\nEpoch 3/10\r\nStep 10000/10000\r\nEpoch 4/10\r\nStep 10000/10000\r\nEpoch 5/10\r\nStep 10000/10000\r\nEpoch 6/10\r\nStep 578/10000   <= in progress\r\n```", "comments": ["Closing this issue since the associated PR has been merged. Tested against tf-nightly-2.0-preview (2.0.0-dev20190605). Thanks!"]}, {"number": 26509, "title": "Made the variable const", "body": "Variables which are not modified are made const", "comments": ["\r\n\r\n\r\n@rthadur , i think the below error is not related to my changes, can you pls check and re trigger the build.\r\n\r\n> $TEST_TMPDIR defined: output root default is '/tmpfs/bazel_output' and max_idle_secs default is '15'.\r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> Traceback (most recent call last):\r\n>   File \"check_load_py_test.py\", line 91, in <module>\r\n>     main()\r\n>   File \"check_load_py_test.py\", line 85, in main\r\n>     '\\n'.join(files_missing_load)))\r\n> RuntimeError: The following files are missing load(\"//tensorflow:tensorflow.bzl\", \"py_test\").\r\n> This load statement is needed because otherwise pip tests will try to use their dependencies, which are not visible to them.:\r\n>  tensorflow/contrib/input_pipeline/BUILD\r\n> tensorflow/contrib/periodic_resample/BUILD\r\n> \r\n\r\nRegards\r\nAmit", "rthadur@, could you help take a look on why merging still blocked? Thanks!"]}, {"number": 26508, "title": "Improved tf.image.adjust_gamma for uint8 images", "body": "At the moment there is an inconsistency in the `adjust_*` functions of [`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image): While almost all of them work perfectly fine on either `float32` or `uint8` versions of the images by converting them internally at first to the `float32` representation and them applying the modification, the `adjust_gamma` does not behave in the same way. Instead of converting the image at first to `float32` version it immediately applies the transformation. This inconsistency might confuse users or cause an unwanted behavior.\r\n\r\n**Changes**:\r\n\r\n- inserted a type cast to `float32` and back to the original type like in the other `adjust_*` functions\r\n- updated docstring\r\n- extended unit tests", "comments": ["Are you still available to review this PR @drpngx or should someone else take over?", "Can we merge this now?", "@rthadur Is there something left for me to do or can this be merged into `master` now?"]}]