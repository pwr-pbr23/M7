[{"number": 53079, "title": "tflite::ops::builtin::BuiltinOpResolver resolver is causing crash in a webassembly application opened in Android chrome browser.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): Tensorflow lite (compiled for web assembly)\r\n- TensorFlow version (or GitHub SHA if from source): (2.5.0)\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\n  void init(const char *api_key)\r\n    {\r\n\r\n        cout << \"initilizing the init function\"<< endl;\r\n        const char *env = getenv(\"ENVIRONMENT\");\r\n        ENV = std::string(env);\r\n        sdk_status = statuscode->running;\r\n        \r\n        cout << \"initilizing step 2\"<< endl;\r\n\r\n    \r\n        sdk_status = statuscode->success;\r\n\r\n        auto landmark_predict_model_ = tflite::FlatBufferModel::BuildFromFile(\"data/face_detection_front_128x128_float32.tflite\");\r\n        if(!landmark_predict_model_){\r\n            cout <<  \"Failed to load model \" << endl;\r\n        }else{\r\n                cout <<  \"Model loaded \" << endl;\r\n        }\r\n        cout <<  \"start \" << endl;\r\n        // tflite::MutableOpResolver resolver;\r\n        // RegisterSelectedOps(&resolver);\r\n        tflite::ops::builtin::BuiltinOpResolver resolver;    //this is causing crash. \r\n        // std::unique_ptr<tflite::Interpreter> landmark_interpreter_;\r\n        // tflite::InterpreterBuilder(*landmark_predict_model_, resolver)(&landmark_interpreter_);\r\n        // landmark_interpreter_->AllocateTensors();\r\n        // landmark_interpreter_->SetAllowFp16PrecisionForFp32(true);\r\n        // landmark_interpreter_->SetNumThreads(4);\r\n\r\n        cout << \"initilizing step 3\"<< endl;\r\n\r\n    }\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\nmodel file: tflite model from [here](https://drive.google.com/uc?export=download&id=1DOEH2xS_bGVXEsriNJkKE3my62ZH3FUl)\r\n\r\n**Any other info / logs**\r\nThe crash is happening only in chrome running in mobile phones (tested on Android phone, Processor: snapdragon 710 and above). It is running properly in PC chrome browser. \r\n\r\n", "comments": ["@Jargon4072 Could you please try to execute your code on latest stable version of TF `2.7.0` and let us know if the issue still persists? Thank you!", "Hi @sushreebarsa. I will try it and let you know by tomorrow. Thank you! ", "@Jargon4072 Could you please let us know if you have tried with TF v2.7.0 ? Please let us know the outcome .Thank you!", "@sushreebarsa I am not able to build tflite for wasm. I have tried with version 2.6.1 (source : https://github.com/visualcamp/tensorflow) and it is still giving issues. I have tried bazel to build but there are two issues: 1. there is no proper instruction to build tflite for wasm, 2. if I am setting --config=wasm, it is giving an error that .bazelrc doesn't have any config for wasm. Can anyone please help me in builduing tf 2.7.0 for wasm?  "]}, {"number": 53069, "title": "full 8 bit quantization generates unreasonable quant_scale for uint8 on input tensor", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.6.0\r\n\r\n### 2. Code\r\n\r\n```\r\ndef representative_dataset():\r\n    for img in imgs: # img are shape (224,640,3)\r\n        data = np.expand_dims(img, axis=0)\r\n        yield [data.astype(np.float32)]\r\n        \r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model) # path to keras model\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8 \r\n#converter.experimental_new_quantizer = False\r\n\r\ntflite_quant_model = converter.convert()\r\nopen(f\"converted_model.tflite\", \"wb\").write(tflite_quant_model)\r\n```\r\nKeras Model: https://cloud.haw-hamburg.de/index.php/s/FPxeczAoaJJgMly (Password: tf)\r\nTensorflow Lite converted Model: https://cloud.haw-hamburg.de/index.php/s/m9RKnJdOtHg8K3I (Password: tf)\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\nThe conversion succeeds but the quant_scale provided by the 'input_details[\"quantization\"]' on the input tensor seems wrong. It is set to 0.029... which would map my float32 [0.0-1.0] images to [0-34] (zero_point = 0). That are a lot of details gone on the input, which makes the model perform bad.\r\n\r\nWhen I set the 'experimental_new_quantizer' to 'False', it geneartes 0.0039... for quant_scale and still 0 for zero_point. That would map my images to [0-255], but the invoke fails with an error, that the input tensor lacks data. \r\n\r\nTo my knowledge, the quant_scale is calculated by (max-min) / quant_range. quant_range is 255 for uint8. Where does the min max come from? from the representative dataset? I have checked that my representative dataset does not go below 0.0 or above 1.0.\r\n", "comments": ["The latest recommended way is to use tf.int instead of tf.uint, is there any specific reason you're using uint?", "just for convenience. If I convert my model using int8, I still get 0.027 as scale, just with -128 as zero point. That maps my float32 [0.0, 1.0] to [-128, -91]. I am wondering why I am getting these scales. A color depth of around 37 is not usable in my case. ", "When I try it on a different machine, I get a scale of 0.074 which is even worse. Any ideas? "]}, {"number": 53065, "title": "Change `min_epsilon` to 0", "body": "Compiling issue #37768 on epsilon condition as per users input `\"the value of epsilon is required to be greater or equal to CUDNN_BN_MIN_EPSILON which was defined in the cudnn.h file to the value 1e-5. This threshold value is now lowered to 0.0 to allow a wider range of epsilon value.` in this [document ](https://docs.nvidia.com/deeplearning/cudnn/release-notes/)", "comments": ["This is not what the issue meant. \r\nCuDNN doc says \"This threshold value is now lowered to 0.0 to allow a wider range of epsilon value\". So the correct fix is to change `min_epsilon` to 0.", "Still not following the comment review", "Hi @mihaimaruseac ! Shall we close this pull request? it has been going through a lot of loops for a long time now. I am afraid that it might disturb the peace of processes happening here.", "I don't have time to see why the internal tests are failing. If you can take a look and fix them that would be great."]}, {"number": 53063, "title": "xla cache .", "body": null, "comments": ["https://github.com/tensorflow/tensorflow/issues/52645", "I am a novice, and I don't know whether this pr is useful. so I simple change my code. If you think this pr is useful, I will detailly add this function and test . ", "@thomasjoerg Can you please review this PR ? Thanks!", "@thomasjoerg Can you please review this PR ? Thanks!", "I hope this fixes an OOM problem I am facing right now, when using XLA functions on CPU, thx @ all who are involved in fixing this :)"]}, {"number": 53062, "title": "r2.7 not recognizing old GPU with compute capability 3.0 (GTX 770) at runtime, while everything seems fine during build and installation.", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: \r\n    No, the only lines I'm trying are \r\n    ```\r\n    import tensorflow as tf\r\n    print(\"TensorFlow version:\", tf.__version__)\r\n    \r\n    tf.config.list_physical_devices('GPU')\r\n    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n    ```\r\n-   **OS Platform and Distribution**:\r\n    Linux Mint 20.02\r\n-   **TensorFlow installed from**:\r\n    Source\r\n-   **TensorFlow version**:\r\n    r2.7 (last official release branch up to date)\r\n-   **Python version**:\r\n    3.8\r\n-   **Bazel version**:\r\n    3.7.2\r\n-   **GCC/Compiler version**:\r\n    GCC 9.3.0\r\n-   **CUDA/cuDNN version**:\r\n    10.2 / 8.2.4.15 (all samples for both work without a flaw, except for half precision samples obviously failing, cc being 3.0). NVidia Driver Version is 440.33.01\r\n-   **GPU model and memory**:\r\n    Geforce GTX 770 2GB\r\n\r\n### Describe the problem\r\nAs stated in documentation, after installing Bazel (3.7.2) I ran the `.configure` with the cuda option enabled. It went like this:\r\n```\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is /home/alessandro/tensorflow/bin/python3]: \r\n\r\nFound possible Python library paths:\r\n  /home/alessandro/tensorflow/lib/python3.8/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/alessandro/tensorflow/lib/python3.8/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: \r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.2 in:\r\n    /usr/local/cuda-10.2/targets/x86_64-linux/lib\r\n    /usr/local/cuda-10.2/targets/x86_64-linux/include\r\nFound cuDNN 8 in:\r\n    /usr/local/cuda-10.2/targets/x86_64-linux/lib\r\n    /usr/local/cuda-10.2/targets/x86_64-linux/include\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 3.0\r\n\r\nWARNING: XLA does not support CUDA compute capabilities lower than 3.5. Disable XLA when running on older GPUs.\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: \r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n```\r\n\r\nAfter this, having found a **very** similar [issue](https://github.com/tensorflow/tensorflow/issues/27840), I always add to `.tf_configure`: `build:opt --copt=-DTF_EXTRA_CUDA_CAPABILITIES=3.0` (even if it seems present as a build `--action_env` option), and the two lines to exclude XLA: `build --define=with_xla_support=false` and `build --action_env TF_ENABLE_XLA=0`.\r\n\r\nMy `.tf_configure` looks like this:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/alessandro/tensorflow/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/alessandro/tensorflow/lib/python3.8/site-packages\"\r\nbuild --python_path=\"/home/alessandro/tensorflow/bin/python3\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda-10.2\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"3.0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/x86_64-linux-gnu-gcc-9\"\r\nbuild --config=cuda\r\n\r\nbuild --define=with_xla_support=false\r\nbuild --action_env TF_ENABLE_XLA=0\r\nbuild:opt --copt=-DTF_EXTRA_CUDA_CAPABILITIES=3.0\r\n\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-Wno-sign-compare\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_env=LD_LIBRARY_PATH\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only\r\n```\r\n\r\nThen, I simply run `bazel build //tensorflow/tools/pip_package:build_pip_package` and this goes on without any error or even warnings.\r\n\r\nI finally  ran\r\n```./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg```\r\n\r\nAnd even if with warnings, wheel gets built.\r\n```\r\nMon 15 Nov 2021 11:01:36 AM CET : === Preparing sources in dir: /tmp/tmp.AZIEudvT8k\r\n~/Downloads/tensorflow ~/Downloads/tensorflow\r\n~/Downloads/tensorflow\r\n~/Downloads/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow ~/Downloads/tensorflow\r\n~/Downloads/tensorflow\r\n/tmp/tmp.AZIEudvT8k/tensorflow/include ~/Downloads/tensorflow\r\n~/Downloads/tensorflow\r\nMon 15 Nov 2021 11:01:52 AM CET : === Building wheel\r\nwarning: no files found matching 'README'\r\nwarning: no files found matching '*.pyd' under directory '*'\r\nwarning: no files found matching '*.pyi' under directory '*'\r\nwarning: no files found matching '*.pd' under directory '*'\r\nwarning: no files found matching '*.dylib' under directory '*'\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\nwarning: no files found matching '*.csv' under directory '*'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*.proto' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/third_party'\r\n/home/alessandro/tensorflow/lib/python3.8/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\r\n  warnings.warn(\r\nMon 15 Nov 2021 11:02:19 AM CET : === Output wheel file is in: /tmp/tensorflow_pkg\r\n```\r\n\r\nand `pip install /tmp/tensorflow_pkg/tensorflow-2.7.0-cp38-cp38-linux_x86_64.whl` goes without a flaw:\r\n```\r\nProcessing /tmp/tensorflow_pkg/tensorflow-2.7.0-cp38-cp38-linux_x86_64.whl\r\nRequirement already satisfied: flatbuffers<3.0,>=1.12 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (2.0)\r\nRequirement already satisfied: libclang>=9.0.1 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (12.0.0)\r\nRequirement already satisfied: keras<2.8,>=2.7.0rc0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (2.7.0)\r\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.41.1)\r\nRequirement already satisfied: typing-extensions>=3.6.6 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (3.10.0.2)\r\nRequirement already satisfied: h5py>=2.9.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (3.5.0)\r\nRequirement already satisfied: keras-preprocessing>=1.1.1 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.1.2)\r\nRequirement already satisfied: termcolor>=1.1.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.1.0)\r\nRequirement already satisfied: wrapt>=1.11.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.13.3)\r\nRequirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (2.7.0)\r\nRequirement already satisfied: google-pasta>=0.1.1 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (0.2.0)\r\nRequirement already satisfied: protobuf>=3.9.2 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (3.19.1)\r\nRequirement already satisfied: tensorboard~=2.6 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (2.7.0)\r\nRequirement already satisfied: absl-py>=0.4.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.0.0)\r\nRequirement already satisfied: opt-einsum>=2.3.2 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (3.3.0)\r\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (0.22.0)\r\nRequirement already satisfied: numpy>=1.14.5 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.21.4)\r\nRequirement already satisfied: gast<0.5.0,>=0.2.1 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (0.4.0)\r\nRequirement already satisfied: astunparse>=1.6.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.6.3)\r\nRequirement already satisfied: six>=1.12.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.16.0)\r\nRequirement already satisfied: wheel<1.0,>=0.32.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (0.37.0)\r\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\r\nRequirement already satisfied: google-auth<3,>=1.6.3 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.3.3)\r\nRequirement already satisfied: setuptools>=41.0.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (58.3.0)\r\nRequirement already satisfied: werkzeug>=0.11.15 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.0.2)\r\nRequirement already satisfied: requests<3,>=2.21.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.26.0)\r\nRequirement already satisfied: markdown>=2.6.8 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.3.4)\r\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.0)\r\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\r\nRequirement already satisfied: rsa<5,>=3.1.4 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.7.2)\r\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.2.4)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\r\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2021.10.8)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.26.7)\r\nRequirement already satisfied: charset-normalizer~=2.0.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.0.7)\r\nRequirement already satisfied: idna<4,>=2.5 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (3.3)\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\r\nRequirement already satisfied: oauthlib>=3.0.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.1.1)\r\nInstalling collected packages: tensorflow\r\nSuccessfully installed tensorflow-2.7.0\r\n```\r\n\r\nEventually, I run my python test:\r\n```\r\nimport tensorflow as tf\r\nprint(\"TensorFlow version:\", tf.__version__)\r\n\r\ntf.config.list_physical_devices('GPU')\r\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n```\r\n\r\nAnd result is:\r\n```\r\nTensorFlow version: 2.7.0\r\nNum GPUs Available:  0\r\n2021-11-15 11:07:33.547632: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n2021-11-15 11:07:33.547653: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: alessandro-MS-7B79\r\n2021-11-15 11:07:33.547656: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: alessandro-MS-7B79\r\n2021-11-15 11:07:33.547715: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 440.33.1\r\n2021-11-15 11:07:33.547728: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 440.33.1\r\n2021-11-15 11:07:33.547732: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 440.33.1\r\n```\r\n\r\nI'm switching to Tensorflow since pyTorch can't seem to handle compute capability 3.0 while Tensorflow can. But can it?", "comments": ["Hi @AlessandroFlati ! As per [GPU documentation](https://www.tensorflow.org/install/gpu#hardware_requirements) , Minimum compute capability is 3.5. Thanks!", "Hi @mohantym and thanks for the quick reply.\r\nAs far as I understood, compute capability allowed is >= 3.5 for **binaries**, not for building from scratch. The same link I attached as the \"very similar issue\" talks about 3.0.\r\nMaybe you meant I have to drop the tensorflow version, or downgrade CUDA/Nvidia Drivers, but I don't expect all of a sudden that a perfectly working and CUDA/CUDNN capable GPU will work with NO version of Tensorflow.\r\n", "Ok @AlessandroFlati ! Could you try again with CUDA 11.2 and cuDNN 8.1 and GCC 7.3.1 as this [document ](https://www.tensorflow.org/install/source#gpu)suggests? Thanks!", "Considerations:\r\n- GCC 7.3.1 is nearly impossible to install in an Ubuntu system (it seems easy on RedHat with devtoolset-7, but I can't try rn), so I went for GCC 7.5.0.\r\n- After installing cuda and nvidia drivers, CUDA Samples compiled well, but generated runtime errors like `CUDA error at particleSystem_cuda.cu:121 code=13(cudaErrorInvalidSymbol) \"cudaMemcpyToSymbol(params, hostParams, sizeof(SimParams))\"` or `BlackScholes.cu(160) : getLastCudaError() CUDA error : BlackScholesGPU() execution failed\r\n : (209) no kernel image is available for execution on the device.` (before, those went all smooth). \r\nI didn't even compile Tensorflow, being this the situation; I believe these CUDA/cuDNN versions are just too much for my Geforce GTX 770. Since 10.2 / 8.2.4.15 were working well, as your document suggests maybe I should go for tensorflow **2.3** with `GCC 7.3.1 | Bazel 3.1.0 | cuDNN 7.6 | CUDA 10.1`.\r\nI'll try this asap.\r\n", "Update: CUDA 10.1 is not compatible with GCC 7.5 **or** (in a boolean sense) with my graphics card. The only combination that worked was GCC 8 and CUDA 10.2. I hope it doesn't make so much difference with the above combination.\r\nJust to be clear, I'm now testing\r\n```r2.3 | GCC 7.5 (for TF) / GCC 8.4 (for CUDA and cuDNN) | Bazel 3.1.0 | cuDNN 7.6 | CUDA 10.2```\r\nwhich seems to be the closest to the tested builds.\r\n", "Installation of CUDA and CUDNN went smoother than ever: cuDNN 7.6 seems to be even more fit to CUDA 10.2 + CC 3.0 than previous cuDNN 8.2, since it also passed half precision tests now.\r\n\r\nAlas, the `bazel build //tensorflow/tools/pip_package:build_pip_package` with `GCC 7.5` fails. It reveals many info/warnings while building with Bazel 3.1.0, though: nothing that seems too important, since most come from `-Wmaybe-uninitialized`, `-Wunused-dunction`, `-Wsign-compare`, `-Wcomment`, `-Wreturn-type` etc. and many other are just `it was declared here`, but some other seem to be a little more worrying, like `directory doesn't exist` for `protobuf` and a lot of warning for `external/eigen_archive/unsupported/Eigen/CXX11/` headers. Obviously, if you need it I can provide a complete log.\r\n\r\nI'll now try to recompile with GCC 8, so that next combination is:\r\n`r2.3 | GCC 8.4 (for CUDA, cuDNN and TF) | Bazel 3.1.0 | cuDNN 7.6 | CUDA 10.2`\r\n", "`GCC 8.4`, while compiling perfectly CUDA and cuDNN samples, gives about the same errors in TF 2.3; here's the log\r\n[tensorflow_r2.3withgcc8.log](https://github.com/tensorflow/tensorflow/files/7541337/tensorflow_r2.3withgcc8.log)\r\n\r\nTrying `GCC 9`\r\n```\r\nr2.3 | GCC 8.4 (for CUDA, cuDNN) / GCC 9 (for TF) | Bazel 3.1.0 | cuDNN 7.6 | CUDA 10.2\r\n```\r\ngets something predictable,\r\n```\r\nERROR: /home/alessandro/.cache/bazel/_bazel_alessandro/66cb6378d0a5667806d8c4794375ceb9/external/nccl_archive/BUILD.bazel:53:1: C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1)\r\nIn file included from /usr/local/cuda-10.2/bin/../targets/x86_64-linux/include/cuda_runtime.h:83,\r\n                 from <command-line>:\r\n/usr/local/cuda-10.2/bin/../targets/x86_64-linux/include/crt/host_config.h:138:2: error: #error -- unsupported GNU version! gcc versions later than 8 are not supported!\r\n  138 | #error -- unsupported GNU version! gcc versions later than 8 are not supported!\r\n      |  ^~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nI can't compile with GCC 9 something that comes on top of GCC8-built CUDA.\r\nSo, my last resort is to rise the release version, that is the reason I tried latest `r2.7` in the first place, with no success.\r\n\r\nWhat's your advice?", "Just to recap:\r\n\r\n| TF version | GCC version for TF | GCC version for CUDA and cuDNN | Bazel version | CUDA version | cuDNN version | CUDA Compiles | CUDA/cuDNN Samples work | TF Compiles |\r\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\r\n| x | x | GCC 8.4 | x | x | CUDA 11.2 | **Yes** | **Runtime errors on CUDA samples** | x | \r\n| x | x | GCC 8.4 | x | x | CUDA 10.1 | **No** | x | x |\r\n| r2.3 | GCC 9.3 | GCC 8.4 | Bazel 3.1.0 | cuDNN 7.6 | CUDA 10.2 | **Yes** | **Yes** | **No** |\r\n| r2.3 | GCC 8.4 | GCC 8.4 | Bazel 3.1.0 | cuDNN 7.6 | CUDA 10.2 | **Yes** | **Yes** | **No** |\r\n| r2.3 | GCC 7.5 | GCC 8.4 | Bazel 3.1.0 | cuDNN 7.6 | CUDA 10.2 | **Yes** | **Yes** | **No** |\r\n| r2.4 | GCC 8.4 | GCC 8.4 | Bazel 3.1.0 | cuDNN 7.6 | CUDA 10.2 | **Yes** | **Yes** | **No** |\r\n| r2.5 | GCC 8.4 | GCC 8.4 | Bazel 3.7.2 | cuDNN 8.2 | CUDA 10.2 | **Yes** | **Yes** | **Bazel fails with `no such package '@local_cuda//'`** |\r\n| r2.6 | GCC 8.4 | GCC 8.4 | Bazel 3.7.2 | cuDNN 7.6 | CUDA 10.2 | **Yes** | **Yes** | **TF _compiles_ but see below** |\r\n| r2.7 | GCC 8.4 | GCC 8.4 | Bazel 3.7.2 | cuDNN 7.6 | CUDA 10.2 | **Yes** | **Half precision samples of cuDNN fail** | **TF _compiles_, but gives runtime error** |\r\n\r\n### r2.6 Output\r\nOut of the `pip install`, the sample code \r\n\r\n```python\r\nimport tensorflow as tf\r\n```\r\n\r\ngave \r\n\r\n```python\r\nRuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\r\n```\r\n\r\nI then proceeded to `pip install numpy --upgrade`, and the error vanishes.\r\nThe following minimal sample code\r\n```python\r\nimport tensorflow as tf\r\nprint(\"TensorFlow version:\", tf.__version__)\r\ntf.config.list_physical_devices('GPU')\r\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n```\r\n\r\nproduces\r\n\r\n```python\r\nTensorFlow version: 2.6.2\r\n2021-11-16 10:45:04.371930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-16 10:45:04.377871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-16 10:45:04.378119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-16 10:45:04.378283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1885] Ignoring visible gpu device (device: 0, name: GeForce GTX 770, pci bus id: 0000:27:00.0, compute capability: 3.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\r\nNum GPUs Available:  0\r\n```\r\nJust to get rid of the non-fatal infos on NUMA, I used the famous\r\n```bash\r\nfor a in /sys/bus/pci/devices/*; do echo 0 | sudo tee -a $a/numa_node; done\r\n```\r\n\r\nand, in fact, I'm now left with an (incomprehensible)\r\n\r\n```python\r\nTensorFlow version: 2.6.2\r\nNum GPUs Available:  0\r\n2021-11-16 10:49:15.727029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1885] Ignoring visible gpu device (device: 0, name: GeForce GTX 770, pci bus id: 0000:27:00.0, compute capability: 3.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\r\n```\r\n\r\nI can't say for sure, but everything seems to point at the fact that the \"right\" CUDA/cuDNN/NVidia Drivers for my graphics card are 10.2/7.6/440.33.01. Given this, and the fact that its compute capabilities are 3.0, I can't understand why the `r2.3` and `r2.4` version won't compile and give all those warnings (I mean, knowing your team I know for sure that you wouldn't release something that out of the box has SO many warnings for a given compiler / settings).\r\n\r\nAlso, I would like to point out that `r2.6` `configure` script indicates **3.0** as the **minimum compute compatibility**, which is not aligned to what the runtime says in the above message.\r\n\r\nI'm in your hands now.", "Hi @Saduf2019! Could you please look at this issue! ", "Going to the root of the problem (`tensorflow/core/common_runtime/gpu/gpu_device.cc:1885`), it seems that 3.0 gets not added to the `cuda_supported_capabilities`.\r\nThis probably comes from the fact that the function\r\n\r\n```c++\r\nstd::vector<se::CudaComputeCapability> GetSupportedCudaComputeCapabilities() {\r\n  std::vector<se::CudaComputeCapability> cuda_caps = {\r\n      ComputeCapabilityFromString(\"3.5\"), ComputeCapabilityFromString(\"5.2\")};\r\n#ifdef TF_EXTRA_CUDA_CAPABILITIES\r\n// TF_EXTRA_CUDA_CAPABILITIES should be defined a sequence separated by commas,\r\n// for example:\r\n//   TF_EXTRA_CUDA_CAPABILITIES=3.0,4.0,5.0\r\n// Use two-level macro expansion for stringification.\r\n#define TF_XSTRING(...) #__VA_ARGS__\r\n#define TF_STRING(s) TF_XSTRING(s)\r\n  string extra_cuda_caps = TF_STRING(TF_EXTRA_CUDA_CAPABILITIES);\r\n#undef TF_STRING\r\n#undef TF_XSTRING\r\n  auto extra_capabilities = str_util::Split(extra_cuda_caps, ',');\r\n  for (const auto& capability : extra_capabilities) {\r\n    cuda_caps.push_back(ComputeCapabilityFromString(capability));\r\n  }\r\n#endif\r\n  return cuda_caps;\r\n}\r\n#endif  // GOOGLE_CUDA\r\n```\r\n\r\nsets the minimum default as 3.5 (as opposed to configure, saying it is 3.0), and searches for other capabilities under the CMake(/environment I guess) variable `TF_EXTRA_CUDA_CAPABILITIES`, and not `TF_CUDA_COMPUTE_CAPABILITIES` as generated by `./configure`.\r\nThen again, I explicitely added `build:opt --copt=-DTF_EXTRA_CUDA_CAPABILITIES=3.0`, but probably it never reaches this part of the code (I guess why). I'll try and debug this part, but maybe it's just simpler to align the minimum default capability?", "Adding by hand the extra compatibility to that piece of code\r\n\r\n```c++\r\n  std::vector<se::CudaComputeCapability> cuda_caps = {\r\n      ComputeCapabilityFromString(\"3.0\"), ComputeCapabilityFromString(\"3.5\"), ComputeCapabilityFromString(\"5.2\")};\r\n```\r\n\r\ncaused no problem whatsoever in the compile, and a sample code of\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nprint(\"TensorFlow version:\", tf.__version__)\r\n\r\ntf.config.list_physical_devices('GPU')\r\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n\r\npredictions = model(x_train[:1]).numpy()\r\ntf.nn.softmax(predictions).numpy()\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nloss_fn(y_train[:1], predictions).numpy()\r\nmodel.compile(optimizer='adam',\r\n              loss=loss_fn,\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\nmodel.evaluate(x_test,  y_test, verbose=2)\r\n\r\nprobability_model = tf.keras.Sequential([\r\n  model,\r\n  tf.keras.layers.Softmax()\r\n])\r\n\r\nprobability_model(x_test[:5])\r\n\r\n```\r\n\r\nran without an issue, with complete output\r\n\r\n```python\r\nTensorFlow version: 2.6.2\r\nNum GPUs Available:  1\r\n2021-11-16 15:17:02.968210: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-11-16 15:17:03.330760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1183 MB memory:  -> device: 0, name: GeForce GTX 770, pci bus id: 0000:27:00.0, compute capability: 3.0\r\n2021-11-16 15:17:03.887366: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\r\nEpoch 1/5\r\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.2942 - accuracy: 0.9148\r\nEpoch 2/5\r\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.1406 - accuracy: 0.9578\r\nEpoch 3/5\r\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.1056 - accuracy: 0.9687\r\nEpoch 4/5\r\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.0852 - accuracy: 0.9733\r\nEpoch 5/5\r\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.0728 - accuracy: 0.9782\r\n313/313 - 0s - loss: 0.0709 - accuracy: 0.9790\r\n\r\nProcess finished with exit code 0\r\n```\r\n\r\nNow - how can I be sure that TF is actually using my GPU and that I just didn't deactivate a right alarm? Is there some kind of check on tensors like `is_on_gpu()` or something similar?", "[Apparently yes](https://www.tensorflow.org/tutorials/customization/basics#gpu_acceleration)\r\n\r\nThank you all for your... support. I would suggest to fix both documentation and `tensorflow/core/common_runtime/gpu/gpu_device.cc` adding `CC 3.0`/`CUDA 10.2`/`CUDNN 7.6` to `r2.6` (and, who knows, maybe also `r2.7`) compatibility.", "I can confirm, also r2.7.", "I can confirm the issue exists, as I manually compiled Tensorflow 2.3.0 against CUDA 10.1 and cuDNN v7, and produces the same runtime ComputeCapability Error (3.0 < 3.5) mentioned above. ", "Reproduced in r2.3.4 and fixed using the method proposed by @AlessandroFlati. Hope this fix is going to be merged in upcoming versions. ", "> Adding by hand the extra compatibility to that piece of code\r\n> \r\n> ```c++\r\n>   std::vector<se::CudaComputeCapability> cuda_caps = {\r\n>       ComputeCapabilityFromString(\"3.0\"), ComputeCapabilityFromString(\"3.5\"), ComputeCapabilityFromString(\"5.2\")};\r\n> ```\r\n\r\nStill work on r2.7.0. CUDA10.1/cudnn8.0.5 NV GT750M\r\nBig thanks."]}, {"number": 53047, "title": "Android: GPU delegate fails with a YOLOv4 model", "body": "#### System Information\r\n\r\n* Custom code: none, using the upstream benchmark\r\n* OS: Android 12\r\n* Device: Google Pixel 4a\r\n* TensorFlow version: nightly release benchmark build (definite version unspecified in the URL)\r\n\r\n#### Steps to Reproduce\r\n\r\n1. Enable developer options and USB debugging.\r\n2. Execute the script below \u2014\u00a0it will download the TF benchmark and a model.\r\n\r\nPlease note that we have an internal YOLOv4 model we cannot share. I\u2019ve found an existing one. However, the result is more or less the same so I\u2019m gonna guess something is up with the model architecture. Also \u2014\u00a0the model executes fine on CPU / NPU.\r\n\r\nResults:\r\n\r\n* Expected: no error messages.\r\n* Actual: a lot of error messages \u2014\u00a0seem to be an error for each model node. See the output below.\r\n\r\n```bash\r\n#!/bin/bash\r\nset -eou pipefail\r\n\r\n\r\nMODEL_FILE_NAME=\"model.tflite\"\r\n\r\nBENCHMARK_PATH=\"$(mktemp -d)\"\r\nBENCHMARK_FILE_NAME=\"tensorflow-benchmark\"\r\n\r\nDEVICE_PATH=\"/data/local/tmp\"\r\n\r\n\r\necho \":: Fetch benchmark...\"\r\ncurl \\\r\n  --location \"https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model\" \\\r\n  --output \"${BENCHMARK_PATH}/${BENCHMARK_FILE_NAME}\"\r\n\r\necho \":: Fetch model...\"\r\ncurl \\\r\n  --location \"https://github.com/theAIGuysCode/tensorflow-yolov4-tflite/raw/master/android/app/src/main/assets/yolov4-416-fp32.tflite\" \\\r\n  --output \"${MODEL_FILE_NAME}\"\r\n\r\n\r\necho \":: Move benchmark to the device...\"\r\nadb push \"${BENCHMARK_PATH}/${BENCHMARK_FILE_NAME}\" \"${DEVICE_PATH}\"\r\nadb shell chmod +x \"${DEVICE_PATH}/${BENCHMARK_FILE_NAME}\"\r\n\r\necho \":: Move model to the device...\"\r\nadb push \"${MODEL_FILE_NAME}\" \"${DEVICE_PATH}\"\r\n\r\necho \":: Run benchmark...\"\r\nadb shell taskset f0 \"${DEVICE_PATH}/${BENCHMARK_FILE_NAME}\" \\\r\n  --graph=\"${DEVICE_PATH}/${MODEL_FILE_NAME}\" \\\r\n  --use_gpu=true\r\n\r\necho \":: Remove benchmark...\"\r\nadb shell rm \"${DEVICE_PATH}/${BENCHMARK_FILE_NAME}\"\r\nrm -rf \"${BENCHMARK_PATH}\"\r\n\r\necho \":: Remove model...\"\r\nadb shell rm \"${DEVICE_PATH}/${MODEL_FILE_NAME}\"\r\nrm -rf \"${MODEL_FILE_NAME}\"\r\n```\r\n```\r\n:: Fetch benchmark...\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100 6029k  100 6029k    0     0  11.8M      0 --:--:-- --:--:-- --:--:-- 11.8M\r\n:: Fetch model...\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100   196  100   196    0     0    336      0 --:--:-- --:--:-- --:--:--   335\r\n100 23.1M  100 23.1M    0     0  8255k      0  0:00:02  0:00:02 --:--:-- 23.7M\r\n:: Move benchmark to the device...\r\n/var/folders/d8/zmkczjms4jxbtbw24wt7qzbw0000gp/T/tmp.Yg5JHGh1/tens...ark: 1 file pushed, 0 skipped. 99.8 MB/s (6174376 bytes in 0.059s)\r\n:: Move model to the device...\r\nmodel.tflite: 1 file pushed, 0 skipped. 36.6 MB/s (24279948 bytes in 0.632s)\r\n:: Run benchmark...\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [/data/local/tmp/model.tflite]\r\nUse gpu: [1]\r\nLoaded model /data/local/tmp/model.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nGPU delegate created.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nINFO: Replacing 144 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\nExplicitly applied GPU delegate, and the model graph will be completely executed by the delegate.\r\nThe input model file size (MB): 24.2799\r\nInitialized session in 2394.17ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\nERROR: TfLiteGpuDelegate Invoke: Given object is not valid\r\nERROR: Node number 144 (TfLiteGpuDelegateV2) failed to invoke.\r\nERROR: TfLiteGpuDelegate Invoke: Given object is not valid\r\nERROR: Node number 144 (TfLiteGpuDelegateV2) failed to invoke.\r\nERROR: TfLiteGpuDelegate Invoke: Given object is not valid\r\nERROR: Node number 144 (TfLiteGpuDelegateV2) failed to invoke.\r\nERROR: TfLiteGpuDelegate Invoke: Given object is not valid\r\nERROR: Node number 144 (TfLiteGpuDelegateV2) failed to invoke.\r\nERROR: TfLiteGpuDelegate Invoke: Given object is not valid\r\nERROR: Node number 144 (TfLiteGpuDelegateV2) failed to invoke.\r\nERROR: TfLiteGpuDelegate Invoke: Given object is not valid\r\n>>> THIS CONTINUES FOR A WHILE <<<\r\ncount=852 first=237 curr=175 min=17 max=381 avg=176.995 std=47\r\n\r\nBenchmarking failed.\r\n```", "comments": ["@arturdryomov \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "@sushreebarsa, added a bit more relevant information.", "@arturdryomov \r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset , tensorflow version you are using. Thanks!\r\n", "@sushreebarsa, PTAL at the original issue. It contains the script downloading the official TensorFlow benchmark and the model. There is no custom code / project needed to reproduce the issue.", "I have the same issue on tfLite 2.8.0,  target sdk 32. So far the only workaround I have is to downgrade tensorflow to 2.5.0 if I would like to keep GPU support for yolo v4 , otherwise I can go with tf 2.8.0.\r\n\r\nIt would be great if someone would develop a more future ready solution"]}, {"number": 53025, "title": "Issue with conversion of dilated Convolutions #29509 still happening in version 2.6.0", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.6.0\r\n\r\n### 2. Code\r\nStandalone code to reproduce the issue - \r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow_model_optimization.python.core.quantization.keras import quantize\r\nfrom tensorflow.python import keras\r\n\r\nl = tf.keras.layers\r\n\r\ntf.config.run_functions_eagerly(True)\r\n\r\ndef functional_model():\r\n    \"\"\"Builds an MNIST functional model.\"\"\"\r\n    inp = tf.keras.Input(shape=image_input_shape())\r\n    x = l.Conv2D(filters=32, kernel_size=5, padding='same', activation='relu',\r\n                 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(inp)\r\n    x = l.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\r\n    # TODO(pulkitb): Add BatchNorm when transformations are ready.\r\n    # x = l.BatchNormalization()(x)\r\n    x = l.Conv2D(filters=64, kernel_size=5, padding='same', activation='relu',\r\n                 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(x)\r\n    x = l.Conv2D(filters=64, kernel_size=3, dilation_rate=(3, 3), padding='same', activation='relu',\r\n                 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(x)\r\n    x = l.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\r\n    x = l.Flatten()(x)\r\n    x = l.Dense(1024, activation='relu')(x)\r\n    x = l.Dropout(0.4)(x)\r\n    out = l.Dense(10, activation='softmax')(x)\r\n\r\n    return tf.keras.Model(inp, [out])\r\n\r\n\r\ndef image_input_shape(img_rows=28, img_cols=28):\r\n    if tf.keras.backend.image_data_format() == 'channels_first':\r\n        return 1, img_rows, img_cols\r\n    else:\r\n        return img_rows, img_cols, 1\r\n\r\n\r\ndef preprocessed_data(img_rows=28,\r\n                      img_cols=28,\r\n                      num_classes=10):\r\n    \"\"\"Get data for mnist training and evaluation.\"\"\"\r\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n\r\n    if tf.keras.backend.image_data_format() == 'channels_first':\r\n        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n    else:\r\n        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n\r\n    x_train = x_train.astype('float32')\r\n    x_test = x_test.astype('float32')\r\n    x_train /= 255\r\n    x_test /= 255\r\n\r\n    # convert class vectors to binary class matrices\r\n    y_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\n    y_test = tf.keras.utils.to_categorical(y_test, num_classes)\r\n\r\n    return x_train, y_train, x_test, y_test\r\n\r\n\r\nmodel = functional_model()\r\nmodel.summary()\r\nx_train, y_train, x_test, y_test = preprocessed_data()\r\n\r\nmodel.compile(\r\n    loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, batch_size=500)\r\n_, model_accuracy = model.evaluate(x_test, y_test, verbose=0)\r\n\r\nprint(\"Quantizing model\")\r\n\r\nquantized_model = quantize.quantize_model(model)\r\nquantized_model.compile(\r\n    loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\n\r\nquantized_model.fit(x_train, y_train, batch_size=500)\r\n_, quantized_model_accuracy = quantized_model.evaluate(\r\n    x_test, y_test, verbose=0)\r\nmodel.save(\"/home/anurag/git/train_data/testOrig.h5\")\r\nquantized_model.save(\"/home/anurag/git/train_data/test.h5\")\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(quantized_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\nconverter.change_concat_input_ranges = True\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntfliteModel = converter.convert()\r\nwith open(\"/home/anurag/git/train_data/test.tflite\", 'wb') as outfile:\r\n    outfile.write(tfliteModel)\r\n```\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\n- The latency in model is high due to the fact that atrous convolution is being broken down to spacetodepth, conv2d and depth to space and then does not apply quantization to conv2d.\r\n- Model produces correct results, but it is slower than expected.\r\n\r\nThis is the same as #29509. Issue was solved but appears again on newer releases.", "comments": ["@jvishnuvardhan  As discussed in #52290 created a new issue here with standalone code.", "@nutsiepully any updates on this issue?", "Hello, I find myself having exactly the same issue as described above. Are there any updates on the status of this issue?\r\n\r\nThank you", "The same issue still exists in versions 2.6.3 and 2.7.1 too.", "Our team is facing similar issue, is there some solution to this yet", "Facing same issue quantizing  for Android deployment.   \r\n\r\nAny update on atrous convolution support? ", "Hello. I have tried to quantize model (TensorFlow: 2.6.0,  Ubuntu: 20.04) and got the same issue - model inference is slower than expected. Are there any updates? ", "It seems that it is still an issue (tf 2.6.1, ubuntu 18.04). Could you please provide some updates on it?", "Hello, would appreciate an update on this issue, facing the same problem , inference is quite slow after quantization (ubuntu 18.04, TF 2.6.0)", "Have the same issue("]}, {"number": 53012, "title": "Update autoclustering_xla.ipynb", "body": "Changed `lr` to `learning_rate`.\r\nRemoves a deprecation warning. Tested the [updated .ipynb](https://colab.research.google.com/gist/chunduriv/47c56142de41dcd6c7f2b54ac293197e/lr.ipynb) file.", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/53012\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>"]}, {"number": 53011, "title": "[TFLite] Add int16x8 support for DEPTH_TO_SPACE and SPACE_TO_DEPTH operators", "body": "Hello,\r\n\r\nThis PR adds int16x8 support for DEPTH_TO_SPACE and SPACE_TO_DEPTH operators.\r\n\r\nThanks,\r\nJohan.", "comments": ["@miaout17 Can you please review this PR ? Thanks!"]}, {"number": 53005, "title": "Update `custom_gradient.py`: fix comment w.r.t `gradients` now being `GradientTape`", "body": "In TF 2.x 'gradients' should be replaced with 'GradientTape'. Hence made necessary changes. Please refer updated [gist](https://colab.research.google.com/gist/RenuPatelGoogle/8641fd64952c2febb8038a5fb2e1d51a/untitled30.ipynb).", "comments": []}, {"number": 53002, "title": "TF2.6 compile error with custom eigen repo", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.6\r\n- Python version: Python 3.6.8 \r\n- Installed using virtualenv? pip? conda?: pip \r\n- Bazel version (if compiling from source): bazel 3.7.2\r\n- GCC/Compiler version (if compiling from source): gcc version 6.5.1 20190307\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nHi, guys,\r\nI want to compile tensorflow r2.6 with a custom eigen. But it occurs errors. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n**TF r2.6 is compiled successfully.**\r\n```\r\n#bazel build  //tensorflow/tools/pip_package:build_pip_package\r\n...\r\nINFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (430 packages loaded, 26907 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 634.229s, Critical Path: 344.77s\r\nINFO: 7345 processes: 387 internal, 6958 local.\r\nINFO: Build completed successfully, 7345 total actions\r\n```\r\n\r\nBased on the same environment, I modified  eigen `workspace.bzl` of TF r26.\r\n```\r\n--- a/third_party/eigen3/workspace.bzl\r\n+++ b/third_party/eigen3/workspace.bzl\r\n@@ -9,13 +9,19 @@ def repo():\r\n     EIGEN_COMMIT = \"12e8d57108c50d8a63605c6eb0144c838c128337\"\r\n     EIGEN_SHA256 = \"f689246e342c3955af48d26ce74ac34d21b579a00675c341721a735937919b02\"\r\n \r\n-    tf_http_archive(\r\n-        name = \"eigen_archive\",\r\n-        build_file = \"//third_party/eigen3:eigen_archive.BUILD\",\r\n-        sha256 = EIGEN_SHA256,\r\n-        strip_prefix = \"eigen-{commit}\".format(commit = EIGEN_COMMIT),\r\n-        urls = [\r\n-            \"https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen/-/archive/{commit}/eigen-{commit}.tar.gz\".format(commit = EIGEN_COMMIT),\r\n-            \"https://gitlab.com/libeigen/eigen/-/archive/{commit}/eigen-{commit}.tar.gz\".format(commit = EIGEN_COMMIT),\r\n-        ],\r\n-    )\r\n+    native.new_local_repository(\r\n+       name = \"eigen_archive\",\r\n+       path = \"/mnt/eigen\",\r\n+       build_file = \"//third_party/eigen3:eigen_archive.BUILD\",\r\n+    )\r\n```\r\n\r\nThen I checkout the eigen repo to the same commit id `12e8d57108c50d8a63605c6eb0144c838c128337`\r\n```\r\n$ cd /mnt/eigen\r\n$ git remote -vvv\r\norigin  https://gitlab.com/libeigen/eigen.git (fetch)\r\norigin  https://gitlab.com/libeigen/eigen.git (push)\r\n$ git checkout -b current 12e8d57108c50d8a63605c6eb0144c838c128337\r\n```\r\nBut got an error:\r\n```\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (430 packages loaded, 26907 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /tensorflow/tensorflow/core/grappler/costs/BUILD:67:11: C++ compilation of rule '//tensorflow/core/grappler/costs:graph_properties' failed (Exit 1): gcc failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib64::/usr/lib/python2.7/site-packages/tensorflow \\\r\n    PATH=/opt/zookeeper/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/tops/bin:/opt/zookeeper/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/tops/bin:/usr/X11R6/bin:/opt/satools \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/tops/bin/python3 \\\r\n    PYTHON_LIB_PATH=/home/tops/lib/python3.6/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/k8-opt/bin/tensorflow/core/grappler/costs/_objs/graph_properties/graph_properties.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/grappler/costs/_objs/graph_properties/graph_properties.pic.o' -fPIC -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote. -iquotebazel-out/k8-opt/bin -iquoteexternal/com_google_absl -iquotebazel-out/k8-opt/bin/external/com_google_absl -iquoteexternal/nsync -iquotebazel-out/k8-opt/bin/external/nsync -iquoteexternal/eigen_archive -iquotebazel-out/k8-opt/bin/external/eigen_archive -iquoteexternal/gif -iquotebazel-out/k8-opt/bin/external/gif -iquoteexternal/libjpeg_turbo -iquotebazel-out/k8-opt/bin/external/libjpeg_turbo -iquoteexternal/com_google_protobuf -iquotebazel-out/k8-opt/bin/external/com_google_protobuf -iquoteexternal/com_googlesource_code_re2 -iquotebazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquoteexternal/farmhash_archive -iquotebazel-out/k8-opt/bin/external/farmhash_archive -iquoteexternal/fft2d -iquotebazel-out/k8-opt/bin/external/fft2d -iquoteexternal/highwayhash -iquotebazel-out/k8-opt/bin/external/highwayhash -iquoteexternal/zlib -iquotebazel-out/k8-opt/bin/external/zlib -iquoteexternal/double_conversion -iquotebazel-out/k8-opt/bin/external/double_conversion -iquoteexternal/snappy -iquotebazel-out/k8-opt/bin/external/snappy -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/grappler/costs/graph_properties.cc -o bazel-out/k8-opt/bin/tensorflow/core/grappler/costs/_objs/graph_properties/graph_properties.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\ntensorflow/core/grappler/costs/graph_properties.cc: In member function 'tensorflow::Status tensorflow::grappler::{anonymous}::DisjointSet<Handle>::Merge(Handle, Handle)':\r\ntensorflow/core/grappler/costs/graph_properties.cc:236:30: error: type/value mismatch at argument 1 in template parameter list for 'template<class> struct std::rank'\r\n   if (x_root->rank < y_root->rank) {\r\n                              ^~~~\r\ntensorflow/core/grappler/costs/graph_properties.cc:236:30: note:   expected a type, got 'y_root->.rank'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 74.775s, Critical Path: 63.70s\r\nINFO: 457 processes: 97 internal, 360 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nIt really confused me. The error occurs in `tensorflow/core/grappler/costs/graph_properties.cc`.\r\nAny response is welcomed. Thank you.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@axiqia,\r\nPlease post this issue on Eigen repo for faster resolution  https://gitlab.com/libeigen/eigen/issues. "]}, {"number": 52992, "title": "Providing OS thread ids when tracing", "body": "**System information**\r\n\r\n - Debian 9\r\n - python 3.5\r\n - TensorFlow 2.0.0\r\n\r\n**Are you willing to contribute it (Yes/No):**\r\n\r\nYes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, tracing only reports internal ids for threads:\r\n\r\n```json\r\n{\r\n    \"ph\": \"t\",\r\n    \"cat\": \"DataFlow\",\r\n    \"name\": \"bert/encoder/layer_0/output/dense/bias\",\r\n    \"pid\": 1,\r\n    \"tid\": 5,\r\n    \"ts\": 1635482148893406,\r\n    \"id\": 1\r\n}\r\n```\r\n\r\nIt would helpful to be able to also report the OS thread id. Unfortunately, `threading.enumerate` does not appear to report these threads. Examining the runtime from the OS also does not have enough information to identify which is the executing thread.\r\n\r\n**Will this change the current api? How?**\r\n\r\nI am not quite sure of all the implications but I am imagining a boolean argument to enable collection of the ids:\r\n\r\n```python\r\n# hook for estimator\r\nhook = tf.estimator.ProfilerHook(save_secs=1, output_dir=output, os_ids=True)\r\n\r\n# tensorboard callback\r\ncallback = tf.keras.callbacks.TensorBoard(log_dir = logs,\r\n                                                 histogram_freq = 1,\r\n                                                 profile_batch = '500,520',\r\n                                                 os_ids=True)\r\n```\r\n\r\nIn addition, downstream concurrent code such as threads, pools, etc would need to changed to grab the id, probably with `getppid`. Finally, the tracing data structures need to be updated to include this field. The new trace output could look something like:\r\n\r\n```json\r\n{\r\n    \"ph\": \"t\",\r\n    \"cat\": \"DataFlow\",\r\n    \"name\": \"bert/encoder/layer_0/output/dense/bias\",\r\n    \"pid\": 1,\r\n    \"tid\": 5,\r\n    \"os_pid\": 120311,\r\n    \"ts\": 1635482148893406,\r\n    \"id\": 1\r\n}\r\n```\r\n\r\nI am not 100% clear on all the consequences but I am very willing to dig into this once I know where to look.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThis would allow additional context to be attached to the trace. For example, linux reports detailed task information through the `/proc` system, such as the executing cpu. This can then be attached to data that is broken down by cpu.\r\n", "comments": []}, {"number": 52989, "title": "Error in converting tensorflow model hdf5 to onnx format", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows on jupyter notebook\r\n- TensorFlow version: 2.6.0\r\n- Python version: 3.8\r\n\r\n**Describe the problem**\r\nI didn't reach to convert my hdf5 model to onnx format with tf2onnx.convert.\r\nBut, i tested the second method\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ntf2onnx error with the command: \r\npython -m tf2onnx.convert  --saved-model saved_model --inputs input_1:0 --outputs gaze_output:0 --output mobilenet.onnx\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-11-08 17:01:09,696 - WARNING - '--tag' not specified for saved_model. Using --tag serve\r\nTraceback (most recent call last):\r\n  File \"D:\\MyPrograms\\anaconda3\\envs\\GazeEnv\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"D:\\MyPrograms\\anaconda3\\envs\\GazeEnv\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"D:\\MyPrograms\\anaconda3\\envs\\GazeEnv\\lib\\site-packages\\tf2onnx\\convert.py\", line 617, in <module>\r\n    main()\r\n  File \"D:\\MyPrograms\\anaconda3\\envs\\GazeEnv\\lib\\site-packages\\tf2onnx\\convert.py\", line 228, in main\r\n    use_graph_names=args.use_graph_names)\r\n  File \"D:\\MyPrograms\\anaconda3\\envs\\GazeEnv\\lib\\site-packages\\tf2onnx\\tf_loader.py\", line 612, in from_saved_model\r\n    tag, signatures, concrete_function, large_model, use_graph_names)\r\n  File \"D:\\MyPrograms\\anaconda3\\envs\\GazeEnv\\lib\\site-packages\\tf2onnx\\tf_loader.py\", line 549, in _from_saved_model_v2\r\n    imported = tf.saved_model.load(model_path, tags=tag)  # pylint: disable=no-value-for-parameter\r\n  File \"D:\\MyPrograms\\anaconda3\\envs\\GazeEnv\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 864, in load\r\n    result = load_internal(export_dir, tags, options)[\"root\"]\r\n  File \"D:\\MyPrograms\\anaconda3\\envs\\GazeEnv\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 903, in load_internal\r\n    ckpt_options, options, filters)\r\n  File \"D:\\MyPrograms\\anaconda3\\envs\\GazeEnv\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 162, in __init__\r\n    self._load_all()\r\n  File \"D:\\MyPrograms\\anaconda3\\envs\\GazeEnv\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 259, in _load_all\r\n    self._load_nodes()\r\n  File \"D:\\MyPrograms\\anaconda3\\envs\\GazeEnv\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 448, in _load_nodes\r\n    slot_variable = optimizer_object.add_slot(\r\nAttributeError: '_UserObject' object has no attribute 'add_slot'\r\n\r\nSECOND method:\r\nloaded_model = tensorflow.keras.models.load_model('my_eye_tracking_model.h5')\r\nonnx_model = tf2onnx.convert.from_keras(loaded_model)\r\nWARNING:tensorflow:From D:\\MyPrograms\\anaconda3\\envs\\GazeEnv\\lib\\site-packages\\tf2onnx\\tf_loader.py:703: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\n\r\nBut the onnx_model has the architecture with input and output\r\n\r\nimport onnx\r\nonnx.save_model(onnx_model, \"test.onnx\")\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi, could you please try the solution suggested in [this](https://stackoverflow.com/questions/69007059/) link and let us know if this helps. Thanks!", "> Hi, could you please try the solution suggested in [this](https://stackoverflow.com/questions/69007059/) link and let us know if this helps. Thanks!\r\n\r\nI 've try this :\r\n##### It can be used to reconstruct the model identically.\r\nmodel = keras.models.load_model(\"my_h5_model.h5\")\r\ntf.saved_model.save(model, \"tmp_model\")\r\n== It works , but the next instruction didn't work and i 've got the same error \r\npython3 -m tf2onnx.convert --saved-model tmp_model --output \"model.onnx\"\r\n \r\nThe error:     slot_variable = optimizer_object.add_slot(\r\nAttributeError: '_UserObject' object has no attribute 'add_slot'", "I'm running into this same issue. Attempting to convert model to onnx.\r\n\r\n` AttributeError: '_UserObject' object has no attribute 'add_slot'`"]}, {"number": 52988, "title": "TensorFlow 2.7 does not detect CUDA installed through conda", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 11.2/8.1\r\n- GPU model and memory: GTX 2080Ti\r\n\r\n**Describe the current behavior**\r\n\r\nAfter installing cuda/cudnn through conda (`conda install cudatoolkit=11.2 cudnn=8.1`), TensorFlow 2.7 reports that it cannot find the cuda libraries.\r\n\r\n```\r\n2021-11-08 14:49:16.412959: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-11-08 14:49:16.413006: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2021-11-08 14:49:22.640508: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-11-08 14:49:22.640617: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\r\n2021-11-08 14:49:22.640698: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\r\n2021-11-08 14:49:22.640776: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2021-11-08 14:49:22.640853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2021-11-08 14:49:22.640941: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\r\n2021-11-08 14:49:22.641022: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\r\n2021-11-08 14:49:22.641099: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\r\n2021-11-08 14:49:22.641120: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and se\r\ntup the required libraries for your platform.\r\n```\r\n\r\nInstalling TensorFlow 2.6 (or earlier) in the same environment, with the same cuda/cudnn installation, doesn't show any problem, it detects the libraries and GPU support works as expected.\r\n\r\nThe problem can be worked around by manually adding the conda lib directory to `LD_LIBRARY_PATH` (`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib`). However, obviously this is not ideal, as it needs to be repeated/adjusted for every new conda environment. It would be better if TensorFlow just detected the conda installed libraries, as it did in TensorFlow < 2.7.\r\n\r\n**Describe the expected behavior**\r\n\r\nTensorFlow should detect cuda/cudnn libraries installed through `conda`, as it did in TensorFlow<2.7.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nconda create -n tmp python=3.8\r\nconda activate tmp\r\nconda install -c conda-forge cudatoolkit=11.2 cudnn=8.1\r\npip install \"tensorflow==2.7.0\"\r\npython -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"  # displays []\r\nLD_LIBRARY_PATH=LD_LIBRARY_PATH:$CONDA_PREFIX/lib python -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"  # displays [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\npip install \"tensorflow<2.7.0\"\r\npython -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"  # displays [[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]]\r\n```", "comments": ["@drasmuss ,\r\nWe can see that you have installed tensorflow from conda environment.Installation issues within the Anaconda environment are tracked in the Anaconda repo.Please try to install in new virtual environment from this [link](https://github.com/tensorflow/tensorflow/releases) and let us know if it is still an issue.Thanks!", "I'm not installing tensorflow from conda, just cuda/cudnn. Tensorflow is being installed from `pip` like normal. And you can see in the reproduction steps I posted above that we're starting from a new virtual environment (repeated below for convenience).\r\n```\r\nconda create -n tmp python=3.8\r\nconda activate tmp\r\nconda install -c conda-forge cudatoolkit=11.2 cudnn=8.1\r\npip install \"tensorflow==2.7.0\"\r\npython -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"  # displays []\r\nLD_LIBRARY_PATH=LD_LIBRARY_PATH:$CONDA_PREFIX/lib python -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"  # displays [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\npip install \"tensorflow<2.7.0\"\r\npython -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"  # displays [[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]]\r\n```\r\nAlso note that nothing has changed on the conda side of things; we're still using the exact same environment with the same cuda/cudnn libraries, but it works in TF 2.6 and fails in TF 2.7. So I don't think the issue is on the conda side, something has changed in TensorFlow that has made this stop working.", "Open the terminal and type \r\n\r\n`nano ~/.bashrc`\r\n\r\nat the end of the file add the following two lines\r\n\r\n`export PATH=$PATH:/usr/local/cuda-11.2/bin`\r\n`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.2/lib64`\r\n\r\nensure no spaces on both side of '=' sign.\r\n\r\nif it still does not works, try adding for version 11.0\r\n\r\n`export PATH=$PATH:/usr/local/cuda-11.0/bin`\r\n`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.0/lib64`\r\n", "As mentioned, CUDA is being installed through conda, so `/usr/local/cuda-` is not the correct path (the correct path is given in the original post: `$CONDA_PREFIX/lib`). However, hard coding that into `.bashrc` isn't a solution, because `$CONDA_PREFIX` changes depending on which conda environment you have active.", "Conda installs are not officially supported by Google", "I installed Tensorflow 2.7 on Windows with CUDA 11.2 and cuDNN 8.1 (no conda involved). I received the same `Could not load dynamic library` errors. I switched to CUDA to 11.0 and it worked. I am guessing that the pip packages for Tensorflow 2.7 were accidentally built against CUDA 11.0 instead of 11.2.", "> Open the terminal and type\r\n> \r\n> `nano ~/.bashrc`\r\n> \r\n> at the end of the file add the following two lines\r\n> \r\n> `export PATH=$PATH:/usr/local/cuda-11.2/bin` `export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.2/lib64`\r\n> \r\n> ensure no spaces on both side of '=' sign.\r\n> \r\n> if it still does not works, try adding for version 11.0\r\n> \r\n> `export PATH=$PATH:/usr/local/cuda-11.0/bin` `export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.0/lib64`\r\n\r\nThank you, this also works with cuda-11.4. But how would you fix this issue in a jupyter notebook? For the pretty niche use case that you would need tf=2.7.0 features.\r\n\r\nWhen I start a jupyter server within a env that has these PATHs exported, it only shows the CPU. When exporting Paths in the notebook it doesn't work either.\r\n", "This seems to solve the issue:\r\n\r\nconda activate ENVNAME\r\n\r\n```sh\r\ncd $CONDA_PREFIX\r\nmkdir -p ./etc/conda/activate.d\r\nmkdir -p ./etc/conda/deactivate.d\r\ntouch ./etc/conda/activate.d/env_vars.sh\r\ntouch ./etc/conda/deactivate.d/env_vars.sh\r\n```\r\nEdit ./etc/conda/activate.d/env_vars.sh as follows:\r\n```sh\r\n#!/bin/sh\r\n\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib\r\n```\r\n\r\nEdit ./etc/conda/deactivate.d/env_vars.sh as follows:\r\n\r\n```sh\r\n#!/bin/sh\r\n\r\nunset LD_LIBRARY_PATH\r\n```\r\n\r\nSource\r\n---- \r\nhttps://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#macos-and-linux", "I don't want to be dismissive here, but there is a lack of understanding of the problem specifically introduced by TF 2.7:\r\n- A conda environment does install native libraries and does ensure they will be found by the os dynamic loader mechanism for the programs that want to find these libraries.\r\n- Until TF 2.7 this was the way it worked, like the gazillion other native apps (including cuda ones)\r\n- TF 2.7, not conda, specifically broke that by ignoring the os loading mechanism for an unknown/undocumented reason\r\n\r\nThis problem is not just a techie point, it does have deep implication for businesses that do real products.\r\nThis method of working is the only reliable one for teams that work on more than one TF project, require multiple TF/CUDA/Python combinations on the same workstation (without root access).\r\nBy the way, the CUDA stack from the official nvidia channel, like nvcc/ptxas perfectly work in conda and is recommended by Nvidia itself.\r\n\r\nFor my suffering peers, if you don't have access to root, you can use this small poorly-documented feature in your environment.yml:\r\n```\r\nname: base-tf-cuda-env\r\nchannel:\r\n  - nvidia\r\n  - conda-forge\r\n  - defaults\r\ndependencies:\r\n  - python=3.8\r\n# Install cuda libs + ptxas compiler from nvidia channel\r\n# This will accelerate the compilation of kernels for your specific card\r\n  - cudatoolkit=11\r\n  - cudnn=8\r\n  - cupti=11\r\n  - cuda-nvcc\r\n...\r\n  - pip\r\n  - pip:\r\n     - tensorflow==2.7.*\r\nvariables:\r\n  # In case you want to see your own logs and tame the TF loggorrhea\r\n  TF_CPP_MIN_LOG_LEVEL: 3\r\n  # Adjust to point to your local env path:\r\n  LD_LIBRARY_PATH: /home/me/.conda/envs/thisenvname/lib\r\n```\r\n\r\nUpon conda activate, the env variables will be set for you, and unset on deactivation.\r\nBetter than nothing, but might interfere with some other configuration...", "> Upon conda activate, the env variables will be set for you, and unset on deactivation.\r\n> Better than nothing, but might interfere with some other configuration...\r\n\r\nReally appreciate the file you provided!\r\nThere is a typo for `channelS` part, but that's awesome, thanks :+1: ", "> Upon conda activate, the env variables will be set for you, and unset on deactivation. Better than nothing, but might interfere with some other configuration...\r\n\r\n@holongate 's env is a good workaround and solves the problem for me.\r\n\r\nI'm quite astonished by how little thought was given on the issue - which is clearly a problem with TF 2.7 itself, and not with conda - and by how much time you waste on commenting that conda installs are not supported by Google.\r\n \r\n\r\n", "For anyone looking for a one-liner solution, you can do\r\n```\r\nconda env config vars set LD_LIBRARY_PATH=$CONDA_PREFIX/lib\r\n```\r\n(with the environment you want to modify activated). This has a similar effect as @jesusdpa1's solution here https://github.com/tensorflow/tensorflow/issues/52988#issuecomment-984025384, it'll set `LD_LIBRARY_PATH` when the environment is activated and unset it when it's deactivated.\r\n\r\nYou still need to repeat that for every new conda environment though. It would be better if TensorFlow just detected the conda installed libraries, as it did in TensorFlow<=2.6.", "fwiw when modifying LD_LIBRARY_PATH to $CONDA_PREFIX/lib, you would risk conflicts in OpenSSL (thereby making it impossible to use git or ssh); this generally impacts fedora-like systems (CentOS and the like). Of course, if you have the cuda libraries elsewhere (e.g. /usr/local/cuda) that would not be an issue; and generally pointing LD_LIBRARY_PATH to these local libraries (non-conda) will work", "Not defending this change, since it is obviously inconvenient and can cause serious issues for people on HPC (e.g. OpenSSL conflicts), but this LD_LIBRARY_PATH stuff seems to have been documented here: https://www.tensorflow.org/install/gpu#linux_setup (not sure when it was added)", "> > Upon conda activate, the env variables will be set for you, and unset on deactivation. Better than nothing, but might interfere with some other configuration...\r\n> \r\n> @holongate 's env is a good workaround and solves the problem for me.\r\n> \r\n> I'm quite astonished by how little thought was given on the issue - which is clearly a problem with TF 2.7 itself, and not with conda - and by how much time you waste on commenting that conda installs are not supported by Google.\r\n\r\nRight. Quite sad to see there is an army of TF guardians of the orthodoxy to censor my remarks about lack of interest for this kind of issue but none to engage in a conversation.\r\nAnd writing anything about the P-devil competition is almost instantly ~~torched~~ deleted"]}, {"number": 52987, "title": "Function _implements is missing when create a new object", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):debian10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.7.0\r\n- Python version:3.9.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n```_implements``` is missing in new objects.\r\n**Describe the expected behavior**\r\n```_implements``` exists.\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nexperimental_implements = \" \".join(['name: \"addons:MaxUnpooling2D\"'])\r\n@tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32), tf.TensorSpec(shape=[], dtype=tf.float32)], experimental_implements = experimental_implements)\r\ndef test(a, b):\r\n    return a+b\r\n\r\n\r\n\r\nclass Test(tf.Module):\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32), tf.TensorSpec(shape=[], dtype=tf.float32)], experimental_implements = experimental_implements)\r\n    def test(self, a, b):\r\n        return a+b\r\n\r\nmodel = Test()\r\nmodel2 = Test()\r\nmodel.test = test\r\nassert model.test._implements == test._implements\r\nassert model2.test._implements == Test.test._implements\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi @Saduf2019! Could you please look into this issue? It is replicating in TF [2.6,](https://colab.research.google.com/gist/mohantym/bdbcb7c7483706e884ac09f932d0ba4e/github_52987.ipynb#scrollTo=Ep2KdhZbrhga) [2.7 ](https://colab.research.google.com/drive/1ZfkNCjNIb4qY5jkeJ3_NYo9q1nLAeQ70?resourcekey=0-lEO7IYc7t-tJOS2o8IDQ7A#scrollTo=Ep2KdhZbrhga)and [Nightly ](https://colab.research.google.com/gist/mohantym/0a8bef4beafa4c302960d74877f38967/github_52987.ipynb#scrollTo=iEQw-Z_rr_jk). "]}, {"number": 52984, "title": "[TFLite] Add int16x8 support for ARG_MIN and ARG_MAX operators", "body": "Hello,\r\n\r\nThis PR adds int16x8 support for ARG_MIN and ARG_MAX operators.\r\n\r\nThanks,\r\nJohan.", "comments": ["@miaout17 Can you please review this PR ? Thanks!", "@miaout17 Can you please review this PR ? Thanks!"]}, {"number": 52977, "title": "OCR example app crashes with new image", "body": "I'm not certain if this is the correct issue type, I'm using the documentation type since the [examples repo](https://github.com/tensorflow/examples) instructed me to do so (specifically the \"To file an issue\" link in the README).\r\n\r\n## URL(s) with the issue:\r\nhttps://github.com/tensorflow/examples/blob/03f796596c9ca9d3d42e5cb43a726b8e220c73b2/lite/examples/optical_character_recognition/android/app/src/main/java/org/tensorflow/lite/examples/ocr/OCRModelExecutor.kt#L200\r\n\r\n## Description of issue (what needs changing):\r\nI tried modifying the OCR example app by using a different image and it resulted in a crash with an odd error message stating `E/cv::error(): OpenCV(4.5.3) Error: Assertion failed (m.dims >= 2) in Mat, file /home/quickbirdstudios/opencv/releases/opencv-4.5.3/modules/core/src/matrix.cpp, line 751`.\r\nAfter some debugging I found the error was caused by [line 200 of OCRModelExecutor.kt](https://github.com/tensorflow/examples/blob/03f796596c9ca9d3d42e5cb43a726b8e220c73b2/lite/examples/optical_character_recognition/android/app/src/main/java/org/tensorflow/lite/examples/ocr/OCRModelExecutor.kt#L200) when `detectedConfidences` is empty. I believe there should be some check here to prevent passing an empty list and instead taking some other action. Alternatively, the error should be caught and a useful message should be providing explaining the error instead of printing the confusing message from OpenCV.\r\n\r\nI also tried lowering the threshold on [line 164](https://github.com/tensorflow/examples/blob/03f796596c9ca9d3d42e5cb43a726b8e220c73b2/lite/examples/optical_character_recognition/android/app/src/main/java/org/tensorflow/lite/examples/ocr/OCRModelExecutor.kt#L164) to 0.1. This resulted in a different error message about `Native Mat has unexpected type or size`, but at least the app didn't crash.", "comments": ["Hi @sachinprasadhs ! Could you please look into this issue?"]}, {"number": 52973, "title": "Release pre-built wheels for manylinux2014_aarch64 and macosx_arm64", "body": "**System information**\r\n- TensorFlow version (you are using): 2.6\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nYou can't `pip install tensorflow` natively on an M1 Mac, since there are no pre-built wheels for macosx_arm64 or manylinux2014_aarch64 (for a native Linux container or VM on M1). \r\n\r\nPyTorch [has provided these pre-built binaries](https://pypi.org/project/torch/#files) for several versions now. It would be great if TF was similarly easy to install on the latest generation of Macs. \r\n\r\n(I know that the separate PyPI package tensorflow-macos exists, but it isn't a portable solution and is not documented.)\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAny user of the latest and future generations of Macs.", "comments": ["We don't have yet support for these architectures.", "Really looking forward to this support. Thanks!", "@mihaimaruseac It would be helpful to compile a list of outstanding issues that must be resolved before being able to provide these wheels. This would allow people to track progress, and contribute to the blocking issues. ", "We don't even have access to infra to run these builds. While we're blocked on that we're completely blind.", "Agree that aarch64 wheels would be a great addition and a way of supporting Linux containers on M1 Macs without emulation (which currently requires compiling without AVX instructions discussed in https://github.com/tensorflow/tensorflow/issues/52845)\r\n\r\nIt looks like a Dockerfile for building manylinux2014 wheels was recently added in https://github.com/tensorflow/tensorflow/commit/c5ae019abd2f260cf3400abcce4962c75cc5182c I think what would be needed for this is support for cross-compiling tensorflow for aarch64 (for these wheels (a quick search on aarch64 shows that most of the support for aarch64 is in tflite) or natively on arm64 infrastructure.", "Just a side note also having a Linux aarch64 image on M1 we are not going to be able to use the AMX2 instruction set:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/52845#issuecomment-974559363\r\n\r\nhttps://nod.ai/comparing-apple-m1-with-amx2-m1-with-neon/\r\n", "And also ANE https://developer.apple.com/forums/thread/673627", "From the GPU perspective: \r\n\r\n- It looks like TensorRT won't work on ARM ([missing](https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/) `libnvinfer` deb) ref https://github.com/tensorflow/tensorrt/issues/142#issuecomment-1077394968\r\n- The rest of the [CUDA math libraries](https://developer.nvidia.com/gpu-accelerated-libraries) and `cuDNN` should be fine, they're all available for arm: they are for instance included in the cuda [`runtime`](https://hub.docker.com/r/nvidia/cuda/tags?page=1&name=cudnn8-runtime) images which are also built for arm64, on which [`gpu.Dockerfile`](https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/tools/dockerfiles/dockerfiles/gpu.Dockerfile) could base", "Any progress on this? It would be great if someone could provide some sort of timeline, as in \"next minor version\" or >1 year away or something like that? ", "There are talks about this support, analyzing all possible roadmaps that we could take. A nice thing is that now we have manylinux2014 support, but we still need to tackle the question of how to test these builds if CI infra is not yet there."]}, {"number": 52969, "title": "Use the official devel image for presubmit litting", "body": "This is temp failing now cause we don't distribute a `pytlint` reference version in our official devel image.\r\nI've tried to fix the missing reference version at https://github.com/tensorflow/tensorflow/pull/48371 but it was not reviewed  and some month later my rationale was reject also in https://github.com/tensorflow/tensorflow/pull/51914#issuecomment-916540675.\r\n\r\nAs I suppose that we don't have too much free time to maintain another potentially unaligned environment I think it is better to lint the PR code in the CI (TF OSS) with exactly the same (prepared) environment that we give to contributors to build, lint and test their contributing code/PR on their own hw resources (or Github workspaces available on this platform).", "comments": ["@bhack Can you please resolve conflicts? Thanks!", "@gbaned We need to wait for a developer image that includes `pylint` so that we could use exactly the same image in CI with this PR. We need to ask to the SIG Build team what are the plans."]}, {"number": 52968, "title": "`tf.svd` fails during GradientTape mode", "body": "**System information**\r\n- OS Platform and Distribution: macOS v12.0.1\r\n- TensorFlow installed from: pypi\r\n- TensorFlow version: v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.8.9\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nHi, I'm trying to contract a network with multiple tensors and using singular value decomposition during contraction to simplify the contraction process. Whilst this works perfectly when I'm not taking any gradient, it fails once gradient tape starts to watch the tensors (I'm not sure why this is related). Below I wrote my simple contraction function and the function that I'm taking `svd`:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef contraction_step(network, max_singular_values: int):\r\n\r\n    bottom = network[-1]\r\n    uppper = network[-2]\r\n\r\n    def contract_up_down(up,dn):\r\n        shu = tf.shape(up)\r\n        shd = tf.shape(dn)\r\n        c = tf.einsum(\"ijkxlm,nkpyqr->injpxylqmr\", up, dn)\r\n        return tf.reshape(c, (\r\n            shu[0]*shd[0], shu[1], shd[2], shu[-3], shd[-3], shu[-2]*shd[-2], shu[-1]*shd[-1]\r\n        ))\r\n\r\n    new = []\r\n    multiplier = tf.eye(tf.shape(bottom[-1])[-1]*tf.shape(uppper[-1])[-1], dtype=uppper[-1].dtype)\r\n    for ix in reversed(range(len(bottom))):\r\n        tensor = contract_up_down(uppper[ix], bottom[ix])\r\n        t = tf.einsum(\"ludpxor,ij->ludpxoj\",tensor, multiplier)\r\n        u, s, vh = svd(t, 1, max_singular_values = max_singular_values)\r\n        multiplier = tf.tensordot(u, s/tf.norm(s), axes=(-1,0))\r\n        new.insert(0, vh)\r\n    new[-1] = tf.tensordot(new[-1], multiplier, axes=(-1,0))\r\n\r\n    return network[:-2] + [new]\r\n\r\ndef svd(tensor,pivot,max_singular_values = None,cutoff = 0.0):\r\n    left_dims = tf.shape(tensor)[:pivot]\r\n    right_dims = tf.shape(tensor)[pivot:]\r\n    tensor = tf.reshape(tensor, (tf.reduce_prod(left_dims), tf.reduce_prod(right_dims)))\r\n\r\n    s, u, v = tf.linalg.svd(tensor)\r\n\r\n    s_shape = tf.math.count_nonzero(\r\n        tf.cast(s >= cutoff, dtype = tf.int32), dtype = tf.int32\r\n    )\r\n    if max_singular_values is None:\r\n        max_singular_values = s_shape\r\n    else:\r\n        max_singular_values = tf.cast(tf.constant(max_singular_values), dtype = tf.int32)\r\n    num_sing_vals_keep = tf.maximum(\r\n        tf.minimum(max_singular_values, s_shape), tf.constant(1, dtype = tf.int32)\r\n    )\r\n\r\n    s = tf.slice(s, [0], [num_sing_vals_keep])\r\n    u = tf.slice(u, [0, 0], [tf.shape(u)[0], num_sing_vals_keep])\r\n    v = tf.slice(v, [0, 0], [tf.shape(v)[0], num_sing_vals_keep])\r\n\r\n    vh = tf.linalg.adjoint(v)\r\n\r\n    dim_s = tf.shape(s)[0]  # must use tf.shape (not s.shape) to compile\r\n    u = tf.reshape(u, tf.concat([left_dims, [dim_s]], axis = -1))\r\n    vh = tf.reshape(vh, tf.concat([[dim_s], right_dims], axis = -1))\r\n\r\n    return u, tf.linalg.diag(s), vh\r\n```\r\nThese functions work perfectly while using standalone:\r\n```\r\nupper = [tf.random.uniform((5,3,3,2,1,5), dtype=tf.float64) for _ in range(5)]\r\nlower = [tf.random.uniform((5,3,3,2,1,5), dtype=tf.float64) for _ in range(5)]\r\n\r\ncontracted = contraction_step([upper, lower], 2)[0]\r\nprint(f\"shapes: {', '.join([str(x.shape) for x in contracted])}\")\r\n\r\n# shapes: (2, 3, 3, 2, 2, 1, 2), (2, 3, 3, 2, 2, 1, 2), (2, 3, 3, 2, 2, 1, 2), (2, 3, 3, 2, 2, 1, 2), (2, 3, 3, 2, 2, 1, 2)\r\n```\r\nHowever, with the gradient, I get the following error:\r\n```\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(upper + lower)\r\n    contracted = contraction_step([upper, lower], 2)[0]\r\n\r\nNotImplementedError: SVD gradient has not been implemented for input with unknown inner matrix shape.\r\n```\r\nIt seems like for some reason during gradient mode TensorFlow loses the shape information of the tensors. Note that I get the same error when I set the tensors as `tf.Variable` instead of watching them manually. Any help would be highly appreciated!\r\n\r\nThe same error has been reproduced in [Colab as well](https://colab.research.google.com/drive/1Cg5D-qZz8hIlY0bNxU-2WmuBpyDo8wGs?usp=sharing).\r\n\r\nThanks\r\n", "comments": ["You can refer [this](https://stackoverflow.com/questions/66896059/) which shows working of tf.linalg.svd with Gradient Tape.\r\nIf you are using any dynamic matrix dimension, you can try providing static matrix to debug the issue. Thanks!", "Hi, @sachinprasadhs I'm aware that svd works with GradientTape that's why I'm using it that's not the issue. During my calculation, I'm regularizing my network with svd (as seen in the code above). During this regularization, svd method loses the shape information of the tensor which can be directly traced to `tensorflow/python/ops/linalg_grad.py` L825: `a_shape = a.get_shape().with_rank_at_least(2)`. It works perfectly without `GradientTape` as you can see I did not even try to take the gradient in the code above I'm just trying to contract the network with regularization there within `GradientTape` to take the gradient after further calculation. \r\n\r\nPS when I try to print the tensor in `tensorflow/python/ops/linalg_grad.py` \r\n\r\n`tensorflow/python/ops/linalg_grad.py:: L811`\r\n```python\r\n@ops.RegisterGradient(\"Svd\")\r\ndef _SvdGrad(op, grad_s, grad_u, grad_v):\r\n  \"\"\"Gradient for the singular value decomposition.\"\"\"\r\n\r\n  # The derivation for the compute_uv=False case, and most of\r\n  # the derivation for the full_matrices=True case, are in\r\n  # Giles' paper (see reference at top of file).  A derivation for\r\n  # the full_matrices=False case is available at\r\n  # https://j-towns.github.io/papers/svd-derivative.pdf\r\n  # The derivation for complex valued SVD can be found in\r\n  # https://re-ra.xyz/misc/complexsvd.pdf or\r\n  # https://giggleliu.github.io/2019/04/02/einsumbp.html\r\n  a = op.inputs[0]\r\n  print(a)\r\n```\r\nit gives me the following:\r\n```python\r\nTensor(\"Reshape_17:0\", shape=(None, None), dtype=float64)\r\n```\r\nas you can see the shape information is `None`.\r\n\r\nI also tried to change L825 `a_shape = a.get_shape().with_rank_at_least(2)` -> `a_shape = tf.TensorShape(tf.shape(a)).with_rank_at_least(2)` but it didn't work in that scenario I'm getting `TypeError`.", "Thanks for the clarification, I was able to reproduce your issue with Tensorflow 2.7, please find the Gist [here](https://colab.research.google.com/gist/sachinprasadhs/8e98a9a1431360d09c5695bf2ef4dcfc/52968.ipynb).", "Hi to update the problem here I also tried without reassigning `multiplier` tensor in `contraction_step` function by initializing it as a list and appending in it with every iteration. However this approach also ends up with giving the exact same error. Here is the modified function for future reference;\r\n\r\n```python\r\ndef contraction_step(network, max_singular_values: int):\r\n\r\n    bottom = network[-1]\r\n    uppper = network[-2]\r\n\r\n    def contract_up_down(up,dn):\r\n        shu = tf.shape(up)\r\n        shd = tf.shape(dn)\r\n        c = tf.einsum(\"ijkxlm,nkpyqr->injpxylqmr\", up, dn)\r\n        return tf.reshape(c, (\r\n            shu[0]*shd[0], shu[1], shd[2], shu[-3], shd[-3], shu[-2]*shd[-2], shu[-1]*shd[-1]\r\n        ))\r\n\r\n    new = []\r\n    multiplier = [tf.eye(tf.shape(bottom[-1])[-1]*tf.shape(uppper[-1])[-1], dtype=uppper[-1].dtype)]\r\n    for ix in reversed(range(len(bottom))):\r\n        tensor = contract_up_down(uppper[ix], bottom[ix])\r\n        t = tf.einsum(\"ludpxor,ij->ludpxoj\",tensor, multiplier[-1])\r\n        u, s, vh = svd(t, 1, max_singular_values = max_singular_values)\r\n        multiplier.append(tf.tensordot(u, s/tf.norm(s), axes=(-1,0)))\r\n        new.insert(0, vh)\r\n    new[-1] = tf.tensordot(new[-1], multiplier[-1], axes=(-1,0))\r\n\r\n    return network[:-2] + [new]\r\n```\r\nAny help on how to fix this issue would be greatly appreciated.", "Hi @sachinprasadhs is there any update on this?"]}, {"number": 52960, "title": "Make line wrapping and precision of tf.print configurable.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): not relevant\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\ntf.print() wraps past the 4th element for me. Output from printing an (N, 7, 7), where N=2 in this case, looks like this:\r\n\r\n```\r\n[[[ 5.62430954e+00 -3.49580956e+00 -8.04360168e+02 -5.40295654e+02\r\n   -8.65279853e-01 -1.06588173e+00 -1.18294978e+00]\r\n  [-3.49580956e+00  7.82608986e+00  4.73972961e+02  1.18741919e+03\r\n    1.61477351e+00  1.88551021e+00  1.97545743e+00]\r\n  [-8.04360168e+02  4.73972961e+02  1.31743172e+05  7.44966484e+04\r\n    2.14444519e+02  2.22681152e+02  1.98635208e+02]\r\n  [-5.40295654e+02  1.18741919e+03  7.44966484e+04  1.97479641e+05\r\n    2.52062241e+02  2.85624451e+02  2.64948578e+02]\r\n  [-8.65279853e-01  1.61477351e+00  2.14444519e+02  2.52062241e+02\r\n    3.18258524e+00  2.82087207e+00  2.33711910e+00]\r\n  [-1.06588173e+00  1.88551021e+00  2.22681152e+02  2.85624451e+02\r\n    2.82087207e+00  2.64690948e+00  2.28564334e+00]\r\n  [-1.18294978e+00  1.97545743e+00  1.98635208e+02  2.64948578e+02\r\n    2.33711910e+00  2.28564334e+00  2.24237132e+00]]\r\n\r\n [[ 6.57234287e+00 -4.52555466e+00 -1.04999585e+03 -7.79698792e+02\r\n   -1.47998118e+00 -2.11049843e+00 -4.34581661e+00]\r\n  [-4.52555466e+00  8.80841637e+00  7.03340576e+02  1.52776123e+03\r\n    2.77018666e+00  3.56193233e+00  6.32034349e+00]\r\n  [-1.04999585e+03  7.03340576e+02  1.78551625e+05  1.23507805e+05\r\n    4.00629730e+01  1.89105148e+02  5.56380432e+02]\r\n  [-7.79698792e+02  1.52776123e+03  1.23507805e+05  2.79413969e+05\r\n    3.76035431e+02  5.24331360e+02  1.04614160e+03]\r\n  [-1.47998118e+00  2.77018666e+00  4.00629730e+01  3.76035431e+02\r\n    1.25411425e+01  1.00502367e+01  9.19474983e+00]\r\n  [-2.11049843e+00  3.56193233e+00  1.89105148e+02  5.24331360e+02\r\n    1.00502367e+01  9.68517876e+00  1.09725800e+01]\r\n  [-4.34581661e+00  6.32034349e+00  5.56380432e+02  1.04614160e+03\r\n    9.19474983e+00  1.09725800e+01  1.55766268e+01]]]\r\n```\r\n\r\nI work a lot with these fixed-sized matrices (eg: 7x7, or 9x9). Debugging those is hard with line wrapping.\r\nI'd like to get them all 7 elements of the matrix row on one line, at least. Additionally, I would fancy having less precision, as that is not that informative to debug.\r\n\r\n**Will this change the current api? How?**\r\nEither have like numpy a `set_print_options()`, or have extra arguments to tf.print along the lines of `line_length=None` (where None means \"no wrapping at all\") and `format=\"%12.7e\"` where I could specify for example `format=\"%10.4f\"`.\r\nI am slightly in favor of the custom options per `tf.print()` call to allow granular control.\r\n\r\n**Who will benefit with this feature?**\r\nMost likely anyone debugging stuff with matrices.\r\n", "comments": ["I don't think it is the limitation of tf.print, even if you try with normal print in colab, it truncates to certain numbers in a line.\r\nYou could just try the below line in your IDE.\r\n`print(np.random.randn(2,7,7))`\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sachinprasadhs This is completely beside the point. I'm printing TensorFlow tensors with tf.print(). I'm __*not*__ dealing with numpy arrays being printed by the regular Python print()."]}, {"number": 52952, "title": "\"tf.print()\" breaks the tensor value order in OrderedDict", "body": "**System information**\r\n- OS Platform and Distribution: macOS Big Sur 11.6\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: The issue occurs in both TF v1.15.2 and TF v2.7.\r\n- Python version: Python 3.6.5 with TF v1.15.2, or Python 3.8.3 with TF v2.7\r\n\r\n**Issue Description & Repro Code**\r\n\r\nI am using `tf.print()` to log the intermediate values evaluated from a OrderedDict containing the string-tensor pair, but the evaluation result shows that the tensor order is messed up. The following python script `try_tf_print.py` is an example.\r\n\r\n```\r\nfrom collections import OrderedDict\r\n\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n\r\ndef build_tf_print_ops_with_dict() -> None:\r\n    a = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, 1])\r\n    b = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, 3])\r\n    c = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, 2])\r\n    d = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, 5])\r\n    p_dict = OrderedDict([\r\n        (\"foo\", a),\r\n        (\"bar\", b),\r\n        (\"baz\", c),\r\n        (\"qux\", d)\r\n    ])\r\n    tf_print_ops = tf.print(p_dict)\r\n    sess = tf.compat.v1.Session()\r\n    sess.run(tf_print_ops, feed_dict={\r\n        a: [[1]],\r\n        b: [[2, 4, 6]],\r\n        c: [[0.2, 0.2]],\r\n        d: [[100, 200, 300, 400, 500]]\r\n    })\r\n\r\n\r\nbuild_tf_print_ops_with_dict()\r\n```\r\nI'd expect tf.print() evaluation result to be as `OrderedDict([('foo', [[1]]), ('bar', [[2 4 6]]), ('baz', [[0.2 0.2]]), ('qux', [[100 200 300 400 500]])])`, while what I got was `OrderedDict([('foo', [[2 4 6]]), ('bar', [[0.2 0.2]]), ('baz', [[1]]), ('qux', [[100 200 300 400 500]])])`, in which the tensor value order is broken -- It is supposed to follow the OrderedDict order `(\"foo\", a), (\"bar\", b), (\"baz\", c), (\"qux\", d)`, but the order it gets is `(\"foo\", b), (\"bar\", c), (\"baz\": a), (\"qux\", d)`. The issue could be 100% reproduced by running above script `try_tf_print.py`.\r\n\r\n\r\n", "comments": ["I am able to reproduce the issue in colab with `2.6.0`. Please find the [gist here](https://colab.sandbox.google.com/gist/sanatmpa1/d22ff38bff24c7313182ec3c623c60e2/52952.ipynb#scrollTo=yNgtEmUglESQ)", "@RuofanKong,\r\n\r\nI've found [this](https://github.com/tensorflow/community/blob/master/rfcs/20180824-tf-print-v2.md#supported-input-types) documentation, which states that \r\n> Tensors nested inside of python sets will not be properly and their values will not be shown/formatted when the tf.print kernels run.\r\n\r\nCan you take a look on it? Thanks!", "Thank you @sanatmpa1 for the comment. I just looked at the doc that you shared, and the following is the feedback from a user point of view.\r\n\r\n1.  The note stated in the documentation is critical for users to be aware of, while I don't see it is being included in the TF official public doc (https://www.tensorflow.org/api_docs/python/tf/print)\r\n2. Regardless of 1, per my experience as a user, it is a very common use case to have the struct tensor to work with `tf.print()`, and not supporting it is unreasonable to me. Plus, the official doc here has an example that it should work with a tensor embedded in the python dict. see here (https://www.tensorflow.org/api_docs/python/tf/print#example)\r\n\r\nHope this makes sense.", "Thanks for the clarification. I've found a similar [issue](https://github.com/tensorflow/tensorflow/issues/51532) here which also discusses about the similar problem.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Any updates on it?"]}, {"number": 52944, "title": "tf.print() with XLA compilation", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\nHello,\r\n\r\nDoes anyone know whether tf.print (or an equivalent workaround) can be made to work with XLA compilation? I get an error saying that XLA does not recognize the \"printV2\" operation if I have a tf.print statement inside a function decorated with @tf.function(jit_compile=True).\r\n\r\nIf this functionality does not exist, would like to request that it be added as a new feature!\r\n\r\nThanks!\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.6.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\ntf.print() does not seem to work when used within functions that are compiled with XLA ( decorated with @tf.function(jit_compile=True)\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nUsers of XLA-compiled tensorflow code. \r\n\r\n**Any Other info.**\r\n", "comments": ["Could you please provide the code for which you are facing error. Thanks!", "`\r\nimport tensorflow as tf\r\n\r\n@tf.function(jit_compile=True)\r\ndef print_ten():\r\n    for i in range(10):\r\n        tf.print('printing')\r\n    return\r\n\r\nprint_ten()\r\n`", "Why not\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function(jit_compile=True)\r\ndef print_ten():\r\n  for i in range(10):\r\n    print('printing ' +str(i))\r\n  return\r\n\r\nprint_ten()\r\n```", "Normal print functions works without any limitations, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/8140cf768b36a8daa07505687fceb4a1/52944.ipynb). Thanks!", "The problem is that with normal print, the print statements are only executed on the first call to the function if the function is called multiple times with the same arguments (since tf will use an already made graph, which does not include the calls to print) - (please see https://www.tensorflow.org/guide/function under section \"what is tracing?\" for details)\r\n\r\ntf.print() on the other hand will always print in graph mode (without XLA), even on non-first calls. However, tf.print() is not currently supported when XLA compilation is used.\r\n\r\nHope this clarifies the nature of the problem -please do let me know if you need more info/code examples of the issue.\r\n", "For example,\r\n\r\n`import tensorflow as tf\r\n\r\n@tf.function(jit_compile=True)\r\ndef print_ten():\r\n  for i in range(10):\r\n    print('printing ' +str(i))\r\n  return\r\n\r\nprint_ten()\r\nprint_ten()\r\n`\r\nWill only print 10 times (corresponding to the first call). If the regular print is replaced with tf.print(), the code will NOT run since XLA does not support tf.print at this time.", "Yes if you want to print on every function call it is not supported. You can print the returned value outside of the compiled function.\r\nI don't know if it is still updated but here you can find the list of the supported ops:\r\nhttps://github.com/tensorflow/tensorflow/issues/14798#issuecomment-350814888", "Yup - that's why I made a feature request for adding tf.print() to the supported ops for XLA compilation."]}, {"number": 52939, "title": "the prority of session config", "body": "https://github.com/tensorflow/tensorflow/issues/52938", "comments": ["Re-assigning to @mhong for triage.", "@mhong Can you please review this PR ? Thanks!", "@mhong Can you please review this PR ? Thanks!", "@bramandia  Can you please review this PR ? Thanks!", "@bramandia Can you please review this PR ? Thank you!"]}, {"number": 52933, "title": "Failing to Quantize for Edge TPU. Requires tf.float32 type for conversion but tf.uint8 for Edge", "body": "I am attempting to quantize a model post creation for use on a coral product. This requires me to quantize the model, unfortunately quantization throws a significant error. Below is the architecture of my model and associated Colab notebook showing the error. In short, the model successfully converts to a tflite when I allow the input and output to be tf.float32, which renders a TPU useless, but when I code the model to adjust the input and output to tf.uint8 I get a ValueError. \r\n\r\nI have spoken with Coral extensively and they have referred me here. I really hope you can help out!\r\n\r\n\r\n\r\n 1. System information\r\n\r\n- Windows 10\r\n- TensorFlow installation Anaconda\r\n- TensorFlow library: 2.6.0\r\n\r\n 2. Code\r\n\r\nhttps://colab.research.google.com/drive/1CdjguiaWf-M5VXv9mqmfcbJXrGt7m-xt?usp=sharing\r\n\r\n\r\nError log\r\n\r\nLog from Edge TPU compiler when input output is allowed to be tf.float32\r\n\r\nEdge TPU Compiler version 16.0.384591198\r\nStarted a compilation timeout timer of 180 seconds.\r\n\r\nModel compiled successfully in 472 ms.\r\n\r\nInput model: IVUS_v1.tflite\r\nInput size: 133.24MiB\r\nOutput model: IVUS_v1_edgetpu.tflite\r\nOutput size: 133.23MiB\r\nOn-chip memory used for caching model parameters: 0.00B\r\nOn-chip memory remaining for caching model parameters: 0.00B\r\nOff-chip memory used for streaming uncached model parameters: 0.00B\r\nNumber of Edge TPU subgraphs: 0\r\nTotal number of operations: 81\r\nOperation log: IVUS_v1_edgetpu.log\r\n\r\nModel successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\r\nNumber of operations that will run on Edge TPU: 0\r\nNumber of operations that will run on CPU: 81\r\n\r\nOperator                       Count      Status\r\n\r\nCONV_2D                        28         Operation is working on an unsupported data type\r\nTRANSPOSE_CONV                 7          Operation is working on an unsupported data type\r\nLOGISTIC                       1          Operation is working on an unsupported data type\r\nFULLY_CONNECTED                1          Operation is working on an unsupported data type\r\nADD                            1          Operation is working on an unsupported data type\r\nPRELU                          28         Operation is working on an unsupported data type\r\nMAX_POOL_2D                    7          Operation is working on an unsupported data type\r\nCONCATENATION                  7          Operation is working on an unsupported data type\r\nMUL                            1          Operation is working on an unsupported data type\r\nCompilation child process completed within timeout period.\r\nCompilation succeeded!", "comments": ["https://drive.google.com/drive/folders/1Edy4ARMLZOIDuS_3Is4u_bXf6Paa3M97?usp=sharing\r\n\r\nThis is a link to the data I used.", "Update: My model successfully compiles on tensorflow version 2.2.0 but I have to remove the PreReLU activation function.\r\nhttps://colab.research.google.com/drive/1jR1nRQ1PK2nxUaQFHvYQPVJjs5cki5dI?usp=sharing", "PReLU has some limitations on edge TPU. \r\n`Alpha must be 1-dimensional (only the innermost dimension can be >1 size). If using Keras PReLU with 4D input (batch, height, width, channels), then shared_axes must be [1,2] so each filter has only one set of parameters.`\r\nInstead you can use RELU which doesn't have any limitations on edge TPU.\r\nFor all the limitations, refer [this](https://coral.ai/docs/edgetpu/models-intro/#supported-operations) table.\r\n", "I am confused, as I have only one value in PReLU. Is there some way I could alter that function to work properly instead of using ReLU?", "Also, replacing PReLU alone did not resolve the issue. I have to revert to an older version of Tensorflow.", "Did  you try with nightly version and the details in the above linked document?", "I am not sure what you mean by nightly version but I am in compliance with the linked document. I was in conversation with them for several days discussing what to do. They had access to my full architecture, they referred me to you after they gave up.", "How do I use the nightly version?", "You can install the nightly version by `pip install tf-nightly` and then import tensorflow by `import tensorflow as tf`", "I will try that version. I am making my way up the versions testing each one. Version 2.4.4 works correctly but version 2.5.0 yields the following error: ValueError: The inference_input_type and inference_output_type must be tf.float32.", "Also, I have removed all PReLU functions for now. \r\n\r\nNightly results in the same error. ", "in Tensorflow 2.4.4 does it work perfectly even with PReLU?", "In Tensorflow 2.4.4 it quantizes correctly however it does not compile in the Edge compiler. I have found that the latest version to both quantize and compile properly is 2.2.3. This functions properly even with PReLU", "This is the working version with tf 2.2.3: https://colab.research.google.com/drive/1jR1nRQ1PK2nxUaQFHvYQPVJjs5cki5dI?usp=sharing", " Also of importance is to note that while model quantizes in 2.4.4, interpretation of the model as written in the last block fails.", "Beginning in Version 2.3.0 the following error is generated when trying to use a lite interpreter on the quantized model.\r\n\r\nCode:\r\n\r\n# Load the TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"IVUS_v1.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test the model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n\r\n\r\nError:\r\n\r\n---------------------------------------------------------------------------\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n\r\n<ipython-input-18-8e65e7ad1c2d> in <module>()\r\n      1 # Load the TFLite model and allocate tensors.\r\n      2 interpreter = tf.lite.Interpreter(model_path=\"IVUS_v1.tflite\")\r\n----> 3 interpreter.allocate_tensors()\r\n      4 \r\n      5 # Get input and output tensors.\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)\r\n    241   def allocate_tensors(self):\r\n    242     self._ensure_safe()\r\n--> 243     return self._interpreter.AllocateTensors()\r\n    244 \r\n    245   def _safe_to_run(self):\r\n\r\nRuntimeError: tensorflow/lite/kernels/conv.cc:334 bias->type != kTfLiteInt32 (INT8 != INT32)Node number 1 (CONV_2D) failed to prepare.\r\n", "This bug seems to have been forgotten in the pipeline.", "Hi @nneubert, this can happen when a bias is used in another operator as an input or weight. This issue was fixed in 2.5, so I would suggest you run the tflite converter from 2.5 with representative dataset again on your saved model. ", "@daverim Thank you for your suggestion. I attempted this code with 2.5.0 and 2.6.0 just today and received the following error twice\r\n\r\nINFO:tensorflow:Assets written to: /tmp/tmpk9v24yxv/assets\r\n\r\n---------------------------------------------------------------------------\r\n\r\nValueError                                Traceback (most recent call last)\r\n\r\n<ipython-input-10-bed7ae94e3d4> in <module>()\r\n     17 converter.inference_input_type = tf.uint8\r\n     18 converter.inference_output_type = tf.uint8\r\n---> 19 tflite_model = converter.convert()\r\n     20 \r\n     21 with open('IVUS_v1.tflite', 'wb') as f:\r\n\r\n8 frames\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py in _validate_inference_input_output_types(self, quant_mode)\r\n    805     elif (self.inference_input_type not in default_types or\r\n    806           self.inference_output_type not in default_types):\r\n--> 807       raise ValueError(\"The inference_input_type and inference_output_type \"\r\n    808                        \"must be tf.float32.\")\r\n    809 \r\n\r\nValueError: The inference_input_type and inference_output_type must be tf.float32.\r\n\r\n\r\n", "> Hi @nneubert, this can happen when a bias is used in another operator as an input or weight. This issue was fixed in 2.5, so I would suggest you run the tflite converter from 2.5 with representative dataset again on your saved model.\r\n\r\n@daverim \r\n\r\nJust want to let you know that the issue is not fixed. I can get TF version 2.2.3 to successfully quantize using this code here: https://colab.research.google.com/drive/1jR1nRQ1PK2nxUaQFHvYQPVJjs5cki5dI#scrollTo=NfHYCknyAqvT However it does not quantize in version 2.5 or version 2.7. If there is anything you can do to assist, please help.\r\n\r\n", "Hi @nneubert ! I used inference_input_type and inference_output_type as tf.float32 as the  error suggested. Attaching [Gist in 2.7 ](https://colab.sandbox.google.com/gist/mohantym/6edc71631e6e86d39abc2032a87a4f7f/version-2-2-3.ipynb#scrollTo=2qQwyeMHAnZE) for reference. Thanks!", "> \r\n\r\nI know that you can do that, but that defeats the purpose of quantizing the model. If you don't change the input and output to int8 then it can't be run on a coral TPU which is the whole goal. "]}, {"number": 52914, "title": "NNAPI on Kirin 980 NPU. Model won't run on NPU, libc : Access denied finding property \"ro.hardware.chipname\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 11.0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Huawei P30 Pro\r\n- TensorFlow installed from (source or binary):  android_aarch64_benchmark_tool\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nI have been experiencing NNAPI producing longer inference times on my device than with standard CPU delegate, so i ran benchmark binary with tensorflow prebuilt NNAPI models. The debug from logcat shows that it get stuck with the following error\r\n\r\n```\r\n11-02 14:18:11.933 29254 29254 E libc    : Access denied finding property \"ro.hardware.chipname\"\r\n11-02 14:18:11.928 29254 29254 W benchmark_model: type=1400 audit(0.0:752014): avc: denied { read } for pid=29254 name=\"u:object_r:vendor_default_prop:s0\" dev=\"tmpfs\" ino=15480 scontext=u:r:shell:s0 tcontext=u:object_r:vendor_default_prop:s0 tclass=file permissive=0\r\n```\r\n\r\nI have tested this on Huawei P30 Pro and Huawei Nova 5T with the same results. \r\nI downloaded mobilenet_v1_1.0_224_quant.tflite NNAPI prebuilt model from here https://www.tensorflow.org/lite/performance/nnapi#use_supported_models_and_ops\r\n\r\nThen i pushed to the device and invoked the benchmarking tool\r\n\r\n .downloaded android aarch64 benchmark tool as per instructions and invoked the model like so\r\n```\r\nadb shell /data/local/tmp/benchmark_model \\\r\n  --num_threads=4 \\\r\n  --graph=/data/local/tmp/mobilenet_v1_1.0_224_quant.tflite \\\r\n  --warmup_runs=1 \\\r\n  --num_runs=50 --use_nnapi=1 --verbose=1 --disable_nnapi_cpu=0 --nnapi_accelerator_name=\"ipuadaptor\"\r\n```\r\n\r\nI enabled verbose nnapi log using this command\r\n\r\n```adb shell setprop debug.nn.vlog 1```\r\n\r\n```\r\n11-02 14:18:11.894 29254 29254 D skia    : HME SkHmeDecFunction1 constructor ok\r\n11-02 14:18:11.894 29254 29254 I HiTraceC: entered LogRegisterGetIdFun\r\n11-02 14:18:11.894 29254 29254 I HiTraceC: entered HiTraceInit\r\n11-02 14:18:11.896 29254 29254 I Manager : DeviceManager::DeviceManager\r\n11-02 14:18:11.896 29254 29254 I Manager : findAvailableDevices\r\n11-02 14:18:11.898 29254 29254 I Manager : Found interface ipuadaptor\r\n11-02 14:18:11.903  1029 15537 I aiserver: IPUNNAdaptor getCapabilities_1_1\r\n11-02 14:18:11.903  1029 15537 I aiserver: AiModelMngrService getCapabilities_1_1\r\n11-02 14:18:11.903  1029 15537 I aiserver: AiModelMngrService getCapabilities\r\n11-02 14:18:11.904  1029 15537 I hcs     : AndroidNNMLUExecutor::GetCapabilities Get device capabilities success.\r\n11-02 14:18:11.904  1029 15537 I AndroidNN: AnnHcsService::GetCapabilities(169)::\"Get capabilities done!\"\r\n11-02 14:18:11.904 29254 29254 I Manager : Capab {.relaxedFloat32toFloat16PerformanceScalar = {.execTime = 0.470000, .powerUsage = 0.660000}, .relaxedFloat32toFloat16PerformanceTensor = {.execTime = 0.470000, .powerUsage = 0.660000}, .operandPerformance = [7]{{.type = FLOAT32, .info = {.execTime = 0.470000, .powerUsage = 0.660000}}, {.type = INT32, .info = {.execTime = 0.470000, .powerUsage = 0.250000}}, {.type = UINT32, .info = {.execTime = 0.470000, .powerUsage = 0.250000}}, {.type = TENSOR_FLOAT32, .info = {.execTime = 0.470000, .powerUsage = 0.660000}}, {.type = TENSOR_INT32, .info = {.execTime = 0.470000, .powerUsage = 0.250000}}, {.type = OEM, .info = {.execTime = 0.470000, .powerUsage = 0.250000}}, {.type = TENSOR_OEM_BYTE, .info = {.execTime = 0.470000, .powerUsage = 0.250000}}}}\r\n11-02 14:18:11.905 29254 29254 I tflite  : Initialized TensorFlow Lite runtime.\r\n11-02 14:18:11.905 29254 29254 I tflite  : Created TensorFlow Lite delegate for NNAPI.\r\n11-02 14:18:11.906 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 1 offset 2405268 size 864\r\n11-02 14:18:11.906 29254 29254 I TypeManager: TypeManager::TypeManager\r\n11-02 14:18:11.906 29254 29254 I TypeManager: Failed to read /vendor/etc/nnapi_extensions_app_allowlist ; No app allowlisted for vendor extensions use.\r\n11-02 14:18:11.906 29254 29254 I TypeManager: NNAPI Vendor extensions enabled: 0\r\n11-02 14:18:11.906 29254 29254 I Memory  : add()\r\n11-02 14:18:11.906 29254 29254 I Memory  : It's new\r\n11-02 14:18:11.906 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 2 offset 4219296 size 128\r\n11-02 14:18:11.906 29254 29254 I Memory  : add()\r\n11-02 14:18:11.906 29254 29254 I ModelBuilder: setOperandValue for operand 3 size 4\r\n11-02 14:18:11.906 29254 29254 I ModelBuilder: Copied small value to offset 0\r\n11-02 14:18:11.906 29254 29254 I ModelBuilder: setOperandValue for operand 4 size 4\r\n11-02 14:18:11.906 29254 29254 I ModelBuilder: Copied small value to offset 4\r\n11-02 14:18:11.906 29254 29254 I ModelBuilder: setOperandValue for operand 5 size 4\r\n11-02 14:18:11.906 29254 29254 I ModelBuilder: Copied small value to offset 8\r\n11-02 14:18:11.906 29254 29254 I ModelBuilder: setOperandValue for operand 6 size 4\r\n11-02 14:18:11.906 29254 29254 I ModelBuilder: Copied small value to offset 12\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 8 offset 2400848 size 288\r\n11-02 14:18:11.907 29254 29254 I Memory  : add()\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 9 offset 4219436 size 128\r\n11-02 14:18:11.907 29254 29254 I Memory  : add()\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 10 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 16\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 11 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 20\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 12 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 24\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 13 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 28\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 14 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 32\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 16 offset 3995772 size 2048\r\n11-02 14:18:11.907 29254 29254 I Memory  : add()\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 17 offset 4219576 size 256\r\n11-02 14:18:11.907 29254 29254 I Memory  : add()\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 18 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 36\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 19 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 40\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 20 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 44\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 21 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 48\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 23 offset 2400260 size 576\r\n11-02 14:18:11.907 29254 29254 I Memory  : add()\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 24 offset 4219844 size 256\r\n11-02 14:18:11.907 29254 29254 I Memory  : add()\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 25 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 52\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 26 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 56\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 27 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 60\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 28 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 64\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 29 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 68\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 31 offset 3997840 size 8192\r\n11-02 14:18:11.907 29254 29254 I Memory  : add()\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 32 offset 4220112 size 512\r\n11-02 14:18:11.907 29254 29254 I Memory  : add()\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 33 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 72\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 34 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 76\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 35 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 80\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 36 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 84\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 38 offset 2397036 size 1152\r\n11-02 14:18:11.907 29254 29254 I Memory  : add()\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 39 offset 4220636 size 512\r\n11-02 14:18:11.907 29254 29254 I Memory  : add()\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 40 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 88\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 41 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 92\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 42 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 96\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 43 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 100\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 44 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 104\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 46 offset 2376020 size 16384\r\n11-02 14:18:11.907 29254 29254 I Memory  : add()\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 47 offset 3431192 size 512\r\n11-02 14:18:11.907 29254 29254 I Memory  : add()\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 48 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 108\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 49 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 112\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 50 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 116\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 51 size 4\r\n11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 120\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 53 offset 4217608 size 1152\r\n11-02 14:18:11.908 29254 29254 I Memory  : add()\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 54 offset 4218772 size 512\r\n11-02 14:18:11.908 29254 29254 I Memory  : add()\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 55 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 124\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 56 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 128\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 57 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 132\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 58 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 136\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 59 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 140\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 61 offset 3962980 size 32768\r\n11-02 14:18:11.908 29254 29254 I Memory  : add()\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 62 offset 4221160 size 1024\r\n11-02 14:18:11.908 29254 29254 I Memory  : add()\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 63 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 144\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 64 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 148\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 65 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 152\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 66 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 156\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 68 offset 3436360 size 2304\r\n11-02 14:18:11.908 29254 29254 I Memory  : add()\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 69 offset 4222196 size 1024\r\n11-02 14:18:11.908 29254 29254 I Memory  : add()\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 70 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 160\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 71 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 164\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 72 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 168\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 73 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 172\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 74 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 176\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 76 offset 4141756 size 65536\r\n11-02 14:18:11.908 29254 29254 I Memory  : add()\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 77 offset 4223232 size 1024\r\n11-02 14:18:11.908 29254 29254 I Memory  : add()\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 78 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 180\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 79 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 184\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 80 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 188\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 81 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 192\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 83 offset 2369080 size 2304\r\n11-02 14:18:11.908 29254 29254 I Memory  : add()\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 84 offset 4207316 size 1024\r\n11-02 14:18:11.908 29254 29254 I Memory  : add()\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 85 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 196\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 86 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 200\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 87 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 204\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 88 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 208\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 89 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 212\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 91 offset 4006048 size 131072\r\n11-02 14:18:11.908 29254 29254 I Memory  : add()\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 92 offset 4224268 size 2048\r\n11-02 14:18:11.908 29254 29254 I Memory  : add()\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 93 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 216\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 94 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 220\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 95 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 224\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 96 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 228\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 98 offset 2102304 size 4608\r\n11-02 14:18:11.908 29254 29254 I Memory  : add()\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 99 offset 4226328 size 2048\r\n11-02 14:18:11.908 29254 29254 I Memory  : add()\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 100 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 232\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 101 size 4\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 236\r\n11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 102 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 240\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 103 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 244\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 104 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 248\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 106 offset 2106924 size 262144\r\n11-02 14:18:11.909 29254 29254 I Memory  : add()\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 107 offset 2403208 size 2048\r\n11-02 14:18:11.909 29254 29254 I Memory  : add()\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 108 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 252\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 109 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 256\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 110 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 260\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 111 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 264\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 113 offset 2097684 size 4608\r\n11-02 14:18:11.909 29254 29254 I Memory  : add()\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 114 offset 4228388 size 2048\r\n11-02 14:18:11.909 29254 29254 I Memory  : add()\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 115 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 268\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 116 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 272\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 117 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 276\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 118 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 280\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 119 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 284\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 121 offset 1835524 size 262144\r\n11-02 14:18:11.909 29254 29254 I Memory  : add()\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 122 offset 2401148 size 2048\r\n11-02 14:18:11.909 29254 29254 I Memory  : add()\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 123 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 288\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 124 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 292\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 125 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 296\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 126 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 300\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 128 offset 2392416 size 4608\r\n11-02 14:18:11.909 29254 29254 I Memory  : add()\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 129 offset 4230448 size 2048\r\n11-02 14:18:11.909 29254 29254 I Memory  : add()\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 130 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 304\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 131 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 308\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 132 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 312\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 133 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 316\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 134 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 320\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 136 offset 1573368 size 262144\r\n11-02 14:18:11.909 29254 29254 I Memory  : add()\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 137 offset 4232508 size 2048\r\n11-02 14:18:11.909 29254 29254 I Memory  : add()\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 138 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 324\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 139 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 328\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 140 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 332\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 141 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 336\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 143 offset 4137136 size 4608\r\n11-02 14:18:11.909 29254 29254 I Memory  : add()\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 144 offset 4234568 size 2048\r\n11-02 14:18:11.909 29254 29254 I Memory  : add()\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 145 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 340\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 146 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 344\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 147 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 348\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 148 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 352\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 149 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 356\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 151 offset 460 size 262144\r\n11-02 14:18:11.909 29254 29254 I Memory  : add()\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 152 offset 2398200 size 2048\r\n11-02 14:18:11.909 29254 29254 I Memory  : add()\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 153 size 4\r\n11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 360\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 154 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 364\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 155 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 368\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 156 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 372\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 158 offset 3431740 size 4608\r\n11-02 14:18:11.910 29254 29254 I Memory  : add()\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 159 offset 4236628 size 2048\r\n11-02 14:18:11.910 29254 29254 I Memory  : add()\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 160 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 376\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 161 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 380\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 162 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 384\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 163 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 388\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 164 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 392\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 166 offset 1311208 size 262144\r\n11-02 14:18:11.910 29254 29254 I Memory  : add()\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 167 offset 4238688 size 2048\r\n11-02 14:18:11.910 29254 29254 I Memory  : add()\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 168 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 396\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 169 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 400\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 170 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 404\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 171 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 408\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 173 offset 2371400 size 4608\r\n11-02 14:18:11.910 29254 29254 I Memory  : add()\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 174 offset 4240748 size 2048\r\n11-02 14:18:11.910 29254 29254 I Memory  : add()\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 175 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 412\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 176 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 416\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 177 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 420\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 178 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 424\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 179 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 428\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 181 offset 3438680 size 524288\r\n11-02 14:18:11.910 29254 29254 I Memory  : add()\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 182 offset 4242808 size 4096\r\n11-02 14:18:11.910 29254 29254 I Memory  : add()\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 183 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 432\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 184 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 436\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 185 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 440\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 186 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 444\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 188 offset 4208356 size 9216\r\n11-02 14:18:11.910 29254 29254 I Memory  : add()\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 189 offset 4246916 size 4096\r\n11-02 14:18:11.910 29254 29254 I Memory  : add()\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 190 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 448\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 191 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 452\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 192 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 456\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 193 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 460\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 194 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 464\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 196 offset 262616 size 1048576\r\n11-02 14:18:11.910 29254 29254 I Memory  : add()\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 197 offset 4251024 size 4096\r\n11-02 14:18:11.910 29254 29254 I Memory  : add()\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 198 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 468\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 199 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 472\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 200 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 476\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 201 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 480\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 203 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 484\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 204 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 488\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 205 size 4\r\n11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 492\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 206 size 4\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 496\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 207 size 4\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 500\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 208 size 4\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 504\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 210 offset 2406152 size 1025024\r\n11-02 14:18:11.911 29254 29254 I Memory  : add()\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 211 offset 4255132 size 4004\r\n11-02 14:18:11.911 29254 29254 I Memory  : add()\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 212 size 4\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 508\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 213 size 4\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 512\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 214 size 4\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 516\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 215 size 4\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 520\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 217 offset 4259148 size 8\r\n11-02 14:18:11.911 29254 29254 I Memory  : add()\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 219 size 4\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 524\r\n11-02 14:18:11.911 29254 29254 I ModelBuilder: copyLargeValuesToSharedMemory has 0 values.\r\n11-02 14:18:11.912 29254 29254 I GraphDump: // ModelBuilder::finish\r\n11-02 14:18:11.912 29254 29254 I GraphDump: digraph {\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d0 [style=filled fillcolor=black fontcolor=white label=\"0\\nTQ8A(1x224x224x3)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d1 [label=\"1: REF\\nTQ8A(32x3x3x3)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d2 [label=\"2: REF\\nTI32(32)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d3 [label=\"3: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d4 [label=\"4: COPY\\nI32 = 2\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d5 [label=\"5: COPY\\nI32 = 2\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d6 [label=\"6: COPY\\nI32 = 0\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d7 [label=\"7\\nTQ8A(1x112x112x32)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d8 [label=\"8: REF\\nTQ8A(1x3x3x32)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d9 [label=\"9: REF\\nTI32(32)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d10 [label=\"10: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d11 [label=\"11: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d12 [label=\"12: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d13 [label=\"13: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d14 [label=\"14: COPY\\nI32 = 0\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d15 [label=\"15\\nTQ8A(1x112x112x32)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d16 [label=\"16: REF\\nTQ8A(64x1x1x32)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d17 [label=\"17: REF\\nTI32(64)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d18 [label=\"18: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d19 [label=\"19: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d20 [label=\"20: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d21 [label=\"21: COPY\\nI32 = 0\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d22 [label=\"22\\nTQ8A(1x112x112x64)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d23 [label=\"23: REF\\nTQ8A(1x3x3x64)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d24 [label=\"24: REF\\nTI32(64)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d25 [label=\"25: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d26 [label=\"26: COPY\\nI32 = 2\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d27 [label=\"27: COPY\\nI32 = 2\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d28 [label=\"28: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d29 [label=\"29: COPY\\nI32 = 0\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d30 [label=\"30\\nTQ8A(1x56x56x64)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d31 [label=\"31: REF\\nTQ8A(128x1x1x64)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d32 [label=\"32: REF\\nTI32(128)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d33 [label=\"33: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d34 [label=\"34: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d35 [label=\"35: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d36 [label=\"36: COPY\\nI32 = 0\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d37 [label=\"37\\nTQ8A(1x56x56x128)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d38 [label=\"38: REF\\nTQ8A(1x3x3x128)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d39 [label=\"39: REF\\nTI32(128)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d40 [label=\"40: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d41 [label=\"41: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d42 [label=\"42: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d43 [label=\"43: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d44 [label=\"44: COPY\\nI32 = 0\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d45 [label=\"45\\nTQ8A(1x56x56x128)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d46 [label=\"46: REF\\nTQ8A(128x1x1x128)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d47 [label=\"47: REF\\nTI32(128)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d48 [label=\"48: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d49 [label=\"49: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d50 [label=\"50: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d51 [label=\"51: COPY\\nI32 = 0\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d52 [label=\"52\\nTQ8A(1x56x56x128)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d53 [label=\"53: REF\\nTQ8A(1x3x3x128)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d54 [label=\"54: REF\\nTI32(128)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d55 [label=\"55: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d56 [label=\"56: COPY\\nI32 = 2\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d57 [label=\"57: COPY\\nI32 = 2\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d58 [label=\"58: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d59 [label=\"59: COPY\\nI32 = 0\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d60 [label=\"60\\nTQ8A(1x28x28x128)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d61 [label=\"61: REF\\nTQ8A(256x1x1x128)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d62 [label=\"62: REF\\nTI32(256)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d63 [label=\"63: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d64 [label=\"64: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d65 [label=\"65: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d66 [label=\"66: COPY\\nI32 = 0\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d67 [label=\"67\\nTQ8A(1x28x28x256)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d68 [label=\"68: REF\\nTQ8A(1x3x3x256)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d69 [label=\"69: REF\\nTI32(256)\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d70 [label=\"70: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d71 [label=\"71: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d72 [label=\"72: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d73 [label=\"73: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d74 [label=\"74: COPY\\nI32 = 0\"]\r\n11-02 14:18:11.912 29254 29254 I GraphDump:     d75 [label=\"75\\nTQ8A(1x28x28x256)\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d76 [label=\"76: REF\\nTQ8A(256x1x1x256)\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d77 [label=\"77: REF\\nTI32(256)\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d78 [label=\"78: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d79 [label=\"79: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d80 [label=\"80: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d81 [label=\"81: COPY\\nI32 = 0\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d82 [label=\"82\\nTQ8A(1x28x28x256)\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d83 [label=\"83: REF\\nTQ8A(1x3x3x256)\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d84 [label=\"84: REF\\nTI32(256)\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d85 [label=\"85: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d86 [label=\"86: COPY\\nI32 = 2\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d87 [label=\"87: COPY\\nI32 = 2\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d88 [label=\"88: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d89 [label=\"89: COPY\\nI32 = 0\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d90 [label=\"90\\nTQ8A(1x14x14x256)\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d91 [label=\"91: REF\\nTQ8A(512x1x1x256)\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d92 [label=\"92: REF\\nTI32(512)\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d93 [label=\"93: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d94 [label=\"94: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d95 [label=\"95: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d96 [label=\"96: COPY\\nI32 = 0\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d97 [label=\"97\\nTQ8A(1x14x14x512)\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d98 [label=\"98: REF\\nTQ8A(1x3x3x512)\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d99 [label=\"99: REF\\nTI32(512)\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d100 [label=\"100: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d101 [label=\"101: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d102 [label=\"102: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d103 [label=\"103: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d104 [label=\"104: COPY\\nI32 = 0\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d105 [label=\"105\\nTQ8A(1x14x14x512)\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d106 [label=\"106: REF\\nTQ8A(512x1x1x512)\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d107 [label=\"107: REF\\nTI32(512)\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d108 [label=\"108: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.913 29254 29254 I GraphDump:     d109 [label=\"109: COPY\\nI32 = 1\"]\r\n11-02 14:18:11.922  1029 15537 I aiserver: IPUNNAdaptor getSupportedOperations_1_1\r\n11-02 14:18:11.922  1029 15537 I aiserver: AiModelMngrService getSupportedOperations_1_1\r\n11-02 14:18:11.923  1029 15537 E aiserver: AiModelMngrServic::getSupportedOperations_1_1 not relaxed Model.\r\n11-02 14:18:11.927  2375  2470 I OllieMsgCenter: publishEvt what:2135162884\r\n11-02 14:18:11.933 29254 29254 E libc    : Access denied finding property \"ro.hardware.chipname\"\r\n11-02 14:18:11.928 29254 29254 W benchmark_model: type=1400 audit(0.0:752014): avc: denied { read } for pid=29254 name=\"u:object_r:vendor_default_prop:s0\" dev=\"tmpfs\" ino=15480 scontext=u:r:shell:s0 tcontext=u:object_r:vendor_default_prop:s0 tclass=file permissive=0\r\n```\r\n\r\nTo confirm that NPU exists and is available\r\n\r\n```\r\nrobert@robert:~/Downloads$ adb shell lshal | grep neural\r\nWarning: Skipping \"android.frameworks.cameraservice.service@2.0::ICameraService/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.frameworks.displayservice@1.0::IDisplayService/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.frameworks.schedulerservice@1.0::ISchedulingPolicyService/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.frameworks.sensorservice@1.0::ISensorManager/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.frameworks.stats@1.0::IStats/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.audio.effect@5.0::IEffectsFactory/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.audio@5.0::IDevicesFactory/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.biometrics.fingerprint@2.1::IBiometricsFingerprint/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.bluetooth.audio@2.0::IBluetoothAudioProvidersFactory/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.bluetooth@1.0::IBluetoothHci/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.camera.provider@2.4::ICameraProvider/legacy/0\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.gatekeeper@1.0::IGatekeeper/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.gnss@1.0::IGnss/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.gnss@1.1::IGnss/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.gnss@2.0::IGnss/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.graphics.composer@2.1::IComposer/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.graphics.composer@2.2::IComposer/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.health@2.0::IHealth/backup\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.health@2.0::IHealth/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.ir@1.0::IConsumerIr/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.keymaster@3.0::IKeymasterDevice/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.light@2.0::ILight/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.media.c2@1.0::IComponentStore/software\": no information for PID 1021, are you root?\r\nWarning: Skipping \"android.hardware.memtrack@1.0::IMemtrack/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.nfc@1.0::INfc/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.nfc@1.1::INfc/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.nfc@1.2::INfc/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.power.stats@1.0::IPowerStats/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.power@1.0::IPower/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio.config@1.0::IRadioConfig/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio.config@1.1::IRadioConfig/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio@1.0::IRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio@1.1::IRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio@1.2::IRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio@1.3::IRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio@1.4::IRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.secure_element@1.0::ISecureElement/SIM1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.sensors@1.0::ISensors/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.soundtrigger@2.0::ISoundTriggerHw/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.thermal@1.0::IThermal/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.usb@1.0::IUsb/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.vibrator@1.0::IVibrator/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.wifi.supplicant@1.0::ISupplicant/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.wifi.supplicant@1.1::ISupplicant/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.wifi.supplicant@1.2::ISupplicant/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.wifi@1.0::IWifi/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.wifi@1.1::IWifi/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.wifi@1.2::IWifi/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.wifi@1.3::IWifi/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/SIM1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/ashmem\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/backup\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/eid\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/hiaiserver\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/hiaiserver_modelmanager\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/hiaiserver_v2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/huawei.camera.cfgsvr\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/huawei.cameraresource.service\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/huaweiantitheft\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/huaweisigntool\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/hwfactoryinterface_hal\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/hwhiview_hal\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/hwsched\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/hwstp\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/hwvoiceid\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/ipuadaptor\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/legacy/0\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/perfgenius\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/perfpolicy\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/rildi\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/software\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/uniperf\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/virtualcamera.streamchange\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.system.net.netd@1.0::INetd/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.system.net.netd@1.1::INetd/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.system.suspend@1.0::ISystemSuspend/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.system.wifi.keystore@1.0::IKeystore/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.activity_recognition@1.0::IActivityRecognition/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.activity_recognition@1.1::IActivityRecognition/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.audioremote@1.0::IAudioRemoteConnect/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.biometrics.fingerprint@2.1::IExtBiometricsFingerprint/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.biometrics.fingerprint@2.2::IExtBiometricsFingerprint/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.biometrics.hwfacerecognize@2.0::IBiometricsFaceRecognize/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.biometrics.hwsecurefacerecognize@2.0::ISecureBiometricsFaceRecognize/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.biometrics.hwvoiceid@2.0::IBiometricsVoiceId/hwvoiceid\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.bluetooth@1.0::IHwBluetoothHciExt/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.bluetooth@1.1::IHwBluetoothHciExt/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.camera.camResource@1.0::IHwCameraResourceService/huawei.cameraresource.service\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.camera.camResource@1.1::IHwCameraResourceService/huawei.cameraresource.service\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.camera.camResource@1.2::IHwCameraResourceService/huawei.cameraresource.service\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.camera.camResource@1.3::IHwCameraResourceService/huawei.cameraresource.service\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.camera.cfgsvr@1.0::IHwCamCfgSvr/huawei.camera.cfgsvr\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.camera.cfgsvr@1.1::IHwCamCfgSvr/huawei.camera.cfgsvr\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.camera.factory@1.0::ICameraFactory/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.camera.vircamera@1.0::IVirCameraChannel/virtualcamera.streamchange\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.dolby.dms@1.0::IDms/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.dubai@1.0::IDubaiManager/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.dubai@1.1::IDubaiManager/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.eid@1.0::IEid/eid\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.eid@1.1::IEid/eid\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.fm@1.0::IFmControl/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.fusd@1.0::IFusdLbs/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.fusd@1.1::IFusdLbs/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.fusd@1.2::IFusdLbs/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.fusd@1.3::IFusdLbs/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.fusd@1.4::IFusdLbs/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.fusd@1.5::IFusdLbs/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.fusd@1.6::IFusdLbs/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.gnss@2.0::IHWGnss/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.graphics.hwcinterface@1.0::IHwcInterface/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.graphics.hwcinterface@1.1::IHwcInterface/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.graphics.mediacomm@2.0::IMediaComm/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.graphics.mediacomm@2.1::IMediaComm/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hinetmanager@1.0::IHinetmanagerDevice/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hisiradio@1.0::IHisiRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hisiradio@1.1::IHisiRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hisiradio@1.2::IHisiRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hisiradio@1.3::IHisiRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hivrar@2.0::IHiVRAR/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hivrar@2.1::IHiVRAR/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.huaweiantitheft@1.0::IHuaweiAntiTheft/huaweiantitheft\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.huaweisigntool@1.0::ISignTool/huaweisigntool\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwdisplay.displayengine@1.0::IDisplayEngineWrapper/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwdisplay.displayengine@1.1::IDisplayEngineWrapper/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwdisplay.displayengine@1.2::IDisplayEngineWrapper/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwdisplay@1.0::IDisplay/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwfactoryinterface@1.0::IHwFactoryInterface/hwfactoryinterface_hal\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwfactoryinterface@1.1::IHwFactoryInterface/hwfactoryinterface_hal\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwfs@1.0::IHwfs/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwhiview@1.0::IHwHiviewInterface/hwhiview_hal\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwhiview@1.1::IHwHiviewInterface/hwhiview_hal\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwsched@1.0::ISched/hwsched\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwstp@1.0::IHwStp/hwstp\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwstp@1.1::IHwStp/hwstp\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwvibrator@1.0::IHWVibrator/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwvibrator@1.1::IHWVibrator/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.iawareperf@1.0::IUniPerf/uniperf\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.iawareperfpolicy@1.0::IPerfPolicy/perfpolicy\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.iked@1.0::IIkedDevice/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.jpegdec@1.0::IJpegDecode/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.libteec@3.0::ILibteecGlobal/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.light@2.0::ILight/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.modemchr@1.0::IModemchrDevice/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.motion@1.0::IMotion/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.nfc@1.0::IHWNfc/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.perfgenius@2.0::IPerfGenius/perfgenius\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.power@1.0::IHWPower/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio.chr@1.0::IRadioChr/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio.deprecated@1.0::IOemHook/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio.ims@2.0::IRadioIms/rildi\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio@2.0::IRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.sensors@1.0::ISensors/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.sensors@1.1::ISensors/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.tp@1.0::ITouchscreen/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.wifi.supplicant@3.0::ISupplicant/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.wifi@1.0::IHwWifiExt/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.wifi@1.1::IHwWifiExt/default\": cannot be fetched from service manager (null)\r\nDM,FC Y android.hardware.neuralnetworks@1.0::IDevice/ipuadaptor                                               N/A        N/A    \r\nDM,FC Y android.hardware.neuralnetworks@1.1::IDevice/ipuadaptor                                               N/A        N/A    \r\n```\r\n\r\n", "comments": ["Update: Appears to be a problem of benchmark_model not having the correct permissions because when i have executed the same model inside a custom app, i don't have the problem. \r\nShould i close this issue and report as permission issue with android benchmark_model ?", "@miaowang14 Can you take a look?\r\n"]}, {"number": 52913, "title": "[TFLite] Add int16x8 support for RELU_N1_TO_1 and PRELU operators", "body": "Hello,\r\n\r\nThis PR adds int16x8 support for RELU_N1_TO_1 and PRELU operators.\r\n\r\nThanks,\r\nJohan.", "comments": ["@miaout17 Can you please review this PR ? Thanks!", "@miaout17 Can you please review this PR ? Thanks!", "@johan-gras  Can you please sign CLA. Thank you!"]}, {"number": 52907, "title": "tensor lost shape info after tf.math.unsorted_segment_*", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: Python 3.8.5\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 11.1/n/a\r\n- GPU model and memory: GeForce GTX 1070 8117MiB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass M1(tf.keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super(M1, self).__init__(**kwargs)\r\n        self.fc = tf.keras.layers.Dense(units=1)\r\n\r\n    @tf.function\r\n    def call(self, inputs, training=None):\r\n        x, y, z = inputs\r\n        x = tf.math.unsorted_segment_sum(x, tf.squeeze(y), z)\r\n        return self.fc(x)\r\n\r\n\r\nclass M2(tf.keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super(M2, self).__init__(**kwargs)\r\n        self.fc = tf.keras.layers.Dense(units=1)\r\n\r\n    @tf.function\r\n    def call(self, inputs, training=None):\r\n        x, y, z = inputs\r\n        x = tf.math.segment_sum(x, tf.squeeze(y))\r\n        return self.fc(x)\r\n\r\n\r\ndef gen():\r\n    for _ in range(1024):\r\n        offset = np.random.randint(1, 10, size=1024)\r\n        y = np.repeat(np.arange(1024), offset)\r\n        z = 1024\r\n        x = np.random.rand(offset.sum(), 32)\r\n        yield (x, y, z), np.random.rand(1024)\r\n\r\n\r\nds = tf.data.Dataset.from_generator(\r\n    gen,\r\n    output_signature=((tf.TensorSpec(shape=(None, 32), dtype=tf.float64),\r\n                      tf.TensorSpec(shape=(None, ), dtype=tf.int64),\r\n                      tf.TensorSpec(shape=[], dtype=tf.int32)),\r\n                       tf.TensorSpec(shape=(None, ), dtype=tf.float64))\r\n)\r\n\r\n\r\n\r\nm1, m2 = M1(), M2()\r\nm1.compile(loss=tf.keras.losses.MeanSquaredError(),\r\n           optimizer=tf.keras.optimizers.Adagrad())\r\nm2.compile(loss=tf.keras.losses.MeanSquaredError(),\r\n           optimizer=tf.keras.optimizers.Adagrad())\r\n# tf.math.segment_* is OK\r\nm2.fit(ds, epochs=8)\r\n# tf.math.unsorted_segment_* FAILED\r\nm1.fit(ds, epochs=8)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nTraceback (most recent call last):\r\n  File \"/tmp/pycharm_project_124/python/test/dev.py\", line 54, in <module>\r\n    m1.fit(ds, epochs=8)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 1184, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 933, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 759, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3066, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3463, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3298, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1007, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 668, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 994, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in user code:\r\n\r\n    /usr/local/miniconda3/lib/python3.8/site-packages/keras/engine/training.py:853 train_function  *\r\n        return step_function(self, iterator)\r\n    /tmp/pycharm_project_124/python/test/dev.py:14 call  *\r\n        return self.fc(x)\r\n    /usr/local/miniconda3/lib/python3.8/site-packages/keras/engine/base_layer.py:1030 __call__  **\r\n        self._maybe_build(inputs)\r\n    /usr/local/miniconda3/lib/python3.8/site-packages/keras/engine/base_layer.py:2659 _maybe_build\r\n        self.build(input_shapes)  # pylint:disable=not-callable\r\n    /usr/local/miniconda3/lib/python3.8/site-packages/keras/layers/core.py:1175 build\r\n        raise ValueError('The last dimension of the inputs to `Dense` '\r\n\r\n    ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n\r\n\r\nProcess finished with exit code 1\r\n", "comments": ["@sonogoto \r\nPlease refer to similar error issue and let us know:[link](https://stackoverflow.com/questions/56918388/error-valueerror-the-last-dimension-of-the-inputs-to-dense-should-be-defined),[link2](https://github.com/tensorflow/tensorflow/issues/35464)", "> @sonogoto Please refer to similar error issue and let us know:[link](https://stackoverflow.com/questions/56918388/error-valueerror-the-last-dimension-of-the-inputs-to-dense-should-be-defined),[link2](https://github.com/tensorflow/tensorflow/issues/35464)\r\n\r\nI do not think it is the same issue with [link2](https://github.com/tensorflow/tensorflow/issues/35464) since there is no broadcast operation in my code at all.", "@sonogoto \r\nDoes link1 help.", "> @sonogoto Does link1 help.\r\n\r\nI'm afraid not", "Was able to reproduce your issue in Tensorflow 2.7, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/b3b8c22a5692fd3f8d410a092a02e708/52907.ipynb). Thanks!", "> Was able to reproduce your issue in Tensorflow 2.7, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/b3b8c22a5692fd3f8d410a092a02e708/52907.ipynb). Thanks!\r\n\r\nIt was"]}, {"number": 52896, "title": "tf.linalg.triangular_solve different behaviour on CPU vs GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- https://colab.research.google.com/drive/1uTUzEQXDR6JvCkFQyS-3n2pAUKAH-TLk?usp=sharing#scrollTo=BcSb0SDQ5nI0\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (also Colab)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.3 (also Colab)\r\n- Python version: 3.7.10\r\n\r\n**Describe the current behavior**\r\nWhen trying to apply tf.linalg.triangular_solve on an invalid input matrix (in my case I included np.nan in the matrix) - different behaviour is observed when running on CPU vs GPU. On GPU - the functions returns NaNs, whereas on CPU an InvalidArgumentError occurs\r\n\r\n**Describe the expected behavior**\r\nConsistent behaviour between CPU & GPU\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1uTUzEQXDR6JvCkFQyS-3n2pAUKAH-TLk?usp=sharing#scrollTo=BcSb0SDQ5nI0\r\n\r\n\r\n", "comments": ["@qiq208,\r\n\r\nAs per the colab link provided, with CPU we get `InvalidArgumentError` and with GPU, it returns NaNs whereas in the description you have mentioned in the opposite way. So I've updated the issue description.", "I am able to reproduce the issue reported, in colab with both TF `2.6.0` and nightly `2.8.0-dev20211101`. Please find the [gist here](https://colab.sandbox.google.com/gist/sanatmpa1/3e3ef7ffc3c1fff14c57d2b112679d38/triangular_solve_cpu_gpu.ipynb)"]}, {"number": 52887, "title": "Converting from TensorFlow model input to MHLO", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.3\r\n- Are you willing to contribute it (Yes/No):\r\nYes\uff0cI would like to but have no ideas\r\n\r\n**Describe the feature and the current behavior/state.**\r\nHave no guide on how to convert a TF model to MLIR, and hope that build up a doc that shows how to make the conversion. Meanwhile, it would be better if there are more tutorials about TF and MLIR. \r\n\r\n**Will this change the current api? How?**\r\nHave no ideas\r\n\r\n**Who will benefit with this feature?**\r\nThe people who would like to use TensorFlow as the frontend in MLIR. It will enhance the combination of\r\nthe TensorFlow eco-system and MLIR eco-system.\r\n\r\n**Any Other info.**\r\n", "comments": ["Thanks for filing this, to add context from origin thread, the request was for \"standalone infrastructure that imports the TF model and exports the MHLO\".  I'll refrain this issue to be scoped to that and assign to bridge team for comment.\r\n\r\nI think there are 2 parts here 1) documenting how to perform the conversion (this has been requested in a couple of threads) and 2) the standalone tool.", "Thank Jacques for the refinement. The MLIR thread is repeated [here](https://llvm.discourse.group/t/convert-tensorflow-tf-lite-model-to-mhlo-linalg-standalone/4611/2), which maybe help the guys who are interested in TF and MLIR, hopefully.", "Hi Chunjie.  I work with Jacques on the part of the TF stack (bridge team) that can do this conversion.  We are looking at making a standalone tool for this. ", "@paynecl Quite excited to get this news . Looking forward to this feature. Thanks ahead:)", "Sorry for the late response.\r\n\r\nIf I understand correctly, you want to convert a saved model loaded by --savedmodel-objectgraph-to-mlir to MHLO? You could do so by \r\n\r\n``` tf-opt out.mlir -tf-standard-pipeline | tf-opt -tf-to-hlo-pipeline ```\r\n\r\nIf you want to do this in a programmatic way, then you could use CreateTFStandardPipeline and createConvertToMHLOPass.\r\n \r\nWe will add a documentation covering different use-cases.\r\n\r\nLet us know if this doesn't address your use-case.", "This may run into saved model dialect remaining. Is there perhaps an open SavedModel example you are looking at @Jackwin ? That could allow ensuring the instructions work.", "Hi @smit-hinsu & @jpienaar \r\nI randomly picked a saved model named **faster_rcnn_resnet50_v1_640x640** from the [tensorhub](https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1) in TF version 2.0.\r\n\r\n> If I understand correctly, you want to convert a saved model loaded by --savedmodel-objectgraph-to-mlir to MHLO?\r\n\r\nYes. Have tried the following instructions to translate a saved model to mlir, and got the TF excecute dialect.\r\n```\r\ntf-mlir-translate --savedmodel-objectgraph-to-mlir \\\r\n--tf-savedmodel-exported-names=predict  \\\r\n./faster_rcnn_resnet50_v1_640x640 \\\r\n-o faster_rcnn_resnet50_v1_640x640.mlir\r\n```\r\nWhen converting from TF execute dialect to MHLO, it seems not to work since the output still keeps the TF execute dialect. In this process, several passes have been inserted in `tf-opt` without any effect, such as `--tf-to-hlo-pipeline`, `-tf-saved-model-optimize-global-tensors`, and `-tf-saved-model-optimize-global-tensors`.  Hope that you could provide an example.\r\n\r\n> If you want to do this in a programmatic way, then you could use CreateTFStandardPipeline and createConvertToMHLOPass.\r\n\r\nNot familiar with these two passes. It would be great if any docs could explain them.\r\n\r\nIn summary, my concerns are\r\n1. what passes should be picked to complete the translation from a saved model to MHLO?\r\n2. As the saved model usually contains the data, is there any method of getting rid of the data, and just focusing on the graph expression in MLIR? Because the data size in the saved model is quite huge, not friendly to read the MLIR.\r\n\r\nThanks.\r\n\r\n", "Hi folks, any update about this issue? Thanks", "We need to use `--savedmodel-signaturedefs-to-mlir` instead of `--tf-savedmodel-exported-names=predict` for TF v1 models.\r\n\r\nCurrently, there is no command line method to freeze variables. So, we need to make use of the TFLite Converter with a slight code change. Just remove the block from L247 to L265 [here.](https://github.com/tensorflow/tensorflow/blob/fec390fc795df46555aa79ab89146c3cc3769a1f/tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc#L247)\r\n\r\nAfter that the model can be imported in TF dialect with:\r\n\r\n```tf_tfl_translate -savedmodel-signaturedefs-to-mlir  --tf-savedmodel-exported-names=serving_default -o ~/Downloads/resnet/translate_tflite_input.mlir  ~/Downloads/resnet/```\r\n\r\nThen, we will need to change shape type of argument for @serving_default function from tensor<?x?x?x?xf32> to tensor<?x224x224x3xf32> and similar change for the op that is using this argument.\r\n\r\nAfter that we could run the following command to convert it to mhlo.\r\n\r\n```tf-opt -tf-to-hlo-pipeline  -o ~/Downloads/resnet/translate_mhlo.mlir ~/Downloads/resnet/translate_tf.mlir```\r\n\r\nRegarding getting rid of the data, there is a mechanism to use splat constants but that is not available for saved model. See [here](https://github.com/tensorflow/tensorflow/blob/ebe7d691f706e4d40a7569fec719a7d1e7404073/tensorflow/compiler/mlir/tensorflow/translate/tf_mlir_translate_registration.cc#L58)\r\n\r\nWhat is the use-case you are trying? Let us know if you have any other questions."]}]