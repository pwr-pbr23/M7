[{"number": 21699, "title": "[cmake/linux] Add many systemlib_* options [WIP, don't review]", "body": "Replaces this one: https://github.com/tensorflow/tensorflow/pull/21031\r\n\r\nTHIS PR is currently still a **work in progress** and I may **force push** at any time. Please don't review.\r\nThe patch will eventually be used in debian packaging. https://github.com/tensorflow/tensorflow/issues/720", "comments": ["Nagging Assignee @protoget: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "well, I'm not interested in the cmake build anymore."]}, {"number": 21698, "title": "INT TFLITE very much slower than FLOAT TFLITE", "body": "Hi Guys. I am using Mobilenet 0.25,128. I used the pretrained models provided in the repo for obtaining the int tflite and float tflite models for the same [found here](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). I am trying to infer some images. Using the imagenet images and some other test images as well, the FLOAT TFLITE is faster than the INT TFLITE (FLOAT TFLITE takes roughly 3-4 milliseconds while INT one takes 8-9 ms). Any suggestions as to why this is happening ?I am running the inferences using the tflite interpreter following the documentation [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#interpreter). This issue happens with the tf-nightly builds as well as the normal tensorflow. Tried on both, and also on both python 2 as well as 3.\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:Binary\r\n- **TensorFlow version (use command below)**:1.10 (tf-nightly) \r\n- **Python version**:2\r\n- **Bazel version (if compiling from source)**\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: No\r\n- **GPU model and memory**: No\r\n\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\n\r\n# ####INT TFLITE CODE####\r\n\r\n```\r\n#Load TFLite model and allocate tensors. Select the appropriate model and give its path\r\ninterpreter = tf.contrib.lite.Interpreter(model_path='mobilenet_v1_0.25_128_quant.tflite')\r\n\r\n\r\n\r\n#For allocating the model tensors\r\ninterpreter.allocate_tensors()\r\n\r\n#Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n#print(input_details)\r\n#print(output_details)\r\n\r\nk=1\r\nfor image_path in TEST_IMAGE_PATHS:\r\n\t#Loading the image and resizing into the correct shape\r\n\t#Change the .astype(np.uint8) (used when using int tflite) to .astype(np.float32) if using float tflite\r\n\timg = np.array(PIL.Image.open(image_path).resize((128, 128))).astype(np.uint8)\r\n\r\n\timg = img.reshape(1,128,128,3)\r\n\t#print(input_details)\r\n\t#print (img.shape)\r\n\t\r\n\t#Setting the input image to the input tensor\r\n\tinterpreter.set_tensor(input_details[0]['index'], img)\r\n\r\n\t#Running inference and timing it\r\n\tstart = time.time()\r\n\tinterpreter.invoke()\r\n\tend = time.time()\r\n\r\n\t#Getting the output information\r\n\toutput_data = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n\t#Converting the prediction into human readable form\r\n\t#label_map = imagenet.create_readable_names_for_imagenet_labels()  \r\n\tprint(\"Top 1 Prediction: \", output_data.argmax()-1, output_data.max(), k+1,labels[output_data.argmax()-1])\r\n\t\r\n\ttotal_infer_time+=(end-start)\r\n\tk+=1\r\n\r\n#printing average inference time along with the accuracy\r\nprint(\"Total infer time avg (in seconds) == \",total_infer_time/no_images)\r\n\r\n\r\n\r\n```\r\n# ####FLOAT TFLITE CODE####\r\n```\r\n#Load TFLite model and allocate tensors. Select the appropriate model and give its path\r\ninterpreter = tf.contrib.lite.Interpreter(model_path='mobilenet_v1_0.25_128.tflite')\r\n\r\n\r\n#For allocating the model tensors\r\ninterpreter.allocate_tensors()\r\n\r\n#Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n#print(input_details)\r\n#print(output_details)\r\nk = 1           \r\nfor image_path in TEST_IMAGE_PATHS:\r\n\tprint image_path\r\n\t#Loading the image and resizing into the correct shape\r\n\t#Change the .astype(np.uint8) (used when using int tflite) to .astype(np.float32) if using float tflite\r\n\timg = np.array(PIL.Image.open(image_path).resize((128, 128))).astype(np.float32) / 128 - 1\r\n\r\n\timg = img.reshape(1,128,128,3)\r\n\t#print (img.shape)\r\n\t\r\n\t#Setting the input image to the input tensor\r\n\tinterpreter.set_tensor(input_details[0]['index'], img)\r\n\r\n\t#Running inference and timing it\r\n\tstart = time.time()\r\n\tinterpreter.invoke()\r\n\tend = time.time()\r\n\r\n\t#Getting the output information\r\n\toutput_data = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n\t#Converting the prediction into human readable form\r\n\t#label_map = imagenet.create_readable_names_for_imagenet_labels()  \r\n\t#print(\"Top 1 Prediction: \", output_data.argmax()-1, output_data.max(), k+1,labels[output_data.argmax()-1])\r\n\t\r\n\t#detect_dog(output_data)\r\n\r\n\r\n\t'''\r\n\t#For accuracy\r\n\tnumber = []\r\n\tnumber = int(mobilenet_labels[k])\r\n\t        \r\n\r\n\tif(number==output_data.argmax()):\r\n\t    accuracy+=1\r\n\r\n\t'''\r\n\ttotal_infer_time+=(end-start)\r\n\tprint total_infer_time/(k+1)\r\n\tk+=1\r\n\r\n#printing average inference time along with the accuracy\r\nprint(\"Total infer time avg (in seconds) == \",total_infer_time/no_images)\r\n\r\n``````\r\n\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "This is likely because quantized int requires an arm neon to be faster than float. On a PC (which is what I assume you are running on, float is likely better). This is because quantized int relies on special instructions that have not been emphasized on intel x86_64.", "It makes sense. Thanks. That means that it should give good time improvement when I run it on raspberry pi or some other arm device. Thanks a ton. I'll reopen the issue if there's further issue. "]}, {"number": 21697, "title": "ProfilerHook generates wrong timeline traces", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A\r\n- **TensorFlow installed from (source or binary)**:dockerhub 1.9.0-devel-gpu\r\n- **TensorFlow version (use command below)**:1.9.0\r\n- **Python version**:2.7.12\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:9.0/7.1\r\n- **GPU model and memory**:Tesla V100 16GB\r\n- **Exact command to reproduce**:\r\n        model\uff1ahttps://github.com/tensorflow/models/tree/master/research/deep_speech\r\n        command:python deep_speech.py --model_dir=./deepSpeech --train_data_dir=./librispeech_data/train-clean-100/LibriSpeech/train-clean-100.csv --eval_data_dir=./librispeech_data/dev-clean/LibriSpeech/dev-clean.csv --num_gpus=-1 --wer_threshold=0.23 --seed=1 --hooks=profilerhook**\r\n     \r\n### Describe the problem\r\n     The time of the timeline trace  is not correct\uff0c which lasts hundreds of years.It's correct at the beginning, and becoming abnormal from the third trace\uff08the 21th step for save_steps=10\uff09.   \r\n\r\n### Source code / logs\r\n![image](https://user-images.githubusercontent.com/4105051/44328172-71ba9080-a493-11e8-8926-92249de87f76.png)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nMobile device", "I am having a similar problem. If someone can look into this, that will be great!", "@tensorflowbutler Yes", "Nagging Assignee @tatianashp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "same here", "@petermattson  - Can you take a look at this bug?", "Hi Tatiana -- I'm not sure who owns timelines, but unfortunately that\nperson is not me.\n\nOn Fri, Nov 16, 2018 at 5:37 PM Tatiana Shpeisman <notifications@github.com>\nwrote:\n\n> @petermattson <https://github.com/petermattson> - Can you take a look at\n> this bug?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21697#issuecomment-439576428>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AhFaHY-S_R1h2Om-mCJG7ihlLrugNXcZks5uv2hGgaJpZM4WCd6r>\n> .\n>\n", "I've run into this problem also. Haven't had a chance to dig in yet, but a few more notes about the issue:\r\n1) In my application, the shift appears to happen on every session.run, but the ops that get shifted are not always consistent.\r\n2) The amount of the shift appears to be (2^64 / 1000) us, or 2^64 ns. This suggests that the timing values might be stored as 64-bit integers representing timing in nanoseconds. They might be overflowing or the overflow is not being handled correctly.", "Is this still an issue with the latest version of TF?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=21697\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=21697\">No</a>\n"]}, {"number": 21696, "title": "TensorFlow GDR RDMA verbs compilation failure on 1.10 ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux 7.4.1708\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.0\r\n- **CUDA/cuDNN version**: 9.2.1.88\r\n- **GPU model and memory**: nVidia Volta V100 16GB\r\n- **Exact command to reproduce**: bazel build --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nI attempted to compile TensorFlow 1.10.0 using RDMA/VERBS options+GDR options as I'm using an all infiniband EDR network with my Volta V100 HPC environment. The same compile-time strings for Bazel build worked correctly without this error in RC 1.0.8rc1 and this error was not thrown. I am using mvpachi2 + GNU GCC 7.2.0, CUDA 9.2.8.11 + Basel 0.16.0 at compile time. NCCL is at v 2.2.13. The error basel throws after 16xxx objects is as follows:\r\n                                                                                                                                      ^\r\n```\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option '-Wno-self-assign'\r\nERROR: /opt/ohpc/pub/apps/tensorflow_1.10/tensorflow/BUILD:576:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Aborted): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\n2018-08-18 16:25:22.109083: F tensorflow/core/framework/allocator_registry.cc:52] New registration for AllocatorFactory with name=BFCRdmaAllocator priority=101 at location tensorflow/contrib/gdr/gdr_memory_manager.cc:204 conflicts with previous registration at location tensorflow/contrib/verbs/rdma_mgr.cc:277\r\n\r\nINFO: Elapsed time: 934.788s, Critical Path: 236.40s\r\nINFO: 13405 processes: 13405 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n\r\n### Source code / logs\r\nThe above is about all I've gotten out of it, thus far. There is no obvious compile time log output from Basel-build that I can see of use. \r\n", "comments": ["And further, I needed to do this, to avoid the error in this post, related to SWIG.\r\n\r\nhttps://github.com/bazelbuild/bazel/issues/4053#issuecomment-343134886\r\n\r\n```\r\n--- tensorflow/tensorflow.bzl.orig\t2017-11-01 21:21:13.000000000 +0100\r\n+++ tensorflow/tensorflow.bzl\t2017-11-09 11:49:41.536361562 +0100\r\n@@ -920,6 +920,7 @@\r\n   args += [src.path]\r\n   outputs = [ctx.outputs.cc_out, ctx.outputs.py_out]\r\n   ctx.action(\r\n+      use_default_shell_env = True,\r\n       executable=ctx.executable._swig,\r\n       arguments=args,\r\n       inputs=list(inputs),\r\n```", "i have fixed this by disable the verbs.", "I thought that verbs and gdr are supposed to be mutually exclusive build options, as they offer essentially the same functionality.\r\n\r\nIt would be possible to fix this error by changing the name of the allocator defined in one of these two places:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma_mgr.cc#L277\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gdr/gdr_memory_manager.cc#L204", "Added a PR #21766 for the compilation fix."]}, {"number": 21695, "title": "TensorFlow not logging any event files, tensorboard showing nothing", "body": "I'm currently running `TensorFlow 1.10.0`. My custom estimator is created with `tf.estimator.Estimator`, and run without a glitch. However, I don't find any event files under `model_dir `, and TensorBoard simply shows nothing.\r\n\r\nHere's how I setup my estimator:\r\n```\r\nclassifier= tf.estimator.Estimator(\r\n    model_fn=lr_model_fn, model_dir=PATH)\r\n```\r\nand I open TensorBoard via `tensorboard --logdir=tf_models/ --host=127.0.0.1` where `tf_models` is the dir `PATH`.\r\n\r\nFiles logged in `model_dir ` include `checkpoint`, `graph.pbtxt`, `model.ckpt-*`, etc. No `events.out.tfevents*` files reside there. Is this normal? Did I have something misconfigured?\r\n\r\nEDIT: \r\nIt seems that it only outputs `events.out.tfevents.*` file the very first time in a Jupyter session. If I clean up the PATH `model_dir` and re-train the model whatever times afterwards, there's always a events file missing there. And I have to restart Jupyter to make it work again. Is this a bug or something?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "You can use tf.summary to write the data you want to visualize to the log directory.\r\nAlso, this question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "It may happen when you delete the previously defined `model_dir` in your `Estimator` configuration: `FileWriter` will fail to write in the deleted directory (even a directory with the same path is created).\r\n\r\nYou can use `tf.summary.FileWriterCache.clear()` to clear current cache if this happens. You will lose anterior cached summaries though.\r\n", "> It may happen when you delete the previously defined `model_dir` in your `Estimator` configuration: `FileWriter` will fail to write in the deleted directory (even a directory with the same path is created).\r\n> \r\n> You can use `tf.summary.FileWriterCache.clear()` to clear current cache if this happens. You will lose anterior cached summaries though.\r\n\r\nthis is exactly the solution.", "How do we do this in tf2?", "FYI for tf2 `tf.summary.FileWriterCache.clear()` is now in `tf.compat.v1.summary.FileWriterCache.clear()`\r\n\r\nAlso because i didn't understand what i was doing i eventually realised i needed this to be called just before my training.\r\n\r\ntf.compat.v1.summary.FileWriterCache.clear()\r\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"]}, {"number": 21694, "title": "[INTEL MKL] Upgrade mkldnn to 0.16 for bazel build", "body": "upgrade mkldnn from version 0.15 to 0.16 for bazel build", "comments": []}, {"number": 21693, "title": "GPU Kernels for the RandomUniform Op", "body": "@alextp Is there a particular reason why there is an int32 type constraint on the shape input to the RandomUniform op GPU kernels? It's currently forcing some ops on the CPU and due to initialization colocation constraints this propagates to some variables and their updates making training about twice as slow on a machine with a Pascal 1080. Thanks!", "comments": ["Because shapes need to be on the host since shapes guide memory allocation\nand memory allocation happens on the host and int32s are used for the shape\nof the randomuniform op.\n\nOn Fri, Aug 17, 2018 at 3:49 PM Anthony Platanios <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> Is there a particular reason why\n> there is an int32 type constraint on the shape input to the RandomUniform\n> op GPU kernels? It's currently forcing some ops on the CPU and due to\n> initialization colocation constraints this propagates to some variables and\n> their updates making training about twice as slow on a machine with a\n> Pascal 1080. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21693>, or mute the\n> thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxd_J_kD-FQVe9V7SXhGafVSpRrNdks5uR0iNgaJpZM4WCSla>\n> .\n>\n\n\n-- \n - Alex\n", "But the op allows for int64 tensors to be passed in for the shape. I\u2019m not talking about the host memory annotations but rather just the type constraint on the shape. The CPU kernel registrations for RandomUniform allow int64 shapes so why not allow them for the GPU kernels too?", "You can allow int64 shapes for GPUs but int64 tensors aren't automatically\nplaced on host memory so you need to be careful or your model will silently\nrun shape computations on the GPU and then block the host to synchronize\nthe stream before executing the random op. I'd accept a PR allowing int64\nthere, though you should be very careful when using it.\n\nOn Fri, Aug 17, 2018 at 4:03 PM Anthony Platanios <notifications@github.com>\nwrote:\n\n> But the op allows for int64 tensors to be passed in for the shape. I\u2019m not\n> talking about the host memory annotations but rather just the type\n> constraint on the shape. The CPU kernel registrations for RandomUniform\n> allow int64 shapes so why not allow them for the GPU kernels too?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21693#issuecomment-414008189>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxen6ekb_9UZp0xYIuMbI1wvns4ITks5uR0vMgaJpZM4WCSla>\n> .\n>\n\n\n-- \n - Alex\n", "For now I have temporarily changed my API so all shape inputs to RandomUniform are cast to int32 automatically. I believe that allowing int64 is not an elegant solution given the whole weirdness with int32 tensor support in GPUs. I believe that it would be more appropriate if TensorFlow constrained shape-related tensors to be int32 (e.g., that argument to the random uniform op), or documented somewhere what the deal is with int32 vs int64. I have some understanding currently of the underlying reasons, but it easily leads to suboptimal performance that is hard to debug. My MT model from processing 9K words per second while training to 25K once I forced that int32 cast **(note that this is a 3x performance gain achieved by adding a cast -- also note that I have 4 GPUs on that machine and now I can train 4 models in parallel at 25K words per second each, whereas before I would get ~4K words per second each because the CPU was becoming a constraint)**. This was due to colocation constraints propagating but I don't believe something like that should happen so easily.", "Nagging Assignee @tatatodd: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hey @alextp! I recently made a big update to TF Scala ([PR](https://github.com/eaplatanios/tensorflow_scala/pull/131)) that makes all of the graph construction API statically-typed (finally :))). This raised the following question that relates to this issue: whenever a shape is used when a tensor is expected, I implicitly convert the shape to a tensor (similarly to what the Python API does). However, I was converting to `Tensor[Long]` previously, because I had assumed shapes are represented as `int64` tensors. If I keep doing that though I run into issues like the one described above. \r\n\r\nSo, what should be the convention for implicit conversion of shapes to tensors? Should they be converted to `int32` tensors or `int64` tensors?\r\n\r\nI believe it would be good to make a decision about this and stick with it, given the awkward handling of `int32` tensors when it comes to GPUs. Also, what is currently happening in the Python API? Is it `int32` or `int64` that is used for these implicit conversions?\r\n\r\nThanks! :)", "Currently you'll have better performance on GPUs if you convert shapes to\nint32 tensors. We're finally close to getting rid of this problem, though.\n\nOn Wed, Oct 24, 2018 at 9:56 PM Anthony Platanios <notifications@github.com>\nwrote:\n\n> Hey @alextp <https://github.com/alextp>! I recently made a big update to\n> TF Scala (PR <https://github.com/eaplatanios/tensorflow_scala/pull/131>)\n> that makes all of the graph construction API statically-typed (finally\n> :))). This raised the following question that relates to this issue:\n> whenever a shape is used when a tensor is expected, I implicitly convert\n> the shape to a tensor (similarly to what the Python API does). However, I\n> was converting to Tensor[Long] previously, because I had assumed shapes\n> are represented as int64 tensors. If I keep doing that though I run into\n> issues like the one described above.\n>\n> So, what should be the convention for implicit conversion of shapes to\n> tensors? Should they be converted to int32 tensors or int64 tensors?\n>\n> I believe it would be good to make a decision about this and stick with\n> it, given the awkward handling of int32 tensors when it comes to GPUs.\n> Also, what is currently happening in the Python API? Is it int32 or int64\n> that is used for these implicit conversions?\n>\n> Thanks! :)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21693#issuecomment-432914008>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxdPaOuz458ahSgZ2h59P7Xms_Sukks5uoUR8gaJpZM4WCSla>\n> .\n>\n\n\n-- \n - Alex\n", "Sounds good! Thanks Alex! :)", "I'll close this issue since we know what the problem is and it'll be probably be dealt with more generally than just adding `int64` for the random ops."]}, {"number": 21692, "title": "Upgrade gradle to fix a resolution error", "body": "I am never an expert of gradle, but I needed these changes to avoid `Failed to resolve: androidx.test.espresso:espresso-core:3.1.0-alpha3` when building the demo app.  See #20828.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I am covered by Google Corporate CLA with IBM.", "CLAs look good, thanks!\n\n<!-- ok -->", "@jdduke, could you review the PR, thanks!", "Can you rebase? We no longer have the espresso dependency."]}, {"number": 21691, "title": "[INTEL MKL] Upgrade mkldnn to 0.16 for bazel build", "body": "Upgrade mkldnn to 0.16 for bazel build", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 21690, "title": "[INTEL MKL] Memory control to avoid caching large buffer for MKL primitives", "body": "This PR enables memory usage control: \r\nTo avoid caching large buffer for MKL primitives, \"caching\" MKL primitives in the following scenarios is prevented:\r\n1. conv2d backward - always by default (see bottom comment on the environment variable)\r\n2. conv2d forward: large batch size (> 32) and for old systems which do not support \r\n                               AVX2 or AVX512 (e.g. Ivebridge). \r\n3. conv2d forward: large batch size (>32) and for 1x1 kernel and stride != 1\r\n\r\nEnvironment variable TF_MKL_OPTIMIZE_PRIMITVE_MEMUSE is \"true\" by default or not set. \r\nTo enable primitive reuse for all ops and all cases, set it to \"false\". ", "comments": ["Could you please also address the comment abou private class member  'bool do_not_cache_'. I don't understand why it's a class member instead of a local variable given that it's used in just one method.", "Sure, I will address your review comments. Thanks", "Hi, I committed in new changes by replacing the private member \"do_not_cache_\" with a local\r\nvariable \"do_not_cache\", in 3 source files. \r\n\r\nPlease let me know if there is any other blocker."]}, {"number": 21689, "title": "Simplyfing the evaluation step", "body": "Simplyfing the evaluation step by taking argmax of the softmax of the predictions instead of tf.multinomial. This makes it easier to understand.", "comments": ["@alextp @yifeif ", "Done", "@alextp Resolved merge conflicts", "If all the required tests have passed, shouldn't the copybara bot merge this PR?", "A lot of tests failed, apparently.", "@protoget Can you merge this?", "This is the fifth time that these tests have failed lol \ud83d\ude02\ud83d\ude02.\r\n\r\nNone of them are failing because of this PR."]}, {"number": 21688, "title": "[INTEL MKL] Enable Relu 3D Ops", "body": "The enhancement enables Relu 3D operations. \r\n\r\nThis PR should be merged AFTER Relu primitive relu PR (approved but not merged yet)\r\n    https://github.com/tensorflow/tensorflow/pull/19400", "comments": ["Rebase with master has been done to address conflicts.", "After merge conflict being addressed, there should not be any blocker. Thanks!"]}, {"number": 21687, "title": "ERROR: /bin/bash: line 1: 12830 Segmentation fault  ", "body": "Hi,\r\ni'm trying to install tensorflow with CUDA support and I have the following issue whe i execute\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\nERROR:\r\n\r\n/bin/bash: line 1: 12830 Segmentation fault      bazel-out/host/bin/tensorflow/create_tensorflow.python_api --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/k8-opt/genfiles/tensorflow --apiname=tensorflow --apiversion=1 --package=tensorflow.python --output_package=tensorflow bazel-out/k8-opt/genfiles/tensorflow/__init__.py bazel-out/k8-opt/genfiles/tensorflow/app/__init__.py bazel-out/k8-opt/genfiles/tensorflow/bitwise/__init__.py bazel-out/k8-opt/genfiles/tensorflow/compat/__init__.py bazel-out/k8-opt/genfiles/tensorflow/data/__init__.py bazel-out/k8-opt/genfiles/tensorflow/debugging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/distributions/__init__.py bazel-out/k8-opt/genfiles/tensorflow/dtypes/__init__.py bazel-out/k8-opt/genfiles/tensorflow/errors/__init__.py bazel-out/k8-opt/genfiles/tensorflow/feature_column/__init__.py bazel-out/k8-opt/genfiles/tensorflow/gfile/__init__.py bazel-out/k8-opt/genfiles/tensorflow/graph_util/__init__.py bazel-out/k8-opt/genfiles/tensorflow/image/__init__.py bazel-out/k8-opt/genfiles/tensorflow/io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/initializers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/activations/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/densenet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/inception_v3/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/mobilenet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/nasnet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/resnet50/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/vgg16/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/vgg19/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/xception/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/backend/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/callbacks/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/constraints/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/boston_housing/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/cifar10/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/cifar100/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/imdb/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/mnist/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/reuters/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/estimator/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/initializers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/layers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/losses/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/metrics/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/models/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/optimizers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/preprocessing/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/preprocessing/image/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/preprocessing/sequence/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/preprocessing/text/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/regularizers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/wrappers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/wrappers/scikit_learn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/layers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/linalg/__init__.py bazel-out/k8-opt/genfiles/tensorflow/logging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/losses/__init__.py bazel-out/k8-opt/genfiles/tensorflow/manip/__init__.py bazel-out/k8-opt/genfiles/tensorflow/math/__init__.py bazel-out/k8-opt/genfiles/tensorflow/metrics/__init__.py bazel-out/k8-opt/genfiles/tensorflow/nn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/nn/rnn_cell/__init__.py bazel-out/k8-opt/genfiles/tensorflow/profiler/__init__.py bazel-out/k8-opt/genfiles/tensorflow/python_io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/quantization/__init__.py bazel-out/k8-opt/genfiles/tensorflow/resource_loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/strings/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/builder/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/main_op/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/signature_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/signature_def_utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/tag_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/sets/__init__.py bazel-out/k8-opt/genfiles/tensorflow/sparse/__init__.py bazel-out/k8-opt/genfiles/tensorflow/spectral/__init__.py bazel-out/k8-opt/genfiles/tensorflow/summary/__init__.py bazel-out/k8-opt/genfiles/tensorflow/sysconfig/__init__.py bazel-out/k8-opt/genfiles/tensorflow/test/__init__.py bazel-out/k8-opt/genfiles/tensorflow/train/__init__.py bazel-out/k8-opt/genfiles/tensorflow/train/queue_runner/__init__.py bazel-out/k8-opt/genfiles/tensorflow/user_ops/__init__.py\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 4401.213s, Critical Path: 251.23s\r\nINFO: 7900 processes: 7900 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nOS: CentOS 6.2\r\nTensorflow installed from source.\r\nTensorflow version: 1.10\r\nPython version: 2.7\r\nBazel version: 0.16.0\r\nGCC version: 6.3.0\r\nCUDA 8.0\r\ncuDNN: 6.0\r\nGPU: GeForce GTX 770", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Can you please list out step by step what you did? So that we can try and reproduce it and then debug?", "Hi,\r\nThanks for the answer.\r\n1) I installed Bazel 0.16 in /share/apps/bazel running the script ./compile.sh\r\n2) I installed python dependencies numpy, dev, pip, wheel on python2.7.\r\n3) i set environment variables $PATH and LD_LIBRARY_PATH:\r\nmodule load python2.7\r\nmodule load gcc-4.8.2\r\nPATH=/share/apps/binutils/binutils-2.29.1/bin/:$PATH\r\nLD_LIBRARY_PATH=/share/apps/binutils/binutils-2.29.1/lib/:$LD_LIBRARY_PATH\r\nPATH=/share/apps/bazel-0.16.0/output/:$PATH\r\n4) i ran the script ./configure from tensorflow with CUDA 8-0 and cuDNN 6.0 \r\n5) I modified the file: tensorflow/tensorflow.bzl by adding the line:\r\n\r\n   args += [src.path]\r\n   outputs = [ctx.outputs.cc_out, ctx.outputs.py_out]\r\n   ctx.action(                  (l\u00ednea 1187)\r\n++  **use_default_shell_env = True,** \r\n       executable=ctx.executable._swig,\r\n       arguments=args,\r\n       inputs=list(inputs),\r\n\r\nBecause i had the following error:\r\nERROR: /local/jrodriguez.cct/tensor_nuevo/tensorflow-master/tensorflow/contrib/lite/python/interpreter_wrapper/BUILD:22:1: SWIGing tensorflow/contrib/lite/python/interpreter_wrapper/interpreter_wrapper.i failed (Exit 1): swig failed: error executing command \r\n\r\n6) FInally i ran the command:\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\nAnd I get the error I detailed above: Segmentation fault\r\n\r\nThanks Again.\r\n\r\n", "After Debugging the compilation i see that the line which gives me problems is the following:\r\n**os.execv(args[0], args)**\r\nIn the file:\r\n/root/.cache/bazel/_bazel_root/95b102d38f4be088a2028c5510fdf1c2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/**create_tensorflow.python_api** \r\n", "Nagging Assignee @rohan100jain: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21686, "title": "Error to compile tflite bechmark (rpi cross build) ", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.10\r\n- **Exact command to reproduce**: ./tensorflow/contrib/lite/build_rpi_lib.sh\r\n\r\nI'm having problems compilling tflite for rpi. \r\n\r\n```\r\n./tensorflow/contrib/lite/build_rpi_lib.sh\r\n+ set -e\r\n+++ dirname ./tensorflow/contrib/lite/build_rpi_lib.sh\r\n++ cd ./tensorflow/contrib/lite\r\n++ pwd\r\n+ SCRIPT_DIR=/home/thom/workspace/tensorflow/tensorflow/contrib/lite\r\n+ cd /home/thom/workspace/tensorflow/tensorflow/contrib/lite/../../..\r\n+ CC_PREFIX=arm-linux-gnueabihf-\r\n+ make -j 3 -f tensorflow/contrib/lite/Makefile TARGET=RPI TARGET_ARCH=armv7\r\n/bin/sh: 1: [[: not found\r\n/home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/bin/rpi_armv7/benchmark_model\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/../../../ -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.cc -o /home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/../../../ -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/tools/benchmark/benchmark_model.cc -o /home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/tools/benchmark/benchmark_model.o\r\nIn file included from ./tensorflow/contrib/lite/tools/benchmark/benchmark_model.h:26:0,\r\n                 from tensorflow/contrib/lite/tools/benchmark/benchmark_model.cc:16:\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h: In instantiation of \u2018tflite::benchmark::TypedBenchmarkParam<T>::TypedBenchmarkParam(const T&) [with T = int]\u2019:\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:39:50:   required from \u2018static std::unique_ptr<tflite::benchmark::BenchmarkParam> tflite::benchmark::BenchmarkParam::Create(const T&) [with T = int]\u2019\r\ntensorflow/contrib/lite/tools/benchmark/benchmark_model.cc:53:65:   required from here\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:53:20: error: \u2018static tflite::benchmark::BenchmarkParam::ParamType tflite::benchmark::BenchmarkParam::GetValueType() [with T = int]\u2019 is private\r\n   static ParamType GetValueType();\r\n                    ^\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:62:39: error: within this context\r\n       : BenchmarkParam(GetValueType<T>()), value_(value) {}\r\n                                       ^\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h: In instantiation of \u2018tflite::benchmark::TypedBenchmarkParam<T>::TypedBenchmarkParam(const T&) [with T = float]\u2019:\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:39:50:   required from \u2018static std::unique_ptr<tflite::benchmark::BenchmarkParam> tflite::benchmark::BenchmarkParam::Create(const T&) [with T = float]\u2019\r\ntensorflow/contrib/lite/tools/benchmark/benchmark_model.cc:54:67:   required from here\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:53:20: error: \u2018static tflite::benchmark::BenchmarkParam::ParamType tflite::benchmark::BenchmarkParam::GetValueType() [with T = float]\u2019 is private\r\n   static ParamType GetValueType();\r\n                    ^\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:62:39: error: within this context\r\n       : BenchmarkParam(GetValueType<T>()), value_(value) {}\r\n                                       ^\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h: In instantiation of \u2018tflite::benchmark::TypedBenchmarkParam<T>::TypedBenchmarkParam(const T&) [with T = std::__cxx11::basic_string<char>]\u2019:\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:39:50:   required from \u2018static std::unique_ptr<tflite::benchmark::BenchmarkParam> tflite::benchmark::BenchmarkParam::Create(const T&) [with T = std::__cxx11::basic_string<char>]\u2019\r\ntensorflow/contrib/lite/tools/benchmark/benchmark_model.cc:56:75:   required from here\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:53:20: error: \u2018static tflite::benchmark::BenchmarkParam::ParamType tflite::benchmark::BenchmarkParam::GetValueType() [with T = std::__cxx11::basic_string<char>]\u2019 is private\r\n   static ParamType GetValueType();\r\n                    ^\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:62:39: error: within this context\r\n       : BenchmarkParam(GetValueType<T>()), value_(value) {}\r\n                                       ^\r\ntensorflow/contrib/lite/Makefile:204: recipe for target '/home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/tools/benchmark/benchmark_model.o' failed\r\nmake: *** [/home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/tools/benchmark/benchmark_model.o] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\nIn file included from ./tensorflow/contrib/lite/tools/benchmark/benchmark_model.h:26:0,\r\n                 from ./tensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.h:25,\r\n                 from tensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.cc:16:\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h: In instantiation of \u2018tflite::benchmark::TypedBenchmarkParam<T>::TypedBenchmarkParam(const T&) [with T = std::__cxx11::basic_string<char>]\u2019:\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:39:50:   required from \u2018static std::unique_ptr<tflite::benchmark::BenchmarkParam> tflite::benchmark::BenchmarkParam::Create(const T&) [with T = std::__cxx11::basic_string<char>]\u2019\r\ntensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.cc:167:74:   required from here\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:53:20: error: \u2018static tflite::benchmark::BenchmarkParam::ParamType tflite::benchmark::BenchmarkParam::GetValueType() [with T = std::__cxx11::basic_string<char>]\u2019 is private\r\n   static ParamType GetValueType();\r\n                    ^\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:62:39: error: within this context\r\n       : BenchmarkParam(GetValueType<T>()), value_(value) {}\r\n                                       ^\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h: In instantiation of \u2018tflite::benchmark::TypedBenchmarkParam<T>::TypedBenchmarkParam(const T&) [with T = bool]\u2019:\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:39:50:   required from \u2018static std::unique_ptr<tflite::benchmark::BenchmarkParam> tflite::benchmark::BenchmarkParam::Create(const T&) [with T = bool]\u2019\r\ntensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.cc:172:74:   required from here\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:53:20: error: \u2018static tflite::benchmark::BenchmarkParam::ParamType tflite::benchmark::BenchmarkParam::GetValueType() [with T = bool]\u2019 is private\r\n   static ParamType GetValueType();\r\n                    ^\r\n./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:62:39: error: within this context\r\n       : BenchmarkParam(GetValueType<T>()), value_(value) {}\r\n                                       ^\r\ntensorflow/contrib/lite/Makefile:204: recipe for target '/home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.o' failed\r\nmake: *** [/home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.o] Error 1\r\n```\r\n", "comments": ["@thxmxx : Are you able to reproduce this now, I tried on master today and looks like recent Makefile refactoring has fixed it.", "@shashishekhar on master there is no `build_rpi_lib.sh` nor `tensorflow/contrib/lite/Makefile`. Am I wrong?", "Ahh. The path changed... The doc is wrong (`tensorflow/blob/master/tensorflow/contrib/lite/g3doc/rpi.md`), now the path is `tensorflow/contrib/lite/tools/make/build_rpi_lib.sh` and not `tensorflow/contrib/lite/build_rpi_lib.sh`.\r\n\r\nIt compiles now. Thank you."]}, {"number": 21685, "title": "Bazel build for llvm fails in sandboxed mode", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.9.0, 1.10.0, master\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**: 6.3\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n```\r\n$ bazel build @llvm//:orc_jit --spawn_strategy=sandboxed\r\nINFO: Analysed target @llvm//:orc_jit (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /home/sguo/.cache/bazel/_bazel_sguo/312e4bfd87ddc7075e69d12d6c5f1e44/external/llvm/BUILD.bazel:1641:1: C++ compilation of rule '@llvm//:orc_jit' failed (Exit 1)\r\nexternal/llvm/lib/ExecutionEngine/Orc/CompileOnDemandLayer.cpp:12:40: fatal error: llvm/Bitcode/BitcodeWriter.h: No such file or directory\r\n #include \"llvm/Bitcode/BitcodeWriter.h\"\r\n                                        ^\r\ncompilation terminated.\r\nTarget @llvm//:orc_jit failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```\r\n```\r\n$ bazel build @llvm//:analysis --spawn_strategy=sandboxed\r\nINFO: Analysed target @llvm//:analysis (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /home/sguo/.cache/bazel/_bazel_sguo/312e4bfd87ddc7075e69d12d6c5f1e44/external/llvm/BUILD.bazel:598:1: C++ compilation of rule '@llvm//:analysis' failed (Exit 1)\r\nexternal/llvm/lib/Analysis/MemoryBuiltins.cpp:24:39: fatal error: llvm/Analysis/Utils/Local.h: No such file or directory\r\n #include \"llvm/Analysis/Utils/Local.h\"\r\n                                       ^\r\ncompilation terminated.\r\nTarget @llvm//:analysis failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```\r\n\r\n### Describe the problem\r\n\r\nThe bazel build targets for llvm are missing header file declarations. They only build when Bazel doesn't enforce a sandboxed environment. When sandboxing in enabled, the builds fail.\r\n\r\n### Source code / logs\r\n\r\n", "comments": ["@jart @gunan @martinwicke would you PTAL?", "This would be a problem for the XLA build on remote workers. @tatatodd could you take a look?", "any update?", "@klimek recently fixed this and got XLA+TF build on remote workers.", ":+1: "]}, {"number": 21684, "title": "Two optimizers, with two assign ops on all .get_variable variables when using biderctional_dynamic_rnn", "body": "### Environment Information\r\n```\r\n\r\n== cat /etc/issue ===============================================\r\nLinux nazariy-box 4.15.0-32-generic #35~16.04.1-Ubuntu SMP Fri Aug 10 21:54:34 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux nazariy-box 4.15.0-32-generic #35~16.04.1-Ubuntu SMP Fri Aug 10 21:54:34 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                              1.14.3     \r\nnumpydoc                           0.8.0      \r\nprotobuf                           3.5.2.post1\r\ntensorflow                         1.8.0      \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n/home/nazariy/anaconda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/lib:/usr/lib/lp_solve/lp_solve_5.5\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: 105: tf_env_collect.sh: nvidia-smi: not found\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/MATLAB/R2017b/bin/glnxa64/libcudart.so.8.0.44\r\n```\r\n### Additional System Questions\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.1\r\nTensorFlow installed from (source or binary): pip\r\nTensorFlow version (use command below): 1.8.0\r\nPython version: 3.6.5\r\nBazel version (if compiling from source): none\r\nGCC/Compiler version (if compiling from source): none\r\nCUDA/cuDNN version: none\r\nGPU model and memory: none\r\nExact command to reproduce: none\r\nMobile device: No\r\n\r\n### Problem outline\r\nI have written an architecture whereby I have attention layers and in the middle I have a `bidirectional_dynamic_rnn` with forward and backward GRU cells like so:\r\n\r\n```\r\nclass TensorGRU(object):\r\n\r\n    def __init__(self, inputs, hidden_size, dtype=tf.float64):\r\n        with tf.name_scope(\"GRULayer\"):\r\n            self.GRU_forward = tf.nn.rnn_cell.GRUCell(num_units=hidden_size, activation=tf.nn.tanh, name=\"GRU_Forward\")\r\n            self.GRU_backward = tf.nn.rnn_cell.GRUCell(num_units=hidden_size, activation=tf.nn.tanh, name=\"GRU_Backward\")\r\n\r\n            self.GRU_bi, _ = tf.nn.bidirectional_dynamic_rnn(self.GRU_forward, self.GRU_backward, inputs=inputs,\r\n                                                             dtype=dtype)\r\n            self.h = tf.concat([self.GRU_bi[0], self.GRU_bi[1]], axis=2, name='h')\r\n            tf.summary.histogram('GRU_Output', self.h)\r\n```\r\nAs soon as I add the `tf.nn.bidirectional_dymaic_rnn` line I get two optimizers (Adam in my case) \"attached\" to every `.get_variable()` variable in my code. For example, see the screenshot provided from Tensorboard Debugger:\r\n<img width=\"204\" alt=\"issue_1\" src=\"https://user-images.githubusercontent.com/13678461/44277083-6f60f800-a241-11e8-8e20-fe0b7ac85fe2.PNG\">\r\nand from graph:\r\n<img width=\"548\" alt=\"issue_2\" src=\"https://user-images.githubusercontent.com/13678461/44277095-77b93300-a241-11e8-9c04-f378b8409e22.PNG\">\r\n\r\nHere is the MLP code for your convenience, but there is nothing special there:\r\n\r\n```\r\nclass MLP(object):\r\n\r\n    # incoming is V, which is [batch, 2*hidden]\r\n    def __init__(self, n_input, n_hidden_1, n_hidden_2, n_classes, dtype=tf.float64,\r\n                 input_layer=None, activation_func=None):\r\n\r\n        if input_layer is None:\r\n            self.input_layer = tf.placeholder(dtype=dtype, shape=(None, n_input),\r\n                                              name=\"PL_Output\")\r\n        else:\r\n            if isinstance(input_layer, tf.Tensor):\r\n                self.input_layer = input_layer\r\n            else:\r\n                self.input_layer = tf.convert_to_tensor(input_layer, name=\"PL_Output\")\r\n\r\n        if activation_func is None:\r\n            self.activation_func = tf.tanh\r\n        else:\r\n            self.activation_func = activation_func\r\n\r\n        with tf.variable_scope(\"MLP_Variables\"):\r\n            self.weights = {\r\n                'l1': tf.get_variable(\"MLP_Layer_1_Weights\", dtype=dtype, shape=(n_input, n_hidden_1),\r\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\r\n                'l2': tf.get_variable(\"MLP_Layer_2_Weights\", dtype=dtype, shape=(n_hidden_1, n_hidden_2),\r\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\r\n                'out': tf.get_variable(\"MLP_Layer_Out_Weights\", dtype=dtype, shape=(n_hidden_2, n_classes),\r\n                                       initializer=tf.contrib.layers.xavier_initializer(dtype=dtype))\r\n            }\r\n\r\n            self.biases = {\r\n                'l1': tf.get_variable(\"MLP_Layer_1_Biases\", dtype=dtype, shape=(n_hidden_1,),\r\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\r\n                'l2': tf.get_variable(\"MLP_Layer_2_Biases\", dtype=dtype, shape=(n_hidden_2,),\r\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\r\n                'out': tf.get_variable(\"MLP_Layer_Out_Biases\", dtype=dtype, shape=(n_classes,),\r\n                                       initializer=tf.contrib.layers.xavier_initializer(dtype=dtype))\r\n            }\r\n\r\n        self.layer_1 = None\r\n        self.layer_2 = None\r\n        self.layer_out = None\r\n        self.expected_output = None\r\n\r\n        self.multilayer_perceptron()\r\n\r\n    def multilayer_perceptron(self):\r\n        self.layer_1 = tf.add(tf.matmul(self.input_layer, self.weights['l1']), self.biases['l1'])\r\n        self.layer_1 = self.activation_func(self.layer_1)\r\n\r\n        self.layer_2 = tf.add(tf.matmul(self.layer_1, self.weights['l2']), self.biases['l2'])\r\n        self.layer_2 = self.activation_func(self.layer_2)\r\n\r\n        self.layer_out = tf.add(tf.matmul(self.layer_2, self.weights['out']), self.biases['out'])\r\n        self.layer_out = self.activation_func(self.layer_out)\r\n\r\n        self.expected_output = tf.nn.softmax(self.layer_out, axis=1)\r\n\r\n        return self.expected_output\r\n```\r\n\r\n**I would expect that each variable only has one optimizer**.", "comments": ["It's the same optimizer, just with two different slot variables (first and second moments, or \"m\" and \"v\" here: https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer). Granted the names Adam and Adam_1 are not great.", "I see, thanks, so I guess this should be closed then?"]}, {"number": 21683, "title": "Error while freezing a graph", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:No\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:1.10\r\n- **Python version**:3.6\r\n- **Bazel version (if compiling from source)**:0.16.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:bazel-bin/tensorflow/python/tools/freeze_graph \\ --input_graph= /home/my-model/premade_model105/graph.pbtxt \\ --input_checkpoint = /home/my-model/premade_model105/model.ckpt-1000.meta \\ --output_graph=/home/my-model/premade_model105/frozen_graph.pb \\ --output_node_names= dnn/logits/BiasAdd\\ --input_binary=false \r\n\r\n\r\n### Describe the problem\r\nI am trying to freeze a graph an filling the parameter input_checkpoint with a valid path but I am still getting this error and following the instructions of the official page:\r\n\r\n_The input_checkpoint should be the most recent saved checkpoint. As mentioned in the checkpoint section, you need to give the common prefix to the set of checkpoints here, rather than a full filename._\r\n\r\nbut Iam getting this error:\r\n\r\n### **Input checkpoint '' doesn't exist!**\r\n\r\n\r\nI am using a DNN premade classifier\r\n", "comments": ["I don't know why the parameter is not being identify, instead it is read as an empty string", "You have spaces before and after the `=` for some of the flags. For example `--input_checkpoint = /home/my-model/premade_model105/model.ckpt-1000.meta` should be `--input_checkpoint=/home/my-model/premade_model105/model.ckpt-1000.meta`\r\n\r\nMake sure do you that for the other flags and this error should disappear. \r\n\r\nPlease reopen if that doesn't resolve it. Thanks!"]}, {"number": 21682, "title": "Branch42", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}, {"number": 21681, "title": "Faster bilinear resize for x86", "body": "@alextp This is a rework of our earlier bilinear resize PR (#20746).\r\n\r\nPerformance for TF master prior to this PR (run on DGX-1 with Volta):\r\n```\r\nBenchmark                                           Time(ns) Iterations\r\n-----------------------------------------------------------------------\r\nBM_CropAndResize_cpu_DT_UINT8_1_640_640_3_512_512    5097690        100  241.1M items/s\r\nBM_CropAndResize_cpu_DT_UINT8_1_640_640_1_512_512    3449158        202  118.8M items/s\r\nBM_CropAndResize_cpu_DT_HALF_1_640_640_3_512_512     7290470        100  168.5M items/s\r\nBM_CropAndResize_cpu_DT_HALF_1_640_640_1_512_512     4387491        159  93.4M items/s\r\nBM_CropAndResize_cpu_DT_FLOAT_1_640_640_3_512_512    4273981        161  287.5M items/s\r\nBM_CropAndResize_cpu_DT_FLOAT_1_640_640_1_512_512    3053134        231  134.2M items/s\r\nBM_CropAndResize_cpu_DT_FLOAT_1_80_80_512_7_7          85094       6208\r\n```\r\n\r\nPerformance benchmark including this PR:\r\n```\r\nBenchmark                                           Time(ns) Iterations\r\n-----------------------------------------------------------------------\r\nBM_CropAndResize_cpu_DT_UINT8_1_640_640_3_512_512    1097690        581  1119.4M items/s\r\nBM_CropAndResize_cpu_DT_UINT8_1_640_640_1_512_512     503559       1400  813.4M items/s\r\nBM_CropAndResize_cpu_DT_HALF_1_640_640_3_512_512     2726747        249  450.6M items/s\r\nBM_CropAndResize_cpu_DT_HALF_1_640_640_1_512_512     1069408        650  383.0M items/s\r\nBM_CropAndResize_cpu_DT_FLOAT_1_640_640_3_512_512    1251613        496  981.8M items/s\r\nBM_CropAndResize_cpu_DT_FLOAT_1_640_640_1_512_512     448486       1576  913.3M items/s\r\nBM_CropAndResize_cpu_DT_FLOAT_1_80_80_512_7_7          81488       6333\r\n```", "comments": ["@mingxingtan can you review the vectorized code? I'm not too familiar with that", "I have updated this pull request with all the changes suggested by the reviewers. \r\nThanks,\r\nThor", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I was told via e-mail that I am a member of our corporate gating e-mail group nvidia-google-contributors@googlegroups.com. If you need to verify that, please contact Amal Prabhu, amalp@nvidia.com.\n\n\nThanks,\nThor\n\n\n________________________________\nFrom: googlebot <notifications@github.com>\nSent: Friday, September 7, 2018 9:41 PM\nTo: tensorflow/tensorflow\nCc: Thor Johnsen; Manual\nSubject: Re: [tensorflow/tensorflow] Faster bilinear resize for x86 (#21681)\n\n\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n?? Please visit https://cla.developers.google.com/ to sign.\n\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\n________________________________\nWhat to do if you already signed the CLA\nIndividual signers\n\n  *   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data<https://cla.developers.google.com/clas> and verify that your email is set on your git commits<https://help.github.com/articles/setting-your-email-in-git/>.\n\nCorporate signers\n\n  *   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot<http://go/cla#troubleshoot> (Public version<https://opensource.google.com/docs/cla/#troubleshoot>).\n  *   The email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data<https://cla.developers.google.com/clas> and verify that your email is set on your git commits<https://help.github.com/articles/setting-your-email-in-git/>.\n  *   The email used to register you as an authorized contributor must also be attached to your GitHub account<https://github.com/settings/emails>.\n\n-\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/21681#issuecomment-419606778>, or mute the thread<https://github.com/notifications/unsubscribe-auth/Anqg63jjnxmK_6fJdjZVl7C8qKQWKXrmks5uYy5ZgaJpZM4WBuPs>.\n\n-----------------------------------------------------------------------------------\nThis email message is for the sole use of the intended recipient(s) and may contain\nconfidential information.  Any unauthorized review, use, disclosure or distribution\nis prohibited.  If you are not the intended recipient, please contact the sender by\nreply email and destroy all copies of the original message.\n-----------------------------------------------------------------------------------\n", "@martinwicke any idea what's happening with the CLA here?", "@gunan any idea what is happening with the CLA here?", "Just an update on the CLA status. The person who manages the nvidia-google-contributors list may have forgotten to add me to it. He sent me en email yesterday that he would add me, but he hasn't confirmed it yet. There is an internal event at NVIDIA today and tomorrow, most of the engineers are attending it, so this may take a day or two to resolve. ", "Our nvidia-google-contributors admin confirms that my email address is on the list and has been so for a while. I am not sure how to resolve the CLA problem. Is there perhaps a way to override this requirement? ", "I  read somewhere that PR's that involved commits from multiple authors could be held up until the author(s) that did not create the PR gave consent to include their commits. Just to make it clear: I hereby give consent to include my commits in this PR.", "@googlebot Look again please.", "@thorjohnsen thanks. The problem in this case seems to be not your CLA, but @nluehr's. \r\n\r\nNathan, can you check that you are in the right groups? Or alternatively, Thor, you can send a PR yourself? ", "I've manually verified that everything is fine on this PR, so am overriding the CLA status.", "err, nevermind. I'm not an owner on this so can't override. :) @martinwicke or others... feel free to manually set the cla: yes label.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "I am a member of the appropriate contributors list. When I first submitted this PR the CLA check succeeded even though it already at that point included commits from both @thorjohnsen and myself. I guess the cla bot doesn't like subsequent pushes coming from a second author (or maybe it just expects the last commit to match the PR author?) In any case, thanks for manually approving.", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Of the 5 tests that failed, two are due to issues that are not related to this PR (Windows Bazel GPU and XLA). The remaining 3 tests fail due to a single unit test: image_grad_test, which includes an op (resize_bilinear_op) that is affected by this PR. This unit test passes consistently on our linux systems, so I made a commit that limits the explicitly vectorized code to linux systems, hoping that this will make the image_grad_test pass on MacOS. ", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Hopefully, this will be my last commit. I fixed the clang-format issue and removed the compile target override from the BUILD file. This means that the default build will compile the fallthrough code, but even that is ~2x faster than the code it replaces. We compile the NVIDIA containers with -march=sandybridge, which selects the SSE4.1 vectorized code path. I have tested that extensively, I ran 90 epochs with Alexnet, Googlenet and Resnet50, that's ~350 million random crop and resize calls with no failure. The image_grad_test passes for all the code paths in our container. ", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "@alextp this is as good as it gets. The two failures are unrelated issues. Windows Bazel GPU fails to build because of a missing file (nccl.so) and XLA fails in the Log test, which is unaffected by this PR. In my opinion, this PR is ready for merge.", "@thorjohnsen agreed. I'll keep retrying (our automatic system doesn't make progress until the tests are green AFAIK)", "@nluehr could you check the `image_grad_test`? It seems to be failing.", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "@nluehr could you run `clang-format-3.6.0`?", "I will reformat with clang.", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@alextp @drpngx Please review the latest changes that have been clang'ed.\r\nThanks,\r\nThor", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "Looks like there's some missing library?\r\n```\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/k8-opt/bin/tensorflow/cc/gradients_array_grad_test '-Wl,-rpath,$ORIGIN/../../_solib_local/' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U_S_Stensorflow_Scc_Cgradients_Uarray_Ugrad_Utest___Utensorflow' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' -Lbazel-out/k8-opt/bin/_solib_local/_U_S_Stensorflow_Scc_Cgradients_Uarray_Ugrad_Utest___Utensorflow -Lbazel-out/k8-opt/bin/_solib_local -Lbazel-out/k8-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/k8-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/k8-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -Wl,-z,muldefs -pthread -pthread -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -Wl,-no-as-needed -pie -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/k8-opt/bin/tensorflow/cc/gradients_array_grad_test-2.params)\r\nbazel-out/k8-opt/bin/external/nccl_archive/libmax.a(max_all_reduce.cu.o): In function `__sti____cudaRegisterAll()':\r\ntmpxft_00002457_00000000-5_max_all_reduce.cu.cudafe1.cpp:(.text.startup+0x1d): undefined reference to `__cudaRegisterLinkedBinary_52_tmpxft_00002457_00000000_6_max_all_reduce_cu_cpp1_ii_b56bb4c7'\r\nbazel-out/k8-opt/bin/external/nccl_archive/libmax.a(max_reduce.cu.o): In function `__sti____cudaRegisterAll()':\r\ntmpxft_000023cc_00000000-5_max_reduce.cu.cudafe1.cpp:(.text.startup+0x1d): undefined reference to `__cudaRegisterLinkedBinary_48_tmpxft_000023cc_00000000_6_max_reduce_cu_cpp1_ii_8fe180b0'\r\nbazel-out/k8-opt/bin/external/nccl_archive/libmax.a(max_reduce_scatter.cu.o): In function `__sti____cudaRegisterAll()':\r\ntmpxft_00002467_00000000-5_max_reduce_scatter.cu.cudafe1.cpp:(.text.startup+0x1d): undefined reference to `__cudaRegisterLinkedBinary_56_tmpxft_00002467_00000000_6_max_reduce_scatter_cu_cpp1_ii_f5c28209'\r\nbazel-out/k8-opt/bin/external/nccl_archive/libmin.a(min_all_reduce.cu.o): In function `__sti____cudaRegisterAll()':\r\ntmpxft_000024be_00000000-5_min_all_reduce.cu.cudafe1.cpp:(.text.startup+0x1d): undefined reference to `__cudaRegisterLinkedBinary_52_tmpxft_000024be_00000000_6_min_all_reduce_cu_cpp1_ii_b1f185ed'\r\nbazel-out/k8-opt/bin/external/nccl_archive/libmin.a(min_reduce.cu.o): In function `__sti____cudaRegisterAll()':\r\ntmpxft_00002453_00000000-5_min_reduce.cu.cudafe1.cpp:(.text.startup+0x1d): undefined reference to `__cudaRegisterLinkedBinary_48_tmpxft_00002453_00000000_6_min_reduce_cu_cpp1_ii_8b7bb19a'\r\nbazel-out/k8-opt/bin/external/nccl_archive/libmin.a(min_reduce_scatter.cu.o): In function `__sti____cudaRegisterAll()':\r\ntmpxft_0000240f_00000000-5_min_reduce_scatter.cu.cudafe1.cpp:(.text.startup+0x1d): undefined reference to `__cudaRegisterLinkedBinary_56_tmpxft_0000240f_00000000_6_min_reduce_scatter_cu_cpp1_ii_f158b323'\r\nbazel-out/k8-opt/bin/external/nccl_archive/libprod.a(_prodall_reduce.cu.o): In function `__sti____cudaRegisterAll()':\r\ntmpxft_000023c4_00000000-5__prodall_reduce.cu.cudafe1.cpp:(.text.startup+0x1d): undefined reference to `__cudaRegisterLinkedBinary_53_tmpxft_000023c4_00000000_6__prodall_reduce_cu_cpp1_ii_6ba35bfb'\r\nbazel-out/k8-opt/bin/external/nccl_archive/libprod.a(_prodreduce.cu.o): In function `__sti____cudaRegisterAll()':\r\ntmpxft_0000243c_00000000-5__prodreduce.cu.cudafe1.cpp:(.text.startup+0x1d): undefined reference to `__cudaRegisterLinkedBinary_49_tmpxft_0000243c_00000000_6__prodreduce_cu_cpp1_ii_a49dbe7e'\r\nbazel-out/k8-opt/bin/external/nccl_archive/libprod.a(_prodreduce_scatter.cu.o): In function `__sti____cudaRegisterAll()':\r\ntmpxft_00002429_00000000-5__prodreduce_scatter.cu.cudafe1.cpp:(.text.startup+0x1d): undefined reference to `__cudaRegisterLinkedBinary_57_tmpxft_00002429_00000000_6__prodreduce_scatter_cu_cpp1_ii_5cbea597'\r\nbazel-out/k8-opt/bin/external/nccl_archive/libsum.a(sum_all_gather.cu.o): In function `__sti____cudaRegisterAll()':\r\ntmpxft_00002652_00000000-5_sum_all_gather.cu.cudafe1.cpp:(.text.startup+0x1d): undefined reference to `__cudaRegisterLinkedBinary_52_tmpxft_00002652_00000000_6_sum_all_gather_cu_cpp1_ii_e02335fb'\r\nbazel-out/k8-opt/bin/external/nccl_archive/libsum.a(sum_all_reduce.cu.o): In function `__sti____cudaRegisterAll()':\r\ntmpxft_00002658_00000000-5_sum_all_reduce.cu.cudafe1.cpp:(.text.startup+0x1d): undefined reference to `__cudaRegisterLinkedBinary_52_tmpxft_00002658_00000000_6_sum_all_reduce_cu_cpp1_ii_6afb4f5f'\r\nbazel-out/k8-opt/bin/external/nccl_archive/libsum.a(sum_broadcast.cu.o): In function `__sti____cudaRegisterAll()':\r\ntmpxft_00002642_00000000-5_sum_broadcast.cu.cudafe1.cpp:(.text.startup+0x1d): undefined reference to `__cudaRegisterLinkedBinary_51_tmpxft_00002642_00000000_6_sum_broadcast_cu_cpp1_ii_2e7fe711'\r\nbazel-out/k8-opt/bin/external/nccl_archive/libsum.a(sum_reduce.cu.o): In function `__sti____cudaRegisterAll()':\r\ntmpxft_00002633_00000000-5_sum_reduce.cu.cudafe1.cpp:(.text.startup+0x1d): undefined reference to `__cudaRegisterLinkedBinary_48_tmpxft_00002633_00000000_6_sum_reduce_cu_cpp1_ii_50717b28'\r\nbazel-out/k8-opt/bin/external/nccl_archive/libsum.a(sum_reduce_scatter.cu.o): In function `__sti____cudaRegisterAll()':\r\ntmpxft_0000263f_00000000-5_sum_reduce_scatter.cu.cudafe1.cpp:(.text.startup+0x1d): undefined reference to `__cudaRegisterLinkedBinary_56_tmpxft_0000263f_00000000_6_sum_reduce_scatter_cu_cpp1_ii_2a527991'\r\nbazel-out/k8-opt/bin/_solib_local/libexternal_Snccl_Uarchive_Slibdevice_Ucode.so: undefined reference to `__fatbinwrap_56_tmpxft_0000241d_00000000_6_max_reduce_scatter_cu_cpp1_ii_f5c28209'\r\n```", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@drpngx There is nothing in this PR that changes link paths nor any code that uses NCCL, so the linker failure sounds like an unrelated issue. I ran the code through clang again and resubmitted it.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->"]}, {"number": 21680, "title": "FixedLengthRecordDataset does not support compression_type", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.8.0-4337-gbfcfad55b7 1.9.0-rc0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nTextLineDataset and TFRecordDataset both support a \"compression_type\" argument, allowing compressed files to be read.  However, FixedLengthRecordDataset does not for some reason.  Would adding this be feasible?", "comments": ["cc @mrry, the authority on this issue.", "@mrry , care to comment on this feature request? I can mark as contributions welcome, too, if you think it would be generally useful.", "I'll mark this as \"contributions welcome\" although be aware that there is some hassle involved:\r\n\r\n* We'll need a new `\"FixedLengthDatasetV2\"` op with the new option, defaulting to uncompressed.\r\n* For forwards compatibility, the Python wrapper will need to avoid creating a `\"FixedLengthDatasetV2\"` op for the uncompressed case for three weeks from the submission of the code.", "Added a PR #21861 for compression support.", "Nagging Assignee @mrry: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21679, "title": "How to clip weights of a dense layer between every step of training op?", "body": "Is there any possible way to do a custom op (for example, to clip by values of the weights of a dense layer manually every training step)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\r\n        weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='logits/kernel')[0]\r\n        clip_op = tf.assign(weights, tf.clip_by_value(weights, 0.01, 0.1))\r\n        train_op = optimizer.minimize(\r\n            loss=loss,\r\n            global_step=tf.train.get_global_step())\r\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n\r\nAbove, I want to clip the weights of `logits/kernel` layer between every training op, but it does not work as intended, the weights of that layer would still drift outside the range of (0.01, 0.1). I wonder what I'm missing here.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Sorry for the delayed response. I somehow managed to get a walkaround solution with `kernel_constraint` arguments. This issue can be closed for now I guess.", "Nagging Assignee @tatatodd: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue as said by @jingwb222 "]}, {"number": 21678, "title": "adding libnsync.a when available.", "body": "iOS needs these flags to build:\r\n```\r\n-lprotobuf_experimental\r\n-lnsync\r\n-force_load $(PROJECT_DIR)/tensorflow_experimental.framework/tensorflow_experimental\r\n```\r\n\r\nso adding the `libnsync.a` directly into the framework should be a good idea.\r\nif the file doesn't exist, it is still ok to finish the build without conflicts to the document or failure.", "comments": ["The parts I know look plausible.  I'm checking with the original author of the file \r\n     create_ios_frameworks.sh", "@RockfordWei\r\nAre you able to test the multi-architecture flow, as Yu-Cheng suggested above?\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 21677, "title": "Fix embedding_ops_test on AVX512 builds", "body": "This commit modifies the embedding_ops_test unit test so that it\r\npasses on AVX512 builds.  The test compares the result of an embedding\r\noperation to a precomputed expected result, both of which are computed\r\nusing square roots.  The test appears to fail on AVX512 builds as the\r\nembedding operation makes use of Eigen's fast vectorized square root\r\nalgorithm for doubles, which can return slightly different results to\r\nthose computed using numpy.  As the test expects both results to be\r\nidentical it fails on AVX512 builds, even though the difference\r\nbetween the results is insignificant.  This commit fixes the issue by\r\nmodifying the equality tests to use assertAllClose instead of\r\nassertAllEqual.  Note that the test passes in its current form on AVX2\r\nbuilds as Eigen does not yet implement an AVX2 version of the fast\r\nsquare root algorithm for doubles.\r\n\r\nFixes: https://github.com/tensorflow/tensorflow/issues/21676\r\n\r\nSigned-off-by: Mark Ryan <mark.d.ryan@intel.com>", "comments": ["@protoget Could I ask you to take quick look at this PR?  It fixes one of the last remaining unit test failures on AVX512 builds.", "The PR is still valid and all comments have been addressed.  IMHO the \"awaiting response\" label should be removed.  Is this something I can do?", "@protoget Are you expecting to see more changes to the comments?  If not, could you remove the \"awaiting response\" label?", "@protoget Could I possibly ask you to take one more look at this?  It would be great to get it into the next release.", "@protoget Could you take another look at this? The tests still fail on AVX512 builds of master.  This patch still applies on master and still fixes the issue.", "@protoget  Could you please take a look and approve or suggest changes(if required). Thanks !", "Nagging Reviewers : You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@hgadig I've just retested on master.  The underlying bug is still present and this PR still applies cleanly and still fixes the issue.  Can anything be done to speed up its review?  The PR is now over 6 months old.", "@michaelisard  @mrry  Can any of you please review this ?"]}, {"number": 21676, "title": "embedding_ops_test fails on AVX512 builds", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.8.0-6288-g335336a', '1.10.0-rc1')\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: [bazel release 0.15.0]\r\n- **GCC/Compiler version (if compiling from source)**: g++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A \r\n- **Exact command to reproduce**:\r\n\r\nbazel test --config=opt -- //tensorflow/python/kernel_tests:embedding_ops_test\r\n\r\n### Describe the problem\r\n\r\nThe test case fails on AVX512 builds.  See the logs below.\r\n\r\n### Source code / logs\r\n\r\n```\r\n======================================================================\r\nFAIL: testTransform (__main__.EmbeddingLookupTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/user/.cache/bazel/_bazel_user/0c0d18d29bdc5ed463543206f26b0a12/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/embedding_ops_test.py\", line 609, in testTransform\r\n    self.assertAllEqual(simple, sharded)\r\n  File \"/home/user/.cache/bazel/_bazel_user/0c0d18d29bdc5ed463543206f26b0a12/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1466, in assertAllEqual\r\n    np.testing.assert_array_equal(a, b, err_msg=msg)\r\n  File \"/home/user/.local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py\", line 855, in assert_array_equal\r\n    verbose=verbose, header='Arrays are not equal')\r\n  File \"/home/user/.local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py\", line 779, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not equal\r\n\r\n(mismatch 16.6666666667%)\r\n x: array([[2., 2., 2.],\r\n       [2., 2., 2.],\r\n       [2., 2., 2.],\r\n       [2., 2., 2.]])\r\n y: array([[2., 2., 2.],\r\n       [2., 2., 2.],\r\n       [2., 2., 2.],\r\n       [2., 2., 2.]])\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 3.798s\r\n\r\nFAILED (failures=1)\r\nnot equal where =  (array([2, 3]), array([0, 1]))\r\nnot equal lhs =  [2. 2.]\r\nnot equal rhs =  [2. 2.]\r\n~\r\n```\r\n\r\n", "comments": ["I've submitted a [PR](https://github.com/tensorflow/tensorflow/pull/21677) that fixes the issue.  The underlying problem is that the existing code expects the results of two different floating point computations to match exactly.  In AVX512 builds one of the computations uses an optimised  Eigen algorithm for computing the square roots of vectors of doubles and this algorithm can produce slightly different results from the unoptimised version.  These differences cause the exact match to fail.  The PR fixes the issue by modifying the test case so that the results of the two computations only need to be close to each other, rather than matching exactly.", "Thanks! I will leave this as contributions welcome until the PR is merged.", "Note //tensorflow/python/kernel_tests:embedding_ops_test_gpu is now also failing on master on AVX512 builds.  This unit test along with //tensorflow/python/kernel_tests:embedding_ops_test_gpu can be fixed by merging #21677.", "AFAIK, this is the last remaining AVX512 issue, now that #21265 has been merged.  It would be great to get this fixed for the next release."]}, {"number": 21675, "title": "op type not registered 'DenseToDenseSetOperation' when running tfcompile", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (code from maskrcnn, and export.py to freeze my model)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Laptop\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: ('v1.9.0-rc2-2314-g3a99980', '1.10.0')\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609\r\n- **CUDA/cuDNN version**: no cuda\r\n- **GPU model and memory**:  Geforce GT 730M, 1Gb memory\r\n- **Exact command to reproduce**: \r\n\r\n`bazel-bin/tensorflow/compiler/aot/tfcompile --graph=/home/mehdi/Desktop/data/resnet101_model.pb --config=/home/mehdi/Desktop/data/config.pbtxt --cpp_class=\"foo::bar\" --out_function_object=\"./out_model.o\" --out_header=\"./out.h\" --out_metadata_object=\"./out_helper.o\"`\r\n\r\nI have been testing [mask rcnn](https://github.com/matterport/Mask_RCNN) in order to do object detection and segmentation on images. It worked quite well and now I am trying to export my model to a standalone binary that can be used without having to install tensorflow on my target system. In order to do this I tried to use tfcompile. The first step was to take my .h5 keras model and converted it to a frozen tf .pb file. I used [this script](https://github.com/ericj974/Mask_RCNN/blob/master/scripts/export.py) that was especially written to convert mask rcnn models. I then downloaded the source code of tensorflow, compiled it by running\r\n\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --jobs 3`\r\n\r\nbefore that, I set support XLA JIT to true when running configure.\r\nAfter compilation was done I installed tensorflow and it was importable from python.\r\n\r\nThen in order to compile my model I run:\r\n`bazel-bin/tensorflow/compiler/aot/tfcompile --graph=/home/mehdi/Desktop/data/resnet101_model.pb --config=/home/mehdi/Desktop/data/config.pbtxt --cpp_class=\"foo::bar\" --out_function_object=\"./out_model.o\" --out_header=\"./out.h\" --out_metadata_object=\"./out_helper.o\"`\r\n\r\nwhere resnet101_model.pb is the result of running export.py on my keras .h5 model.\r\nconfig.pgtxt is as follows:\r\n\r\n```\r\n# Each feed is a positional input argument for the generated function.  The order\r\n# of each entry matches the order of each input argument.  Here \u201cx_hold\u201d and \u201cy_hold\u201d\r\n# refer to the names of placeholder nodes defined in the graph.\r\nfeed {\r\n  id { node_name: \"input_image\" }\r\n  shape {\r\n    dim { size: 320 }\r\n    dim { size: 320 }\r\n    dim { size: 4 }\r\n  }\r\n}\r\n\r\n# Each fetch is a positional output argument for the generated function.  The order\r\n# of each entry matches the order of each output argument.  Here \u201cx_y_prod\u201d\r\n# refers to the name of a matmul node defined in the graph.\r\nfetch {\r\n  id { node_name: \"mrcnn_mask/Reshape_1\" }\r\n}\r\n```\r\n\r\nUnfortunately I can't upload my model as Github only allows 10Mb. But [here](https://github.com/matterport/Mask_RCNN/releases) is a similar model trained on the same network  (mask_rcnn_balloon.h5)\r\n\r\ntfcompile fails with the following error:\r\n\r\n`2018-08-16 12:38:27.078016: F tensorflow/compiler/aot/tfcompile_main.cc:150] Non-OK-status: status status: Not found: Op type not registered 'DenseToDenseSetOperation' in binary running on mehdi-t440p. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) tf.contrib.resampler should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. Aborted`\r\n\r\nThis is difficult to understand for me as DenseToDenseSetIOperation is existing and registered in tensorflow/core/ops/set_ops.cc (line 35).\r\nI searched for tutorials or documentation, even watched the video presentation made by tensorflow guys, but there is no troubleshooting support anywhere. So I thought Github is the right place for this kind of problem.\r\n\r\n\r\n\r\n", "comments": ["@tatatodd any hint?", "@drpngx thanks for forwarding the issue, can you maybe try to reproduce the issue?", "Sorry this fell through the cracks. Can you confirm that this is still current?", "Yes it is!", "@drpngx. I still have the same problem", "I got the same problem too..", "Hi There,\r\n\r\nWe are moving this issue to closed status, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.\r\nPlease open a new issue for any help you need against 2.x, and we will get you the right help.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21675\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21675\">No</a>\n"]}, {"number": 21674, "title": "copy cuda_fp16.h", "body": "tensorflow/core/util/cuda_kernel_helper.h include cuda_fp16.h.", "comments": ["> Starting from TensorFlow 1.11, Windows builds will use Bazel. Therefore, we will drop official support for cmake.\r\n\r\nSo, close PR."]}, {"number": 21672, "title": "tflite label_image: add -llog and update doc", "body": "1. -llog needed for Android after 2c623eaa\r\n2. update label_image.md\r\n   a. reflect changes\r\n   b. where to get test model and data\r\n   c. describe --cxxopt=-DTFLITE_PROFILING_ENABLED", "comments": ["Nagging Assignee @protoget: It has been 75 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "close this because lite is not longer in contrib/. Will send another one later"]}, {"number": 21671, "title": " ./tensorflow/contrib/lite/java/build_aar_for_release.sh command build .so library failed ", "body": "### Output information for the command :\r\n```\r\n++ mktemp -d\r\n+ TMPDIR=/tmp/tmp.1epU8vtRBz\r\n+ trap 'rm -rf /tmp/tmp.1epU8vtRBz' EXIT\r\n+ VERSION=1.0\r\n+ BUILDER=bazel\r\n+ BASEDIR=tensorflow/contrib/lite\r\n+ CROSSTOOL=//external:android/crosstool\r\n+ HOST_CROSSTOOL=@bazel_tools//tools/cpp:toolchain\r\n+ BUILD_OPTS='--cxxopt=--std=c++11 -c opt'\r\n+ CROSSTOOL_OPTS='--crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain'\r\n+ test -d tensorflow/contrib/lite\r\n+ rm -rf /tmp/tmp.1epU8vtRBz\r\n+ mkdir -p /tmp/tmp.1epU8vtRBz/jni\r\n+ build_basic_aar /tmp/tmp.1epU8vtRBz\r\n+ local OUTDIR=/tmp/tmp.1epU8vtRBz\r\n+ bazel build --cxxopt=--std=c++11 -c opt tensorflow/contrib/lite/java:tensorflowlite.aar\r\nINFO: Build options have changed, discarding analysis cache.\r\nINFO: Analysed target //tensorflow/contrib/lite/java:tensorflowlite.aar (0 packages loaded).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/contrib/lite/java:tensorflowlite.aar up-to-date:\r\n  bazel-bin/tensorflow/contrib/lite/java/tensorflowlite.aar\r\nINFO: Elapsed time: 0.499s, Critical Path: 0.00s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\n+ unzip -d /tmp/tmp.1epU8vtRBz bazel-bin/tensorflow/contrib/lite/java/tensorflowlite.aar\r\nArchive:  bazel-bin/tensorflow/contrib/lite/java/tensorflowlite.aar\r\n  inflating: /tmp/tmp.1epU8vtRBz/AndroidManifest.xml  \r\n  inflating: /tmp/tmp.1epU8vtRBz/classes.jar  \r\n   creating: /tmp/tmp.1epU8vtRBz/res/\r\n  inflating: /tmp/tmp.1epU8vtRBz/R.txt  \r\n+ sed -i -e 's/<application>/<uses-sdk android:targetSdkVersion=\"25\"\\/><application>/' /tmp/tmp.1epU8vtRBz/AndroidManifest.xml\r\n+ build_arch arm64-v8a arm64-v8a /tmp/tmp.1epU8vtRBz\r\n+ local ARCH=arm64-v8a\r\n+ local CONFIG=arm64-v8a\r\n+ local OUTDIR=/tmp/tmp.1epU8vtRBz\r\n+ mkdir -p /tmp/tmp.1epU8vtRBz/jni/arm64-v8a/\r\n+ bazel build --cxxopt=--std=c++11 -c opt --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=arm64-v8a tensorflow/contrib/lite/java:libtensorflowlite_jni.so\r\nINFO: Build options have changed, discarding analysis cache.\r\nINFO: Analysed target //tensorflow/contrib/lite/java:libtensorflowlite_jni.so (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /home/tclxa/tensorflow/tensorflow/contrib/lite/profiling/BUILD:37:1: C++ compilation of rule '//tensorflow/contrib/lite/profiling:time' failed (Exit 1)\r\nIn file included from tensorflow/contrib/lite/profiling/time.cc:15:\r\n./tensorflow/contrib/lite/profiling/time.h:18:10: fatal error: 'cstdint' file not found\r\n#include <cstdint>\r\n         ^\r\n1 error generated.\r\nTarget //tensorflow/contrib/lite/java:libtensorflowlite_jni.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.575s, Critical Path: 0.15s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n+ rm -rf /tmp/tmp.1epU8vtRBz\r\n```\r\nwhen I try again, this information would changed.\r\n```\r\n.....\r\n.....\r\n.....\r\nERROR: /home/tclxa/tensorflow/tensorflow/contrib/lite/BUILD:272:1: C++ compilation of rule '//tensorflow/contrib/lite:util' failed (Exit 1)\r\nIn file included from tensorflow/contrib/lite/util.cc:15:\r\n./tensorflow/contrib/lite/util.h:24:10: fatal error: 'vector' file not found\r\n#include <vector>\r\n         ^\r\n1 error generated.\r\nTarget //tensorflow/contrib/lite/java:libtensorflowlite_jni.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n....\r\n....\r\n```\r\n\r\nIt`s a kind of problem.  I try to change gcc-5.4 into gcc-4.7 and g++-5.4 into g++-4.7 .  but its don`t work. so how to solve this problem,  very thanks.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "OS Platform and Distribution\r\nubuntu 16.04LTS\r\n\r\nTensorFlow installed from\r\nSource\r\nTensorFlow version\r\ntensorflow-1.9.0\r\nCUDA/cuDNN version\r\nN/A\r\nGPU model and memory\r\nN/A\r\nExact command to reproduce\r\nsudo  ./tensorflow/contrib/lite/java/build_aar_for_release.sh \r\nMobile device\r\nN/A\r\n\r\n@suharshs \r\n@tensorflowbutler \r\n@poxvoculi \r\n@miaout17 \r\n@ry ", "What Android NDK revision are you using?", "NDK version: 18.0", "Nagging Assignee @jdduke: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Looks like this might have been fixed in https://github.com/bazelbuild/bazel/issues/5585. Can you give it another try?", "I was able to compile with NDK r18b (on Linux) using the latest bazel build. Feel free to reopen if you find otherwise."]}, {"number": 21670, "title": "freeze_graph returns with error", "body": "I used `bazel-bin/tensorflow/python/tools/freeze_graph` to freeze my graph. This program complied with bazel several minutes ago.\r\nBut this returns with error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/roche/git/download/tensorflow-1.10.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 48, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/roche/git/download/tensorflow-1.10.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"/home/roche/git/download/tensorflow-1.10.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py\", line 52, in <module>\r\n    from tensorflow.python.framework.importer import import_graph_def\r\n  File \"/home/roche/git/download/tensorflow-1.10.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/importer.py\", line 27, in <module>\r\n    from tensorflow.python.framework import function\r\n  File \"/home/roche/git/download/tensorflow-1.10.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/function.py\", line 39, in <module>\r\n    from tensorflow.python.ops import variable_scope as vs\r\n  File \"/home/roche/git/download/tensorflow-1.10.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 181, in <module>\r\n    class _ReuseMode(enum.Enum):\r\nNameError: name 'enum' is not defined\r\n```\r\n\r\nHere's my environment specification:\r\nSys:Debian 9.5\r\npython:python 3.5.3\r\nTensorflow: tensorflow (1.10.0) from pip3\r\nbazel:0.16.1\r\ngcc:gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\n\r\ntensorflow souce code from 1.10.0 release.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "OS: Debian 9.5\r\npython: python 3.5.3\r\nTensorflow: tensorflow (1.10.0) installed from pip3, CPU version\r\nCPU: Intel i5 3470\r\nbazel: 0.16.1\r\nfreeze_graph complied from: tensorflow 1.10.0 release\r\ngcc: gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\nExact command to reproduce: ./bazel-bin/tensorflow/python/tools/freeze_graph\r\n", "Is the source modified at all? \r\nThe `enum` symbol it is complaining about should be coming from the Python standard library imported [here](https://github.com/tensorflow/tensorflow/blob/0e53c66f338a60be5ea348fdeb7b19e5b579971f/tensorflow/python/ops/variable_scope.py#L24). ", "No modification.\r\nI just download the tf 1.10 release, then complied it.", "Seems like some quirk of the environment this is being run in. Does the following work:\r\n\r\n```python\r\nimport enum\r\n\r\nclass Foo(enum.Enum):\r\n  pass\r\n```\r\n?\r\n\r\nMarking as \"Community Support\" to engage the broader community in this (also, I'm unable to reproduce this in my environment, using:\r\n\r\n```sh\r\n./configure\r\nbazel build -c opt //tensorflow/python/tools:freeze_graph\r\n./bazel-bin/tensorflow/tools/python/freeze_graph --help\r\n```\r\n", "@roche-k ,\r\nWe see that you are using older version of tensorflow (1.x) which is not actively supported. We recommend that you upgrade to latest stable version of tensorflow 2.6.0 and let us know if the issue still persists in newer versions .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21670\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21670\">No</a>\n"]}, {"number": 21669, "title": "no device type supports both of those nodes and the other nodes colocated with them.", "body": "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\r\n  File \"D:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\clip_ops.py\", line 222, in clip_by_global_norm\r\n    use_norm = global_norm(t_list, name)\r\n  File \"D:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\clip_ops.py\", line 161, in global_norm\r\n    half_squared_norms.append(gen_nn_ops.l2_loss(v))\r\n  File \"D:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 4662, in l2_loss\r\n    \"L2Loss\", t=t, name=name)\r\n  File \"D:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"D:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\r\n    op_def=op_def)\r\n  File \"D:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n\r\nInvalidArgumentError (see above for traceback): Cannot colocate nodes 'model/optimizer/global_norm/L2Loss_17' and 'model/optimizer/gradients/model/inference/CudnnRNN_1_grad/CudnnRNNBackprop' because no device type supports both of those nodes and the other nodes colocated with them.\r\nColocation Debug Info:\r\nColocation group had the following types and devices:\r\nCudnnRNNBackprop: GPU\r\nIdentity:\r\nL2Loss: CPU\r\n\r\nColocation members and user-requested devices:\r\n  model/optimizer/gradients/model/inference/CudnnRNN_1_grad/CudnnRNNBackprop (CudnnRNNBackprop)\r\n  model/optimizer/gradients/model/inference/CudnnRNN_1_grad/tuple/control_dependency_3 (Identity)\r\n  model/optimizer/global_norm/L2Loss_17 (L2Loss)\r\n\r\n         [[Node: model/optimizer/global_norm/L2Loss_17 = L2Loss[T=DT_FLOAT, _class=[\"loc:@model/optimizer/gradients/model/inference/CudnnRNN_1_grad/CudnnRNNBackprop\"]](model/optimizer/gradients/model/inference/CudnnRNN_1_grad/tuple/control_dependency_3)]]\r\n\r\nWhy? and how to fix it, thanks\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "CUDA 9\r\nCUDNN 7\r\n\r\nWindows 10\r\n2x GPU 1070\r\n "]}]