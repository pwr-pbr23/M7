[{"number": 34513, "title": "Q", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information** \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached...\r\n", "comments": ["@mouse36872 Please go through the contributing guidelines at https://www.tensorflow.org/community/contribute."]}, {"number": 34512, "title": "Learning VM", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@mouse36872 it doesn't look like your raising any issue here. Kindly edit and add a clear description of the issue your reporting/facing."]}, {"number": 34511, "title": "Grammar.txt not found pip3 install --upgrade tensorflow-gpu", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Win10 Pro [Version 10.0.19013.1122]\r\n- Tried to install TensorFlow  installed using pip3\r\n- TensorFlow version: auto-select\r\n- Python version: Python 3.5.0 (v3.5.0:374f501f4567)\r\n- Installed using - pip\r\n- CUDA/cuDNN version: Cuda compilation tools, release 10.1, V10.1.243\r\n- GPU model and memory: NVIDIA GeForce 920M, 2GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI have `python-3.5.0-embed-amd64` on my system and `pip`, `pip3` also.\r\nThey are also defined in path.\r\n\r\nNow when I try to install : **pip3 install --upgrade tensorflow-gpu**\r\n\r\nIt gives me error saying :\r\n```\r\nerror: [Errno 2] No such file or directory: 'e:\\\\software\\\\python-3.5.0-embed-amd64\\\\python35.zip\\\\lib2to3\\\\Grammar.txt'\r\n```\r\nFull error : [here](https://pastebin.com/VxQRXKYj)\r\n\r\nBut as you can see, it it present actually:\r\n\r\n[![grammer.txt not found pip3 install --upgrade tensorflow-gpu][1]][1]\r\n\r\n  [1]: https://i.stack.imgur.com/6BnRr.png\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\n\r\nFull error : [here](https://pastebin.com/VxQRXKYj)\r\n", "comments": ["Solved with `Python 3.5.0 (v3.5.0:374f501f4567, Sep 13 2015, 02:27:37)`, when installed using an executable installer. \r\n\r\nReference : [stackoverflow](https://stackoverflow.com/questions/58988652/grammar-txt-not-found-pip3-install-upgrade-tensorflow-gpu?noredirect=1#comment104230564_58988652)\r\n\r\nThat's all !!!"]}, {"number": 34510, "title": "BufferedInputStream should avoid doing Reset() in Seek when new position is still in buffer. ", "body": "Check the [code here](https://github.com/tensorflow/tensorflow/blob/597a30bc61134ee1deec0b439b3649f346f1f119/tensorflow/core/lib/io/buffered_inputstream.cc#L161) \r\n``` \r\n // Position of the buffer within file.\r\n  const int64 bufpos = Tell();\r\n  if (position < bufpos) {\r\n    // Reset input stream and skip 'position' bytes.\r\n    TF_RETURN_IF_ERROR(Reset());\r\n    return SkipNBytes(position);\r\n  }\r\n```\r\n\r\n I see two issues, correct me if i'm wrong:\r\n1. Tell() doesn't return the position of the buffer within file as suggested in comment, in fact it returns the position of the `pos_` within the file.\r\n2. when `position` is still inside buffer, it will do a `Reset` anyway, which could impact the performance for read later since it would fill buffer of the previously buffered data. \r\n\r\nFixing PR: #34515 ", "comments": ["Closing this issue since the associated PR is closed. Feel free to reopen if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34510\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34510\">No</a>\n"]}, {"number": 34509, "title": "Add networking related headers in pip wheel", "body": "This is for building the networking plugins in https://github.com/tensorflow/networking that sub-class from the gRPC-base distributed runtime in TF core. Currently all distributed runtime headers are missing from the PyPI wheels, and we will need to download and build TF core from source every time we build a networking plugin. This is a subsequent change after 11385c9cf21cf9c098040075fe82a4667906155b.\r\n\r\nPing @mrry @dubey for review.", "comments": ["Reassigning this review to Ayush!", "@gunan @sjamesr @joker-eph Should we effectively extend the visibility of these existing headers or should we settle on a public API instead?\r\n\r\nThe headers we're talking about are related to the GRPC runtime.\r\n\r\n@byronyi can you clarify specifically what header files we're looking at here? I worry because currently this particular build rule has internal-only visibility, and I wonder if the best solution is to make a new header file with public visibility and a minimal subset of the APIs needed to unblock you.", "@alextp I list specific headers here required for building the plugins:\r\n\r\nExtending gRPC-based distributed runtime by sub-classing (all current plugins):\r\n\r\n```\r\n#include \"tensorflow/core/distributed_runtime/rpc/grpc_server_lib.h\"\r\n```\r\n\r\nExtending transport protocols for SendRecvOps:\r\n\r\n```\r\n#include \"tensorflow/core/distributed_runtime/base_rendezvous_mgr.h\"\r\n#include \"tensorflow/core/distributed_runtime/worker_env.h\"\r\n```\r\n\r\nExtending transport protocols for CollectiveOps:\r\n\r\n```\r\n#include \"tensorflow/core/distributed_runtime/collective_param_resolver_distributed.h\"\r\n#include \"tensorflow/core/distributed_runtime/device_resolver_distributed.h\"\r\n#include \"tensorflow/core/distributed_runtime/rpc_collective_executor_mgr.h\"\r\n```\r\n\r\nOther headers required by current plugins:\r\n\r\nGDR:\r\n\r\n```\r\n#include \"tensorflow/core/distributed_runtime/cancellable_call.h\"\r\n#include \"tensorflow/core/distributed_runtime/graph_mgr.h\"\r\n#include \"tensorflow/core/distributed_runtime/rendezvous_mgr_interface.h\"\r\n#include \"tensorflow/core/distributed_runtime/request_id.h\"\r\n#include \"tensorflow/core/distributed_runtime/rpc/grpc_call.h\"\r\n#include \"tensorflow/core/distributed_runtime/rpc/grpc_tensor_coding.h\"\r\n#include \"tensorflow/core/distributed_runtime/rpc/grpc_util.h\"\r\n#include \"tensorflow/core/distributed_runtime/tensor_coding.h\"\r\n#include \"tensorflow/core/distributed_runtime/worker.h\"\r\n#include \"tensorflow/core/distributed_runtime/worker_cache.h\"\r\n#include \"tensorflow/core/distributed_runtime/worker_interface.h\"\r\n#include \"tensorflow/core/distributed_runtime/worker_session.h\"\r\n```\r\n\r\nMPI:\r\n\r\n```\r\n#include \"tensorflow/core/distributed_runtime/recent_request_ids.h\"\r\n```\r\n\r\nVerbs:\r\n\r\n```\r\n#include \"tensorflow/core/distributed_runtime/call_options.h\"\r\n#include \"tensorflow/core/distributed_runtime/rendezvous_mgr_interface.h\"\r\n#include \"tensorflow/core/distributed_runtime/rpc/grpc_util.h\"\r\n#include \"tensorflow/core/distributed_runtime/rpc/grpc_worker_cache.h\"\r\n#include \"tensorflow/core/distributed_runtime/session_mgr.h\"\r\n```\r\n\r\nAlternatively, using a work-in-process C API for networking ([transitively](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/experimental/BUILD#L34) depends on ``grpc_server_lib.h``:\r\n\r\n```\r\n#include \"tensorflow/c/experimental/network.h\"\r\n```", "Also pulling in @annarev , a while ago she looked into these.\r\nI also remember @poxvoculi  looked into these before, but not sure how much he is involved in these nowadays.", "I'd be happy exposing the C API for networking as that is intended for public consumption and won't lead to too much coupling with the details of the current runtime.\r\n\r\nI worry about the other headers, though, as many of them are very specific to details of the current implementations (like the worker cache ones, the rendezvous manager ones, etc). \r\n\r\nMaybe it's worth adding some new headers which are lightweight APIs (that is, just have the interfaces you need but no implementation, etc) to avoid coupling these plugins too hard in a way that prevents the system from being refactored?", "@alextp okay, how about I change this PR to only #include \"tensorflow/c/experimental/network.h\"?", "Then I'll approve it!\n\nThat and changes to extend network.h with more APIs you'll need in the\nfuture.\n\nOn Mon, Nov 25, 2019 at 6:30 PM Bairen Yi <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> okay, how about I change this PR to\n> only #include \"tensorflow/c/experimental/network.h\"?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/34509?email_source=notifications&email_token=AAABHRJQGBEYSYBJAJGXK5LQVSC4LA5CNFSM4JQL6LQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEFEPUII#issuecomment-558430753>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNH5HJEDN53TOREQH3QVSC4LANCNFSM4JQL6LQA>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp I have made the change to this PR. Let me know if you have any other concern.\r\n\r\n> Then I'll approve it! That and changes to extend network.h with more APIs you'll need in the future.\r\n> [\u2026](#)", "Gently ping @alextp ", "@byronyi  Could you please address Ubuntu Sanity errors? Thanks!", "Sorry for my typo. @gbaned @alextp mind to kick off the CI again?", "That seems to be caused by a Eigen dependency that fails to be downloaded...it happens for nightly TF builds (including our in-house one, starting from 11-18 837b26bd578d33cc44ca449f5d3b33357fddac36 but only effective since today), not only this particular PR.\r\n\r\nhttp://eigen.tuxfamily.org/index.php?title=News:Migration_to_GitLab.com_scheduled_on_the_December_4th\r\n\r\nFixing this should be easy in principle, as one only needs to upload to the backup url mirror, as shown in 3bc9dd3b776c26fbd8d4eba7917fe873a5596e40.\r\n\r\nPing @gunan; but I am sure the team will get notified one way or another. Just that this PR needs to be kicked off for CI once the Eigen issue is resolved."]}, {"number": 34508, "title": "How to get sample_weights and learning_phase? (TF2 Eager)", "body": "`model.sample_weights` works w/ `from keras` imports but not `from tensorflow.keras`; I need the sample weights _tensor_  to feed to `K.function` to get layer gradients, outputs, etc.\r\n\r\nUsing `model._feed_sample_weights` instead does eliminate the error, but returns `[]`, and feeding `np.ones(32)` vs. `4*np.ones(32)` for sample weights yields the same outputs (it shouldn't).\r\n\r\n<hr>\r\n\r\n**Note**: see [this thread](https://github.com/tensorflow/tensorflow/issues/34507) regarding learning_phase\r\n\r\n<hr>\r\n\r\n**Applied example**:\r\n\r\n```python\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.models import Model\r\nimport numpy as np\r\n\r\nipt = Input((16,))\r\nout = Dense(16)(ipt)\r\nmodel = Model(ipt, out)\r\nmodel.compile('adam', 'mse')\r\n\r\nx = np.random.randn(32, 16)\r\nmodel.train_on_batch(x, x)\r\n\r\ngrads = model.optimizer.get_gradients(model.total_loss, model.layers[-1].output)\r\ngrads_fn = K.function(inputs=[model.inputs[0], model._feed_targets[0], model.sample_weights[0]], \r\n                      outputs=grads)\r\n```\r\n\r\n**Full error trace**:\r\n\r\n```python\r\nFile \"<ipython-input-3-ef022490b960>\", line 16, in <module>\r\n  outputs=grads)\r\nFile \"D:\\Anaconda\\envs\\tf2_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 3773, in function\r\n  return EagerExecutionFunction(inputs, outputs, updates=updates, name=name)\r\nFile \"D:\\Anaconda\\envs\\tf2_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 3670, in __init__\r\n  base_graph=source_graph)\r\nFile \"D:\\Anaconda\\envs\\tf2_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\lift_to_graph.py\", line 249, in lift_to_graph\r\n  visited_ops = set([x.op for x in sources])\r\nFile \"D:\\Anaconda\\envs\\tf2_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\lift_to_graph.py\", line 249, in <listcomp>\r\n  visited_ops = set([x.op for x in sources])\r\n\r\nAttributeError: 'NoneType' object has no attribute 'op'\r\n```", "comments": ["Hi @OverLordGoldDragon The above code works fine for TensorFlow 1.15\r\nBut it seems to fail for TF 2.0\r\n\r\nThe reason it is failing is the ``` python3 model.sample_weights[0] ``` is outputting None\r\nI don't know the reason for the output None for the model.sample_weights \r\n\r\nPlease ping me if your issue gets resolved or you find any more issue.", "I have tried on colab with TF version 2.0 ,2.1.0-dev20191124 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/eb1a3406aab356a8128b7ecd391fbfa5/untitled405.ipynb).However i am not seeing any issue with TF version 1.15. Thanks!\r\n", "@ravikyram Indeed, TF 1.14/1.15 works fine. \r\n\r\nOff-topic but interestingly, your Colab notebook doesn't show any \"Owners\" in \"Notebook info\" - how did you do it? Any share option I use includes my Colab g-mail", "fchollet assigned - hope there's a response before TF3 release", "A partly working, but unstable workaround was found in a [related SO](https://stackoverflow.com/questions/58987264/how-to-get-learning-phase-in-tensorflow-2-eager/59059306?noredirect=1#answer-59059306):\r\n\r\n```python\r\nimport tensorflow.python.keras.backend as K\r\nlearning_phase = K.symbolic_learning_phase()\r\n```\r\nThis'll work in a simple example, but is [problematic](https://stackoverflow.com/questions/58279628/what-is-the-difference-between-tf-keras-and-tf-python-keras?noredirect=1&lq=1) in practice", "Any progress on this in TF2.2?", "@OverLordGoldDragon is this still an issue, this maybe coming very late, and you might have figured this out already  [get_gradients is only supported in v1 style graph mode](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer?version=nightly#get_gradients). for computing gradients in Eager mode, please check this [link](https://www.tensorflow.org/guide/eager#eager_training)\r\n", "@goldiegadde [Resolved](https://stackoverflow.com/a/61952452/10133797).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34508\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34508\">No</a>\n", "@OverLordGoldDragon could you please add more details on symbolic_learning_phase issue? I want to use learning phase as a part of the model in graph mode to control layer behavior depend on lf  and can't do this due to it just an int, not a tensor"]}, {"number": 34507, "title": "How to get symbolic learning_phase? (TF2 Eager)", "body": "`K.learning_phase()` fetches the value, not the tensor itself - following _backend.py_, I found somewhat of a workaround, but it isn't user/API-friendly. I need the learning phase tensor to feed to `K.function` to get layer gradients, outputs, etc. Works fine w/ `import keras.backend as K`, but fails for `import tensorflow.keras.backend as K`.\r\n\r\nPassing the symbolic learning_phase into `K.function` yields:\r\n\r\n```python\r\nValueError: Cannot create an execution function which is comprised of elements \r\nfrom multiple graphs.\r\n```\r\n\r\n<hr>\r\n\r\n**Minimal applied example**:\r\n\r\n```python\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.models import Model\r\nimport numpy as np\r\n\r\nipt = Input((16,))\r\nout = Dense(16)(ipt)\r\nmodel = Model(ipt, out)\r\nmodel.compile('adam', 'mse')\r\n\r\nx = np.random.randn(32, 16)\r\nmodel.train_on_batch(x, x)\r\n\r\ngrads = model.optimizer.get_gradients(model.total_loss, model.layers[-1].output)\r\ngrads_fn = K.function(inputs=[model.inputs[0], model._feed_targets[0], K.learning_phase()], \r\n                      outputs=grads)\r\n```\r\n\r\n**Full error trace**:\r\n\r\n```python\r\nFile \"<ipython-input-2-7f74922d7492>\", line 3, in <module>\r\n  outputs=grads)\r\nFile \"D:\\Anaconda\\envs\\tf2_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 3773, in function\r\n  return EagerExecutionFunction(inputs, outputs, updates=updates, name=name)\r\nFile \"D:\\Anaconda\\envs\\tf2_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 3670, in __init__\r\n  base_graph=source_graph)\r\nFile \"D:\\Anaconda\\envs\\tf2_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\lift_to_graph.py\", line 249, in lift_to_graph\r\n  visited_ops = set([x.op for x in sources])\r\nFile \"D:\\Anaconda\\envs\\tf2_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\lift_to_graph.py\", line 249, in <listcomp>\r\n  visited_ops = set([x.op for x in sources])\r\n\r\nAttributeError: 'int' object has no attribute 'op'\r\n```\r\n\r\n<hr>\r\n\r\n**Partial workaround**:\r\n\r\n```python\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.python.eager import context\r\nfrom tensorflow.python.ops import array_ops\r\nimport weakref\r\nfrom tensorflow.python.framework import func_graph\r\n\r\ndef symbolic_learning_phase():\r\n  graph = get_graph()\r\n  with graph.as_default():\r\n    if graph not in _GRAPH_LEARNING_PHASES:\r\n      with K.name_scope(''):\r\n        phase = array_ops.placeholder_with_default(\r\n            False, shape=(), name='keras_learning_phase')\r\n      _GRAPH_LEARNING_PHASES[graph] = phase\r\n    return _GRAPH_LEARNING_PHASES[graph]\r\n\r\ndef get_graph():\r\n  if context.executing_eagerly():\r\n    global _GRAPH\r\n    if _GRAPH is None:\r\n      _GRAPH = func_graph.FuncGraph('keras_graph')\r\n    return _GRAPH\r\n  else:\r\n    return ops.get_default_graph()\r\n\r\n_GRAPH = None\r\n_GRAPH_LEARNING_PHASES = weakref.WeakKeyDictionary()\r\n\r\nsymbolic_learning_phase()\r\n# <tf.Tensor 'keras_learning_phase:0' shape=() dtype=bool>\r\n```\r\n", "comments": ["I have tried on colab with TF version 2.0 ,2.1.0-dev20191124 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/849d81a1d109e547e6ab269aa83afe63/untitled406.ipynb).However i am not seeing any issue with TF version 1.15. Thanks!", "Duplicate #34508. Tracking this issue in another thread. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34507\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34507\">No</a>\n"]}, {"number": 34506, "title": "[tf.data] Fix deadlock with Prefetch+ParallelMap", "body": "PiperOrigin-RevId: 281851149\r\nChange-Id: I1b776edb68b45eabc9a0e931135470cae1b6e8f1", "comments": []}, {"number": 34505, "title": "Cherrypicks for r2.1", "body": "", "comments": []}, {"number": 34504, "title": "[Intel MKL] Automatic BFloat16 converter", "body": "This PR enables a grappler based pass that automatically converts FP32\r\ngraph into BFloat16 graph for training and inference. The PR also adds\r\nsupport for 1) producing verbose logs in a file, and 2) context-based rewrite\r\nfor some ops (such as element-wise) that may not benefit with BFloat16 always.\r\nThe PR also adds grappler-based unit tests. The optimizer is turned OFF\r\nby default (for now). It can turned ON via RewriterConfig.\r\n\r\nChange in meta_optimizer.cc - one change is suggested by Clang and not related\r\nto this feature.", "comments": ["@apassos FYI", "@rmlarsen Thanks for review. I've addressed your comments. Pls take a look.", "Hi @rmlarsen, can you pls take a look at my update that addresses your comments? Thanks.", "Please try merging this functionality into the existing [auto_mixed_precision](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/auto_mixed_precision.cc) pass. Understandably, it will be tough to integrate a 300 line pass into an existing 1800 line pass. But I don't think we should have separate passes that do the same thing, just for different dtypes and devices.\r\n\r\nI don't think you'll have to modify a significant amount of the auto_mixed_precision pass. You'll have to modify the parts that currently hardcode DT_HALF and that check if a node is on a GPU. You'll also have to create a new set of lists in [auto_mixed_precision_lists.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/auto_mixed_precision_lists.h) for MKL (although maybe the clearlist can be the same?)\r\n\r\nIf it's possible to detect if MKL is available and the CPU supports bfloat16, you should have the auto_mixed_precision pass rewrite nodes on the CPU as well as the GPU. Otherwise, you should register a new pass, say \"auto_mixed_precision_mkl\". Then add a constructor argument to AutoMixedPrecision indicating whether the CUDA version or the MKL version should be used.\r\n\r\nA summary of the auto_mixed_precision pass is [here](https://github.com/tensorflow/tensorflow/blob/65c7a019aff4f08fe8a48348921dcbf0336909f2/tensorflow/core/grappler/optimizers/auto_mixed_precision.cc#L1318).\r\n\r\nPlease let me know if you have any questions. I apologize for the late response.\r\n\r\n/CC @benbarsdell @nluehr, who wrote much of auto_mixed_precision.", "hi @reedwm, thanks for your suggestion. I had taken a look at AutoMixedPrecision pass before I implemented a separate pass for BFloat16Converter.\r\n\r\nThe reason I decided not to modify AutoMixedPrecision pass was that the core algorithm that we use for deciding whether to rewrite a node or not in case of BFloat16Converter is lot more simple (given that we do not need to worry about numerical instability issue). To be precise, we don't have pass 2 (to propagate black forwards), pass 3 and pass 4 are also simplified a lot (given that Eigen CPU backend has a limited BFloat16 support). Using AutoMixedPrecision algorithm seems an overkill for BFloat16Converter.\r\n\r\nAnother complication is build dependency: AutoMixedPrecision depends on CUDA config option, while BFloat16Converter depends on MKL config option, and two build options are mutually exclusive. Not sure how to get handle this complication?\r\n\r\nThe only advantage of moving my pass to AutoMixedPrecision would be to reuse some of the utility and boilerplate code. But given that this code is around 200 lines in BFloat16Converter, I am not sure it means much saving.", "@nhasabni, I still think it's worth merging the two passes. Even though the BFloat16Converter is a lot simpler, I think it's better to have one place in TensorFlow where we have the logic to rewrite the graph to use mixed precision.\r\n\r\nAs for the CUDA and MKL configs, the auto_mixed_precision pass does not actually have a dependency on CUDA. For the MKL-specific functionality (`mkl_op_registry::GetMklOpName`, etc), I think we can put that functionality in a separate file which has a dependency on MKL. Or maybe ifdefs can be used.\r\n\r\nAnyway, since I am familiar with the `auto_mixed_precision` pass and because this PR is relatively simple, I can try merging this grappler pass into auto_mixed_precision. If I realize this is too challenging, I can instead review this PR as is and leave the two grappler passes unmerges. Would you be OK with me trying to merge them? If so, I can send a PR to your `Intel-tensorflow:nhasabni/bf16_converter` branch, or I can directly commit to it (I think you gave TensorFlow reviewers commit access to that branch already).", "@reedwm OK, sure, pls go ahead with trying to merge two passes. There are a couple of points that we would like to ensure though: 1) MKL pass should still have a separate boolean flag (in RewriterConfig) to control it from Python API, 2) MKL pass should allow controlling operators that should be/should not be rewritten to BFloat16 (I think this is straightforward given that MKL pass will have a separate white, black and grey lists.)", "@nhasabni I will try to merge the two passes. Can you give me instructions for running the tests? I checked out this PR and ran the following\r\n\r\n```\r\nyes '' | ./configure\r\nbazel test --config=mkl --config=opt //tensorflow/core/grappler/optimizers:convert_to_bfloat16_test\r\n```\r\n\r\nBut it doesn't seem like none of the tests run. Even if I add `EXPECT_EQ(0, 1);` to a test, it still passes.\r\n\r\n> MKL pass should allow controlling operators that should be/should not be rewritten to BFloat16 (I think this is straightforward given that MKL pass will have a separate white, black and grey lists.)\r\n\r\nThe auto_mixed_precision pass already has environmental variables such as `TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_WHITELIST_ADD`, so this will not require any extra effort once I merge the passes.", "I also tried running\r\n\r\n```\r\nbazel test --copt=-DENABLE_INTEL_MKL_BFLOAT16 --config=mkl --config=opt //tensorflow/core/grappler/optimizers:convert_to_bfloat16_test\r\n```\r\n\r\nBut the test failed with the following error\r\n```\r\n[ RUN      ] BFloat16ConverterTest.AlreadyBFloat16\r\n2020-01-28 03:12:21.184809: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at mkl_relu_op.cc:547 : Aborted: Operation received an exception:Status: 5, message: could not create a primitive descriptor iterator, in file tensorflow/core/kernels/mkl_relu_op.cc:544\r\n2020-01-28 03:12:21.185276: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at mkl_relu_op.cc:547 : Aborted: Operation received an exception:Status: 5, message: could not create a primitive descriptor iterator, in file tensorflow/core/kernels/mkl_relu_op.cc:544\r\n2020-01-28 03:12:21.185400: F tensorflow/core/grappler/utils/grappler_test.cc:107] Non-OK-status: session->Run(run_options, inputs, node_names, node_names, &output_tensors, nullptr) status: Aborted: Operation received an exception:Status: 5, message: could not create a primitive descriptor iterator, in file tensorflow/core/kernels/mkl_relu_op.cc:544\r\n\t [[{{node d}}]]\r\n*** Received signal 6 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n/usr/local/google/home/reedwm/.cache/bazel/_bazel_reedwm/02a78c87331ebad0fd7841cf0e5b369b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_k8/_U_S_Stensorflow_Score_Sgrappler_Soptimizers_Cconvert_Uto_Ubfloat16_Utest___Utensorflow/libtensorflow_framework.so.2(+0xf4d63e)[0x7f8e3f30263e]\r\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x123a0)[0x7f8e32de63a0]\r\n/lib/x86_64-linux-gnu/libc.so.6(gsignal+0x10b)[0x7f8e32623cfb]\r\n/lib/x86_64-linux-gnu/libc.so.6(abort+0x129)[0x7f8e3260e8ad]\r\n/usr/local/google/home/reedwm/.cache/bazel/_bazel_reedwm/02a78c87331ebad0fd7841cf0e5b369b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_k8/_U_S_Stensorflow_Score_Sgrappler_Soptimizers_Cconvert_Uto_Ubfloat16_Utest___Utensorflow/libtensorflow_framework.so.2(+0x141a1e7)[0x7f8e3f7cf1e7]\r\n/usr/local/google/home/reedwm/.cache/bazel/_bazel_reedwm/02a78c87331ebad0fd7841cf0e5b369b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_k8/libtensorflow_Score_Sgrappler_Sutils_Slibgrappler_Utest.so(_ZNK10tensorflow8grappler12GrapplerTest13EvaluateNodesERKNS_8GraphDefERKSt6vectorINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEESaISB_EERKS5_ISt4pairISB_NS_6TensorEESaISI_EE+0x21b)[0x7f8e3e1a97fb]\r\n/usr/local/google/home/reedwm/.cache/bazel/_bazel_reedwm/02a78c87331ebad0fd7841cf0e5b369b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_k8/libtensorflow_Score_Sgrappler_Sutils_Slibgrappler_Utest.so(_ZNK10tensorflow8grappler12GrapplerTest13EvaluateNodesERKNS_8GraphDefERKSt6vectorINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEESaISB_EE+0x34)[0x7f8e3e1a9924]\r\n/usr/local/google/home/reedwm/.cache/bazel/_bazel_reedwm/02a78c87331ebad0fd7841cf0e5b369b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/convert_to_bfloat16_test.runfiles/org_tensorflow/tensorflow/core/grappler/optimizers/convert_to_bfloat16_test(+0x58ffd5)[0x55abe695dfd5]\r\n/usr/local/google/home/reedwm/.cache/bazel/_bazel_reedwm/02a78c87331ebad0fd7841cf0e5b369b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8internal35HandleExceptionsInMethodIfSupportedINS_4TestEvEET0_PT_MS4_FS3_vEPKc+0x4d)[0x7f8e3bf96fbd]\r\n/usr/local/google/home/reedwm/.cache/bazel/_bazel_reedwm/02a78c87331ebad0fd7841cf0e5b369b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing4Test3RunEv+0xdb)[0x7f8e3bf9722b]\r\n/usr/local/google/home/reedwm/.cache/bazel/_bazel_reedwm/02a78c87331ebad0fd7841cf0e5b369b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8TestInfo3RunEv+0x121)[0x7f8e3bf97561]\r\n/usr/local/google/home/reedwm/.cache/bazel/_bazel_reedwm/02a78c87331ebad0fd7841cf0e5b369b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing9TestSuite3RunEv+0xc7)[0x7f8e3bf97837]\r\n/usr/local/google/home/reedwm/.cache/bazel/_bazel_reedwm/02a78c87331ebad0fd7841cf0e5b369b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8internal12UnitTestImpl11RunAllTestsEv+0x40c)[0x7f8e3bf97cdc]\r\n/usr/local/google/home/reedwm/.cache/bazel/_bazel_reedwm/02a78c87331ebad0fd7841cf0e5b369b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8internal35HandleExceptionsInMethodIfSupportedINS0_12UnitTestImplEbEET0_PT_MS4_FS3_vEPKc+0x4d)[0x7f8e3bf97e2d]\r\n/usr/local/google/home/reedwm/.cache/bazel/_bazel_reedwm/02a78c87331ebad0fd7841cf0e5b369b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8UnitTest3RunEv+0x96)[0x7f8e3bf98066]\r\n/usr/local/google/home/reedwm/.cache/bazel/_bazel_reedwm/02a78c87331ebad0fd7841cf0e5b369b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/../../../../_solib_k8/libtensorflow_Score_Slibtest_Umain.so(main+0xbd)[0x7f8e3ffcca4d]\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xeb)[0x7f8e3261052b]\r\n/usr/local/google/home/reedwm/.cache/bazel/_bazel_reedwm/02a78c87331ebad0fd7841cf0e5b369b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/grappler/optimizers/convert_to_bfloat16_test.runfiles/org_tensorflow/tensorflow/core/grappler/optimizers/convert_to_bfloat16_test(+0x57c6ea)[0x55abe694a6ea]\r\n*** END MANGLED STACK TRACE ***\r\n\r\n*** Begin stack trace ***\r\n\ttensorflow::CurrentStackTrace[abi:cxx11]()\r\n\t\r\n\t\r\n\tgsignal\r\n\tabort\r\n\t\r\n\ttensorflow::grappler::GrapplerTest::EvaluateNodes(tensorflow::GraphDef const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&) const\r\n\ttensorflow::grappler::GrapplerTest::EvaluateNodes(tensorflow::GraphDef const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) const\r\n\t\r\n\tvoid testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*)\r\n\ttesting::Test::Run()\r\n\ttesting::TestInfo::Run()\r\n\ttesting::TestSuite::Run()\r\n\ttesting::internal::UnitTestImpl::RunAllTests()\r\n\tbool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*)\r\n\ttesting::UnitTest::Run()\r\n\tmain\r\n\t__libc_start_main\r\n\t\r\n*** End stack trace ***\r\nexternal/bazel_tools/tools/test/test-setup.sh: line 310: 126224 Aborted                 \"${TEST_PATH}\" \"$@\" 2>&1\r\n```", "@reedwm I've pushed a commit that handles changes introduced by updates in master branch. The test passes in my setup. Pls let me know if this does not work for you.\r\n\r\n```\r\nbazel --output_user_root=/localdisk/nhasabni/tfbuilds/public_tfbuild test --config=mkl --copt=-DENABLE_INTEL_MKL_BFLOAT16 -c opt //tensorflow/core/grappler/optimizers:convert_to_bfloat16_test\r\n\r\nINFO: Found 1 test target...\r\nTarget //tensorflow/core/grappler/optimizers:convert_to_bfloat16_test up-to-date:\r\n  bazel-bin/tensorflow/core/grappler/optimizers/convert_to_bfloat16_test\r\nINFO: Elapsed time: 9.617s, Critical Path: 8.91s\r\nINFO: 4 processes: 4 local.\r\nINFO: Build completed successfully, 6 total actions\r\n//tensorflow/core/grappler/optimizers:convert_to_bfloat16_test           PASSED in 0.8s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\nThere were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see whichINFO: Build completed successfully, 6 total actions```", "@nhasabni I am still getting the same error. It looks like the issue is the \"could not create a primitive descriptor iterator\" error in the logs in my previous message. I think this is from [this line](https://github.com/intel/mkl-dnn/blob/56ef626d6627e93da039c15e032603e1a4bc8af4/include/mkldnn.hpp#L1829) in MKL. Any ideas what could be causing this issue?", "> @nhasabni I am still getting the same error. It looks like the issue is the \"could not create a primitive descriptor iterator\" error in the logs in my previous message. I think this is from [this line](https://github.com/intel/mkl-dnn/blob/56ef626d6627e93da039c15e032603e1a4bc8af4/include/mkldnn.hpp#L1829) in MKL. Any ideas what could be causing this issue?\r\n\r\n@reedwm It looks like MKL-DNN cannot create RELU primitive descriptor in bfloat16 type. Can you specify your config? I suspect if arch difference is a problem. My config is ``` Intel(R) Xeon(R) Platinum 8180 CPU (SkyLake), gcc-7.4, Ubuntu-18.04.2```. MKL-DNN does not support bfloat16 type for arch < Skylake.\r\n", "I have a Intel\u00ae Xeon\u00ae Processor E5-2690 v4 CPU, which is Broadwell according to [this page](https://ark.intel.com/content/www/us/en/ark/products/91770/intel-xeon-processor-e5-2690-v4-35m-cache-2-60-ghz.html). That would explain why I cannot run the test. Ideally the error message would be improved.\r\n\r\nIs it possible to run the test on Broadwell? If not I can use a google cloud instance.", "> I have a Intel\u00ae Xeon\u00ae Processor E5-2690 v4 CPU, which is Broadwell according to [this page](https://ark.intel.com/content/www/us/en/ark/products/91770/intel-xeon-processor-e5-2690-v4-35m-cache-2-60-ghz.html). That would explain why I cannot run the test. Ideally the error message would be improved.\r\n> \r\n> Is it possible to run the test on Broadwell? If not I can use a google cloud instance.\r\n\r\n@reedwm See section on Hardware Limitations here - https://intel.github.io/mkl-dnn/dev_guide_data_types.html. MKL-DNN does not support bfloat16 in arch < Skylake. So, unfortunately, you will need Skylake/Cascade Lake processors to test this (GCP should have these.) Let me know if you cannot find one. In the worst case, I can test your changes on my system. ", "FYI, it looks like MKL cannot be compiled in debug mode. For example, when I run the command\r\n\r\n```\r\nbazel build  --test_output=errors --config=mkl --copt=-DENABLE_INTEL_MKL_BFLOAT16 -c dbg //tensorflow/core/kernels:mkl_fused_batch_norm_op\r\n```\r\n\r\nI get the error:\r\n\r\n```\r\nIn file included from ./tensorflow/core/lib/core/arena.h:21:0,\r\n                 from ./tensorflow/core/graph/graph.h:49,\r\n                 from ./tensorflow/core/graph/mkl_graph_util.h:22,\r\n                 from ./tensorflow/core/util/mkl_util.h:31,\r\n                 from tensorflow/core/kernels/mkl_fused_batch_norm_op.cc:22:\r\n./tensorflow/core/util/mkl_util.h: In member function 'bool tensorflow::MklDnnShape::CompareMklDnnLayouts(const mkldnn::memory::desc&, const mkldnn::memory::desc&) const':\r\n./tensorflow/core/util/mkl_util.h:341:32: error: no match for 'operator==' (operand types are 'mkldnn_primitive_kind_t' and 'mkldnn::primitive::kind')\r\n     assert(mdd1.primitive_kind == mkldnn::primitive::kind::memory);\r\n            ~~~~~~~~~~~~~~~~~~~~^~~~\r\n./tensorflow/core/util/mkl_util.h:341:32: note: candidate: operator==(mkldnn::primitive::kind, mkldnn::primitive::kind) <built-in>\r\n./tensorflow/core/util/mkl_util.h:341:32: note:   no known conversion for argument 1 from 'mkldnn_primitive_kind_t' to 'mkldnn::primitive::kind'\r\n./tensorflow/core/util/mkl_util.h:341:32: note: candidate: operator==(mkldnn_primitive_kind_t, mkldnn_primitive_kind_t) <built-in>\r\n./tensorflow/core/util/mkl_util.h:341:32: note:   no known conversion for argument 2 from 'mkldnn::primitive::kind' to 'mkldnn_primitive_kind_t'\r\n```\r\n\r\nAdding `--copt=-DNDEBUG` to bazel seems to fix this, but this is somewhat inconvenient.", "@reedwm I've temporarily commented assert that was causing the problem. I have tested both `dbg` and `opt` builds in my setup. Pls let me know if you still face any issues.", "@nhasabni I merged with the head of master (419ebe51e023e871590b19eb4df1c1fdbe9da51e) to resolve a different issue, but when I try to compile with MKL, I get the following error\r\n\r\n```\r\ntensorflow/core/kernels/mkl_conv_grad_input_ops.cc: In instantiation of 'tensorflow::TensorShape tensorflow::MklConvCustomBackpropInputOp<Device, T, is_depthwise, eager_mode>::MakeInputTfShape(tensorflow::OpKernelContext*, const tensorflow::Tensor&) [with Device = Eigen::ThreadPoolDevice; T = tensorflow::bfloat16; bool is_depthwise = true; bool eager_mode = false]':\r\ntensorflow/core/kernels/mkl_conv_grad_input_ops.cc:333:50:   required from 'void tensorflow::MklConvCustomBackpropInputOp<Device, T, is_depthwise, eager_mode>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = tensorflow::bfloat16; bool is_depthwise = true; bool eager_mode = false]'\r\ntensorflow/core/kernels/mkl_conv_grad_input_ops.cc:613:1:   required from here\r\ntensorflow/core/kernels/mkl_conv_grad_input_ops.cc:526:20: error: 'class tensorflow::MklConvCustomBackpropInputOp<Eigen::ThreadPoolDevice, tensorflow::bfloat16, true, false>' has no member named 'MakeShape'; did you mean 'MakeInputTfShape'?\r\n     CHECK_EQ(this->MakeShape(input_tensor, &input_tf_shape).ok(), true);\r\n              ~~~~~~^\r\n./tensorflow/core/platform/default/logging.h:399:64: note: in definition of macro 'CHECK_OP_LOG'\r\n                  ::tensorflow::internal::GetReferenceableValue(val1), \\\r\n                                                                ^~~~\r\n./tensorflow/core/platform/default/logging.h:407:30: note: in expansion of macro 'CHECK_OP'\r\n #define CHECK_EQ(val1, val2) CHECK_OP(Check_EQ, ==, val1, val2)\r\n                              ^~~~~~~~\r\ntensorflow/core/kernels/mkl_conv_grad_input_ops.cc:526:5: note: in expansion of macro 'CHECK_EQ'\r\n     CHECK_EQ(this->MakeShape(input_tensor, &input_tf_shape).ok(), true);\r\n     ^\r\n```\r\n\r\nThis occurs even on a clean branch without the changes of this PR applied. Can you resolve this issue? This might be caused by 271f6bb49d2140b4c1bca88391caedd1791561cf.", "Ah it looks like this is probably solved by #36395.", "When I built a bfloat16 graph explicitly, I get a \"No registered '_MklConv3DBackpropInputV2' OpKernel\" error. For example, when I build a pip package with:\r\n\r\n```\r\nbazel build  --test_output=errors --config=mkl --copt=-DENABLE_INTEL_MKL_BFLOAT16 -c opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nThen run the following code:\r\n\r\n```python\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n\r\nx = tf.ones([1, 1, 1, 1, 1], dtype='bfloat16')\r\nf = tf.ones([1, 1, 1, 1, 1], dtype='bfloat16')\r\ny = tf.nn.conv3d(x, f, strides=[1, 1, 1, 1, 1], padding='SAME')\r\ng = tf.gradients(y, x)\r\nwith tf.Session() as sess:\r\n  sess.run(g)\r\n```\r\n\r\nI get the following error:\r\n\r\n```\r\n2020-02-01 03:26:16.679048: E tensorflow/core/common_runtime/executor.cc:661] Executor failed to create kernel. Not found: No registered '_MklConv3DBackpropInputV2' OpKernel for 'XLA_CPU' devices compatible with node {{node gradients/Conv3D_grad/Conv3DBackpropInputV2}}\r\n\t.  Registered:  device='CPU'; label='MklLayoutDependentOp'; T in [DT_BFLOAT16]\r\n  device='CPU'; label='MklLayoutDependentOp'; T in [DT_FLOAT]\r\n\r\n\t [[gradients/Conv3D_grad/Conv3DBackpropInputV2]]\r\nTraceback (most recent call last):\r\n  File \"/home/reedwm/venvs/mkl_source/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1367, in _do_call\r\n    return fn(*args)\r\n  File \"/home/reedwm/venvs/mkl_source/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1352, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"/home/reedwm/venvs/mkl_source/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1445, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered '_MklConv3DBackpropInputV2' OpKernel for 'XLA_CPU' devices compatible with node {{node gradients/Conv3D_grad/Conv3DBackpropInputV2}}\r\n\t.  Registered:  device='CPU'; label='MklLayoutDependentOp'; T in [DT_BFLOAT16]\r\n  device='CPU'; label='MklLayoutDependentOp'; T in [DT_FLOAT]\r\n\r\n\t [[gradients/Conv3D_grad/Conv3DBackpropInputV2]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 9, in <module>\r\n    sess.run(g)\r\n  File \"/home/reedwm/venvs/mkl_source/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 960, in run\r\n    run_metadata_ptr)\r\n  File \"/home/reedwm/venvs/mkl_source/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1183, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/reedwm/venvs/mkl_source/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1361, in _do_run\r\n    run_metadata)\r\n  File \"/home/reedwm/venvs/mkl_source/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1386, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered '_MklConv3DBackpropInputV2' OpKernel for 'XLA_CPU' devices compatible with node node gradients/Conv3D_grad/Conv3DBackpropInputV2 (defined at test.py:7) \r\n\t.  Registered:  device='CPU'; label='MklLayoutDependentOp'; T in [DT_BFLOAT16]\r\n  device='CPU'; label='MklLayoutDependentOp'; T in [DT_FLOAT]\r\n\r\n\t [[gradients/Conv3D_grad/Conv3DBackpropInputV2]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node gradients/Conv3D_grad/Conv3DBackpropInputV2:\r\n ones_1 (defined at test.py:5)\r\n```\r\n\r\nI am explicitly using bfloat16 to debug a test from the grappler pass. It's hard to debug it effectively if I can not run graphs where I explicitly use bfloat16. Do you know what the issue is?", "@reedwm It looks like either 1) during `./configure` before building Tensorflow, compile for XLA is set to `true` --- can you try setting it to `false`, --- or 2) the graph pass is not working properly --  a node meant for `XLA_CPU` (`Conv3DBackpropInputV2`) should not be rewritten to `_MklConv3DBackpropInputV2`.\r\n\r\n`_MklConv3DBackpropInputV2` is supported for `CPU` device in `bfloat16` type, but not for `XLA_CPU`. [See here.](https://github.com/tensorflow/tensorflow/blob/191c028917f03e0c206ee2af9ea1aec80520cce7/tensorflow/core/kernels/mkl_conv_grad_input_ops.cc#L598-L610)", "When I try to compile with XLA disabled, I get the error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/reedwm/venvs/mkl_source_noxla/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1367, in _do_call\r\n    return fn(*args)\r\n  File \"/home/reedwm/venvs/mkl_source_noxla/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\r\n    self._extend_graph()\r\n  File \"/home/reedwm/venvs/mkl_source_noxla/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1390, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Conv3D' used by {{node Conv3D}} with these attrs: [dilations=[1, 1, 1, 1, 1], padding=\"SAME\", T=DT_BFLOAT16, data_format=\"NDHWC\", strides=[1, 1, 1, 1, 1]]\r\nRegistered devices: [CPU]\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_HALF]\r\n\r\n\t [[Conv3D]]\r\n```\r\n\r\nIn this case, it seems the Conv3D op is not properly being converted to the MKL version.", "> When I try to compile with XLA disabled, I get the error\r\n> \r\n> ```\r\n> Traceback (most recent call last):\r\n>   File \"/home/reedwm/venvs/mkl_source_noxla/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1367, in _do_call\r\n>     return fn(*args)\r\n>   File \"/home/reedwm/venvs/mkl_source_noxla/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\r\n>     self._extend_graph()\r\n>   File \"/home/reedwm/venvs/mkl_source_noxla/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1390, in _extend_graph\r\n>     tf_session.ExtendSession(self._session)\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Conv3D' used by {{node Conv3D}} with these attrs: [dilations=[1, 1, 1, 1, 1], padding=\"SAME\", T=DT_BFLOAT16, data_format=\"NDHWC\", strides=[1, 1, 1, 1, 1]]\r\n> Registered devices: [CPU]\r\n> Registered kernels:\r\n>   device='CPU'; T in [DT_DOUBLE]\r\n>   device='CPU'; T in [DT_FLOAT]\r\n>   device='CPU'; T in [DT_HALF]\r\n> \r\n> \t [[Conv3D]]\r\n> ```\r\n> \r\n> In this case, it seems the Conv3D op is not properly being converted to the MKL version.\r\n\r\nPushed a commit that should fix the problem. The issue is that Conv3D and its gradient ops do not have BFLOAT16 kernel registered for Eigen CPU backend. So when the graph passes execute, the placer pass (?) throws an error when it sees Conv3D doesn't have a kernel registered in BFLOAT16 type. The placer runs even before AutoBFloat16Converter runs, and that leads to the problem. \r\n\r\nCurrent mitigation is to register NoOp kernel for all of these operators in BFLOAT16 type that are not supported by Eigen CPU backend.", "Thank you for the fix, it worked great! If this is a good permanent fix, can you please make the fixes in a separate PR? That way this PR can be dedicated just to the grappler pass.\r\n\r\nAlso, when running the auto_mixed_precision tests, I am having trouble passing a test that runs conv3d. In particular, it looks like the gradients with respect to the filter are sometimes incorrect when bfloat16 is used with MKL. I created a stand-alone program to reproduce the conv3d issue:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow.compat.v1 as tf\r\ntf.compat.v1.disable_v2_behavior()\r\n\r\nnp.random.seed(0)\r\nx_np = np.random.normal(size=(2, 4, 4, 4, 1))\r\nf_np = np.ones((2, 1, 1, 1, 1))\r\nx = tf.Variable(x_np, dtype='float32')\r\nf = tf.Variable(f_np, dtype='float32')\r\n\r\ndef compute_grad(x, f):\r\n  y = tf.nn.conv3d(x, f, strides=[1, 1, 1, 1, 1], padding='SAME')\r\n  (g,) = tf.gradients(y, [f])\r\n  g = tf.cast(g, 'float32')\r\n  with tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    return sess.run(g)\r\n\r\ngrad = compute_grad(x, f)\r\ngrad16 = compute_grad(tf.cast(x, 'bfloat16'), tf.cast(f, 'bfloat16'))\r\nprint('f32 grad: %s' % np.reshape(grad, (2,)))\r\nprint('bf16 grad: %s' % np.reshape(grad16, (2,)))\r\ntf.test.TestCase().assertAllClose(grad, grad16, atol=1e-1, rtol=1e-1)\r\n```\r\n\r\nWith MKL, the output is typically \r\n```\r\nf32 grad: [ 17.3849678   10.21814537]\r\nbf16 grad: [ 17.375   6.75 ]\r\n``` \r\nThen it crashes due to the failed assertions. It occasionally passes with MKL, so the gradient op does not appear to be deterministic. I tried reproducing with smaller shapes, but it would pass if I decreased the shape size.\r\n\r\nThis passes if TensorFlow is built without MKL. In that case, I think XLA will automatically run the op, since there is no non-XLA non-MKL CPU bfloat16 version of conv3d.", "Thanks for testing my changes. Yes, I can add them into separate PR.\r\n\r\nAbout Conv3D unit test - it could be a bug in MKL-DNN. I can triage it and file an issue against it. Are you blocked by it? Just asking because it may take a while for MKL-DNN team to fix it.", "I am not blocked, as I can work around it for now by disabling certain tests.", "Do you think you can address this before I continue working on the PR? Most of the Python auto_mixed_precision tests sometimes fail when MKL is enabled. What makes it especially difficult is that sometimes the test only fail sometimes as the issue is nondeterministic, so it can be difficult to tell if the test is buggy or not.\r\n\r\nI also am worried that users may end up affected by this issue when they enable the grappler pass.\r\n\r\nIf fixing this is difficult, and you haven't observed this affecting model quality in practice, I can continue working on the PR. ", "I will looking into it soon (hopefully tomorrow). But the fix for the test could take a while - I will need to reproduce the issue with MKL-DNN library, and if it is a bug, then they would have to fix it. If it is an issue with TF-MKL backend, then I can fix it quickly.\r\n\r\nDo you see this issue in only Conv3D? or some other ops are also failing? We have not been testing bfloat16 support in MKL backend on models using Conv3D, so it is difficult to tell. But we have tested bfloat16 support in MKL backend on standard models such as ResNet50. \r\n\r\nIf Conv3D is the only problematic op, then can we temporarily disable that test for MKL backend, and continue on the PR?", "Thank you for looking into this!\r\n\r\nI tried to get a reproducer for Conv2D, but in doing so I ran into another bug. When I run a Conv2d and BatchNorm in float32, then I change the Conv2d to bfloat16 and run it again, I sometimes get a crash. For example, the following will crash with the error message  \"free(): invalid next size (fast)\".\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n\r\ndef build_model(bfloat16_conv):\r\n  x = tf.Variable(tf.truncated_normal([1, 1, 1, 1]))\r\n  f = tf.Variable(tf.truncated_normal([1, 1, 1, 2]))\r\n  if bfloat16_conv:\r\n    x = tf.cast(x, 'bfloat16')\r\n    f = tf.cast(f, 'bfloat16')\r\n  x = tf.nn.conv2d(x, f, strides=[1, 1, 1, 1], padding='SAME')\r\n  if bfloat16_conv:\r\n    x = tf.cast(x, 'float32')\r\n  s = tf.Variable(tf.truncated_normal([2]))\r\n  o = tf.Variable(tf.truncated_normal([2]))\r\n  y, _, _ = tf.nn.fused_batch_norm(x, scale=s, offset=o)\r\n  return y\r\n\r\noutput32 = build_model(bfloat16_conv=False)\r\noutput16 = build_model(bfloat16_conv=True)\r\n\r\nwith tf.Session() as sess:\r\n  print('Computing with float32')\r\n  sess.run(tf.global_variables_initializer())\r\n  sess.run(output32)\r\n\r\n  for _ in range(20):\r\n    print('Computing with bfloat16')\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(output16)\r\n```\r\nInterestingly, the issue occurs even if I replace `bfloat16` with `float16` or `float64`, indicating the issue does not involve bfloat16 itself. However, the issue only occurs when I compile with MKL.\r\n\r\nThis issue is affecting the unit tests, but I can work around it for now.", "I found another issue: MKL bfloat16 ops crash in eager mode. For example, the following results in a segmentation fault:\r\n\r\n```python\r\nimport tensorflow as tf\r\ni = tf.ones([2, 8, 8, 1], dtype='bfloat16')\r\nf = tf.ones([3, 3, 1, 6], dtype='bfloat16')\r\nprint('Doing conv2d:')\r\nx = tf.nn.conv2d(i, f, strides=[1, 1, 1, 1], padding='SAME')\r\n```\r\n\r\nThis does not affect the grappler pass, which only runs in graph mode. However, it will prevent bfloat16 from being used with the `tf.keras.mixed_precision` API. It also makes it harder to debug mixed precision issues. Can you please look into this?", "@nhasabni Can you please check reedwm's comments and keep us posted? Thanks!", "Hi @reedwm, sorry I was not able to update earlier. I will work on this as soon as you can (most likely next week).", "> Thank you for looking into this!\r\n> \r\n> I tried to get a reproducer for Conv2D, but in doing so I ran into another bug. When I run a Conv2d and BatchNorm in float32, then I change the Conv2d to bfloat16 and run it again, I sometimes get a crash. For example, the following will crash with the error message \"free(): invalid next size (fast)\".\r\n> \r\n> ```\r\n> from __future__ import absolute_import\r\n> from __future__ import division\r\n> from __future__ import print_function\r\n> \r\n> import tensorflow.compat.v1 as tf\r\n> tf.disable_v2_behavior()\r\n> \r\n> def build_model(bfloat16_conv):\r\n>   x = tf.Variable(tf.truncated_normal([1, 1, 1, 1]))\r\n>   f = tf.Variable(tf.truncated_normal([1, 1, 1, 2]))\r\n>   if bfloat16_conv:\r\n>     x = tf.cast(x, 'bfloat16')\r\n>     f = tf.cast(f, 'bfloat16')\r\n>   x = tf.nn.conv2d(x, f, strides=[1, 1, 1, 1], padding='SAME')\r\n>   if bfloat16_conv:\r\n>     x = tf.cast(x, 'float32')\r\n>   s = tf.Variable(tf.truncated_normal([2]))\r\n>   o = tf.Variable(tf.truncated_normal([2]))\r\n>   y, _, _ = tf.nn.fused_batch_norm(x, scale=s, offset=o)\r\n>   return y\r\n> \r\n> output32 = build_model(bfloat16_conv=False)\r\n> output16 = build_model(bfloat16_conv=True)\r\n> \r\n> with tf.Session() as sess:\r\n>   print('Computing with float32')\r\n>   sess.run(tf.global_variables_initializer())\r\n>   sess.run(output32)\r\n> \r\n>   for _ in range(20):\r\n>     print('Computing with bfloat16')\r\n>     sess.run(tf.global_variables_initializer())\r\n>     sess.run(output16)\r\n> ```\r\n> \r\n> Interestingly, the issue occurs even if I replace `bfloat16` with `float16` or `float64`, indicating the issue does not involve bfloat16 itself. However, the issue only occurs when I compile with MKL.\r\n> \r\n> This issue is affecting the unit tests, but I can work around it for now.\r\n\r\nHi Reed, finally got some time to work on this. Above example works for me without any error. Even Conv3D example is working fine with latest master branch compiled for MKL backend. Note that MKLDNN version that we are using now has gone up to 1.2, and that upgrade could have fixed these issues.", "I am looking to failure in eager test case that you shared. But all other examples are working with latest master that I merged with this branch. Pls let me know. I will update about Eager test case failure soon.", "Thanks for the update. I will start working on this again. Eager is not blocking me but would be nice to have fixed.", "I've fixed issue leading to failure in Eager-mode test case also. The problem was BFloat16 support for Eager mode in MKL backend is not there yet. I've temporarily added it. The test case now passes. Pls check. We will send a separate and complete PR to enable BFloat16 support in eager mode for MKL backend. Thanks.", "@reedwm Can you pls provide any update on this feature if you have any? Would it be possible for you to provide your changes to us so that we can test it internally first before pushing it to master? Thanks.", "@nhasabni Could you please resolve the conflicts? Thanks!", "@gbaned I have resolved the conflict. Thanks.", "Actually you don't ever have to resolve conflicts, since I am working on a version of this which I'll push up soon.", "@reedwm Sure, I just resolved it to remove 'awaiting response' label. :) BTW, would you be able to share your implementation with us so that we can also have a glance? Thanks.", "Just pushing some fixes for conversion problems noticed so far.", "Sorry for the delay. I just pushed my implementation [on this branch](https://github.com/reedwm/tensorflow/tree/auto_mp_mkl). See reedwm/tensorflow@0c6eb989ecca8bb7263f42c22e2a1960f75068c3 for the diff. With this implementation, the same algorithm is used for the CUDA and MKL versions.\r\n\r\nOne notable difference between this implementation and this PR is that this implementation does not check if an op is an MKL op by calling `IsMklOp` or checking if the op is a special fused MKL op. Instead, only MKL ops should be added to the 4 lists in auto_mixed_precision_lists.h. Let me know if you think I should use `IsMklOp`. I'm not quite sure what this function does or how to use it.\r\n\r\nI had to disable some tests due to bfloat16 errors, however. For example, the `test_conv_bn_dropout` gives the error:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not create a primitive descriptor iterator, in file tensorflow/core/kernels/mkl_fused_batch_norm_op.cc:1271\r\n\t [[{{node gradients/FusedBatchNormV3_1_grad/FusedBatchNormGradV3}}]]\r\n```\r\n\r\nI created a standalone program to reproduce the error.\r\n\r\n```python\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n\r\ndef build_model(bfloat16_conv):\r\n  x = tf.Variable(tf.truncated_normal([1, 1, 1, 1]))\r\n  f = tf.Variable(tf.truncated_normal([1, 1, 1, 2]))\r\n  if bfloat16_conv:\r\n    x = tf.cast(x, 'bfloat16')\r\n    f = tf.cast(f, 'bfloat16')\r\n  x = tf.nn.conv2d(x, f, strides=[1, 1, 1, 1], padding='SAME')\r\n  s = tf.Variable(tf.truncated_normal([2]))\r\n  o = tf.Variable(tf.truncated_normal([2]))\r\n  y, _, _ = tf.nn.fused_batch_norm(x, scale=s, offset=o)\r\n  if bfloat16_conv:\r\n    y = tf.cast(y, 'float32')\r\n  g = tf.gradients(y, [x])\r\n  return g\r\n\r\noutput32 = build_model(bfloat16_conv=False)\r\noutput16 = build_model(bfloat16_conv=True)\r\n\r\nwith tf.Session() as sess:\r\n  print('Computing with float32')\r\n  sess.run(tf.global_variables_initializer())\r\n  sess.run(output32)\r\n\r\n  print('Computing with bfloat16')\r\n  sess.run(tf.global_variables_initializer())\r\n  sess.run(output16)\r\n```\r\n\r\nCould you take a look?\r\n\r\nOnce this issue is resolved, I can either create a new PR with the new implementation, or push the implementation as a commit to this PR. In either case, I can respond to comments from the reviewers.", "> Sorry for the delay. I just pushed my implementation [on this branch](https://github.com/reedwm/tensorflow/tree/auto_mp_mkl). See [reedwm/tensorflow@0c6eb98](https://github.com/reedwm/tensorflow/commit/0c6eb989ecca8bb7263f42c22e2a1960f75068c3) for the diff. With this implementation, the same algorithm is used for the CUDA and MKL versions.\r\n> \r\n> One notable difference between this implementation and this PR is that this implementation does not check if an op is an MKL op by calling `IsMklOp` or checking if the op is a special fused MKL op. Instead, only MKL ops should be added to the 4 lists in auto_mixed_precision_lists.h. Let me know if you think I should use `IsMklOp`. I'm not quite sure what this function does or how to use it.\r\n> \r\n> I had to disable some tests due to bfloat16 errors, however. For example, the `test_conv_bn_dropout` gives the error:\r\n> \r\n> ```\r\n> tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not create a primitive descriptor iterator, in file tensorflow/core/kernels/mkl_fused_batch_norm_op.cc:1271\r\n> \t [[{{node gradients/FusedBatchNormV3_1_grad/FusedBatchNormGradV3}}]]\r\n> ```\r\n> \r\n> I created a standalone program to reproduce the error.\r\n> \r\n> ```python\r\n> import tensorflow.compat.v1 as tf\r\n> tf.disable_v2_behavior()\r\n> \r\n> def build_model(bfloat16_conv):\r\n>   x = tf.Variable(tf.truncated_normal([1, 1, 1, 1]))\r\n>   f = tf.Variable(tf.truncated_normal([1, 1, 1, 2]))\r\n>   if bfloat16_conv:\r\n>     x = tf.cast(x, 'bfloat16')\r\n>     f = tf.cast(f, 'bfloat16')\r\n>   x = tf.nn.conv2d(x, f, strides=[1, 1, 1, 1], padding='SAME')\r\n>   s = tf.Variable(tf.truncated_normal([2]))\r\n>   o = tf.Variable(tf.truncated_normal([2]))\r\n>   y, _, _ = tf.nn.fused_batch_norm(x, scale=s, offset=o)\r\n>   if bfloat16_conv:\r\n>     y = tf.cast(y, 'float32')\r\n>   g = tf.gradients(y, [x])\r\n>   return g\r\n> \r\n> output32 = build_model(bfloat16_conv=False)\r\n> output16 = build_model(bfloat16_conv=True)\r\n> \r\n> with tf.Session() as sess:\r\n>   print('Computing with float32')\r\n>   sess.run(tf.global_variables_initializer())\r\n>   sess.run(output32)\r\n> \r\n>   print('Computing with bfloat16')\r\n>   sess.run(tf.global_variables_initializer())\r\n>   sess.run(output16)\r\n> ```\r\n> \r\n> Could you take a look?\r\n> \r\n> Once this issue is resolved, I can either create a new PR with the new implementation, or push the implementation as a commit to this PR. In either case, I can respond to comments from the reviewers.\r\n\r\nThanks for the update. \r\n\r\nIsMklOp ([defined here](https://github.com/tensorflow/tensorflow/blob/9664ba19296e58f4437feab4d4b2789cc1e38fd4/tensorflow/core/graph/mkl_graph_util.h#L210)) is a simple function that determines if an op (specified as a name) has a corresponding Mkl op defined. Ops that are supported by MKL backend have _Mkl as prefix to their name. And we rely on a kernel registration and labels in the kernel registration to ensure that the op is indeed Mkl op.\r\n\r\nI think using IsMklOp API is an extendible way to adding new Mkl ops to auto_mixed_precision pass than to rely on white/black lists. AutoBF16Converter policy is very simple -- if an op is Mkl op, then it should be in the white list, except for ops in the black list. And we are perfectly fine to enumerate blacklisted ops (since this list is very small.) So we would prefer to use IsMklOp API. Let me know if you need any further details about using it.\r\n\r\nFor BatchNorm issue - I am looking into it. I've recently seen this issue internally also.", "> I think using IsMklOp API is an extendible way to adding new Mkl ops to auto_mixed_precision pass than to rely on white/black lists.\r\n\r\nI implemented a prototype where we add bfloat16-supported ops to the graylist [here](https://github.com/reedwm/tensorflow/commit/cb6f4728f0338c68dc9628adc7e13c07742ce2a0). The change will add an op to the graylist if either `IsMklOp` returns true or if the op kernel supports bfloat16. I implemented this by copying code from this PR. The change can easily be modified to instead add the ops to the whitelist, which would unconditionally convert them to bfloat16 unless they are on the gray/black/clearlist.\r\n\r\nHowever, I am nervous about this change as it seems dangerous. Someone in the future may add a bfloat16 kernel to an op, which would then change the grappler pass to change that op to bfloat16, potentially breaking models. For example, suppose L2Loss originally only supported float32, so we wouldn't bother adding it to the blacklist. If someone then added bfloat16 support, it would cause the grappler pass to convert every L2Loss to bfloat16, breaking many models. If we instead stick with the original plan of hardcoded white/gray/black/clearlists, this issue does not occur.\r\n\r\nAlso, there may be ops we haven't considered which should be on the blacklist/graylist/clearlist, so defaulting them to the whitelist is dangerous. For example, IdentityN is not on any list, but putting it on the whitelist is probably a mistake, as it should probably be in float32 if it follows an L2Loss op. I can add IdentityN to the graylist manually, but there will be likely be other ops that should be in float32 that neither of us have considered. With the original plan of hardcoded lists, an op will be float32 if its not on any list (which is basically equivalent to being on the blacklist).\r\n\r\nAdding bfloat16-supported ops to the graylist instead of the whitelist is a lot better but still has these problems to an extent.\r\n\r\nWhat are your thoughts? I think we should stick with the plan of hardcoded lists.\r\n\r\n> For BatchNorm issue - I am looking into it. I've recently seen this issue internally also.\r\n\r\nThanks for looking into this!", "I definitely think a whitelist is safer here. Can we just create the\nwhitelist by listing all the ops for which we have kernels today?\n\nOn Fri, May 1, 2020 at 6:09 PM Reed <notifications@github.com> wrote:\n\n> I think using IsMklOp API is an extendible way to adding new Mkl ops to\n> auto_mixed_precision pass than to rely on white/black lists.\n>\n> I implemented a prototype where we add bfloat16-supported ops to the\n> graylist here\n> <https://github.com/reedwm/tensorflow/commit/cb6f4728f0338c68dc9628adc7e13c07742ce2a0>.\n> The change will add an op to the graylist if either IsMklOp returns true\n> or if the op kernel supports bfloat16. I implemented this by copying code\n> from this PR. The change can easily be modified to instead add the ops to\n> the whitelist, which would unconditionally convert them to bfloat16 unless\n> they are on the gray/black/clearlist.\n>\n> However, I am nervous about this change as it seems dangerous. Someone in\n> the future may add a bfloat16 kernel to an op, which would then change the\n> grappler pass to change that op to bfloat16, potentially breaking models.\n> For example, suppose L2Loss originally only supported float32, so we\n> wouldn't bother adding it to the blacklist. If someone then added bfloat16\n> support, it would cause the grappler pass to convert every L2Loss to\n> bfloat16, breaking many models. If we instead stick with the original plan\n> of hardcoded white/gray/black/clearlists, this issue does not occur.\n>\n> Also, there may be ops we haven't considered which should be on the\n> blacklist/graylist/clearlist, so defaulting them to the whitelist is\n> dangerous. For example, IdentityN is not on any list, but putting it on the\n> whitelist is probably a mistake, as it should probably be in float32 if it\n> follows an L2Loss op. I can add IdentityN to the graylist manually, but\n> there will be likely be other ops that should be in float32 that neither of\n> us have considered. With the original plan of hardcoded lists, an op will\n> be float32 if its not on any list (which is basically equivalent to being\n> on the blacklist).\n>\n> Adding bfloat16-supported ops to the graylist instead of the whitelist is\n> a lot better but still has these problems to an extent.\n>\n> What are your thoughts? I think we should stick with the plan of hardcoded\n> lists.\n>\n> For BatchNorm issue - I am looking into it. I've recently seen this issue\n> internally also.\n>\n> Thanks for looking into this!\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/34504#issuecomment-622646830>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRJMMMAVYHHCAIWN6L3RPNXFFANCNFSM4JQKA4RQ>\n> .\n>\n\n\n-- \n - Alex\n", "> > I think using IsMklOp API is an extendible way to adding new Mkl ops to auto_mixed_precision pass than to rely on white/black lists.\r\n> \r\n> I implemented a prototype where we add bfloat16-supported ops to the graylist [here](https://github.com/reedwm/tensorflow/commit/cb6f4728f0338c68dc9628adc7e13c07742ce2a0). The change will add an op to the graylist if either `IsMklOp` returns true or if the op kernel supports bfloat16. I implemented this by copying code from this PR. The change can easily be modified to instead add the ops to the whitelist, which would unconditionally convert them to bfloat16 unless they are on the gray/black/clearlist.\r\n> \r\n> However, I am nervous about this change as it seems dangerous. Someone in the future may add a bfloat16 kernel to an op, which would then change the grappler pass to change that op to bfloat16, potentially breaking models. For example, suppose L2Loss originally only supported float32, so we wouldn't bother adding it to the blacklist. If someone then added bfloat16 support, it would cause the grappler pass to convert every L2Loss to bfloat16, breaking many models. If we instead stick with the original plan of hardcoded white/gray/black/clearlists, this issue does not occur.\r\n> \r\n> Also, there may be ops we haven't considered which should be on the blacklist/graylist/clearlist, so defaulting them to the whitelist is dangerous. For example, IdentityN is not on any list, but putting it on the whitelist is probably a mistake, as it should probably be in float32 if it follows an L2Loss op. I can add IdentityN to the graylist manually, but there will be likely be other ops that should be in float32 that neither of us have considered. With the original plan of hardcoded lists, an op will be float32 if its not on any list (which is basically equivalent to being on the blacklist).\r\n> \r\n> Adding bfloat16-supported ops to the graylist instead of the whitelist is a lot better but still has these problems to an extent.\r\n> \r\n> What are your thoughts? I think we should stick with the plan of hardcoded lists.\r\n> \r\n> > For BatchNorm issue - I am looking into it. I've recently seen this issue internally also.\r\n> \r\n> Thanks for looking into this!\r\n\r\nIMO, creating lists (white/grey/black) to control bfloat16 rewrite in grappler pass could create confusion: on one hand, it means that we do not trust kernel writer/developer by assuming that they could register L2Loss for bfloat16 type by mistake, while on other hand, it expects them to register their \"correct\" bfloat16 op (Conv2D for example) with the grappler pass (by adding it to the white list). The former case could lead to convergence/accuracy issues, while later could lead to leaving some performance on the table. If we want to ensure the accuracy of rewritten models then explicit enumeration of ops seems right. It is just that it may not be the highly-performant model though. Given those choices, I am okay with explicit enumeration.\r\n", "> IMO, creating lists (white/grey/black) to control bfloat16 rewrite in grappler pass could create confusion: on one hand, it means that we do not trust kernel writer/developer by assuming that they could register L2Loss for bfloat16 type by mistake, while on other hand, it expects them to register their \"correct\" bfloat16 op (Conv2D for example) with the grappler pass (by adding it to the white list). The former case could lead to convergence/accuracy issues, while later could lead to leaving some performance on the table. If we want to ensure the accuracy of rewritten models then explicit enumeration of ops seems right. It is just that it may not be the highly-performant model though. Given those choices, I am okay with explicit enumeration.\r\n\r\nIt's more important to ensure accuracy than it is to ensure maximal performance, so explicit enumeration is more important. In practice, this has worked well with float16 on CUDA GPUs and has not caused major performance issues. So we should stick with explicit enumeration here as well. If you find any concrete performance issues on a model with this approach, we can always add more ops to the whitelist.\r\n\r\nAlso consider the [`tf.keras.mixed_precision` API](https://www.tensorflow.org/guide/keras/mixed_precision), which we recommend over the auto_mixed_precision grappler pass. This API does not have a whitelist/blacklist/graylist/clearlist but instead has Keras layers cast their inputs to bfloat16 unconditionally, while keeping variables, regularizers, and certain parts of losses in float32.\r\n\r\n> Can we just create the whitelist by listing all the ops for which we have kernels today?\r\n\r\nYes, this would work. [These are the MKL bfloat16 ops](https://gist.github.com/reedwm/97562e1c86211c70123cfcb357525ffb). @nhasabni, if you want, I can write a script to find all ops with a bfloat16 kernel (more specifically, all op kernels which have an input or output that can be bfloat16). I think the current whitelist, which I copied and tweaked from the CUDA float16 version, is fine though.\r\n\r\nIn any case, let me know when you resolved the batch norm issue. If its hard to fix, we can alternatively try to detect when the issue occurs and keep certain ops in float32, but then we need a reliable way of detecting when the issue would occur.", "> > IMO, creating lists (white/grey/black) to control bfloat16 rewrite in grappler pass could create confusion: on one hand, it means that we do not trust kernel writer/developer by assuming that they could register L2Loss for bfloat16 type by mistake, while on other hand, it expects them to register their \"correct\" bfloat16 op (Conv2D for example) with the grappler pass (by adding it to the white list). The former case could lead to convergence/accuracy issues, while later could lead to leaving some performance on the table. If we want to ensure the accuracy of rewritten models then explicit enumeration of ops seems right. It is just that it may not be the highly-performant model though. Given those choices, I am okay with explicit enumeration.\r\n> \r\n> It's more important to ensure accuracy than it is to ensure maximal performance, so explicit enumeration is more important. In practice, this has worked well with float16 on CUDA GPUs and has not caused major performance issues. So we should stick with explicit enumeration here as well. If you find any concrete performance issues on a model with this approach, we can always add more ops to the whitelist.\r\n> \r\n> Also consider the [`tf.keras.mixed_precision` API](https://www.tensorflow.org/guide/keras/mixed_precision), which we recommend over the auto_mixed_precision grappler pass. This API does not have a whitelist/blacklist/graylist/clearlist but instead has Keras layers cast their inputs to bfloat16 unconditionally, while keeping variables, regularizers, and certain parts of losses in float32.\r\n> \r\n> > Can we just create the whitelist by listing all the ops for which we have kernels today?\r\n> \r\n> Yes, this would work. [These are the MKL bfloat16 ops](https://gist.github.com/reedwm/97562e1c86211c70123cfcb357525ffb). @nhasabni, if you want, I can write a script to find all ops with a bfloat16 kernel (more specifically, all op kernels which have an input or output that can be bfloat16). I think the current whitelist, which I copied and tweaked from the CUDA float16 version, is fine though.\r\n> \r\n> In any case, let me know when you resolved the batch norm issue. If its hard to fix, we can alternatively try to detect when the issue occurs and keep certain ops in float32, but then we need a reliable way of detecting when the issue would occur.\r\n\r\nOK, sure, I think it is better to create a script to detect ops supporting bfloat16 type in MKL backend. This list could be different than CUDA float16 list. If you have pushed this list to the PR, I can check it.\r\n\r\nFor BatchNorm issue, the fix is known, MKL-DNN team is working on releasing it for DNNL-v1.4.\r\nBelow is the change that fixes the problem, if you would like to try it now. You would need to\r\nfind out location of `mkl_dnn_v1` in your Bazel output_user_root to apply the patch.\r\n\r\n```\r\ndiff --git a/src/cpu/ref_batch_normalization.hpp b/src/cpu/ref_batch_normalization.hpp\r\nindex 7042280..6be6701 100644\r\n--- a/src/cpu/ref_batch_normalization.hpp\r\n+++ b/src/cpu/ref_batch_normalization.hpp\r\n@@ -91,7 +91,7 @@ struct ref_batch_normalization_bwd_t : public primitive_t {\r\n                             diff_src_md()->data_type)\r\n                     && platform::has_data_type_support(d_type)\r\n                     && IMPLICATION(use_scaleshift(),\r\n-                            utils::everyone_is(d_type, weights_md()->data_type,\r\n+                            utils::everyone_is(f32, weights_md()->data_type,\r\n                                     diff_weights_md()->data_type))\r\n                     && attr()->has_default_values();\r\n             if (!ok) return status::unimplemented;\r\n```", "Sorry I did not see your link to the list of BFloat16 MKL ops. The list looks good to me, except that Softmax should be in black list.", "@reedwm  Any update on this PR? Please. Thanks!\r\n", "@reedwm We tested your branch/commit for the feature in our internal tests, and it looks to be passing on all the tests that we have. We are fine if you would like to merge the PR with the master. Just one comment though - can you guard the feature with ENABLE_INTEL_MKL_BFLOAT16 macro? Currently, we have guarded bfloat16 code path in MKL backend, and can be conditionally enabled by passing this compile time flag. Thanks for all the efforts!", "@reedwm Any update on this PR? Please. Thanks!", "@reedwm I merged my [auto_mp_mkl](https://github.com/reedwm/tensorflow/tree/auto_mp_mkl) with master and added a feature guard for ENABLE_INTEL_MKL_BFLOAT16.\r\n\r\nI think it's ready to merge except for the fact tests `test_conv_bn_dropout` and `test_conv_pool` still don't pass with MKL. I tried apply your patch. `test_conv_bn_dropout` now fails on the `assertAllClose` call, and still fails even if I increase the tolerance to 0.1. `test_conv_pool` still segfaults.\r\n\r\nIf you know which situations in MKL cause the tests to fail, we can modify the grappler pass to keep certain ops in float32 if they do not work properly in bfloat16.", "Thanks @reedwm for the work! I will check the failing tests -- MKLDNN version has changed recently, so it should have the fix.", "@nhasabni, any update on the failing tests?\r\n\r\nAs an alternative to fixing this in MKL or TensorFlow, if we know exactly when the issues causing the test failures occur, we can keep certain ops in float32 to prevent this from occurring. I'm unsure what is causing the test failures, but if you know, let me know.\r\n\r\nNote it's unlikely to get this change into TF 2.3 since it is being cut June 22.", "@reedwm I missed these failing tests somehow. Sorry my bad. How can I run these tests? I can check if they still fail.", "The tests can be run with:\r\n\r\n```\r\nbazel test --flaky_test_attempts=1 --test_output=errors --config=mkl --copt=-DENABLE_INTEL_MKL_BFLOAT16 -c opt //tensorflow/python:auto_mixed_precision_test //tensorflow/core/grappler/optimizers:auto_mixed_precision_test\r\n```\r\n\r\nThe error occurs in the `//tensorflow/python:auto_mixed_precision_test` test.", "Thanks @reedwm. Looking into this. Will update soon (by EOD in worst case).", "> Thanks @reedwm. Looking into this. Will update soon (by EOD in worst case).\r\n\r\n@reedwm I see that both the tests **pass** (**ran 10 times**) with `auto_tp_mkl` branch on my SkyLake (Intel(R) Xeon(R) Platinum 8180) machine. MKL-DNN does not support BFloat16 on non-AVX512 platforms. It looks like your platform is not AVX512 enabled?\r\n\r\n```\r\n//tensorflow/python:auto_mixed_precision_test                   PASSED in 7.0s\r\n//tensorflow/core/grappler/optimizers:auto_mixed_precision_test          PASSED in 0.4s\r\n```\r\n\r\nHere is latest commit on that branch:\r\n\r\n```\r\ncommit 60086a217a66f0bc934529939c9877e5b560ff86 (HEAD -> auto_mp_mkl, origin/auto_mp_mkl)\r\nAuthor: Reed <reedwm@google.com>\r\nDate:   Thu Jun 4 16:34:41 2020 -0700\r\n\r\n    Minor fixes\r\n```", "Try uncommenting [this line](https://github.com/reedwm/tensorflow/blob/60086a217a66f0bc934529939c9877e5b560ff86/tensorflow/python/grappler/auto_mixed_precision_test.py#L510) and commenting out the line after it, to run with MKL.  And same with [this line](https://github.com/reedwm/tensorflow/blob/60086a217a66f0bc934529939c9877e5b560ff86/tensorflow/python/grappler/auto_mixed_precision_test.py#L545). I forgot I had disabled these tests with MKL so that I could check the other tests pass", "> Try uncommenting [this line](https://github.com/reedwm/tensorflow/blob/60086a217a66f0bc934529939c9877e5b560ff86/tensorflow/python/grappler/auto_mixed_precision_test.py#L510) and commenting out the line after it, to run with MKL. And same with [this line](https://github.com/reedwm/tensorflow/blob/60086a217a66f0bc934529939c9877e5b560ff86/tensorflow/python/grappler/auto_mixed_precision_test.py#L545). I forgot I had disabled these tests with MKL so that I could check the other tests pass\r\n\r\nOK, I see the failure now. It is a different problem than previous BatchNorm issue (that I specified patch for). Looking into it.", "@nhasabni There are conflicts. Could you please rebase?\r\n@reedwm How much more work is needed for this PR?", "Is this replaced by https://github.com/tensorflow/tensorflow/pull/40596 ? The file sets are a bit different so I'm not sure. ", "> Is this replaced by #40596 ? The file sets are a bit different so I'm not sure.\r\n\r\nYes, #40596 replaces this PR. So we can close this.", "@nhasabni Thank you for the quick reply! Closing the PR now then.\r\n\r\nReason for closing: This PR has been replaced by #40596 (Merged)."]}, {"number": 34503, "title": "[TF2.0] Enable nested tf.custom_gradient", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Python 3.7\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nImplementing a custom op in python with a custom second-order derivative requires a lot of redundant code because tf.custom_gradient functions can not be nested. The following code will not use the defined second-order gradient:\r\n```\r\n@tf.custom_gradient\r\ndef op(x):\r\n  @tf.custom_gradient\r\n  def grad(dy):\r\n    def grad2(dx, ddy):\r\n      print('This will not be executed')\r\n      return 1, 1 # I return a second-order gradient wrt to x and a gradient wrt to dy\r\n    return dy*5, grad2\r\n  return x*5, grad\r\n\r\nx = tf.random.normal((1,))\r\nwith tf.GradientTape() as tape:\r\n  tape.watch(x)\r\n  with tf.GradientTape() as tape2:\r\n    tape2.watch(x)\r\n    y = op(x)\r\n  dydx = tape2.gradient(y, x)\r\nd2ydx2 = tape.gradient(dydx, x)\r\n```\r\nd2ydx2 is None in this example but I expect it to be 1.\r\n\r\nA workaround is the following:\r\n```\r\n@tf.custom_gradient\r\ndef grad(x, dy):\r\n  def grad2(dx2):\r\n    print('This is executed')\r\n    return 1, 2 # the first gradient is wrt to x, the second one wrt to dy\r\n  return 5, grad2\r\n\r\n@tf.custom_gradient\r\ndef op(x):\r\n  return x*5, lambda dy: grad(x, dy)\r\n\r\nx = tf.random.normal((1,))\r\nwith tf.GradientTape() as tape:\r\n  tape.watch(x)\r\n  with tf.GradientTape() as tape2:\r\n    tape2.watch(x)\r\n    y = op(x)\r\n  dydx = tape2.gradient(y, x)\r\nd2ydx2 = tape.gradient(dydx, x)\r\nprint(d2ydx2)\r\n```\r\nIn this case d2ydx2 is 1.\r\n\r\n**Will this change the current api? How?**\r\nIt should not have a direct impact on the API. The behavior of tf.custom_gradient has to include the return values of the parent function. The higher-order derivatives should be computed wrt to every parameter of all parent functions.\r\n\r\n**Who will benefit with this feature?**\r\nPeople who need to implement custom higher-order derivatives either because of performance or because of numerical stability issues.", "comments": ["In case someone runs into the same issue, there is an example given in the [custom_gradient.py](https://github.com/tensorflow/tensorflow/blob/b6d720fd9303d2841ddbb34536653eb1ec3b3dd9/tensorflow/python/ops/custom_gradient.py#L128) file which is not available on the online documentation."]}, {"number": 34502, "title": "Concat within tf.range loop throwing error when converting to tflite: StatelessWhile custom implementation needed", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.14.6\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (or github SHA if from source): '2.0.0-dev20191002'\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nConverterError: See console for info.\r\n2019-11-21 19:25:54.528881: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: dlopen(libhdfs.dylib, 6): image not found\r\n2019-11-21 19:25:56.545440: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-21 19:25:56.559290: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f9d67546760 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-21 19:25:56.559306: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-11-21 19:25:56.568668: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: StatelessWhile\r\n2019-11-21 19:25:56.568994: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 4 operators, 19 arrays (0 quantized)\r\n2019-11-21 19:25:56.569203: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 4 operators, 19 arrays (0 quantized)\r\n2019-11-21 19:25:56.569378: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 3 operators, 18 arrays (0 quantized)\r\n2019-11-21 19:25:56.569450: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 3 operators, 18 arrays (0 quantized)\r\n2019-11-21 19:25:56.569489: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 3 operators, 18 arrays (0 quantized)\r\n2019-11-21 19:25:56.569553: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 6769536 bytes, theoretical optimal value: 6769536 bytes.\r\n2019-11-21 19:25:56.569584: I tensorflow/lite/toco/toco_tooling.cc:439] Estimated count of arithmetic ops: 0 ops, equivalently 0 MACs\r\n2019-11-21 19:25:56.569588: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 828\r\n2019-11-21 19:25:56.569891: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: RESHAPE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: StatelessWhile.\r\n\r\n```\r\n\r\n\r\nBelow is the function I am trying to convert to tflite (which will eventually be put into a custom layer). I'm trying to extract argmax coordinates from a series of filters. I was initially using **tf.tensor_scatter_nd_update** to accomplish this task, however tflite does not support that op either, so I went with the 'concat_with_padding' loop as described here:  https://www.tensorflow.org/tutorials/customization/performance. The function works fine, it only throws that error when trying to convert to tflite. \r\n\r\n```\r\n@tf.function(input_signature=[tf.TensorSpec(shape=[1,24,34,61,17], dtype=tf.float32)])\r\ndef extract_points(heatmaps):\r\n    batch_size = heatmaps.shape[0]\r\n    timesteps = heatmaps.shape[1]\r\n    filter_width = heatmaps.shape[2]\r\n    filter_height = heatmaps.shape[3]\r\n    keypoints = heatmaps.shape[-1]\r\n    \r\n    n = batch_size*timesteps*keypoints\r\n    hm2 = tf.transpose(heatmaps, [0,1,4,2,3])\r\n    hm3 = tf.reshape(hm2, (n,-1))\r\n    x = tf.zeros([n, 2])\r\n\r\n    for i in tf.range(n):\r\n        X_argm = tf.argmax(hm3[i])\r\n        coords = tf.reshape(tf.cast(tf.unravel_index(X_argm, (filter_width, filter_height)), 'float32'), (1,2))\r\n        x = tf.concat([x[:i], coords, tf.zeros([n-1-i, 2])], axis=0)\r\n        x = tf.reshape(x, [n, 2])\r\n    return  x       \r\n\r\nconcrete_func = extract_points.get_concrete_function()\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\ntflite_model = converter.convert()\r\n```\r\n\r\n\r\n\r\n", "comments": ["Adding haozha111 to triage the issue since this involves converting loops. Thanks.", "Hi,\r\n\r\ncan you try the new TF Lite converter by setting:\r\nconverter.experimental_new_converter = True.\r\n\r\nThanks.", "> Hi,\r\n> \r\n> can you try the new TF Lite converter by setting:\r\n> converter.experimental_new_converter = True.\r\n> \r\n> Thanks.\r\n\r\nI tried that as follows: \r\n```\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\n```\r\n\r\nGot the same error:\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: RESHAPE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: StatelessWhile.\r\nTraceback (most recent call last):\r\n  File \"/anaconda3/envs/posenet-v5/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/anaconda3/envs/posenet-v5/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/anaconda3/envs/posenet-v5/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/anaconda3/envs/posenet-v5/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/anaconda3/envs/posenet-v5/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/anaconda3/envs/posenet-v5/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\n```\r\n", "It actually appears to be \"unravel_index\" that is the main problem. However this still doesnt address the ''**StatelessWhile**\" issue is:\r\n\r\n```\r\n@tf.function(input_signature=[tf.TensorSpec(shape=[1,24,34,61,17], dtype=tf.float32)])\r\ndef extract_points(heatmaps):\r\n    batch_size = heatmaps.shape[0]\r\n    timesteps = heatmaps.shape[1]\r\n    filter_width = heatmaps.shape[2]\r\n    filter_height = heatmaps.shape[3]\r\n    keypoints = heatmaps.shape[-1]\r\n    \r\n    n = batch_size*timesteps*keypoints\r\n    hm2 = tf.transpose(heatmaps, [0,1,4,2,3])\r\n    hm3 = tf.reshape(hm2, (n,-1))\r\n    x = tf.zeros([n, 2])\r\n    y = tf.zeros([n, 2])\r\n    \r\n    X_argm = tf.argmax(hm3[0])\r\n    coords = tf.unravel_index(X_argm, (filter_width, filter_height))\r\n    return  coords    \r\n\r\nconcrete_func = extract_points.get_concrete_function()\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\n```\r\nHere is the error:\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ARG_MAX, RESHAPE, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: UnravelIndex.\r\n```\r\n", "from the error message, it seems that it's still using the old tflite converter. can you download the latest tf-nighly pip and then try (this could probably resolve the statelesswhile issue)?\r\n\r\nfor the `UnravelIndex` op, unfortunately currently it's not supported by TF Lite as a built-in op."]}, {"number": 34501, "title": "ValueError: slice index 12231 of dimension 1 out of bounds. for 'strided_slice_14' (op", "body": "Hello, \r\n\r\nI am trying to work on an Intepretable AI solution for Image classification. Our work is based on Smooth Grad CAM plus plus. I had a perfectly working code until  I installed pytorch in my environment. I had been working with keras library. I am passing image to a classifier and trying to visualize the layers.\r\n\r\nI am getting an error only after I installed pytorch in the same environment as I was working on my code. Now I am not able to remove the error even if I have created different environments with different tensorflow versions. \r\n\r\n\r\nHere is the fullstack of the error:\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\nC:\\Anaconda\\envs\\python3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1658   try:\r\n-> 1659     c_op = c_api.TF_FinishOperation(op_desc)\r\n   1660   except errors.InvalidArgumentError as e:\r\n\r\nInvalidArgumentError: slice index 7231 of dimension 1 out of bounds. for 'strided_slice_5' (op: 'StridedSlice') with input shapes: [?,1000], [2], [2], [2] and with computed input tensors: input[1] = <0 7231>, input[2] = <1 7232>, input[3] = <1 1>.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-15-4856e9167fcf> in <module>\r\n     10     gradCAM_VGG16 = cam(GradCAM,base_model,target_layer,X)\r\n     11     gradCAMPP_VGG16 = cam(GradCAMPlusPlus,base_model,target_layer,X)\r\n---> 12     smoothgradCAMPP_VGG16 = cam(SmoothGradCAMPlusPlus, base_model, target_layer, X, 10, 0.15)\r\n     13     print(\"Guided CAMs for \",decoded_y[0],\" using \",base_model)\r\n     14 \r\n\r\n<ipython-input-11-de0c0cde57a2> in cam(classname, model, target_layer, image, n_samples, stdev_spread)\r\n      1 def cam(classname,model,target_layer,image,n_samples=25, stdev_spread=0.15):\r\n      2     class_object = classname(model,target_layer=target_layer)\r\n----> 3     cam = class_object(image)\r\n      4 \r\n      5     return cam\r\n\r\n<ipython-input-4-4b25ccb616c7> in __call__(self, x)\r\n     18 \r\n     19     def __call__(self, x):\r\n---> 20         return self.forward(x)\r\n     21 \r\n     22     def forward(self, x):\r\n\r\n<ipython-input-4-4b25ccb616c7> in forward(self, x)\r\n     29 \r\n     30         prediction = np.argmax(self.model.predict(x))\r\n---> 31         cam = self.getSmoothGradCAMPP(prediction,x)\r\n     32 \r\n     33         return cam\r\n\r\n<ipython-input-4-4b25ccb616c7> in getSmoothGradCAMPP(self, prediction, x)\r\n     47             #x_with_noise = GaussianNoise(np.array(x))\r\n     48             x_with_noise = K.random_normal(shape = (x.shape), mean=x,stddev=std_tensor)\r\n---> 49             smg_cam = cam(GradCAMPlusPlus,self.model,self.target_layer,x_with_noise)\r\n     50 \r\n     51             if i == 0:\r\n\r\n<ipython-input-11-de0c0cde57a2> in cam(classname, model, target_layer, image, n_samples, stdev_spread)\r\n      1 def cam(classname,model,target_layer,image,n_samples=25, stdev_spread=0.15):\r\n      2     class_object = classname(model,target_layer=target_layer)\r\n----> 3     cam = class_object(image)\r\n      4 \r\n      5     return cam\r\n\r\n<ipython-input-3-9e2b9ebbb414> in __call__(self, x)\r\n     35 \r\n     36     def __call__(self, x):\r\n---> 37         return self.forward(x)\r\n     38 \r\n     39     def getGradCAMPlusPlus(self, prediction,image):\r\n\r\n<ipython-input-3-9e2b9ebbb414> in forward(self, x, idx)\r\n     30         \"\"\"\r\n     31         prediction = np.argmax(self.model.predict(x,steps=16))\r\n---> 32         cam = self.getGradCAMPlusPlus(prediction,x)\r\n     33 \r\n     34         return cam\r\n\r\n<ipython-input-3-9e2b9ebbb414> in getGradCAMPlusPlus(self, prediction, image)\r\n     48         cam: class activation map.  shape=> (1, 1, H, W)\r\n     49         '''\r\n---> 50         class_score = self.model.output[0, prediction]\r\n     51         conv_layer_output = self.model.get_layer(self.target_layer).output\r\n     52         grads = K.gradients(class_score, conv_layer_output)[0]\r\n\r\nC:\\Anaconda\\envs\\python3.6.5\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py in _slice_helper(tensor, slice_spec, var)\r\n    652         ellipsis_mask=ellipsis_mask,\r\n    653         var=var,\r\n--> 654         name=name)\r\n    655 \r\n    656 \r\n\r\nC:\\Anaconda\\envs\\python3.6.5\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py in strided_slice(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\r\n    818       ellipsis_mask=ellipsis_mask,\r\n    819       new_axis_mask=new_axis_mask,\r\n--> 820       shrink_axis_mask=shrink_axis_mask)\r\n    821 \r\n    822   parent_name = name\r\n\r\nC:\\Anaconda\\envs\\python3.6.5\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py in strided_slice(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\r\n  11364                         ellipsis_mask=ellipsis_mask,\r\n  11365                         new_axis_mask=new_axis_mask,\r\n> 11366                         shrink_axis_mask=shrink_axis_mask, name=name)\r\n  11367   _result = _op.outputs[:]\r\n  11368   _inputs_flat = _op.inputs\r\n\r\nC:\\Anaconda\\envs\\python3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    786         op = g.create_op(op_type_name, inputs, output_types, name=scope,\r\n    787                          input_types=input_types, attrs=attr_protos,\r\n--> 788                          op_def=op_def)\r\n    789       return output_structure, op_def.is_stateful, op\r\n    790 \r\n\r\nC:\\Anaconda\\envs\\python3.6.5\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py in new_func(*args, **kwargs)\r\n    505                 'in a future version' if date is None else ('after %s' % date),\r\n    506                 instructions)\r\n--> 507       return func(*args, **kwargs)\r\n    508 \r\n    509     doc = _add_deprecated_arg_notice_to_docstring(\r\n\r\nC:\\Anaconda\\envs\\python3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in create_op(***failed resolving arguments***)\r\n   3298           input_types=input_types,\r\n   3299           original_op=self._default_original_op,\r\n-> 3300           op_def=op_def)\r\n   3301       self._create_op_helper(ret, compute_device=compute_device)\r\n   3302     return ret\r\n\r\nC:\\Anaconda\\envs\\python3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   1821           op_def, inputs, node_def.attr)\r\n   1822       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\r\n-> 1823                                 control_input_ops)\r\n   1824 \r\n   1825     # Initialize self._outputs.\r\n\r\nC:\\Anaconda\\envs\\python3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1660   except errors.InvalidArgumentError as e:\r\n   1661     # Convert to ValueError for backwards compatibility.\r\n-> 1662     raise ValueError(str(e))\r\n   1663 \r\n   1664   return c_op\r\n\r\nValueError: slice index 7231 of dimension 1 out of bounds. for 'strided_slice_5' (op: 'StridedSlice') with input shapes: [?,1000], [2], [2], [2] and with computed input tensors: input[1] = <0 7231>, input[2] = <1 7232>, input[3] = <1 1>.\r\n", "comments": ["Please help me in solving this issue, I have tried every possible methods I am aware of..", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@frincyc \r\n\r\nAny update on the issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Hello @ravikyram \r\nSorry for the late response. I had the issue resolved. I was using Tensorflow 2.0. There were some deprecations in that and I tried version 1.15 on Google colab, it was working fine. I also had some issue with the dimensions of input I was passing. My application call inceptionv3 in loop for prediction in a loop, but it was accidentally re-reading the output of previous predictions. I had to change the flow of reading the input, converting to required size and then passing for prediction, by re-initializing each time in the loop. After these two changes, the code started working again.\r\n\r\nThanks for your reponse. "]}, {"number": 34500, "title": "[TF2.1] Performance: Control flow and scalar ops 225x slower than raw Python and 24000x slower than C++", "body": "**Summary**\r\nTF Op performance for a simple subgraph (built with AutoGraph) is at-least 2 orders of magnitude slower than expected: looping over 100K numbers takes 4+ seconds instead of 18ms (or much faster)\r\n\r\n**Benchmark code and repro instructions:**\r\nhttps://github.com/divyekapoor/ml-op-benchmarks\r\n\r\n1. Clone the repo\r\n2. `make tfbench`\r\n\r\n```\r\nclass FizzBuzz(tf.Module):\r\n    @tf.function(input_signature=[tf.TensorSpec([], tf.int32)])\r\n    def model(self,\r\n              n  # Shape [] -- int32 the max number to loop FizzBuzz to\r\n              ):  # Returns counts for fizz, buzz and fizzbuzz. Shape: [1] with length 3\r\n        fizz = 0\r\n        buzz = 0\r\n        fizzbuzz = 0\r\n        for i in range(n):\r\n            if i % 6 == 0:\r\n                fizzbuzz += 1\r\n            elif i % 3 == 0:\r\n                buzz += 1\r\n            elif i % 2 == 0:\r\n                fizz += 1\r\n        return [fizz, buzz, fizzbuzz]\r\n```\r\nRaw python: Running the same code without tf.Module and @tf.function.\r\nRaw C++: Equivalent implementation in straight C++.\r\n\r\nPerformance table:\r\n\r\nFizzBuzz Iteration Counts | 100000 | \u00a0 | \u00a0 | \u00a0\r\n-- | -- | -- | -- | --\r\n\u00a0 | Raw Latency (ms) | Per Run Latency (usec) | Python Multiplier | C++ Multiplier\r\nTensorflow Python | 4087 | 40.87 | **227.06** | 24327\r\nTensorflow Saved Model Python | 4046 | 40.46 | **224.78** | 24083\r\nRaw Python | 18 | 0.18 | 1.00 | 107\r\nRaw C++ | 0.168 | 0.00168 | 0.01 | 1\r\n\r\nThe multiplers use the corresponding Python and C++ code as unit = 1.\r\nBenchmark script is attached at the bottom of this issue and only has a dependency on Tensorflow.\r\n\r\nhttps://github.com/divyekapoor/ml-op-benchmarks has something to directly clone and execute.\r\n\r\n(If it would help, TF ops are ~40% slower than Torch ops for the same FizzBuzz benchmark)\r\nhttps://github.com/pytorch/pytorch/issues/30365 for the related PyTorch issue.\r\n\r\nFizzBuzz Iteration Counts | 100000 | \u00a0 | \u00a0 | \u00a0\r\n-- | -- | -- | -- | --\r\n\u00a0 | Raw Latency (ms) | Per Run Latency (usec) | Python Multiplier | C++ Multiplier\r\nPyTorch Python | 4007 | 40.07 | 222.61 | 23851\r\nPyTorch TorchScript Python (from Loaded TorchScript) | 2830 | 28.3 | **157.22** | 16845\r\nPyTorch TorchScript C++ (Native) | 255 | 2.55 | **14.17** | 1518\r\nPyTorch TorchScript C++ (Native + ATen Tensors) | 252 | 2.52 | **14.00** | 1500\r\nRaw Python | 18 | 0.18 | 1.00 | 107\r\nRaw C++ | 0.168 | 0.00168 | 0.01 | 1\r\n\r\n\r\n\r\n\r\nPerformance similar to raw Python is the expected behavior.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.6 (18G1012)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested.\r\n- TensorFlow installed from (source or binary): binary.\r\n- TensorFlow version (use command below): \r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.1.0-dev20191107\r\ntf.version.GIT_VERSION = v1.12.1-17543-gb4b5ce680c\r\ntf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.11.45.5)\r\n- Python version: \r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 7, 4, 'final', 0)\r\n- Bazel version (if compiling from source): NA.\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Setup**\r\nPerformance benchmark for conditional ops set up with a FizzBuzz test case:\r\nInput: n -> a range limit (100K)\r\nOutput: a 3 element tensor with counts for (fizz, buzz, fizzbuzz)\r\nGoal: To estimate the performance overhead of TF ops versus raw python / raw C++.\r\nBenchmark file is attached.\r\n\r\n**Describe the current behavior**\r\nFizzBuzz with TF ops is 225x slower than the same code in Raw Python and 24K+x slower than the corresponding C++ implementation. \r\n\r\n**Describe the expected behavior**\r\nFizzBuzz with TF ops should be within 10-50% of raw Python or faster.  \r\n\r\n**Code to reproduce the issue**\r\nAttached to this report.\r\nTo reproduce:\r\n```\r\n$ python3 -m venv venv3\r\n$ source venv3/bin/activate\r\n$ pip3 install tensorflow\r\n$ python3 fizz.py\r\n```\r\nFull version: https://github.com/divyekapoor/ml-op-benchmarks\r\n\r\n**Other info / logs**\r\nPerformance table:\r\n\r\nFizzBuzz Iteration Counts | 100000 | \u00a0 | \u00a0 | \u00a0\r\n-- | -- | -- | -- | --\r\n\u00a0 | Raw Latency (ms) | Per Run Latency (usec) | Python Multiplier | C++ Multiplier\r\nTensorflow Python | 4087 | 40.87 | 227.06 | 24327\r\nTensorflow Saved Model Python | 4046 | 40.46 | 224.78 | 24083\r\nRaw Python | 18 | 0.18 | 1.00 | 107\r\nRaw C++ | 0.168 | 0.00168 | 0.01 | 1\r\n\r\nRaw latency == run with range input N = 100K\r\nPer Run latency == Raw latency / 100K (one run through the op graph)\r\n[fizz.tar.gz](https://github.com/tensorflow/tensorflow/files/3877175/fizz.tar.gz)\r\n", "comments": ["First, it's great to see these measurements put together!\r\n\r\nThere are a few issues that you'll need to address before the benchmark is conclusive, please see below. I'd love to continue the discussion though; I think such a benchmark will be a useful indicator for the performance of control flow ops, which are becoming more pervasive even in ML.\r\n\r\nThe speed differences are somewhat expected, because TF ops (and PyTorch, as well as NumPy, for that matter) are optimized for compute-intensive vector calculations where what seems like huge overhead is dwarfed by the computation itself. So when single scalars are involved, they really don't shine, and even something as slow as Python easily outmatches them. Of course, NumPy is probably faster due to its lower overhead, but still slower than pure Python. I ran this test to convince myself of that - feel free to add it to your suite:\r\n\r\n```\r\nclass FizzBuzz(tf.Module):\r\n    def model(self,n):\r\n        fizz = np.array(0)\r\n        buzz = np.array(0)\r\n        fizzbuzz = np.array(0)\r\n        # Force everything to be a numpy scalar, for an even comparison\r\n        for i in np.arange(n)[:, np.newaxis]:\r\n            if i % 6 == 0:\r\n                fizzbuzz += 1\r\n            elif i % 3 == 0:\r\n                buzz += 1\r\n            elif i % 2 == 0:\r\n                fizz += 1\r\n        return [fizz, buzz, fizzbuzz]\r\n```\r\n\r\nI believe we should aim to match the performance of NumPy, so there's definitely much room for improvement.\r\n\r\n1. The bechmarks time the *initial execution* of tf.function, which is known to be significantly slower than subsequent invocations - even excluding potential compilation and optimization overhead, we know merely constructing a TF graph is very inefficient right now. Running from a saved model probably alleviates that, but probably not by much.\r\n\r\n2. The benchmarks seem to only execute a single measurement, which will probably be noisy. I recommend averaging across at least 100 invocations for each experiment. Combined with #1 above, I recommend the following setup: (a) warm-up by running the payload 3 times to get rid of any transient effects; (b) time the execution 10-100 times and average the results.\r\n\r\n3. A bit of a nitpick is that the TF benchmarks really time the performance of TF's control flow ops (tf.cond, tf.while_loop). There are no AutoGraph ops per se - it just creates the usual TF control flow ops. You could confirm this by running this roughly equivalent code, which should give results comparable with AutoGraph:\r\n\r\n```\r\nclass FizzBuzz(tf.Module):\r\n    @tf.function(input_signature=[tf.TensorSpec([], tf.int32)], autograph=False)\r\n    def model(self,\r\n              n  # Shape [] -- int32 the max number to loop FizzBuzz to\r\n              ):  # Returns counts for fizz, buzz and fizzbuzz. Shape: [1] with length 3\r\n        fizz = 0\r\n        buzz = 0\r\n        fizzbuzz = 0\r\n\r\n        def cond(i, fizz, buzz, fizzbuzz):\r\n          return i < n\r\n\r\n        def body(i, fizz, buzz, fizzbuzz):\r\n          return (i + 1,) + tf.cond(\r\n              i % 6 == 0,\r\n              lambda: (fizz, buzz, fizzbuzz + 1),\r\n              lambda: tf.cond(\r\n                  i % 3 == 0,\r\n                  lambda: (fizz, buzz + 1, fizzbuzz),\r\n                  lambda: tf.cond(\r\n                      i % 2 == 0,\r\n                      lambda: (fizz + 1, buzz, fizzbuzz),\r\n                      lambda: (fizz, buzz, fizzbuzz)\r\n                  )\r\n              )\r\n          )\r\n\r\n        _, fizz, buzz, fizzbuzz = tf.while_loop(\r\n            cond, body, (0, fizz, buzz, fizzbuzz))\r\n        return [fizz, buzz, fizzbuzz]\r\n```\r\n\r\n4. As mentioned above, your benchmark test long-running algorithms operating on scalars, something which even Pyhton is fairly good at. I'd recommend running the same test on larger vectors, maybe by initializing `fizz` with a large matrix.\r\n", "@mdanatg Thanks for the response.\r\n\r\nRe: (1) and (2) - the performance gap is so wide that measurement variance is not material. Here's 10 runs of both the saved model and direct execution (run as a for-loop in the same session). The best run is highlighted.\r\n\r\n```\r\nTime taken (TF Python) (ms):  4121.040513\r\nTime taken (TF Python) (ms):  3896.005856\r\nTime taken (TF Python) (ms):  3752.050521\r\nTime taken (TF Python) (ms):  3751.995625\r\nTime taken (TF Python) (ms):  3740.542564\r\nTime taken (TF Python) (ms):  ***3723.199538***\r\nTime taken (TF Python) (ms):  3792.319686\r\nTime taken (TF Python) (ms):  4279.583034\r\nTime taken (TF Python) (ms):  3818.070901\r\nTime taken (TF Python) (ms):  4053.846837\r\n```\r\n\r\n```\r\nTime taken (SavedModel) (ms):  4132.69518\r\nTime taken (SavedModel) (ms):  3889.249228\r\nTime taken (SavedModel) (ms):  4177.042328\r\nTime taken (SavedModel) (ms):  4409.928661\r\nTime taken (SavedModel) (ms):  4209.030763\r\nTime taken (SavedModel) (ms):  4370.232748\r\nTime taken (SavedModel) (ms):  3881.061122\r\nTime taken (SavedModel) (ms):  3929.917805\r\nTime taken (SavedModel) (ms):  ***3781.373323***\r\nTime taken (SavedModel) (ms):  3808.811347\r\n```\r\nThank you for the pointer to the NumPy variant of the test for a reasonable baseline. \r\nI've updated the repo. Some nits with the NumPy baseline are that it actually allocates the memory instead of being a pure loop iteration (but with or without correcting for that, the gap is an order of magnitude as compared to raw Python and one order of magnitude below the current TF performance profile).\r\n\r\nAfter NumPy inclusion and no AutoGraph inclusion (raw ops):\r\n\r\nFizzBuzz Iteration Counts | 100000 | \u00a0 | \u00a0 | \u00a0\r\n-- | -- | -- | -- | --\r\n\u00a0 | Method Latency (ms) | Iteration Latency (usec) | Python Multiplier | C++ Multiplier\r\nTensorflow Python | 4087 | 40.87 | ***227.06*** | 24327\r\nTensorflow Saved Model Python | 4046 | 40.46 | ***224.78*** | 24083\r\nTensorflow Python no Autograph | 3981 | 39.81 | **221.16** | 23696\r\nNumPy Python | 420 | 0.42 | ***23.3*** | 2500\r\nRaw Python | 18 | 0.18 | 1.00 | 107\r\nRaw C++ | 0.168 | 0.00168 | 0.01 | 1\r\n\r\nIt's safe to say that the measurement variance exists, so the 3 TF setups are pretty much within a stone's throw of each other and not qualitatively different.\r\n\r\nAnd as you've rightly pointed out, the systems are set-up for vectorized calculations and this isn't really an AutoGraph bug (it's a core-ops runtime performance issue). We're considering these ops to generate cross features as part of our critical serving path (so the slowdown would have direct user-visible-latency impact - our current code is straight C++ and we're willing to accept some latency hit). Having some insight into what's driving this kind of overhead / slowdown and a way to avoid it would be very much appreciated. ", "Thanks for double-checking - I still think it's a good practice to average several executions, but the important part was to confirm the steady-state performance within the same session.\r\n\r\nThe efficiency of TF control flow ops is something that we hope to improve in the future, but it will take a bit of time.\r\n\r\nIn the meantime, I'd be happy to help optimize your specific code, so that at least this issue doesn't block you - can you describe the computations you need to perform for these cross features?\r\n\r\nAs a side note, an effective way to minimize this kind of slowdown is to use vector ops. Here's the same fizzbuzz code rewritten to demonstrate:\r\n\r\n```\r\nclass FizzBuzz(tf.Module):\r\n    @tf.function(input_signature=[tf.TensorSpec([], tf.int32)])\r\n    def model(self,\r\n              n  # Shape [] -- int32 the max number to loop FizzBuzz to\r\n              ):  # Returns counts for fizz, buzz and fizzbuzz. Shape: [1] with length 3\r\n        i = tf.range(n)\r\n        fizz_v = (i % 2 == 0)\r\n        buzz_v = (i % 3 == 0)\r\n        fizz = tf.reduce_sum(tf.cast(fizz_v, tf.int32))\r\n        buzz = tf.reduce_sum(tf.cast(buzz_v, tf.int32))\r\n        fizzbuzz = tf.reduce_sum(tf.cast(tf.logical_and(fizz_v, buzz_v), tf.int32))\r\n        return [fizz, buzz, fizzbuzz]\r\n```\r\n\r\nThe performance target for such code should be the C++ benchmark, because the op kernels themselves are implemented in C++. I think it already rivals NumPy, or at least it did in my measurements:\r\n\r\n```\r\nclass FizzBuzz(tf.Module):\r\n    def model(self,\r\n              n  # Shape [] -- int32 the max number to loop FizzBuzz to\r\n              ):  # Returns counts for fizz, buzz and fizzbuzz. Shape: [1] with length 3\r\n        i = np.arange(n)\r\n        fizz_v = i % 2 == 0\r\n        buzz_v = i % 3 == 0\r\n        fizz = np.sum(fizz_v.astype(np.int32))\r\n        buzz = np.sum(buzz_v.astype(np.int32))\r\n        fizzbuzz = np.sum(np.logical_and(fizz_v, buzz_v).astype(np.int32))\r\n        return [fizz, buzz, fizzbuzz]\r\n```\r\n\r\nOf course, not all computations can be vectorized in this fashion so the method is not always applicable.", "Update - the numbers improve a lot for this benchmark when using XLA compilation, approaching Python in my tests:\r\n\r\n```\r\ntf.xla.experimental.compile(fb_saved_model.model, [tf.constant(100000)])\r\n```\r\n\r\nUnfortunately, this API is experimental, so I wouldn't recommend using it in production. It should still give us a better indication for what the baseline should be, though.", "This is very helpful. Thank you.\r\nI'll try and include these in the comparative benchmarks.\r\n\r\nA colleague also got a vectorized PyTorch implementation working in about 8ms (vs 15ms for Python). So the baseline is about 8ms for a vectorized implementation.\r\n\r\nRe the XLA compile, is it on the control flow impl or the vectorized impl?", "I tested it with the control flow impl:\r\n\r\n```\r\nimport tensorflow.compat.v2 as tf\r\ntf.enable_v2_behavior()\r\n\r\nclass FizzBuzz(tf.Module):\r\n    @tf.function(input_signature=[tf.TensorSpec([], tf.int32)])\r\n    def model(self,\r\n              n  # Shape [] -- int32 the max number to loop FizzBuzz to\r\n              ):  # Returns counts for fizz, buzz and fizzbuzz. Shape: [1] with length 3\r\n        fizz = 0\r\n        buzz = 0\r\n        fizzbuzz = 0\r\n        for i in range(n):\r\n            if i % 6 == 0:\r\n                fizzbuzz += 1\r\n            elif i % 3 == 0:\r\n                buzz += 1\r\n            elif i % 2 == 0:\r\n                fizz += 1\r\n        return [fizz, buzz, fizzbuzz]\r\n\r\nfb = FizzBuzz()\r\n\r\ntf.saved_model.save(fb, '/tmp/fizzbuzz.m')\r\nfb_saved_model = tf.saved_model.load('/tmp/fizzbuzz.m')\r\n\r\ntf.xla.experimental.compile(fb_saved_model.model, [tf.constant(1)])\r\n\r\n%timeit tf.xla.experimental.compile(fb_saved_model.model, [tf.constant(100000)])\r\n```\r\n\r\nXLA requires all shapes to be static and complains about the use of `tf.range(n)`, so compiling the vectorized implementation might be a bit trickier.\r\n", "BTW, there is a better way to use XLA, one that is more likely to be supported and has less overhead as well:\r\n\r\n```\r\ncompiled_saved_model = tf.function(fb_saved_model.model, experimental_compile=True)\r\n\r\ncompiled_saved_model(tf.constant(1))\r\n\r\n%timeit compiled_saved_model(tf.constant(100000))\r\n```\r\n\r\nWith this, performance starts to approach the order of magnitude of C++.\r\n\r\nOf course, we would like things to be this fast by default without resorting to arcane flags, so there still is a bit of work to be done.", "I ran the XLA compiled version with TF nightly:\r\n\r\n```\r\nTime taken (XLA SavedModel 1st run) (ms):  132.143258\r\nTime taken (XLA SavedModel 2nd run) (ms):  81.370215\r\nTime taken (XLA SavedModel 3rd run) (ms):  82.8199\r\nTime taken (Python3) (ms):  16.207496\r\n```\r\n\r\nEven if I hardcode the count (100K) in the range(n), the speeds improve but aren't C++ comparable:\r\n\r\n```\r\nTime taken (XLA SavedModel 1st run) (ms):  71.305211\r\nTime taken (XLA SavedModel 2nd run) (ms):  27.728747\r\nTime taken (XLA SavedModel 3rd run) (ms):  26.095412\r\n```\r\n\r\nDid you mean: \"approach the order of magnitude of Python?\" (not C++?). The gap seems to be still 400x C++ code (though only 4.5x Python). Could you take a look as to the XLA implementation in the repo? https://github.com/divyekapoor/ml-op-benchmarks\r\n\r\n```\r\n>>> tf.__version__\r\n'2.1.0-dev20200102'\r\n```\r\n\r\nReadme updated with the new numbers. ", "FWIW, with a constant value in range(n) (n=100K) the non XLA version of the model gets a huge boost in speed:\r\n\r\n```\r\nTime taken (TF Python) (ms):  4.499958\r\nTime taken (Python3) (ms):  15.678116\r\n```\r\nand it beats Python and is about 20x slower than C++ (quite nice!). The XLA version of the model though isn't able to optimize the graph (as above it's at around 26ms instead of 4ms - a 6x gap vs the prod version of saved model). ", "Summarizing the issue: Seems we have the following open questions - \r\n1. Perf: overhead of tf control ops: tf.cond / tuple increment / tf.while is too high (by 200x+) - this bug. (seems this is validated - seems the proposed solution is XLA but it isn't ready yet; baseline: ~4ms vs 4000ms)\r\n2. XLA compiler can't handle constant folding as well as the prod compiler. (let's treat this separately -- discussion above). Confirmation of: \"Did you mean: \"approach the order of magnitude of Python?\" (not C++?)\" for XLA based on my numbers above.\r\n\r\nFor moving forward with (1), any other next steps @mdanatg ?\r\n", "Your summary looks right. We're working on improving the graph executor and that should help resolve both (1) and (2), but it's a longer-term effort and I don't have a clear estimate when it will be ready.\r\n\r\nOddly, I ran a simplified version of the code (without the tf.Module / saved model bit) and there the results were significantly better than Python (see the results and code below). So perhaps there is some extra overhead around either the module or the saved model. But I think that's besides the point, which is that control flow is currently slow in TF graph.\r\n\r\n---\r\n\r\nSide note - here's the code and the timings I got:\r\n\r\nMethod | Time (\u00b5s)\r\n-- | --\r\nTF 2 (function + XLA) | 712\r\nTF 2 (vectorized) | 2240\r\nPython (plain) | 14900\r\nTF 2 (function only) | 3040000\r\n\r\n```\r\n@tf.function(input_signature=[tf.TensorSpec([], tf.int32)], experimental_compile=True)\r\ndef fb(n):  # Returns counts for fizz, buzz and fizzbuzz. Shape: [1] with length 3\r\n    fizz = 0\r\n    buzz = 0\r\n    fizzbuzz = 0\r\n    for i in range(n):\r\n        if i % 6 == 0:\r\n            fizzbuzz += 1\r\n        elif i % 3 == 0:\r\n            buzz += 1\r\n        elif i % 2 == 0:\r\n            fizz += 1\r\n    return (fizz, buzz, fizzbuzz)\r\n\r\nfb(tf.constant(1))\r\n\r\n%timeit -n100 -r3 fb(tf.constant(100000))\r\n```\r\n\r\n\r\n```\r\n@tf.function(input_signature=[tf.TensorSpec([], tf.int32)])\r\ndef fb(n):  # Returns counts for fizz, buzz and fizzbuzz. Shape: [1] with length 3\r\n  i = tf.range(n)\r\n  fizz_v = (i % 2 == 0)\r\n  buzz_v = (i % 3 == 0)\r\n  fizz = tf.reduce_sum(tf.cast(fizz_v, tf.int32))\r\n  buzz = tf.reduce_sum(tf.cast(buzz_v, tf.int32))\r\n  fizzbuzz = tf.reduce_sum(tf.cast(tf.logical_and(fizz_v, buzz_v), tf.int32))\r\n  return [fizz, buzz, fizzbuzz]\r\n\r\nfb(tf.constant(1))\r\n\r\n%timeit -n100 -r3 fb(tf.constant(100000))\r\n```\r\n\r\n```\r\ndef fb(n):\r\n    fizz = 0\r\n    buzz = 0\r\n    fizzbuzz = 0\r\n    for i in range(n):\r\n        if i % 6 == 0:\r\n            fizzbuzz += 1\r\n        elif i % 3 == 0:\r\n            buzz += 1\r\n        elif i % 2 == 0:\r\n            fizz += 1\r\n    return (fizz, buzz, fizzbuzz)\r\n\r\nfb(1)\r\n\r\n%timeit -n100 -r3 fb(100000)\r\n```\r\n\r\n```\r\n@tf.function(input_signature=[tf.TensorSpec([], tf.int32)])\r\ndef fb(n):  # Returns counts for fizz, buzz and fizzbuzz. Shape: [1] with length 3\r\n    fizz = 0\r\n    buzz = 0\r\n    fizzbuzz = 0\r\n    for i in range(n):\r\n        if i % 6 == 0:\r\n            fizzbuzz += 1\r\n        elif i % 3 == 0:\r\n            buzz += 1\r\n        elif i % 2 == 0:\r\n            fizz += 1\r\n    return (fizz, buzz, fizzbuzz)\r\n\r\nfb(tf.constant(1))\r\n\r\n%timeit -n10 -r3 fb(tf.constant(100000))\r\n```", "@divyekapoor \r\nCould you please verify with later tf versions and let us know if this is still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 34499, "title": "E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] layout failed: Invalid argument: size of values 0 does not match size of permutation 4.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below):  2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: RTX 2080Ti 11GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nOriginally, I built this model in tensorflow 1.1x and I transferred the model to TF 2.0 manually to use tf.keras. It is working but it shows me this error message  (E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] layout failed: Invalid argument: size of values 0 does not match size of permutation 4.) and its performance is worse than tf 1.1x. \r\n\r\nI suspect that this error interrupts to train somehow. \r\n\r\nI didn't put any permutation layer in my model. It is hard to find it. \r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["same issue for me.", "@wkdgnsgo, Please paste the standalone code to reproduce the issue.\r\nFollow the instructions mentioned in the [Tensorflow](https://www.tensorflow.org/guide/migrate) site to migrate from TF1 to TF2. Thanks!", "I have the exact same issue training a cycleGAN, my operations are built in `tf.Module` and I\u2019m using `tf.optimizers.Adam` ", "I get the same error message(/warning?). `Python 3.6.8 [GCC 8.3.0]`, tf.version `v2.0.0-rc2-26-g64c3d38 2.0.0` in the official tf docker image. \r\n\r\nQuite a large I2I translation project, so it's hard to make minimal reproducing code. An issue that _might_ be related is a shape error that is produced if the `train` function is decorated with `@tf.function`, which works in EE mode. \r\n\r\nA par of subclassed `tf.keras.Model`s `f_x, f_y` is used with the train step pattern\r\n```python \r\nself.loss_object = tf.keras.losses.MeanSquaredError()\r\nself.optimizer = tf.keras.optimizers.Adam(tf.keras.optimizers.schedules.ExponentialDecay(...))\r\n\r\ndef train_step(x,y,cross_loss_weight)\r\n    with tf.GradientTape as tape: \r\n        y_hat, x_hat = f_x(x), f_y(y)\r\n        x_tilde, y_tilde = f_y(y_hat), f_x(x_hat)\r\n        fx_loss = {\r\n                \"cross\": self.loss_object(y, y_hat, clw),\r\n                \"cycle\": self.loss_object(x, x_tilde),\r\n                \"reg\": sum(self._fx.losses),  # regularization from submodel\r\n        }\r\n        fx_loss = {key: self.lambdas[key] * value for key, value in fx_loss.items()}\r\n        fx_total_loss = sum(fx_loss.values())\r\n        ... same for f_y...\r\n    gradient_targets = self._fx.trainable_variables + self._fy.trainable_variables\r\n    gradients = tape.gradient(loss_value, gradient_targets)\r\n    self.optimizer.apply_gradients(zip(gradients, gradient_targets))\r\n``` \r\n\r\nSo it seems the use is similar to @miguelalba96 with a cyclic loss term, but no adversarial term in my case. The model is hardly a DAG, but rather two DAGs trained in conjunction. Can the issue be related to this?", "I am training a VAE and I encountered the same issue and managed to remove the error but I do not understand what caused the issue. When I run this code I get the error: \r\n```python\r\n@tf.function\r\ndef forward(x_real):\r\n    eps = tf.random.normal([FLAGS.batch_size, FLAGS.latent_dim])\r\n    z_mu, z_log_sigma = E(x_real, training=True)\r\n    z = z_mu + tf.exp(z_log_sigma) * eps\r\n    x_real_mu = D(z, training=True)\r\n\r\n    kl_loss = tf.reduce_mean(\r\n        kl_divergence(z_mu, z_log_sigma))\r\n    ll_loss = tf.reduce_mean(\r\n        negative_log_likelyhood(x_real, x_real_mu))\r\n    return x_real_mu, ll_loss, kl_loss\r\n```\r\nbut the error vanishes if I run this one:\r\n```python\r\n@tf.function\r\ndef forward(x_real):\r\n    eps = tf.random.normal([FLAGS.batch_size, FLAGS.latent_dim])\r\n    z_mu, z_log_sigma = E(x_real, training=True)\r\n    z = z_mu + tf.exp(z_log_sigma) * eps\r\n    x_real_mu = D(z, training=True)\r\n\r\n    kl_loss = tf.reduce_mean(\r\n        kl_divergence(z_mu, z_log_sigma))\r\n    ll_loss = tf.reduce_mean(\r\n        negative_log_likelyhood(x_real+0.0, x_real_mu)) # <-- change here\r\n    return x_real_mu, ll_loss, kl_loss\r\n```\r\nI wonder why the first version raises the error...", "@AntoinePlumerault, Could provide the complete code snippet to replicate the reported issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34499\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34499\">No</a>\n", "I confirm the same issue. I am working with python 3.7 and tensorflow-gpu 2.0 release installed from pip. \r\nHere is the code to reproduce the issue. You would need a folder of images with the same size to run this code:\r\n\r\n[CVAE-model-error-code.txt](https://github.com/tensorflow/tensorflow/files/3980620/CVAE-model-error-code.txt)\r\n\r\nAs @AntoinePlumerault mentioned, I could solve the issue by changing line 173,\r\n`  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)`\r\n\r\nto this,\r\n\r\n`  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x+0.0)`", "Similar issue. Here is my solution. In my case the error was caused by the following code:\r\n```\r\ndef _ndtr(self, x):\r\n  # x is a tensor with shape [bs, height, width, channels]\r\n  half_sqrt_2 = 0.5 * np.sqrt(2.)\r\n  w = x * half_sqrt_2\r\n  z = tf.abs(w)\r\n  y = tf.where(z < half_sqrt_2, 1. + tf.math.erf(w), tf.where(w > 0., 2. - tf.math.erfc(z), tf.math.erfc(z)))\r\n  return 0.5 * y\r\n```\r\nWhich gives `layout failed: Invalid argument: Size of values 0 does not match size of permutation 4 @ fanin shape ingradient_tape/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer`.  \r\n\r\nLooks like in the layout optimization stage tensors are transposed from NHWC to NCHW for performance, but some are failed.\r\nSo transposing it manually will solve:\r\n```\r\ndef _ndtr(self, x):\r\n  # x is a tensor with shape [bs, height, width, channels]\r\n  x = tf.transpose(x, [0, 3, 1, 2])\r\n  half_sqrt_2 = 0.5 * np.sqrt(2.)\r\n  w = x * half_sqrt_2\r\n  z = tf.abs(w)\r\n  y = tf.where(z < half_sqrt_2, 1. + tf.math.erf(w), tf.where(w > 0., 2. - tf.math.erfc(z), tf.math.erfc(z)))\r\n  y = tf.transpose(y, [0, 2, 3, 1])\r\n  return 0.5 * y\r\n```\r\nThe best solution is writing your model in NCHW format, so you can skip the layout optimization stage. [Here](https://web.stanford.edu/class/cs245/slides/TFGraphOptimizationsStanford.pdf) a tutorial on what tf does in graph optimization stage. "]}, {"number": 34498, "title": "[TFLite] Add support for Particle MCUs to TensorFlow Lite Micro", "body": "This is a WIP PR for adding support for Particle MCUs to TFLite Micro. It includes the following:\r\n\r\n- [x] `debug_log` implementation for Particle\r\n- [x] `hello_world` example source\r\n- [x] Particle additions to `Makefile` and `helper_functions.inc`\r\n- [x] `test_particle.sh`\r\n- [x] Generate and merge zips in a similar fashion to Arduino builds\r\n- [x] Modifications to `transform_source.py`\r\n- [x] Add `magic_wand` source\r\n- [x] Add `micro_speech` source\r\n\r\nStill to complete:\r\n\r\n- [ ] Remove `fprintf` calls from `kissfft` for Particle projects\r\n- [ ] Transform example projects for standard Particle project formats (`project.properties` and `src` directory)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34498) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34498) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34498) for more info**.\n\n<!-- ok -->", "@bsatrom This PR is in draft, any update on this? Please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "No longer valid"]}, {"number": 34497, "title": "Make docker has the same bazel version as the env that invokes it.", "body": "PiperOrigin-RevId: 281789221\r\nChange-Id: I6b2ebbe4bf787bb2e591905c8e5368cfac793e0e", "comments": []}, {"number": 34496, "title": "\"Conv2DBackpropFilter uses a while_loop. Fix that!\"", "body": "\r\n**Describe the current behavior**\r\nCurrently when I compute Jacobian with the option \"experimental_use_pfor=True\" on Conv2D layers, the GPU memory will explode. For other layers like FC (with the same amount of parameters), there is no such issue.\r\n\r\n**Describe the expected behavior**\r\nIf there is any efficient alternative way of doing this (hopefully YES, as indicated by the warning message in the code \"Conv2DBackpropFilter uses a while_loop. Fix that!\"), please kindly update.\r\n\r\n**Code to reproduce the issue**\r\nCall tape.jacobian on a Conv2D network of a moderate size with \"experimental_use_pfor=True\".\r\n\r\n", "comments": ["@hnyu, Thanks for reporting this issue.\r\nPlease provide the complete standalone code to reproduce the reported issue. And also provide the Tensorflow version. Thanks!", "@hnyu, Could you post your code snippet to analyze the issue. ", "@gadagashwini Sorry for the delay because of the vacation. Below is a minimal reproducible example:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nif __name__ == \"__main__\":\r\n    gpus = tf.config.experimental.list_physical_devices('GPU')\r\n    for gpu in gpus:\r\n        tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n    img = tf.zeros([64, 10, 10, 1])\r\n    deconv = tf.keras.layers.Conv2DTranspose(\r\n        padding=\"valid\",\r\n        filters=1,\r\n        kernel_size=4,\r\n        strides=2)\r\n\r\n    with tf.GradientTape() as tape:\r\n        pred_img = deconv(img)\r\n        cost = tf.reduce_sum(pred_img)\r\n\r\n    jacob = tape.jacobian(\r\n        cost,\r\n        deconv.trainable_variables,\r\n        experimental_use_pfor=True,\r\n        unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n```\r\nFor this code, it can run and terminate successfully. At the end of the output, there is one line:\r\n\r\n```W1205 12:10:23.189974 139735109752640 pfor.py:1677] Conv2DBackpropFilter uses a while_loop. Fix that!```\r\n\r\nIt seems that computing Jacobian with the option ```experimental_use_pfor=True``` on Conv2DTranspose will throw this message located in [pfor.py](https://github.com/tensorflow/tensorflow/blob/cec0d99443b8ad1930594ceaf35c51e80247b7d2/tensorflow/python/ops/parallel_for/pfor.py#L1766), while doing it on Conv2D alone will not. I had a memory explosion issue for my larger-scale task accompanied with this message (which shouldn't happen because I can compute Jacobian on an MLP with a roughly equal amount of parameters). I suspect the memory issue is related to this warning message, but am not sure. \r\n\r\nBut as the warning message suggests, it might be better if it can be addressed.", "I could replicate the issue with Tenosrflow 2.0 and 2.1-rc0.\r\nPlease see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/8b0694efd70cde49845e2c8745e9610e/untitled296.ipynb). Thanks!", "It's not clear why the while_loop based fallback for handling pfor should be causing the memory increase. The warning here is because the fallback could cause slowdowns compared to a more optimized vectorization. It will be worth looking at some memory profiles to pin down the cause.\r\nJacobian computation is going to be expensive and memory intensive. It's worth doing some calculations on expected memory growth to see if the observed memory is inline or not.\r\n\r\n", "It looks like cost is a scalar. Why not use tape.gradient here ? tape.jacobian is expected to be more memory inefficient. "]}, {"number": 34495, "title": "[tf.data] Fix OOM when tf.data map_and_batch is used with num_paralle\u2026", "body": "\u2026l_calls = autotune, batch_size = 1.\r\n\r\nCloses #33516.\r\n\r\nPiperOrigin-RevId: 281775472\r\nChange-Id: Ie10cea0ef1515d5aff8e3dddadc069ddee1a5a76", "comments": []}, {"number": 34494, "title": "[r2.1:Cherrypick]Change default CUDA version on Windows to 10.1.", "body": "PiperOrigin-RevId: 279416693\nChange-Id: Ie818bf80bfc190e3ef1c0b14f2e57ea8f899a4d0", "comments": []}, {"number": 34493, "title": "Fix nested function inside XLA context.", "body": "", "comments": []}, {"number": 34492, "title": "TFLite TOCO: Verify trivial operator has no activation function", "body": "Bug scenario:\r\nAdd layer with zero weights is omitted, despite having a fused activation function.", "comments": ["@miaout17 Can you please take a look on this PR? Thanks!"]}, {"number": 34491, "title": "tf.keras computes incorrect loss values with Masking", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow  /  pip install tf-nightly-2.0-preview\r\n- TensorFlow version (use command below): 2.0  /  2.0.0-dev20191002\r\n- Python version: 3.7.5\r\n\r\n**Describe the current behavior**\r\n\r\nWhen fitting an LSTM model with a Masking layer using Keras, if you use binary_crossentropy both as loss and metric, the values are different. In particular, the printed metric is correct, while the loss appears to be calculated without masking.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nfrom numpy.random import normal, randint\r\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Masking, LSTM, Activation, Dense\r\n\r\ntime_steps = 20\r\nnum_seqs = 100\r\nX = normal(size=(num_seqs, time_steps))  # create artificial data\r\nY = np.where(X > 0, 1, 0)  # create simple target\r\n\r\nlens = randint(low=1, high=time_steps, size=num_seqs)  # create lengths < time_steps (padding needed)\r\nseqs = [row[:row_len] for row, row_len in zip(X, lens)]  # artificially cut sequences\r\ntarget_seqs = [row[:row_len] for row, row_len in zip(Y, lens)]  # artificially cut target sequences\r\n\r\npadded_seqs = pad_sequences(seqs, padding='post', dtype='float32', maxlen=time_steps).reshape(num_seqs, time_steps, -1)\r\npadded_targets = pad_sequences(target_seqs, dtype='float32', padding='post', maxlen=time_steps).reshape(num_seqs, time_steps, -1)\r\n\r\nLSTM_SIZE = 16\r\nmodel = Sequential()\r\nmodel.add(Masking(mask_value=0., input_shape=(time_steps,1)))\r\nmodel.add(LSTM(LSTM_SIZE, return_sequences=True))\r\nmodel.add(Dense(1))\r\nmodel.add(Activation(\"sigmoid\"))\r\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"binary_crossentropy\"])\r\nprint(model.summary())\r\n\r\nmodel.fit(padded_seqs, padded_targets, batch_size=1024, epochs=10)\r\n```\r\n\r\nand observe that the loss and metric values are different:\r\n\r\n```\r\nTrain on 100 samples\r\nEpoch 1/10\r\n100/100 [==============================] - 4s 38ms/sample - loss: 0.3365 - binary_crossentropy: 0.6434\r\nEpoch 2/10\r\n100/100 [==============================] - 0s 253us/sample - loss: 0.3361 - binary_crossentropy: 0.6427\r\nEpoch 3/10\r\n100/100 [==============================] - 0s 260us/sample - loss: 0.3357 - binary_crossentropy: 0.6419\r\nEpoch 4/10\r\n100/100 [==============================] - 0s 271us/sample - loss: 0.3353 - binary_crossentropy: 0.6412\r\nEpoch 5/10\r\n100/100 [==============================] - 0s 266us/sample - loss: 0.3349 - binary_crossentropy: 0.6404\r\nEpoch 6/10\r\n100/100 [==============================] - 0s 250us/sample - loss: 0.3345 - binary_crossentropy: 0.6396\r\nEpoch 7/10\r\n100/100 [==============================] - 0s 275us/sample - loss: 0.3341 - binary_crossentropy: 0.6389\r\nEpoch 8/10\r\n100/100 [==============================] - 0s 247us/sample - loss: 0.3337 - binary_crossentropy: 0.6381\r\nEpoch 9/10\r\n100/100 [==============================] - 0s 196us/sample - loss: 0.3333 - binary_crossentropy: 0.6373\r\nEpoch 10/10\r\n100/100 [==============================] - 0s 217us/sample - loss: 0.3329 - binary_crossentropy: 0.6366\r\n```\r\n\r\n**Other info / logs**\r\n\r\nI already saw this closed issue: https://github.com/tensorflow/tensorflow/issues/25970 and the linked https://stackoverflow.com/questions/54802328/why-am-i-getting-different-values-between-loss-functions-and-metrics-in-tensorfl\r\nThat problem appears to be solved in my version, but my problem persists.\r\n \r\nI also saw this stackoverflow post: https://stackoverflow.com/questions/53808163/same-function-in-keras-loss-and-metric-give-different-values-even-without-regula\r\n\r\nThe solution doesn't work for me. I tried changing the import statements to\r\n\r\n```\r\nfrom keras.preprocessing.sequence import pad_sequences\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Masking, LSTM, Activation, Dense\r\n```\r\n\r\nbut the problem remains.\r\n", "comments": ["@Innuendo1975 ,\r\nIssue replicating for the given code in TF-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/00ae89e92a84ba5d3c606a1c8b535d86/34491.ipynb) of colab.Thanks!", "Assigning to Parvithra who is the expert for loss and metrics.", "Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/9690ce05dde0b89479d59f4cc5c5356b/34491.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/b46ba960d996e63f5919cc533bb46e41/34491-tf-nightly.ipynb). Please find the linked gist. Thanks!", "@Innuendo1975 Is this still an issue for you. I changed `mask_value` to 1.0, then the loss and metrics are matching as expected.\r\n\r\n`model.add(Masking(mask_value=1., input_shape=(time_steps,1)))`\r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/9bd03b507787953cbdbd3b36dd7cbeb9/34491-tf-nightly.ipynb) is a gist for reference. Thanks!\r\n\r\nPlease close the issue if this was resolved for you. thanks!", "Hi @jvishnuvardhan, yes it is still an issue. \r\n\r\nPoint is that 'mask_value' in the Masking layer should match the 'value' optional parameter of pad_sequences, which was left at its default 0.0 in the attached example. Otherwise, the Masking layer won't find any padding.\r\n\r\nNote that in your gist example you pass the mask_value=1.0, and the 1.0 is absent in the X data since you are using random float values from normal distribution (the probability to hit exactly 1.0 is zero). \r\n\r\nOn the other hand, 0. appears in the X data, since this is precisely what is set by pad_sequences with value=0.0; more in general, one would want to pad with values way out of the distribution (e.g. -1000), and set the same as mask_value, to avoid confusion between padding values and actual data.\r\n\r\nPlease follow our example to reproduce the issue. If both the 'value' parameter of pad_sequences and the 'mask_value' parameter of Masking are set at 1., the problem persists.\r\n\r\nTensorFlow version: 2.4.1 / 2.5.0-dev20210322 (nightly)", "Was able to reproduce the issue in TF 2.6.0-dev20210529,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/bdba4274c9de2e40012d366840e094f1/untitled86.ipynb)..Thanks !", "This is actually a symptom of a larger problem: the loss values are incorrect when sample weights are used because the total weighted loss is divided by the number of elements in the tensor (not divided by the total sample weight). When masking is applied, it appears that the sample weights are computed correctly according to the mask, it's just that those weights are not correctly used in the loss calculation.\r\n\r\nFor more details, see https://github.com/tensorflow/tensorflow/issues/34158#issuecomment-756813250 which includes compact examples of the issue.", "I was able to fix this issue by overwriting some code in `losses_util.py`. This seems to work in my use case, but I have **not** fully tested it. This patch is for TF 2.5.0. I put this code in a file then import it after importing tensorflow. It edits 3 lines so that the mean's denominator is set to be the total sample weight, not the total number of samples. Lines affected: https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/utils/losses_utils.py#L253-L320\r\n\r\nI think a more complete solution would define a new Reduction type in case people want to also use the old method. That would mimic what is done for the metrics, which has a `WEIGHTED_MEAN` option: https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/utils/metrics_utils.py#L44-L56\r\n\r\n```\r\nfrom tensorflow.python.keras.utils import losses_utils\r\nfrom tensorflow.python.keras.utils.losses_utils import ReductionV2, squeeze_or_expand_dimensions, _safe_mean\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.keras import backend\r\nfrom tensorflow.python.keras.engine import keras_tensor\r\nfrom tensorflow.python.ops.ragged import ragged_tensor\r\n\r\n\r\ndef reduce_weighted_loss(weighted_losses,\r\n                         sample_weight,  # !!!!!!!!!!!!!!!!!!!!THIS IS A CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n                         reduction=ReductionV2.SUM_OVER_BATCH_SIZE):\r\n  \"\"\"Reduces the individual weighted loss measurements.\"\"\"\r\n  if reduction == ReductionV2.NONE:\r\n    loss = weighted_losses\r\n  else:\r\n    loss = math_ops.reduce_sum(weighted_losses)\r\n    if reduction == ReductionV2.SUM_OVER_BATCH_SIZE:\r\n      loss = _safe_mean(loss, math_ops.reduce_sum(sample_weight))  # !!!!!!!!!!!!!!!!!!!!THIS IS A CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n  return loss\r\n\r\n\r\ndef compute_weighted_loss(losses,\r\n                          sample_weight=None,\r\n                          reduction=ReductionV2.SUM_OVER_BATCH_SIZE,\r\n                          name=None):\r\n  \"\"\"Computes the weighted loss.\r\n  Args:\r\n    losses: `Tensor` of shape `[batch_size, d1, ... dN]`.\r\n    sample_weight: Optional `Tensor` whose rank is either 0, or the same rank as\r\n      `losses`, or be broadcastable to `losses`.\r\n    reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to loss.\r\n      Default value is `SUM_OVER_BATCH_SIZE`.\r\n    name: Optional name for the op.\r\n  Raises:\r\n    ValueError: If the shape of `sample_weight` is not compatible with `losses`.\r\n  Returns:\r\n    Weighted loss `Tensor` of the same type as `losses`. If `reduction` is\r\n    `NONE`, this has the same shape as `losses`; otherwise, it is scalar.\r\n  \"\"\"\r\n  ReductionV2.validate(reduction)\r\n\r\n  # If this function is called directly, then we just default 'AUTO' to\r\n  # 'SUM_OVER_BATCH_SIZE'. Eg. Canned estimator use cases.\r\n  if reduction == ReductionV2.AUTO:\r\n    reduction = ReductionV2.SUM_OVER_BATCH_SIZE\r\n  if sample_weight is None:\r\n    sample_weight = 1.0\r\n  with backend.name_scope(name or 'weighted_loss'):\r\n    # Save the `reduction` argument for loss normalization when distributing\r\n    # to multiple replicas. Used only for estimator + v1 optimizer flow.\r\n    ops.get_default_graph()._last_loss_reduction = reduction  # pylint: disable=protected-access\r\n\r\n    if not isinstance(losses,\r\n                      (keras_tensor.KerasTensor, ragged_tensor.RaggedTensor)):\r\n      losses = ops.convert_to_tensor_v2_with_dispatch(losses)\r\n    input_dtype = losses.dtype\r\n\r\n    if not isinstance(sample_weight, keras_tensor.KerasTensor):\r\n      sample_weight = ops.convert_to_tensor_v2_with_dispatch(sample_weight)\r\n\r\n    # TODO(psv): Handle casting here in a better way, eg. if losses is float64\r\n    # we do not want to lose precision.\r\n    losses = math_ops.cast(losses, 'float32')\r\n    sample_weight = math_ops.cast(sample_weight, 'float32')\r\n    # Update dimensions of `sample_weight` to match with `losses` if possible.\r\n    losses, _, sample_weight = squeeze_or_expand_dimensions(  # pylint: disable=unbalanced-tuple-unpacking\r\n        losses, None, sample_weight)\r\n    weighted_losses = math_ops.multiply(losses, sample_weight)\r\n\r\n    # Apply reduction function to the individual weighted losses.\r\n    loss = reduce_weighted_loss(weighted_losses, sample_weight, reduction)  # !!!!!!!!!!!!!!!!!!!!THIS IS A CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n    # Convert the result back to the input type.\r\n    loss = math_ops.cast(loss, input_dtype)\r\n    return loss\r\n\r\n\r\nlosses_utils.compute_weighted_loss = compute_weighted_loss\r\n```", "@Innuendo1975 Could you please refer to the [comment ](https://github.com/tensorflow/tensorflow/issues/34491#issuecomment-869086784) above and try to execute your code using latest TF v2.6 .Please let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Please do not close this. This is a very important issue to me.", "I have created a new issue in keras-team/keras to track this since the keras code has been moved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34491\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34491\">No</a>\n", "The issue still exists and can be reproduced even with the version 2.6.\r\nI have also tried the patch above https://github.com/tensorflow/tensorflow/issues/34491#issuecomment-869086784 with no success.\r\n\r\n`Epoch 1/10\r\n1/1 [==============================] - 3s 3s/step - loss: 0.3452 - binary_crossentropy: 0.6601\r\nEpoch 2/10\r\n1/1 [==============================] - 0s 26ms/step - loss: 0.3448 - binary_crossentropy: 0.6592\r\nEpoch 3/10\r\n1/1 [==============================] - 0s 26ms/step - loss: 0.3443 - binary_crossentropy: 0.6584\r\nEpoch 4/10\r\n1/1 [==============================] - 0s 23ms/step - loss: 0.3439 - binary_crossentropy: 0.6575\r\nEpoch 5/10\r\n1/1 [==============================] - 0s 24ms/step - loss: 0.3434 - binary_crossentropy: 0.6566\r\nEpoch 6/10\r\n1/1 [==============================] - 0s 21ms/step - loss: 0.3430 - binary_crossentropy: 0.6558\r\nEpoch 7/10\r\n1/1 [==============================] - 0s 19ms/step - loss: 0.3425 - binary_crossentropy: 0.6549\r\nEpoch 8/10\r\n1/1 [==============================] - 0s 26ms/step - loss: 0.3421 - binary_crossentropy: 0.6540\r\nEpoch 9/10\r\n1/1 [==============================] - 0s 29ms/step - loss: 0.3416 - binary_crossentropy: 0.6532\r\nEpoch 10/10\r\n1/1 [==============================] - 0s 29ms/step - loss: 0.3411 - binary_crossentropy: 0.6523`"]}, {"number": 34490, "title": "Tensorflow 1.6 without SSE4.1", "body": "Hello,\r\nIt seems that Tensorflow 1.6 and later versions are configured to require SSE4.1 instructions by default. Some processors (like AMD Phenom II x4 965) do not have these instructions.\r\n\r\n**How to configure / rebuild Tensorflow for Python 3 to run on CPUs without SSE4.1?**\r\n", "comments": ["You'll need to build TensorFlow from source following the instructions here:\r\n\r\nhttps://www.tensorflow.org/install/source\r\n\r\nNote the table here: https://www.tensorflow.org/install/source#cpu, on what version of bazel, python and gcc to use for TensorFlow 1.6\r\n\r\n", "@josephernest As mentioned by William above, you can try building it from source. Also I would recommend you using the latest version of Tensorflow as for the versions less than 1.11, the support is minimal. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34490\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34490\">No</a>\n", "@gowthamkpr @wdirons Thank you for your answers. Even if there is a HOWTO, this seems to be quite like a long task (installing the right `bazel` (?), the right gcc which is probably not the one I have currently installed, configure the build settings, probably a full day or two when you're new to it, etc.).\r\n\r\nWould you have a TF 1.14 build with SSE4.1 off? I think it could be useful for many people using such processors. \r\n\r\nOr maybe are there some unofficial builds available somewhere for this?\r\n(A bit similar to what Gohlke does: https://www.lfd.uci.edu/~gohlke/pythonlibs/, but for Linux 64 in my case.)\r\n\r\nThank you very very much if you have this!", "There are plans for an on-demand build service document here: https://github.com/tensorflow/community/blob/master/rfcs/20190225-tf-on-demand.md, but details like who is providing the pool of machines to build on is not worked out. \r\n\r\nI'm not aware of anyone providing unofficial builds without SSE4.1. ", "I should also point out that if you use a devel docker container then all the prereqs should already be installed\r\n\r\n```\r\ndocker pull tensorflow/tensorflow:1.11.0-devel-py3\r\nor\r\ndocker pull tensorflow/tensorflow:1.6.0-devel-py3\r\n```", "> There are plans for an on-demand build service document here: \r\n \r\nOh this would be really great... Is there a waiting list for this service, or could my use case (TF 1.14 without SSE4.1) serve as a demo case for this service? :-)\r\n\r\nIf you had a little spare time on a machine to run such a build in background, I would be really interested @wdirons if we can send the .whl via file transfer ;) (my email is on my profile)\r\n\r\n> I should also point out that if you use a devel docker container then all the prereqs should already be installed\r\n\r\nWould you have a list of commands in this example:\r\n\r\n    docker pull tensorflow/tensorflow:1.14.0-devel-gpu(?)-py3   # i'd like GPU CUDA support for Nvidia Gefore GTX\r\n    docker run ...\r\n    somethingconfigure_SSE4.1off\r\n    somethingbuild\r\n  \r\nin order to get a .whl at the end that I can export outside of the container and use in my normal project?\r\n\r\nThanks in advance :)"]}, {"number": 34489, "title": "how to build tensorflow lite static library for android use ndk?", "body": "i check all issue about build static library with ndk for android c++ code, but no one give a complete guide.  and different version from r1.12 r.113 r1.15 has very different tensorflow/lite/tools/make   build_**.sh and target/**.inc, any guide for me to use cmake or bazel to build a libtensorflow-lite.a with armv-8a and armv7a on mac or linux machine.", "comments": ["Hi mlinxiang, thanks for using tensorflow lite.\r\n\r\nFor your use case, could you please try to follow https://www.tensorflow.org/lite/guide/android (and https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/android/README.md)? That should create a standard image classification model.\r\n\r\nThanks.", "@jianlijianli i want to build a libtensorflow-lite.a to compile my c++ code which will be used by android through jni.  i find one way to do this, but need a lot change about Makefile and target/***.inc, and a lot env settings.  i think maybe this could be more official guide for new people. later i can share my steps to do so", "Is there a reason you need a static library, rather than a shared library? We're close to enabling C API usage from the nightly AAR builds, which you can integrate into your JNI gradle project.", "@jdduke my project pipeline is  android app --> jni --> c++ sdk,  the c++ sdk code and  jni<use ndk> compile together as a so<build by cmake+ndk not in android studio>, which can be used by any android project, and the c++ sdk code also can compile to a \".a\" static library used by ios.  so all the thirdparty librarys are use static library link.  the tensorflow/lite/tools/make contain all arch, but not contain cross compile toolchain such as ndk.\r\n\r\ni build the static library by modify makefile and target inc use ndk toolchain and generate static library.  but i still have some problem with how to enable gpu support.", "@mlinxiang Hi, I also want to build tensorflow lite static library, but fail, can you share your steps how to build tendorflow lite \".a\" static library ?", "The problem with .so library is that is has lots of symbols which are not needed. Specifically tensorflow lite if compiled as .so library is ~50MB in size vs //tensorflow/lite/java:libtensorflowlite_jni.so which is ~1MB. I would guess *_jni.so library is so much smaller due to stripping of unused sections and symbol info. Distributing an app with 50MB lib is impractical.", "Hi @yavol , which platform are you targeting? We do strip symbols for Linux/Mac when you build `//tensorflow/lite:libtensorflowlite.so` and `//tensorflow/lite/c:libtensoflowlite_c.so` via bazel. The shared libraries for those should be ~1-2MB when you build with `-c opt`.", "@linyongye sorry for late response, please use this link to download the scripts : https://pan.baidu.com/s/12Y1ftf2BjIWw1d-WyjaZOQ  access code: km9h \r\ncontain build_armv7a_lib.sh build_armv8a_lib.sh, and Makefile.andriod,  please set env parameter ANDROID_NDK , the min version is ndk18,  and some tensorflow root WORKSPACE add follow config:\r\nandroid_sdk_repository(\r\n148    name = \"androidsdk\",\r\n149    api_level = 26,\r\n150    build_tools_version = \"26.0.2\",\r\n151    path = \"/Users/***/Library/Android/sdk\",\r\n152 )\r\n153\r\n154 android_ndk_repository(\r\n155    name=\"androidndk\",\r\n157    path=\"/Users/****/Library/Android/sdk/ndk/20.0.5594570\",\r\n159    api_level=24\r\n160 )\r\n\r\n tensorflow 1.15   tensorflow/lite/delegates/gpu/gl/kernels/test_util.h  need to delete\r\n-#include <gmock/gmock.h>\r\n-#include <gtest/gtest.h>\r\n\r\ncompile need some runtime generate head file, please first use bazel to compile so, then copy to source file dir:  bazel build --cxxopt=--std=c++11 -c opt --config android_arm64 tensorflow/lite/delegates/gpu:gl_delegate\r\nthe include file need to scan the tensorflow/lite source dir get all *.h file, and the download flatbuffer has it's prebuilded include, copy this to flatbuff root dir\r\ntensorflow 1.15 need as follow :\r\ntensorflow/lite/delegates/gpu/gl/common_generated.h\r\ntensorflow/lite/delegates/gpu/gl/compiled_model_generated.h\r\ntensorflow/lite/delegates/gpu/gl/metadata_generated.h\r\ntensorflow/lite/delegates/gpu/gl/workgroups_generated.h\r\n\r\nand add a soft link to ndk root dir as  name \"ndk\" at tensorflow root dir\r\n", "@mlinxiang , the link you shared is not working. Is there any other way you can share the scripts please?\r\n\r\nWhen I try to build static lib with bazel, it fails building flatbuffers.\r\n\r\nThanks! ", "@knightfork google drive link: https://drive.google.com/open?id=1lBhZ4HbcQ0XAwVC2lyiTMdKo1UT2S9fw  \r\nas tensorflow2.2 the download_dependence.sh flatbuffer version maybe low need to change the download url version in the .sh scripts", "@mlinxiang , thanks a lot!", "@mlinxiang hi build tensorflow lite static lib for android base ndk  standalone toolchain base on tensorflow2.3\r\n\r\nhttps://github.com/sunfrank1987/build_tensorflow_lite", "Note that we've committed an experimental version of CMake support @ https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/cmake. This should make it easier to generate the static library, when necessary, or to integrate directly into your CMake-based project/toolchain.", "@jdduke https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/cmake, I have generated static files,but when I integrate them into cmakelist,all  undefined reference to `TfLiteModelCreate'   undefined reference to  undefined reference to\r\n\r\n", "> Note that we've committed an experimental version of CMake support @ https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/cmake. This should make it easier to generate the static library, when necessary, or to integrate directly into your CMake-based project/toolchain.\r\n\r\nWell, yes and no: this works fine as long as you are using it directly from another CMake build; if you just want to build a one-off static library to use in a different build system (Make, in my case), you still have to track down and build all the transitive dependencies, which are missing. \r\n\r\nAmusingly, the Make-produced static library for e.g. Linux doesn't have this issue and works fine, but the stuff in tools/make seems to support every sort of mobile arm platform *except* for Android.\r\n\r\n(Does Bazel even support the ability to produce standalone static libraries? Last time I checked, it didn't, so I haven't checked for TFLite...)\r\n\r\n\r\n\r\n\r\n", "> Does Bazel even support the ability to produce standalone static libraries? Last time I checked, it didn't, so I haven't checked for TFLite...\r\n\r\nSadly, no, at least not that I'm aware of. There are some ways to traverse the transitive cc_library deps w/ Bazel, and manually generate a static library. We've considered adding a helper rule for doing so, but it's somewhat brittle. ", "FWIW, the [instructions](https://www.tensorflow.org/lite/guide/android) for using Bazel to build a **dynamic** library for Android don't work either...\r\n\r\n```\r\n$ bazel build -c opt --config=android_arm64 //tensorflow/lite:libtensorflowlite.so\r\n...stuff...\r\nERROR: /private/var/tmp/_bazel_srj/5404487c5e532e1d3fa71c94c3e65bf0/external/local_config_cc/BUILD:41:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'\r\nERROR: Analysis of target '//tensorflow/lite:libtensorflowlite.so' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\n```", "@steven-johnson to cross compile tensorflow lite for a specific `ARCH` and platform type, you need to set `WORKSPACE`. \r\n\r\nThis can be done using `./configure` under tensorflow repo, as mentioned [here](https://www.tensorflow.org/lite/guide/build_android#configure_workspace_and_bazelrc) \r\n\r\nNow run following build command: \r\n```\r\nbazel build -c opt --config=android_arm64 //tensorflow/lite:libtensorflowlite.so\r\n```\r\n\r\nFor me its creates the shared object but when i run it on Android phone, i get following error: \r\n\r\n```\r\n2020-12-26 11:36:14.960 7288-7288/com.example.ndkcamlibs E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.ndkcamlibs, PID: 7288\r\n    java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"_ZN6tflite4impl18InterpreterBuilderclEPNSt6__ndk110unique_ptrINS0_11InterpreterENS2_14default_deleteIS4_EEEE\" referenced by \"/data/app/com.example.ndkcamlibs-XB6q0orTTyV629Xo1BR27w==/base.apk!/lib/arm64-v8a/libndksamplecam.so\"...\r\n        at java.lang.Runtime.loadLibrary0(Runtime.java:1016)\r\n        at java.lang.System.loadLibrary(System.java:1657)\r\n        at com.example.ndkcamlibs.MainActivity.<clinit>(MainActivity.java:293)\r\n        at java.lang.Class.newInstance(Native Method)\r\n        at android.app.Instrumentation.newActivity(Instrumentation.java:1190)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2837)\r\n```\r\n\r\nI have copied `libtensorflowlite.so` to `jniLibs` folder in Android studio. \r\n\r\n**EDIT: 05-Jan-2021**\r\n\r\nMy bad, when i copied all the tensorflow header file `.h` files for building, i copied `.cc` file too. Only copy the .h files from tensorflow/lite/ folder for compilation and not all the files, and it will build just fine. ", "@milinddeore I gave up on building that target at all, and settled on using the C-API-only variant (`//tensorflow/lite/c:tensorflowlite_c`), which builds reliably under Bazel.", "@steven-johnson But just in case you would like to its super easy, i made mistake when i copied header file, i mistakenly copied `.cc` file too. I have done a edit to my previous comment. (_Just in case you would like to try._)", "I've been trying to build static.  There are numerous problems so far mostly with its inability to deselect alot of Ruy library code which becomes a dependency even when you compile without it:\r\n\r\nThe following is from https://github.com/tensorflow/tensorflow/issues/52373\r\n\r\ncmake ../tensorflow/lite -DTFLITE_ENABLE_RUY=OFF -DTFLITE_ENABLE_NNAPI=OFF -DTFLITE_ENABLE_GPU=OFF -DTFLITE_ENABLE_XNNPACK=OFF -DTFLITE_ENABLE_MMAP=OFF\r\nmake\r\n\r\nIt proceeds to compile RUY code anyway, and it produces a .a file but it has these dependencies which shouldn't be there. "]}, {"number": 34488, "title": "ListWrapper does not support insert method for nested lists of layers", "body": "**System information**\r\n- Have I written custom code: **yes**\r\n- OS Platform and Distribution: **Linux Ubuntu 18.04**\r\n- Mobile device: **N/A**\r\n- TensorFlow installed from: **binary**\r\n- TensorFlow version: **2.0.0**\r\n- Python version: **3.7.5**\r\n- Bazel version: **N/A**\r\n- GCC/Compiler version: **N/A**\r\n- CUDA/cuDNN version: **10.1 / 7.6.4**\r\n- GPU model and memory: **GeForce GTX 1070, 8 GB**\r\n\r\n**Describe the current behavior**\r\nList of layers added to another `list` with the `insert` method are ignored by the model; it works fine with the `append` method.\r\n\r\nThis seems to be a problem with ListWrapper.\r\n\r\n**Describe the expected behavior**\r\nAll `list` methods should be supported for defining nested list of layers.\r\n\r\n**Code to reproduce the issue**\r\n\r\nMinimal example:\r\n\r\n```python\r\nclass MyModel(tf.keras.Model): \r\n    def __init__(self, **kwargs): \r\n        super().__init__(**kwargs) \r\n        self.conv1 = [] \r\n        self.conv1.append([tf.keras.layers.Conv2D(8, 3, name=\"conv1\")]) \r\n        self.conv2 = [] \r\n        self.conv2.insert(0, [tf.keras.layers.Conv2D(16, 3, name=\"conv2\")]) \r\n\r\n    def call(self, inputs): \r\n        x = inputs \r\n        x = self.conv1[0][0](x) \r\n        x = self.conv2[0][0](x) \r\n        return x\r\n\r\n\r\nm = MyModel()\r\nm.build((None, None, None, 3))\r\nm.summary()\r\n\r\nfor w in m.trainable_weights: \r\n    print(w.name)\r\n```\r\noutput is:\r\n```\r\nModel: \"my_model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv1 (Conv2D)               multiple                  224       \r\n=================================================================\r\nTotal params: 224\r\nTrainable params: 224\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n\r\nconv1/kernel:0\r\nconv1/bias:0\r\n```\r\nNote that `conv2` has been ignored.\r\n\r\n**Other info / logs**\r\n\r\nN/A", "comments": ["I have tried on colab with TF version 2.0 ,2.1.0-dev20191124 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/6a4e1d47c491f31d5f0ff5c817ab8263/untitled407.ipynb). Thanks!", "Assigning to Allen who is the owner of ListWrapper.", "Checkpointing doesn't support insert, so it'll throw an exception on save. The Layer is added to the list, though. Keras probably still wants to collect layers through lists in this case. Hopefully someone who works on Keras can iron out that policy. Scott, can you find an owner?", "Was able to replicate the issue in TF 2.6.0-dev20210529,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/fd7e0cd798fc9e635285b0f8d45395c0/untitled85.ipynb)..Thanks !", "Was able to replicate the issue in TF 2.6, please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/1c5283163f2a31d9971487cb27635654/untitled85.ipynb#scrollTo=ZSbnQvUSYxvF) Thanks !", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34488\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34488\">No</a>\n"]}, {"number": 34487, "title": "Training deep learning model on a cluster of machines", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["Since I'm facing a lack of GPU resources on my PC for training a deep learning model, I thought of using all my friend's PCs as resources in combined form to train the model. So, is there any way to train the model using all machines as a cluster in which each machine may or may not have GPUs?\r\nPlease share any useful link or medium to tackle this issue.", "Apologies for the delay in response. Perhaps you may want to use distributed tensorflow in this case.\r\nSee https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md to  know more. Thanks!"]}, {"number": 34486, "title": "Relax the check for state_size", "body": "The behaviour of `hasattr` is to evaluate the state_size member. In the case of `tfa.seq2seq.AttentionWrapper`, that is a `@property` attribute that is built at graph runtime after calling `setup_memory`, thus `hasattr` returns an error when using AttentionWrapper with dynamic memories.\r\n\r\nMore details: https://github.com/tensorflow/addons/issues/680", "comments": ["Adding @qlzh727 who is an expert here", "@pavithrasv Can you please see the comment of @qlzh727 in https://github.com/tensorflow/addons/issues/680#issuecomment-556121857 ?", "@qlzh727 Can you please take a look on this PR? Thanks!", "Thanks for the change and sorry for the long wait.", "Sure, I'll update it later this week, currently traveling back to Ireland after a holiday break.", "Please take a look, @qlzh727 \r\nI am not yet in my lab, so I could not test the change properly"]}, {"number": 34485, "title": "Tensorflow 2.0 does not use GPU, while Tensorflow 1.15 does", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\n\r\n- TensorFlow installed from (source or binary):\r\nbinary (pip install tensorflow==2.0 vs pip install tensorflow==1.15\r\n\r\n- TensorFlow version (use command below):\r\n2.0 and 1.15\r\n\r\n- Python version:\r\nPython 3.6.5\r\n\r\n- CUDA/cuDNN version:\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart.so.10.1.243\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130\r\n\r\n- GPU model and memory:\r\nNvidia Quadro T2000\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nimport tensorflow as tf\r\ntf.test.is_gpu_available()\r\nthis gives False for Tensorflow 2.0 and True for Tensorflow 1.15\r\n\r\n**Describe the expected behavior**\r\nIt should be True for both\r\n\r\n**Code to reproduce the issue**\r\nimport tensorflow as tf\r\ntf.test.is_gpu_available()\r\n\r\n**Other info / logs**\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Nov 21 14:40:53 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 430.26       Driver Version: 430.26       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro T2000        Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   55C    P8     1W /  N/A |    654MiB /  3914MiB |      5%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      2059      G   /usr/lib/xorg/Xorg                           331MiB |\r\n|    0      2237      G   /usr/bin/gnome-shell                         146MiB |\r\n|    0      2606      G   ...quest-channel-token=3079153680037509795   172MiB |\r\n|    0      4718      G   /snap/pycharm-community/167/jbr/bin/java       2MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart.so.10.1.243\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.0.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/derk/playground/venv/lib/python3.6/site-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 6, 5, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n", "comments": ["In TensorFlow 2.0, if you want to use GPU, you need to `pip install tensorflow-gpu==2.0`\r\n\r\nSome history:\r\nFor a long time the `tensorflow` pip package was CPU only and the `tensorflow-gpu` pip package was GPU only. This was still true when TensorFlow 2.0 was released.\r\n\r\nTensorFlow 1.15.0 was released after TensorFlow 2.0, and that was the first package where the `tensorflow` pip package could work with either GPU or CPU.\r\n\r\nTensorFlow 2.1.0, when released, should work the same as TensorFlow 1.15.0 does, but for now to use GPU with TensorFlow 2.0, you need to install `tensorflow-gpu`", "@dmus, \r\nIn Tensorflow 1.15.0, pip package will by default include GPU support (same as tensorflow-gpu now) for the platforms we currently have GPU support (Linux and Windows).\r\nIn Tenosrflow 2.0, for GPU support use `pip install tensorflow-gpu==2.0.0.`\r\nFor information Please refer the [release notes](https://github.com/tensorflow/tensorflow/releases). Thanks! ", "@dmus, Is this still an issue. Thanks!", "Closing since its resolved. Please feel free to open if issue still persists. Thanks!", "tensorflow 1.12 ok for gpu.\r\n>>> print( tf.__version__)\r\n1.12.0\r\n>>> tf.test.is_gpu_available()\r\nTrue", "> In TensorFlow 2.0, if you want to use GPU, you need to `pip install tensorflow-gpu==2.0`\r\n> \r\n> Some history:\r\n> For a long time the `tensorflow` pip package was CPU only and the `tensorflow-gpu` pip package was GPU only. This was still true when TensorFlow 2.0 was released.\r\n> \r\n> TensorFlow 1.15.0 was released after TensorFlow 2.0, and that was the first package where the `tensorflow` pip package could work with either GPU or CPU.\r\n> \r\n> TensorFlow 2.1.0, when released, should work the same as TensorFlow 1.15.0 does, but for now to use GPU with TensorFlow 2.0, you need to install `tensorflow-gpu`\r\n\r\nbuild from source show wrong compatible type=> https://www.tensorflow.org/install/source#gpu . change **pip install tensorflow==2.0** to pip **install tensorflow-gpu==2.0.0.**"]}, {"number": 34484, "title": "Tests for minimum/maximum quantization quantization", "body": "Problem description:\r\nMINIMUM and MAXIMUM operations were not quantized properly. Specifically, the problem was that only one of the inputs was quantized while another one was left in the original data type. Because of this reason, TFLite interpreter was failing to prepare since quantization params of ops did not match.\r\n\r\nProblem cause:\r\nMINIMUM and MAXIMUM operators were created in the way that they only had one input, not two. Hence when looping through inputs while quantizing them, only one of two inputs was quantized.\r\n\r\nProblem fix:\r\nChange the definition of the aforementioned properties.\r\n\r\nThis patch contains fixes for the problem described above.\r\n\r\n- Properties of MINIMUM and MAXIMUM operators were altered to have\r\ntwo inputs rather than one. This is safe since both 1.* and 2.* branches\r\nhave only two inputs for these ops\r\n- Test suite for testing Minimum and Maximum ops quantization is added\r\n- Two small binaries have been added for testing purposes\r\n\r\nThis is an updated version of the [PR](https://github.com/tensorflow/tensorflow/pull/32582). The old PR was raised by KonstantinARM, who has left ARM.", "comments": ["@wwwind Can you please resolve conflicts? Thanks!", "Comment from Anton: \r\n\"Hi @suharshs. Thanks for checking this PR. Can you please review the PR #34484 we raised to replace this one? - It extends your fix to cover for the case of inputs with different quantization parameters, and includes the C++ tests to cover both aspects of the problem.\"\r\n\r\n\r\nReply:\r\nI think the current code should apply rescales on the inputs correctly as it does for Concat operations. The combination of arbitrary_inputs and restrict_same_input_output_scale should be sufficient AFAICT. Does that make sense, or am I missing something?\r\n\r\n", "The c++ tests would still be welcome additions though. Thanks!", "Hi Suharsh, \r\n\r\nIn our original fix we kept the fixed number of inputs to 2, but, yes, arbitrary_inputs variable solves the issue. I removed traces of the original fix and left only C++ tests.\r\nCould you please review them ?\r\n\r\nThanks!", "@wwwind Could you please address Ubuntu Sanity errors? Thanks!", "Hi @suharshs, could you please take a look again ? I had to push a fix for the sanity check - there was a small complaint from the buildifier.\r\nThanks!"]}]