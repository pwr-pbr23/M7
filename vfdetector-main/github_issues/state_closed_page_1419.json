[{"number": 10407, "title": "Fix windows tests", "body": "", "comments": ["MacOS failure is an infra issue.\r\nMerging."]}, {"number": 10406, "title": "Build Error gcc: error: \u201d-DEIGEN_USE_VML\u201d: No such file or directory", "body": "$ bazel build --config=mkl --copt=\u201d-DEIGEN_USE_VML\u201d -c opt //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: /home/mstf/workspace/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.\r\nWARNING: /home/mstf/workspace/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.\r\nINFO: Found 1 target...\r\nERROR: /home/mstf/.cache/bazel/_bazel_mstf/ac8273660b2bd94e42901d7fff45bcd5/external/grpc/BUILD.bazel:71:1: C++ compilation of rule '@grpc//:gpr' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 34 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ngcc: error: \u201d-DEIGEN_USE_VML\u201d: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 119.271s, Critical Path: 14.01s\r\n", "comments": ["Can you share the solution for this error? \r\n**gcc: error: \u201d-DEIGEN_USE_VML\u201d: No such file or directory**", "I have the same error.", "--copt=-DEIGEN_USE_VML\r\nWithout using the quotation marks, I have compiled the code with no this error.\r\nPlease let me know if it's right.", "I have the same error too, anyone has solutions?\r\n@mstfldmr @mrsabhar @Paeans ", "I think I ended up updating gcc. It was one of those situations where I did a lot of stuff, and at the end, it wasn't clear what fixed it.", "I've had the same problem, copied the build command from an [intel blog article](https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture). As it turns out it really was the quotation marks, as @Paeans pointed out. Simply replace `\u201d` (stylish quotation marks) with `\"` (simply double ticks)."]}, {"number": 10405, "title": "Operation functions return operations, not Tensors", "body": "The way the return value for many TensorFlow operations is described in the documentation is a potential (and actual, from experience) source of confusion. \r\n\r\nFor example the docs for `tf.acos` say (like many ops): \r\n\r\n> \"Returns: A Tensor. ...\" \r\n\r\nBut that's not true, is it: Tensors are graph *edges* (pipes through with values flow), while `tf.acos` is a graph *node* (an operation that needs to be executed to put a value into the pipe). Right?\r\n\r\nShouldn't the documentation say that these functions return _operations_, which, when evaluated, produce a Tensor? (And if not why does the TensorFlow documentation say that these operations return Tensors?)", "comments": ["`tf.acos` does not return a node in the graph, it creates a node in the graph and returns a [`tf.Tensor`]\r\n(https://www.tensorflow.org/api_docs/python/tf/Tensor) Python object that is a handle to the output of that operation. For example, in the snippet:\r\n\r\n```python\r\nx = tf.acos(tf.constant(1))\r\ny = tf.cos(x)\r\n```\r\n\r\nThe `x` refers to the value (the \"Tensor\") that is flowing on the \"edges\" between the \"acos\" and \"cos\" nodes.\r\n\r\nHope that helps.", "@asimshankar \u2014 Yes, it does help. \r\n\r\nI think what's confusing me is the notion of running or evaluating operations. But now I think I see. \r\n\r\nFunctions like `tf.cos` add a node to the graph and return an edge, and when I `run` or `eval` I'm tugging on the edge and _that_, rather than directly \"running\" the operation, is what invokes the operation (and so on for any edges it tugs on in turn).\r\n\r\nIn effect, if you will, I don't make the machinery go by pressing a button on the op (which is what I naively expect), but by drawing from the pipe that comes out of it; and its that action that fills all the pipes in the graph with values. That's why I'm handed back a pipes (`tf.Tensor`): everything operates by tapping them, not the operations they emerge from.\r\n\r\n(I'm still a bit [puzzled about this](https://stackoverflow.com/q/44335033/656912) though \u2014 and [the answer](https://stackoverflow.com/a/44335999/656912) given \u2014 which is what sent me down the road to this misunderstanding in the first place.)"]}, {"number": 10404, "title": "Branch 155393864", "body": "Pushing internal commits.", "comments": ["The run with --expunge is messing with other runs,\r\n'external/local_config_cuda/cuda/lib/libcudnn.so.6' is a dangling symbolic link.\r\n@gunan, should we merge https://github.com/tensorflow/tensorflow/pull/10307? Or should we only add --expunge first?", "Ugh, this is bad.\r\nWill chat offline about the issue.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins Test this, please.", "@tensorflow-jenkins Test this, please."]}, {"number": 10403, "title": "contrib/learn/python/learn_io/data_feeder.py : \"y_is_dict\" instead of \"x_is_dict\" at line 325", "body": "version r1.2\r\nI was trying to create a regressor with X as dict, and y as ndarray. Got error that at line mentioned in title saying ndarray object as no attribute items.\r\nWhen I checked the code, it look like\r\n\r\n```\r\n    x_is_dict, y_is_dict = isinstance(x, dict), y is not None and isinstance(y, dict)\r\n    if isinstance(y, list):\r\n      y = np.array(y)\r\n\r\n    self._x = dict([(k, check_array(v, v.dtype)) for k, v in list(x.items())\r\n                   ]) if x_is_dict else check_array(x, x.dtype)\r\n    self._y = None if y is None else \\\r\n      dict([(k, check_array(v, v.dtype)) for k, v in list(y.items())]) if x_is_dict else check_array(y, y.dtype)\r\n```\r\nThis **`x_is_dict`** in the last line should be `y_is_dict`!! Or do I have to send both as dict??", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Our apologies. I think @navya-xx is correct that the x_is_dict on the last line should be a y_is_dict.", "No worries. I am glad that its been solved now.. Thanks.. :)", "I think this issue has been fixed in PR #12562 several weeks ago?", "Yes, thanks for reminding.. i will close this issue now.. ", "Sorry.. I guess someone from admin has to close it.. I don't have the rights.. ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Let me close this issue as it has been fixed."]}, {"number": 10402, "title": "fix quotes in example code from \u201c to \"", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for fixing this bug. I came across it last week as well!\r\n\r\nIs there a reason why this wasn't deployed to the main webpage `https://www.tensorflow.org/install/install_c` already? (See screenshot of what I see right now + trying to copy/past the current code from the webpage causes compile errors due to the wrongly formated quote.)\r\n\r\n<img width=\"1535\" alt=\"screen shot 2017-07-02 at 22 02 48\" src=\"https://user-images.githubusercontent.com/191719/27773107-954891a8-5f72-11e7-80b1-4afa399ca9f8.png\">\r\n", "@jviereck The website has not been deployed after the PR. It is usually deployed during each release. Sorry about the inconvenience.", "Hi @caisq,\r\n\r\n> @jviereck The website has not been deployed after the PR. It is usually deployed during each release. Sorry about the inconvenience.\r\n\r\nNo worries. Has things been changed to deploy the documentation at each commit? I think this is already done for parts of the API documentation, which is synced to the master branch?\r\n\r\nCheers\r\n\r\n\\jv"]}, {"number": 10401, "title": "Fix unbatch for Datasets with multiple elements", "body": "The current implementation of `Dataset.unbatch` gives an error for datasets with multiple data elements, e.g.\r\n\r\n``` python\r\ndata = [tf.range(10) for _ in range(3)]\r\ndata = tf.contrib.data.Dataset.from_tensor_slices(data)\r\ndata = data.batch(2)\r\ndata = data.unbatch()\r\n```\r\nthe `data.unbatch()` call gives error\r\n```\r\nTypeError: from_tensor_slices() takes 1 positional argument but 3 were given\r\n```\r\n\r\nThis is because `unbatch` tries to call `map` with `Dataset.from_tensor_slices` as the function.  The map function is expected to take multiple arguments as input, where each argument is one part of the data structure.  However, `Dataset.from_tensor_slices` expects the whole data structure to be passed as the first argument.\r\n\r\nThis fix changes `unbatch` so that `Datset.from_tensor_slices` is called correctly.\r\n\r\nHowever, this does have one undesirable consequence, which is that datasets without structure are converted to lists of length one, e.g.\r\n```python \r\ndata = tf.range(10)\r\ndata0 = tf.contrib.data.Dataset.from_tensor_slices(data)\r\nprint(data0.output_shapes)\r\ndata1 = data0.batch(2)\r\ndata2 = data1.unbatch()\r\nprint(data2.output_shapes)\r\n```\r\nIdeally we'd expect `data0` to be equivalent to `data2`.  But `data0` will produce scalars, and `data2` will produce elements with shape `(1,)`.\r\n\r\nI don't think there's any way around this, because there is no way within the `map` function to tell if the function was called with a single element as input, or a list with length one.  Both will just appear as a single argument to the `map` function.\r\n\r\nSo a more thorough fix might require changing how `Dataset.map` works, but in the meantime this behaviour seemed the best option.  @mrry might be able to comment on whether that larger change would be worthwhile (or if he sees a better fix for this issue).", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please.", "Jenkins, test this please"]}, {"number": 10400, "title": "add metric op", "body": "This pull request is aimed to create a metric_op for TF-Slim learning to be able to show the streaming metrics(accuracy,...) for the training phase.\r\nThe op reset the total and count local variables defined by streaming_metric(if it is streaming_accuracy ,...) per each epoch of training so the accumulative accuracy per epoch can be represented.", "comments": ["Can one of the admins verify this patch?", "Please revert all the indentation changes, use 2 spaces per indentation level.", "@astorfi could you rebase and push again?", "@drpngx There were some issues regarding my pool request. I am trying to solve them yet.", "Ping for @astorfi ... "]}, {"number": 10399, "title": "export step of Experiment fails in python 3.5 (likely due to string-type mismatch)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yep\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX\r\n- **TensorFlow installed from (source or binary)**: binary 1.1.0\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**:\r\ngit clone https://github.com/amygdala/tensorflow-workshop\r\npython tensorflow-workshop/workshop_sections/wide_n_deep/widendeep/model.py\r\n\r\n\r\n### Describe the problem\r\n\r\nWhen running Experiment stuff, it works fine in 2.7 but breaks in 3.5. From what I can tell, it's happening in gc.py, which is being called _after_ the model has been exported and everything is done. \r\n\r\nMy best guess from looking at similar bugs on SO is that there's a mismatch somewhere between 'byte' strings and python strings, possibly because the base folder path of the export is generated (e.g. /exports/Servo/123456789/saved_model.py), so the somehow the concat isn't happy with this.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nFile \"/Users/yufengg/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/utils/gc.py\", line 202, in get_paths\r\n    p = parser(Path(os.path.join(base_dir, r), None))\r\n  File \"/Users/yufengg/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/posixpath.py\", line 89, in join\r\n    genericpath._check_arg_types('join', a, *p)\r\n  File \"/Users/yufengg/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/genericpath.py\", line 145, in _check_arg_types\r\n    raise TypeError(\"Can't mix strings and bytes in path components\") from None\r\nTypeError: Can't mix strings and bytes in path components\r\n", "comments": ["Thanks for the report @yufengg . Can you include the full stracktrace and ideally a reproducible test case?", "Same problem here. All you need in your code is to include an export strategy and the checkpoints garbage collector will raise an exception:\r\n\r\nsaved_model_export_utils.make_export_strategy(\r\n            model.serving_input_fn,\r\n            default_output_alternative_key=None,\r\n            exports_to_keep=1\r\n", "Same here, exactly the same code that @martin-gorner pointed out is causing the crash.\r\n\r\nUsing TF 1.2.1 (binary) on Python 3.6.1 on Linux.", "Does https://github.com/tensorflow/tensorflow/pull/9719/files fix this? ", "@xiejw ,\r\n\r\nI have also come across this issue; Do you know which ver. of TF is that https://github.com/tensorflow/tensorflow/pull/9719/files is released in (or which ver is this going to be released in?)? ", "I did not see r1.2 branch has this change. So, it should go to v1.3\r\n\r\nr1.2\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/learn/python/learn/utils/gc.py", "@xiejw ,\r\n\r\nCorrect, this is now working in TF 1.3...however, I can only get the CPU ver of TF running, the GPU ver of 1.3 throws the pywrap error, although the GPU ver for TF 1.2 works like a charm....strange, that's what is it... ", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 10398, "title": "[Docs] \"arg_scope\" overrides \"defined in\" paths", "body": "I found that some ops have incorrect \"defined in\" paths in the [`contrib.layers` docs](https://www.tensorflow.org/api_docs/python/tf/contrib/layers).\r\n\r\nExamples: `avg_pool2d, batch_norm, bias_add, conv2d, conv2d_in_plane, conv2d_transpose, dropout, flatten, fully_connected, layer_norm, one_hot_encoding, separable_conv2d, softmax`\r\n\r\nIt always uses [`tensorflow/contrib/framework/python/ops/arg_scope.py`](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/framework/python/ops/arg_scope.py) which is clearly outright wrong.\r\n\r\nThis seems to be the case for all ops defined in [`tensorflow/contrib/layers/python/layers/layers.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py) which are annotated with `@add_arg_scope`", "comments": ["@dr4b Our friend has identified has at least thirteen functions in contrib where the \"defined in\" link needs to be updated.", "I don't own the tool that generates these; reassigning to Mark to take a look.", "Seems to be resolved now, closing."]}, {"number": 10397, "title": "Tensorboard Regex does not work - Docker Container (non-gpu)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nTensorflow\r\n- **TensorFlow installed from (source or binary)**:\r\nfrom docker image: tensorflow/tensorflow\r\n- **TensorFlow version (use command below)**:\r\nfrom docker image: tensorflow/tensorflow\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nDeployed to a kubernetes cluster; below is .yaml file.\r\n\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  labels:\r\n    app: tensorboard\r\n  name: tensorboard\r\nspec:\r\n  ports:\r\n  - port: 80\r\n    targetPort: 6006\r\n  selector:\r\n    app: tensorboard\r\n  type: LoadBalancer\r\n---\r\napiVersion: extensions/v1beta1\r\nkind: Deployment\r\nmetadata:\r\n  labels:\r\n    app: tensorboard\r\n  name: tensorboard\r\nspec:\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: tensorboard    \r\n      containers:\r\n      - name: tensorboard\r\n        command: [\"/bin/sh\", \"-c\"]\r\n        args: [\"tensorboard --logdir /tensorboard\"]\r\n        image: tensorflow/tensorflow\r\n        ports:\r\n        - containerPort: 6006\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n![tensorboard_regex_bug](https://cloud.githubusercontent.com/assets/6099287/26727906/65192646-4776-11e7-93f2-c2a78accbb83.png)\r\n\r\nregex just simply is not working.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Sorry you ran into this. I believe this has been fixed in the upcoming release, could you try with the docker image `tensorflow/tensorflow:1.2.0-rc1`?\r\n", "Please try running `pip install tensorflow-tensorboard` to get the latest TensorBoard. If it's still broken, file an issue at https://github.com/tensorflow/tensorboard"]}, {"number": 10396, "title": "inference require the old training datasets exist", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 1604\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**: r1.1\r\n- **Bazel version (if compiling from source)**: /\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: GTX1080/8G\r\n \r\n**Problem description**\r\nThe exact problem is Inference requires training dataset \r\nThis may because some of the stuff in import_meta_graph(). After you create the training graph and op train_input/ReaderRead would save the 'file names' as const in the graph. And then use saver to save the model. The next time you retrain your model, the import_meta_graph() would requires training dataset existing, or you may found the error below.\r\n\r\nFor example, such a case:\r\n\r\nwhen I build my graph, I use such code to generate input tensor:\r\n\r\n  ```\r\nlogging.info(\"Using batch size of \" + str(batch_size) + \" for training.\")\r\n  with tf.name_scope(\"train_input\"):\r\n    files = gfile.Glob(data_pattern)\r\n    if not files:\r\n      raise IOError(\"Unable to find training files. data_pattern='\" +\r\n                    data_pattern + \"'.\")\r\n    logging.info(\"Number of training files: %s.\", str(len(files)))\r\n    filename_queue = tf.train.string_input_producer(\r\n        files, num_epochs=num_epochs, shuffle=True)\r\n    training_data = [\r\n        reader.prepare_reader(filename_queue) for _ in range(num_readers)\r\n    ]\r\n\r\n    return tf.train.shuffle_batch_join(\r\n        training_data,\r\n        batch_size=batch_size,\r\n        capacity=batch_size * 5,\r\n        min_after_dequeue=batch_size,\r\n        allow_smaller_final_batch=True,\r\n        enqueue_many=True)\r\n```\r\n\r\nAfter trained the model , I want to inference the model then I rebuild my input tensor and recover my model like this:\r\n\r\n```\r\nsaver = tf.train.import_meta_graph(meta_graph_location, clear_devices=True)\r\n\r\n   saver.restore(sess, latest_checkpoint)\r\n```\r\n\r\n**Everything is ok when the training datasets could be found**, However, after I train the model I remove the training data,  inference.py got error like this: \r\n```\r\nTraceback (most recent call last):\r\n  File \"inference.py\", line 197, in <module>\r\n    app.run()\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"inference.py\", line 193, in main\r\n    FLAGS.output_file, FLAGS.batch_size, FLAGS.top_k)\r\n  File \"inference.py\", line 166, in inference\r\n    coord.join(threads)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\r\n    sess.run(enqueue_op)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /data_tmp/train1Z.tfrecord\r\n\t [[Node: train_input/ReaderReadV2_5 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](train_input/TFRecordReaderV2_5, train_input/input_producer)]]\r\n\t [[Node: train_input/DecodeRaw_11/_143 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_18_train_input/DecodeRaw_11\", tensor_type=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nCaused by op u'train_input/ReaderReadV2_1', defined at:\r\n  File \"inference.py\", line 197, in <module>\r\n    app.run()\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"inference.py\", line 193, in main\r\n    FLAGS.output_file, FLAGS.batch_size, FLAGS.top_k)\r\n  File \"inference.py\", line 122, in inference\r\n    saver = tf.train.import_meta_graph(meta_graph_location, clear_devices=True)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1595, in import_meta_graph\r\n    **kwargs)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 499, in import_scoped_meta_graph\r\n    producer_op_list=producer_op_list)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 308, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nNotFoundError (see above for traceback): \r\n/data_tmp/traintY.tfrecord\r\n\t [[Node: train_input/ReaderReadV2_1 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](train_input/TFRecordReaderV2_1, train_input/input_producer)]]\r\n\t [[Node: train_input/DecodeRaw_2/_127 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_47_train_input/DecodeRaw_2\", tensor_type=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n```\r\n\r\n \r\n\r\nI wonder why train.import_meta_graph() does such check? Because after restore the weights I will run the graph by feeding a new input tensor, for example:\r\n```\r\nvideo_id_batch_val, video_batch_val,num_frames_batch_val = sess.run([video_id_batch, video_batch, num_frames_batch])\r\n          predictions_val, = sess.run([predictions_tensor], feed_dict={input_tensor: video_batch_val, num_frames_tensor: num_frames_batch_val})\r\n```\r\nwhen it is inferencing, nothing concerns about the 'old input producer' in training section, however , when the 'old files for training' changed in the environment, the error arise and I have to rebuild the whole graph again instead of using import_meta_graph().  It's really confused.\r\n\r\n \r\n", "comments": ["Is that the full error output?\r\n\r\nCould you also please fill out the system / install information form that was in the issue template.\r\n\r\nSee also https://stackoverflow.com/questions/38829641/tensorflow-train-import-meta-graph-does-not-work in case it helps.", "@jart \r\nI've update my comment.\r\n\r\nthe problem is that:\r\n```\r\n/data_tmp/train1Z.tfrecord\r\n/data_tmp/traintY.tfrecord\r\n```\r\nare the files that I used in training section. They were saved as const in trained model(ckpt.meta). When I inference the model use import_meta_graph(),  it seems that it would insure these 'old files' exist. \r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing due to lack of activity, but please reopen if this is still a problem.", "@KangHsi I have the same issue using a pretrained model. Did you find the solution?", "@SharoneDayan  I don't have a good solution. A work around is that rebuild the entire graph instead of using import_meta_graph()."]}, {"number": 10395, "title": "Fixed typo in code comment", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 10394, "title": "How to disable checkpoints?", "body": "This is a classifier:\r\nclassifier = SKCompat(tf.contrib.learn.DNNClassifier(feature_columns=feature_columns, \r\n                                            hidden_units=[7, 4, 2], \r\n                                            n_classes=2,\r\n                                            optimizer=tf.train.GradientDescentOptimizer(0.08)\r\n                                                    )\r\n                     )\r\n\r\nand when i try to fit model, i get this:\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\home\\AppData\\Local\\Temp\\tmprit6vryq\\model.ckpt. \r\nINFO:tensorflow:loss = 0.71007, step = 1\r\nINFO:tensorflow:Loss for final step: 0.71007.\r\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\r\nINFO:tensorflow:Restoring parameters from C:\\Users\\home\\AppData\\Local\\Temp\\tmprit6vryq\\model.ckpt-1\r\n\r\nHow to disable saving? It takes 3-4 seconds. It's too long for me. Thanks for helping!\r\n\r\nI want to fit my model \"online\" (1 example, 1 step). ", "comments": ["Please don't crosspost [StackOverflow questions](https://stackoverflow.com/questions/44330025/how-to-disable-checkpoints-in-fit-tensorflow). CC: @alextp"]}, {"number": 10393, "title": "I want read train or test data in next_batch by tf.cond", "body": "```\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport os\r\nbatch_size = 4\r\nim_w = 32\r\nim_h = 32\r\nim_d = 3\r\nlabel_bytes = 1\r\nim_bytes = im_w*im_h*im_d\r\n\r\ndef next_path(data_dir, is_train, batch_size, shuffle):\r\n    # if is_train:\r\n    #     filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)]\r\n    # else:\r\n    #     filenames = [os.path.join(data_dir, 'test_batch.bin')]\r\n\r\n    a = tf.cast([os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)], tf.string)\r\n    b = tf.cast([os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)], tf.string)\r\n    filenames = tf.cond(is_train, lambda: a, lambda: b)\r\n\r\n    # filenames = tf.cast([os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)], tf.string)\r\n    filenames_queue = tf.train.string_input_producer(filenames)\r\n    reader = tf.FixedLengthRecordReader(label_bytes+im_bytes)\r\n    key, value = reader.read(filenames_queue)\r\n    record_bytes = tf.decode_raw(value, tf.uint8)\r\n    label = tf.slice(record_bytes, [0], [label_bytes])\r\n    label = tf.cast(label, tf.int32)\r\n\r\n    im_raw = tf.slice(record_bytes, [label_bytes], [im_bytes])\r\n    im_raw = tf.reshape(im_raw, [im_d, im_h, im_w])\r\n    im = tf.transpose(im_raw, [1, 2, 0])\r\n    im = tf.cast(im, tf.float32)\r\n\r\n    # im = tf.image.per_image_standardization(im)\r\n    if shuffle:\r\n        images, labels = tf.train.shuffle_batch([im, label],\r\n                                                batch_size=batch_size,\r\n                                                capacity=1000,\r\n                                                num_threads=16,\r\n                                                min_after_dequeue=100)\r\n    else:\r\n        images, labels = tf.train.batch([im, label], batch_size=batch_size, capacity=1000, num_threads=16)\r\n\r\n    labels = tf.reshape(labels, [batch_size])\r\n\r\n    return images, labels\r\n\r\nis_train = tf.placeholder(tf.bool)\r\nimages, labels = next_path('cifar-10-batches-bin', is_train, batch_size, True)\r\n\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n    for step in range(1):\r\n        images_, _ = sess.run([images, labels], feed_dict={is_train:True})\r\n        plt.imshow(images_[0, :,:,:])\r\n        plt.show()\r\n\r\n    coord.request_stop()\r\n    coord.join(threads=threads)\r\n```\r\n```\r\n\r\na = tf.cast([os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)], tf.string)\r\nb = tf.cast([os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)], tf.string)\r\nfilenames = tf.cond(is_train, lambda: a, lambda: b)\r\n```\r\n    \r\nthis is error ,but below is ok, why and how to do\r\n   ` filenames = tf.cast([os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)], tf.string)\r\n`", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@zhoulinyuan This is because all operations for tf.cond should be created within the lambda function as described [here] (https://stackoverflow.com/questions/35833011/how-to-add-if-condition-in-a-tensorflow-graph)"]}, {"number": 10392, "title": "transfer problem", "body": "How to transfer model.ckpt-152000.meta into .pb file, we will apply to graph_transfer?\r\nThank you!!", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10391, "title": "Fix value error generated on is_scalar check", "body": "`is_scalar = shape is not None and not shape` raises a value error when shape is a scalar, \"ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\"", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.\r\n\r\nJonathan, I assume this is good to go pending tests?", "Jenkins, test this please.", "Rebuilding broken Mac test here: \r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-mac/5317/console", "These failures (in the Linux CPU Py3 build) seem related.", "@erilyth any luck with the test failures?", "@erilyth here's an example failure:\r\n\r\n```\r\nERROR: testIrisMomentum (__main__.CustomOptimizer)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimators_test.py\", line 142, in testIrisMomentum\r\n    classifier.fit(x_train, y_train, steps=400)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 296, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 442, in fit\r\n    SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1353, in fit\r\n    monitors=all_monitors)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 296, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 458, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 958, in _train_model\r\n    model_fn_ops = self._get_train_ops(features, labels)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1165, in _get_train_ops\r\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1136, in _call_model_fn\r\n    model_fn_results = self._model_fn(features, labels, **kwargs)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/dnn.py\", line 209, in _dnn_model_fn\r\n    logits=logits)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1059, in create_model_fn_ops\r\n    enable_centered_bias=self._enable_centered_bias)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 649, in _create_model_fn_ops\r\n    batch_size, loss_fn, weight_tensor)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1911, in _train_op\r\n    train_op = train_op_fn(loss)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/dnn.py\", line 202, in _train_op_fn\r\n    summaries=[])\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/optimizers.py\", line 180, in optimize_loss\r\n    initializer=init_ops.constant_initializer(learning_rate))\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 367, in get_variable\r\n    validate_shape=validate_shape, use_resource=use_resource)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 321, in _true_getter\r\n    use_resource=use_resource)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/estimators_test.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 463, in _get_partitioned_variable\r\n    \"shape: %s\" % shape)\r\nValueError: A partitioned Variable must have rank at least 1, shape: ()\r\n\r\n----------------------------------------------------------------------\r\n```", "Jenkins, test this please.", "ping @erilyth on the test failures.", "Jenkins, test this please"]}, {"number": 10390, "title": "error \uff0ca configure error! help!", "body": "\r\n\u279c  tensorflow git:(master) \u2717 ./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nFound possible Python library paths:\r\n  /opt/ros/kinetic/lib/python2.7/dist-packages\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/opt/ros/kinetic/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /opt/ros/kinetic/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N] \r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] \r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] \r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] u\r\nInvalid selection:  u\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N] n\r\nnvcc will be used as CUDA compiler\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the cuDNN version you want to use. [Leave empty to use system default]: \r\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: \r\nDo you wish to build TensorFlow with MPI support? [y/N] n\r\nMPI support will not be enabled for TensorFlow\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading options for 'clean' from /home/lulin/tensorflow/tools/bazel.rc:\r\n  Inherited 'build' options: --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --genrule_strategy=standalone -c opt\r\nINFO: Reading options for 'clean' from /home/lulin/tensorflow/.tf_configure.bazelrc:\r\n  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/opt/ros/kinetic/lib/python2.7/dist-packages --define PYTHON_BIN_PATH=/usr/bin/python --define PYTHON_LIB_PATH=/opt/ros/kinetic/lib/python2.7/dist-packages --force_python=py2 --host_force_python=py2 --python2_path=/usr/bin/python --define with_jemalloc=true --action_env TF_NEED_CUDA=1 --action_env TF_NEED_OPENCL=0 --action_env TF_CUDA_CLANG=0 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda --action_env TF_CUDA_VERSION=8.0 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc --action_env TF_CUDNN_VERSION= --action_env CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2\r\nUnrecognized option: --action_env\r\n\r\n\r\nbazel is 0.3.0;cuda8.0;cudnn5.0.\r\n\r\n", "comments": ["You need to upgrade bazel.\r\nTF at head requires bazel version 0.4.5 or higher.", "thank!.i had got it.the error is the bazel! thanks"]}, {"number": 10389, "title": "slim.conv2d Error: Input has undefined `axis` dimension.", "body": "Here is my code:\r\n```\r\nimport tensorflow as tf\r\nslim = tf.contrib.slim\r\ndata_format='NCHW'\r\n\r\ninput = tf.placeholder(dtype=tf.float32, shape=[2, 4, None, None])\r\nwith slim.arg_scope([slim.batch_norm], data_format=data_format):\r\n    net = slim.conv2d(input, 4, kernel_size=3, stride=1, padding='SAME', rate=2,\r\n                      data_format=data_format, normalizer_fn=slim.batch_norm)\r\n```\r\n\r\nHere is the logs:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/zzy/workspace/tf-models/mind_slim/examples/test.py\", line 9, in <module>\r\n    net = slim.conv2d(input, 4, kernel_size=3, stride=1, padding='SAME', rate=2, data_format=data_format, normalizer_fn=slim.batch_norm)\r\n  File \"/home/zzy/anaconda2/envs/tf-gpu-py27-source/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/home/zzy/anaconda2/envs/tf-gpu-py27-source/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 927, in convolution\r\n    outputs = normalizer_fn(outputs, **normalizer_params)\r\n  File \"/home/zzy/anaconda2/envs/tf-gpu-py27-source/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/home/zzy/anaconda2/envs/tf-gpu-py27-source/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 528, in batch_norm\r\n    outputs = layer.apply(inputs, training=is_training)\r\n  File \"/home/zzy/anaconda2/envs/tf-gpu-py27-source/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 320, in apply\r\n    return self.__call__(inputs, **kwargs)\r\n  File \"/home/zzy/anaconda2/envs/tf-gpu-py27-source/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 286, in __call__\r\n    self.build(input_shapes[0])\r\n  File \"/home/zzy/anaconda2/envs/tf-gpu-py27-source/lib/python2.7/site-packages/tensorflow/python/layers/normalization.py\", line 118, in build\r\n    input_shape)\r\nValueError: ('Input has undefined `axis` dimension. Input shape: ', TensorShape([Dimension(2), Dimension(None), Dimension(None), Dimension(None)]))\r\n```\r\n\r\nWhen I change the rate to 1\r\nOr I change the data_format to 'NHWC' and input to [2, None, None, 4]\r\nIt works without any error.\r\nMy tensorflow version: 1.1", "comments": ["Thanks for the report.\r\n\r\nI haven't traced how, but it seems that things are fixed in the release candidate for the next release (version 1.2.0-rc1). Could you give that a spin?", "@asimshankar \r\nI've tried on version 1.2.0-rc1, and the error still remains.", "@asimshankar @aselle Has anyone looked into this issue?", "@sguada : Could you take a look?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Not sure why that happens, can you try the following to verify the shape information is propagated.\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nslim = tf.contrib.slim\r\ndata_format='NCHW'\r\n\r\ninput = tf.placeholder(dtype=tf.float32, shape=[2, 4, None, None])\r\nnet = slim.conv2d(input, 4, kernel_size=3, stride=1, padding='SAME', rate=2, data_format=data_format)\r\nprint(net.shape)\r\n```\r\n\r\nIf the shape is not well defined then try:\r\n\r\n```\r\ninput = tf.placeholder(dtype=tf.float32, shape=[2, 4, None, None])\r\nnet = tf.layers.convolution2d(input, 4, kernel_size=3, stride=1, padding='SAME', rate=2, data_format='channels_first')\r\nprint(net.shape)\r\n```", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 10388, "title": "Update random_poisson_test", "body": "", "comments": ["Can one of the admins verify this patch?", "Closing because the change seems unnecessary. Please indicate the intended effect of your pull request in the description. Thanks!"]}, {"number": 10387, "title": "Update the instruction for building a minimal xla benchmark", "body": "1. the instruction for building lstm_layer_inference_benchmark doesn't work.\r\n2. the \"cc_target_os\" in \"android_armeabi\" is an unknown option\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "@martinwicke Can you merge this PR?  Thanks!"]}, {"number": 10386, "title": "Fix typos", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 10385, "title": "make gcc-5 on Ubuntu 16.04 happy", "body": "gcc-5 complains of ambiguity and refuses to go when doing something\r\nlike 'bazel build -c opt tensorflow/...'\r\n\r\nError messages:\r\n> ...\r\n> ERROR: /hack/freedom/tensorflow/freedom/tensorflow/tensorflow/core/kernels/BUILD:3854:1: C++ compilation of rule '//tensorflow/core/kernels:mfcc_test' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 136 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n> In file included from external/gmock_archive/googletest/include/gtest/gtest.h:58:0,\r\n>                  from ./tensorflow/core/platform/test.h:35,\r\n>                  from tensorflow/core/kernels/mfcc_test.cc:18:\r\n> tensorflow/core/kernels/mfcc_test.cc: In member function 'virtual void tensorflow::MfccTest_AvoidsNansWithZeroInput_Test::TestBody()':\r\n> tensorflow/core/kernels/mfcc_test.cc:66:29: error: 'isnan' was not declared in this scope\r\n>      EXPECT_FALSE(isnan(value));\r\n>                              ^\r\n> tensorflow/core/kernels/mfcc_test.cc:66:29: note: suggested alternatives:\r\n> In file included from /usr/include/c++/5/complex:44:0,\r\n>                  from ./tensorflow/core/framework/numeric_types.h:19,\r\n>                  from ./tensorflow/core/framework/allocator.h:23,\r\n>                  from ./tensorflow/core/framework/op_kernel.h:23,\r\n>                  from ./tensorflow/core/kernels/mfcc_dct.h:23,\r\n>                  from ./tensorflow/core/kernels/mfcc.h:23,\r\n>                  from tensorflow/core/kernels/mfcc_test.cc:16:\r\n> /usr/include/c++/5/cmath:641:5: note:   'std::isnan'\r\n>      isnan(_Tp __x)\r\n>      ^\r\n> In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:536:0,\r\n>                  from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\r\n>                  from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n>                  from ./tensorflow/core/framework/numeric_types.h:21,\r\n>                  from ./tensorflow/core/framework/allocator.h:23,\r\n>                  from ./tensorflow/core/framework/op_kernel.h:23,\r\n>                  from ./tensorflow/core/kernels/mfcc_dct.h:23,\r\n>                  from ./tensorflow/core/kernels/mfcc.h:23,\r\n>                  from tensorflow/core/kernels/mfcc_test.cc:16:\r\n> external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GlobalFunctions.h:88:3: note:   'Eigen::isnan'\r\n>    EIGEN_ARRAY_DECLARE_GLOBAL_UNARY(isnan,scalar_isnan_op,not-a-number test,\\sa Eigen::isinf DOXCOMMA Eigen::isfinite DOXCOMMA ArrayBase::isnan)\r\n>    ^\r\n> In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:371:0,\r\n>                  from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\r\n>                  from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n>                  from ./tensorflow/core/framework/numeric_types.h:21,\r\n>                  from ./tensorflow/core/framework/allocator.h:23,\r\n>                  from ./tensorflow/core/framework/op_kernel.h:23,\r\n>                  from ./tensorflow/core/kernels/mfcc_dct.h:23,\r\n>                  from ./tensorflow/core/kernels/mfcc.h:23,\r\n>                  from tensorflow/core/kernels/mfcc_test.cc:16:\r\n> external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/MathFunctions.h:1108:46: note:   'Eigen::numext::isnan'\r\n>  template<typename T> EIGEN_DEVICE_FUNC bool (isnan)   (const T &x) { return internal::isnan_impl(x); }\r\n>                                               ^\r\n> In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:411:0,\r\n>                  from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\r\n>                  from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n>                  from ./tensorflow/core/framework/numeric_types.h:21,\r\n>                  from ./tensorflow/core/framework/allocator.h:23,\r\n>                  from ./tensorflow/core/framework/op_kernel.h:23,\r\n>                  from ./tensorflow/core/kernels/mfcc_dct.h:23,\r\n>                  from ./tensorflow/core/kernels/mfcc.h:23,\r\n>                  from tensorflow/core/kernels/mfcc_test.cc:16:\r\n> external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/Half.h:372:45: note:   'Eigen::half_impl::isnan'\r\n>  EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool (isnan)(const half& a) {\r\n>                                              ^\r\n> In file included from /usr/include/c++/5/complex:44:0,\r\n>                  from ./tensorflow/core/framework/numeric_types.h:19,\r\n>                  from ./tensorflow/core/framework/allocator.h:23,\r\n>                  from ./tensorflow/core/framework/op_kernel.h:23,\r\n>                  from ./tensorflow/core/kernels/mfcc_dct.h:23,\r\n>                  from ./tensorflow/core/kernels/mfcc.h:23,\r\n>                  from tensorflow/core/kernels/mfcc_test.cc:16:\r\n> /usr/include/c++/5/cmath:641:5: note:   'std::isnan'\r\n>      isnan(_Tp __x)\r\n> ", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 10384, "title": "moved code generated by protoc-gen-go out of internal pkg so it can be referenced/used", "body": "would like to call String() method on GraphDef. can't do so if its in the internal pkg. there are likely other use cases here. \r\n\r\n\r\nwhat do you think?", "comments": ["Can one of the admins verify this patch?", "@asimshankar can this be done in the sort term until the Go code generator to uses the same configuration file as the code generator for Python and C++ (and others)?", "@asimshankar my goal is more than debugging its training / teaching people the go APIs for tensorflow. as such having a human readable non-binary export of tensorflow graph is very useful. e.g.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/graph_io.py#L69", "Jenkins, test this please.\r\n\r\n@asimshankar what do you think?", "docker workaround:\r\n\r\n```\r\n#Begin: install protoc\r\nENV PROTOC_VERSION 3.3.0\r\nENV PROTOC_LIB_ZIP protoc-$PROTOC_VERSION-linux-x86_64.zip\r\nENV TARGET_DIRECTORY /usr/local\r\nRUN  curl -fsSL \"https://github.com/google/protobuf/releases/download/v$PROTOC_VERSION/$PROTOC_LIB_ZIP\" -o $PROTOC_LIB_ZIP && \\\r\n     sudo unzip $PROTOC_LIB_ZIP -d $TARGET_DIRECTORY && \\\r\n     rm -Rf $TPROTOC_LIB_ZIP\r\n#End: install protoc\r\n\r\n#Begin: generate go files from protobufs\r\nRUN go get github.com/golang/protobuf/proto\r\nRUN go get github.com/golang/protobuf/protoc-gen-go\r\nENV TF_DIR /go/src/github.com/tensorflow/tensorflow\r\nENV TF_PB_DIR /go/src/github.com/tensorflow/tensorflow/tensorflow/go/pb\r\nRUN mkdir -p $TF_PB_DIR\r\nRUN /usr/local/bin/protoc -I $TF_DIR \\\r\n  --go_out=$TF_PB_DIR \\\r\n  $TF_DIR/tensorflow/core/framework/*.proto\r\n#End: generate go files from protobufs\r\n```", "cgo program to list out ops (using docker workaround):\r\nhttps://github.com/ctava/tensorflow-go-opslist/blob/master/main.go#L29"]}, {"number": 10383, "title": "error\uff0c I use tensorboard with pandas error", "body": "\r\nhello , during this days I was learning use tensorflow to build NN, when I use the tensorboard .In my code I use:\r\n`self.writer = tf.summary.FileWriter('./board', self.graph)`\\\r\nto create the board file. and use qurery ,and It successfully have a file called events.out.tfevents.1496363479.dyy there. the Iuse query :tensorboard --logdir=board\r\nthen I got the error info.\r\n`panda@dyy:~/code/number_test$ tensorboard --logdir=board\r\nTraceback (most recent call last):\r\n  File \"/home/panda/anaconda3/bin/tensorboard\", line 7, in <module>\r\n    from tensorflow.tensorboard.tensorboard import main\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/tensorboard/tensorboard.py\", line 33, in <module>\r\n    from tensorflow.tensorboard.backend import application\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/tensorboard/backend/application.py\", line 47, in <module>\r\n    from tensorflow.tensorboard.plugins.projector import projector_plugin\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/tensorboard/plugins/projector/projector_plugin.py\", line 28, in <module>\r\n    from tensorflow.contrib.tensorboard.plugins.projector import PROJECTOR_FILENAME\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 30, in <module>\r\n    from tensorflow.contrib import factorization\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/factorization/__init__.py\", line 24, in <module>\r\n    from tensorflow.contrib.factorization.python.ops.gmm import *\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/factorization/python/ops/gmm.py\", line 27, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators import estimator\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/__init__.py\", line 87, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/__init__.py\", line 23, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/__init__.py\", line 25, in <module>\r\n    from tensorflow.contrib.learn.python.learn import estimators\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/__init__.py\", line 297, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py\", line 29, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 31, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators import estimator\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 49, in <module>\r\n    from tensorflow.contrib.learn.python.learn.learn_io import data_feeder\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_io/__init__.py\", line 21, in <module>\r\n    from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_io/dask_io.py\", line 26, in <module>\r\n    import dask.dataframe as dd\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/dask/dataframe/__init__.py\", line 3, in <module>\r\n    from .core import (DataFrame, Series, Index, _Frame, map_partitions,\r\n  File \"/home/panda/anaconda3/lib/python3.6/site-packages/dask/dataframe/core.py\", line 38, in <module>\r\n    pd.computation.expressions.set_use_numexpr(False)\r\nAttributeError: module 'pandas' has no attribute 'computation'\r\n`\r\nmy python is 3.5.2 (in anaconda 4.2.0 )and my tensorflow is 1.1, and the pandas is 0.20.1\uff08at first it is 0.18.1,but the error was about pandas so I try to update it\uff09,this is run in ubuntu 15 with x64\u3002", "comments": ["Looking at the stacktrace, it seems that you might have a old `dask` version. The stacktrace is triggered by `/home/panda/anaconda3/lib/python3.6/site-packages/dask/dataframe/core.py`, line 38, which is trying to use `pd.computation`. It seems that in pandas 0.20.0 this was changed (this was fixed in dask 0.14.2 I think - https://github.com/dask/dask/commit/bab4942b4b9eab6862226c0da868389629ee074f)\r\n\r\nCan you disable dask in your anaconda environment or update it and retry?\r\n\r\nFWIW, dask is not required by TensorFlow, but if it is installed it tries to use it. It seems that the versions of dask and pandas installed on your system are not compatible.", "Closing due to inactivity. If you still believe there's a bug in TensorFlow or TensorBoard, let me know and I'll re-open.", "By using `conda update dask`, I solved the problem. (windows10, python 3.5 (w/ Anaconda), tensorflow-gpu 1.2.1)", "Thanks! `conda update dask`  fixed it for me."]}, {"number": 10382, "title": "Cant start work with tensorflow, import error", "body": "### System information\r\nWindows 7 64.\r\nInstalled CUDA, cuDNN 8.0.\r\nInstalled TensorFlow gpu from pip.\r\nPyrhon 3.5 by anaconda.\r\n\r\nTry inpute import tensorflow and get this\r\n\r\nImportError                               Traceback (most recent call last)\r\nC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     17         try:\r\n---> 18             return importlib.import_module(mname)\r\n     19         except ImportError:\r\n\r\nC:\\Anaconda3\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n\r\nC:\\Anaconda3\\lib\\importlib\\_bootstrap.py in _gcd_import(name, package, level)\r\n\r\nC:\\Anaconda3\\lib\\importlib\\_bootstrap.py in _find_and_load(name, import_)\r\n\r\nC:\\Anaconda3\\lib\\importlib\\_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\nC:\\Anaconda3\\lib\\importlib\\_bootstrap.py in _load_unlocked(spec)\r\n\r\nC:\\Anaconda3\\lib\\importlib\\_bootstrap.py in module_from_spec(spec)\r\n\r\nC:\\Anaconda3\\lib\\importlib\\_bootstrap_external.py in create_module(self, spec)\r\n\r\nC:\\Anaconda3\\lib\\importlib\\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\nImportError: DLL load failed: Specified module not found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\nC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     40     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)\r\n---> 41   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     42   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     20             return importlib.import_module('_pywrap_tensorflow_internal')\r\n---> 21     _pywrap_tensorflow_internal = swig_import_helper()\r\n     22     del swig_import_helper\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     19         except ImportError:\r\n---> 20             return importlib.import_module('_pywrap_tensorflow_internal')\r\n     21     _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\nC:\\Anaconda3\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-a649b509054f> in <module>()\r\n----> 1 import tensorflow\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\nC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     49 import numpy as np\r\n     50 \r\n---> 51 from tensorflow.python import pywrap_tensorflow\r\n     52 \r\n     53 # Protocol buffers\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     50 for some common reasons and solutions.  Include the entire stack trace\r\n     51 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 52   raise ImportError(msg)\r\n     53 \r\n     54 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 919, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: \u041d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u0434\u0443\u043b\u044c.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["CPU ver works fine.", "https://github.com/tensorflow/tensorflow/issues/10033 indicates this might be due to a problem with the `%PATH%` environment variable. There's a few other [reports](https://www.google.com/search?q=%22ImportError%3A+DLL+load+failed%22+tensorflow&oq=%22ImportError%3A+DLL+load+failed%22+tensorflow&aqs=chrome..69i57.3519j0j4&sourceid=chrome&ie=UTF-8) of this issue in the past too.", "Fixed by\r\nimport os\r\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"", "I am also having this issue. How do you work out what DLL it is trying to load? Is it possible to get tensorflow to log the DLLs it is attempting to load. It is pretty hard to fix DLL path issues like this without that."]}, {"number": 10381, "title": "Clarify tf.matmul documentation", "body": "Maybe I'm confused about what \"inner\" and \"outer\" tensor dimensions are, but the documentation for `tf.matmul` puzzles me:\r\n\r\n> The inputs must be matrices (or tensors of rank > 2, representing\r\n> batches of matrices), with matching inner dimensions, possibly after\r\n> transposition.\r\n\r\nIsn't it the case that R-rank arguments need to have matching (or no) R-2 outer dimensions, and that (as in normal matrix multiplication) the Rth, inner dimension of the first argument must match the R-1st dimension of the second. That is, in\r\n\r\n    A = tf.constant(..., shape=[a, ..., z, p, x])\r\n    B = tf.constant(..., shape=[a', ..., z', x', q]) \r\n    C = tf.matmul(A, B)\r\n\r\nThe outer dimensions `a, ..., z` must be identical to `a', ..., z'` (or not exist), and `x` and `x'` must match (while `p` and `q` can be anything).\r\n\r\nOr put another way, shouldn't the docs say: \r\n\r\n> The inputs must, following any transpositions, be tensors of rank \u2265 2 where the inner 2 dimensions specify valid matrix multiplication arguments, and any further outer dimensions match.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I agree the existing language is confusing, but the updated language is also difficult to parse. I think the example you gave is much clearer. Should we put that there instead? Something like:\r\n\r\nFor rank > 2:\r\n```\r\nA = tf.constant(..., shape=[a, ..., z, p, x])\r\nB = tf.constant(..., shape=[a', ..., z', x', q]) \r\nC = tf.matmul(A, B)\r\n```\r\nThe outer dimensions a, ..., z must be identical to a', ..., z', and x and x' must match.", "Adding Andrew in case he has an opinion.", "@jhseu : That could be added but would need to also make clear what transposed arguments also work, and that \"matching\" for  a, ..., z and a', ..., z' means they can both be absent.\r\n\r\nI'm partial to using the proposed\r\n\r\n> The inputs must, following any transpositions, be tensors of rank \u2265 2 where the inner 2 dimensions specify valid matrix multiplication arguments, and any further outer dimensions match.\r\n\r\nSince even if it's a bit hard to parse at first, the example (which I agree should be added) clears it up.", "Jenkins, test this please", "Jenkins, test this please", "I think the tests don't like the non-ascii `\u2265` character in there. Mind switching it to >=?", "Actually, I just did it instead.\r\n\r\nJenkins, test this please"]}, {"number": 10380, "title": "Check EIGEN_MAX_ALIGN_BYTES to prevent mod-by-0", "body": "If EIGEN_MAX_ALIGN_BYTES is set to 0, alignment checks that mod by EIGEN_MAX_ALIGN_BYTES fail at runtime.\r\n\r\n* Returns true, as in tensorflow/core/framework/tensor.h\r\n* Update unit tests", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n>  Please visit https://cla.developers.google.com/ to sign.\r\n> \r\n> Once you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.\r\n\r\nI signed the CLA!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "Jenkins, test this please"]}, {"number": 10379, "title": "Fix typo", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 10378, "title": "What are the 'from' and 'to' dimensions of transition_params in tf.contrib.crf.crf_log_likelihood?", "body": "On TensorFlow, I want to pass a transition_params matrix as argument to tf.contrib.crf.crf_log_likelihood (https://www.tensorflow.org/api_docs/python/tf/contrib/crf/crf_log_likelihood) in order to initialize the transitions matrix of the CRF. Although, in the documentation, it doesn't state which dimension of this matrix corresponds to the first tag of the transition and which dimension corresponds to the second.\r\n\r\nSo, let T be the transitions matrix, does T[i,j] represent the score of the transition from tag i to tag j, or is it the other way around?\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is a larger community that reads questions there.\r\n\r\nWe also give amazing code reviews to anyone generous enough to send us pull requests that improve the documentation of contrib packages."]}]