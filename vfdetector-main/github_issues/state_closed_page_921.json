[{"number": 25826, "title": "[TF 2.0 API Docs] tf.compat.path_to_str", "body": "\r\n**System information**\r\n- TensorFlow version: 2.0\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/compat/path_to_str\r\n\r\n### Existing URLs containing the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/compat/path_to_str\r\n\r\n### Description of issue (what needs changing):\r\n* **Correct Links**\r\n  https://github.com/tensorflow/tensorflow/blob/master/python/util/compat.py is incorrect. The correct link should be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/util/compat.py.\r\n* **Clear Description**\r\n  There could be a description of use cases where using this symbol would be appropriate. \r\n* **Raises Listed and Defined**\r\n  Errors are not defined.\r\n* **Visuals, if Applicable**\r\nI do not think visuals are necessary for the symbol.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["@rohanmeh Is this Issue still exist?\r\nI would like to fix this.", "@parths007 pretty sure it still exists.", "Solved by #27335, additional attempt in #28502 came too late"]}, {"number": 25825, "title": "tf.data.experimental.make_csv_dataset unable to handle missing data", "body": "I believe this is a bug. I've been unable to get `tf.data.experimental.make_csv_dataset` to work with a CSV that contains missing data. As a toy example, suppose I have a single CSV file `example.csv` with the following contents:\r\n\r\n```\r\ndate, value\r\nD1,1017.1\r\nD2, NA\r\n```\r\n\r\nWhen I run the following code, I receive the following error: `tensorflow.python.framework.errors_impl.InvalidArgumentError: Field 1 in record is not a valid float:  NA [Op:IteratorGetNextSync]`\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\ndataset = tf.data.experimental.make_csv_dataset(\r\n            file_pattern=filenames,\r\n            column_names=['date', 'value'],\r\n            column_defaults=[tf.string, tf.float32],\r\n            header=True,\r\n            field_delim=',',\r\n            na_value='NA'\r\n        )\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n```\r\n\r\nAm I misusing the function? How am I supposed to specify that `NA` should be interpreted as a missing value? Replacing `NA` with a literal empty string results in the same problem:\r\n\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: Field 1 is required but missing in record! [Op:IteratorGetNextSync]`\r\n\r\nMy system's details:\r\n\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.6", "comments": ["Someone on SO point out that the error is caused by specifying the `column_defaults` argument, which forces the default to be a float32 dtype only. I have two follow up questions. First, how could I have discovered that interaction? I just reread the documentation and that behavior isn't clear to me at all. Second, if I remove `column_defaults`, then missing floating values are replaced by 0s, whereas if I replace the missing floating values in the CSV with `'NA'`, then `tf.data.experimental.make_csv_dataset` sets `dtype=object`. How do I keep the column type as `tf.float32` while maintaining the NAs?", "Rachel, I think this is a feature request for better missing value handling in `make_csv_dataset()`", "Is the issue here the space character before the NA in your data? Matching the na_value requires an exact match. What if you did `na_value=\" NA\"`? ", "@RylanSchaeffer ", "@rachellim I just tried the opposite approach (removing the space), but it doesn't seem to make a difference. I created `test.csv` in a directory \r\n\r\n\r\n```\r\n$ head test.csv \r\ndate,value\r\nD1,1017.1\r\nD2,NA\r\n\r\n```\r\n\r\nHere's the exact code that I ran with Python 3 from the same directory containing `test.csv`:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfilenames = ['test.csv']\r\n\r\ntf.enable_eager_execution()\r\ndataset = tf.data.experimental.make_csv_dataset(\r\n            file_pattern=filenames,\r\n            column_names=['date', 'value'],\r\n            column_defaults=[tf.string, tf.float32],\r\n            header=True,\r\n            field_delim=',',\r\n            na_value='NA',\r\n            batch_size=1\r\n        )\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n```\r\n\r\nBut I received the same error:\r\n\r\n```\r\n>>> next_element = iterator.get_next()\r\n2019-02-21 16:42:53.228099: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at iterator_ops.cc:1031 : Invalid argument: Field 1 is required but missing in record!\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 623, in get_next\r\n    return self._next_internal()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 564, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2266, in iterator_get_next_sync\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 2, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Field 1 is required but missing in record! [Op:IteratorGetNextSync]\r\n```", "Oh, I see. I misunderstood the question earlier. There are two different behaviors here, depending on whether you specify `column_defaults`:\r\n1) If you specify `column_defaults` as a type instead of a value (i.e., `tf.float32` instead of, for example, `0.0`, or `tf.constant(0, tf.float32)`), it treats the field as 'required', i.e. NA values not permitted. This is stated in the documentation: \"If a dtype is provided instead of a tensor, the column is also treated as required. \" I'll improve the error message here so that it's clearer.\r\n2) If `column_defaults` is not provided, it tries to infer the column types from the data. This respects the `na_value` parameter, so it should correctly infer the type to be `tf.float32` if you supply the correct NA value. Let me know if you find any issues with this.\r\n\r\nThese two statements should give you your expected behavior:\r\n```\r\ndataset = tf.data.experimental.make_csv_dataset(\r\n            file_pattern=filenames,\r\n            column_names=['date', 'value'],\r\n            column_defaults=[tf.string, 3.14],  # or any other float default value you want here; missing values will default to this value\r\n            header=True,\r\n            field_delim=',',\r\n            na_value='NA',\r\n            batch_size=1\r\n        )\r\n```\r\n\r\n```\r\ndataset = tf.data.experimental.make_csv_dataset(\r\n            file_pattern=filenames,\r\n            column_names=['date', 'value'],\r\n            header=True,\r\n            field_delim=',',\r\n            na_value='NA',\r\n            batch_size=1\r\n        )   # no column_defaults, na values default to empty string or 0\r\n```", "Hmm, my interpretation of \"If a dtype is provided instead of a tensor, the column is also treated as required. \" meant that the column itself needs to exist, not that every row in the column needs to have a value consistent with the specified type.\r\n\r\nThat said, I don't think either of the two solutions is what I'm looking for. In the first option, 'NA' will be converted to the specified float (e.g. 3.14 in the above example), and in the second option, 'NA' will be converted to the default float(e.g. 0.0). But I want the resulting tensor to contain `nan`. For example, in numpy, I can do `np.array([0., float('nan')])` and this returns `array([ 0., nan])`. \r\n\r\nSo maybe this issue should be a feature request, not a bug? Or is there a way to accomplish this currently?", "Ah, I see - I'll clarify the docstring to make that clearer.\r\n\r\nWhat about doing `column_defaults=[tf.string, tf.constant(float('nan'), tf.float32)]`? That should make NA values for the second column become `nan`.", "Fantastic! Thank you :)"]}, {"number": 25824, "title": "Bug: ImportError: DLL load failed", "body": "\r\nImportError: DLL load failed\r\ndoesnt show the name of the DLL which failed", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "I think it was resolved. Closing due to lack of recent activity. Please open new ticket if you see similar issue. Thanks!"]}, {"number": 25823, "title": "Please delete this", "body": "incredibly easy fix slightly unrelated to tensorflow", "comments": []}, {"number": 25822, "title": " Illegal instruction: 4 when importing tensorflow", "body": "I've installed tensorflow (Version: 1.12.0) using pip3 on MacOS High Sierra 10.13.6, and it crashes on import with Illegal Instruction: 4. I get the same error using any version of python 3.7 and 3.6.8. Same error even after downgrading to tensorflow 1.5 and numpy 1.13. Identical issue whether I install through homebrew or pip3. Would appreciate any help to get it working.\r\nThanks.\r\n```\r\n$ python3\r\nPython 3.6.8 (v3.6.8:3c6b436a57, Dec 24 2018, 02:04:31) \r\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nIllegal instruction: 4\r\n```\r\n I'm also attaching the full error log, with all the details of my system configuration. \r\n[ill4.txt](https://github.com/tensorflow/tensorflow/files/2873724/ill4.txt)\r\n\r\n\r\n\r\n", "comments": ["What is your current numpy version?\r\nThe numpy version should be  >= 1.14.5 and < 2.0", "Numpy version is 1.16.1. Also tried with version 1.14.5 with the same\nresult.\n\nOn Wed, Feb 20, 2019 at 9:16 PM ymodak <notifications@github.com> wrote:\n\n> What is your current numpy version?\n> The numpy version should be >= 1.14.5 and < 2.0\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25822#issuecomment-465833460>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALFK0eelR9wif_bl87oAFcb8gRRbQ5mQks5vPgGUgaJpZM4a_8W->\n> .\n>\n", "what is your CPU make/model?\r\nTF requires CPUs that have AVX instruction sets. If you are running on an old machine, it will fail to load.", "The CPU is an Intel Xeon X5680, which does not have AVX instruction sets.\nIs it possible to run tensorflow in a conda virtual environment with this\nprocessor?\n\nOn Thu, Feb 21, 2019 at 12:20 AM Gunhan Gulsoy <notifications@github.com>\nwrote:\n\n> what is your CPU make/model?\n> TF requires CPUs that have AVX instruction sets. If you are running on an\n> old machine, it will fail to load.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25822#issuecomment-465864783>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALFK0YDMuf1sPZUC02Sti8ERI6VOXCAIks5vPiyzgaJpZM4a_8W->\n> .\n>\n", "Not the prepackaged versions.\r\nIf you build TF from sources, you can run TF on this machine:\r\nhttps://www.tensorflow.org/install/source", "I'm facing the same problem using Tensorflow 1.12.0 built from source on MacOS High Sierra 10.13.6. The problem persists in Python 3.7 but does not exist in Python2.7. So maybe it's bug related to Python 3.7.", "As a workaround, I able to run tensorflow, installed through anaconda, in\nan anaconda virtual environment (both python 3.7 and 2.7):\nhttps://www.anaconda.com/tensorflow-in-anaconda/\n\nOn Sun, Feb 24, 2019 at 8:52 PM Hanlin_Tan <notifications@github.com> wrote:\n\n> I'm facing the same problem using Tensorflow 1.12.0 built from source. The\n> problem persists in Python 3.7 but does not exist in Python2.7. So maybe\n> it's bug related to Python 3.7.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25822#issuecomment-466844201>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALFK0ddKQiEdvFR8BRbs9VFIAkcZ_0BCks5vQ0HvgaJpZM4a_8W->\n> .\n>\n", "@TomHeaven TF 1.12 does not support Python 3.7 however TF 1.13.0-rc2 supports Python 3.7. You may want to give it a try.", "@vrl2 Try using Tensorflow 1.5 with Python 2.7. I got the same error and I have tried Tensorflow versions 1.5, 1.5.1, 1.7, and 1.13 but only v1.5 worked. You can try doing that.\r\nMy system info: MacOS Mojave 10.14; Python 2.7; TensorFlow 1.5", "Closing this issue since its resolved. Thanks!\r\n\r\n"]}, {"number": 25821, "title": "Add a better error message when unsupported ops are used for Dimension", "body": "\r\nThis fix is related to #25790. In #25790, `Dimension` with division will thrown an TypeError:\r\n```\r\noutput_dim = input_shape[-1] / 2\r\n...\r\nTypeError: unsupported operand type(s) for /: 'Dimension' and 'int'\r\n```\r\nThe error is expected, as Dimension only support `//` (not `/`).\r\n\r\nHowever, for backward compatible reasons, `__div__`(`/`) is actually implemented (but deprecated). See code.\r\n\r\nThis issue causes confusion and it has been mentioned multiple times in GitHub (https://github.com/DrSleep/tensorflow-deeplab-resnet/issues/83) and StackOverflow (https://stackoverflow.com/questions/51692253/typeerror-unsupported-operand-types-for-dimension-and-int)\r\n\r\nThis fix is an attempt to print a better error message with:\r\n```\r\nTypeError: unsupported operand type(s) for /: 'Dimension' and 'int', please use // instead\r\n```\r\n\r\nNote that this PR does not change any current behavior, and have no impact with respect\r\nto backward-compatible `__div__`. It merely adds additional notes in the error message.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @drpngx for the review. The PR has been updated. Please take a look."]}, {"number": 25820, "title": "[TF 2.0 API Docs] tf.math.add_n", "body": "**System information**\r\n- TensorFlow version: 2.0\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/add_n\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\n* **Links**\r\nhttps://github.com/tensorflow/tensorflow/blob/master/python/ops/math_ops.py\r\nshould be\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py\r\n\r\n* **Clear Description**\r\nThe description is not clear enough.  In the description, it should probably link to [tf.math.accumulate_n](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/accumulate_n) and explain when to use `tf.math.add_n`.\r\n\r\n* **Usage example**\r\nNo usage example is provided.\r\n\r\n* **Visuals, if Applicable**\r\n  No visuals are included.\r\n\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nYes.\r\n", "comments": ["Hi @dynamicwebpaige. Sorry, this is my first contribution to TensorFlow. Should I wait for a person to review this issue before working on this, or can I go ahead and send a PR? I got a bit confused with the label.", "@dynamicwebpaige If it hasn't already been resolved, I'd be happy to go ahead and work on this", "Hi @dynamicwebpaige is this issue still open? I wrote two books on TensorFlow and I am happy to help with the documentation... I teach deep learning at the university and maybe I can help with doc issues... Let me know. I noticed that some material has been added already.", "@michelucci I think this issue is resolved thanks to @shashvatshahi1998's PR.\r\n\r\nBut we always welcome doc fixes, if you see anything that could be better explained, please send a PR our way.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25820\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25820\">No</a>\n"]}, {"number": 25819, "title": "logical issue tf.less and tf.logical.end", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nAnaconda\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNone\r\n- TensorFlow installed from (source or binary):\r\n1.12\r\n- TensorFlow version (use command below):\r\n1.12\r\n- Python version:\r\nMatching, no issue\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\nMatching, no issue\r\n- GPU model and memory:\r\nTitan 1080 11GB x2\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI have in my code from a 3rd party library:\r\n```\r\n            is_up = tf.less(phi, (r_on_broadcast * 0.5))\r\n            is_down = tf.logical_and(tf.less(phi, r_on_broadcast), tf.logical_not(is_up))\r\n```\r\nThe code runs in training without error. In serving it gives an error when `phi = r_on_broadcast`\r\nIs there a difference between training and serving?\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I found the reason of the error. The issue is not about serving."]}, {"number": 25818, "title": "Add support for cudnn's group convolution.", "body": "This PR enables group convolution in cudnn, a feature that's highly desired for many years (#3332, https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-320465264, #11662, #10482).\r\n\r\nWith this PR, now it's allowed to call `tf.nn.conv2d(inputs, filters)`, where the depth of `inputs` is not necessarily equal to `filters.shape[2]`, but be a multiple of `filters.shape[2]`.\r\n\r\nThe core of this PR is only two lines of code (https://github.com/tensorflow/tensorflow/issues/3332#issuecomment-464308902) which removes the shape check. Then I added some extra checks and tests.\r\n\r\nThis benchmark script:\r\n```python\r\nimport tensorflow as tf\r\nimport time\r\nimport os\r\n\r\nN = 64\r\nC = 256\r\nG = 32\r\nH, W = 64, 64\r\nprint(\"N, C, H, W:\", [N, C, H, W])\r\n\r\n\r\ndef benchmark_all(use_loop, format):\r\n    shape4d = [N, C, H, W] if format == 'NCHW' else [N, H, W, C]\r\n\r\n    tf.reset_default_graph()\r\n    input = tf.get_variable('input', shape=shape4d, dtype=tf.float32)\r\n    filter = tf.get_variable('filter', shape=[3, 3, C // G, C], dtype=tf.float32)\r\n\r\n    if use_loop:\r\n        inputs = tf.split(input, G, axis=1 if format == 'NCHW' else 3)\r\n        filters = tf.split(filter, G, axis=3)\r\n        output = tf.concat(\r\n            [tf.nn.conv2d(i, f,\r\n                strides=[1,1,1,1],\r\n                padding='SAME',\r\n                data_format=format) for i, f in zip(inputs, filters)], axis=1 if format == 'NCHW' else 3)\r\n    else:\r\n        output = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME', data_format=format)\r\n\r\n\r\n    forward_op = output.op\r\n    cost = tf.reduce_sum(output)\r\n    backward_op = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\r\n\r\n    def benchmark(op, nr_iter=200, nr_warmup=10):\r\n        for k in range(nr_warmup):\r\n            op.run()\r\n        start = time.perf_counter()\r\n        for k in range(nr_iter):\r\n            op.run()\r\n        end = time.perf_counter()\r\n        itr_per_sec = nr_iter * 1. / (end - start)\r\n        return itr_per_sec\r\n\r\n    sess = tf.Session()\r\n    with sess.as_default():\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        spd_forward = benchmark(forward_op)\r\n        print(\"Loop={}, Format={}, Forward: {} itr/s\".format(use_loop, format, spd_forward))\r\n        spd_backward = benchmark(backward_op)\r\n        print(\"Loop={}, Format={}, Backward: {} itr/s\".format(use_loop, format, spd_backward))\r\n\r\n\r\nformats = ['NHWC', 'NCHW']\r\nfor format in formats:\r\n    for use_loop in [True, False]:\r\n        benchmark_all(use_loop, format)\r\n```\r\nExecuted on V100, cuda10, cudnn 7.4.2, it prints:\r\n```\r\nN, C, H, W: [64, 256, 64, 64]\r\nLoop=True, Format=NHWC, Forward: 65.49446747235214 itr/s\r\nLoop=True, Format=NHWC, Backward: 32.26484275606916 itr/s\r\nLoop=False, Format=NHWC, Forward: 117.40288830454352 itr/s\r\nLoop=False, Format=NHWC, Backward: 50.051492362319074 itr/s\r\nLoop=True, Format=NCHW, Forward: 98.8428390274372 itr/s\r\nLoop=True, Format=NCHW, Backward: 35.672312085388455 itr/s\r\nLoop=False, Format=NCHW, Forward: 152.24726060851506 itr/s\r\nLoop=False, Format=NCHW, Backward: 56.21414524041962 itr/s\r\n```\r\nwhich shows around 50~80% speed up over a naive loop-based implementation.", "comments": ["I found this op actually does not have the correct gradient w.r.t the filters. Please hold the PR for now.", "The backward kernels appear to support group conv, but in fact the implementation used wrong shapes and was never tested.\r\nNow the backward pass is correct and unit tests are added.", "Hi Yuxin, thanks a lot for your contribution. I'm trying to merge your CL, but I'm hitting some test failures. It might take a few more days before this lands. Thanks for your patience!", "Hi Yuxin, I've tried to merge this CL but there are failing XLA convolution tests.\r\n\r\nYou can enable XLA here:\r\nhttps://github.com/tensorflow/tensorflow/blob/bfad72b3ebf3e75df11ebded563cadf54677b7fb/tensorflow/tensorflow.bzl#L2108\r\n\r\nand then run:\r\nbazel test --config=cuda --config=xla //tensorflow/python/kernel_tests:conv_ops_test \r\n\r\nThanks again for your contribution!", "I noticed that the code in `compiler/tf2xla/kernels/conv_op_helpers.cc` does not properly translate group convolution to XLA ops, although the XLA ops seem to have support for it already.\r\nHowever, I was unable to reproduce test failures. \r\nI rebase this PR on top of cf3c25b7fb635e9f (latest master), rebuild after `bazel clean`, and saw no test failures:\r\n```\r\n$bazel test --jobs 1 --cache_test_results=no  --config=cuda --config=xla //tensorflow/python/kernel_tests:conv_ops_test\r\n$TEST_TMPDIR defined: output root default is '/tmp/bazel/' and max_idle_secs default is '15'.\r\nWARNING: The following configs were expanded more than once: [cuda, xla]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: /XX/tensorflow/tensorflow/python/BUILD:3239:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new\r\nfeatures, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /XX/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions\r\nwill not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/python/kernel_tests:conv_ops_test (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 test target...\r\nTarget //tensorflow/python/kernel_tests:conv_ops_test up-to-date:\r\n  bazel-bin/tensorflow/python/kernel_tests/conv_ops_test\r\nINFO: Elapsed time: 221.848s, Critical Path: 116.95s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 5 processes: 5 local.\r\nINFO: Build completed successfully, 6 total actions\r\n//tensorflow/python/kernel_tests:conv_ops_test                           PASSED in 93.2s\r\n  Stats over 4 runs: max = 93.2s, min = 28.3s, avg = 49.1s, dev = 25.7s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\nINFO: Build completed successfully, 6 total actions\r\n$git diff\r\ndiff --git i/tensorflow/tensorflow.bzl w/tensorflow/tensorflow.bzl\r\nindex 28c189e953..df0b5a64c7 100644\r\n--- i/tensorflow/tensorflow.bzl\r\n+++ w/tensorflow/tensorflow.bzl\r\n@@ -2106,7 +2106,7 @@ def gpu_py_tests(\r\n         shard_count = shard_count,\r\n         tags = test_tags,\r\n         xla_enabled = xla_enabled,\r\n-        xla_enable_strict_auto_jit = False,\r\n+        xla_enable_strict_auto_jit = True,\r\n     )\r\n\r\n # terminology changes: saving cuda_* definition for compatibility\r\n```\r\n@chsigg could you help me figure out how to properly run the XLA tests?", "Sorry, I think I got the wrong line to change. Can you try this one:\r\nhttps://github.com/tensorflow/tensorflow/blob/bfad72b3ebf3e75df11ebded563cadf54677b7fb/tensorflow/tensorflow.bzl#L1994\r\n", "Thanks! Now I can reproduce the failures. The code was updated to include group conv support in `tf2xla`.", "Group-convolution is not supported even in TF2.0(alpha) :-1:  \r\nIt's hard to implement many kinds of newest efficient models using TensorFlow :(", "Hi @chsigg , is there an update on this CL?", "There are a few test failures still, I'm looking into them and will merge ASAP. Thanks for your patience.", "Can we expect this in 1.14?\r\nAlso, this is only for 2D or?", "I'm using bool_mask and sequence_mask to simulate dense group convolution in Eager Execution. Getting a similar function except for the waste of time. Any other solutions are welcome.", "This is already in 1.14 and the usage is above.", "Is group convolution supported on TPU?", "@andravin \r\n\r\nUnfortunately, currently not supported.\r\nYou can have a try PyTorch on TPU if you are familiar with it, which originally supports group convolution.\r\n\r\nMore info is as follow:\r\n> https://github.com/tensorflow/tpu/issues/415", "Hi Andrew,\r\nI have not used a TPU, and if I'm not mistaken the TPU compiler stack running on the TPU server is closed-source so I cannot tell what it does. My best guess is that group conv is supported, since the XLA stack is able to lower the group conv operations (implemented in this PR), and some official TPU examples contain depthwise conv.\r\n\r\nUnrelated, but Pytorch apparently does not support group conv on TPU: https://github.com/pytorch/xla/blob/master/torch_xla/csrc/aten_xla_type.cpp#L814-L818", "1.14 is not yet supported on cloud TPU: https://cloud.google.com/tpu/docs/supported-versions\r\n", "Grouped convolutions are supported on TPUs.", "@andravin \r\nCloud TPU supports 1.14.1-dev20190508 and 1.14.1-dev20190518.\r\nYou can also find a nightly version of TensorFlow, and please ignore the inadequate documents.\r\n\r\n@amitsabne1 \r\nWould you please tell me which version supports the group convolution ops? \r\nThanks.", "AFAIK, grouped convolutions have been around on TPUs for a while; definitely since 1.12. Please let me know if you find otherwise.", "The XLA test for grouped convolutions \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/tests/grouped_convolution_test.cc\r\nworks on TPUs. The examples there may help you map back to TF programs.", "@amitsabne1 Thanks, but realistically I am not going to spend any more time on this.", "Grouped convolutions are still unsupported on CPUs btw...\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/29005", "why group convolution for half type is extremely slow? it can consume more than 100ms while a regular loop-based implementation only consume several ms. @ppwwyyxx ", "Hi, I implement the group convolution layer in KERAS, according to the Code example above.\r\nboth in LOOP way and in native tf.conv group convolution .\r\n\r\nI found a big problem here, the two ways have different parameter sizes, and different meaning.\r\nthe loop way has **32** small 3x3 filtters , the tf.conv way has **only one** 3x3 filtter. in a resnet-20 3stage configuration, the tf.conv way has 0.5million~ parameters fewer , and the model is much worse in accuracy.\r\n\r\naccording to the original paper:\"Aggregated Residual Transformations for Deep Neural Networks\", \r\n\r\n> section 3.3 AggregatedTransformations\r\n\r\n> F(x)=XTi(x),\r\n> We settheindividualtransformationTitobethebottleneck-\r\n shapedarchitecture[14],asillustratedinFig.1(right).In thiscase,the\ufb01rst1\u00d71layerineachTiproducesthelow-dimensionalembedding.\r\n![EasyCapture333](https://user-images.githubusercontent.com/60679873/74296903-2cbda480-4d80-11ea-947f-5fc1866fe82a.png)\r\n\r\nand it shows in the picture, each small filters better to be different, in order to have an aggregated effect.\r\n\r\nattach my code for reference:\r\n\r\n```\r\nclass GroupConv2D(Conv2D):\r\n    def __init__(self, filters,\r\n                 kernel_size,\r\n                 strides=(1, 1),\r\n                 padding='same',\r\n                 data_format=None,\r\n                 dilation_rate=(1, 1),\r\n                 num_group=1,\r\n                 activation=None,\r\n                 use_bias=True,\r\n                 kernel_initializer='glorot_uniform',\r\n                 bias_initializer='zeros',\r\n                 kernel_regularizer=None,\r\n                 bias_regularizer=None,\r\n                 activity_regularizer=None,\r\n                 kernel_constraint=None,\r\n                 bias_constraint=None,\r\n                 **kwargs):\r\n        super(GroupConv2D, self).__init__(\r\n            filters=filters,\r\n            kernel_size=kernel_size,\r\n            strides=strides,\r\n            padding=padding,\r\n            data_format=data_format,\r\n            dilation_rate=dilation_rate,\r\n            activation=activation,\r\n            use_bias=use_bias,\r\n            kernel_initializer=kernel_initializer,\r\n            bias_initializer=bias_initializer,\r\n            kernel_regularizer=kernel_regularizer,\r\n            bias_regularizer=bias_regularizer,\r\n            activity_regularizer=activity_regularizer,\r\n            kernel_constraint=kernel_constraint,\r\n            bias_constraint=bias_constraint,\r\n            **kwargs)\r\n        self.num_group = num_group\r\n        if self.filters % self.num_group != 0:\r\n            raise ValueError(\"filters must divided by num_group with no remainders!\")\r\n        #self.input_spec = InputSpec(ndim=4)\r\n\r\n    def build(self, input_shape):\r\n        \r\n        input_shape = tensor_shape.TensorShape(input_shape)\r\n\r\n        input_channel = self._get_input_channel(input_shape)\r\n\r\n        kernel_shape = self.kernel_size + (input_channel//self.num_group, self.filters)\r\n\r\n\r\n\r\n        self.kernel = self.add_weight(\r\n\r\n            name='kernel',\r\n\r\n            shape=kernel_shape,\r\n\r\n            initializer=self.kernel_initializer,\r\n\r\n            regularizer=self.kernel_regularizer,\r\n\r\n            constraint=self.kernel_constraint,\r\n\r\n            trainable=True,\r\n\r\n            dtype=self.dtype)\r\n\r\n        if self.use_bias:\r\n\r\n          self.bias = self.add_weight(\r\n\r\n              name='bias',\r\n\r\n              shape=(self.filters,),\r\n\r\n              initializer=self.bias_initializer,\r\n\r\n              regularizer=self.bias_regularizer,\r\n\r\n              constraint=self.bias_constraint,\r\n\r\n              trainable=True,\r\n\r\n              dtype=self.dtype)\r\n\r\n        else:\r\n\r\n          self.bias = None\r\n\r\n        channel_axis = self._get_channel_axis()\r\n\r\n        self.input_spec = InputSpec(ndim=self.rank + 2,\r\n\r\n                                    axes={channel_axis: input_channel})\r\n\r\n\r\n\r\n        self._build_conv_op_input_shape = input_shape\r\n\r\n        self._build_input_channel = input_channel\r\n\r\n        self._padding_op = self._get_padding_op()\r\n\r\n        self._conv_op_data_format = conv_utils.convert_data_format(\r\n\r\n            self.data_format, self.rank + 2)\r\n\r\n        self._convolution_op = nn_ops.Convolution(\r\n\r\n            input_shape,\r\n\r\n            filter_shape=self.kernel.shape,\r\n\r\n            dilation_rate=self.dilation_rate,\r\n\r\n            strides=self.strides,\r\n\r\n            padding=self._padding_op,\r\n\r\n            data_format=self._conv_op_data_format)\r\n\r\n        self.built = True\r\n\r\n    def get_config(self):\r\n        config = super(Conv2D, self).get_config()\r\n        config.pop('rank')\r\n        config[\"num_group\"] = self.num_group\r\n        return config\r\n\r\n\r\n\r\ndef grouped_convolution_block(x, grouped_channels, cardinality , strides,cudnn = True, weight_decay=1e-4):\r\n\r\n    \r\n\r\n    \r\n\r\n\r\n    group_list = []\r\n\r\n\r\n\r\n    if cardinality == 1:\r\n\r\n        # with cardinality 1, it is a standard convolution\r\n        x = BatchNormalization()(x)\r\n        \r\n        x = Activation('relu')(x)\r\n\r\n        x = Conv2D(grouped_channels, (3, 3), padding='same',  strides=(strides, strides),use_bias=True,\r\n\r\n                   kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(x)\r\n\r\n        return x\r\n\r\n\r\n    x = BatchNormalization()(x)\r\n        \r\n    x = Activation('relu')(x)\r\n    \r\n    if cudnn == True:\r\n        group_merge =  GroupConv2D(grouped_channels, (3, 3), padding='same', \r\n                                   use_bias=True, kernel_initializer='he_normal',strides=(strides, strides),\r\n                                        kernel_regularizer=l2(weight_decay))(x)\r\n    else:\r\n        input_list = tf.split(x, cardinality, axis=-1)\r\n        output_list = []\r\n        for conv_idx, input_tensor in enumerate(input_list):\r\n            tmp = Conv2D(grouped_channels, (3, 3), padding='same', use_bias=True, strides=(strides, strides),\r\n\r\n                       kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(input_tensor)\r\n\r\n            output_list.append(tmp)\r\n        group_merge = concatenate(output_list, axis=-1)\r\n\r\n\r\n\r\n\r\n    return group_merge\r\n\r\n\r\n```\r\n\r\nAnd the performances of two ways\uff1a\r\n\r\nloop:\r\nEpoch 6/60\r\n353/353 [==============================] - 83s 235ms/step - loss: 2.6220 - dense_1_loss: 1.4720  - dense_1_accuracy: 0.5991 - val_dense_1_accuracy: 0.5416\r\n\r\ncudnn:\r\nEpoch 6/60\r\n353/353 [==============================] - 53s 150ms/step - loss: 4.3920 - dense_1_loss: 2.8750  - dense_1_accuracy: 0.2673  - val_dense_1_accuracy: 0.2203 \r\n\r\n", "The code in the PR above uses the same filter for both branch, so apparently they have the same size. ", "@ppwwyyxx\r\n\r\nI see ,the code above using the same filter for both branch, \r\nbut it doesn't solve the problem in implementing RESNEXT ,\r\nApparantly, RESNEXT's group convolution needs 32 different filters, not one for 32 group.", "You probably have misunderstanding of resnext.", "at least, In my exeperiment , using the group convolution code here constructing resnext is not on par of resnet of same parameter size,\r\nusing loop group convolution code, resnext can perform better than resnet. ", "My experiments using this PR worked (https://github.com/tensorpack/tensorpack/tree/master/examples/ResNet).", "\r\n> My experiments using this PR worked (https://github.com/tensorpack/tensorpack/tree/master/examples/ResNet).\r\n\r\nI think I made a mistake in my code, I set the group to 1 by default and forgot to set them to 4.\r\nThanks for your patience.", "and in \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py\r\nin \r\nclass Convolution(object):\r\n```\r\n\r\nif not input_channels_dim.is_compatible_with(\r\n\r\n        filter_shape[num_spatial_dims]):\r\n\r\n      raise ValueError(\r\n\r\n          \"number of input channels does not match corresponding dimension of \"\r\n\r\n          \"filter, {} != {}\".format(input_channels_dim,\r\n\r\n                                    filter_shape[num_spatial_dims]))\r\n\r\n```\r\nshould be deleted too\r\n", "Hi, could the group convolution be used on CPU? I tried it, but error happened for an un-matched tensor shape, but the same code works on GPU."]}, {"number": 25817, "title": "ERROR: /home/max/tensorflow/tensorflow/python/eager/BUILD:10:1: undeclared inclusion(s) in rule '//tensorflow/python/eager:pywrap_tfe_lib':", "body": "****System information**\r\n\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n* TensorFlow installed from (source or binary): source\r\n* TensorFlow version: 1.13.0rc2\r\n* Python version: 3.7.2\r\n* Installed using virtualenv? pip? conda?: pip\r\n* Bazel version (if compiling from source): 0.22\r\n* GCC/Compiler version (if compiling from source): 6.5.0\r\n* CUDA/cuDNN version: 10.0/7.3\r\n* GPU model and memory: RTX 2070 and 8GB\r\n\r\n**Describe the problem**\r\n\r\nERROR: /home/max/tensorflow/tensorflow/python/eager/BUILD:10:1: undeclared inclusion(s) in rule '//tensorflow/python/eager:pywrap_tfe_lib':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/python/eager/pywrap_tensor.cc':\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/arrayobject.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/ndarrayobject.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/ndarraytypes.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_common.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/numpyconfig.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/_numpyconfig.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_endian.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_cpu.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/utils.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/_neighborhood_iterator_imp.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/__multiarray_api.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_interrupt.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/ufuncobject.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_math.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_common.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/arrayobject.h'\r\n'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/__ufunc_api.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build**", "comments": ["downgrade to Python 2 fixed this. ", "@lorenzesz Did you try TF2.0-preview? Thanks!", "I have compiled it with python 3.6 now. 2.0 is not ready for development, but I can compile for testing this night.", "@lorenzesz Is this issue resolved? If yes, please close the issue. Thanks!", "Does it work with python 3? I don't believe you can compile tensorflow only with python 2."]}, {"number": 25816, "title": "Update sequential.py. Found typo", "body": "Typo in the docs", "comments": ["Thank you , Change was made internally, so closing this PR"]}, {"number": 25815, "title": "Replace math_ops.div with math_ops.divide", "body": "While testing the issue of #23709, noticed several warnings:\r\n```\r\nW0217 12:33:32.155959 139635342989120 deprecation.py:323] \\\r\n    From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:106: \\\r\n    div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a \\\r\n    future version.\r\n```\r\nThis fix replaces math_ops.div with math_ops.divide to remove the warning\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 25814, "title": "Batchnorm does not work in Eager mode in TF 1.12: InternalError", "body": " Could not find valid device for node ", "comments": ["@ameyinvent Please provide the details requested in the [template](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md). Please provide as many details as possible to find rootcause of the bug. Also, check this [resource](https://github.com/tensorflow/tensorflow/issues/25525) that might help you. Thanks!", "@ameyinvent Could you provide a code to reproduce the bug? If this was resolved already, please close the issue or let me know. Thanks!", "I think it was resolved. Closing due to lack of recent activity. Please open new ticket if you see similar issue. Thanks!"]}, {"number": 25813, "title": "Expose layer_norm variance_epsilon as an argument", "body": "In some cases it is necessary to change the epsilon value in order to avoid numerical instabilities during gradient computation. \r\nThis makes it consistent with batch_norm, which also exposes the epsilon value. The epsilon is exposed with the same name (i.e. \"epsilon\") as in batch norm.", "comments": ["contrib is deprecated and will soon disappear. The core layer has an epsilon parameter already. I will close this PR. "]}, {"number": 25812, "title": "Ingesting Tensorflow in larger (Bazel) project", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 16.04\r\n- TensorFlow version: 1.10\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: Custom\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.2/7\r\n- GPU model and memory: compute 6.1\r\n\r\n**Describe the problem**\r\nI am trying to use Tensorflow in a repository which also uses Bazel as build system.\r\nCurrently, Tensorflow C++ API library is pre-compiled and ingested as cc_library; however, I am trying to ingest it and build it from source directly since I need to support a number of configurations (and pre-compiling them is a pain)\r\n\r\nI have been successful with a CPU only version. I did this by following the method used by Tensorflow Serving. I included the following in my WORKSPACE file\r\n\r\n```\r\nload(\"//repo.bzl\", \"tensorflow_http_archive\")\r\n\r\ntensorflow_http_archive(\r\n    name = \"org_tensorflow\",\r\n    git_commit = \"355cc566efd2d86fe71fa9d755ceabe546d577a7\",\r\n    sha256 = \"a19b64d5c7aecb8487302843b27c8fde916dc08df8d9c2fdded20d94e0947c34\",\r\n)\r\n\r\nload(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\r\n\r\n# TensorFlow depends on \"io_bazel_rules_closure\" so we need this here.\r\n# Needs to be kept in sync with the same target in TensorFlow's WORKSPACE file.\r\nhttp_archive(\r\n    name = \"io_bazel_rules_closure\",\r\n    sha256 = \"a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae\",\r\n    strip_prefix = \"rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1\",\r\n    urls = [\r\n        \"https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz\",\r\n        \"https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz\",  # 2018-04-13\r\n    ],\r\n)\r\n\r\n# Specify the minimum required bazel version.\r\nload(\"@org_tensorflow//tensorflow:version_check.bzl\", \"check_bazel_version_at_least\")\r\n\r\ncheck_bazel_version_at_least(\"0.20.0\")\r\n\r\nload(\"@org_tensorflow//tensorflow:workspace.bzl\", \"tf_workspace\")\r\n\r\ntf_workspace(\r\n    path_prefix = \"\",\r\n    tf_repo_name = \"org_tensorflow\",\r\n)\r\n```\r\n\r\nwhere I defined repo.bzl as \r\n\r\n```\r\n\"\"\" TensorFlow Http Archive\r\nModified http_archive that allows us to override the TensorFlow commit that is\r\ndownloaded by setting an environment variable. This override is to be used for\r\ntesting purposes.\r\nAdd the following to your Bazel build command in order to override the\r\nTensorFlow revision.\r\nbuild: --action_env TF_REVISION=\"<git commit hash>\"\r\n  * `TF_REVISION`: tensorflow revision override (git commit hash)\r\n\"\"\"\r\n\r\n_TF_REVISION = \"TF_REVISION\"\r\n\r\ndef _tensorflow_http_archive(ctx):\r\n    git_commit = ctx.attr.git_commit\r\n    sha256 = ctx.attr.sha256\r\n\r\n    override_git_commit = ctx.os.environ.get(_TF_REVISION)\r\n    if override_git_commit:\r\n        sha256 = \"\"\r\n        git_commit = override_git_commit\r\n\r\n    strip_prefix = \"tensorflow-%s\" % git_commit\r\n    urls = [\r\n        \"https://mirror.bazel.build/github.com/tensorflow/tensorflow/archive/%s.tar.gz\" % git_commit,\r\n        \"https://github.com/tensorflow/tensorflow/archive/%s.tar.gz\" % git_commit,\r\n    ]\r\n    ctx.download_and_extract(\r\n        urls,\r\n        \"\",\r\n        sha256,\r\n        \"\",\r\n        strip_prefix,\r\n    )\r\n\r\ntensorflow_http_archive = repository_rule(\r\n    implementation = _tensorflow_http_archive,\r\n    attrs = {\r\n        \"git_commit\": attr.string(mandatory = True),\r\n        \"sha256\": attr.string(mandatory = True),\r\n    },\r\n)\r\n```\r\n\r\nHowever, once I try using the GPU version, I run into problems... I have tried this by adding the following to my .bazelrc file, which are the same as the onces generated by a manual ./configure step in a standalone Tensorflow build\r\n\r\n```\r\n# Options for CUDA enabled Tensorflow\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\nbuild --action_env TF_CUDA_VERSION=\"9.2\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr/local/cuda-9.2\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_NCCL_VERSION=\"1\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/usr/local/cuda-9.2/lib64:\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --define=using_cuda=true --define=using_cuda_nvcc=true\r\n```\r\n\r\nI get the following error:\r\n```\r\n/cuda_configure.bzl\", line 423, in cuda_toolkit_path\r\n                auto_configure_fail(\"Cannot find cuda toolkit path.\")\r\n        File \"/home/ferronfr/.cache/bazel/_bazel_ferronfr/fb0eee7f7437f761aece7d670e37cf54/external/org_tensorflow/third_party/gpus/cuda_configure.bzl\", line 342, in auto_configure_fail\r\n                fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find cuda toolkit path.\r\n```\r\n\r\nDoes anybody know how to solve this?\r\n\r\nCheers!", "comments": ["Please check the [tested build config](https://www.tensorflow.org/install/source#tested_build_configurations). For  TF1.10, CUDA/cuDNN 9.0/7.0 is supported. Did you had a chance to migrate to TF1.12? Thanks!", "Hi, \r\n\r\nActually the reason why the CUDA toolkit is \"missing\" is that in my Bazel project, the CUDA toolkit is itself a Bazel library target, CUDNN library is another target, etc so that's why the Tensorflow configuration doesn't find it, because it lives in some crazy long path in Bazel cache like \r\n`/home/ferronfr/.cache/bazel/_bazel_ferronfr/fb0eee7f7437f761aece7d670e37cf54/cuda_pc_amd64` \r\n\r\nDigging a bit, it seems Tensorflow only uses the environment variables defined by the --action_env to create a @local_config_cuda target in [`cuda_configure.bzl`](https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda_configure.bzl). This is over a thousand lines of scripting so I'm a bit lost...but I am just wondering if with the current targets cuda and cudnn library targets I have available in my Bazel project, I can somehow hand these over to Tensorflow? It seems plausible since there is a function called `_create_remote_cuda_repository()` ?\r\n\r\nI tried also TF1.12 but the underlying issue is the same... thanks!", "Could you try running `configure.py` from your main repository root?\r\n@r4nt may also have some ideas.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 25811, "title": "toco: error: argument --output_file is required", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n`IMAGE_SIZE=224\r\ntoco \\\r\n  --input_file=tf_files/retrained_graph.pb \\\r\n  --output_file=tf_files/optimized_graph.tflite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \\\r\n  --input_array=input \\\r\n  --output_array=final_result \\\r\n  --inference_type=FLOAT \\\r\n  --input_data_type=FLOAT`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nIm using windows subsytems for linux\r\n- TensorFlow version (use command below):\r\ntensorflow 1.12\r\n- Python version:\r\n2.7\r\n\r\n**Describe the current behavior**\r\ni want to convert my custom model to tflite as stated in [this](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#2) but it gives a toco error saying toco: error: argument --output_file is required\r\n\r\n\r\n**Other info / logs**\r\n\r\nthis was the whole log:\r\n`usage: toco [-h] --output_file OUTPUT_FILE\r\n            (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE)\r\n            [--output_format {TFLITE,GRAPHVIZ_DOT}]\r\n            [--inference_type {FLOAT,QUANTIZED_UINT8}]\r\n            [--inference_input_type {FLOAT,QUANTIZED_UINT8}]\r\n            [--input_arrays INPUT_ARRAYS] [--input_shapes INPUT_SHAPES]\r\n            [--output_arrays OUTPUT_ARRAYS]\r\n            [--saved_model_tag_set SAVED_MODEL_TAG_SET]\r\n            [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]\r\n            [--std_dev_values STD_DEV_VALUES] [--mean_values MEAN_VALUES]\r\n            [--default_ranges_min DEFAULT_RANGES_MIN]\r\n            [--default_ranges_max DEFAULT_RANGES_MAX]\r\n            [--post_training_quantize] [--drop_control_dependency]\r\n            [--reorder_across_fake_quant]\r\n            [--change_concat_input_ranges {TRUE,FALSE}] [--allow_custom_ops]\r\n            [--converter_mode {DEFAULT,TOCO_FLEX,TOCO_FLEX_ALL}]\r\n            [--dump_graphviz_dir DUMP_GRAPHVIZ_DIR] [--dump_graphviz_video]\r\ntoco: error: argument --output_file is required`", "comments": ["duplicate of #20984", "already solved this with reinstalling tensorflow as `pip install \u2013 user tensorflow==1.12.0 \u2013ignore-installed`"]}, {"number": 25809, "title": "Added tests for tensors, inputs, outputs, opcodes", "body": "This was one of the TODO, just complete the same", "comments": ["@rthadur , can you pls help to get this merged, as this is approved for long and i am afraid that if it stays longer again some merge conflicts might come.\r\n\r\nRegards\r\nAmit", "@rthadur, can you please help to get this merged, this is approved for long.\r\n\r\nRegards \r\nAmit ", "> @rthadur, can you please help to get this merged, this is approved for long.\r\n> \r\n> Regards\r\n> Amit\r\n\r\nSure", "> > @rthadur, can you please help to get this merged, this is approved for long.\r\n> > Regards\r\n> > Amit\r\n> \r\n> Sure\r\n\r\n@rthadur , thanks for the reply, can you pls help to get this merged.\r\n\r\nRegards\r\nAmit", "@shashishekhar , thanks for approving the PR can you please help me to merge this PR, this is approved for long time.\r\n\r\nRegards\r\nAmit", "@rthadur , this is a long pending approved PR, can you please help to get this merged.\r\n\r\nRegards\r\nAmit", "@rthadur , this is a long pending approved PR, with all necessary checks passed,\r\n can you please help to get this merged.\r\n\r\nRegards\r\nAmit", "@rthadur , can you please help to merge this PR.\r\n\r\nRegards\r\nAmit", "> @rthadur , can you please help to merge this PR.\r\n> \r\n> Regards\r\n> Amit\r\n\r\ninitially failed internal tests , will try to merge this again", "@amitsrivastava78 here is the internal error \r\n\r\n`ERROR: /tmpfs/tensor_flow/tensorflow/lite/toco/tflite/BUILD:104:1: Couldn't build file tensorflow/lite/toco/tflite/_objs/export_test/export_test.o: C++ compilation of rule '//tensorflow/lite/toco/tflite:export_test' failed (Exit 1): clang failed: error executing command \r\n  (cd /tmpfs/tmp/bazel/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    BAZEL_DO_NOT_DETECT_CPP_TOOLCHAIN=1 \\\r\n    PATH=/bin:/usr/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python2 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_PYTHON_CONFIG_REPO=@org_tensorflow//third_party/toolchains/cpus/py \\`\r\ncan you please check.", "@rthadur , thanks for the update the PR is quite old and the member of struct ExportParams got little change, i have resolved the issue now.\r\n\r\n@shashishekhar , I know it is old PR, but can you please re-approve the same.\r\n\r\nRegards\r\nAmit", "Can one of the admins verify this patch?", "@shashishekhar Can you please take a look on this PR? Thanks!", "@karimnosseir can you review?", "@amitsrivastava78 can you please check below errors \r\n\r\n`tensorflow/lite/toco/tflite/export_test.cc:879:3: error: ignoring return value of function declared with 'warn_unused_result' attribute\r\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~/tensorflow/lite/toco/tflite/export_test.cc:901:3: error: ignoring return value of function declared with 'warn_unused_result' attribute \r\n  Export(input_model_, &result, params);\r\n  ^~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/lite/toco/tflite/export_test.cc:922:3: error: ignoring return value of function declared with 'warn_unused_result' attribute \r\n  Export(input_model_, &result, params);\r\n  ^~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~/tensorflow/lite/toco/tflite/export_test.cc:946:3: error: ignoring return value of function declared with 'warn_unused_result' attribute \r\n  Export(input_model_, &result, params);\r\n  ^~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n4 errors generated.`", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 25808, "title": "'tensorflow' has no attribute 'enable_eager_execution'", "body": "**System information**\r\n-Google Colab \r\n-GPU\r\n-Python 3\r\n-Tensorflow tf-nightly-2.0-preview\r\n\r\nWhen I write the code:\r\n  `from __future__ import absolute_import, division, print_function`\r\n  `!pip install tf-nightly-2.0-preview`\r\n  `import tensorflow as tf`\r\n  `tf.enable_eager_execution()`\r\nIt gives me the error **AttributeError: 'module' object has no attribute 'enable_eager_execution'**\r\n\r\nwhereas when I simply run the code as:\r\n  `from __future__ import absolute_import, division, print_function`\r\n  `!pip install tf-nightly-2.0-preview`\r\n  `import tensorflow as tf`\r\n  `tf.executing_eagerly()` \r\nIt returns **'True'**, which means that eager execution is working by default without even enabling it before.\r\nIf I proceed without using tf.enable_eager_execution() in this collab tutorial [https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/image_captioning.ipynb#scrollTo=U8l4RJ0XRPEm](url)\r\nIn the cell Caching the features extracted from InceptionV3, \r\nthe last line with \"np.save(path_of_feature, bf.numpy())\" gives me an error saying **\"Tensor has no attribute called numpy()\"**\r\nIf I see the type() of batch_features in the line:\r\n`batch_features = image_features_extract_model(img)` line is returning a graph tensor, instead of an eager tensor. It's not supposed to do that. Does this means that eager execution is not yet enabled or is this a bug.", "comments": ["hey @howareyoudoing  even i am facing the same error while following this colab tutorial.\r\nSomone has already raised this issue precisely and someone from the tensorflow team has taken up this  issue to resolve so you could probably watch this issue page for now..\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/25731", "Thanks for providing the link.", "Closing this issue since its a duplicate of #25731.\r\nLets track it there. Thanks!"]}, {"number": 25807, "title": "tensorflow/stream_executor/dso_loader.cc couldn't import cupti64_100.dll which exists in path", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): gpu 1.13.0rc2\r\n- Python version: 3.7.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.0/7.4.2\r\n- GPU model and memory: RTX2080Ti GDDR6 11GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n2019-02-17 16:49:22.874964: I tensorflow/stream_executor/dso_loader.cc:142] Couldn't open CUDA library cupti64_100.dll\r\n2019-02-17 16:49:22.876813: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Failed precondition: could not dlopen DSO: cupti64_100.dll; dlerror: cupti64_100.dll not found\r\n\r\n**Describe the expected behavior**\r\nshould import properly because it exists in path\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\npython .\\mnist_softmax_xla.py  in mnist tutorial\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nit works until few days ago\r\nonly i do recently is update nvidia driver to 418.91", "comments": ["![image](https://user-images.githubusercontent.com/4515120/52910032-06803880-32d5-11e9-87f2-a4a7627d0c30.png)\r\n\r\n![image](https://user-images.githubusercontent.com/4515120/52910040-1dbf2600-32d5-11e9-9d79-ad55149e04fb.png)\r\n", "same problem in other windows 10 machine, which has geforce mx150 GDDR5 2GB", "Reassigning this to @yifeif, since I think she made the recent switch to dynamically loading CUDA DLLs, and might know of any pitfalls here. ", "That is very odd. We didn't change the behavior of dsoloader or how we handle cupti.\r\n@alanpurple when it worked for you before, was it the same pip package etc, and the only difference is the driver version?", "@yifeif \r\nI uninstalled CUDA and reinstalled it, of course copy cudnn files and check CUPTI lib path again,\r\nbut same result\r\n\r\nhas anyone with latest windows 10 up to date + latest nvidia driver with 1.13.0rc2 binary build ever ( since I post this issue )succeeded to open cupti64_100.dll?", "strange thing is, yet cublas dll is successfully loaded", "I have no idea what's related, but I just \"git pull\"ed and this problem has been resolved\r\n\r\nclosing this issue\r\n\r\n\r\n\r\nAh, I update conda python, which is same 3.7.2 but different build\r\n\r\nmaybe this wasn't related to tensorflow", "confirmed.\r\n\r\nIn other machine this is resolved after conda upgrade --all( which upgrade python from 3.7.2-blahblah to 3.7.2-otherblahblah)", "Glad it's fixed!", "I just installed TF 2 and updated my CUDA from `v9` to `v10` along with cuDNN but I am still getting similar error, environment variables are in place in Windows 10, system has been restarted, dlls are available in `\\bin` under `CUDA\\v10.0`\r\n\r\n`W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found`", "Same error occurred here. \r\n\r\n```\r\ntensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found.\r\n```\r\n\r\nI don't know it is just related with that installed CUDA's version is `v10.1`, in my case.\r\n\r\n- tensorflow 2.0.0\r\n- CUDA 10.1\r\n- windows 10 x64\r\n- cuDNN 7.6.3\r\n- gtx1060\r\n\r\nof course, there are `cudart64_101.dll` in `CUDA\\v.10.1`.", "Check the environment variable and make sure it points to `v10.0\\bin` instead of `\\v10.0` only\r\n\r\nI spent way too long figuring this out than I should have \ud83d\ude14", "So, is this problem solved? I have all the requirements fulfilled and still get the error. If this is solved then where is the solution?", "In the Anaconda virtual environment of tensorflow-gpu 2.0.0, Windows 10, python 3.6.x or 3.7.x, CUDA 10.0.x, cuDNN 7.6.x, and 1 x GeForce GTX 960M (4097 MiB), I have the same problem too, although PATH is correctly set including relevant dynamical libraries.\r\n\r\nError message: 2019-10-15 00:01:37.822980: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\r\n", "> In the Anaconda virtual environment of tensorflow-gpu 2.0.0, Windows 10, python 3.6.x or 3.7.x, CUDA 10.0.x, cuDNN 7.6.x, and 1 x GeForce GTX 960M (4097 MiB), I have the same problem too, although PATH is correctly set including relevant dynamical libraries.\r\n> \r\n> Error message: 2019-10-15 00:01:37.822980: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\r\n \r\n@drag05 @tonandr Did you see my comment above? make sure you are not making the same mistake as I did", "Yes @zubairahmed-ai I have seen your comments above. Anything else? ", "Yes, I checked @zubairahmed-ai's comments. In my case, after upgrading tensorflow to 2.0, this error happens. Still, tensorflow under 1.4 is imported correctly.", "You should install Cuda 10.0. If you installed Cuda 10.1, there would be no file `cudart64_100.dll`. I rename `cudart64_101.dll` to `cudart64_100.dll,` then it work.", "@HaiDuong93 : Not necessarily true as it seems that _cudart64_100.dll_ resides in anaconda or miniconda\\pkgs\\cudatoolkit. \r\n\r\nPlease see my results regarding: tps://github.com/tensorflow/tensorflow/issues/33319#issuecomment-542253199 and: https://github.com/tensorflow/tensorflow/issues/33319#issuecomment-542363704\r\n\r\nIn my case tensorflow-gpu v2.0.0 installation worked with last version CUDA/cuDNN (v. 10.1/ 7 respectively) and last version NVIDIA drivers by avoiding tensorflow-gpu installation through `keras::install_keras()` or `tensorflow::install_tensorflow()` functions but instead, just using `pip install` in conda prompt - in my case - miniconda.\r\n\r\nThank you!", "My case isn't resolved yet.", "@tonandr Sorry to hear that! Here is my 2 cents:\r\n- if you have a cuda environment built for tensorflow, remove it completely (in my case, I have completely uninstalled Anaconda and opted for Miniconda instead - half the size on disk)\r\n- make sure you have the latest GPU drivers installed and also CUDA/CUDNN with Environment variables set (I had to say that!)\r\n\r\n- open conda  prompt and (in base environment) write the following line:\r\n\r\n`conda create -n tensorflow tensorflow-gpu`\r\n\r\nthis will automatically solve the version compatibility issue and will install the right version of python, tensorflow-gpu and cudatoolkit\r\n\r\n- move to tensorflow environment \r\n\r\n` conda activate tensorflow`\r\n\r\n- make sure you have cudatoolkit, python and tensorflow-gpu installed\r\n\r\n`conda list`\r\n\r\n- from tensorflow environment check tensorflow-gpu configuration by calling python:\r\n```\r\npython\r\nimport tensorflow as tf\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n```\r\nif everything is ok a relatively long output will follow mentioning your gpu card type etc.\r\nThis means that your gpu is recognized and configured to work with tensorflow (at least this is what happened in my case).\r\n\r\nYou can force the installation of tensorflow v.2.0.0 by modifying the line above\r\n`conda create -n tensorflow python=3.6.8` followed by a \r\n\r\n`pip install tensorflow-gpu=2.0.0`\r\n\r\nbut I would not recommend this for now as I do not recommend using  \r\n\r\n`keras::install_keras(tensorflow-gpu)` or `tensorflow::install_tensorflow()` \r\n\r\nfrom R Studio (see related Issue #33695)\r\n\r\nI hope this helps!", " Hello @tonandr, further to my earlier answer, you may want to check again this likely solved issue #33695 now closed. It clarifies some aspects on installing TensorfFlow from Python using Nvidia CUDA/CUDNN or installing TensorFlow from *conda using `cudatoolkit` in which case the Nvidia CUDA installation is not necessary.", "I install CUDA v9 is that okey for the INTEL processor?\r\n", "You need a NVIDIA graphics card for CUDA. For example, check this https://developer.nvidia.com/cuda-gpus or https://www.khronos.org/conformance/adopters/conformant-products/opencl", "I think this error is related to tf.summary. To fix it, simply copy over cupti.lib and cupti64_100.dll in v10.0\\extras\\CUPTI\\libx64 to v10.0\\bin", "> You need a NVIDIA graphics card for CUDA. For example, check this https://developer.nvidia.com/cuda-gpus or https://www.khronos.org/conformance/adopters/conformant-products/opencl\r\n\r\nThank you...", "> _Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template_\r\n> \r\n> **System information**\r\n> \r\n> * Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n> * TensorFlow installed from (source or binary): binary\r\n> * TensorFlow version (use command below): gpu 1.13.0rc2\r\n> * Python version: 3.7.2\r\n> * Bazel version (if compiling from source): N/A\r\n> * GCC/Compiler version (if compiling from source): N/A\r\n> * CUDA/cuDNN version: 10.0/7.4.2\r\n> * GPU model and memory: RTX2080Ti GDDR6 11GB\r\n> \r\n> You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n> You can also obtain the TensorFlow version with\r\n> python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n> \r\n> **Describe the current behavior**\r\n> 2019-02-17 16:49:22.874964: I tensorflow/stream_executor/dso_loader.cc:142] Couldn't open CUDA library cupti64_100.dll\r\n> 2019-02-17 16:49:22.876813: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Failed precondition: could not dlopen DSO: cupti64_100.dll; dlerror: cupti64_100.dll not found\r\n> \r\n> **Describe the expected behavior**\r\n> should import properly because it exists in path\r\n> \r\n> **Code to reproduce the issue**\r\n> Provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n> python .\\mnist_softmax_xla.py in mnist tutorial\r\n> \r\n> **Other info / logs**\r\n> Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n> \r\n> it works until few days ago\r\n> only i do recently is update nvidia driver to 418.91\r\n\r\nI found this issue too,and you can just install `cudatoolkit` using conda, and version is same as the message system report, my error message is:dlerror: cudart64_100.dll not found\r\nSo I use: `conda install cudatollkit==10.0` to fix it.", "My fix is uninstalling the several version I had on my computer and reinstall cuda v10.\r\n\r\nWith this clean installation the folder does not have a 'v10' in it.\r\n\r\nI'm on windows.", "I fixed this by adding \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\extras\\CUPTI\\libx64\" to my Windows PATH variable and restarting python/jupyter and so on which was not in it before. \r\nI installed TF with pip under Anaconda", "For Windows users.\r\n\r\nIf the PATH variable is too long it won't work, you'll have to move it to the beginning.\r\n\r\n![image](https://user-images.githubusercontent.com/47270244/73134865-ad746500-404c-11ea-8ab8-e3403eff466a.png)\r\n\r\nSimply adding the path to the PATH environment variables isn't gonna work if PATH is too long, so I just moved to the beginning and it worked.\r\n\r\nThis happens because the PATH get's truncated (because there is a maximum length).\r\n\r\nI hope this helps\r\n", "> In the Anaconda virtual environment of tensorflow-gpu 2.0.0, Windows 10, python 3.6.x or 3.7.x, CUDA 10.0.x, cuDNN 7.6.x, and 1 x GeForce GTX 960M (4097 MiB), I have the same problem too, although PATH is correctly set including relevant dynamical libraries.\r\n> \r\n> Error message: 2019-10-15 00:01:37.822980: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\r\n\r\nHi I have the same issue and fixed it with renaming all missing DLLs that I have them with _10.Dll\r\nlike it: I have cudart64_10.dll rename it to ==> cudart64_100.dll\r\nthis all DLLs are in CUDA10.1  ", "\r\n![](https://user-images.githubusercontent.com/47270244/73134865-ad746500-404c-11ea-8ab8-e3403eff466a.png)\r\n\r\nWhere can I edit this things?!  ", "I run in Anaconda and met the same problem.\r\nI have CUDA install on my PC, and I copied the file\r\n\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\extras\\CUPTI\\libx64\\cupti64_100.dll\r\n\r\nto the path of Anaconda's envs\r\n\r\n> C:\\Users\\Coco\\AppData\\Local\\Continuum\\anaconda3\\envs\\ACE_DC_py372\\Library\\bin\r\n\r\nand the kernel died disappeared!", "How did you solve the issue\r\n", "> How did you solve the issue\r\n\r\nHi @Rifat004 ,\r\n\r\nThere are many solutions provided above, I don't know which one are you asking for (?)\r\n(Since you didn't describe your case, there's no clue at all...)\r\nIn my case, the running env is in Anaconda, and it seems like it couldn't find **cupti64_100.dll** under my env's running path,\r\nso I copied that file from CUDA's directory I install from [nvidia's website](https://developer.nvidia.com/cuda-10.0-download-archive)\r\nHope this help!", "@zubairahmed-ai @baicaigithub  i am on windows with cudav9 and python3.7 i am still getting the error \"tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\" i have installed tensorflow-gpu =1.15.0 can you let me knw what the solution for this ", "I also can't seem to get any of these solutions to work for me. Here is my error:\r\n```\r\n2021-06-06 15:22:42.083594: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-06-06 15:22:42.083750: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n```\r\n\r\nI have a GTX 1070. I just installed CUDA 11.3, and I can see the `cudart64_110.dll` file inside `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\bin`, and my environment variables look fine.\r\n\r\nEnvironment:\r\n```\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\r\nCUDA_PATH_V10_1=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\r\nCUDA_PATH_V11_2=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\r\nCUDA_PATH_V11_3=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\r\n```\r\nAll of those different versions of CUDA are installed, and confirmed definitely exist in those folders.\r\n`C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\bin` is in my `PATH` as well.", "> \r\n> \r\n> @zubairahmed-ai @baicaigithub i am on windows with cudav9 and python3.7 i am still getting the error \"tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\" i have installed tensorflow-gpu =1.15.0 can you let me knw what the solution for this\r\n\r\nThis cann't work for you if you are on V9", "> \r\n> \r\n> I also can't seem to get any of these solutions to work for me. Here is my error:\r\n> \r\n> ```\r\n> 2021-06-06 15:22:42.083594: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n> 2021-06-06 15:22:42.083750: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n> ```\r\n> \r\n> I have a GTX 1070. I just installed CUDA 11.3, and I can see the `cudart64_110.dll` file inside `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\bin`, and my environment variables look fine.\r\n> \r\n> Environment:\r\n> \r\n> ```\r\n> CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\r\n> CUDA_PATH_V10_1=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\r\n> CUDA_PATH_V11_2=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\r\n> CUDA_PATH_V11_3=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\r\n> ```\r\n> \r\n> All of those different versions of CUDA are installed, and confirmed definitely exist in those folders.\r\n> `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\bin` is in my `PATH` as well.\r\n\r\nLook at your error. It is looking for V11.0", "@GGDRriedel All the 11.x versions contain a DLL with the same name. I installed 11.0 and I still get the same error.\r\n\r\nEdit: I followed the Windows instructions at the bottom of the page [here](https://www.tensorflow.org/install/gpu). I added the path to cuDNN to the PATH, and the paths to the other directories mentioned there. Still getting the same error.", "> \r\n> \r\n> @GGDRriedel All the 11.x versions contain a DLL with the same name. I installed 11.0 and I still get the same error.\r\n> \r\n> Edit: I followed the Windows instructions at the bottom of the page [here](https://www.tensorflow.org/install/gpu). I added the path to cuDNN to the PATH, and the paths to the other directories mentioned there. Still getting the same error.\r\n\r\nTrue, but I'm not sure the correct folder is found. I think it's looking for a folder named v11.0", "@DavidS32 \r\n> Where can I edit this things?!\r\n\r\n[Windows environment variables](https://docs.oracle.com/en/database/oracle/machine-learning/oml4r/1.5.1/oread/creating-and-modifying-environment-variables-on-windows.html#GUID-DD6F9982-60D5-48F6-8270-A27EC53807D0), set the PATH variable\r\n\r\n\r\n"]}, {"number": 25806, "title": "About gradient reduction in DistributionStrategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below): master\r\n- Python version: 2.3.7\r\n- Bazel version (if compiling from source): 0.2.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 9.0, cudnn 7\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n1. Currently the gradient reduction in `distributed_apply` is `SUM` not `MEAN`. This makes learning rate adjustment more confusing. In general, learning rate should be adjusted according to total batch size. If it is `SUM` case, the learning rate adjustment will be different between single mode and distribution mode given the same total batch size.  \r\n\r\n2. The gradient reduction is implemented in `_distributed_apply` function. That means the gradient clipping will be done before gradient reduction. This is not identical with single mode because the gradient clipping should be done after reduction along the `batch size` dimenstion. Another, in `clip_by_global_norm` case, it blocks the overlapping between computation and communication. The performance is even worse then using `CollectiveAllReduceStrategy`. I have tried to put the gradient reduction in `def gradients` instead of `_distributed_apply` locally. If you think this is necessary, I will refine my code and submit a pull request here.\r\n \r\n**Describe the expected behavior**\r\n1. Change the gradient reduction from `SUM` to `MEAN`\r\n2. Move gradient reduction from `_distributed_apply` to `def gradients`\r\n\r\n**Other info / logs**\r\nI will contribute for it if you think the above are necessary. Thanks. @yuefengz \r\n", "comments": ["For 1), the gradient reduction is `SUM` because there is no nccl mean. Using `SUM` avoids the extra division for all gradients. We actually [scaled down the loss](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/losses/losses_impl.py#L195) on each replica so that the gradient reduction is effectively `MEAN`.\r\n\r\nFor 2), let me think about it. @guptapriya can also comment on it.", "@yuefengz\r\nFor 1), I have some questions to ask.\r\n1. For scaling down the loss, There must be another two conditions for taking it into effect.  a) To build loss function by using `tf.losses.softmax_cross_entropy` instead of `tf.nn.softmax_cross_entropy`. b) [`reduction](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/losses/losses_impl.py#L722)` params should be set to `MEAN` from outside, right?\r\n2. `Estimator` will [do `SUM` for loss](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/estimator.py#L1270). So it just have effects on displaying the loss, right? ", "@yuefengz For 1), it seems that [loss is only scaled down](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/estimator.py#L1300-L1328) for display's convenience, while at this time the replica [training graph has been built](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/estimator.py#L1294-L1299), which means each replica will use their own loss at backward pass.  Hence in my opinion the gradient is still `SUM` but not `MEAN`.\r\n\r\n> For 1), the gradient reduction is `SUM` because there is no nccl mean. Using `SUM` avoids the extra division for all gradients. We actually [scaled down the loss](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/losses/losses_impl.py#L195) on each replica so that the gradient reduction is effectively `MEAN`.\r\n> \r\n> For 2), let me think about it. @guptapriya can also comment on it.\r\n\r\n", "Loss scaling happens in different places depending on which high level API and optimizer you're using. If you're using estimator with TF 1 optimizer, loss scaling happens in [optimizer.compute_gradients](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py#L468). In 2.0 when using Keras + TF 2 optimizer, the scaling happens inside Keras instead of Optimizer. \r\n\r\n@fanshiqing the code you linked to about loss is not scaling it, it is just aggregating the loss from different replicas. By this point, the loss from each replica has already been scaled (by the number of replicas). \r\n\r\n@wangsiyu - yes the loss reduction type must be MEAN for any loss scaling to happen. If loss reduction type is SUM, we will just be doing SUM of gradients across replicas as well (which matches what you would do on one replica as well)\r\n\r\nTo recap, we (1) scale the loss on each replica by dividing by number of replicas, (2) compute gradients on each replica (3) sum the gradients from each replica (4) add the loss from each replica for reporting purpose. So effectively, we (1) \"Average gradients\" (because scaling the loss has the same effect as scaling the gradients) and (2) \"Average the loss across replicas for reporting\". \r\n\r\nRE: gradient clipping, I don't understand why it should only happen after the reduction across replicas. You mention \"gradient clipping should be done after reduction along the batch size dimension\". But gradients do not have a batch dimension, so there is no reduction along batch dimension. The reduction is only across replicas. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Thanks for your reply. \r\nFor gradient reduction, I did not notice the loss scale in `compute_gradients` function. I have a litter suggestion. You can move the loss scale from `compute_gradients` to `tf.gradients`. Because some developers still use `tf.gradients` instead of that one.\r\nFor gradient clipping, my statement is somewhat ambiguous. The key point is reduction after clipping is not equivalent with clipping after reduction. For example, when using global clipping, the global norm is not the same in these two cases. Another, reduction after global clipping is not conducive to the overlap of computation and communication. ", "For 2.0, as I mentioned, we've removed the scaling from compute_gradients. Now the scaling is left up to the user if using custom training loops. If you're using estimator in TF 2.0 also, you need to do the scaling in your model_fn yourself. \r\nAs for gradient clipping, I think there is no one right answer. If you prefer to do the clipping after reduction, you can override the optimizer's `apply_gradients` call perhaps? ", "Closing this issue since there is no bug here, and the questions have been answered."]}, {"number": 25805, "title": "About model size in ckpt format or numpy format", "body": "Hello, I got a strange question when I tried to convert model from ckpt format to numpy format.\r\nI have two files for the source model in ckpt format\r\nmodel.ckpt.index - 53.7KB\r\nmodel.ckpt.data-00000-of-0001 - 330MB\r\n\r\nBut when I tried to use get_variable_to_shape_map() to get all variables from the src model and save them into a file with numpy format, I got a npy file with 494MB !!!\r\n\r\nI wonder if I used the wrong way to convert ckpt to npy? Or if the ckpt model has been saved in some code formats so it looks small. \r\n\r\nThank you !", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 25804, "title": "tensorflow/stream_executor/cuda/cuda_dnn.cc:482] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR", "body": "<em>I have installed TensorFlow from scratch and I ht following problem while running CuDNN</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Uubntu 18.10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12\r\n- Python version: 3.67 & 2.7.15\r\n- Installed using virtualenv? pip? conda?: pip \r\n- Bazel version (if compiling from source): 0.22.0\r\n- GCC/Compiler version (if compiling from source): 6.5.0\r\n- CUDA/cuDNN version: 7.4.1\r\n- GPU model and memory: Nvidia RTX 2080 / 7949MiB\r\n\r\n**Logs**\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\n2019-02-17 05:00:46.661914: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened CUDA library libcublas.so.10.0\r\n2019-02-17 05:00:46.811147: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened CUDA library libcudnn.so.7\r\n2019-02-17 05:00:47.318462: E tensorflow/stream_executor/cuda/cuda_dnn.cc:482] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-02-17 05:00:47.324283: E tensorflow/stream_executor/cuda/cuda_dnn.cc:482] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"/home/suhahil/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    return fn(*args)\r\n  File \"/home/suhahil/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1320, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/suhahil/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1408, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv/Conv2D}}]]\r\n\t [[Relu/_47]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n**CuDNN Test**\r\nsuhahil@ml:/usr/src/cudnn_samples_v7/mnistCUDNN$ ./mnistCUDNN \r\ncudnnGetVersion() : 7401 , CUDNN_VERSION from cudnn.h : 7401 (7.4.1)\r\nHost compiler version : GCC 6.5.0\r\nThere are 1 CUDA capable devices on your machine :\r\ndevice 0 : sms 46  Capabilities 7.5, SmClock 1800.0 Mhz, MemSize (Mb) 7949, MemClock 7000.0 Mhz, Ecc=0, boardGroupID=0\r\nUsing device 0\r\n\r\nTesting single precision\r\nLoading image data/one_28x28.pgm\r\nPerforming forward propagation ...\r\nTesting cudnnGetConvolutionForwardAlgorithm ...\r\nFastest algorithm is Algo 0\r\nTesting cudnnFindConvolutionForwardAlgorithm ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.012224 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.041856 time requiring 57600 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.045056 time requiring 3464 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.059808 time requiring 2057744 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.071168 time requiring 207360 memory\r\nResulting weights from Softmax:\r\n0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000 \r\nLoading image data/three_28x28.pgm\r\nPerforming forward propagation ...\r\nResulting weights from Softmax:\r\n0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000 \r\nLoading image data/five_28x28.pgm\r\nPerforming forward propagation ...\r\nResulting weights from Softmax:\r\n0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006 \r\n\r\nResult of classification: 1 3 5\r\n\r\nTest passed!\r\n\r\n\r\n", "comments": ["any updates? @jvishnuvardhan ", "@maat16 I think there is an issue with CUDA and cuDNN. There might issues with CUDA path. Please check tested build configurations [here](https://www.tensorflow.org/install/source#linux). \r\n\r\nVersion |                     Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow_gpu-1.12.0 | 2.7, 3.3-3.6 | GCC 4.8 | Bazel 0.15.0 |        7        | 9\r\n\r\n\r\nI think it is better to start with clean state. Uninstall python and tensorflow and downgrade Bazel to 0.15.0. Could you follow the installation instructions [here](https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions) to install TF_GPU_1.12? It worked for me as I have Uubntu 18.10. Please let me know how it progresses. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "The problem occurs due to memory segmentation issue in GPU. We can solve this issue by Allowing GPU memory growth in Tensorflow session or fixed memory allocation.\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\nsess = tf.Session(config=config)\r\nOR,\r\nTo only allocate 70% of the total memory of each GPU by:\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.7", "@maat16 Thanks for posting solution here.!"]}, {"number": 25803, "title": "tf.nn.conv2d_transpose error if output-shape is tuple #25609", "body": "pr for #25609,\r\none line to throw error if output-shape is a tuple instead of list in python client nn_ops.py.", "comments": ["Why erroring for tuple instead of converting it to list?", "thought it would be an unexpected behavior if I was expecting a tuple output. \r\nhttps://github.com/tensorflow/tensorflow/issues/25609 proposed error, should I change it to convert to list?", "Yes\n\nOn Tue, Feb 19, 2019 at 11:37 AM ryan jiang <notifications@github.com>\nwrote:\n\n> thought it would be an unexpected behavior if I was expecting a tuple\n> output.\n> #25609 <https://github.com/tensorflow/tensorflow/issues/25609> proposed\n> error, should I change it to convert to list?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/25803#issuecomment-465277055>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxRs5tDBsrQZXVgbtN3ap9mohFdY-ks5vPFJjgaJpZM4a_Wi5>\n> .\n>\n\n\n-- \n - Alex\n", "changed it"]}, {"number": 25802, "title": "[TF 2.0 API Docs] tf.math.acos", "body": "**System information**\r\n- TensorFlow version: 2.0\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/acos\r\n\r\n**Describe the documentation issue**\r\n\r\nDocumentation for `tf.math.acos` is created from a generated file, `python/ops/gen_math_ops.py`. It would be excellent to have a link to the files that are used to generate `gen_math_ops.py` - so a user could make modifications quickly, without having to search through `tensorflow/tensorflow`.\r\n\r\n`tf.math.acos` could use a clear description, usage examples, and example visuals. A great template to model this on could be [`numpy.arccos`](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.arccos.html).\r\n\r\nUsers also experience obfuscated errors from unexpected arguments (ex: strings, Booleans, and even just `int`s). Some examples are shared below. The file used to generate this error is located [here](https://github.com/tensorflow/tensorflow/blob/e1d20b3b4f25047679cf34107f8d87329cc9070f/tensorflow/core/common_runtime/eager/execute.cc#L201). All `tf.math.*` operations and the operations they influence (e.g., `tf.linspace`) would experience these same obfuscated XLA errors.\r\n\r\n```\r\nInvalidArgumentError: Invalid cast from floating point type to S32 in ConstantR0WithType.\r\n\t [[{{node Acos}}]] [Op:Acos]\r\n```\r\n\r\n```\r\nInternalError: Could not find valid device for node.\r\nNode: {{node Acos}}\r\nAll kernels registered for op Acos :\r\n  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_COMPLEX64, DT_INT64, DT_COMPLEX128, DT_HALF]\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_COMPLEX64, DT_INT64, DT_COMPLEX128, DT_HALF]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n [Op:Acos]\r\n```\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nI shall certainly try! :) \r\n", "comments": ["Hi @dynamicwebpaige. I searched through the tensorflow repo, and it looks like one needs to edit the .pbtxt files inside `tensorflow/tensorflow/core/api_def/base_api` to modify the documentation. Can you confirm this?", "@Sudeepam97 The `gen_math_ops.py` file is generated from [`tensorflow.bzl`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl). The C++ code located [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/math_ops.cc) defines documentation for each math op -- an example of how to modify the description via `.Doc` can be found [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/math_ops.cc#L531). :slightly_smiling_face: \r\n\r\ncc: @MarkDaoust @lamberta to keep me honest, though! Do we have a recommended process for updating those docs?", "@Sudeepam97 Just confirmed that you are correct! Apologies for the confusion - the `.pbtxt`files in [`/base_api`](https://github.com/tensorflow/tensorflow/tree/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/core/api_def/base_api) still create the documentation for several of the TF 2.0 math endpoints.\r\n\r\nLet me take a look at your PR; we might need to tweak the syntax a little bit (format is a special flavor of markdown). Check out [this](https://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/core/api_def/base_api/api_def_Pow.pbtxt) endpoint for a usage example.", "@dynamicwebpaige, that's great! Please take your time to review the PR, I'll be happy to work on any changes that are necessary. :smile: \r\nI seem to understand how to modify the API docs now. I can work on other similar issues as well.", "@dynamicwebpaige \r\nCreated 5 PRs covering some of the math functions. Working on other functions.", "I would love contributing to this. Please tell me how to proceed as I am newcomer.\r\n", "@SSaishruthi are there any ops left to do", "Just a note for people working on this: if you add python doctests, please make sure to do so in Python files, not in base_api/api_def files as those are the APIs for other languages too.", "Looks like the changes made to `tf.math.acos` and `tf.math.add` were not merged with the previous PRs. I have opened a new PR to address this. Since this is my first PR I'm limiting the number of changes I make to only the `.pbtxt` files. Once I get this sorted out I will begin to modify `.Docs` in `math_ops.cc` to reflect the changes.", "is anyone working on this issue? @dynamicwebpaige ", "#42619 solved this\r\n\r\nThe [nightly version](https://www.tensorflow.org/api_docs/python/tf/math/acos?version=nightly) has updated docstring."]}, {"number": 25801, "title": "TensorBoard logging batch level metrics with tf.keras", "body": "This is something that has been added quite recently in the Keras library, see discussion [there](https://github.com/keras-team/keras/issues/6692):\r\n\"It'd be useful if there was some batch-level logging in TensorBoard when using the TensorBoard callback (as defined in keras/callbacks.py). I think this'd be generally useful when trying to keep track of stats between epochs.\"\r\n\r\nCurrently using the `TensorBoard` callback like so will only log metrics at the end of each epoch, which can quickly become a problem for big datasets:\r\n```\r\ntensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\r\nmodel.fit(x=training_set, epochs=epochs, validation_data=validation_set, callbacks=[tensorboard])\r\n```\r\n\r\nThis is already merged in the Keras library, see [here](https://github.com/keras-team/keras/pull/11152).\r\n", "comments": ["@Threynaud Please fill the system information as requested in template [here](https://github.com/tensorflow/tensorflow/issues/new?template=30-feature-request.md). Thanks!", "@Threynaud From the source code it looks like the feature has already been added in tensorflow.keras:\r\n\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L1131-L1136\r\n", "Indeed, I guess I only had a look at the 1.12 documentation! Thanks for pointing this out @yongtang !"]}, {"number": 25800, "title": "Cannot install TensorFlowLiteSwift with CocoaPods", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nI'm excited to see that TensorFlow Lite for Swift is available!\r\n[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/swift](url)\r\n\r\nI tried installing it with CocoaPods using the instructions at the link above, but I get the following error. Is the CocoaPod available yet? Thanks!\r\n`[!] Unable to find a specification for TensorFlowLiteSwift`\r\n\r\n", "comments": ["Hi eospi, thank you for trying out the TensorFlow Lite Swift library. Unfortunately, the CocoaPod is not yet available. Will remove the instructions from the README. Sorry for the confusion.", "Thanks, @temrich. I will close this issue then. ", "Hi, it seems these instructions have been added back as it says you should put this line in your podfile:\r\npod 'TensorFlowLiteSwift'\r\n\r\nBut seems that it's still not available.  The instructions came back in this commit [76e879d1c18c74ee5cbc5a1162ff254a0cfb221b](https://github.com/tensorflow/tensorflow/commit/76e879d1c18c74ee5cbc5a1162ff254a0cfb221b)\r\n\r\nMaybe I'm just jumping the gun and this will show up in CocoaPods after a nightly build or something?", "Hey Jesse,\r\nAt the moment the CocoaPods have not been pushed publicly. Hoping to do that soon.\r\n\r\nIn the meantime (if you want), you can play around with the pods [locally](https://guides.cocoapods.org/making/making-a-cocoapod.html#testing).\r\n\r\nYou will need to configure the TensorFlowLiteC podspec which requires building the TensorFlowLiteC.framework. You can do that using Bazel by following these [steps](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/c/TensorFlowLiteC.bazel#L1).\r\nThen you will need to push the TensorFlowLiteC.podspec to a local CPDC repo and point your Podfile to the local cpdc `source` and local TensorFlowLiteSwift or TensorFlowLiteObjC pods, which will be located in the root tensorflow directory after you follow the [Getting Started steps](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/swift/README.md#getting-started).\r\n\r\nIf you decide to test locally, please let me know if you run into any issues. Again, hoping to push these pods to public CPDC over the next few days.\r\n\r\n\r\n\r\n\r\n", "Hi @temrich I followed your steps (thank you) with the following results:\r\n\r\n- Building TensorFlowLiteC.framework worked fine\r\n- The first two steps under the \"Bazel\" section seem like they're for after you get the TFLSwift library built, so I skipped them.  The next step `bazel build tensorflow/lite/experimental/swift:TensorFlowLite` went fine, but I wonder if it's not supposed to have a switch to build for multiple CPU archs like the C framework had?  In any case, the `bazel test ...` step failed with the following output (snipped to relevant section I hope)\r\n\r\n>   File \"/private/var/tmp/_bazel_jpangburn/820d999d6733d6499781c2d354fc5533/execroot/org_tensorflow/bazel-out/host/bin/external/build_bazel_rules_apple/tools/codesigningtool/codesigningtool.runfiles/build_bazel_rules_apple/tools/codesigningtool/codesigningtool.py\", line 107, in _filter_codesign_output\r\n    for line in codesign_output.split(\"\\n\"):\r\nTypeError: a bytes-like object is required, not 'str'\r\n\r\nI'm new to bazel so maybe there's some obvious signing step I should have implicitly done, like configuring a wildcard provisioning profile or something.  So I hoped the `bazel test ...` step was unnecessary and proceeded.\r\n\r\n- Next I used the generate_xcodeproj.sh script to generate that Xcode project which went fine.  Opened that project in Xcode 10.1 and it was pointed at a scheme for a framework named \"tensorflow-lite-experimental-swift-TensorFlowLite\" with the device set to a connected iPad.  Tried to run the build but it failed:\r\n\r\n>  ERROR: /Users/jpangburn/Documents/tensorflowlite/tensorflow/tensorflow/lite/experimental/swift/BUILD:17:1: Compiling Swift module TensorFlowLite failed (Exit 1): bazel_xcode_wrapper failed: error executing command \r\n  (cd /private/var/tmp/_bazel_jpangburn/820d999d6733d6499781c2d354fc5533/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM=iPhoneOS \\\r\n    APPLE_SDK_VERSION_OVERRIDE=12.1 \\\r\n    XCODE_VERSION_OVERRIDE=10.1.0 \\\r\n  bazel-out/host/bin/external/build_bazel_rules_swift/tools/wrappers/bazel_xcode_wrapper bazel-out/host/bin/external/build_bazel_rules_swift/tools/wrappers/swift_wrapper /usr/bin/xcrun swiftc '-Xwrapped-swift=-ephemeral-module-cache' @bazel-out/ios_arm64-dbg/bin/tensorflow/lite/experimental/swift/TensorFlowLite.swiftmodule-0.params @bazel-out/ios_arm64-dbg/bin/tensorflow/lite/experimental/swift/TensorFlowLite.swiftmodule-1.params)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n/Users/jpangburn/Documents/tensorflowlite/tensorflow/tensorflow/lite/experimental/swift/Sources/Interpreter.swift:59:69: error: value of optional type 'CVaListPointer?' must be unwrapped to a value of type 'CVaListPointer'\r\n                  let message = String(cFormat: cFormat, arguments: arguments)\r\n\r\nSo I edited the Interpreter.swift file and commented out lines 54-67 because the problem was occurring inside a section about error logging (nice to have of course, but I assumed not critical) and I am pretty new to Swift so I didn't know the right way to fix this.  The build completed, successfully it appears as it says it produced this library file:\r\n\r\n> Copying libTensorFlowLite.a to /Users/jpangburn/Library/Developer/Xcode/DerivedData/TensorFlowLite-adluxtrdxpqerubkxpcnjtavgswx/Build/Products/Debug-iphoneos/libtensorflow-lite-experimental-swift-TensorFlowLite.a\r\n\r\n- In the root of my tensorflow directory, I now see symlinks to TensorFlowLiteObjC.podspec and TensorFlowLiteSwift.podspec.  I'm not sure what a CPDC repo is (Googled, no luck) but I'm guessing it's something like a CocoaPods private repo.  I guess the point is for the Podfile to find the Swift podspec and for the Swift podspec to then find the C podspec, so I added the following to a Podfile I made for a test Xcode project:\r\n\r\n>  pod 'TensorFlowLiteC', :path => '/Users/jpangburn/Documents/tensorflowlite/tensorflow/tensorflow/lite/experimental/c/'\r\n>  pod 'TensorFlowLiteSwift', :path => '/Users/jpangburn/Documents/tensorflowlite/tensorflow/'\r\n\r\nThen I did a `pod install --repo-update` and it generated an Xcode workspace file.\r\n\r\n- Opening that workspace file after restarting Xcode, the build fails and says:\r\n\r\n>  no such module 'TensorFlowLiteC'\r\n\r\nIt appears the Swift code can't import that module.  So my workaround for not knowing what CPDC is and instead doing the above thing in the Podfile didn't really work- just allowed it to make that workspace file but not for it to really work.  Being new to CocoaPods I'm guessing here but figured the vendored_frameworks line in the TensorFlowLiteC.podspec file needs to actually point at the framework.  So copied TensorFlowLiteC.podspec to the root tensorflow directory and copied the framework file there too and changed that line to:\r\n\r\n>    s.vendored_frameworks = 'TensorFlowLiteC.framework'\r\n\r\nChanging the Podfile to point the TensorFlowLiteC path also to the root tensorflow directory, this time when I generated the workspace the TensorFlowLiteC pod actually showed the framework inside it.  But when I tried to run the project I got:\r\n\r\n>  <module-includes>:1:9: note: in file included from <module-includes>:1:\r\n#import \"Headers/TensorFlowLiteC.h\"\r\n        ^\r\n/Users/jpangburn/Documents/tensorflowlite/tensorflow/TensorFlowLiteC.framework/Headers/TensorFlowLiteC.h:1:9: note: in file included from /Users/jpangburn/Documents/tensorflowlite/tensorflow/TensorFlowLiteC.framework/Headers/TensorFlowLiteC.h:1:\r\n#import \"c_api.h\"\r\n        ^\r\n/Users/jpangburn/Documents/tensorflowlite/tensorflow/TensorFlowLiteC.framework/Headers/c_api.h:24:10: error: 'tensorflow/lite/context.h' file not found\r\n#include \"tensorflow/lite/context.h\"\r\n         ^\r\n/Users/jpangburn/Documents/tensorflowlite/tensorflow/tensorflow/lite/experimental/swift/Sources/Interpreter.swift:16:8: error: could not build Objective-C module 'TensorFlowLiteC'\r\nimport TensorFlowLiteC\r\n       ^\r\n\r\nSo I guess that framework that got built doesn't include the necessary .h files?  Tried to work around this by adding a `-I` flag in the xcconfig file for the TensorFlowLiteSwift pod, but then the error changes to:\r\n\r\n> /Users/jpangburn/Documents/tensorflowlite/tensorflow/TensorFlowLiteC.framework/Headers/TensorFlowLiteC.h:1:9: note: in file included from /Users/jpangburn/Documents/tensorflowlite/tensorflow/TensorFlowLiteC.framework/Headers/TensorFlowLiteC.h:1:\r\n#import \"c_api.h\"\r\n        ^\r\n/Users/jpangburn/Documents/tensorflowlite/tensorflow/TensorFlowLiteC.framework/Headers/c_api.h:24:10: error: include of non-modular header inside framework module 'TensorFlowLiteC.c_api': '/Users/jpangburn/Documents/tensorflowlite/tensorflow/tensorflow/lite/context.h'\r\n\r\nSeems to be that it doesn't like having stuff in the framework include headers that are outside the framework.  So I'm guessing the framework isn't built correctly and it needs to include that context.h header and whatever other headers are chained from there.\r\n\r\nStopping for right now on this, hope the above comments are useful.  At least the early problems like the \"bazel test\" problem and the \"CVaListPointer\" problem.  Thanks again for your help!\r\n", "You will only need Bazel to generate the TensorFlowLiteC.framework, which will produce a `zip` file. Once you have the zip file, you can unzip it in the tensorflow root directory which is where the TensorFlowLiteC.podspec will look once it has been pushed to your local CocoaPods repo.\r\n`unzip bazel-bin/tensorflow/lite/experimental/c/TensorFlowLiteC_framework.zip -d /Users/path/to/tensorflow/Frameworks`\r\n\r\nThis [line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/c/TensorFlowLiteC.podspec#L9) will need to be updated to point to your local TF git repo that contains the TensorFlowLiteC.podspec and the TensorFlowLiteC.framework. Update to the following:\r\n`s.source = { :git => '/Users/path/to/local/tensorflow/.git' }\r\n\r\nRegarding local CPDC, you are correct, was referring to a local repo, for example:\r\n`cd ~/.cocoapods/repos`\r\n`pod repo add <local_repo_name> /Users/path/to/local/tensorflow`\r\n\r\nAt this point, you should be able to push the TensorFlowLiteC.podspec to your local CocoaPods repo, for example:\r\n`pod repo push <local_repo_name> Users/path/to/local/tensorflow/tensorflow/lite/experimental/c/TensorFlowLiteC.podspec`\r\n\r\nIn your Podfile, you will need the following:\r\n`source '/Users/<username>/.cocoapods/repos/<local_repo_name>'\r\n\r\nPoint your Podfile to the TensorFlowLiteSwift or TensorFlowLiteObjC podspecs:\r\npod 'TensorFlowLiteSwift', :path => '/Users/path/to/local/tensorflow'\r\nor\r\npod 'TensorFlowLiteObjC', :path => '/Users/path/to/local/tensorflow'\r\n\r\nHopefully these steps will help, but please let me know if you still are running into build issues. Really appreciate you trying this out!\r\n", "OK, here are the steps I took and results.\r\n\r\n- Moved TensorFlowLiteC.framework into a new Frameworks directory in the root of my tensorflow (same level as the .git directory).  Reverted the TensorFlowLiteC.podspec's s.vendored_frameworks to 'Frameworks/TensorFlowLiteC.framework', and updated the s.source as directed above with my own path.\r\n\r\n- Did `cd ~/.cocoapods/repos` and `pod repo add localtfrepo /Users/jpangburn/Documents/tensorflowlite/tensorflow`.  Then `pod repo push localtfrepo /Users/jpangburn/Documents/tensorflowlite/tensorflow/TensorFlowLiteC.podspec` failed saying:\r\n>  ERROR | [iOS] file patterns: The `vendored_frameworks` pattern did not match any file.\r\n\r\nDouble checked spelling then tried `pod lib lint TensorFlowLiteC.podspec` and it doesn't have the error, just a warning that \"Git sources should specify a tag\".\r\n\r\n- Tried committing the Frameworks/TensorFlowLiteC.framework's child files to my local tensorflow git repository then running the `pod repo push ...` command again.  It copied those framework files to the local repo and created the podspec file there too, then gave an error about pushing to \"origin master\".  I assume this error is fine since all I care about is having the files in the local repo.\r\n\r\n- Updated my Podfile to set source at the top level (same level as `platform`) to \"source '/Users/jpangburn/.cocoapods/repos/localtfrepo'\" which is the name of my local repo.  Removed the \"pod 'TensorFlowLiteC' ...\" line since it should be found in the local repo now.  Left the other line as:\r\n>  pod 'TensorFlowLiteSwift', :path => '/Users/jpangburn/Documents/tensorflowlite/tensorflow/'\r\n\r\n- Executed `pod install --repo-update` (cleaned out old files first) and it was successful.  Strangely, it cloned my local repo while it did this:\r\n>  $ pod install --repo-update\r\nUpdating local specs repositories\r\nCloning spec repo `/users/jpangburn/.cocoapods/repos/localtfrepo-1` from `/Users/jpangburn/.cocoapods/repos/localtfrepo`\r\n\r\n- Tried the build again, but unfortunately it's the same result:\r\n> <module-includes>:1:9: note: in file included from <module-includes>:1:\r\n#import \"Headers/TensorFlowLiteC.h\"\r\n        ^\r\n/Users/jpangburn/Documents/Xcode_projects/TestWorkspace/TFLiteSwiftCocoaPodTestApp/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/Headers/TensorFlowLiteC.h:1:9: note: in file included from /Users/jpangburn/Documents/Xcode_projects/TestWorkspace/TFLiteSwiftCocoaPodTestApp/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/Headers/TensorFlowLiteC.h:1:\r\n#import \"c_api.h\"\r\n        ^\r\n/Users/jpangburn/Documents/Xcode_projects/TestWorkspace/TFLiteSwiftCocoaPodTestApp/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/Headers/c_api.h:24:10: error: 'tensorflow/lite/context.h' file not found\r\n#include \"tensorflow/lite/context.h\"\r\n         ^\r\n/Users/jpangburn/Documents/tensorflowlite/tensorflow/tensorflow/lite/experimental/swift/Sources/Interpreter.swift:16:8: error: could not build Objective-C module 'TensorFlowLiteC'\r\nimport TensorFlowLiteC\r\n       ^\r\n\r\nThe only difference now from before is that under the \"Development Pods\" group in the Xcode Pods project- the TensorFlowLiteC pod has moved out of there and into the \"Pods\" group.  The \"TensorFlowLiteSwift\" pod remains in the \"Development Pods\" group.  Ultimately, the problem seems the same because the TensorFlowLiteSwift pod's Interpreter.swift imports the TensorFlowLiteC module which causes the compiler to import the TensorFlowLiteC.h file in the TensorFlowLiteC.framework, which imports c_api.h which imports the \"tensorflow/lite/context.h\" which is not visible to the compiler.  I was hoping this line in the Podfile that you mentioned was going to cause that to be set properly \"source '/Users/jpangburn/.cocoapods/repos/localtfrepo'\" but it didn't seem to matter.  I also tried moving that line under the target in the Podfile but no change.  I assume if I change the compiler flags to look at my tensorflow directory for includes that I'll just get the \"non-modular header\" import problem again.  Last time, I tried the setting to allow non-modular headers in both the Pods and my app build settings, but it made no difference.\r\n\r\nI'm happy to try some more, or wait for the pod so as not to take up more of your time.  For me, this project will save me having to make ObjC bridge files and map data to unsafe pointers to feed data to my model.  So it's worth playing with :-)  Thanks again for your time!", "I was able to repro the issue you are running into for a Swift project. Doesn't appear to be an issue with an Objective-C project. The TensorFlowLiteC framework needs to be updated a bit to make it modular. Right now, the framework depends on `tensorflow/lite/context.h` and `tensorflow/lite/c/c_api_internal.h` that are not located in the `tensorflow/lite/experimental/c` directory. We can configure this when we generate the TensorFlowLiteC.framework for the public TensorFlowLiteC CocoaPod, but it's a bit of a pain when developing locally. Working on a fix for that.\r\n\r\nIn the meantime, you can do the following to get this working locally:\r\n\r\n1. Update the `ios_static_framework` target, then regenerate the framework:\r\nios_static_framework(\r\n    name = \"TensorFlowLiteC_framework\",\r\n    hdrs = [\r\n        \"c_api.h\",\r\n        \"//tensorflow/lite:context.h\",\r\n        \"//tensorflow/lite/c:c_api_internal.h\",\r\n    ],\r\n    bundle_name = \"TensorFlowLiteC\",\r\n    minimum_os_version = \"9.0\",\r\n    version = \":TensorFlowLiteC_version\",\r\n    deps = [\":c_api\"],\r\n)\r\n\r\n2. When you get to the stage where you are ready to build the Swift project, you will still hit compiler errors with not being able to find the `context.h` and `c_api_internal.h` headers. You just need to strip the the `tensorflow/lite` and `tensorflow/lite/c` paths from those includes and it should build. Reason is that the TensorFlowLiteC.framework adds its public headers to the `TensorFlowLiteC.framework/Headers` directory and doesn't maintain the TF directory structure.\r\n\r\nPlease let me know if you still run into any issues.", "That worked on the simulator, thanks a lot!  After it built successfully I was able to init an Interpreter object using an absolute path to a tflite model file, and called `.allocateTensors()` and `.invoke()` on it with no errors.  So it was able to process my model (with whatever default values were inside the tensors after the allocation).  For me, this is enough for now as I have plenty to do learning this API and the rest of the project to use it from.\r\n\r\nThat said, you might want to take a look at the following items as you make the public version:\r\n- Doesn't compile for real device.  Commenting Interpreter.swift lines 54-67 resolved that.\r\n- Doesn't link for real device.  I got the following build error:\r\n>  ld: '/Users/jpangburn/Documents/tensorflowlite/tensorflow/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC(c_api.o)' does not contain bitcode. You must rebuild it with bitcode enabled (Xcode setting ENABLE_BITCODE), obtain an updated library from the vendor, or disable bitcode for this target. for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n- On the simulator, I can init the Interpreter with a model on an absolute path, but having trouble with a relative path.  Probably my mistake, but I added my model file to the \"build phases\" \"copy bundle resources\" and I see the model.tflite file at the top of the generated .app.  I've tried \"model.tflite\", \"./model.tflite\", and \"TFLiteSwiftCocoaPodTestApp/model.tflite\" and it always says \"Could not open ...\".\r\n\r\nAgain, thanks for your help, very excited to play with this!  Don't worry about these last items on my behalf unless you want to :-) If you do, I'll gladly test it.  But I imagine for me and anyone else following along at this point, we'll have our hands full on the simulator- unless someone is using a camera to gather data I guess, which I'm not.\r\n", "That's great to hear! Regarding not compiling/linking on a real device, we will definitely look into that before pushing the pod public. Appreciate the feedback.\r\n\r\nFor loading the model via a relative path, can you provide the code you are using?", "Also, for \"Doesn't compile for real device. Commenting Interpreter.swift lines 54-67 resolved that.\", is that only when Bitcode is enabled for your app? If you disable Bitcode (temporarily), does the app build successfully on a real device?\r\n\r\nIf that's a separate issue, can you provide the compiler error that you see prior to commenting out lines 54-67?\r\n\r\nThank you!", "Yes, disabling Bitcode allowed the app to build/run on a real device.\r\n\r\nThe compiler error prior to commenting out 54-67 is that CVaListPointer error:\r\n> /Users/jpangburn/Documents/tensorflowlite/tensorflow/tensorflow/lite/experimental/swift/Sources/Interpreter.swift:59:69: error: value of optional type 'CVaListPointer?' must be unwrapped to a value of type 'CVaListPointer'\r\n                  let message = String(cFormat: cFormat, arguments: arguments)\r\n                                                                    ^\r\n/Users/jpangburn/Documents/tensorflowlite/tensorflow/tensorflow/lite/experimental/swift/Sources/Interpreter.swift:59:69: note: coalesce using '??' to provide a default when the optional value contains 'nil'\r\n                  let message = String(cFormat: cFormat, arguments: arguments)\r\n                                                                    ^\r\n                                                                              ?? <#default value#>\r\n/Users/jpangburn/Documents/tensorflowlite/tensorflow/tensorflow/lite/experimental/swift/Sources/Interpreter.swift:59:69: note: force-unwrap using '!' to abort execution if the optional value contains 'nil'\r\n                  let message = String(cFormat: cFormat, arguments: arguments)\r\n                                                                    ^\r\n                                                                             !\r\n\r\nFor loading the model, here's the code I use that works- for now just changed the viewDidLoad in the ViewController for a default Single Page iOS app)\r\n\r\n    override func viewDidLoad() {\r\n        super.viewDidLoad()\r\n        // Do any additional setup after loading the view, typically from a nib.\r\n        do {\r\n            let interpreter : Interpreter = try Interpreter(modelPath: \"/Users/jpangburn/Documents/Xcode_projects/TestWorkspace/TFLiteSwiftCocoaPodTestApp/TFLiteSwiftCocoaPodTestApp/vggish_converted.tflite\")\r\n            try interpreter.allocateTensors()\r\n            try interpreter.invoke()\r\n        } catch {\r\n            print(\"caught exception initing model\")\r\n            return\r\n        }\r\n        print(\"completed viewDidLoad\")\r\n    }\r\n\r\nThe actual model name is \"vggish_converted.tflite\".  Changing it to those relative paths doesn't work, and the debugger won't go down into the C code to see why (.e.g the path is relative from somewhere else).", "For the Bitcode issue, mind regenerating the framework with the following (see [Bazel CROSSTOOL change to support Bitcode](https://github.com/bazelbuild/bazel/commit/f8e38d1a4b4d8cc6b4855c100971706ba77b7f30)):\r\n`bazel build tensorflow/lite/experimental/c:TensorFlowLiteC_framework -c fastbuild --ios_multi_cpus=x86_64,armv7,arm64 --apple_bitcode=embedded`\r\n\r\nFor the compiler error for `CVaListPointer?` type, can you replace the call to the ErrorReporter with the following (appears to be a Swift compiler [bug](https://bugs.swift.org/browse/SR-3429)):\r\n\r\n```\r\nTFL_InterpreterOptionsSetErrorReporter(\r\n  cOptions,\r\n  { (_, format, args) -> Void in\r\n    let optionalArgs: CVaListPointer? = args\r\n    guard let cFormat = format,\r\n          let arguments = optionalArgs,\r\n          let message = String(cFormat: cFormat, arguments: arguments)\r\n    else {\r\n      return\r\n    }\r\n    print(String(describing: InterpreterError.tensorFlowLiteError(message)))\r\n  },\r\n  nil\r\n)\r\n```\r\n\r\nWe will push these updates shortly. For the relative path to model issue, still looking into this.", "The compiler error fix to `CVaListPointer?` worked great!\r\n\r\nThe Bitcode issue was not resolved on my machine.  I took the following steps:\r\n\r\n1. deleted tensorflow/Frameworks/TensorFlowLiteC.framework\r\n2. bazel clean\r\n3. Cut'n'paste your updated bazel command, it completed with no errors\r\n4. copied the new TensorFlowLiteC.framework to tensorflow/Frameworks/TensorFlowLiteC.framework\r\n5.  edited the c_api.h and context.h to fix those include paths\r\n6.  clean and build the project in Xcode\r\n\r\nGot the same linker error but the compiler error was gone (as mentioned above).  Disabled Bitcode on the project to verify the app still starts up and it does.  I see that change is relatively new, so I checked my bazel version and it's `0.23.2` which appears to be released nearly a month after that commit and I checked `bazel help build | grep apple` and I see the `apple_bitcode` flag with `embedded` option so I don't see any issue there.  Not sure what else to try for that?", "Can you try also passing `--copt=-fembed-bitcode`?\r\n\r\n`bazel build tensorflow/lite/experimental/c:TensorFlowLiteC_framework -c fastbuild --ios_multi_cpus=x86_64,armv7,arm64 --apple_bitcode=embedded --copt=-fembed-bitcode`\r\n", "Looks like that worked!  Bitcode is enabled on my project and it compiled and executed on a real device.  I checked to make sure it didn't break the simulator and it works fine there too.\r\n\r\nSo the relative path thing is all you have left to figure out before you publish the pod?  Or maybe you have a laundry list :-)  Either way, pretty exciting stuff.", "That's good to hear that everything is working now! Thanks again for your all your help with testing out the pipeline!\r\n\r\nRegarding loading a mode file, are you trying to load the file from the main bundle or a custom bundle? Did you try something like this:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/swift/TestApps/TensorFlowLiteApp/TensorFlowLiteApp/ViewController.swift#L95", "Yeah, sorry my prior iOS work was with Cordova, I'm just starting out on Swift.  I know nothing of bundles, and figured I could just use a relative path.  I had been playing with this example [SpeechCommands](https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/ios/SpeechCommands) and mistakenly thought it was a simple relative path to the model file.  After your comment I dug into it and it also uses this bundle thing.  Looks straightforward to use.  Sorry for the ignorant mistake!\r\n\r\nBTW, I checked the tensor sizes reported with this Swift API vs what that example was writing into unsafe pointers and they match up perfectly.  So I don't see any obvious issues left.  You're welcome for the help testing, looking forward to making apps with the cocoapods for this- so easy!", "No problem at all! Please feel free to open up a new issue if you run into any new problems.\r\n\r\nHoping to push the new CocoaPods public over the next week or two. ", "The switches on that last bazel command that allow bitcode to work on the framework compilation are horrible for performance from my initial tests- you may want to investigate this a bit before pushing those CocoaPods.  I converted the SpeechCommands sample that I linked above to use this Swift API and tested the latency to run the `interpreter.invoke()` on the model from that sample with this code:\r\n\r\n        dateBefore = Date()\r\n        try interpreter.invoke()\r\n        dateAfter = Date().timeIntervalSince(dateBefore) * 1000.0\r\n\r\nUsing the framework compiled with bitcode support on a real device it took ~1200 ms vs ~72 ms without the bitcode support.  On a simulator it was ~400 ms with bitcode support vs ~11 ms without.  These are all debug builds running from Xcode.", "Are you running in Release or Debug mode when targeting a real iOS device? Also, the command to generate the framework is using `-c fastbuild` rather than `-c opt`, which may affect performance as well. We need to wait for [Bazel 0.24](https://github.com/bazelbuild/bazel/commit/f8e38d1a4b4d8cc6b4855c100971706ba77b7f30) to be able to use `-c opt` and bitcode.\r\n\r\nIf you are seeing this in Release mode, can you please open a new issue with the details you provided above? Thank you!", "I was running in Debug mode for both.  I edited my scheme to use Release mode for running, cleaned and rebuilt, and the performance is about the same.  I'll open a new issue, thanks!", "@temrich is there a rough estimate of when we might see a publicly available `TensorFlowLiteSwift` pod made available?", "Hoping to push them to public CPDC this week. Will post here once they have been pushed. ", "The [`TensorFlowLiteSwift`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/swift) and [`TensorFlowLiteObjC`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/objc) pods are now available!\r\n\r\nPlease note that these libraries are still under \"experimental,\" so the APIs may change. \r\n\r\nKnown issue: installing the ObjC or Swift pod into your project for the first time may take a bit longer than normal as CocoaPods has to clone the **entire** TensorFlow git repo so that it can grab the `source_files` that are defined in each podspec.\r\n\r\nIf you run into any other issues or would like to provide general feedback, please use the [comp:lite](https://github.com/tensorflow/tensorflow/labels/comp%3Alite) label.\r\n\r\nThank you!", "Congrats :-) I tested the publicly available `TensorFlowLiteSwift` pod just now with the SpeechCommands example project that I converted to Swift and it worked great.  Looks like you got that bitcode performance problem resolved too!  Runs at the same speed now whether or not bitcode is enabled.", "@jpangburn thank you for testing and verifying! "]}, {"number": 25799, "title": "Error importing TensorFlow nightly gpu (docker)", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from: Docker\r\n- TensorFlow version: nightly-gpu\r\n- Python version: 2\r\n- CUDA version: Driver Version: 410.79 CUDA Version: 10.0\r\n- GPU model and memory: Nvidia Tesla P100 16280MiB\r\n\r\n(similar issue when installed from pip, not in a container with cuDNN v7 not sure if this would affect the Docker container)\r\n\r\n**Describe the problem**\r\nUsing `tensorflow/tensorflow:nightly-gpu` image with the following code\r\n```python\r\nimport tensorflow as tf\r\n\r\nprint(\"hello world\")\r\n```\r\nyields\r\n```\r\nFloating point exception (core dumped)\r\n```\r\nas output\r\n\r\nUsing `tensorflow/tensorflow:latest-gpu` image works perfectly\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nFresh installation of ubuntu 18.04  \r\ninstalled nvidia drivers and cuda through `apt-get` \r\nverified by building code samples and running the `deviceQuery` executable as described [here](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#running-binaries)\r\n\r\ninstalled docker as described [here](https://docs.docker.com/install/linux/docker-ce/ubuntu/) and nvidia-docker as described [here](https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0))\r\n\r\n\r\nsame issue when using version installed from pip (`pip install --user tf-nightly-gpu`), not in a container\r\ncuDNN v7, installation verified by running [mnistCUDNN](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#verify) example\r\n\r\n\r\n**Any other info / logs**\r\nRunning through gdb (not in a container, using pip version as above) gives the following\r\n```\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x7ffff4088700 (LWP 4806)]\r\n[New Thread 0x7ffff3887700 (LWP 4807)]\r\n[New Thread 0x7fffef086700 (LWP 4808)]\r\n[New Thread 0x7fffec885700 (LWP 4809)]\r\n[New Thread 0x7fffec084700 (LWP 4810)]\r\n[New Thread 0x7fffe9883700 (LWP 4811)]\r\n[New Thread 0x7fffe5082700 (LWP 4812)]\r\n[New Thread 0x7fffe2881700 (LWP 4813)]\r\n[New Thread 0x7fffe0080700 (LWP 4814)]\r\n[New Thread 0x7fffdd87f700 (LWP 4815)]\r\n[New Thread 0x7fffdb07e700 (LWP 4816)]\r\n[New Thread 0x7fffd887d700 (LWP 4817)]\r\n[New Thread 0x7fffd607c700 (LWP 4818)]\r\n[New Thread 0x7fffd387b700 (LWP 4819)]\r\n[New Thread 0x7fffd107a700 (LWP 4820)]\r\n[New Thread 0x7fffce879700 (LWP 4821)]\r\n[New Thread 0x7fffcc078700 (LWP 4822)]\r\n[New Thread 0x7fffcb877700 (LWP 4823)]\r\n[New Thread 0x7fffc7076700 (LWP 4824)]\r\n[New Thread 0x7fffc4875700 (LWP 4825)]\r\n[New Thread 0x7fffc2074700 (LWP 4826)]\r\n[New Thread 0x7fffbf873700 (LWP 4827)]\r\n[New Thread 0x7fffbf072700 (LWP 4828)]\r\n\r\nThread 1 \"python\" received signal SIGFPE, Arithmetic exception.\r\n0x00007fff83ffdeba in _GLOBAL__sub_I_jit_avx512_common_conv_kernel.cpp ()\r\n   from /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n```\r\n", "comments": ["This looks like an issue in the nightly pip packages, not with the docker images specifically (thank you for mentioning that the problem also appears outside of Docker). @jvishnuvardhan, could you find a more fitting assignee, please?", "I am seeing similar SIGFPE when doing import on a system that is running CentOS 6.7 and no GPU. I am using TensorFlow 1.10 from Anaconda 5.3 (python 2.7.15). It also happens with TensorFlow 1.12. \r\n\r\n`Program received signal SIGFPE, Arithmetic exception.\r\n0x00007fffda47f846 in _GLOBAL__sub_I_jit_avx512_common_conv_kernel.cpp () from /home/user1/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so`\r\n\r\nEdit: Found out that this problem only happens with versions 1.9 and later. Version 1.8 and below work fine.", "@bkazi Could you check with later version of TF and let us know whether the bug persists? Thanks!", "Tried it on a fresh Ubuntu 18.04 VM\r\nFollowed the [GPU support guide](https://www.tensorflow.org/install/gpu) to install drivers and libs\r\nDriver Version: 418.40.04    CUDA Version: 10.1\r\nPython 3.6.7\r\nTensorFlow 1.13.1\r\nand it works! \ud83d\udc4d", "@bkazi Thanks for the confirmation. I am closing this issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25799\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25799\">No</a>\n"]}, {"number": 25797, "title": "can not import libcublas.so.9.0 in centos7 systemctl service", "body": "I try to put a service into systemctl service, so I write a systemctl service command. But when I run this systemctl service, it occur ImportError, my error message is below!\r\n![image](https://user-images.githubusercontent.com/30991932/52897039-72e23580-320a-11e9-836e-af54822007e4.png)\r\n\r\nthis is my systemctl service command detail.\r\n![image](https://user-images.githubusercontent.com/30991932/52897052-a8871e80-320a-11e9-8ecf-324ad1cd8297.png)\r\n\r\nbut when I import tensorflow in python command, it correct.\r\n![image](https://user-images.githubusercontent.com/30991932/52897066-cbb1ce00-320a-11e9-9e7b-399f4beffd94.png)\r\n\r\nthis is my environment.\r\ntensorflow=1.12\r\ncuda=9.0\r\ncudnn=7.4\r\n\r\nIs this a bug of tensorflow 1.12", "comments": ["Can anyone help me. Please!", "Have you exported your Cuda path to PATH?\r\nYou should get something like this.\r\n```\r\n$ echo $PATH\r\n/usr/local/cuda-9.0/bin:balabala......\r\n```", "this is my ~/.bashrc detail. but it still not work.\r\n![image](https://user-images.githubusercontent.com/30991932/52897616-a1174380-3211-11e9-8b9b-4b8eaf2b95d0.png)\r\n\r\n", "And this is my path detail.\r\n![image](https://user-images.githubusercontent.com/30991932/52897635-c4da8980-3211-11e9-92e1-41fa02285927.png)\r\n", "And what's the output of `ls /usr/local/cuda-9.0/lib64/`? It should include `libcublas.so.9.0`. Normally, it should not be a bug of tensorflow. It may be caused by the wrong configuration.", "It's include\n\n\nfrom Alimail iPhone\n ------------------Original Mail ------------------\nFrom:BenjaminChou <notifications@github.com>\nDate:2019-02-16 18:52:06\nRecipient:tensorflow/tensorflow <tensorflow@noreply.github.com>\nCC:Cally <mx15025700935@aliyun.com>, Author <author@noreply.github.com>\nSubject:Re: [tensorflow/tensorflow] can not import libcublas.so.9.0 in centos7 systemctl service (#25797)\nAnd what's the output of ls /usr/local/cuda-9.0/lib64/? It should include 'libcublas.so.9.0'.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread. ", "> It's include from Alimail iPhone ------------------Original Mail ------------------ From:BenjaminChou <notifications@github.com> Date:2019-02-16 18:52:06 Recipient:tensorflow/tensorflow <tensorflow@noreply.github.com> CC:Cally <mx15025700935@aliyun.com>, Author <author@noreply.github.com> Subject:Re: [tensorflow/tensorflow] can not import libcublas.so.9.0 in centos7 systemctl service (#25797) And what's the output of ls /usr/local/cuda-9.0/lib64/? It should include 'libcublas.so.9.0'. \u2014 You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub, or mute the thread.\r\n\r\nIt is quite weird. How about try this?\r\n`python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\nIf this won't work, I believe there's some wrong configuration. But I am not sure without extra info. You could check the cuda & cudnn installations provided by Nvidia. And try to build the tensorflow from source instead of using `pip install tensorflow-gpu`. Sorry can't help u.", "Not mind. can i keep the question open util i solve the problem?\n\n\nfrom Alimail iPhone\n ------------------Original Mail ------------------\nFrom:BenjaminChou <notifications@github.com>\nDate:2019-02-16 19:25:04\nRecipient:tensorflow/tensorflow <tensorflow@noreply.github.com>\nCC:Cally <mx15025700935@aliyun.com>, Author <author@noreply.github.com>\nSubject:Re: [tensorflow/tensorflow] can not import libcublas.so.9.0 in centos7 systemctl service (#25797)\n\nIt's include from Alimail iPhone ------------------Original Mail ------------------ From:BenjaminChou notifications@github.com Date:2019-02-16 18:52:06 Recipient:tensorflow/tensorflow tensorflow@noreply.github.com CC:Cally mx15025700935@aliyun.com, Author author@noreply.github.com Subject:Re: [tensorflow/tensorflow] can not import libcublas.so.9.0 in centos7 systemctl service (#25797) And what's the output of ls /usr/local/cuda-9.0/lib64/? It should include 'libcublas.so.9.0'. \u2014 You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub, or mute the thread. \nIt is quite weird. How about try this?\n python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\n If this won't work, I believe there's some wrong configuration. But I am not sure without extra info. You could follow the cuda & cudnn installations provided by Nvidia. And try to build the tensorflow from source. Sorry can't help u.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread. ", "> Not mind. can i keep the question open util i solve the problem? from Alimail iPhone ------------------Original Mail ------------------ From:BenjaminChou <notifications@github.com> Date:2019-02-16 19:25:04 Recipient:tensorflow/tensorflow <tensorflow@noreply.github.com> CC:Cally <mx15025700935@aliyun.com>, Author <author@noreply.github.com> Subject:Re: [tensorflow/tensorflow] can not import libcublas.so.9.0 in centos7 systemctl service (#25797) It's include from Alimail iPhone ------------------Original Mail ------------------ From:BenjaminChou notifications@github.com Date:2019-02-16 18:52:06 Recipient:tensorflow/tensorflow tensorflow@noreply.github.com CC:Cally mx15025700935@aliyun.com, Author author@noreply.github.com Subject:Re: [tensorflow/tensorflow] can not import libcublas.so.9.0 in centos7 systemctl service (#25797) And what's the output of ls /usr/local/cuda-9.0/lib64/? It should include 'libcublas.so.9.0'. \u2014 You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub, or mute the thread. It is quite weird. How about try this? python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\" If this won't work, I believe there's some wrong configuration. But I am not sure without extra info. You could follow the cuda & cudnn installations provided by Nvidia. And try to build the tensorflow from source. Sorry can't help u. \u2014 You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub, or mute the thread.\r\n\r\nWhy not? Wish someone could help u. I recommend you check your installation again and build it from source.", "@policeme DId you try the installation instructions at Tensorflow [website](https://www.tensorflow.org/install/source)? or check the solution provided at this [resource](https://github.com/tensorflow/tensorflow/issues/25557). Please let me know how it progresses. Thanks!", "@jvishnuvardhan  you want to me installing the bazel?", "@policeme It looks like there is a configuration issue. You could uninstall current TF and build TF from source using the resource i provided to you or any other good resource. Please let me know how it progresses. Thanks! ", "@jvishnuvardhan  sorry it's difficult to uninstall current TF, because the current TF is in product environment, sorry.\r\nthis is my steps of install tensorflow-gpu\r\n1. install Anaconda3\r\n   a. yum install -y bzip2\r\n   b.add environment path of anaconda3\r\n   c. install tensorflow-gpu\r\n2. install cuda-9.0\r\n    a. Wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda_9.0.176_384.81_linux-run \r\n    b. install cuda-toolkit, just install cuda-tookit during install cuda-9.0, not install others, like opengl and so on.\r\n3.install cudnn 7.4\r\n    a. download from cudnn website, version is 7.4\r\n    b. copy cudnn file into cuda-9.0\r\n        1) cp include/* /usr/local/cuda-9.0/inlcude/\r\n        2) cp lib64/lib* /usr/local/cuda-9.0/lib64/\r\n4. add environment of cuda-9.0\r\n    a. sudo vim /etc/profile\r\n    b. export PATH=/usr/local/cuda-9.0/bin:$PATH\r\n    c. export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64:$LD_LIBRARY_PATH\r\n     d.source /etc/profile\r\n\r\nfinish install tensorlfow-gpu\r\n\r\nDo I had some error install steps?\r\n   ", "@policeme I wrote the installation [steps](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12/blob/master/README.md) for installing TF1.12_gpu on Windows 10 system. The steps are similar to what you mentioned. From my experience when you install through Anaconda( for TF_gpu), some of the CUDA paths are not referenced correctly. I think it is better to install NVDIA drivers, CUDA, cuDNN, visual studio IDE, and python before installing tennsorflow_gpu. Please check the steps I listed and follow similar procedure and let me know how it progresses. Thanks!", "@jvishnuvardhan I installed the NVIDIA driver before install Anaconda, my GPU model is NVIDIA V100 16G", "@policeme Not sure about Anaconda. If you could follow the steps I listed and some thing is not working, then I could help. There is no support for Anaconda based installation. You could check stackoverflow or closed Anaconda related issues on GitHub. I will close the issue. Thanks!"]}, {"number": 25796, "title": "Add tests for TF_CUDNN_DETERMINISTIC", "body": "Some of the tests for the environment variable `TF_CUDNN_DETERMINISTIC`, which was added in pull requests [24747](https://github.com/tensorflow/tensorflow/pull/24747) and [25269](https://github.com/tensorflow/tensorflow/pull/25269).", "comments": ["I work on XLA; I can't review this change.  @chsigg might be a better person."]}, {"number": 25795, "title": "Can't rename checkpoint directories", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 1.12.0\r\n- Are you willing to contribute it (Yes/No): Probably, but I'm not sure exactly what needs to be changed and/or what change would be considered acceptable\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, if you rename or copy a checkpoint directory, tensorflow will refuse to load checkpoints from it. This is because it looks in the `checkpoint` file in that directory and loads a filepath from the protobuf. If that filepath doesn't match, it refuses to load any checkpoints for some reason.\r\n\r\n**Will this change the current api? How?**\r\nIt will make loading checkpoints work as expected. I am not sure if anyone is relying on the current behavior.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to easily back up and restore their checkpoints.\r\n\r\n**Any Other info.**\r\nI am not the first person to run into this issue. I see it was already reported [here](https://github.com/tensorflow/tensorflow/issues/13186) but that issue was closed with a request to resubmit (which apparently never happened).", "comments": ["Assuming you're using `tf.train.Saver`, this is the `save_relative_paths` argument to `__init__`."]}]