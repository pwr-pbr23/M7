[{"number": 43355, "title": "Patch for TF 2.0.3", "body": "Contains multiple cherry-picks.", "comments": []}, {"number": 43354, "title": "Patch for TF 1.15.4", "body": "Contains multiple cherry-picks.", "comments": []}, {"number": 43353, "title": "Pad() gradient", "body": "@saxenasaurabh @alextp \r\nEager mode working; graph is failing due to Eager Tensors being created inside the gradient functions in order to perform operations such as the dimensions in `Concatenate` and beginning indices for `Slice`.", "comments": ["@amturati  Can you please resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 60 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 43352, "title": "cuda and tensorflow", "body": "hello i install cuda_10.1.243_426.00_win10 and cudnn-10.1-windows10-x64-v7.5.0.56 and tensorflow2.3 and tensorflow-gpu2.3 and when i run matrixMul_vs2019.vcxproj \r\nshow\r\n````\r\n\r\n [Matrix Multiply Using CUDA] - Starting...\r\nGPU Device 0: \"GeForce GTX 1650\" with compute capability 7.5\r\n\r\nMatrixA(320,320), MatrixB(640,320)\r\nComputing result using CUDA Kernel...\r\ndone\r\nPerformance= 31.97 GFlop/s, Time= 4.099 msec, Size= 131072000 Ops, WorkgroupSize= 1024 threads/block\r\nChecking computed result for correctness: Result = PASS\r\n\r\nNOTE: The CUDA Samples are not meant for performancemeasurements. Results may vary when GPU Boost is enabled.\r\n\r\nC:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v10.1\\0_Simple\\matrixMul\\../../bin/win64/Debug/matrixMul.exe (process 15308) exited with code 0.\r\nPress any key to close this window . . .\r\n````\r\n\r\nbut when run a tensorflow app this commend show\r\n```\r\n python3 .\\detection_custom.py     2020-09-19 01:41:17.705558: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-09-19 01:41:17.706529: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-09-19 01:41:38.314508: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-09-19 01:41:38.350208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2020-09-19 01:41:38.351252: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-09-19 01:41:38.352227: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found\r\n2020-09-19 01:41:38.353091: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\r\n2020-09-19 01:41:38.353915: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\r\n2020-09-19 01:41:38.354613: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2020-09-19 01:41:38.355197: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found\r\n2020-09-19 01:41:38.355578: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found\r\n2020-09-19 01:41:38.355633: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-09-19 01:41:38.356356: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-19 01:41:38.368268: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x19c0bdffd00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-19 01:41:38.368484: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-19 01:41:38.369451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-19 01:41:38.369768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]\r\n```\r\nand this is my C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\r\n![image](https://user-images.githubusercontent.com/53191454/93645602-99390a00-fa19-11ea-8ac1-ccc3348323e9.png)\r\n\r\ncan you help me\r\nos: windows 10\r\ni7 9750h\r\nnvidia 1650\r\n", "comments": ["Please check https://www.tensorflow.org/install/gpu#windows_setup", "i do\r\n```\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin;%PATH%\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\extras\\CUPTI\\lib64;%PATH%\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\include;%PATH%\r\nSET PATH=C:\\tools\\cuda\\bin;%PATH%\r\n```\r\nbut dont work", "Please check that you have a python 64 bit and that you have installed: https://support.microsoft.com/it-it/help/2977003/the-latest-supported-visual-c-downloads\r\n", "dont work", "Do you have installed Tensorflow with `pip install tensorflow`?", "YES", "pip3 install tensorflow", "Do you have installed python from https://www.python.org/downloads/windows/?", "Also, are you using an Anaconda env?", "> Do you have installed python from https://www.python.org/downloads/windows/?\r\n\r\nyes", "> Also, are you using an Anaconda env?\r\n\r\nno\r\n", "It seems ok. If you have your %PATH% with CUDA paths It will find CUDA libs. \nTry to reboot e print your %PATH% env varaible to verify CUDA paths before run Tensorflow.", "![image](https://user-images.githubusercontent.com/53191454/93705527-7733aa80-fb33-11ea-9dad-e53ce9d948db.png)\r\ndont work", "@hdihd9162 \r\n\r\nLooking at the logs it seems like tensorlow detected GPU.Please, verify this using the below command.\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n```\r\nAlso, please let us know your python version.Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43351, "title": "tf.contrib.lookup.string_to_index_table_from_file optimization  ", "body": "**System information** \r\n\r\n- OS Platform and Distribution : macOS Catalina 10.15.3\r\n\r\n- TensorFlow installed from : binary\r\n\r\n- TensorFlow version : 1.15.x\r\n- Python version: 3.7.3\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nHi ,\r\n\r\nIf there any way we can make reading of vocab faster esp for api \"tf.contrib.lookup.string_to_index_table_from_file\" ?\r\nFor token size more than few thousand it take close to 500ms , how can we optimize it ?", "comments": ["@17patelumang,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.3 and check if you are still facing the same issue. Thanks!", "@amahendrakar  thank you for reply , for TF 2.x api is there any way we can optimize it ?", "@17patelumang Can you please share a simple standalone code to reproduce any performance related issue? Can you please share as many details as possible about use-case and what you had tried and what is your goal of those optimizations. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43350, "title": "TF2.0 SavedModel with feature processing inference code ", "body": "\r\n\r\n\r\n**System information** \r\n\r\n- OS Platform and Distribution : macOS Catalina 10.15.3\r\n\r\n- TensorFlow installed from : binary\r\n\r\n- TensorFlow version : 2.2.0\r\n- Python version: 3.7.3\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nHi ,\r\n\r\nI am looking for sample code to do inference in TF2.0 . We have savedModel in TF2.0 with feature processing as part of saved model.\r\n\r\nIn TF1.x we used to do inference as follows \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nsaved_model_path = \"<path>\"\"\r\n\r\n\r\nwith tf.Session(graph=tf.Graph()) as sess:\r\n  metagraph = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], saved_model_path)\r\n  prediction = sess.run(['loss/scores:0'], feed_dict={'<some place holder>':<input_list>})\r\n  print(prediction)\r\n```\r\nwhat will be corresponding code in TF2.0  since it doesnot use session concept ?  \r\nNote: I dont need to have  working code , please point me to the sample code somethere\r\n\r\n", "comments": ["Check https://www.tensorflow.org/guide/saved_model", "@bhack we checked  that but there is  no clear code, could you p lease share  sample code where we need to specify 'loss/scores:0' & feed_dict ?", "I think the tutorial is very clear. You can interactively run and modify it yourself at https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/saved_model.ipynb.\r\nIf you want to improve this tutorial please open a documentation type issue (not a bug as this) or a PR directly.\r\n\r\nFor support questions please use our Stackoverflow: https://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nThanks", "@bhack thank you for reply. We read this documentation before but in this documentation there is no sample code for inference from savedModel format which has TF feature processing part of  it . Could you please provide sample code. ", "See this comment and the next at https://github.com/tensorflow/tensorflow/issues/31055#issuecomment-516205819", "@bhack thank you  for reply , appreciate it ! , but where to find the inference code ?", "That example is about using preprocessing with saved model e.g. for TF serving. \r\nWhat is your use case?", "@bhack thank you for reply , we have savedModel with TF feature processing part of it in TF 2.x, however we are un able to find the sample inference code in TF2.x from savedModel which has TF feature processing part of it . In the above , we have TF 1.15.x  inference code , where can we refer for similar inference code in TF 2.x ? ", "Do you have a very very minimal but runnable TF2.x example or colab up to the save command?\r\n\r\nGenerally I think that this is a support question for: https://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nBut if you can share something in TF 2.x we could see if there is something that could be improved in the docs.", "I already created https://stackoverflow.com/questions/63962865/tf2-0-savedmodel-with-feature-processing-inference-code but no reply.  We have savedModel in TF 2.x , dont have the code to save model . We are looking for way to do the inference in TF 2.x ", "Sorry but all the step in https://www.tensorflow.org/guide/saved_model starting from\r\n```\r\nloaded = tf.saved_model.load(mobilenet_save_path)\r\nprint(list(loaded.signatures.keys()))  # [\"serving_default\"]\r\n```\r\nIs not inference with savedModel?", "If you are looking instead about how to embed a preprocessing function inside a SavedModel you can take a look at https://sayak.dev/tf.keras/preprocessing/2020/04/13/embedding-image-preprocessing-functions.html", "thank you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43350\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43350\">No</a>\n"]}, {"number": 43349, "title": "Different gradients in tf2 when eager mode is enabled compared to graph mode", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):** Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary):** Binary\r\n- **TensorFlow version (use command below):**  v2.3.0\r\n- **Python version:**  v3.8.5\r\n- **CUDA/cuDNN version:** CUDA  v11.0/ cuDNN V9.1.85\r\n- **GPU model and memory:** GPU: NVIDIA Quadro P1000 / Intel UHD Graphics 630 - Memory: 2x Samsung M471A4G43MB1-CTD\r\n\r\nYou can collect some of this information using our environment capture:\r\n\r\n- [tf_env.txt](https://github.com/tensorflow/tensorflow/files/5247881/tf_env.txt)\r\n\r\n### Describe the current behaviour\r\n\r\nI'm currently porting several TensorFlow v1.x legacy repositories over to tf2.3 with eager execution enabled. I used the steps in the [documentation](https://www.tensorflow.org/guide/migrate) to do this. Unfortunately, one of the RL Agents which is based on the [Lyapunov Actor-Critic ](http://arxiv.org/abs/2004.14288) architecture of [Han et al. 2019](http://arxiv.org/abs/2004.14288) is not training when eager execution is enabled. I did some debugging, and it looks like there is a problem with computing the gradients of the Squashed Gaussian Actor-network:\r\n\r\n```python\r\nclass SquashedGaussianActor(tf.keras.Model):\r\n    def __init__(\r\n        self, obs_dim, act_dim, hidden_sizes, name, seeds=None, **kwargs,\r\n    ):\r\n        \"\"\"Squashed Gaussian actor network.\r\n\r\n        Args:\r\n            obs_dim (int): The dimension of the observation space.\r\n\r\n            act_dim (int): The dimension of the action space.\r\n\r\n            hidden_sizes (list): Array containing the sizes of the hidden layers.\r\n\r\n            name (str): The keras module name.\r\n\r\n            seeds (list, optional): The random seeds used for the weight initialization\r\n                and the sampling ([weights_seed, sampling_seed]). Defaults to\r\n                [None, None]\r\n        \"\"\"\r\n        super().__init__(name=name, **kwargs)\r\n\r\n        # Get class parameters\r\n        self.s_dim = obs_dim\r\n        self.a_dim = act_dim\r\n        self._seed = seeds[0]\r\n        self._initializer = tf.keras.initializers.GlorotUniform(\r\n            seed=self._seed\r\n        )  # Seed weights initializer\r\n        self._tfp_seed = seeds[1]\r\n\r\n        # Create fully connected layers\r\n        self.net = tf.keras.Sequential(\r\n            [\r\n                tf.keras.layers.InputLayer(\r\n                    dtype=tf.float32, input_shape=(self.s_dim), name=name + \"/input\"\r\n                )\r\n            ]\r\n        )\r\n        for i, hidden_size_i in enumerate(hidden_sizes):\r\n            self.net.add(\r\n                tf.keras.layers.Dense(\r\n                    hidden_size_i,\r\n                    activation=\"relu\",\r\n                    name=name + \"/l{}\".format(i + 1),\r\n                    kernel_initializer=self._initializer,\r\n                )\r\n            )\r\n\r\n        # Create Mu and log sigma output layers\r\n        self.mu = tf.keras.Sequential(\r\n            [\r\n                tf.keras.layers.InputLayer(\r\n                    dtype=tf.float32, input_shape=hidden_sizes[-1]\r\n                ),\r\n                tf.keras.layers.Dense(\r\n                    act_dim,\r\n                    activation=None,\r\n                    name=name + \"/mu\",\r\n                    kernel_initializer=self._initializer,\r\n                ),\r\n            ]\r\n        )\r\n        self.log_sigma = tf.keras.Sequential(\r\n            [\r\n                tf.keras.layers.InputLayer(\r\n                    dtype=tf.float32, input_shape=hidden_sizes[-1]\r\n                ),\r\n                tf.keras.layers.Dense(\r\n                    act_dim,\r\n                    activation=None,\r\n                    name=name + \"/log_sigma\",\r\n                    kernel_initializer=self._initializer,\r\n                ),\r\n            ]\r\n        )\r\n\r\n    @tf.function\r\n    def call(self, inputs):\r\n        \"\"\"Perform forward pass.\"\"\"\r\n\r\n        # Retrieve inputs\r\n        obs = inputs\r\n\r\n        # Perform forward pass through fully connected layers\r\n        net_out = self.net(obs)\r\n\r\n        # Calculate mu and log_sigma\r\n        mu = self.mu(net_out)\r\n        log_sigma = self.log_sigma(net_out)\r\n        log_sigma = tf.clip_by_value(\r\n            log_sigma, LOG_SIGMA_MIN_MAX[0], LOG_SIGMA_MIN_MAX[1]\r\n        )\r\n\r\n        # Perform re-parameterization trick\r\n        sigma = tf.exp(log_sigma)\r\n\r\n        # Create bijectors (Used in the re-parameterization trick)\r\n        squash_bijector = SquashBijector()\r\n        affine_bijector = tfp.bijectors.Shift(mu)(tfp.bijectors.Scale(sigma))\r\n\r\n        # Sample from the normal distribution and calculate the action\r\n        batch_size = tf.shape(input=obs)[0]\r\n        base_distribution = tfp.distributions.MultivariateNormalDiag(\r\n            loc=tf.zeros(self.a_dim), scale_diag=tf.ones(self.a_dim)\r\n        )\r\n        epsilon = base_distribution.sample(batch_size, seed=self._tfp_seed)\r\n        raw_action = affine_bijector.forward(epsilon)\r\n        clipped_a = squash_bijector.forward(raw_action)\r\n\r\n        # Transform distribution back to the original policy distribution\r\n        reparm_trick_bijector = tfp.bijectors.Chain((squash_bijector, affine_bijector))\r\n        distribution = tfp.distributions.TransformedDistribution(\r\n            distribution=base_distribution, bijector=reparm_trick_bijector\r\n        )\r\n        clipped_mu = squash_bijector.forward(mu)\r\n\r\n        # Return network outputs and noise sample\r\n        return clipped_a, clipped_mu, distribution.log_prob(clipped_a), epsilon\r\n```\r\n\r\nAlthough gradients are computed for this network, these are very different in eager mode as compared to when the `tf.compat.v1.disable_eager_execution()` flag is used. This is strange since the loss functions, random seeds, weights/biases and inputs are equal. Furthermore, also the outputs of a forward pass through the network are identical in both Eager and legacy Graph mode. This problem does not seem to exist for accompanying Lyapunov Critic (a modified version of a deep Q network). I first wanted to post it here before posting on StackOverflow as I am unsure whether this a translation issue or a bug. I tried searching for possible causes, but I did not find a possible solution to my problem. \r\n\r\n### Describe the expected behaviour\r\n\r\nI expected the gradients to be equal in both the script in which Eager mode is enabled and the one in which Eager mode disabled. Instead, although the Actor loss is equal (`-1084.2743`) the gradients are different:\r\n\r\n**Results eager mode:**\r\n\r\n```python\r\ngrad/l1/weights:\r\n[[ 13.408794     2.178104   -11.570398   108.1129       0.      46.277092  ]\r\n [ 30.369755    -0.23067771 -16.505516    87.53128      0.      22.498222]]\r\n\r\ngrad/l1/bias:\r\n[ 47.802944    0.4118477 -26.826544  185.92018     0.         60.14121]\r\n\r\ngrad/l2/weights:\r\n[[-12.058103     0.          -0.33448434  14.862488     0.      -1.8673693 ]\r\n [ -0.30301327   0.           0.          18.134262     0.      0.        ]\r\n [-58.740616     0.          -1.113349   119.78081      0.      -2.120421  ]\r\n [-24.779123     0.          -0.4045168   66.25852      0.      -0.24545981]\r\n [  0.           0.           0.           0.           0.      0.        ]\r\n [ -3.3655543    0.           0.          41.697414     0.      0.]]\r\n\r\ngrad/l2/bias:\r\n[-98.14556     0.          1.5422362 206.91672     0.         -4.5077815]\r\n\r\ngrad/mu/weights:\r\n[[ 4.81568050e+00 -2.37027216e+00]\r\n [ 0.00000000e+00  0.00000000e+00]\r\n [-1.84061453e-01  1.09151885e-01]\r\n [ 3.29189644e+01 -2.93549042e+01]\r\n [ 0.00000000e+00  0.00000000e+00]\r\n [-4.64032125e-03  6.64837938e-03]]\r\n\r\ngrad/mu/bias:\r\n[110.85972 -87.11501]\r\n\r\ngrad/log_sigma/weights:\r\n[[5.8719816e+00 5.4135699e+00]\r\n [0.0000000e+00 0.0000000e+00]\r\n [2.2099029e-01 3.2953596e-01]\r\n [7.5656143e+01 6.9250507e+00]\r\n [0.0000000e+00 0.0000000e+00]\r\n [6.7091000e-04 9.1812750e-03]]\r\n\r\ngrad/log_sigma/bias:\r\n[238.21657   47.906483]\r\n```\r\n\r\n**Results graph mode:**\r\n\r\n```python\r\ngrad/l1/weights:\r\n[[  1.1570635   2.2042375 -11.598429   88.867676    0.         48.9553   ]\r\n [ 11.854488   -0.2453581 -16.115313   49.29876     0.         24.385675 ]]\r\n\r\ngrad/l1/bias:\r\n[ 29.577682     0.16548407 -29.669703   144.31357      0.      64.56108   ]\r\n\r\ngrad/l2/weights:\r\n[[ -9.905338     0.           4.7838783    3.8737621    0.      -3.4614413 ]\r\n [ -0.30002874   0.           0.          18.411116     0.      0.]\r\n [-53.221275     0.           5.242466    79.01278      0.      -3.936808]\r\n [-23.196867     0.           0.53559065  51.048943     0.      -0.45809162]\r\n [  0.           0.           0.           0.           0.      0. ]\r\n [ -4.0506964    0.           0.          42.10633      0.      0.]]\r\n\r\ngrad/l2/bias:\r\n[-98.10234    0.        16.95784  155.67645    0.        -8.705088]\r\n\r\ngrad/mu/weights:\r\n[[ 1.62482891e+01  1.15455017e+01]\r\n [ 0.00000000e+00  0.00000000e+00]\r\n [ 2.01459303e-01  6.31435871e-01]\r\n [ 1.06661575e+02  4.60808792e+01]\r\n [ 0.00000000e+00  0.00000000e+00]\r\n [-3.92461056e-03  9.14150383e-03]]\r\n\r\ngrad/mu/bias:\r\n[372.89713 178.99115]\r\n\r\ngrad/log_sigma/weights:\r\n[[7.4654112e+00 1.1838960e+01]\r\n [0.0000000e+00 0.0000000e+00]\r\n [3.2749507e-01 6.5140498e-01]\r\n [8.7392433e+01 4.8770226e+01]\r\n [0.0000000e+00 0.0000000e+00]\r\n [1.4014873e-03 9.9075940e-03]]\r\n\r\ngrad/log_sigma/bias:\r\n[289.77673 198.32103]\r\n```\r\n\r\n### Code to reproduce the problem\r\n\r\nI placed two small stand-alone example scripts [tf2_val_grad.py](https://github.com/rickstaa/tf2_eager_vs_graph_grad_problem/blob/master/tf2_val_grad.py) and [tf2_val_grad_eager.py](https://github.com/rickstaa/tf2_eager_vs_graph_grad_problem/blob/master/tf2_val_grad_eager.py) in my repository that can be used to reproduce the problem. [The repository](https://github.com/rickstaa/tf2_eager_vs_graph_grad_problem) also contains a small README.md which explains how to run the code examples.\r\n\r\n### Other info / logs\r\n\r\n[tf2_val_grad_eager_terminal_output.txt](https://github.com/tensorflow/tensorflow/files/5247877/tf2_val_grad_eager_terminal_output.txt)\r\n\r\n[tf2_val_grad_terminal_output.txt](https://github.com/tensorflow/tensorflow/files/5247879/tf2_val_grad_terminal_output.txt)\r\n\r\n### Possible related issues\r\n- #27827\r\n- https://github.com/tensorflow/probability/issues/345\r\n\r\n\r\n### Further debug steps\r\n- Trim the call function down to see where the gradients converge.", "comments": ["Closing this issue for now as. This might be a StackOverflow question after all. I just found out that when I change the loss function from:\r\n\r\n```python\r\na_loss = GRAD_SCALE_FACTOR * (\r\n        labda * l_delta + alpha * tf.reduce_mean(input_tensor=log_pis)\r\n    ) \r\n```\r\n\r\nTo:\r\n\r\n```python\r\na_loss = GRAD_SCALE_FACTOR * (alpha * tf.reduce_mean(input_tensor=log_pis))\r\n```\r\n\r\nThe gradients become equal. Meaning there is a difference in how the gradient is computed when the second actor copy is added.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43349\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43349\">No</a>\n", "For future reference: One of the required gradients was None in the tf2 eager version. This was caused by one of the forward passes that was required to compute these gradients not being taped. I used the [tensorboard debugger](https://www.tensorflow.org/tensorboard/debugger_v2) to find this out."]}, {"number": 43348, "title": "r1.15 Eigen download link is dead", "body": "version r1.15\r\n\r\nThe download link for Eigen is dead  (https://bitbucket.org/eigen/eigen/get/49177915a14a.tar.gz)\r\n\r\nThis patch switches the link to use the google storage bucket\r\n\r\n```\r\nFrom 765a2b04268d030d247a45d600686f3bac5e5d38 Mon Sep 17 00:00:00 2001\r\nFrom: Chris Roed <chris.roed@omcare.com>\r\nDate: Tue, 1 Sep 2020 11:36:49 -0500\r\nSubject: [PATCH 3/3] use the google repo for eigen\r\n\r\n---\r\n tensorflow/lite/tools/make/download_dependencies.sh | 2 +-\r\n 1 file changed, 1 insertion(+), 1 deletion(-)\r\n\r\ndiff --git a/tensorflow/lite/tools/make/download_dependencies.sh b/tensorflow/lite/tools/make/download_dependencies.sh\r\nindex ef4a7777e6..678e35d987 100755\r\n--- a/tensorflow/lite/tools/make/download_dependencies.sh\r\n+++ b/tensorflow/lite/tools/make/download_dependencies.sh\r\n@@ -29,7 +29,7 @@ if [ ! -f $BZL_FILE_PATH ]; then\r\n   exit 1;\r\n fi\r\n \r\n-EIGEN_URL=\"$(grep -o 'http.*bitbucket.org/eigen/eigen/get/.*tar\\.gz' \"${BZL_FILE_PATH}\" | grep -v mirror.tensorflow | head -n1)\"\r\n+EIGEN_URL=\"$(grep -o 'http.*bitbucket.org/eigen/eigen/get/.*tar\\.gz' \"${BZL_FILE_PATH}\" | grep mirror.tensorflow | head -n1)\"\r\n GEMMLOWP_URL=\"$(grep -o 'https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/gemmlowp/.*zip' \"${BZL_FILE_PATH}\" | head -n1)\"\r\n GOOGLETEST_URL=\"https://github.com/google/googletest/archive/release-1.8.0.tar.gz\"\r\n ABSL_URL=\"$(grep -o 'https://github.com/abseil/abseil-cpp/.*tar.gz' \"${BZL_FILE_PATH}\" | head -n1)\"\r\n-- \r\n2.20.1\r\n\r\n```", "comments": ["I don't know on what TF version you are but on master the download URL is ok.", "There's nothing we can do for 1.15. Can you upgrade to the latest release version of TFLite (2.3)? Alternatively, we're working on a CMake-based build system for the upcoming 2.4 release.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43348\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43348\">No</a>\n", "That said, if we end up doing some kind of patched release for 1.15, we can keep this in mind (and feel free to create a PR with your patch), but until then the branch is effectively frozen.", "Sounds good.  I'm stuck on 1.15 for now, but hopefully anyone else running into the issue is able to find this :-)", "It's too late now, but we are planning some patch releases to come out today/tomorrow.\r\n\r\nIf you can make a PR to update, on the next patch releases it will be merged"]}, {"number": 43347, "title": "Documentation about tensorflow ", "body": "Hello,\r\nI would like some information about tensorflow:\r\n\r\n1- if I build tensorflow from the source how much RAM does it take? ;\r\n\r\n2- If I install tensorflow from here https://www.tensorflow.org/install/source and use the instructions to install the tensorflow package with pip is it necessary to use Bazel too or can I do without it? Also will I have instructions for AVX, AVX2 and FMA?\r\n\r\n3- what is the difference between tensorflow 1x and 2x V \r\n\r\nBest Regards\r\n", "comments": ["@lucio-cyber \r\n1- please refer to #30047, [link](https://www.reddit.com/r/tensorflow/comments/b8utg9/how_to_determine_the_amount_of_ram_required_to/)\r\n2- refer to [link](https://github.com/tensorflow/tensorflow/issues/8037), [link1](https://medium.com/@sometimescasey/building-tensorflow-from-source-for-sse-avx-fma-instructions-worth-the-effort-fbda4e30eec3), [link2](https://syslog.ravelin.com/compiling-tensorflow-to-use-all-available-cpu-instructions-607945795b5c).\r\n3- please refer to [link](https://www.datasciencecentral.com/profiles/blogs/tensorflow-1-x-vs-2-x-summary-of-changes)", "Thank you so much for helping me.\r\nBest Regards", "@lucio-cyber \r\nHappy to help."]}, {"number": 43346, "title": "Bump sqlite to 3.33.0", "body": "This should handle CVE-2020-15358.\r\n\r\nPiperOrigin-RevId: 332484006\r\nChange-Id: Id2e7c4e877fcfaa53184fd21139a00f3234a5e3d\r\n\r\nHandles  [CVE-2020-15358](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358).", "comments": []}, {"number": 43345, "title": "Faild build windows", "body": "System information\r\n\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from: source\r\nTensorFlow version: 2.3\r\nPython version: 3.8.3\r\nBazel version (if compiling from source): 3.1.0\r\nCUDA/cuDNN version: 11.0.2_451.48/8.0.2.39\r\nGPU model and memory: 1060 super\r\n```\r\nRepository rule cuda_configure defined at:\r\n  G:/tensorflow/third_party/gpus/cuda_configure.bzl:1407:18: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1377\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1221, in _create_local_cuda_repository\r\n                to_list_of_strings(<1 more arguments>)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1222, in to_list_of_strings\r\n                _cuda_include_path(<2 more arguments>)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 364, in _cuda_include_path\r\n                inc_entries.append(<1 more arguments>)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 364, in inc_entries.append\r\n                realpath(repository_ctx, <1 more arguments>)\r\n        File \"G:/tensorflow/third_party/remote_config/common.bzl\", line 268, in realpath\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"G:/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\n[FATAL 22:56:20.372 src/main/cpp/blaze.cc:1290] Unknown startup option: '-c'.\r\n  For more info, run 'bazel help startup_options'.\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1377\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1221, in _create_local_cuda_repository\r\n                to_list_of_strings(<1 more arguments>)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1222, in to_list_of_strings\r\n                _cuda_include_path(<2 more arguments>)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 364, in _cuda_include_path\r\n                inc_entries.append(<1 more arguments>)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 364, in inc_entries.append\r\n                realpath(repository_ctx, <1 more arguments>)\r\n        File \"G:/tensorflow/third_party/remote_config/common.bzl\", line 268, in realpath\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"G:/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\n[FATAL 22:56:20.372 src/main/cpp/blaze.cc:1290] Unknown startup option: '-c'.\r\n  For more info, run 'bazel help startup_options'.\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1377\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1221, in _create_local_cuda_repository\r\n                to_list_of_strings(<1 more arguments>)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1222, in to_list_of_strings\r\n                _cuda_include_path(<2 more arguments>)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 364, in _cuda_include_path\r\n                inc_entries.append(<1 more arguments>)\r\n        File \"G:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 364, in inc_entries.append\r\n                realpath(repository_ctx, <1 more arguments>)\r\n        File \"G:/tensorflow/third_party/remote_config/common.bzl\", line 268, in realpath\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"G:/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\n[FATAL 22:56:20.372 src/main/cpp/blaze.cc:1290] Unknown startup option: '-c'.\r\n  For more info, run 'bazel help startup_options'.\r\nINFO: Elapsed time: 1.209s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n```\r\n\r\n", "comments": ["@sonfire86 \r\n\r\nPlease, see tested build configurations from [here](https://www.tensorflow.org/install/source_windows#gpu).\r\n\r\nPlease, refer hardware and software requirements from [here](https://www.tensorflow.org/install/gpu#windows_setup).Thanks!", "![image](https://user-images.githubusercontent.com/26105224/93659751-2902a680-fa62-11ea-8832-806aa0fba853.png)\r\nIm set path `cuda`", "Cuda 11 requires master branch.", "I have tried build master branch and the same error", "@sonfire86 \r\n\r\nIf you are using TF 2.3 please use cuda 10.1 as per builded test configurations from [here](https://www.tensorflow.org/install/source_windows#gpu).If you want to use CUDA 11 please try with master branch. Thanks!", "@ravikyram I have tried build master branch and the same error", "I have updated everything to the latest version\r\n![Screenshot_74](https://user-images.githubusercontent.com/26105224/93785179-3a58d800-fc47-11ea-87e1-4e8af2c9731f.png)\r\n![Screenshot_75](https://user-images.githubusercontent.com/26105224/93785180-3af16e80-fc47-11ea-8e38-354b8120c11f.png)\r\n\r\n", "You need to install `coreutils` package", "I put it, the error is the same", "Is the same with realpath command not found?", "yes, realpath command not found", "You are working on an Anaconda env. We don't support Anaconda here.\nPlease use our official build documentation or ask for support to the Anaconda project if you still require anaconda.\n", "Im not work Anaconda env. Im work system python ", "But you are using Anaconda python. Please follow our official build guide.", "Ok, I'm try build for with official python", "@bhack \r\nBuild successful with official python.\r\nWith cuda 11 can I only build maset or 2.3 branch too?", "Officially for Cuda 11 only master (and of course the next 2.4 release).", "It might be worth adding that Anaconda is not supported for installation on Windows", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43345\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43345\">No</a>\n", "> It might be worth adding that Anaconda is not supported for installation on Windows\n\nThere Is already in the installation Docs but It isn't so visibile. I've alteady asked to @lamberta how to better exposed this in another ticket.", "> You need to install `coreutils` package\r\n\r\nHow do I install it?\r\nI have the same error now. Using default python 3.6.\r\n![image](https://user-images.githubusercontent.com/46108258/118174272-38b96b80-b3f4-11eb-8546-364639129b38.png)\r\n", "Managed to solve it by installing coreutils in msys2:\r\npacman -S coreutils"]}, {"number": 43344, "title": "Support AWS IAM roles when using TensorFlow file_io", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1.0, 2.3.0\r\n- Are you willing to contribute it (Yes/No): No, sorry, I'm not very comfortable with C++\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI really appreciate the functionality provided by TensorFlow file_io to allow the user to treat files stored locally on disk in the same was as files stored in S3. It just works! It's wonderful! However, this functionality doesn't work if your AWS credentials are being provided using an AWS IAM role. Unfortunately the default AWS SDK credentials behavior does not account for this situation, and the maintainers have said that they will not incorporate this feature into their default credentials provided. They did offer a suggested way for people using the AWS SDK to support this feature. Here is the thread where this is discussed: https://github.com/aws/aws-sdk-cpp/issues/150.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nTensorFlow users who are using AWS S3 with credentials provided using AWS IAM roles\r\n\r\n**Any Other info.**\r\nHere's an example from TensorFlow 2.3.0 with the error message, run in a Docker container that uses tensorflow/tensorflow:2.3.0-cpu as the base image. The AWS_PROFILE EV is set to use my IAM role. I've confirmed that `aws s3 ls s3://my-bucket/my-file.txt` works.\r\n\r\n```\r\n>>> from tensorflow.python.lib.io import file_io as tf_file_io\r\n>>> tf_file_io.file_exists('s3://my-bucket/my-file.txt')\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 249, in file_exists\r\n    return file_exists_v2(filename)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 267, in file_exists_v2\r\n    _pywrap_file_io.FileExists(compat.as_bytes(path))\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: AWS Credentials have not been set properly. Unable to access the specified S3 location\r\n```", "comments": ["I was able to follow a suggestion on an AWS C++ SDK issue and get this working locally, however, for our use case, there's an issue in the AWS SDK where the credentials are only read from ~/.aws/config, instead of from ~/.aws/credentials, so it still doesn't quite work as we would like even after adding a custom CredentialsProviderChain to TensorFlow. Details here:\r\n\r\nhttps://github.com/aws/aws-sdk-cpp/issues/1330#issuecomment-700195182", "Hi, any word on this issue? IMHO, this is rather critical as it's blocking #1252. I imagine lots of folks do/will want to load Parquet and other formats into TensorFlow from AWS S3. Just adding my vote here; thanks.", "We are moving cloud filesystems to SIG IO due to size constraints on the TF wheel package. SIG IO filesystems already provide more support than what we can offer.", "As per the comment here if your concern is addressed, could you please move this issue to closed. Thanks.\r\n> We are moving cloud filesystems to SIG IO due to size constraints on the TF wheel package. SIG IO filesystems already provide more support than what we can offer.\r\n\r\n", "@sachinprasadhs, I'm not sure who your comment is addressing. I've given up on this functionality.  m(-_-)m \r\n\r\nPerhaps @dgoldenberg-audiomack has what they needed now?", "Have what? I'm not a contributor. Gave up on [the ticket](https://github.com/tensorflow/tensorflow/issues/1252.) that this one was apparently blocking", "@sheromon , Since you have opened this issue and if you don't have this issue anymore, please go ahead and close this issue. Thanks!", "@sachinprasadhs, okay, let me check with my collaborators first.", "Well, I've found workarounds (although I didn't like any of them more than TensorFlow's file_io module), so I guess we can close this.", "@sheromon what was your workaround if you don't mind me asking?", "@dvaldivia Using the smart_open Python package in some places and custom functions that use boto3 in other places. It's not as convenient, but it does the job."]}, {"number": 43343, "title": "Initialize POD types", "body": "Change-Id: I60cfb1744c318521ff7fb70512ebd7540a965bfc\r\n\r\nThis triggers filling fields in a POD type with the default values (0\r\nfor most parts).\r\n\r\nOtherwise 'new' returns uninitialized memory,  and in the guts of\r\nTFLite not all data that is obtained from AllocatePOD is initialized.\r\n\r\nIn particular this is the case for SQUEEZE:\r\nhttps://github.com/tensorflow/tensorflow/blob/70b61c44ae8f60956d936fc965a1c9e46af10d55/tensorflow/lite/core/api/flatbuffer_conversions.cc#L634\r\nIf code doesn't go into the body of an 'if' statement, then params are\r\nleft with uninitialized junk from the heap. Patch resolves this by\r\nzero filling data before when it leaves AllocatePOD.", "comments": []}, {"number": 43342, "title": "Modifying get_optimizer_experimental_options", "body": "PR related to #42450\r\n\r\nThis pull request should fix the [issue](https://github.com/tensorflow/tensorflow/pull/43105#issuecomment-694910626) I brought up in #43105.\r\n\r\nModifications made to:\r\n- `context.py`", "comments": ["@jaingaurav could you review this?", "@Harsh188: This doesn't seem right, I'm not sure what you're trying to achieve here, but what I talked about in #43105 referred to extracting the default values from C++ and exposing them up to the python layer somehow. I'm not sure how this achieves that.", "@jaingaurav I thought about making an API to bring up the values from C++ but when I was reviewing [`context.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/context.py) I noticed something strange.\r\n\r\nThe `get_optimizer_experimental_options` function was not utilizing the global variable `_optimizer_experimental_options` dictionary.\r\n\r\nIf you take a look at [`set_optimizer_experimental_options`](https://github.com/tensorflow/tensorflow/blob/40c3e9fc618a7a1d4a466a877205cfa5a89f2132/tensorflow/python/eager/context.py#L1575) the global dictionary gets updated whenever a user wants to modify the options. There are also two functions on line [965](https://github.com/tensorflow/tensorflow/blob/40c3e9fc618a7a1d4a466a877205cfa5a89f2132/tensorflow/python/eager/context.py#L965) `rewriter_toggle` and `rewriter_bool` which initialize the `_optimizer_experimental_options` dictionary with default values.\r\n\r\nHowever, [`get_optimizer`](https://github.com/tensorflow/tensorflow/blob/40c3e9fc618a7a1d4a466a877205cfa5a89f2132/tensorflow/python/eager/context.py#L1537) was the only function to not utilize the global dictionary. It instead, does the `rewriter_toggle` process, performed in the initial initialization of the dictionary, all over again.\r\n\r\nThis rose a few red flags for me. After taking a closer look at the [`rewriter_toggle`](https://github.com/tensorflow/tensorflow/blob/40c3e9fc618a7a1d4a466a877205cfa5a89f2132/tensorflow/python/eager/context.py#L1546) function defined in `get_optimizer_experimental_options` I noticed the following piece of code:\r\n```python\r\nattr = getattr(rewrite_options, option)\r\n      if attr != 0:\r\n        options[option] = (attr == rewriter_config_pb2.RewriterConfig.ON)\r\n```\r\nYou can see here that if attr is not 1, then the option will not be updated to the dictionary. This is why only two values were being displayed in the [comment](https://github.com/tensorflow/tensorflow/pull/43105#issuecomment-692166581) I left in the original PR.\r\n\r\nEssentially this entire method seemed redundant to me since all it had to do was return the global dictionary which already had the options initialized to their default values. That's why I opened up this PR.\r\n\r\n_Sorry for the delayed response, I've been piled up with college work :'(_", "@jaingaurav Can you please take a look on above comments from @Harsh188. Thanks!", "@jaingaurav Friendly ping :)", "@jaingaurav Can you please take a look on this PR ? Thanks!", "@Harsh188 Sorry for the late response.  Have you confined that `self._optimizer_experimental_options` is correctly initialized with default optimizer options? It wasn't clear to me when I read, and it seemed the dictionary is only updated here https://github.com/tensorflow/tensorflow/blob/9f813ea/tensorflow/python/eager/context.py#L1572 , not at initialization.  Also, in any case, let's add a test in `context_test.py` in the same directory.", "Thanks for the reply @kkimdev!\r\n\r\nYou are correct. I don't think that `self._optimizer_experimental_options` is initialized correctly. It doesn't look like `_optimizer_experimental_options` is ever updated during the initialization process. Do you have any suggestions on how to fix it?\r\n\r\nI would also love to get advice on how I can add a test in `context_test.py`. I'm not sure how to expose the default options set in C++ to test the validity of the optimizer options in the python layer.", "@kkimdev Can you please take a look on the above comment from @Harsh188. Thanks!", "@Harsh188 Default values are implicitly hard-coded here https://github.com/tensorflow/tensorflow/blob/66b124e/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L224 , so we need some refactoring.  I think `MetaOptimizer` should have an API returning default values so that we can initialize `self._optimizer_experimental_options` correctly.", "@Harsh188 Can you please check @kkimdev's comments and keep us posted ? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@Harsh188  Any update on this PR? Please. Thanks!", "Hi, sorry to inform you that I won't be able to continue this any further. Unfortunately, I haven't had much luck in putting together the solution and I don't feel like I'm experienced enough with C++ to debug my issues. I hope someone can pick up where we left off."]}, {"number": 43341, "title": "Deprecated get_losses_for function in migration guide", "body": "\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/migrate?hl=en\r\n\r\n## Description of issue (what needs changing):\r\nIn the **Custom model_fn with minimal changes** section of the migration guide,  it is recommended to use `get_losses_for` but this function is deprecated and mapped to the model.losses property.\r\n https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/engine/base_layer.py#L1895\r\n\r\n\r\nFurthermore, in the example this function is called two times with differents parameters (`None` and `features`) which would results in adding the regularization loss twice. \r\n\r\n\r\n\r\n", "comments": ["Thanks, Can you submit a PR?", "Ok, i'm on it.\r\nSo to be sure, currently, one can simply use `model.losses` ?", "@MarkDaoust,\r\nCan you please confirm if we can close this issue with respect to [this PR](https://github.com/tensorflow/docs/pull/1678) and with respect to [this Commit](https://github.com/tensorflow/docs/commit/ca65d8e310d33de95f721da6f0405f00ef6a4aed)? Thanks!", "Yes, this was fixed by: https://github.com/tensorflow/docs/commit/ca65d8e310d33de95f721da6f0405f00ef6a4aed", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43341\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43341\">No</a>\n"]}, {"number": 43340, "title": "Tensorflow lite speech sample is not detecting silence (wrong detections)", "body": "Hi, I am using the sample https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/android . I am facing the following issues:\r\n- When there is a silence it detects as a speech?\r\n- It also detects fan noise, clap or any tick noises as a speech which it should not as it is not a speech of a person.\r\n- It does not detect voice correctly, like If I have spoken Yes it does not detect and same for other words as well.\r\n\r\nCan you please guide which are the missing steps or which model to use if sample is not update? Speech and silence should be detected correctly. Currently it detects silence as words as well.", "comments": ["Hi Mustansar,\r\n\r\nThe model is trained on small open dataset, contributed by the volunteers. If you want to improve the accuracy for background noise on fan noise and clap sounds, you can record some background noise dataset and use this training script to train a model with better quality.\r\n\r\nhttps://www.tensorflow.org/tutorials/audio/simple_audio\r\n\r\nThanks,\r\nTiezhen", "@mustansarsaeed  Please check the above comment and let us know if you still have any queries.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43340\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43340\">No</a>\n"]}, {"number": 43339, "title": "Tensorflow Lite: iOS 14 breaks the NPU Delegate", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n iOS 14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\niPhone 11 pro,\r\n- TensorFlow installed from (source or binary):\r\nvia pods: pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly'\r\n- TensorFlow version (use command below):\r\nTensorFlowLiteSwift 0.0.1-nightly\r\n\r\n**Describe the current behavior**\r\nThe update to iOS 14 broke the NPU (coreML) Delegate in tensorflow Lite.\r\nThe issue presents itself in the tensorflow lite example on iOS for posenet.\r\nAs soon as the coreML delegate is selected the joints of the body arent recognized properly anymore and the output of the net is garbage.\r\n\r\nImages showing the correct (GPU) and incorrect (NPU) pose estimation:\r\n[https://imgur.com/a/UzMM6Pt](https://imgur.com/a/UzMM6Pt)\r\n\r\n\r\n**Describe the expected behavior**\r\nThe NPU delegate should produce the same results as CPU/GPU\r\n\r\n**Standalone code to reproduce the issue**\r\nDownload the tensorflow Lite example for posenet on iOS and select the NPU delegate on a iOS 14 device\r\n", "comments": ["Hi @boost-app , I can confirm I'm seeing the same issue on iOS 14 (iPhone Xs).\r\n\r\nThanks for the report and will take a look.", "With the manually converted mlmodel from tflite model, I could reproduce the same issue on iOS PoseFinder demo app. (see details on the coremltools issue.) Will keep investigating is this is actually a bug in iOS 14 or the delegate.", "I tested again on iOS 14.1 and 14.2 and it seems to be resolved. Can someone verify?", "Yes, it works well for iOS 13.7, 14.1, 14.2. \r\nBut it didn't work for iOS 14.0.", "@boost-app It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest  stable Version of TF 2.6 and let us know if the issue still persists? Please have a look at the [link](https://www.tensorflow.org/lite/performance/delegates) and let us know if it helps?  Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43339\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43339\">No</a>\n"]}, {"number": 43338, "title": "Revert \"2.3.0-rc2 cherry-pick request: Cherry pick library threadsafestatus \"", "body": "Reverts tensorflow/tensorflow#41669", "comments": []}, {"number": 43337, "title": "Using Tensorflow-2.3.0 with GPU", "body": "I downloaded **tensorflow-gpu** using pip. I set `Ld_PATH_LIBRARY` to:\r\n```\r\nLd_PATH_LIBRARY=\"/usr/local/cuda/lib64\"\r\nLd_PATH_LIBRARY=\"/usr/local/cuda-10.1/lib64\"\r\nLd_PATH_LIBRARY=\"/usr/local/cuda/extras/CUPTI/lib64\"\r\n```\r\nfor all path tensorflow shows a problem:\r\n```\r\n2020-09-18 17:37:06.179293: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\r\n```\r\nI am using **Ubuntu 20.04** and **python-3.8**", "comments": ["@meSajied \r\nWe see that you have not filled the issue template, please let us know the tf version used as pythong 3.8 is compatible from tf 2.2\r\nPlease refer to below issues and let us know:\r\n#43236 #26182 [link](https://stackoverflow.com/questions/55224016/importerror-libcublas-so-10-0-cannot-open-shared-object-file-no-such-file-or) #30638 ", "I see the title has the tf version. Please refer to the issues shared and let us know if it helps.", "@Saduf2019\r\nI asked a question in [stackoverflow](https://stackoverflow.com/questions/63871922/tensorflow-2-3-0-does-not-detect-gpu/63872640?noredirect=1#comment112960769_63872640). You will find all information there.\r\nThese answer was not helpful for me.", "@meSajied \r\nAs the question is asked in SO please move this to closed status, as it is tracked there. Kindly have the issue on one platform.", "@Saduf2019\r\nOkay.. thank you...", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43337\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43337\">No</a>\n"]}, {"number": 43336, "title": "2020-09-18 19:08:48.694940: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found 2020-09-18 19:08:48.699968: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n![image](https://user-images.githubusercontent.com/60377675/93592515-e77fe600-f9e4-11ea-8482-e8dbc12ef6d0.png)\r\n", "comments": ["Can you check https://github.com/tensorflow/tensorflow/issues/43361 ?", "@bhack \r\n\r\nPlease, fill issue template. Also, refer the [link](https://stackoverflow.com/questions/59823283/could-not-load-dynamic-library-cudart64-101-dll-on-tensorflow-cpu-only-install) and see if it helps you.\r\n\r\nIf you installed CUDA,You may need to find which directory these dlls are in, and then update your PATH to include that directory. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43336\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43336\">No</a>\n", "i got this error, when i  install tensorflow-gpu. please help me.\r\n\r\n\r\nW tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/yogitamp/gputry/env/lib/python3.6/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu/:/usr/local/caffe-1.0/build/install/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu/hdf5/serial:/usr/local/cuda/lib64:/usr/local/caffe-1.0/build/install/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu/hdf5/serial:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu/:\r\n2021-10-25 16:22:40.698917: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine."]}, {"number": 43335, "title": "Output of TensorFlow Java API differs from the one in TensorFlow Python", "body": "Hi there, \r\n\r\nI'm processing an image with a TF trained model using the TensorFlow Java API but I get slightly different results from what the model outputs in python. \r\n\r\nThe model was trained with TF version 1.15.2 in Python and libtensorflow-1.15.0.jar. \r\nIs there any well-known difference between the last two?\r\n\r\nThank you!\r\n", "comments": ["@esgomezm,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.3 and check if you are facing the same issue?\r\n\r\nAlso, in order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43334, "title": "F tensorflow/core/framework/tensor_shape.cc:44] Check failed: NDIMS == dims() (2 vs. 5)Asking for tensor of 2 dimensions from a tensor of 5 dimensions", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@18813020802 \r\nPlease update the issue template with details, stand alone code, error log and steps followed before you faced the error.", "For the error reported, please refer to: [link](https://stackoverflow.com/questions/43413293/tensorflow-check-failed-ndims-dims-2-vs-1), [link1](https://github.com/tensorflow/tensorflow/issues/9505)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43333, "title": "Fixing bug where undocumented FileNotFound error is thrown when trying to use tf.io.gfile.glob()", "body": "PR to fix issues presented in Issue [#43319](https://github.com/tensorflow/tensorflow/issues/43319). Currently, a undocumented FileNotFound error is raised when trying to match a non-existent directory and/or the files inside of it. This PR adds logic to catch that error and instead return a empty list corresponding to that pattern.", "comments": ["I think it's better to document the error instead of failing silently. Silent failures cause more issues down the line.", "gotcha, would something like rethrowing the error with a more descriptive message make more sense (+ documentation)?", "Rethrowing the error could help, though see that even linux just says that the path is not found.", "sounds good, I will change this PR to just add documentation. is the website documentation auto generated from docstrings or is there a different repository I need to submit a PR to as well?", "It is generated from docstrings."]}, {"number": 43332, "title": "Tf Lite Micro Global Average Pooling not supported", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Project build on windows, cygwin console:\r\n- TensorFlow installed from source\r\n- Tensorflow version: 2.3.0, commit 4230dd89cff\r\n- Target platform: stm32f429, ARM4\r\n\r\nI use quantized TF Lite model trying to run it on target platform. Model loads correctly, tenaor allocates correctly, but I have a problem during invoking. In my original Keras model I use Global Average Pooling, which is being converted to Mean operation in Tflite file. Netron diagram:\r\n\r\n![image](https://user-images.githubusercontent.com/58625554/93563695-b40f6e00-f988-11ea-9a10-3e754cf3cdf5.png)\r\n\r\n\r\nAfter invoking I get folowing error:\r\n_tensorflow/lite/micro/kernels/reduce.cc Number of Input dimensions != 4 OR the Axis is not either [1, 2] or [2, 1]\r\n\r\nNode MEAN (number 19) failed to invoke with status 1_\r\n\r\nI belive that there is a problem in reduce.cc file and MEAN operation implementation. I am not sure why should I provide 4D input for MEAN operation.\r\nDo you plan to implement 3D impleentation or may suggest any workaround for this problem?\r\n\r\nFault file: \r\ntensorflow_src\\tensorflow\\lite\\micro\\kernels\\reduce.cc\r\nIn EvaMean function:\r\n`TF_LITE_ENSURE_MSG(\r\n      context, is_valid_inputs == true,\r\n      \"Number of Input \"\r\n      \"dimensions != 4 OR the Axis is not either [1, 2] or [2, 1]\");`\r\n\r\n\r\n\r\n\r\n", "comments": ["I've created an internal change to fix this. Hopefully it should be submitted by early next week.", "Update: This should now be fixed with my change to tensorflow/lite/micro/kernels/reduce.cc.", "https://github.com/tensorflow/tensorflow/commit/9a662b14eaa85ce466be82abd758a636c1eaa756"]}, {"number": 43331, "title": "InternalError:cudnn poolforward launch failed", "body": "information:\r\n     centos 7\r\n     tensorflow-gpu: 12.0\r\n     tensorflow-compression: 10.0\r\n     cuda:  9.0\r\n\r\nwhen i doing a forward test code,the wrong information as belows:\r\n![5074244104368395475](https://user-images.githubusercontent.com/37282247/93563200-fef5a980-f9b9-11ea-9913-7a29191d1c3d.jpg)\r\n\r\n\r\nand if i use tensorflow: 12.0 it's ok ,but using cpu runnig  is too slow.\r\n\r\nDoes anyone has anygood idea to solve it?\r\n\r\n\r\n", "comments": ["@weihua04,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.3 and check if you are facing the same issue?\r\n\r\nAlso, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#tested_build_configurations) and make sure you have all the compatible dependencies installed. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43331\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43331\">No</a>\n"]}, {"number": 43330, "title": "How does TF decide to add examples for one API?", "body": "I find that TF add examples for [tf.keras.backend.clear_session()](https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session), concatenate and etc., but not for [tf.keras.backend.batch_normalization](https://www.tensorflow.org/api_docs/python/tf/keras/backend/batch_normalization) or tf.keras.backend.binary_crossentropy. \r\n\r\nDoes it freely for API developers to write examples? If that, does it seems chaotic?", "comments": ["@zjzh \r\nPlease refer to below links and let us know if it helps.\r\n[link](https://www.programcreek.com/python/example/93701/keras.backend.binary_crossentropy) for \"binary_crossentropy\" and[ link](https://docs.w3cub.com/tensorflow~python/tf/keras/backend/batch_normalization/) for \"tf.keras.backend.batch_normalization\" [[also link](https://www.programcreek.com/python/example/93723/keras.backend.batch_normalization])]", "Yes, it is useful. But these codes can not directly run. ", "@zjzh \r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.", "@ry Yes, but I think that whether people should think that the reasonability of adding some examples. Because it seems possess randomness. For example, the users want to add examples for random_uniform, random_normal, https://github.com/tensorflow/tensorflow/issues/31277. But it is strange to add the example , and the added example is a little meaningless. Because the result does not give something.", "@zjzh Thanks for your issue. I agree that there is some degree of randomness when it comes to examples for certain API's. But again most of those examples are added by the community chosen by them independently. Some of the examples by themselves may not mean much but helps new users to familiarize themselves with its usages. For in depth understanding we also point out the tutorials, guides, colabs where those functions are used and why. [For example](https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session#used-in-the-notebooks)\r\nHowever such efforts are still limited and are WIP. We do appreciate community helping out and creating Pull Requests to add more such example usages and wherever they feel there is a scope of doc improvement. Having said that we are constantly working to improve documentation and add more example usages as well. ", "The API surface of TF is huge and documentation is still lacking. Since OSS users using the APIs know better what are relevant usage patterns and can point to the most important APIs, we rely on community to provide documentation.\r\n\r\nWe have internal documentation fix-its to also documenting popular APIs and bring all documentation to a similar standard. But it's a matter of numbers: a few people at Google versus thousand of OSS contributors."]}, {"number": 43329, "title": "Also fix single pip Windows GPU renaming on python 3.8", "body": "", "comments": []}, {"number": 43328, "title": "Also fix single pip Windows GPU renaming on python 3.8", "body": "", "comments": []}, {"number": 43327, "title": "Fix rename of gpu pips for single pip package on Windows GPU", "body": "", "comments": []}, {"number": 43326, "title": "Fix rename of gpu pips for single pip package on Windows GPU", "body": "", "comments": []}]