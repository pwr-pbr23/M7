[{"number": 17667, "title": "Sfu2/numa test CLA", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "close"]}, {"number": 17666, "title": "iOS: Library that made for a specific model doesn't include all needed operators (RandomStandardNormal is missing)", "body": "I created a quantized PB file, here is its detail:\r\n![screenshot from 2018-03-13 08-14-27](https://user-images.githubusercontent.com/11812805/37319105-56c7618e-26a0-11e8-9bc9-cdc696c5267f.png)\r\n\r\nThen I followed the instruction [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile) to create the iOS library for that model only. The command I used:\r\n`tensorflow/contrib/makefile/build_all_ios.sh -g mobile_quantized.pb`\r\n\r\nHere is the generated ops_to_register.h file:\r\n\r\n```\r\n// This file was autogenerated by print_selective_registration_header.py\r\n#ifndef OPS_TO_REGISTER\r\n#define OPS_TO_REGISTER\r\n\r\n    namespace {\r\n      constexpr const char* skip(const char* x) {\r\n        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;\r\n      }\r\n\r\n      constexpr bool isequal(const char* x, const char* y) {\r\n        return (*skip(x) && *skip(y))\r\n                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))\r\n                   : (!*skip(x) && !*skip(y));\r\n      }\r\n\r\n      template<int N>\r\n      struct find_in {\r\n        static constexpr bool f(const char* x, const char* const y[N]) {\r\n          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);\r\n        }\r\n      };\r\n\r\n      template<>\r\n      struct find_in<0> {\r\n        static constexpr bool f(const char* x, const char* const y[]) {\r\n          return false;\r\n        }\r\n      };\r\n    }  // end namespace\r\n    constexpr const char* kNecessaryOpKernelClasses[] = {\r\n\"BinaryOp< CPUDevice, functor::add<float>>\",\r\n\"BinaryOp< CPUDevice, functor::add<int32>>\",\r\n\"BiasOp<CPUDevice, float>\",\r\n\"ConstantOp\",\r\n\"DequantizeOp<CPUDevice, quint8>\",\r\n\"EnterOp\",\r\n\"ExitOp\",\r\n\"UnaryOp< CPUDevice, functor::exp<float>>\",\r\n\"ExpandDimsOp<int32>\",\r\n\"BinaryOp< CPUDevice, functor::greater_equal<float>>\",\r\n\"IdentityOp\",\r\n\"BinaryOp< CPUDevice, functor::less<int32>>\",\r\n\"LoopCondOp\",\r\n\"MatMulOp<CPUDevice, float, false >\",\r\n\"BinaryOp< CPUDevice, functor::maximum<float>>\",\r\n\"ReductionOp<CPUDevice, float, int32, Eigen::internal::MeanReducer<float>>\",\r\n\"MergeOp\",\r\n\"BinaryOp< CPUDevice, functor::minimum<float>>\",\r\n\"BinaryOp< CPUDevice, functor::mul<float>>\",\r\n\"NextIterationOp\",\r\n\"NoOp\",\r\n\"PlaceholderOp\",\r\n\"PhiloxRandomOp<CPUDevice, random::NormalDistribution<random::PhiloxRandom, float>>\",\r\n\"RangeOp<::tensorflow::int32>\",\r\n\"BinaryOp< CPUDevice, functor::div<float>>\",\r\n\"ReshapeOp\",\r\n\"ShapeOp<int32>\",\r\n\"UnaryOp< CPUDevice, functor::sigmoid<float>>\",\r\n\"UnaryOp< CPUDevice, functor::square<float>>\",\r\n\"StridedSliceOp<CPUDevice, ::tensorflow::int32>\",\r\n\"StridedSliceOp<CPUDevice, float>\",\r\n\"BinaryOp< CPUDevice, functor::sub<float>>\",\r\n\"BinaryOp< CPUDevice, functor::sub<int32>>\",\r\n\"ReductionOp<CPUDevice, float, int32, Eigen::internal::SumReducer<float>>\",\r\n\"SwitchOp\",\r\n\"UnaryOp< CPUDevice, functor::tanh<float>>\",\r\n\"TensorArrayPackOrGatherOp<CPUDevice, float, false >\",\r\n\"TensorArrayReadOp<CPUDevice, float>\",\r\n\"TensorArrayUnpackOrScatterOp<CPUDevice, float, false >\",\r\n\"TensorArraySizeOp\",\r\n\"TensorArrayOp\",\r\n\"TensorArrayWriteOp<CPUDevice, float>\",\r\n\"TileOp<CPUDevice, int32>\",\r\n\"TransposeCpuOp\",\r\n\"ZerosLikeOp< CPUDevice, float>\",\r\n\"RecvOp\",\r\n\"SendOp\",\r\n};\r\n#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))\r\n\r\nconstexpr inline bool ShouldRegisterOp(const char op[]) {\r\n  return false\r\n     || isequal(op, \"Add\")\r\n     || isequal(op, \"BiasAdd\")\r\n     || isequal(op, \"Const\")\r\n     || isequal(op, \"Dequantize\")\r\n     || isequal(op, \"Enter\")\r\n     || isequal(op, \"Exit\")\r\n     || isequal(op, \"Exp\")\r\n     || isequal(op, \"ExpandDims\")\r\n     || isequal(op, \"GreaterEqual\")\r\n     || isequal(op, \"Identity\")\r\n     || isequal(op, \"Less\")\r\n     || isequal(op, \"LoopCond\")\r\n     || isequal(op, \"MatMul\")\r\n     || isequal(op, \"Maximum\")\r\n     || isequal(op, \"Mean\")\r\n     || isequal(op, \"Merge\")\r\n     || isequal(op, \"Minimum\")\r\n     || isequal(op, \"Mul\")\r\n     || isequal(op, \"NextIteration\")\r\n     || isequal(op, \"NoOp\")\r\n     || isequal(op, \"Placeholder\")\r\n     || isequal(op, \"RandomStandardNormal\")\r\n     || isequal(op, \"Range\")\r\n     || isequal(op, \"RealDiv\")\r\n     || isequal(op, \"Reshape\")\r\n     || isequal(op, \"Shape\")\r\n     || isequal(op, \"Sigmoid\")\r\n     || isequal(op, \"Square\")\r\n     || isequal(op, \"StridedSlice\")\r\n     || isequal(op, \"Sub\")\r\n     || isequal(op, \"Sum\")\r\n     || isequal(op, \"Switch\")\r\n     || isequal(op, \"Tanh\")\r\n     || isequal(op, \"TensorArrayGatherV3\")\r\n     || isequal(op, \"TensorArrayReadV3\")\r\n     || isequal(op, \"TensorArrayScatterV3\")\r\n     || isequal(op, \"TensorArraySizeV3\")\r\n     || isequal(op, \"TensorArrayV3\")\r\n     || isequal(op, \"TensorArrayWriteV3\")\r\n     || isequal(op, \"Tile\")\r\n     || isequal(op, \"Transpose\")\r\n     || isequal(op, \"ZerosLike\")\r\n     || isequal(op, \"_Recv\")\r\n     || isequal(op, \"_Send\")\r\n  ;\r\n}\r\n#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)\r\n\r\n#define SHOULD_REGISTER_OP_GRADIENT false\r\n#endif\r\n```\r\n\r\nBut when I tried it within my project, it couldn't load the mobile_quantized.pb file and thrown this\r\n\r\n<img width=\"745\" alt=\"screen shot 2018-03-13 at 9 44 18 am\" src=\"https://user-images.githubusercontent.com/11812805/37319803-2d88f064-26a3-11e8-974b-a57a43792055.png\">\r\n\r\nIt's strange because I saw `\"PhiloxRandomOp<CPUDevice, random::NormalDistribution<random::PhiloxRandom, float>>\"` and `|| isequal(op, \"RandomStandardNormal\")` in the generated header file.\r\n\r\n---------\r\nOS Platform and Distribution: MacOS 10.13.3\r\nTensorFlow installed from: Source code\r\nTensorFlow version: 1.6.0\r\nBazel version: 0.11.1-homebrew\r\nCUDA/cuDNN version: None\r\nGPU model and memory: None\r\nExact command to reproduce: Described above", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler, @jart  : updated the question. \r\nThanks", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Dear @jart, I still think it's not a problem for StackOverflow.\r\n\r\nI checked again and found that the `tf_op_files.txt` file contains only:\r\n```\r\ntensorflow/core/ops/random_ops.cc\r\ntensorflow/core/ops/random_grad.cc\r\n```\r\n\r\nAnd missing this: `tensorflow/core/kernels/random_op.cc`\r\n\r\nAfter adding the missing line, it can load the model now", "Ah I see, this is a contrib/makefile maintenance issue. That file might have been left out for a reason, but let's see what happens if I add it. https://github.com/tensorflow/tensorflow/pull/17970"]}, {"number": 17665, "title": "Fix link", "body": "Fix wrong link\r\n\r\nfrom\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/convolution\r\nto \r\nhttps://www.tensorflow.org/api_guides/python/nn#Convolution\r\n\r\nwithout the fix the link is kinda dangling:\r\n>as described in the [comment here](https://www.tensorflow.org/api_docs/python/tf/nn/convolution).\r\n\r\nin\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/convolution\r\n", "comments": []}, {"number": 17664, "title": "Sfu2/numa", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "eee"]}, {"number": 17663, "title": "tf.matrix_solve_ls: Documentation corrupt?", "body": "Hi,\r\nI just found tf.matrix_solve_ls and read its documentation at https://www.tensorflow.org/api_docs/python/tf/matrix_solve_ls :\r\n![grafik](https://user-images.githubusercontent.com/1200058/37315800-cbf13e16-265b-11e8-9ead-727fd6d95c7d.png)\r\nSeems like there are some markdown errors...\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: N/A\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@MarkDaoust ", "Created a PR #18022 to fix this math equation format issue"]}, {"number": 17662, "title": "tfdbg: Feature Request: enable and disable during run", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux XX 3.10.0-514.6.1.el7.x86_64 #1 SMP Sat Dec 10 11:15:38 EST 2016 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7 (Core)\"\r\nVERSION_ID=\"7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.4.0\r\n- **Python version**: \r\n3.5.2\r\n- **Bazel version**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nCUDA 8.0\r\n- **GPU model and memory**:\r\nP100, 16G\r\n- **Exact command to reproduce**:\r\nN/A\r\n\r\n### Describe the problem\r\nI would like to enable and disable tfdbg while running my python code.  At the moment, every time a <code>sess.run()</code> is encountered I get to the <code>tfdbg></code> prompt.\r\n\r\nI am able to run tfdbg by wrapping my session:\r\n\r\n    sess = tf_debug.LocalCLIDebugWrapperSession(sess)\r\n\r\nThis works fine.  However, I'm performing an optimization.  I'd like to, on occasion, enter tfdbg.  But mostly, I don't want to.  It appears like its an either/or proposition at the moment.  I could work-around but would rather have a way to stop or start.  An implementation detail is that I've captured \\<ctrl\\>-C which allows me to make changes to the running optimization.  There I would enable or disable tfdbg such that the next time a sess.run() is hit, tfdbg works as requested.\r\n\r\nI apologize in advance if there is already a way to do this.  I couldn't find it if its there.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "/CC @caisq", "(updated per @tensorflowbutler instructions )", "@robertlugg Thank you for filing this issue. \r\n\r\nThe `run` command of the tfdbg command-line interface has a number of flags\r\n\r\n1. `-n`, which lets you execute the next session.run(), without debugger, e.g.,\r\n\r\n    ```\r\n    tfdbg> run -n\r\n    ```\r\n\r\n2. `-t`, which allows you to run through a number session.run() calls without stopping in the debugger, e.g.,\r\n\r\n    ```\r\n    tfdbg> run -t 10\r\n    ```\r\n\r\nMore details on the the two flags are at: https://www.tensorflow.org/programmers_guide/debugger#tfdbg_cli_frequently-used_commands\r\nAre these two flags good enough for your purposes? If not, I'd like to hear why so I can see how we can make it better. Please let me know. Thanks.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing issue due to no activity. Please feel free to comment or reopen if you have more questions."]}, {"number": 17661, "title": "contrib/quantize: minor spelling", "body": "", "comments": []}, {"number": 17660, "title": "Make tensorflow/python:framework_importer_test large", "body": "tensorflow/python:framework_importer_test sometime times out during release builds", "comments": []}, {"number": 17659, "title": "MKL: Updating Performance Guide Documentations ", "body": "", "comments": ["I will start looking through this for the summit.  I need to verify the linking works as expected with the markdown style used by the site.  Target to have some comments by the end of this week.", "> I need to verify the linking works as expected with the markdown style used by the site.\r\n\r\nI've confirmed that this works as intended.", "This is going to take more time.  I agree with the content but there are too many changes here for me to easily approve.  Our markdown parser for TensorFlow.org is picky and just because it renders in MD in github does not mean it will render on tensorflow.org.  That is being fixed in the next 2-4 weeks.  I will try to come back to this.  I also feel there is some rewriting needed as some sections feel more like marketing now and it feel out of place.  Especially the top where I was summarizing the CPU and GPU perf sections.  A little flowering language is fine but it looks silly to me at this point and I would need to have the tech writers review it.  This is me being overly honest as I would prefer to just accept the changes and get this done."]}, {"number": 17658, "title": "Fix to 'Model' object has no attribute '_container_nodes' error when using tf.keras.utils.plot_model().", "body": "Fix to #17633\r\n'Model' object has no attribute '_container_nodes' error when using tf.keras.utils.plot_model().\r\nReplaced\r\n`if node_key in model._container_nodes:`\r\nwith\r\n`if node_key in model._network_nodes: # pylint: disable=protected-access`\r\n\r\nin tensorflow\\python\\keras_impl\\keras\\utils\\vis_utils.py.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "It seems this fix should be merged into master branch not r1.7. Only severe issue fixes should be made to the releases. Feel free to open another PR to do that.", "Ok, I will open another PR for merging to master.", "Sorry, didn't realize we can change base branch directly. You don't need to open another PR."]}, {"number": 17657, "title": "Fix to tf.keras.utils.plot_model() error", "body": "Fix to #17633\r\n'Model' object has no attribute '_container_nodes' error when using `tf.keras.utils.plot_model()`.\r\nReplaced \r\n`if node_key in model.container_nodes:`\r\nwith\r\n`if node_key in model._network_nodes: # pylint: disable=protected-access`\r\n\r\nin tensorflow\\python\\keras\\_impl\\keras\\utils\\vis_utils.py.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 17656, "title": "Support NUMA aware CPU device", "body": "We noticed TensorFlow does not scale too much from single CPU socket to multiple CPU sockets. From the performance profiling, we found the memory traffic between NUMA nodes are very high. The latency from the remote memory access prevents TensorFlow from scaling to multiple NUMA nodes.\r\n\r\nTo fix this performance issue, we did a prototype to use data parallelism to improve the performance. A NumaDevice class has been added, all threads in a NumaDevice are pinned to the cores in one NUMA node, each NumaDevice has its own memory allocator. With the NumaDevice, most of the tensor are created and accessed by the same Numa node, the memory access should be local. \r\n\r\nThe input data are evenly distributed to all NUMA nodes, the loss value and the gradient are computed on each Numa node and then the gradients from all Numa nodes are averaged and the variables are updated by these gradients. The variable update is done on all CPU cores.\r\n\r\nThis pull request contains a prototype to support NUMA aware CPU device. Some code are hacked to make it run. We want to send this pull request to get the comments about what we are doing.\r\n\r\nFrom our initial performance test, ResNet 50 inference got **1.4x** speedup than the master branch on two socket Skylake. ResNet 50 training got **1.14x** speedup, which is lower than what we expected, we will continue working on it.\r\n\r\nTo make it compile, you need to copy NonBlockingThreadPool.h to compile cache directory/external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool/. Also the benchmark script need be modified slightly. I can send you the instructions for changing the benchmark script.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLA is added."]}, {"number": 17655, "title": "Fix the script entry point for freeze_graph.", "body": "The wrapper created by `setup.py` calls the entry point\r\nfunction with no arguments. `freeze_graph.main` expects\r\nthe global `FLAGS` to be set, and one argument.\r\n\r\nThis change adds a `run_main` function to use as the entry point,\r\nwhich expects no arguments and parses the flags.\r\n\r\nIt also adds a `flags` argument to `freeze_graph.main` so the flags\r\ncan be passed directly without using a `global FLAGS` declaration.", "comments": []}, {"number": 17654, "title": "2D tf.nn.convolution() output is inconsistent between inputs 1x1 and 2x2 with kernel=3x3, strides=2, padding='SAME'.", "body": "Output of Keras `Conv2D()`, which is a passthrough to `tf.nn.convolution()`, is inconsistent between inputs 1x1 and 2x2 with kernel=3x3, strides=2, padding='SAME'.\r\n\r\nThe following code with 2x2 input produces wrong output `8.`, which is inconsistent with correct output `4.` if the input is 1x1. \r\n```\r\nimport numpy as np\r\nfrom keras.layers import Conv2D\r\nfrom keras.models import Sequential\r\n\r\nw = np.arange(3*3*1*1).reshape(3, 3, 1, 1)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(1, kernel_size=3, strides=2, input_shape=(2, 2, 1),\r\n                 padding='SAME', use_bias=False))\r\nmodel.set_weights([w])\r\nprint(model.predict(np.ones((1, 2, 2, 1))))\r\n```", "comments": ["For more information, see https://github.com/keras-team/keras/pull/9473#issuecomment-372166860, which narrowed it down to \"odd kernel / even input\", and also found inconsistency with Theano and CNTK.", "Following the [documentation](https://www.tensorflow.org/versions/r1.0/api_guides/python/nn#Convolution), the correct output is 0+1+3+4=8. So it's working as expected.", "Which documentation is it? As far as I can tell, the difference is entirely due to difference in padding. In the case of 1x1 input, the padded input looks like so:\r\n```\r\n0 0 0\r\n0 1 0\r\n0 0 0\r\n```\r\nbut in case of 3x3 input, the padded input looks like so:\r\n```\r\n1 1 0 0\r\n1 1 0 0\r\n0 0 0 0\r\n0 0 0 0\r\n```\r\nwhile I expected it to look like so (and this is indeed how it is with `strides=1`):\r\n```\r\n0 0 0 0 \r\n0 1 1 0\r\n0 1 1 0\r\n0 0 0 0\r\n```\r\nAn easy way to see it is by adding `w = np.power(2, w)`\r\n\r\nThe closest I see documentation-wise is\r\n>padded_input is obtained by zero padding the input using an effective spatial filter shape of (spatial_filter_shape-1) * dilation_rate + 1 and output striding strides as described in the [comment here](https://www.tensorflow.org/api_docs/python/tf/nn/convolution).\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/convolution\r\n\r\nbut that link is broken", "For the reference, here is a complete code producing the expected result `432` (to me the difference with above is unexpected):\r\n```\r\nimport numpy as np\r\nfrom keras.layers import Conv2D, ZeroPadding2D\r\nfrom keras.models import Sequential\r\n\r\n\r\nw = np.arange(3*3*1*1).reshape(3, 3, 1, 1)\r\nw = np.power(2, w)\r\n\r\nmodel = Sequential()\r\nmodel.add(ZeroPadding2D(padding=(1, 1), input_shape=(2, 2, 1)))\r\nmodel.add(Conv2D(1, kernel_size=3, strides=2, padding='VALID', use_bias=False))\r\nmodel.set_weights([w])\r\nprint(model.predict(np.ones((1, 2, 2, 1))))\r\n```\r\n", "I posted the link to documentation of \"Convolution\".\r\nThe documentation says the padded input is:\r\n```\r\n1 1 0\r\n1 1 0\r\n0 0 0\r\n```", "Oh, yeah, see it now. Thank you. Indeed it is:\r\n\r\nhttps://www.tensorflow.org/api_guides/python/nn#Convolution\r\nhttps://www.tensorflow.org/api_guides/python/nn#Notes_on_SAME_Convolution_Padding\r\n\r\n```\r\nin_height, in_width = 2,2\r\nfilter_height, filter_width = 3,3\r\nstrides=(None,2,2)\r\nout_height = np.ceil(float(in_height) / float(strides[1]))\r\nout_width  = np.ceil(float(in_width) / float(strides[2]))\r\n\r\n#The total padding applied along the height and width is computed as:\r\n\r\nif (in_height % strides[1] == 0):\r\n  pad_along_height = max(filter_height - strides[1], 0)\r\nelse:\r\n  pad_along_height = max(filter_height - (in_height % strides[1]), 0)\r\nif (in_width % strides[2] == 0):\r\n  pad_along_width = max(filter_width - strides[2], 0)\r\nelse:\r\n  pad_along_width = max(filter_width - (in_width % strides[2]), 0)\r\n\r\nprint(pad_along_height, pad_along_width)\r\n  \r\n#Finally, the padding on the top, bottom, left and right are:\r\n\r\npad_top = pad_along_height // 2\r\npad_bottom = pad_along_height - pad_top\r\npad_left = pad_along_width // 2\r\npad_right = pad_along_width - pad_left\r\n\r\nprint(pad_top, pad_bottom, pad_left, pad_right)\r\n```\r\n\r\noutput:\r\n```\r\n1 1\r\n0 1 0 1\r\n```"]}, {"number": 17653, "title": "Tensorflow distributed training: CPU usage difference", "body": "Hi,\r\nGood day. I have a question that I hope someone could help. \r\nPlease forgive my grammar errors and missing words.\r\n\r\nI have tried to combine the Cifar10 training examples \r\nhttps://github.com/tensorflow/models/tree/master/tutorials/image/cifar10\r\nand  example in Distributed TensorFlow website:\r\nhttps://www.tensorflow.org/deploy/distributed\r\nto do the cifar10 distributed training. \r\nI am a little bit confused on the tf.train.MonitoredTrainingSession implementation. \r\nServers are ok to communicate with each other. \r\n\r\n### System information\r\n- Linux Ubuntu 16.04:\r\n- Python3\r\n- CPU\r\n### Describe the problem\r\nI used two servers, each of them are used as 1 PS and 1 Worker and no other programs. So total 2 PS and 2 Workers are trained in this experiment. What I expected is 2 Workers have the same CPU percentage usage when the training is going on. But I found that only one of the server ( with PS 1 and Worker 1) was running with 100% CPU usage, the other server( with PS 2 and Worker 2) only used 3% CPU. \r\n### Source code / logs\r\nThe commands I used are\r\npython3 cifar10_train.py --ps_hosts=Server IP1:2222,Server IP2:2222 --worker_hosts=Server IP1:2223,Server IP2:2000 --job_name=ps --task_index=0\r\n\r\npython3 cifar10_train.py --ps_hosts=Server IP1:2222,Server IP2:2222 --worker_hosts=Server IP1:2223,Server IP2:2000 --job_name=ps --task_index=1\r\n\r\npython3 cifar10_train.py --ps_hosts=Server IP1:2222,Server IP2:2222 --worker_hosts=Server IP1:2223,Server IP2:2000 --job_name=worker --task_index=0\r\n\r\npython3 cifar10_train.py --ps_hosts=Server IP1:2222,Server IP2:2222 --worker_hosts=Server IP1:2223,Server IP2:2000 --job_name=worker --task_index=1\r\n\r\n The f.train.MonitoredTrainingSession is set as:\r\n      with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                               is_chief=(FLAGS.task_index == 0),\r\n                                               checkpoint_dir=\"/tmp/train_logs\",\r\n                                               hooks=hooks) as mon_sess:\r\n          while not mon_sess.should_stop():\r\n            mon_sess.run(train_op)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: Yes\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: Virtualenv\r\nTensorFlow version: 1.6.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "@mrry Can you help here?", "@Danielyijun this may be better question for stackoverflow as the situation you are describing is a common situation which is not caused by a bug. IE, it happens when your model places bulk of parameters on first ps shard, and nothing being placed on remaining shards. The solution is to make sure you have enough variables and they are roughly equally balanced. \r\n"]}, {"number": 17652, "title": "Add python built-in types support for `tf.as_dtype`", "body": "This fix tries to address the issue raised in #17641 where it was not possible to use `tf.as_dtype(float)` the same way as numpy `np.dtype(float)`.\r\nThis fix adds the built-in types support for `tf.as_dtype`, so that it is possible to specify:\r\n```\r\ndtypes.as_dtype(float)   # dtypes.float64\r\ndtypes.as_dtype(int)     # dtypes.int64\r\ndtypes.as_dtype(complex) # dtypes.complex128\r\ndtypes.as_dtype(bool)    # dtypes.bool\r\n```\r\n\r\nThis fix fixes #17641.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks @mrry. I take a second and noticed that there are some discrepancies between np and tf:\r\n```\r\n>>> import numpy as np\r\n>>> np.dtype('int')\r\ndtype('int64')\r\n>>> np.dtype('float')\r\ndtype('float64')\r\n>>> np.dtype('complex')\r\ndtype('complex128')\r\n>>> np.dtype('bool')\r\ndtype('bool')\r\n```\r\nThe above matches the internal data types in python to C.\r\n\r\nFor tf, the `canonical alias` only defines `half`, `float`, and `double` (and `bool`):\r\nhttps://github.com/tensorflow/tensorflow/blob/d57f0213bff0c676b06f4b3f0842ac282738c254/tensorflow/python/framework/dtypes.py#L501-L511\r\n\r\nThere are no canonical definitions pointing to `int` or `complex`. Also, the mappings are:\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.as_dtype('half')\r\ntf.float16\r\n>>> tf.as_dtype('float')\r\ntf.float32\r\n>>> tf.as_dtype('double')\r\ntf.float64\r\n>>> tf.as_dtype('bool')\r\ntf.bool\r\n```", "@mrry Given the discrepancies mentioned above, I think it might make sense to support \r\n```\r\ndtypes.as_dtype(float) => float32\r\ndtypes.as_dtype(bool) => bool\r\n```\r\n\r\nFor `int` and `complex` maybe we should defer to not add the support for now?", "Yep, that sounds fine to me!", "Thanks @mrry. The PR has been updated. Please take a look."]}, {"number": 17651, "title": "Branch 188738133", "body": "", "comments": []}, {"number": 17650, "title": "Allow tf.estimator.train_and_evaluate evaluation frequency in steps", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**:  `v1.6.0-0-gd2e24b6039 1.6.0`\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: GTX 1080/1080Ti, P100\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nWe're using `tf.estimator.train_and_evaluate` for running our training, but we're running into an issue with getting it to run evaluation at the correct frequency. Because the training input pipeline is fully reset after evaluation, we're attempting to follow the docs recommendation of running evaluation after an epoch or two. \r\n\r\nThis is a problem for us because we can only figure out how to set the evaluation frequency using `tf.estimator.EvalSpec.throttle_secs`, which runs evaluation every `throttle_secs` seconds (since our evaluation takes less time than `throttle_secs`).  We run on a few different hardware platforms and configurations that all alter the training speed, so the only way to ensure that we perform evaluation after finishing an epoch is to calculate a value for `throttle_secs` that incorporates that training's training speed. This is obviously suboptimal compared to setting the evaluation frequency in steps rather than seconds.\r\n\r\nHere are the approaches to solving this problem that I've been able to find after a little poking around:\r\n\r\n1. Prevent evaluations triggered by `throttle_secs` passing from saving a new checkpoint, and only run if there is a new checkpoint. This lets the user specify `tf.estimator.RunConfig.save_checkpoints_steps` to set the evaluation frequency. This is actually how I thought `throttle_secs` worked based on my reading of the documentation\r\n2. Allow the user to set `throttle_steps` as a part of the `EvalSpec`. This value would could be used by the `SecondOrStepTimer` to run the evaluation based on how many steps have elapsed instead of seconds.\r\n\r\n\r\nI'd be willing to submit a PR with either fix, but I'm not sure which one would be correct/best, so I'd appreciate any feedback or alternate solutions :smile: ", "comments": ["+1 for this, as it would make single-machine training runs cleanly repeatable when using early stopping with evaluation results, without the need to implement your own train/eval loop. The now-deprecated Experiment API accomplished this for me before. Thanks!", "+1 for this as well. It would be good to have true feature-parity with the Experiment API.", "I'd really like to submit a PR for this since people are clearly interested, but I would still like clarification from the team on which solution should be used. @rohan100jain, any chance you could look at this? ", "I think solution 1 will not work cause the first checkpoint is saved at startup. So as the checkpoint is ready the first eval will be executed at throttle secs and `tf.estimator.RunConfig.save_checkpoints_steps` will not  be respected. ", "/cc @xiejw", "Also when [running in \"local\"](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/training.py#L631) seems to not respect `start_delay_secs`. Is it another bug or this behavior is not documented?", "/cc @ispirmustafa", "@bhack, I think you're correct. I know I thought of a related edge case at one point with 1, but I've forgotten it. I'll take a look at implementing 2 some time in the next week and filing a PR", "+1 for this as per @jgung comment", "+1 I used to do that with tf.contrib.learn.Experiment, with the min_eval_frequency parameter. Now missing this in the new api", "+1", "@skycoop I think I have found a workaround to be able to evaluate after certain number of _epochs_. \r\n\r\nIf you return a _finite_ `tf.data` iterator with certain number of repeated epochs such as:\r\n`dataset= dataset.shuffle(buffer_size=1000).repeat(num_epochs).batch(batch_size)`\r\nand set the `max_steps` in the `TrainSpec` as:\r\n`train_spec = tf.estimator.TrainSpec(train_input_fn, max_steps=max_train_steps)`\r\nwith `max_train_steps` being large enough to allow multiple training epochs, \r\nthen `tf.estimator.train_and_evaluate` will evaluate after each `num_epochs` and return to training after evaluation until reaching `max_train_steps`. This will set a fixed frequency of evaluation based on the number of epochs (and not the seconds passed). \r\n\r\nAnyway, +1 for having a `throttle_steps` in `EvalSpec`.", "+1, it would really help us control how we are monitoring the model flow & progress.", "From https://www.tensorflow.org/versions/master/api_docs/python/tf/estimator/EvalSpec:\r\n\r\n> throttle_secs: Int. Do not re-evaluate unless the last evaluation was started at least this many seconds ago. Of course, evaluation does not occur if no new checkpoints are available, hence, this is the minimum.\r\n\r\nSo I think it is possible to control evaluation frequency using the parameter `save_checkpoints_steps` of `RunConfig`: a new checkpoint is generated every `save_checkpoints_steps`, and only then evaluation is executed (provided `throttle_secs` is sufficiently small).", "The behaviour is changed a little bit https://github.com/tensorflow/tensorflow/commit/3edb609926f2521c726737fc1efeae1572dc6581", "@allaisandrea  -- While this is largely the workaround I use in practice, I've also run into a validation constraint where `throttle_secs > 0` :sweat_smile: , which is generally not a problem but has proven annoying when trying to debug stuff that might take less than a second to evaluate (especially in cases where the timing differs across environments).\r\n\r\n@bhack I'm not sure I fully understand this, but -- is the idea now that there's going to be an eval kicked off after a checkpoint is saved? So you'd be able to control it with just `save_checkpoint_steps`?", "@zmjjmz now (at master) you can set throttle_secs = 0. It will evaluate every checkpoint.\r\n", "Any updates in this feature?", "I believe this has been dealt with in the version of train_and_evaluate that was in TF 1.10.", "hi @ankitvgupta how did the behavior change in TF 1.10. because in the eval_spec I don't see any new fields that could be set to use steps instead of seconds? \r\nThanks", "hi @ankitvgupta even I verified and could not find how this feature has been implemented. Could you verify if that is the case and hopefully tell us what needs to be done for it to get it working?", "@jdvylder @shoubhik Hey, sorry for the delay. Probably someone from Tensorflow should explain to be certain, but there was a major upgrade to the local training version of train_and_evaluate. My understanding is it now runs evaluation via a `tf.train.CheckpointSaverListener`. As such, the evaluation frequency is determined by the saving frequency. You can see the new functionality here: https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/estimator/training.py#L667\r\n\r\nI believe the training input pipeline no longer resets per evaluation as a result, too.", "yes, what @ankitvgupta describes is accurate. ", "+1 for having a throttle_steps in EvalSpec", "Hi @skycoop !\r\nWe see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 17649, "title": "Fix broken links in tutorial/layers", "body": "This PR is to fix:\r\n- As you can see in [tutorial/layers](https://www.tensorflow.org/tutorials/layers), the below link of \"Training and Evaluating the CNN MNIST Classifier\" is actually broken, which should be updated as \"https://www.tensorflow.org/tutorials/layers#training_and_evaluating_the_cnn_mnist_classifier\"\r\n> If you're already experienced with CNNs and TensorFlow Estimators, and find the above code intuitive, you may want to skim these sections or just skip ahead to **\"Training and Evaluating the CNN MNIST Classifier\"**.\r\n\r\n- The below two links are broken due to new lines within the link.\r\n> Note: For a more in-depth look at configuring training ops for Estimator model functions, see @{$get_started/custom_estimators#defining-the-training-op-for-the-model$\"Defining the training op for the model\"} in the @{$get_started/custom_estimators$\"Creating Estimations in tf.estimator\"} tutorial.\r\n\r\n- Delete the alias of the title which is format as {#...} right after the title name since it's not required according to the maner of the most *.md in the doc_src folder.", "comments": ["@frankchn thanks for comments. Actually I've checked there exist no external reference to these anchors, and it's acutually can link to these title by default thru #{with \u2018_\u2019 separated titles}. I'll add them back soon if you prefer to keep them anyway."]}, {"number": 17648, "title": "when i add hard_example_miner to faster_rcnn config,i face resource exhaust problem", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 17647, "title": "How can I gdb tfcompile by using bazel build?", "body": "tfcompile is a binary file to compile an inference graph into executable code for tensorflow XLA which created by bazel build.\r\nIf I want to gdb tfcompile file on ubuntu 16.04,  it will return message \r\n\r\n    Reading symbols from /PWD/tfcompile...(no debugging symbols found)...done.\r\n\r\nIt seem that should make bazel compile with something flag like -g?\r\nI have tried to use \r\n   \r\n    bazel build XXX --compilation_mode=dbg\r\n\r\nbut there seems not change anything.\r\n\r\nHow can I resolve this question?\r\n\r\nby the way , I'm working on Ubuntu 16.04, bazel version 0.8.1 tensorflow r1.0\r\n\r\nThanks for any help!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@xcv9685\r\nIs this problem solved? I have met the similar issue,.though I have compiled the tensorflow/examples/label_image as follows\r\n\r\n`bazel build --compilation_mode=dbg //tensorflow/examples/label_image/..\r\n`\r\n\r\nbut the the final produced binary label_image has no debug symbols .\r\n\r\n> Reading symbols from ./label_image...(no debugging symbols found)...done.\r\n> (gdb) b main\r\n> Breakpoint 1 at 0x8b39ab\r\n> (gdb) r\r\n> Starting program: /home/neo/.cache/bazel/_bazel_neo/b79e78d41078616f7b591dd11d552dc0/execroot/org_tensorflow/bazel-out/k8-dbg/bin/tensorflow/examples/label_image/label_image\r\n", "Yes, it is solved.\nI finally can gdb tfcomile.\nIf my memory serves well, you should also add --strip=never flag in order\nto avoid strip debugging information.\nIf these still can't work, please tell me again.\nThanks.\n\n2018-07-07 10:44 GMT+08:00 neohemstar <notifications@github.com>:\n\n> @xcv9685 <https://github.com/xcv9685>\n> Is this problem solved? I have met the similar issue,.though I have\n> compiled the tensorflow/examples/label_image as follows\n>\n> bazel build --compilation_mode=dbg //tensorflow/examples/label_image/..\n>\n> but the the final produced binary label_image has no debug symbols .\n>\n> Reading symbols from ./label_image...(no debugging symbols found)...done.\n> (gdb) b main\n> Breakpoint 1 at 0x8b39ab\n> (gdb) r\n> Starting program: /home/neo/.cache/bazel/_bazel_neo/\n> b79e78d41078616f7b591dd11d552dc0/execroot/org_tensorflow/\n> bazel-out/k8-dbg/bin/tensorflow/examples/label_image/label_image\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17647#issuecomment-403183119>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AeOIuxmh6yDFBB8l6NYC9ZnmHNKiRf90ks5uECB4gaJpZM4SmyPQ>\n> .\n>\n", "@xcv9685 \r\nwith --strip=never flag ,the  binary was build with debug symbol.\r\nappreciate for your help"]}, {"number": 17646, "title": "Added data_format to layers", "body": "If you train a model in `NCHW` format but then do inference in `NHWC` the results are different as the reshape sets up the resulting array differently.", "comments": ["@joeyearsley Can you re-create the PR to merge into `master` rather than branch `r1.6`? All development happens in `master` and the release branches are for mainly bug patches."]}, {"number": 17645, "title": "r1.6 broken link to nasm package in workspace.bzl", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 2.7/3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0/7\r\n- **GPU model and memory**: NVidia GeForce 1060 6GB\r\n- **Exact command to reproduce**: \r\n\r\n```\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \r\n```\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI had an issue when trying to build due to the broken nasm link in `tensorflow/workspace.bzl`.\r\n\r\nI was trying to build tensorflow from source (Ubuntu 16.04, x64). I checked out r1.6.  I am following all of the instructions in the install guide. Everything  seems to work fine. Once bazel is installed and I run the build command and I receive a error \r\n\r\n```\r\nno such package '@nasm//': java.io.IOException: Error downloading [https://mirror.bazel.build/www.nasm.us/pub/nasm/releasebuilds/2.12.02/nasm-2.12.02.tar.bz2, http://pkgs.fedoraproject.org/repo/pkgs/nasm/nasm-2.12.02.tar.bz2/d15843c3fb7db39af80571ee27ec6fad/nasm-2.12.02.tar.bz2]\r\n```\r\n\r\nThe build is unable to continue at this point. I've run it multiple  times, all with the same error. I've found that this is because there is  only one working mirror link for the nasm package inside of the bazel  config.\r\n\r\nI can confirm that link [http://pkgs.fedoraproject.org/repo/pkgs/nasm/nasm-2.12.02.tar.bz2/d15843c3fb7db39af80571ee27ec6fad/nasm-2.12.02.tar.bz2](http://pkgs.fedoraproject.org/repo/pkgs/nasm/nasm-2.12.02.tar.bz2/d15843c3fb7db39af80571ee27ec6fad/nasm-2.12.02.tar.bz2) is dead (403 response). Adding another mirror such as \"[http://www.nasm.us/pub/nasm/releasebuilds/2.12.02/nasm-2.12.02.tar.bz2](http://www.nasm.us/pub/nasm/releasebuilds/2.12.02/nasm-2.12.02.tar.bz2)\"  to `tensorflow/workspace.bzl` allowed the build to continue This is all done on the r1.6 branch\r\n\r\nNote this is a duplicate of #16862 \r\n\r\nIn order to repro just try to build tensorflow from source using the instructions at [https://www.tensorflow.org/install/install_sources](https://www.tensorflow.org/install/install_sources) while working on the r1.6 branch. \r\n\r\n--\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Closing this since it's a duplicate of #16862 , let's continue discussion there."]}, {"number": 17644, "title": "The value changes incorrectly after assignment using tf.assign", "body": "**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\nTensorFlow installed from (source or binary): Binary\r\nTensorFlow version (use command below): 1.4.0\r\nPython version: Python 2.7.12\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: 8.0/6.0\r\nGPU model and memory: GTX 960, 2G\r\nExact command to reproduce: N/A\r\n\r\n**ISSUE**\r\ndata = load(\"new.mat\")\r\nprint data\r\ntf.assign(old, data)\r\nprint old\r\n\r\nBut\uff1a    old!=data\r\nEx. 0.081345705\u2014\u2014>0.081345707\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I guess this is a float precision problem.\r\nBecause you name the file \"new.mat\", I suppose it is dumped by MATLAB and is of 64-bit precision.\r\nWhile you load it, you may convert it to 32-bit precision, thus you get an approximation.\r\n\r\nActually, you can verify this with the C programming language:\r\n\r\n```\r\n#include <iostream>\r\n#include <iomanip>\r\nusing namespace std;\r\n\r\nint main() {\r\ndouble x = 0.081345705;\r\nfloat y = (float) x;\r\ncout << setprecision(8) << x << \" \" << y << endl;\r\n}\r\n```\r\nYou will see that `x=0.081345705` and `y=0.081345707`.\r\n"]}, {"number": 17643, "title": "Failed to import the TensorFlow module", "body": "I tried to install TensorFlow (CPU only) on windows 10 and python 3.6 (64 bit) and after write\r\nimport tensorflow as tf\r\n{ hello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello)) }\r\non visual studio 2017 I got this\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"E:\\PythonApplication3\\PythonApplication3\\PythonApplication3.py\", line 228, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n \r\n ,I used this script \r\nhttps://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c  to know the proplem\r\nand I got this\r\nERROR: Failed to import the TensorFlow module.\r\n=============================================\r\nSince TensorFlow 1.4, the self-check has been integrated with TensorFlow itself,\r\nand any missing DLLs will be reported when you execute the `import tensorflow`\r\nstatement. The error messages printed below refer to TensorFlow 1.3 and earlier,\r\nand are inaccurate for later versions of TensorFlow.\r\n\r\n- Python version is 3.6.\r\n\r\n- TensorFlow is installed at: C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\r\n\r\n- Could not load 'cudart64_80.dll'. The GPU version of TensorFlow\r\n  requires that this DLL be installed in a directory that is named in\r\n  your %PATH% environment variable. Download and install CUDA 8.0 from\r\n  this URL: https://developer.nvidia.com/cuda-toolkit\r\n\r\n- Could not load 'nvcuda.dll'. The GPU version of TensorFlow requires that\r\n  this DLL be installed in a directory that is named in your %PATH%\r\n  environment variable. Typically it is installed in 'C:\\Windows\\System32'.\r\n  If it is not present, ensure that you have a CUDA-capable GPU with the\r\n  correct driver installed.\r\n\r\n- Could not find cuDNN 6.\r\n\r\n  The GPU version of TensorFlow requires that the correct cuDNN DLL be installed\r\n  in a directory that is named in your %PATH% environment variable. Note that\r\n  installing cuDNN is a separate step from installing CUDA, and it is often\r\n  found in a different directory from the CUDA DLLs. The correct version of\r\n  cuDNN depends on your version of TensorFlow:\r\n\r\n  * TensorFlow 1.2.1 or earlier requires cuDNN 5.1. ('cudnn64_5.dll')\r\n  * TensorFlow 1.3 or later requires cuDNN 6. ('cudnn64_6.dll')\r\n\r\n  You may install the necessary DLL by downloading cuDNN from this URL:\r\n  https://developer.nvidia.com/cudnn\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17642, "title": "Fix unaligned access on Android 4 or 5", "body": "This issue is caused by a breaking change in NDK clang (r13+). In those\r\nrecent NDKs, clang assumes NEON vectors are aligned to 128-bit boundary,\r\nand thus generates 128-bit address hints. However, on old Android\r\nversions, heap-allocated memory are not always aligned to 128-bit.\r\n\r\nAfter consulting NDK maintainers [(here)](https://github.com/android-ndk/ndk/issues/640), the breaking change of compiler will not be fixed - so TensorFlow should fix it instead.\r\n\r\nP.S: it also seems to be responsible for #16317 .\r\n\r\nSecondly, it looks to me that existing code [(here)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/kernels/internal/optimized/neon_tensor_utils.cc#L42-L43) allocates more than it actually needs. The element count should not be multiplied by `sizeof(float32x4_t)` again. **Please help verify this part.**", "comments": ["@aselle Sorry to bother, but it's a bit difficult to predict the CI scheduling...", "It's very right!", "@frankchn @aselle Hi, it has been a month and I am not sure if this PR is forgotten...", "Is there any issue I could help?", "Nagging Reviewer @aselle: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "@scottcjt thank you for the fix. We have manually merged it."]}, {"number": 17641, "title": "Feature request: tf.as_dtype(float) should work just as tf.as_dtype('float')", "body": "In NumPy, `np.dtype(float)` works just the same as `np.dtype(\"float\")`.\r\n\r\nIn TensorFlow `tf.as_dtype(\"float\")` works but `tf.as_dtype(float)` crashes with `TypeError: Cannot convert value <class 'float'> to a TensorFlow DType.`.\r\n\r\nIs there a particular reason for this behaviour or was it just overlooked?\r\n\r\n(same error for other builtins such as `int` and `complex`)", "comments": ["I added  #17652 as an effort to support built-in types for `tf.as_dtype`.\r\n\r\nI think one potential issue is that python's `float` is implemented as C `double`, so there are some discrepancies here.\r\n\r\nNot sure if this is the way to go, but will let tensorflower's to make a call.", "Wow, you're one fast potato @yongtang! \ud83d\ude03\r\n\r\n>I think one potential issue is that python's float is implemented as C double, so there are some discrepancies here.\r\n\r\nYeah, and I guess it can be a little confusing to have `tf.as_dtype(\"float\")` be 32-bit while `tf.as_dtype(float)` would give doubles. I'd probably stick with tf.float32 [here](https://github.com/tensorflow/tensorflow/pull/17652/files#diff-85e323da48079f7831b7e9f6ec24cda2R655) for consistency, despite not matching NumPy, especially considering how the vast majority of TensorFlow users assume single precision floats (e.g. for GPU model training).\r\n\r\n`tf.as_dtype(\"double\") == tf.float64`, also."]}, {"number": 17640, "title": "Fix TensorBoard tutorial links to the wrong MNIST tutorial example code", "body": "This PR is to fix #17600.\r\n\r\nThe [TensorBoard guide](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard) contains example code that it says \"is a modification of the simple MNIST tutorial, in which we have added some summary ops.\" But the tutorial it links to, https://www.tensorflow.org/tutorials/layers, is very different from the example code it gives -- that tutorial uses the tf.layers module to create a CNN, while the example code manually defines a one-hidden-layer fully-connected network. ", "comments": []}, {"number": 17639, "title": "How can we define different rate for different layer?", "body": "Is there any API or operator supports to  define different rate for different layer like caffe?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17638, "title": "minor edit", "body": "based off my understanding of the source code. here's the stackoverflow post I made about my confusion:\r\nhttps://stackoverflow.com/questions/49145541/tensorflow-retrain-py-understanding-train-batch-size", "comments": ["Will this work? I just realized the \"by default\" was repetitive anyways."]}]