[{"number": 10711, "title": "Broken Link for CIFAR-10 tutorial", "body": "On the main page of Tensorflow tutorial for CIFAR-10 i.e.\r\n( [https://www.tensorflow.org/tutorials/deep_cnn#model_inputs](url) ), the links in the **[Code Organisation](https://www.tensorflow.org/tutorials/deep_cnn#code_organization)** section are ALL broken. \r\nTo be precise, the on clicking them - **Github** page says '_Page not found_'\r\n\r\nPlease remove this bug ASAP", "comments": ["They were all moved into another repo\r\n\r\nhttps://github.com/tensorflow/models/tree/master/tutorials/image/cifar10", "@wolffg, I've verified this. Could you take a look?", "This is now fixed."]}, {"number": 10710, "title": "Avoid upgrading tensorFlow dependencies when running windows tests. (\u2026", "body": "\u2026#10709)", "comments": []}, {"number": 10709, "title": "Avoid upgrading tensorFlow dependencies when running windows tests.", "body": "", "comments": []}, {"number": 10708, "title": "Building Java Tensorflow for GPU, from Source, Does not Work as Described.", "body": "First of all, I will not address this in Stackoverflow. This question belongs here. I tried going to Stackflow but nobody responded.\r\n\r\nI need to build tensorflow for Java from source with GPU support in Windows. How can I do this successfully? I need to build the JNI DLLs. Please advise!\r\n\r\nI get:\r\n```\r\nbazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\nERROR: C:/development/projects/tensorflow/tensorflow/java/BUILD:142:1: error loading package 'tensorflow/java/src/main/native': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 958\r\n                _create_cuda_repository(repository_ctx)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 846, in _create_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 656, in _get_cuda_config\r\n                _cudnn_install_basedir(repository_ctx)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 211, in _cudnn_install_basedir\r\n                auto_configure_fail(\"Cannot find cudnn install path....)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 128, in auto_configure_fail\r\n                fail(\"\\n%sAuto-Configuration Error:%s...))\r\n\r\nAuto-Configuration Error: Cannot find cudnn install path.\r\n and referenced by //tensorflow/java:libtensorflow_jni.so.\r\nERROR: Analysis of target '//tensorflow/java:tensorflow' failed; build aborted.\r\nINFO: Elapsed time: 0.464s`\r\n```\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows\r\n- **TensorFlow installed from (source or binary)**: r1.2\r\n- **TensorFlow version (use command below)**:r1.2\r\n- **Bazel version (if compiling from source)**: 0.5.0\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: Nvidia Quadro - K5200\r\n- **Exact command to reproduce**:bazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I think that  error states that cudnn install path. Might be you have to configure cuda path correctly. Check installation on tensor-flow website line by line for bazel and cuda libs.", "Yeah, but I do specify the CUDNN path using ./configure.", "Is something hardcoded in the build script? Remember I need the dll for Windows.", "Can you try to put into environment variable to have wide broader scope and try other options too.\r\nFrom the error log it seems wrong config via bazel", "What are the environment variables I need to set?", "Has anyone been able to successfully build the Java GPU libraries for Windows?\r\n\r\nRunning:\r\n\r\n`./configure`\r\n`bazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni`", "@asimshankar, do you have any useful suggestions?", "Can you try with:\r\n```\r\nbazel build --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\n```\r\n\r\nand let us know if that works out?\r\n", "Tried your suggestion and still the same error:\r\n\r\n\r\n`$ bazel build --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\nERROR: no such package '@local_config_cuda//crosstool': Traceback (most recent call last):\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 958\r\n                _create_cuda_repository(repository_ctx)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 846, in _create_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 656, in _get_cuda_config\r\n                _cudnn_install_basedir(repository_ctx)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 211, in _cudnn_install_basedir\r\n                auto_configure_fail(\"Cannot find cudnn install path....)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 128, in auto_configure_fail\r\n                fail(\"\\n%sAuto-Configuration Error:%s...))\r\n\r\nAuto-Configuration Error: Cannot find cudnn install path.\r\n.\r\nINFO: Elapsed time: 0.527s\r\n`", "Unfortunately, Windows support is highly experimental. Only cmake windows builds are unit tested. As is said on the web page. So what this means is that since you didn't get a response on StackOverflow and people have told you to make sure cudnn is properly getting detected. Probably nobody else has tried this yet, so you are on a trailblazing path.", "I am trying to wrap my ahead around why Windows support is highly experimental..... Isn't Windows a large installation base? Don't most of us have Nvidia cards?  Especially now that deep learning is becoming so popular, it's well worth the effort to make Windows/Java support stable. \r\n\r\nAnyway, I will solve this myself. In the end, I may just write my own deep learning framework for Windows and Java.", "I empathize that you are super frustrated, but as a developer, you must know that we don't have unlimited time to support everything ourself. You are likely one of the few people that have tried to setup Windows/Java/GPUs together. As such we really would appreciate if you solve this problem and contribute it back to the community. I suspect that effort will save you time over writing your own framework.", "Thank you. I hope I find the solution, and certainly would share it with the community. ", "Hi @nectario  @aselle  , I'm just wondering if you found a solution yet and you just forgot to mention here. Because I'm facing the same problem here. \r\nThanks, Harel.", "Sorry, I haven't spent the time required to find a solution. :-(", "Hi @nectario , \r\ntake a look a  the script they are using to build the cpu one. It should build both java and c interface\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/libtensorflow_cpu.sh\r\n\r\nmodify the script, change the line that calls configure cpu to configure gpu in the script.(the function is in another script). Later on you can call the script using bash to build.\r\n\r\nBefore you build, also change the path to the GPU libraries in these two scripts, which contains stuff called by the previous script\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/bazel/common_env.sh\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/bazel/bazel_test_lib.sh\r\n\r\nThen install the bazel and all other stuff needed for bazel, including some missing command tools maybe.\r\n\r\nThen build! \r\n", "Thank you @tcmxx ! This was really helpful. Btw, I am able to successfully build using cmake, but I have no idea where it puts the Java JNI libraries. Using cmake was a lot easier and I used VS Studio to build ALL targets. Do you know where it puts the Java dlls?", "Hello @nectario .\r\nGood to hear that cmake works! \r\nI dont know where the cmake build put the dlls but for bazel, it is under tensorflow/lib_package if everything is successful. \r\n\r\nIt is probably called libtensorflow_jni.so. You might want to look for it in the build folder...", "Thank @tcmxx. I am assuming cmake builds everything, hence, no need to use bazel, am I right? ", "@nectario  I have no idea. I was about to ask you how to build everything else using cmake. Never try it before...haha", "Here is a screenshot of my cmake settings:\r\n\r\nhttp://www.dropbox.com/s/wn3c8rdv9g0lk8n/cmake.jpg\r\nFirst I click configure, then I click generate. After generation finishes, I open the Visual Studio file it created and then build ALL. ", "Sorry had to update the link", "Just updated with the GPU settings", "@nectario I am not sure if this config has java binding. Looks like it is only for python maybe. I am not an expert of it.\r\nIf you still can not figure it out, probably it is worth trying the Bazel. ", "I can also send you the stuff i built. Probably you can just use it directly.", "I see. Sure, thanks! Send me what you built. I can only test it when I am back from vacation (in 2 weeks). But wow, amazing that you managed to build it!", "nice. i will send u the link tomorrow.\n\n2017\u5e748\u670817\u65e5 \u4e0b\u534811:51\uff0c\"nectario\" <notifications@github.com>\u5199\u9053\uff1a\n\n> I see. Sure, thanks! Send me what you built. I can only test it when I am\n> back from vacation (in 2 weeks). But wow, amazing that you managed to build\n> it!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10708#issuecomment-323189738>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGL4iOyay5-zq0lxh2Vp3zotcTVsYeoEks5sZKfQgaJpZM4N6MNq>\n> .\n>\n", "Thank you so much!\r\n", "@nectario https://drive.google.com/open?id=0By5_8Ik3iqjYbjBCYnJ1NmRQRUk", "@tcmxx Thank you! Will give it a shot when I am back!", "@tcmxx I finally got back from vacation and tried out your file! It actually worked! Nice. Thank you. Although I got the following message in the output. Seems that my Cuda capability is 3.5. Time for a new graphics card upgrade??\r\n\r\n**OUTPUT:**\r\n```\r\nHello from 1.3.0-rc2\r\n2017-08-31 20:40:39.667550: I C:\\tmp\\_bazel_tcmxx\\bfwkqzrk\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX\r\n2017-08-31 20:40:40.150913: I C:\\tmp\\_bazel_tcmxx\\bfwkqzrk\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:962] Found device 0 with properties: \r\nname: Quadro K5200 major: 3 minor: 5 memoryClockRate(GHz): 0.771\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.71GiB\r\n2017-08-31 20:40:40.150954: I **C:\\tmp\\_bazel_tcmxx\\bfwkqzrk\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1025] Ignoring visible gpu device (device: 0, name: Quadro K5200, pci bus id: 0000:83:00.0, compute capability: 3.5) with Cuda compute capability 3.5. The minimum required Cuda capability is 3.7.**\r\n\r\n```", "@tcmxx \r\nWhen I try to build it now it is much better but I get the following error now:\r\n`bazel build --config opt tensorflow/java:tensorflow tensorflow/java:libtensorflow_jni\r\n\r\n\u001b[32mINFO: \u001b[0mLoading complete.  Analyzing...\r\n\u001b[1A\u001b[K\u001b[35mWARNING: \u001b[0mC:/development/projects/tensorflow/tensorflow/core/BUILD:1634:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in C:/development/projects/tensorflow/tensorflow/tensorflow.bzl:911:30.\r\n\u001b[32mINFO: \u001b[0mFound 2 targets...\r\n\u001b[32mINFO: \u001b[0mBuilding...\r\n\u001b[1A\u001b[K\u001b[31m\u001b[1mERROR: \u001b[0mmissing input file '@local_jdk//:include/linux/jni_md.h'.\r\n\u001b[32m[6 / 449] \u001b[0mCompiling external/protobuf_archive/src/google/protobuf/util/internal/proto_writer.cc\r\n\u001b[1A\u001b[K\u001b[31m\u001b[1mERROR: \u001b[0mC:/users/nektarios/appdata/local/temp/_bazel_nektarios/ce5f8pzu/external/protobuf_archive/BUILD:93:1: C++ compilation of rule '@protobuf_archive//:protobuf_lite' failed: msvc_cl.bat failed: error executing command external/local_config_cc/wrapper/bin/msvc_cl.bat /DOS_WINDOWS=OS_WINDOWS /DCOMPILER_MSVC /DNOGDI /DNOMINMAX /DPRAGMA_SUPPORTED /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS ... (remaining 33 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 2.\r\n\u001b[31m\u001b[1mERROR: \u001b[0mC:/development/projects/tensorflow/tensorflow/java/src/main/native/BUILD:67:1: //tensorflow/java/src/main/native:copy_jni_md_h: missing input file '@local_jdk//:include/linux/jni_md.h'.\r\n\u001b[31m\u001b[1mERROR: \u001b[0mC:/development/projects/tensorflow/tensorflow/java/src/main/native/BUILD:67:1 1 input file(s) do not exist.\r\n\u001b[32mINFO: \u001b[0mElapsed time: 1.745s, Critical Path: 0.16s\r\n`\r\n\r\nIt seems it is trying to build Linux. How do I force to build windows only?", "@nectario How did you build it? Have you try to run https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/libtensorflow_cpu.sh in shell?\r\nI don remember that i had this error before..", "I went to the root directory where I downloaded the tensorflow code:\r\n(bazel is already installed)\r\n\r\nc:\\Development\\Projects\\tensorflow\r\n\r\nThere, I typed \"configure\" and specified all configuration parameters. For example, where my CUDA and CUDNN directories are etc.\r\n\r\nFinally I typed this command:\r\n\r\nbazel build --config opt tensorflow/java:tensorflow tensorflow/java:libtensorflow_jni\r\n\r\nBtw, this is what Google advises in this link: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/README.md", "I did not follow this instruction.\r\nInstead, go to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/libtensorflow_cpu.sh\r\n\r\nI think this is the shell script that they use to build windows CPU version .\r\nUse msys64 or any shell to run this script. It should work, or at least the errors will be easy to fix. \r\n\r\nThen modify this above script and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/bazel/common_env.sh\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/bazel/bazel_test_lib.sh\r\nto change it to GPU build\r\nthose files contains some config such as CUDA path. \r\n", "@tcmxx \r\nThank you. I am very close to building it now. Getting some other errors that seems compilation related...\r\n\r\n` ERROR: C:/development/projects/tensorflow/tensorflow/tools/proto_text/BUILD:31:1: Linking of rule '//tensorflow/tools/proto_text:gen_proto_text_functions' failed: msvc_cl.bat failed: error executing command\r\n  FAILED: Build did NOT complete successfully\r\n`", "@tcmxx \r\nThis is extremely frustrating... :-( Which Python and version are you using? Seems like some python script is failing...\r\n\r\n`ERROR: C:/tmp/_bazel_nektarios/ce5f8pzu/external/gif_archive/BUILD.bazel:8:1: C++ compilation of rule '@gif_archive//:gif' failed (Exit 1): msvc_cl.bat failed: error executing command`\r\n", "@nectario  my python is 3.5. Not sure what this error means. Seems like something to do with your Bazel environment? ...em...", "I think the issue is from the way I installed bazel (using choco). Downloading the bazel zip I see all these missing scripts...", "Good to hear! Good luck with the build then~", "@tcmxx \r\nThanks. Btw, what version of Bazel did you use to build?", "@nectario  0.5.3 I think", "@tcmxx \r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:104:1: output 'external/protobuf_archive/_objs/protobuf/external/protobuf_archive/src/google/protobuf/util/delimited_message_util.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:104:1: not all outputs were created or valid\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:104:1: output 'external/protobuf_archive/_objs/protobuf/external/protobuf_archive/src/google/protobuf/util/field_mask_util.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:104:1: output 'external/protobuf_archive/_objs/protobuf/external/protobuf_archive/src/google/protobuf/stubs/substitute.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:104:1: output 'external/protobuf_archive/_objs/protobuf/external/protobuf_archive/src/google/protobuf/util/internal/json_objectwriter.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:104:1: output 'external/protobuf_archive/_objs/protobuf/external/protobuf_archive/src/google/protobuf/descriptor.pb.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:104:1: output 'external/protobuf_archive/_objs/protobuf/external/protobuf_archive/src/google/protobuf/compiler/importer.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:104:1: output 'external/protobuf_archive/_objs/protobuf/external/protobuf_archive/src/google/protobuf/compiler/parser.o' was not created\r\n"]}, {"number": 10707, "title": "add support for android sdk version < 18", "body": "Alternatively, the trace calls could be removed completely or replaced by simple Log calls.", "comments": ["Can one of the admins verify this patch?", "I applied your proposed changes and added small comments to improve readability of the trace calls.", "Jenkins, test this please."]}, {"number": 10706, "title": "[Feature Request] can tfdbg support printing or writing into file the whole (complete) tensor ?", "body": "### Describe the problem\r\n\r\nI used tensorflow's new debugger named tfdbg to debug tensorflow's application, which I need to see some intermediate tensors, but, for example, I tried to debug `wide_n_deep_tutorial.py`, \r\nwhen I type command like \r\n`print_tensor linear/linear/native_country/native_country_weights/embedding_lookup_sparse:0`\r\n\r\nit gives me following lines:\r\n```\r\nTensor \"linear/linear/native_country/native_country_weights/embedding_lookup_sparse:0:DebugIdentity\":\r\n  dtype: float32\r\n  shape: (32561, 1)\r\n\r\narray([[-0.11688102],\r\n       [-0.11688102],\r\n       [-0.11688102],\r\n       ..., \r\n       [-0.11688102],\r\n       [-0.11688102],\r\n       [-0.11688102]], dtype=float32)\r\n```\r\n\r\nif I run redirect it to a local file, it also appear like above.\r\n\r\nSo, there is any method that can print / save the whole tensor ?\r\nIf not, can tensorflow add this feature ?\r\nthanks in advance.\r\n\r\n###  Source Code\r\n\r\nadd\r\n```\r\nhooks = None\r\n  if FLAGS.debug:\r\n      debug_hook = tf_debug.LocalCLIDebugHook(ui_type=FLAGS.ui_type)\r\n      debug_hook.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\r\n      hooks = [debug_hook]\r\n      m.fit(input_fn=lambda: input_fn(df_train),\r\n                steps=train_steps,\r\n                monitors=hooks)\r\n\r\n      results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\r\n      for key in sorted(results):\r\n        print(\"%s: %s\" % (key, results[key]))\r\n  else:\r\n      m.fit(input_fn=lambda: input_fn(df_train),\r\n            steps=train_steps)\r\n```\r\nafter `wide_n_deep_tutorial.py` 's `m = build_estimator(..)`\r\nand add some args in main function :\r\n```\r\nparser.add_argument(\r\n      \"--ui_type\",\r\n      type=str,\r\n      default=\"curses\",\r\n      help=\"Command-line user interface type (curses | readline)\")\r\n  parser.add_argument(\r\n      \"--debug\",\r\n      type=\"bool\",\r\n      nargs=\"?\",\r\n      const=True,\r\n      default=False,\r\n      help=\"Use debugger to track down bad values during training\")\r\n```\r\n\r\n", "comments": ["@caisq, could you address this feature request, please. Thanks!", "@whatbeg this is already supported. All you need to do is adding the `-a` option:\r\n\r\ntfdbg> pt -a <tensor_name>\r\n\r\nIf you don't want to type the whole command yourself, you can click the tensor to see the incomplete display, and hit Up Array key to navigate to the last command, which is an automatically entered `pt <tensor_name>` command. Then you can just append ` -a` and hit Enter.\r\n\r\n", "Also note that it can be a little slow if the tensor is large (like in your case). You can use numpy-type slicing in the command, e.g.,\r\n\r\ntfdbg> pt linear/linear/native_country/native_country_weights/embedding_lookup_sparse:0[500:1000]\r\n\r\nThis table contains a list of the commands and their option flags: https://www.tensorflow.org/programmers_guide/debugger#tfdbg_cli_frequently-used_commands", "@caisq thank you very much, I should check the programmers_guide in detail before asking. : )", "but how to write the output of pt -a <tensorname> to a FILE ?", "@yogi81, please use command:\r\n\r\ntfdbg> pt -a my_tensor > /tmp/my_file.txt"]}, {"number": 10705, "title": "undocumented change in variable scope from tf 1.0.1 to tf 1.1.0", "body": "**System Information**\r\n- **Custom code, a minimal reproducible example provided below**\r\n- **Linux Fedora 24 and Fedora 25**\r\n- **TensorFlow installed from binary using pip**\r\n- **TensorFlow version 1.0.1 and 1.1.0**\r\n- **CUDA 8.0/cuDNN 5.1**\r\n- **GeForce GTX 1080**\r\n\r\n**Problem**\r\nI'm trying to run some code that I wrote for tensorflow 1.0.1 on tensorflow 1.1.0.\r\nIt seems like tf.contrib.layers.fully_connected is showing different behaviour for 1.1.0 compared to 1.0.1. See below for a minimal reproducible example showing the difference.\r\n\r\n**Source code and logs**\r\nTensorflow 1.0.1, Fedora 24:\r\n```\r\nPython 2.7.13 (default, May 10 2017, 20:04:36) \r\n[GCC 6.3.1 20161221 (Red Hat 6.3.1-1)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.contrib.layers.fully_connected(inputs=tf.placeholder(shape=[None,3],dtype=tf.float32),num_outputs=3,reuse=True,scope='DNN')\r\n<tf.Tensor 'DNN/Relu:0' shape=(?, 3) dtype=float32>\r\n>>> tf.__version__\r\n'1.0.1'\r\n```\r\nTensorflow 1.1.0, Fedora 25:\r\n```\r\nPython 2.7.13 (default, May 10 2017, 20:04:28) \r\n[GCC 6.3.1 20161221 (Red Hat 6.3.1-1)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.contrib.layers.fully_connected(inputs=tf.placeholder(shape=[None,3],dtype=tf.float32),num_outputs=3,reuse=True,scope='DNN')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1433, in fully_connected\r\n    outputs = layer.apply(inputs)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 320, in apply\r\n    return self.__call__(inputs, **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 286, in __call__\r\n    self.build(input_shapes[0])\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/layers/core.py\", line 123, in build\r\n    trainable=True)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1049, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 948, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 349, in get_variable\r\n    validate_shape=validate_shape, use_resource=use_resource)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1389, in wrapped_custom_getter\r\n    *args, **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 275, in variable_getter\r\n    variable_getter=functools.partial(getter, **kwargs))\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 228, in _add_variable\r\n    trainable=trainable and self.trainable)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1334, in layer_variable_getter\r\n    return _model_variable_getter(getter, *args, **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1326, in _model_variable_getter\r\n    custom_getter=getter, use_resource=use_resource)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 262, in model_variable\r\n    use_resource=use_resource)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 217, in variable\r\n    use_resource=use_resource)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 341, in _true_getter\r\n    use_resource=use_resource)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 671, in _get_single_variable\r\n    \"VarScope?\" % name)\r\nValueError: Variable DNN/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n>>> tf.__version__\r\n'1.1.0'\r\n```\r\nLooking at the release notes for Tensorflow 1.1, there is no mention of change in behaviour of variable scope for tf.contrib.layers.fully_connected. But it seems like in 1.1 we have to create variables manually using tf.get_variable() before using tf.contrib.layers.fully_connected. Am I missing something?\r\n", "comments": ["Generally speaking, there is no guarantee of backward compatibility in the contrib area as it is considered \"in-flux\" and \"unsupported\". However, @martinwicke or @lukaszkaiser  might have some knowledge of this change.\r\n", "No need to create variable manually, sorry for the misunderstanding! The variable reuse mechanism had a bug in 1.0.1 which ignored reuse checking: you shouldn't set \"reuse=True\" if the variable hasn't been created before. Just set \"reuse=None\", I hope that works. I'm closing for now, please reopen if it doesn't work for you, we'll help. Sorry for the problem!", "Thanks for the quick reply @lukaszkaiser!", "@lukaszkaiser @aselle \r\nThanks for the replies, but if I set ```reuse=None```, I can't use these variables again with the same code.\r\nFor example, suppose I have a function that maps inputs to outputs of a certain fixed DNN:\r\n```\r\ndef build_dnn(input):\r\n    output = tf.contrib.layers.fully_connected(inputs=input, num_outputs=3,reuse=None,scope='DNN')\r\n    return output\r\n```\r\nThen in tensorflow 1.1.0, calling build_dnn twice will give me the following error:\r\n```\r\nValueError: Variable DNN/weights already exists, disallowed. Did you mean to set reuse=True in VarScope?\r\n```\r\n\r\nIf I use ```reuse=True``` for ```build_dnn```, I will get the error in my previous post.\r\nBack in Tensorflow 1.0.1, I was able to just set ```reuse=True``` in ```build_dnn``` and call it as many times as I want. How can I achieve something similar in Tensorflow 1.1.0?", "Just add the reuse parameter to your build_dnn function, and call it with \"None\" first time and with \"True\" the other one.", "A note to explain this behavior: it was introduced due to a bug made when moving tf.contrib.layers to use the same code as tf.layers. The behavior now is the same as before the bug (in earlier versions of TF), and it is now the same as tf.layers. So ignoring reuse in tf.contrib.layers was really a contrib bug.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 10704, "title": "Improve docs for `parallel_stack`", "body": "Partly solves #10036\r\nMoves #10593 to `master`", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10703, "title": "Bazel bring up for ROCm", "body": "Hi,\r\nI am trying to add new backend to tensorflow. As a first step, I started changing bazel files around [(Commit here)](https://github.com/ROCmSoftwarePlatform/tensorflow/commit/b75ea3f499a5f63f2580066ae132c93e2b03d0ad). When I enable XLA + ROCM during configure, and run `bazel build -s --config=opt --config=rocm //tensorflow/tools/pip_package:build_pip_package `, I am getting the following error:\r\n```\r\nERROR: no such package '@local_config_rocm//': error loading package 'external': The repository named 'local_config_rocm' could not be resolved.\r\nINFO: Elapsed time: 0.227s\r\n```\r\n\r\nIt would be great if someone can parse the commit mentioned and suggest changes. \r\nThank you!", "comments": ["Could you try to make the minimal change that exhibits your year. Unfortunately, we have many issues and your commit is over 1296 lines.", "Hi,\r\nThank you response. You can ignore the LICENSE and CROSSTOOL_HCC.bzl. \r\nI guess the point of interest will be the rocm_configure.bzl file. https://github.com/ROCmSoftwarePlatform/tensorflow/blob/b75ea3f499a5f63f2580066ae132c93e2b03d0ad/third_party/gpus/rocm_configure.bzl\r\nIf you are familiar with bazel infrastructure, can you guide me to add AMDGPU support to bazel?", "I am unfortunately far from a bazel expert, but @jart may have some suggestions.", "I'm noticing you added this code:\r\n\r\n```py\r\n# Macros for building CUDA code.\r\ndef if_rocm(if_true, if_false = []):\r\n    return select({\r\n        \"@local_config_rocm//rocm:using_hcc\": if_true,\r\n        \"//conditions:default\": if_false\r\n    })\r\n```\r\n\r\nWhat is `@local_config_rocm`? In order to have a label like that, you need to add something like the following to your `tensorflow/workspace.bzl` file:\r\n\r\n```py\r\nnative.new_http_archive(\r\n    name = \"local_config_rocm\",\r\n   # ...\r\n```", "Huh. Didn't know that. I'll add `local_config_rocm` to `tensorflow/workspace.bzl` and see how it goes.\r\nThank you for the response.", "Feel free to ping this bug again if you get stuck on Bazel. If you're writing a feature for TensorFlow, then I'm happy to support you.", "Hi,\r\nI pushed my [code](https://github.com/ROCmSoftwarePlatform/tensorflow/tree/06202017-rocblas-se-v2). I need help stitching bazel around it so that the host compiler can touch them and link against rocm libraries. ", "Have you taken a look into [cuda_configure.bzl](https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda_configure.bzl). We've got a lot of code for configuring cuda in our build process. I'm not familiar with ROCm but if it's AMD's version of CUDA, we'd love to support it, but I must warn you that it would most likely be a highly nontrivial undertaking.", "Hi @jart ,\r\nWe are focusing on supporting XLA first. We want to move away from using HCC (device code compiler for ROCm) and use just LLVM, libraries and runtime to run TF code. The file `cuda_configure.bzl` seems have `nvcc` specific functions, but for rocm support we want just host compiler (g++/clang++) build functions. Will this make the problem trivial?\r\nPS: Can you re-open the issue?", "Hi @jart,\r\nI added bazel files for rocm+xla [here](https://github.com/tensorflow/tensorflow/commit/33fdacaf31f9a9ee2628dc0de36c558cdd607e31) I am getting the following error:\r\n```\r\nbazel build --config=opt --config=rocm //tensorflow/tools/pip_package:build_pip_package \r\nERROR: /home/aditya/rocm/tensorflow/tensorflow/core/platform/default/build_config/BUILD:31:1: error loading package 'tensorflow/stream_executor': Extension file not found. Unable to load package for '@local_config_rocm//rocm:build_defs.bzl': BUILD file not found on package path and referenced by '//tensorflow/core/platform/default/build_config:stream_executor'.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\nINFO: Elapsed time: 1.676s\r\n\r\n```\r\n", "@jart, this does seem like a Bazel issue; PTAL when you have a moment.", "@adityaatluri \r\nI found [a suspicious line](https://github.com/tensorflow/tensorflow/blob/33fdacaf31f9a9ee2628dc0de36c558cdd607e31/third_party/gpus/rocm_configure.bzl#L546) of the BUILD file not found error occurs.\r\nI think changing `cuda:BUILD` to `rocm:BUILD` could resolve the error.\r\n\r\nI would like to support enabling ROCm backend for XLA.\r\nIt would be better separate forked tensorflow repo instead of dangling Github tree. \r\nCould you apply your changes to your forked tensorflow repository?", "Hi @chanil1218 \r\nThank you for the pointer.\r\n\r\nYou are most welcome in enabling ROCm XLA backend. We are actually waiting for CLA approval from our legal team. We would love to have bazel stuff figured out before implementing actual code.\r\n", "Hi,\r\nI started fresh with the code, as plan is not to use AMD compiler (HCC). Here it is:\r\n\r\nCode:\r\nhttps://github.com/ROCmSoftwarePlatform/tensorflow/tree/rocm-v1\r\n\r\nThe build instructions are here:\r\nhttps://github.com/ROCmSoftwarePlatform/tensorflow/blob/rocm-v1/ROCM.md\r\n\r\nI am getting the following error:\r\n```shell\r\n$ bazel build --config=opt --config=rocm //tensorflow/tools/pip_package:build_pip_package\r\n ERROR: no such package '@local_config_rocm//': Traceback (most recent call last):\r\n        File \"/home/aditya/tensorflow/third_party/gpus/rocm_configure.bzl\", line 555\r\n                _create_local_rocm_repository(repository_ctx)\r\n        File \"/home/aditya/tensorflow/third_party/gpus/rocm_configure.bzl\", line 506, in _create_local_rocm_repository\r\n                _tpl(repository_ctx, \"rocm:build_defs.b...\", ...)})\r\n        File \"/home/aditya/tensorflow/third_party/gpus/rocm_configure.bzl\", line 238, in _tpl\r\n                repository_ctx.template(out, Label((\"//third_party/gpus/%s...)), ...)\r\n Unable to load package for //third_party/gpus/rocm:build_defs.bzl.tpl: not found.\r\n INFO: Elapsed time: 0.301s FAILED: Build did NOT complete successfully (0 packages loaded) \r\n```\r\n\r\n`build_defs.bzl.tpl` is present here: https://github.com/ROCmSoftwarePlatform/tensorflow/blob/rocm-v1/third_party/gpus/rocm/build_defs.bzl.tpl\r\nCC: @jart ", "@cy89 can you help me resolve this issue? Thanks!", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@adityaatluri  although this issue appears stale, I thought I'd let you know the https://github.com/ROCmSoftwarePlatform/tensorflow links are dead ", "@Mandrewoid I am guessing this is the new link https://github.com/ROCmSoftwarePlatform/hiptensorflow", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 10702, "title": "Merge pull request #1 from tensorflow/master", "body": "Updated on 2017/6/14", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Please don't spam using meaningless PR, thanks.", "Seems to be created in error."]}, {"number": 10701, "title": "Try both python and python3", "body": "Used `configure` script for reference\r\nReopens mistakenly closed #10525", "comments": ["Can one of the admins verify this patch?", "skipping tests since this is untested."]}, {"number": 10700, "title": "Allow variable sized tensor for RDMA transport", "body": "This PR fixes https://github.com/tensorflow/tensorflow/issues/10699.\r\n\r\nQ: Should we make aggressive buffer extension?", "comments": ["Can one of the admins verify this patch?", "The code is fine. The question is whether we should allow buffers to grow infinitely. Ideally, a buffer should shrink if the current size is unnecessarily large. But I could not think a case where a certain tensor size is ephemeral, i.e.  one will likely see the tensor of the same size in the future training steps.  ", "@michaelisard what do you think?", "Jenkins, test this please.", "@poxvoculi if you have suggestions.", "We have seen models where there is a very large number of different tensor sizes that don't necessarily repeat.  If there's only one buffer and it is allocated from CPU RAM, it probably doesn't matter.", "There is one buffer per tensor, allocated in CPU memory. \r\n\r\nThe original assumption was that tensor sizes (except string tensors) did not change over the course of training. It was valid for the early models, e.g. InceptionV3, VGG16, etc. As new models appear, the assumption is not valid anymore. I think we should let buffer sizes vary. \r\n\r\nWe could have some kind of policy to reclaim the memory if the tensor sizes have been smaller than the buffer size for some period of time. Note the buffers are pinned and registered for RDMA. Frequent allocation and registration can slow down things. The patch is good for now due to its simplicity.", "Use of verbs/rdma is optional.  If the model won't run with this constraint, user can just drop back to gRPC.  I'm ok with this.  @drpngx do you want me to review, or wait for Michael?", "@poxvoculi I'm not sure how much @michaelisard knows about this, maybe you have the most context on this one.", "Thanks!"]}, {"number": 10699, "title": "RDMA transport should support variable sized tensor", "body": "RDMA transport disabled variable size tensor https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L738 and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L759.\r\n\r\nHowever, in the embedding lookup model, the tensor has variable size. It's a common model parallelism case.", "comments": ["@junshi15 @anfeng  Any comments?", "Current design only allows string tensors to change size, when a larger string tensor is requested, the buffer size increases. To allow other tensor to increase size (smaller tensors are fine), we need to remove this check https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L738. \r\n\r\nNote there is no mechanism to shrink the buffer size, so in the end, the buffer is as large as the largest tensor seen so far. If the large tensor tensor is rare, we probably should shrink the buffer to save the memory usage. But maybe this is not an issue in most cases.", "@junshi15 Both L738 and L759 should be removed. The common usage pattern is still unknown, so don't know what is the best policy. Maybe we can remove the checks and first document this potential memory wastage as a warning?", "yes, both changes are needed. I am fine with changing \"assert\" to \"LOG(WARNING)\"."]}, {"number": 10698, "title": "[OpenCL] Fixes grpc test failure for SYCL devices (#87)", "body": "The test constructs a graph containing an IdentityOp on strings,\r\nwhich doesn't exist on CUDA and SYCL devices. The test expects to fail\r\nfor CUDA devices, so add the same expectation for SYCL devices.", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "@jwlawson Could you sign the Google CLA?\r\nIf you already signed it, it is possible that the email you used with the commit is not the one you used to sign the CLA. Is it possible to update the commit?", "@tensorflow-jenkins test this please", "Thanks for the fix, @lukeiwanski . ", "@lukeiwanski can you sign the CLA please?", "I've resubmitted this pull request myself (see #10758). Github was changing my email in the commit when Luke pulled it into his repo, causing the CLA mess.", "Closing!"]}, {"number": 10697, "title": "how can combining cnn with convlutional lstm", "body": "Hi,\r\n\r\ni want to process a image in cnn+convolutinal_lstm , in the connection point  between to network , shape(None,100,100,64) for cnn the should insert to convolution_lstm network ,\r\nbut not  match output cnn with input convlutinal_lstm.\r\nis any example for this combination?\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nIf you think after doing that, it is a bug, submit your reproducible test case. Thanks!"]}, {"number": 10696, "title": "Add input interfaces required by the Java Ops API.", "body": "Those interfaces are at the base of the upcoming Java Ops API, please consult https://github.com/tensorflow/tensorflow/issues/7149 for more details.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 10695, "title": "import Tensorflow not working", "body": "I get following error when installing under anaconda (python 2.7) environment.\r\n\r\n```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/quintendewilde/anaconda2/envs/TensorFlow/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Users/quintendewilde/anaconda2/envs/TensorFlow/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/quintendewilde/anaconda2/envs/TensorFlow/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/quintendewilde/anaconda2/envs/TensorFlow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/Users/quintendewilde/anaconda2/envs/TensorFlow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: dlopen(/Users/quintendewilde/anaconda2/envs/TensorFlow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /Users/quintendewilde/anaconda2/envs/TensorFlow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n  Reason: image not found\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n\r\n```", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "Looks like you are using a MAC... make sure you read this.\r\nhttps://www.tensorflow.org/install/install_mac\r\n\r\nGoogling \"Error importing tensorflow.  Unless you are using bazel,\" yields these results:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/1402\r\nhttps://github.com/tensorflow/tensorflow/issues/6726\r\nhttps://stackoverflow.com/questions/35252888/tensorflow-installation-problems", "I am new to tensorflow and trying to get started\r\nEnvironment - Windows \r\nPython version - 3.5.2\r\nInstalled tensorflow successfully, however when I try to import tensorflow in python shell I am getting the following error.  HELP Please\r\n\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Rajitha_Thalluri\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Rajitha_Thalluri\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Rajitha_Thalluri\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Rajitha_Thalluri\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Rajitha_Thalluri\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Users\\Rajitha_Thalluri\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Rajitha_Thalluri\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Rajitha_Thalluri\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Rajitha_Thalluri\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Rajitha_Thalluri\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Rajitha_Thalluri\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Rajitha_Thalluri\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Rajitha_Thalluri\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Users\\Rajitha_Thalluri\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n>>>"]}, {"number": 10694, "title": "Cannot build tensorflow at all", "body": "OS/X, building on a branch not master.\r\n\r\nAfter pulling the repo this morning (previous pull 12 hours ago) I can no longer build at all:\r\n\r\n```\r\nERROR: /Users/davidn/workspace/tensorflowview/tensorflow/tensorflow/core/BUILD:1394:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /Users/davidn/workspace/tensorflowview/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\n``\r\n\r\n", "comments": ["so it seems like a problem with my git repo.  the gen_git_source.py tool in tensorflow is generating a link to a file in the .git repo that doesn't exist.\r\n"]}, {"number": 10693, "title": "NotFoundError (see above for traceback): Key conv13/weights not found in checkpoin", "body": "Caused by op u'save/RestoreV2_4', defined at:\r\n  File \"test_go.py\", line 140, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"test_go.py\", line 137, in main\r\n    evaluate()\r\n  File \"test_go.py\", line 119, in evaluate\r\n    saver = tf.train.Saver()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1139, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1170, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 691, in build\r\n    restore_sequentially, reshape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 680, in restore_v2\r\n    dtypes=dtypes, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nNotFoundError (see above for traceback): Key conv13/weights not found in checkpoint\r\n\t [[Node: save/RestoreV2_4 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_4/tensor_names, save/RestoreV2_4/shape_and_slices)]]\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "@aselle Thanks for you attention. I have fixed this problem, The problem is just caused by that the directory of checkpoint is wrong. After I change the right directory, it works. Thanks very much!", "Glad you managed to get it to work.", "there is a same question as your .\"NotFoundError (see above for traceback): Key conv1/biases not found in checkpoint\".\r\ni didn't understand  you say \"The problem is just caused by that the directory of checkpoint is wrong.\"could tell me what's wrong with it.thank you for answering.", "\r\n\r\n@aselle\u00a0Thanks for you attention. I have fixed this problem, The problem is just caused by that the directory of checkpoint is wrong. After I change the right directory, it works. Thanks very much!\r\n--\r\n\r\n\r\n\r\nI have the same issues, can you explain what you really doing clearly?"]}, {"number": 10692, "title": "added blocks option for lstm2d", "body": "", "comments": ["Can one of the admins verify this patch?", "thanks for review, maybe someone should add these rules in pylint too", "Waiting for review! ", "Jenkins, test this please."]}, {"number": 10691, "title": "Branch 158919724", "body": "", "comments": ["@tensorflow-jenkins test this please", "Rebuilding makefile at http://ci.tensorflow.org/job/tensorflow-pull-requests-makefile/9165/console", "Thanks Yifei!"]}, {"number": 10690, "title": "Fix the f16 implementation of Literal and LiteralUtil", "body": "Adjust the implementation of Literal to do the right thing for F16.   The new Literal class is nice, in that it is just a std::vector of half types, which is nice and simple.\r\n\r\nThe Proto version still has to have a bit of munging around due to protobuf limitations.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 10689, "title": "Tensorflow build fails with -mavx512f", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 25\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: latest commit f58e6ce , happens with older versions as well (23caaa5, f48673b, 9a15e0a, 3c4cb08 to only name a few)\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: none\r\n- **CPU model and memory**: XEON PHI 7250, 96GiB ram\r\n- **Exact command to reproduce**: \r\n./configure\r\nspecify optimization flags: -mavx512f\r\nbazel  build --config=opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nBuilding Tensorflow from source fails when avx512 instructions are activated.\r\nAfter managing to compile most of the source the build fails somewhere in the eigen part of the code.\r\n\r\n[builderror.txt](https://github.com/tensorflow/tensorflow/files/1072902/builderror.txt)\r\n\r\nDon't let yourself get thrown off by the \"MKL\" in some path names, its just the name of the virtualenv, for this report MKL was turned off in the configure.\r\nSee also similar Issue #9849\r\n\r\n\r\n", "comments": ["Also seems to be like this #7530.\r\n@benoitsteiner, could you take a look?", "I guess my issue is the same.\r\n[Build Log.txt](https://github.com/tensorflow/tensorflow/files/1083578/Build.Log.txt)\r\nOn 16.04.2 with GCC 7.1.0\r\nI did not use -mavx512f but -march=native (which enables those kind of architecture optimizations I guess).\r\nAnd same issue with and without MKL.", "I can confirm the same issue with latest master ( 1209491913def44650d6457c60a6e41d56de3306 ) on a Skylake Xeon CPU, using either -march=native or -mavx512f, with either GCC 6 or 7.\r\n", "Seems possibly related to https://github.com/tensorflow/tensorflow/issues/12781 also.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 10688, "title": "Updating the version from 1.2.0rc2 to 1.2.0.", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 10687, "title": "Cherrypicks", "body": "Tensorboard cherrypicks while reverting tensorboard to previous hash. Please review properly. Doc and release file updates.", "comments": ["Yes, cc @dandelionmane .", "Jenkins, test this please."]}, {"number": 10686, "title": "Expose TFRecordWriter.Flush() in Python", "body": "This fix tries to address the issue raised in #10644 where it was not possible to call `TFRecordWriter.Flush()` in Python. The C++ interface already has the `Flush()` so this fix exposes it to python level.\r\n\r\nThis fix is related to #10644.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "Could you update the API goldens according to the error message in the Linux CPU test?", "@martinwicke The PR has been updated with API golden updated. Please take a look.", "Jenkins, test this please.", "Good to go for API review.", "Thanks!"]}, {"number": 10685, "title": "Darwin support for MKL build / configure script", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMac OS X 10.12.4, Darwin\r\n- **TensorFlow installed from (source or binary)**:\r\nSource, git tag 1.2.0 RC 2\r\n- **TensorFlow version (use command below)**:\r\n1.2 RC 2\r\n- **Bazel version (if compiling from source)**:\r\nbazel version\r\n..............\r\nBuild label: 0.4.3-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Dec 22 15:20:15 2016 (1482420015)\r\nBuild timestamp: 1482420015\r\nBuild timestamp as int: 1482420015\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\n./configure \r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nFound possible Python library paths:\r\n  /Library/Python/2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/Library/Python/2.7/site-packages]\r\n\r\nUsing python library path: /Library/Python/2.7/site-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N] y\r\nMKL support will be enabled for TensorFlow\r\nDo you wish to download MKL LIB from the web? [Y/n] Y\r\nDarwin is unsupported yet\r\n\r\nYou can collect some of this information using our environment capture script:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\nN/A\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDarwin / Mac OS X support for Intel's optional MKL optimizations. MKL appears to have darwin support, so I am unsure why it is not supported in Tensorflow. I imagine this is on a to-do, but a publicly tracked feature request might be helpful. Thank you.\r\n\r\n### Source code / logs\r\nN/A", "comments": ["Any eta on darwin support?", "@tfboyd Who would be a good person to address MKL support on MacOS?", "You have to manually tweak the config script to download the OSX MKL version.  The URL is slightly different.  Intel did not include that check in the script.  I will own this as I want to do the process so I can finish up some benchmarks to include in an updated performance guide.  It would be awesome but not necessary if you (@vade) found the URL and updated the config script.  ", "Isn't there a separate problem that  Tensorflow MKL build requires MKL-DNN which is currently Linux only?  See this issue - https://github.com/01org/mkl-dnn/issues/10.\r\n\r\nDiscussion that Tensorflow MKL build uses MKL-DNN primitives is in the Intel press release - https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture", "I am 99% sure (not 100% as I like to leave room for error) that we are not using MKL-DNN open source.  It downloads the binary MKL if I am not mistaken.  I also spoke to Intel today (the people that did the PRs for MKL) and the person assured me it supports Mac OSX and I needed to tweak the config.  I am trying to do the build but Google policy is slowing me down. \r\n\r\nUpdate:  I checked the ./configure file and it does seem to be downloading the OSS version, which is not what I was told by Intel.  I am talking with them now and working to figure it out.  There is a free but non OSS version of the MKL as well, which might provide better support for OSX.", "Hit the same pole, using a mac mini\r\n```\r\nDo you wish to download MKL LIB from the web? [Y/n] \r\nDarwin is unsupported yet\r\n```\r\nWhat to do next?\r\n```\r\n== cat /etc/issue ===============================================\r\nDarwin yuldeMac-mini.local 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.4\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin16.5.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin yuldeMac-mini.local 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.1)\r\nnumpydoc (0.6.0)\r\nprotobuf (3.2.0)\r\ntensorflow (1.1.0)\r\n\r\n```", "I took a first pass and it was not as easy as tweaking the download and small change to the ./configure file.  This may take a few days or even longer to figure out.  The performance improvement I have personally tested on Linux is substantial which should translate equally to OSX.  I will update this thread as I go.  One option I have not tried is to download the MKL from Intel and give that a try over the OSS version.", "Thank you @tfboyd for looking into this so thoroughly and quickly. With the loss of potential GPU support on OS X, us Mac users can take all the client side gains we can!\r\n\r\nVery appreciated!", "@vade In the mean time make sure you are compiling with AVX or AVX2 as that makes a big difference.  I hope to publish some info on the performance page soon.  I just got an email back from Intel and I believe they are interested in helping us work through it.  Sadly the dream of just swapping out the download was not the solution.  :-)  In some ways I am not sure why I thought it would be that easy....nothing is that easy.  ", "I have been using AVX/AVX2 and FMA and it certainly does help! ", "Update:  Using my limited Bazel and c++ skills I was a able to get it to compile on Mac OSX but when I tried to import tensorflow I ended up with an error.  I am going to try to find someone on the team with a much higher level of skill to work on this.  My gut feeling is this is doing to take some time.  Leaving assigned to me until I find someone and I am very interested in tracking this to completion or finding out it will not work.  And wow, compiling on OSX beyond CPU looks less than fun.  I read all the old bugs on cuDNN, that looked less than pleasant\r\n\r\nThe error I ended up with after I turned of SIP so the libraries would load and used an updated version of clang via brew.  \r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 64, in <module>\r\n    from tensorflow.python.framework.framework_lib import *\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/framework/framework_lib.py\", line 100, in <module>\r\n    from tensorflow.python.framework.subscribe import subscribe\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/framework/subscribe.py\", line 26, in <module>\r\n    from tensorflow.python.ops import variables\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/ops/variables.py\", line 26, in <module>\r\n    from tensorflow.python.ops import control_flow_ops\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 69, in <module>\r\n    from tensorflow.python.ops import math_ops\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 2452, in <module>\r\n    fft = gen_spectral_ops.fft\r\nAttributeError: 'module' object has no attribute 'fft'\r\n\r\n```\r\n", "I managed to compile it on a work computer. Besides editing the configure script I remember also changing the file names in https://github.com/tensorflow/tensorflow/blob/master/third_party/mkl/BUILD to match the ones I downloaded (that file has `.so` for linux shared libraries but I had downloaded `.dylib` dynamic libraries).\r\n\r\nI also symlinked the dylibs into /usr/local/lib/ --- having `configure` set the default path to `/opt/intel/mklml` wasn't sufficient for me because some intermediate binaries were causing the linker to complain about not finding intermediate binaries. passing the /opt/intel/mklml/ path to the linker at the bazel build stage didn't work.", "@nuchi   Were you able to use it after you compiled?  Asking because I was able to get it to compile and MKL was 100% included but then I get the error when I first tired to use it.  \r\n\r\nThis is great data.  We and maybe I will keep trying.  Any info you ca provide would be great.  I also had to do the symlink thing  :-)", "@tfboyd yes, passed the mkl unit tests and also could successfully import tensorflow and eval a hello world tensor. I'll be able to provide more info in a week or so \u2014 I'm away from that computer at the moment.", "That is awesome.  No rush.   Any info is great and knowing it worked for you I will give it a try again from scratch.  By the time mine compiled I had made a huge mess.  :-)", "@tfboyd More information!\r\n\r\nSystem: Mac OS X 10.11.6\r\nIntel MKL library: mklml_mac_20180.0.20170425.tgz (from https://github.com/01org/mkl-dnn/releases)\r\nTensorflow commit: 38e0922d...\r\n\r\nInstalled MKL to `/opt/intel/mklml`\r\nSymlink the two `.dylib` files to `/usr/local/lib`\r\n\r\nChanges to `configure`: Change `MKL_ML_LIB_PATH` to `lib/libmklml.dylib`, `MKL_ML_OMP_LIB_PATH` to `lib/libiomp5.dylib`, comment out the parts that say \"if linux:\", comment out the parts that deal with `libdl.so.2`.\r\n\r\nChanges to `third_party/mkl/BUILD`: Change the list of three `.so` files to the two `.dylib` files\r\n\r\nWhen configuring: Yes to use MKL, No to download. (I downloaded manually.) Path to MKL: `/opt/intel/mklml`\r\n\r\nBuild command: `bazel build --verbose-failures -c opt --config=opt --config=mkl --copt=\"-DEIGEN_USE_VML\" --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 --linkopt=\"-Wl,-rpath,/opt/intel/mklml/lib\" --linkopt=\"-L/opt/intel/mklml/lib\" --linkopt=\"-lmklml\" --linkopt=\"-iomp5\" //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nIt's possible I missed some stuff, but I think that's it. Some of this stuff is surely superfluous, but after it worked I didn't spend any effort trying to pare it down to the minimum.", "@nuchi   This is great.  The team is very much interested in making this work.  I hope we get it sort out officially in a few weeks but no promises as anything can happen and it looks like vacation season is here.  I will try you info and at a min, include it for people on the performance guide page as a short-term solution if I post some MKL stuff in a couple weeks.  Your effort is really appreciated.  I think I was close but I I did not use the rpath variable and I suspect that is why I ended up with a broken build.  :-)  ", "Something like `CC=clang-mp-4.0  bazel build  --copt=-I/usr/local/include/libomp -c opt --config=opt --config=mkl --copt=\"-DEIGEN_USE_VML\" --copt=\"-march=haswell\" --linkopt=\"-Wl,-rpath,/opt/intel/mklml/lib\" --linkopt=\"-L//opt/intel/mklml/lib\" --linkopt=\"-lmklml\" --linkopt=\"-liomp5\" //tensorflow/tools/benchmark:benchmark_model` works for me. The clang come with Xcode doesn't enable OpenMP support. I use clang-4.0 (`clang-mp-4.0`) from MacPort instead. However, on my late 2014 iMac, MKL doesn't provide better performance for most cases I tested. Anyway, I'll try to create patches and send PR later.", "It seems I should delay the patch till https://github.com/tensorflow/tensorflow/pull/11212 from @gunan is done.", "I actually have a patch ready, but I will need to test my patch.\r\nOnce all my changes are in, we will not even need the option in configure script.\r\n`bazel build --config=mkl tensorflow/tools/pip_package:build_pip_package`\r\nwill allow you to build a pip package with mkl support.", "@gunan Thanks. So I'll just wait. BTW, did you test the performance of after applying MKL-DNN. I tested various pre-trained models (frozen and optimized for inference) from https://github.com/tensorflow/models/tree/master/slim with MKL-DNN (for inference) on my late 2014 iMac. Ones with MKL-DNN are usually slower.\r\n", "I have the changes that would make things work for mac, but have not tested it on a mac yet.\r\nOnce it works, I will leave benchmarking to @tfboyd as I will be out on vacation for most of next month.", "I have some numbers only CNN based models.  There are cases where AVX or AVX2 is faster and more so if the wrong settings are used.  I should have a draft done this week and I may (most likely will) just share what I know on this thread ASAP because the info is not secret but it takes time to get all the editing done to meet the quality bar for the tf.org site.  ", "I tried @nuchi's steps from June 25 to build but I keep getting the error below. Any ideas on what tests I can try to pin it down?\r\n\r\ndyld: Symbol not found: _MKL_Detect_Cpu_Global_Lock\r\n  Referenced from: /usr/local/lib/libmkl_intel_lp64.dylib\r\n  Expected in: flat namespace\r\n in /usr/local/lib/libmkl_intel_lp64.dylib\r\n/bin/bash: line 1: 36400 Trace/BPT trap: 5       bazel-out/host/bin/tensorflow/contrib/image/gen_image_ops_py_wrappers_cc 0 > bazel-out/local-opt/genfiles/tensorflow/contrib/image/ops/gen_image_ops.py", "@i-shenl \u2014 I think probably the easiest way at this point to compile with mkl on macOS is to checkout master, then cherry-pick @gunan's commit https://github.com/gunan/tensorflow/commit/ecd85c21545e4c47a78fa331b4e4bfce7af09180\r\n\r\n(I haven't tried it but it certainly sounds much easier than the whole mess I went through)\r\n\r\nedit: and as gunan writes above, probably just `bazel build --config=mkl tensorflow/tools/pip_package:build_pip_package` to build after that. (again, I haven't tried it)", "@gunan Any progress to making MKL work on Mac OS?", "While by locally downloading MKL and setting `LD_LIBRARY_PATH` to point to mkl dylib files you can make it work, we are blocked on https://github.com/bazelbuild/bazel/issues/407 to enable MKL fully on macos.", "@gunan I installed MLK on Mac OS (by default in /opt/intel/mkl)\r\nThen I tried to install tensorflow from source with the following configuration parameters:\r\n\r\nLD_LIBRARY_PATH=/opt/intel/mkl/lib/ ./configure \r\nYou have bazel 0.4.5-homebrew installed.\r\nPlease specify the location of python. [Default is /Users/vostryakov/projects/env/bin/python]: \r\nFound possible Python library paths:\r\n  /Users/vostryakov/projects/env/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/Users/vostryakov/projects/env/lib/python2.7/site-packages]\r\n\r\nUsing python library path: /Users/vostryakov/projects/env/lib/python2.7/site-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N] y\r\nMKL support will be enabled for TensorFlow\r\nDo you wish to download MKL LIB from the web? [Y/n] n\r\nPlease specify the location where MKL is installed. [Default is /opt/intel/mklml]: /opt/intel/mkl \r\n**Darwin is unsupported yet**\r\n\r\nMay be I do something wrong?", "You still need changes similar to this commit:\r\nhttps://github.com/gunan/tensorflow/commit/ecd85c21545e4c47a78fa331b4e4bfce7af09180\r\n\r\nI am waiting the bazel bug referenced to be fixed and released to be able to test the commit, then I will create a PR to the master repository.", "Quick note that blocker https://github.com/bazelbuild/bazel/issues/407 was just closed ", "Which was rolled back with https://github.com/bazelbuild/bazel/commit/52f32bf511cfee4477bece6820e7b827d80c5feb", "*twitch*", "https://github.com/bazelbuild/bazel/commit/600ab49a98bcb6d96231174b44bd4ae335de4dbf ? Was this re-rolled forward correctly @gunan ? \r\n\r\n", "And just to re-iterate - out of the box Darwin MKL support would be awesome. ", "@vade \"And just to re-iterate - out of the box Darwin MKL support would be awesome.\"\r\n\r\nI agree.  ", "I just tested building with mkl again on macos with bazel 0.8, and it is still looking for the dylib files under the wrong directory. I will follow up with bazel team and will soon come back with another update.", "@gunan Thank you very much for continuing to respond to this issue, its very much appreciated! ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Out of the box support really would be nice. The changes in the configure process for 1.4.1 make the previous work arounds less applicable :-)", "Currently I am working to add support for `--config=mkl` support for macos, but I once again ran into another bazel bug.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I am still stuck with the errors in dynamic linking.", "Hi everyone,\r\nI am yet another MacBook Pro owner struggling to gain a bit more performance.\r\nFortunately I had some degree of success with MKL, but only in docker instances so far.\r\nIt gives about 3.5x of performance gain compared to `native` build (tested with resnet50 benchmark)\r\n\r\n---\r\n\r\n## Test Hardware:\r\n```\r\nMacBook Pro (15-inch, 2016)\r\nmacOS High Sierra 10.13.3 (17D47)\r\n2.6 GHz Intel Core i7\r\n16 GB 2133 MHz LPDDR3\r\n```\r\n\r\n## Docker image with MKL\r\n`antonmatosov/tensorflow:latest-devel-cpu-mkl-py3`\r\n\r\n-----\r\n## Here are test results\r\n\r\nNon MKL build - total images/sec: 1.31\r\nMK build - total images/sec: 4.75\r\n\r\n**Total gain: 3.63**\r\n\r\n## Here are full results:\r\n\r\n### Native build with triSYCL enabled, executed on bare OS:\r\n```\r\npython tf_cnn_benchmarks.py --num_gpus=1 --batch_size=32 --model=resnet50 --data_format=NHWC\r\n\r\n\r\nTensorFlow:  1.5\r\nModel:       resnet50\r\nDataset:     imagenet (synthetic)\r\nMode:        training\r\nSingleSess:  False\r\nBatch size:  32 global\r\n             32.0 per device\r\nNum batches: 100\r\nNum epochs:  0.00\r\nDevices:     ['/gpu:0']\r\nData format: NHWC\r\nLayout optimizer: False\r\nOptimizer:   sgd\r\nVariables:   parameter_server\r\n==========\r\n\r\nStep\tImg/sec\ttotal_loss\r\nI0223 23:10:55.230010 123145468305408 tf_logging.py:110] Starting real work at step 10 at time Fri Feb 23 23:10:55 2018\r\n1\timages/sec: 1.3 +/- 0.0 (jitter = 0.0)\tnan\r\n10\timages/sec: 1.3 +/- 0.0 (jitter = 0.0)\tnan\r\n20\timages/sec: 1.3 +/- 0.0 (jitter = 0.0)\tnan\r\n30\timages/sec: 1.3 +/- 0.0 (jitter = 0.0)\tnan\r\n40\timages/sec: 1.3 +/- 0.0 (jitter = 0.0)\tnan\r\n50\timages/sec: 1.3 +/- 0.0 (jitter = 0.0)\tnan\r\n60\timages/sec: 1.3 +/- 0.0 (jitter = 0.0)\tnan\r\n70\timages/sec: 1.3 +/- 0.0 (jitter = 0.0)\tnan\r\n80\timages/sec: 1.3 +/- 0.0 (jitter = 0.0)\tnan\r\n90\timages/sec: 1.3 +/- 0.0 (jitter = 0.0)\tnan\r\nI0223 23:51:10.453911 123145468305408 tf_logging.py:110] Finishing real work at step 109 at time Fri Feb 23 23:51:10 2018\r\n100\timages/sec: 1.3 +/- 0.0 (jitter = 0.0)\tnan\r\n----------------------------------------------------------------\r\ntotal images/sec: 1.31\r\n----------------------------------------------------------------\r\n```\r\n\r\n----\r\n### MKL enabled build on Docker for Mac (4 CPUs, 4GB)\r\n\r\n```\r\npython3 tf_cnn_benchmarks.py --batch_size=32 --model=resnet50\r\n\r\nTensorFlow:  1.5\r\nModel:       resnet50\r\nDataset:     imagenet (synthetic)\r\nMode:        training\r\nSingleSess:  False\r\nBatch size:  32 global\r\n             32.0 per device\r\nNum batches: 100\r\nNum epochs:  0.00\r\nDevices:     ['/gpu:0']\r\nData format: NCHW\r\nLayout optimizer: False\r\nOptimizer:   sgd\r\nVariables:   parameter_server\r\n==========\r\n\r\nStep\tImg/sec\ttotal_loss\r\nI0225 23:12:09.519792 140405827925760 tf_logging.py:110] Starting real work at step 10 at time Sun Feb 25 23:12:09 2018\r\n1\timages/sec: 4.9 +/- 0.0 (jitter = 0.0)\t11.576\r\n10\timages/sec: 4.7 +/- 0.0 (jitter = 0.1)\t8.911\r\n20\timages/sec: 4.7 +/- 0.0 (jitter = 0.1)\t8.426\r\n30\timages/sec: 4.7 +/- 0.0 (jitter = 0.2)\t8.177\r\n40\timages/sec: 4.7 +/- 0.0 (jitter = 0.1)\t15.682\r\n50\timages/sec: 4.7 +/- 0.0 (jitter = 0.1)\t7.956\r\n60\timages/sec: 4.7 +/- 0.0 (jitter = 0.1)\t8.638\r\n70\timages/sec: 4.7 +/- 0.0 (jitter = 0.1)\t8.246\r\n80\timages/sec: 4.7 +/- 0.0 (jitter = 0.2)\t7.947\r\n90\timages/sec: 4.7 +/- 0.0 (jitter = 0.2)\t7.906\r\nI0225 23:23:16.803599 140405827925760 tf_logging.py:110] Finishing real work at step 109 at time Sun Feb 25 23:23:16 2018\r\n100\timages/sec: 4.8 +/- 0.0 (jitter = 0.2)\t7.874\r\n----------------------------------------------------------------\r\ntotal images/sec: 4.75\r\n----------------------------------------------------------------\r\n```\r\n", "I have also managed to build Tensorflow with MKL on Darwin natively (steps below), but crashes at runtime during the benchmark, even tho it works for simple tests.\r\n\r\nHere is the runtime error I get\r\n```\r\n$ python tf_cnn_benchmarks.py --batch_size=32 --model=resnet50 --data_format=NHWC --device=cpu\r\n\r\npython(7794,0x7fff892ed340) malloc: *** error for object 0x182655e990: pointer being freed was not allocated\r\n*** set a breakpoint in malloc_error_break to debug\r\nfish: 'python tf_cnn_benchmarks.py --b\u2026' terminated by signal SIGABRT (Abort)\r\n```\r\nSometimes it is dealloc of already freed pointer. Not sure if it is MKL itself, or something is getting messed up in TF with such a setup.\r\nI am going to spend next couple days trying to tough it out. \r\nI will keep everyone posted.\r\n\r\n### How I built with MKL on macOS\r\n1. I have implemented couple fixes to Bazel and C++ code, available in this fork/branch https://github.com/anton-matosov/tensorflow/tree/v1.5.0-mkl-mac-build\r\n2. Download MKL for Mac archive from https://github.com/intel/mkl-dnn/releases and extract it (I have used v0.12 release)\r\n3. Run build using the following command `env TF_MKL_ROOT=/Users/antonmatosov/Downloads/mklml_mac_2018.0.1.20171227  bazel build --config=mkl --config=\"opt\" --copt=\"-march=native\" --copt=\"-O3\" //tensorflow/tools/pip_package:build_pip_packag`", "@anton-matosov - those NANs in the docker build look vv suspicious - have you verified accuracy ? ", "That's not faster, that's slower one (bigger is better, it's images/sec).\r\nAnd I have not noticed that, and don't even know what this benchmark output actually means.\r\nBut I have used this build to train and run some experiments with OpenAI Gym and it worked without problems.", "@anton-matosov I guess my point is, its not a benchmark if the data is garbage haha :) Thanks for documenting the MLK build on Darwin though, thats awesome\r\n", "@vade makes sense. I will rerun those tests with couple other builds on mac.", "Good news. I have managed to localize the issue (haven't fix yet) and MLK build now runs natively on my Mac!!!\r\nThe issue is in the tensorflow lite, which I don't use at the moment. So I commented out it's import from contrib's __init__ `# from tensorflow.contrib.lite.python import lite` and I got max performance of my i7 CPU\r\n![image](https://user-images.githubusercontent.com/3339485/36704859-2611de50-1b17-11e8-8110-614b2aef3b39.png)\r\n\r\nPerformance: \r\nimages/sec: 5.1\r\n(this is 3.92x gain)\r\n\r\nI will setup clean VM and figure out exact build steps and will post them along with PR that fixes build", "Narrowed it down even deeper.\r\nCrash happens while loading `_tensorflow_wrap_toco.so` in \r\n`tensorflow/contrib/lite/toco/python/tensorflow_wrap_toco.py` line:\r\n`_mod = imp.load_module('_tensorflow_wrap_toco', fp, pathname, description)`", "Following crash trace in LLDB it looks like it is crashing in OpDef::~OpDef() from instance created in \r\ntensorflow/core/ops/function_ops.cc:22\r\n```\r\nREGISTER_SYSTEM_OP(\"_Arg\")\r\n    .Output(\"output: T\")\r\n    .Attr(\"T: type\")\r\n    .Attr(\"index: int >= 0\")\r\n    .SetIsStateful()\r\n    .SetShapeFn([](shape_inference::InferenceContext* context) {\r\n      context->set_output(0, context->UnknownShape());\r\n      return Status::OK();\r\n    })\r\n    .Doc(R\"doc(\r\nA graph node which represents an argument to a function.\r\n\r\noutput: The argument.\r\nindex: This argument is the index-th argument of the function.\r\n)doc\");\r\n```\r\n\r\n```\r\n    frame #0: 0x00007fff5044e9b6 libsystem_malloc.dylib`malloc_error_break\r\n    frame #1: 0x00007fff504407ed libsystem_malloc.dylib`free + 400\r\n    frame #2: 0x000000010932414d _pywrap_tensorflow_internal.so`tensorflow::OpDef::SharedDtor() + 125\r\n    frame #3: 0x0000000109323f5c _pywrap_tensorflow_internal.so`tensorflow::OpDef::~OpDef() + 28\r\n  * frame #4: 0x0000000119b93dcd _tensorflow_wrap_toco.so`_GLOBAL__sub_I_function_ops.cc + 189\r\n    frame #5: 0x0000000100392a0a dyld`ImageLoaderMachO::doModInitFunctions(ImageLoader::LinkContext const&) + 420\r\n    frame #6: 0x0000000100392c3a dyld`ImageLoaderMachO::doInitialization(ImageLoader::LinkContext const&) + 40\r\n    frame #7: 0x000000010038e170 dyld`ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 330\r\n    frame #8: 0x000000010038d2a6 dyld`ImageLoader::processInitializers(ImageLoader::LinkContext const&, unsigned int, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 134\r\n    frame #9: 0x000000010038d33a dyld`ImageLoader::runInitializers(ImageLoader::LinkContext const&, ImageLoader::InitializerTimingList&) + 74\r\n    frame #10: 0x00000001003813e5 dyld`dyld::runInitializers(ImageLoader*) + 82\r\n    frame #11: 0x000000010038a002 dyld`dlopen + 527\r\n    frame #12: 0x00007fff50298e86 libdyld.dylib`dlopen + 86\r\n    frame #13: 0x00000001001ce1b3 python`_PyImport_FindSharedFuncptr + 131```\r\n\r\n```\r\n    0x119b93d44 <+52>:  callq  0x119c8c010               ; tensorflow::OpDefBuilder::OpDefBuilder(tensorflow::StringPiece)\r\n    0x119b93d49 <+57>:  leaq   0x857142(%rip), %rsi      ; \"output: T\"\r\n    0x119b93d50 <+64>:  movl   $0x9, %edx\r\n    0x119b93d55 <+69>:  movq   %rbx, %rdi\r\n    0x119b93d58 <+72>:  callq  0x119c8c1c0               ; tensorflow::OpDefBuilder::Output(tensorflow::StringPiece)\r\n    0x119b93d5d <+77>:  leaq   0x857138(%rip), %r14      ; \"T: type\"\r\n    0x119b93d64 <+84>:  movl   $0x7, %edx\r\n    0x119b93d69 <+89>:  movq   %rbx, %rdi\r\n    0x119b93d6c <+92>:  movq   %r14, %rsi\r\n    0x119b93d6f <+95>:  callq  0x119c8c020               ; tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)\r\n    0x119b93d74 <+100>: leaq   0x857129(%rip), %r15      ; \"index: int >= 0\"\r\n    0x119b93d7b <+107>: movl   $0xf, %edx\r\n    0x119b93d80 <+112>: movq   %rbx, %rdi\r\n    0x119b93d83 <+115>: movq   %r15, %rsi\r\n    0x119b93d86 <+118>: callq  0x119c8c020               ; tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)\r\n    0x119b93d8b <+123>: movq   %rbx, %rdi\r\n    0x119b93d8e <+126>: callq  0x119c8c3e0               ; tensorflow::OpDefBuilder::SetIsStateful()\r\n    0x119b93d93 <+131>: leaq   -0x27a(%rip), %rsi        ; tensorflow::$_0::__invoke(tensorflow::shape_inference::InferenceContext*)\r\n    0x119b93d9a <+138>: movq   %rbx, %rdi\r\n    0x119b93d9d <+141>: callq  0x119c8c630               ; tensorflow::OpDefBuilder::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*))\r\n    0x119b93da2 <+146>: leaq   0x85710b(%rip), %rsi      ; \"\\nA graph node which represents an argument to a function.\\n\\noutput: The argument.\\nindex: This argument is the index-th argument of the function.\\n\"\r\n    0x119b93da9 <+153>: movl   $0x90, %edx\r\n    0x119b93dae <+158>: movq   %rbx, %rdi\r\n    0x119b93db1 <+161>: callq  0x119c8c290               ; tensorflow::OpDefBuilder::Doc(tensorflow::StringPiece)\r\n    0x119b93db6 <+166>: leaq   0x9532cf(%rip), %rdi      ; tensorflow::register_op0\r\n    0x119b93dbd <+173>: movq   %rbx, %rsi\r\n    0x119b93dc0 <+176>: callq  0x119c887e0               ; tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)\r\n    0x119b93dc5 <+181>: movq   %rbx, %rdi\r\n    0x119b93dc8 <+184>: callq  0x11a35cecc               ; symbol stub for: tensorflow::OpDefBuilder::~OpDefBuilder()\r\n->  0x119b93dcd <+189>: leaq   0x857171(%rip), %rsi      ; \"_Retval\"\r\n    0x119b93dd4 <+196>: leaq   -0x160(%rbp), %rbx\r\n    0x119b93ddb <+203>: movl   $0x7, %edx\r\n```\r\n\r\nAt this point I will need help/hint from the tensorflow team to understand better what is going on here", "I have found bunch of other issues that have same crash with protobuf's SharedDtor\r\nhere https://github.com/tensorflow/fold/issues/37\r\nhere https://github.com/tensorflow/deepmath/issues/13\r\nand here https://github.com/tensorflow/tensorflow/issues/12389\r\nhas a comment:\r\nhttps://github.com/tensorflow/tensorflow/issues/12389#issuecomment-323539751\r\n>This sounds a little bit like the known caveat of protobuf that one shouldn't share objects from protobuf across library boundaries, lest the global fixed_address_empty_string might get freed?\r\n>\r\n> (This may be no longer an issue in the protobuf being used for tensorflow - i may be out of date..) ?\r\n\r\nI will experiment with different builds of protobuf to find if there is any working solution on Mac", "Ok. Got rid of the memory issue. It was really caused by the protobuf build (manually built from source....)\r\nThe stock one from `conda` worked fine (after fixing issue with conda's cache).\r\nBut there is a new crash now, again caused by the Toco\r\n\r\n```\r\n2018-02-26 20:56:09.605180: F tensorflow/core/common_runtime/device_factory.cc:77] Duplicate registration of device factory for type CPU with the same priority 60\r\nfish: 'python -c \"from tensorflow.cont\u2026' terminated by signal SIGABRT (Abort)\r\n```\r\n\r\nSteps to reproduce:\r\n`python -c \"from tensorflow.contrib.lite.toco.python.tensorflow_wrap_toco import TocoConvert as _toco_convert_protos\"`", "I have compared _tensorflow_wrap_toco.so on linux and mac and found 1 major difference that I suspect causes all these trouble.\r\nOn linux it depends on Shared library `libtensorflow_framework.so`\r\n```\r\n\r\n 0x0000000000000001 (NEEDED)             Shared library: [libtensorflow_framework.so]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libpthread.so.0]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libdl.so.2]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libm.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libstdc++.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libgcc_s.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [ld-linux-x86-64.so.2]\r\n```\r\n\r\nWhile on mac it has TF linked in statically (no libtensorflow_framework.so here and libmklml is linked to it directly):\r\n```\r\nLC 10: LC_LOAD_DYLIB         \t@rpath/libmklml.dylib\r\nLC 11: LC_LOAD_DYLIB         \t@rpath/libiomp5.dylib\r\nLC 12: LC_LOAD_DYLIB         \t/usr/lib/libSystem.B.dylib\r\nLC 13: LC_LOAD_DYLIB         \t/usr/lib/libc++.1.dylib\r\nLC 14: LC_LOAD_DYLIB         \t/System/Library/Frameworks/Foundation.framework/Versions/C/Foundation\r\nLC 15: LC_LOAD_DYLIB         \t/usr/lib/libobjc.A.dylib\r\n```\r\n\r\nI am trying to put my head around `tf_py_wrap_cc` and all the deps in Bazel. But no luck so far.\r\n\r\nAny help is very welcome here.", "Yay! I finally figured it out with shared libraries.\r\nFor some reason on Mac I always get by `.tf_configure.bazelrc` truncated and it looses last 10 lines including `build --define framework_shared_object=true` which should **unconditionally** be there.\r\n\r\nHowever it's not the end of the story. There is a ton of linker warnings getting generated during the build and it look like final shared library is still not functioning well. On anything more complex then `TF hello world` it results in:\r\n`'python' terminated by signal SIGSEGV (Address boundary error)`", "I am not sure if I am going to keep digging this rabbit hole for too long.\r\nI might stick to build with commented out `lite.toco` or just use it from Docker, as performance overhead is barely noticeable. ", "\ud83c\udf89I DID IT!\r\nIt am not sure what was causing the issue (I was also playing with different optimization level and debug build, switching branches, etc.), but restarting from fresh checkout, clean conda env, rerun configure and build gave a working native binaries!!!\r\n\r\nWith all the possible CPU runtime optimizations settings from [here](https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu) I got this:\r\n# MKL - total images/sec: 5.82\r\n### no MKL - total images/sec: 1.31 \r\n\r\n----\r\nMKL benchmark command line\r\n```\r\npython tf_cnn_benchmarks.py --forward_only=False --device=cpu --mkl=True \\ \r\n            --kmp_blocktime=0 --nodistortions --model=resnet50 --data_format=NCHW \\\r\n            --batch_size=32 --num_inter_threads=1 --num_intra_threads=4\r\n```", "I have uploaded my wheel with MKL here https://github.com/anton-matosov/tensorflow-wheels\r\n\r\nIf you would like to build it yourself, please follow instructions provided here: https://github.com/anton-matosov/tensorflow-wheels/tree/master/Tensorflow-Wheels/MacOS/MKL (Disclaimer: tested on my machine only)\r\n\r\nMeanwhile I am going to complete the MKL build to have MKL dylib files bundled inside the wheel and I will submit PR after that.", "Nagging Assignees @gunan, @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @gunan, @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This issue was fixed by #17396", "Nagging Assignees @gunan, @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @gunan, @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I have just tried and it looks like addition of OMP threads is actually causing a breakage on macos now.\r\n@tatianashp Could you link in Intel on this issue? What is the plan for support for macos, especially since clang does not have openmp support?", "@jbobba It looks like #17931 is breaking MKL support on macos.\r\nWhat is your recommendation?", "@gunan will investigate. \"clang does not have openmp support\" - are you referring to a TF-specific clang version/build? In general, clang does support openmp, so i'm surprised it fails here. ", "I am using the out of the box clang that comes with xcode. A quick search showed that `omp.h` is generally unavailable with xcode's clang.", "The clang version that comes with Mac OS does not support openmp.  The current clang (6.0) that comes with LLVM supports openmp which is how I built TF 1.8 with MKL and can be installed with brew.  Just have to point compiler, include, linker flags to the new location, e.g, CC=/usr/local/opt/llvm/bin/clang and set path so it sees this version of clang and not the default Mac OS version.\r\n\r\nI had to add these statements to bazel-build :   --config=mkl --cpu=darwin   the later statement is because bazel 0.12 seems to want to generate ARM code on the Mac.", "At head bazel 0.12 issue on mac should be resolved.\r\nFor this case, we want to fully support the most common compiler on the platforms, which for macos case is clang that comes with xcode, rather than llvm.", "The Xcode version of clang goes not support openmp (omp.h) which is now required to build TF 1.8 (at least with MKL support).", "@dfumento the PR https://github.com/tensorflow/tensorflow/pull/18726 about supporting  mac mkl build  could resove the issue that default clang goes not support openmp flag in MacOs. \r\nFor defalut macos compiler clang you should use openmp as below:\r\n + if_mkl_darwin([\"-Xpreprocessor\", \"-fopenmp\"]) \r\n", "Unfortunately, adding \"-fopenmp\" on macos will result in:\r\n```\r\nclang: error: unsupported option '-fopenmp'\r\n```", "> Unfortunately, adding \"-fopenmp\" on macos will result in:\r\n\r\n> clang: error: unsupported option '-fopenmp'\r\n\r\nAs I said, you must install the llvm version of clang (6.0) which handles mp support, e.g.  '-fopenmp' . which I did with brew.  Then update the various compiler, include, and linker environment variables to point to the brew installed version of clang as I noted above.", "@dfumento As I mentioned before, the official support of tensorflow will have to stay with whatever default toolchain macos has.\r\nTherefore, if we (TensorFlow team) will support MKL officially, one of the requirements would be to either roll back #17931, or write more preprocessor commands to avoid using openmp on macos.", "@gunan we will look into adding more preprocessor guards to avoid openmp on default macos builds.", "> @gunan we will look into adding more preprocessor guards to avoid openmp on default macos builds.\r\n\r\nI'm assuming you'll put in a bazel-build option flag for the openmp version?", "Yes and _OPENMP around openmp code.", "@gunan @jbobba default toolchain clang of  macos would use [\"-Xpreprocessor\", \"-fopenmp\"] to support openmap flag but xcode version clang can't use in this method.\r\n", "@opencici2006 default toolchain on macOS is \"Xcode command line tools\" which has clang that doesn't support \"-fopenmp\"", "#19136 has the preprocessor guards around openmp code.", "I just verified that this is now working.\r\nThanks @anton-matosov for contributing this, and thanks @jbobba for openmp fixes!"]}, {"number": 10684, "title": "Allow `1/0` to be compatible with tf.bool", "body": "This fix tries to address the issue raised in #5407 where\r\n```\r\nimport numpy as np\r\nfrom tensorflow.python.ops import math_ops\r\nimport tensorflow as tf\r\n\r\nlabel = tf.constant(np.array([1, 0, 1]), dtype=tf.bool)\r\nmath_ops.equal(label, 1)\r\n```\r\nwill raise an exception of type mismatch.\r\n\r\nThis fix updated the `_AssertCompatible` so that `1/0`\r\nis compatible with `tf.bool` types.\r\n\r\nAdditional test cases have been added.\r\n\r\nThis fix fixes #5407\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Looks like there was some transient error.\r\n\r\nJenkins, test this please.", "This would lead to rather strange behavior if your values are float -- since you now depend on the content of the vector to be compared. \r\n\r\nI think this needs to be fixed with a cast, or if this blocks something downstream (e.g. the contrib.metrics issue linked above), then the code that performs type checking which is too strict should be made more tolerant.\r\n\r\nThis change as is is probably too dangerous."]}, {"number": 10683, "title": "Apply correct filter tags to two tests", "body": "session_clusterspec_prop_test\r\nsession_list_devices_test\r\n\r\nShould fix ongoing breakage in nightly GPU builds:\r\nhttp://ci.tensorflow.org/view/Tensorflow%20Jenkins%20Monitored%20builds/job/nightly-matrix-linux-gpu/528/", "comments": ["Also tested: experimental GPU PIP build (Jenkins login required to view): http://ci.tensorflow.org/view/Experimental/job/experimental-cais-matrix-linux-gpu/74/console", "cc @saeta"]}, {"number": 10682, "title": "[OpenCL] Fixes device name comparison stage_op_test (#85)", "body": "Changes the fixed '/device:GPU:0' expected device name string to the\r\nname returned by 'test/gpu_device_name()', as the device name could be\r\n'/device:SYCL:0' or '/device:GPU:0'.", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "@jwlawson can you OK it?", "@googlebot @lukeiwanski Ok with me", "@jwlawson Can you sign the CLA ? We can't accept changes that you authored without a CLA in place.", "We added John to the corporate CLA. Not sure what confuses googlebot.", "Possibly the email used in the git commit?\r\nIt should be the same one added to the corporate CLA.", "I've resubmitted this pull request myself (see #10760). Github was changing my email in the commit when Luke pulled it into his repo, causing the CLA bot to fail.", "Closing."]}]