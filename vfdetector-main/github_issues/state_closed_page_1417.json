[{"number": 10467, "title": "BasicRNNCell comment fix", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 10466, "title": "Windows: Make TensorFlow build without --cpu=x64_windows_msvc", "body": "Since from Bazel 0.5.0, MSVC toolchain became the default toolchain on\r\nWindows. So --cpu=x64_windows_msvc is not required as long as we adjust\r\nthe BUILD files in TensorFlow.\r\n\r\n--cpu=x64_windows_msvc is also supported for now, but should be depracated.\r\nThe configuration for cpu value x64_windows_msvc is a duplicate of\r\nx64_windows, which should be removed in the future.", "comments": ["This may have broken macOS build, could you take a look?", "Yeah, my mistake, fixed now.", "Possible machine issue causing failure.\r\nJenkins, test this please.", "Jenkins, test this please", "Jenkins, test this please", "Known unrelated issues. Merging.", "cmake issues unrelated, merging."]}, {"number": 10465, "title": "Fix AttributeError in resnet.py", "body": "There is no function tf.softmax() in Tensorflow 1.x.\r\n\r\nWhen running the old code, Python interpreter complains:\r\n\r\n  File \"resnet.py\", line 152, in res_net_model\r\n    prediction, loss = res_net(x, y)\r\n  File \"resnet.py\", line 148, in res_net\r\n    return tf.softmax(logits), loss\r\nAttributeError: 'module' object has no attribute 'softmax'", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "Hmm, the CLA didn't seem to work... will hold off on reviewing until it passes."]}, {"number": 10464, "title": "Expected dimension in the range [-1, 1), but got 1", "body": "the error\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\nC:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1021     try:\r\n-> 1022       return fn(*args)\r\n   1023     except errors.OpError as e:\r\n\r\nC:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1003                                  feed_dict, fetch_list, target_list,\r\n-> 1004                                  status, run_metadata)\r\n   1005 \r\n\r\nC:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\contextlib.py in __exit__(self, type, value, traceback)\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n\r\nC:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in raise_exception_on_not_ok_status()\r\n    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 466           pywrap_tensorflow.TF_GetCode(status))\r\n    467   finally:\r\n\r\nInvalidArgumentError: Expected dimension in the range [-1, 1), but got 1\r\n\t [[Node: ArgMax_1 = ArgMax[T=DT_INT32, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_Placeholder_2_0, ArgMax_1/dimension)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-22-5a0d43c01cd7> in <module>()\r\n      1 batch_xs, batch_ys = dm.loadbatch(151,\"data/NoneNematic/validate/NoneNematic\",0)\r\n----> 2 print(sess.run(accuracy, feed_dict={x: batch_xs, z_: batch_ys}))\r\n\r\nC:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    765     try:\r\n    766       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 767                          run_metadata_ptr)\r\n    768       if run_metadata:\r\n    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nC:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    963     if final_fetches or final_targets:\r\n    964       results = self._do_run(handle, final_targets, final_fetches,\r\n--> 965                              feed_dict_string, options, run_metadata)\r\n    966     else:\r\n    967       results = []\r\n\r\nC:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1013     if handle is None:\r\n   1014       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\r\n-> 1015                            target_list, options, run_metadata)\r\n   1016     else:\r\n   1017       return self._do_call(_prun_fn, self._session, handle, feed_dict,\r\n\r\nC:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1033         except KeyError:\r\n   1034           pass\r\n-> 1035       raise type(e)(node_def, op, message)\r\n   1036 \r\n   1037   def _extend_graph(self):\r\n\r\nInvalidArgumentError: Expected dimension in the range [-1, 1), but got 1\r\n\t [[Node: ArgMax_1 = ArgMax[T=DT_INT32, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_Placeholder_2_0, ArgMax_1/dimension)]]\r\n\r\nCaused by op 'ArgMax_1', defined at:\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\r\n    handler_func(fd_obj, events)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-20-1310bf690995>\", line 1, in <module>\r\n    correct_prediction = tf.equal(tf.argmax(z,1), tf.argmax(z_,1))\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 173, in argmax\r\n    return gen_math_ops.arg_max(input, axis, name)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 168, in arg_max\r\n    name=name)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Users\\Owner\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Expected dimension in the range [-1, 1), but got 1\r\n\t [[Node: ArgMax_1 = ArgMax[T=DT_INT32, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_Placeholder_2_0, ArgMax_1/dimension)]]\r\n\r\n\r\nthe code\r\nSTM Data intermediate brain\r\nImport dataset and tensorflow\r\nIn [ ]:\r\n\r\nimport tensorflow as tf\r\nimport math\r\nIn [ ]:\r\n\r\nimport datamaker as dm\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nDefine the Model\r\nIn [ ]:\r\n\r\nNhidden=1024\r\nIMAGE_SIZE = 30\r\nIMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\r\nNUM_CLASSES = 2\r\nIn [ ]:\r\n\r\nx = tf.placeholder(tf.float32, [None, IMAGE_PIXELS])\r\nIn [ ]:\r\n\r\nW = tf.Variable(tf.zeros([IMAGE_PIXELS, Nhidden]))\r\nb = tf.Variable(tf.zeros([Nhidden]))\r\nIn [ ]:\r\n\r\ny = tf.nn.relu(tf.matmul(x, W) + b)\r\nIn [ ]:\r\n\r\nV = tf.Variable(tf.zeros([Nhidden,NUM_CLASSES]))\r\na = tf.Variable(tf.zeros([NUM_CLASSES]))\r\nIn [ ]:\r\n\r\nz = tf.nn.softmax(tf.matmul(y,V)+a)\r\nIn [ ]:\r\n\r\ny_ = tf.placeholder(tf.float32, [None, Nhidden])\r\nIn [ ]:\r\n\r\nz_ = tf.placeholder(tf.int32, [100])\r\nDefine the cost/loss function and training algorithm\r\nIn [ ]:\r\n\r\n#cross_entropy1 = tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=tf.matmul(x, W) + b)\r\nIn [ ]:\r\n\r\n#train_step1 = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy1)\r\nIn [ ]:\r\n\r\ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=z_,logits=tf.matmul(y,V)+a)\r\nIn [ ]:\r\n\r\nloss = tf.reduce_mean(cross_entropy)\r\nIn [ ]:\r\n\r\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\r\nTrain the model\r\nIn [ ]:\r\n\r\nsess = tf.InteractiveSession()\r\nIn [ ]:\r\n\r\ntf.global_variables_initializer().run()\r\nIn [ ]:\r\n\r\nfor b in range(94):\r\n  batch_xs, batch_ys = dm.loadbatch(b,\"data/NoneNematic/train/NoneNematic\",0)\r\n  sess.run(train_step, feed_dict={x: batch_xs, z_: batch_ys})\r\nIn [ ]:\r\n\r\nfor b in range(100):\r\n  batch_xs, batch_ys = dm.loadbatch(b,\"data/Nematic/train/Nematic\",1)\r\n  sess.run(train_step, feed_dict={x: batch_xs, z_: batch_ys})\r\nEvaluate the model\r\nIn [ ]:\r\n\r\ncorrect_prediction = tf.equal(tf.argmax(z,1), tf.argmax(z_,1))\r\nIn [ ]:\r\n\r\nt\r\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.int32))\r\nIn [ ]:\r\n\r\nbatch_xs, batch_ys = dm.loadbatch(151,\"data/NoneNematic/validate/NoneNematic\",0)\r\nprint(sess.run(accuracy, feed_dict={x: batch_xs, z_: batch_ys}))\r\nIn [ ]:\r\n\r\naccuracy.eval({x: batch_xs, z_: batch_ys})\r\n print(\"Accuracy:\", accuracy.eval({x: batch_xs, z_: batch_ys}))\r\nIn [ ]:\r\n\r\nprint(sess.run(z, feed_dict={x:batch_xs}))\r\nprint(sess.run(z, feed_dict={x:batch_xs}))", "comments": ["Hi!\r\nThis question seems best suited for StackOverflow. Could you post your issue there? Also, remember to close your issue if you do so!\r\nThanks!", "Agreed @Dref360 thanks! I am closing for now, please ask on Stackoverflow."]}, {"number": 10463, "title": "Where are quantization dependencies?", "body": "In https://www.tensorflow.org/performance/quantization tutorial, there is //tensorflow/contrib/quantization and //tensorflow/contrib/quantization/kernels required to use a quantized graph.\r\n\r\nI cannot find this in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantization \r\n\r\nHas the location been updated?\r\n\r\n```\r\n# Note: You need to add the dependencies of the quantization operation to the\r\n#       cc_binary in the BUILD file of the label_image program:\r\n#\r\n#     //tensorflow/contrib/quantization:cc_ops\r\n#     //tensorflow/contrib/quantization/kernels:quantized_ops\r\n\r\nbazel build tensorflow/examples/label_image:label_image\r\nbazel-bin/tensorflow/examples/label_image/label_image \\\r\n--image=<input-image> \\\r\n--graph=/tmp/quantized_graph.pb \\\r\n--labels=/tmp/imagenet_synset_to_human_label_map.txt \\\r\n--input_width=299 \\\r\n--input_height=299 \\\r\n--input_mean=128 \\\r\n--input_std=128 \\\r\n--input_layer=\"Mul:0\" \\\r\n--output_layer=\"softmax:0\"\r\n```\r\n", "comments": ["The quantization ops all moved into core tensorflow so you shouldn't need a separate contrib dependency for those ops now. However, I can't vouch for what is needed to make that tutorial work. @petewarden  is anyone maintaining this tutorial?", "The tutorial references the outdated `quantize_graph` tool which is a bit buggy now.\r\nThis should be updated to make use of the new `transform_graph` tool.\r\nI submitted a PR to do so: #10592", "Thanks @androbin! I'll close this bug since that fixes the documentation.", "@petewarden So should the code comments about dependencies be removed as well?\r\n```bash\r\n# Note: You need to add the dependencies of the quantization operation to the\r\n#       cc_binary in the BUILD file of the label_image program:\r\n#\r\n#     //tensorflow/contrib/quantization:cc_ops\r\n#     //tensorflow/contrib/quantization/kernels:quantized_ops\r\n```"]}, {"number": 10462, "title": "[OpenCL] Improves device reporting", "body": "  Prints: id, type, name, vendor and profile of the device", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10461, "title": "Tensor names prepended with \"import/\" when loaded from protobuf", "body": "When I load a (frozen) Tensorflow model from disk using:\r\n\r\n```\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        f = gfile.FastGFile(\"frozen_graph.pb\", \"rb\")\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        tf.import_graph_def(graph_def)\r\n```\r\n\r\nIt seems that all tensor names are prepended with `import/`.\r\nThis is the code I use to print the names:\r\n```\r\nwith tf.Session(graph=graph) as sess:\r\n        all_ops = sess.graph.get_operations()\r\n        op_values =  [op.values() for op in all_ops]\r\n        for values in op_values:\r\n            for each in value:\r\n                print each.name\r\n\r\n```\r\nWhy? Is this some kind of default option that can be overriden? Or can I rely on this in my code? I could not find this documented anyhwere, can anybody point me to references regarding this?\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "That is the default value specified in tf.import_graph_def.\r\nYou can pass in a name argument to use a different prefix.\r\n\r\nhttps://stackoverflow.com/questions/44518790/are-tensor-names-always-prepended-with-import-when-loaded-from-protobuf"]}, {"number": 10460, "title": "tf.import_graph_def() does not return list of tensors or operations", "body": "I read the documentation for `tf.import_graph_def` [here](https://www.tensorflow.org/api_docs/python/tf/import_graph_def).\r\n\r\nAs the documentation says: \r\nReturns: A list of Operation and/or Tensor objects from the imported graph, corresponding to the names in return_elements.\r\n\r\nI tried that with this sample code but the method returns `None` for me.\r\n\r\nI had to run some tests using this feature. But stuck at this issue.\r\n\r\nSample code:\r\n\r\n```python3\r\nimport tensorflow as tf\r\ng = tf.Graph()\r\nwith g.as_default():\r\n    c = tf.constant(30.0)\r\n    b = tf.constant(20.0)\r\n    a = tf.add(b, c)\r\ngdef = g.as_graph_def()\r\nprint(tf.import_graph_def(gdef))```\r\n\r\n", "comments": ["The crucial part of the documentation is \"corresponding to the names in `return_elements`.\" To get the function to return some operations or tensors, you must specify a list names of operations or tensors. If you don't specify `return_elements` when calling `tf.import_graph_def()`, the function will return `None`.\r\n", "Ok got it. Since it did not throw any exception for mandatory parameter, I thought it is optional.\r\n\r\nThanks :)", "Perhaps the documentation is confusing here! It's definitely an optional parameter\u2026 if you don't pass `return_elements` then the import will still happen (as a side effect) but the function won't return anything."]}, {"number": 10459, "title": "//tensorflow/core/kernels:sparse_matmul_op_test_gpu is failing on ppc64le with \"Actual: false but Expected: true\"", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n   Ubuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary)**:\r\n    Installed from source (v1.0.1)\r\n- **TensorFlow version (use command below)**:\r\n    ('v1.0.1-0-ge895d5c-dirty', '1.0.1')\r\n- **Bazel version (if compiling from source)**:\r\n     0.4.4-2017-05-26 (@80a07b5)\r\n- **CUDA/cuDNN version**:\r\n     CUDA = 8.0 and cuDNN = 5.1\r\n- **GPU model and memory**:\r\n    GPU 0: Tesla P100-SXM2-16GB\r\n    GPU 1: Tesla P100-SXM2-16GB\r\n- **Exact command to reproduce**:\r\n     bazel test --config=opt --config=cuda //tensorflow/core/kernels:sparse_matmul_op_test_gpu\r\n\r\n### Describe the problem\r\nThis test is passing successfully on X86, however getting failure on ppc64le with following errors:\r\n\r\n### Source code / logs\r\n```\r\n$  bazel test --config=opt --config=cuda //tensorflow/core/kernels:sparse_matmul_op_test_gpu\r\n\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n-----------------------------------------------------------------------------\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nRunning main() from test_main.cc\r\n[==========] Running 4 tests from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 4 tests from SparseMatmulOpTest\r\n[ RUN      ] SparseMatmulOpTest.BroadcastPacketTest\r\n[0.170094 0.170094 0.170094 0.170094] != [  0.170094    0.14922 -0.0823886   0.026985], differences: [         0 -0.0208738  -0.252482  -0.143109]\r\ntensorflow/core/kernels/sparse_matmul_op_test.cc:257: Failure\r\nValue of: areApprox(ref, data2, PacketSize)\r\n  Actual: false\r\nExpected: true\r\n[  FAILED  ] SparseMatmulOpTest.BroadcastPacketTest (0 ms)\r\n[ RUN      ] SparseMatmulOpTest.InterleavePacketTest\r\n[       OK ] SparseMatmulOpTest.InterleavePacketTest (0 ms)\r\n[ RUN      ] SparseMatmulOpTest.Bfloat16ExpandTest\r\n[       OK ] SparseMatmulOpTest.Bfloat16ExpandTest (0 ms)\r\n[ RUN      ] SparseMatmulOpTest.Bfloat16LoadTest\r\n[       OK ] SparseMatmulOpTest.Bfloat16LoadTest (0 ms)\r\n[----------] 4 tests from SparseMatmulOpTest (0 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 4 tests from 1 test case ran. (0 ms total)\r\n[  PASSED  ] 3 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] SparseMatmulOpTest.BroadcastPacketTest\r\n\r\n 1 FAILED TEST\r\n```\r\nI went through the file tensorflow/core/kernels/sparse_matmul_op_test.cc to find the root cause, but couldn't understand the code. Please provide your suggestions/comments on this.Thanks!\r\n", "comments": ["Marking community support since we don't directly support PPC builds.", "Started looking into the issue starting from the file tensorflow/core/kernels/sparse_matmul_op_test.cc\r\nAs per initial analysis found that following code returning incorrect results on ppc64le https://github.com/tensorflow/tensorflow/blob/v1.0.1/tensorflow/core/kernels/sparse_matmul_op_test.cc#L255-L256 \r\n\r\n```\r\nTEST_F(SparseMatmulOpTest, BroadcastPacketTest) {\r\n  for (int i = 0; i < PacketSize; ++i) ref[i] = data1[0];\r\n  internal::pstoreu(data2, internal::pbroadcast_first<Packet>(\r\n                              internal::ploadu<Packet>(data1)));\r\n   ASSERT_TRUE(areApprox(ref, data2, PacketSize));\r\n\r\n```\r\n\r\nHere we are getting incorrect results for data2 array (`[  0.170094    0.14922 -0.0823886   0.026985]` VS expected `[0.170094 0.170094 0.170094 0.170094]`).\r\nCurrently I am investigating further on this. ", "With a little more investigation I found the following things : \r\n https://github.com/tensorflow/tensorflow/blob/v1.0.1/tensorflow/core/kernels/sparse_matmul_op_test.cc#L255-L256\r\n```\r\n internal::pstoreu(data2, internal::pbroadcast_first<Packet>(\r\n                              internal::ploadu<Packet>(data1)));\r\n```\r\n\r\nFor `internal::pbroadcast_first<Packet>`  line, on both the platform executed different functions, see below-\r\n \r\nOn x86 executed  `EIGEN_STRONG_INLINE Packet4f pbroadcast_first<Packet4f>(const Packet4f& a)` function (https://github.com/tensorflow/tensorflow/blob/v1.0.1/tensorflow/core/kernels/sparse_matmul_op.h#L197)\r\nAnd on ppc64le executed `EIGEN_DEVICE_FUNC inline Packet pbroadcast_first(const Packet& a)` function (https://github.com/tensorflow/tensorflow/blob/v1.0.1/tensorflow/core/kernels/sparse_matmul_op.h#L92)\r\n\r\nThat is why our test case is failing on ppc64le. Investigating this further", "Fixed in PR https://github.com/tensorflow/tensorflow/pull/12138"]}, {"number": 10458, "title": "Transform_graph android error ", "body": "Hi,\r\nOne month back I generated my custom TF model (output_graph.pb ) using Tensor Flow 1.0.1. It was working fine after optimization it using optimize_for_interface.\r\n\r\nNow I plan to reduce its size and improve execution speed, I downloaded Tensor Flow 1.2.0.  I used transform_graph as \r\n bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=./output_graph.pb \\\r\n--out_graph=./transformed_graph.pb \\\r\n--inputs='Mul' \\\r\n--outputs='final_result' \\\r\n--transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  quantize_nodes\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n\r\nI built the APK and ran on a Lenovo Yoga 3 tablet.\r\nIt generated a run time error:\r\n\r\nW/native  (24951): op_kernel.cc:1165 Invalid argument: computed output size would be negative\r\nE/TensorFlowInferenceInterface(24951): Failed to run TensorFlow inference with inputs:[Mul], outputs:[final_result]\r\n--------- beginning of crash\r\nE/AndroidRuntime(24951): FATAL EXCEPTION: inference\r\nE/AndroidRuntime(24951): Process: org.tensorflow.demo, PID: 24951\r\nE/AndroidRuntime(24951): java.lang.IllegalArgumentException: computed output size would be negative\r\nE/AndroidRuntime(24951): \t [[Node: pool_3/eightbit = QuantizedAvgPool[T=DT_QUINT8, ksize=[1, 8, 8, 1], padding=\"VALID\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](mixed_10/join/eightbit, mixed_10/join/eightbit:1, mixed_10/join/eightbit:2)]]\r\nE/AndroidRuntime(24951): \tat org.tensorflow.Session.run(Native Method)\r\nE/AndroidRuntime(24951): \tat org.tensorflow.Session.access$100(Session.java:48)\r\n\r\nIn both the cases, the Bazel version is 0.4.5\r\nAny help to solve this? \r\n\r\n\r\n\r\n", "comments": ["I am waiting to see a reply.", "Running into the same error, by any chance did you change the `INPUT_SIZE`, `IMAGE_MEAN`, and `IMAGE_STD`?", "I changed the image size to IMAGE_SIZE = 299, IMAGE_MEAN = 128, IMAGE_STD = 128.\r\nNow the crash disappeared but the results are biased towards one of two total classes.\r\nThis incorrect results appears in 1.0,1 version of transform_graph too.\r\nAny help to rectify this bias?", "Spent a lot time to choose a Transform Graph function combination to reduce the size and run time of my custom graph. \r\nBut no option works for me to do both using: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md\r\nThe most promising one was the above combination which I tried, but it generates incorrect results.\r\nCould you recommend any Transform Graph combination that would reduce size as well as run time. Thanks. ", "It's hard to tell what the underlying cause is here. Can you try removing some of the transforms to narrow it down to which one is causing your inaccuracies?", "    \"quantize_weights\"\r\n    \"quantize_nodes\" \r\ncause the inaccuracies. Keeping any of them cause incorrect results.\r\nwhen I remove both of them, the results seems to be correct, but there is no performance gain in terms of size / run time.\r\nFYI, I used a Lenovo Phab 2 android device. The general tensorflow_inception_graph model runs @ 300 MS / frame, but my custom model without quantization takes 1.5 S/ Frame. Also, without quantization, the APK size is 89 MB and with it, it is 20 MB but showing incorrect results.     \r\nAny suggestion to improve run time as well as size of my TF custom model? ", "@petewarden \r\nHope the above information is sufficient for you to provide me a direction to make the optimization work.\r\nIf you need more information, I can provide you. Thanks.\r\n", "I recently added a change that may help in this case, though I'm not certain. I increased the threshold for how large a buffer needs to be before it's quantized, from 16 up to 1024. I also made it a configurable parameter:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/quantize_weights.cc#L40\r\n\r\nThis isn't in a release yet, but if you build the latest TensorFlow from source you can give it a try.", "You just need to rename the created txt file from retraining and paste it in the assets directory, the created txt file is not the same name as the default txt file, if so will result to incorrect output, both files \"created.txt and default.txt\" should be on the same directory on android build and the LABEL_FILE should be pointing to created txt file from retraining you renamed.", "hello guys, currently experiencing on this. do u have any ideas?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm hoping this is now fixed, since there hasn't been activity on this for some time. Closing, but please reopen if you're still hitting this.", "@santle @petewarden @mandemeskel \r\nFacing somewhat similar problem, same problem.\r\nAny suggestion will be helpful.\r\n\r\n\r\n```\r\nInvalid argument: Computed output size would be negative: -32 [input_size: 84, effective_filter_size: 117, stride: 1]\r\nInvalid argument: Computed output size would be negative: -32 [input_size: 85, effective_filter_size: 118, stride: 1]\r\nInvalid argument:Computed output size would be negative: -32 [input_size: 86, effective_filter_size: 119, stride: 1]\r\nFailed to run TensorFlow inference with inputs:[inputTensor, dropout_keep_prob], outputs:[output/softmax]\r\n```\r\n\r\n"]}, {"number": 10457, "title": "Fix linking errors of lmdb on Windows", "body": "http://ci.tensorflow.org/job/tf-master-win-bzl/1041/console\r\nThe error was introduced in https://github.com/tensorflow/tensorflow/commit/e6f58186363279496c46563e6f065ce7ea16c501\r\n\r\n```\r\n23:09:48 liblmdb.a(mdb.o) : error LNK2019: unresolved external symbol __imp_InitializeSecurityDescriptor referenced in function mdb_env_setup_locks\r\n23:09:48 liblmdb.a(mdb.o) : error LNK2019: unresolved external symbol __imp_SetSecurityDescriptorDacl referenced in function mdb_env_setup_locks\r\n```\r\n@gunan ", "comments": ["http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/22/console\r\nThe build succeeded, but has some test failures.", "Ill take care of the test failures\r\n"]}, {"number": 10456, "title": "Default CUDA / cuDNN version not used in template string", "body": "When leaving version for CUDA / cuDNN empty, to default to 8.0 / 6, they are not properly used in the template at marks **_blank_**\r\n\r\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]:\r\nPlease specify the location where CUDA **_blank_** toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]:\r\nPlease specify the location where cuDNN **_blank_** library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nInstead, there is just a two-space gap.\r\nThe script seems to miss a proper fallback for `TF_CUDA_VERSION` / `TF_CUDNN_VERSION` unset.", "comments": ["@yifeif can you fix this while rewriting configure?", "This was from #10307. I'll send a fix. cc @gunan FYI.", "@yifeif This doesn't seem resolved yet?\r\n\r\nWe probably should set defaults\r\n```\r\n# Set default CUDNN version if not set\r\nif [ -z \"$TF_CUDNN_VERSION\" ]; then\r\n  TF_CUDNN_VERSION=\"6\"\r\n  export TF_CUDNN_VERSION\r\nfi\r\n```\r\n\r\nright after assigning them explicitly\r\n\r\n```\r\n# Configure the cuDNN version to use.\r\nif [ -z \"$TF_CUDNN_VERSION\" ]; then\r\n  read -p \"Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: \" TF_CUDNN_VERSION\r\nfi\r\n```\r\n\r\nThe same applies to other variables as well.", "@Androbin thanks for the ping. It should be fixed now. Could you try the latest master?", "@yifeif Seems to work just fine now!"]}, {"number": 10455, "title": "Update `configure` script sample", "body": "The `configure` script was changed regularly since the generation of the sample.\r\nThis PR updates the sample to reflect those changes. ", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 10454, "title": "SavedModelBundle.java's load() method is not working with RandomForest Saved Model ", "body": "------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS 10.12\r\n- **TensorFlow installed from (source or binary)**: Binary \r\n- **TensorFlow version (use command below)**: Tried with both 1.1.0 and 1.2.0-rc1\r\n- **CUDA/cuDNN version**: Running on CPU\r\n\r\nI have SavedModel using TensorForestEstimator (RandomForest) by using export_savedmodel() it was saved successfully and able to load from SavedModel-cli too. But when trying to load using \r\n\r\n```\r\nval SAVED_MODEL_PATH =\r\n      \"/Users/Documents/tensordata/1496652787\";\r\n    val bundle = SavedModelBundle.load(SAVED_MODEL_PATH, \"serve\");\r\n```\r\n It's giving following error -:\r\n\r\n```\r\nException in thread \"main\" org.tensorflow.TensorFlowException: Op type not registered 'TreePredictions'\r\n\tat org.tensorflow.SavedModelBundle.load(Native Method)\r\n\tat org.tensorflow.SavedModelBundle.load(SavedModelBundle.java:38)\r\n\tat ai.tenten.ml.ClassifyITCase$.main(ClassifyITCase.scala:59)\r\n```\r\nI am able to load Other saved models like HalfPlusTwo Regression and mnist dense Predict model successfully. Only in RandomForest It's giving error I thing some op is missing from build or Java API is not updated.", "comments": ["@shashank734 : Alas, at this time, any ops defined in the Python `tf.contrib` namespace are not supported in the release binaries of other languages. This stems from the fact that these ops are implemented in their own binaries (in their own shared libraries) and we currently do not support loading custom ops.\r\n\r\nThat's something we're looking into (making custom operations in `tf.contrib` available to other binary distributions), but there is no timeline on that.\r\n\r\nIn the mean time, if you're willing to build from source, I can suggest a workaround, which would involve compiling from source. Something like:\r\n\r\n1. Checkout the source and install bazel:\r\n\r\n```sh\r\ngit checkout github.com/tensorflow/tensorflow\r\ncd tensorflow\r\ngit checkout r1.2 # Just so that you're in the release branch\r\n\r\nbrew install bazel\r\n```\r\n\r\n2. Setup the build system to statically link the contrib ops into the JNI library by adding:\r\n\r\n```\r\n        \"//tensorflow/contrib/tensor_forest:tensor_forest_ops_op_lib\",\r\n        \"//tensorflow/contrib/tensor_forest:tensor_forest_kernels\",\r\n```\r\n\r\nto the `deps` section of the [`libtensorflow_jni.so` build target](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/java/BUILD#L163)\r\n\r\n3.  Build the JNI library:\r\n\r\n```sh\r\nyes \"\" | ./configure\r\nbazel build -c opt //tensorflow/java:libtensorflow_jni.so\r\n```\r\n\r\nNow, you'd want to make this library instead of the one packaged in the release binary distributions. You can do that by adding `-Djava.library.path=bazel-bin/tensorflow/java` to the `java` command.\r\n\r\nDo let us know if this works or not.\r\n\r\nHope that helps.\r\n\r\n(FYI @allenlavoie @girving @martinwicke )\r\n\r\n", "I will try this. But still wait for changes before deploying to production.", "This should now be doable with TensorFlow 1.4. See https://stackoverflow.com/questions/47276624/issue-while-loading-serving-tensorflow-model-in-java-using-estimators/47277367#47277367\r\n\r\nYou'll have to adjust the arguments to `TensorFlow.loadLibrary()` to correspond to the shared libraries included in the `tf.contrib.tensor_forest` module.\r\n\r\nI'm closing this bug out since I believe the process described in the stackoverflow thread will work (albeit it could be made smoother). Please feel free to re-open if you run in to trouble or have comments.\r\n\r\nThanks! \r\n\r\n"]}, {"number": 10453, "title": "Fix misspells on comments", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 10452, "title": "Fix patching issue on Windows", "body": "Fix the issue in https://github.com/tensorflow/tensorflow/issues/10435\r\n@gunan ", "comments": ["I think I was too hasty merging this PR, now we have the merge issue on master.", "Thanks for taking care of this, looks like this commit is the wrong solution."]}, {"number": 10451, "title": "how to restore a tensor defined in a dictionary?", "body": "I wrote the following definitions in my program.\r\n`first_layer = {'w_1': tf.Variable(tf.truncated_normal([600, 300]), name='w_1'), 'b_1':tf.Variable(tf.truncated_normal([300]), name='b_1')}\r\n`\r\nAnd I use the following two lines to save my trained model.\r\n`   saver = tf.train.Saver()\r\n    saver.save(sess, os.path.join(model, 'model'))`\r\nHow can I restore the tensor with the name w_1 ?\r\nThe following codes didn't work out.\r\n`  saver = tf.train.import_meta_graph(os.path.join(c_dir, 'model.meta'))\r\n    sess.run(tf.global_variables_initializer())\r\n    saver.restore(sess, tf.train.latest_checkpoint(c_dir))\r\n   graph = tf.get_default_graph()\r\n    w_1 = graph.get_tensor_by_name(\"w_1:0\")`", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10450, "title": "//tensorflow/core:platform_profile_utils_cpu_utils_test is failing on ppc64le with \"Check failed: cpu_frequency > 0 (-1 vs. 0)\"", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n    Ubuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary)**:\r\n    Installed from source (v1.0.1)\r\n- **TensorFlow version (use command below)**:\r\n   ('v1.0.1-0-ge895d5c-dirty', '1.0.1')\r\n- **Bazel version (if compiling from source)**:\r\n    0.4.4-2017-05-26 (@80a07b5)\r\n- **CUDA/cuDNN version**:\r\n    CUDA = 8.0 and cuDNN = 5.1\r\n- **GPU model and memory**:\r\n    GPU 0: Tesla P100-SXM2-16GB\r\n    GPU 1: Tesla P100-SXM2-16GB\r\n- **Exact command to reproduce**:\r\n   bazel test --config=opt --config=cuda  //tensorflow/core:platform_profile_utils_cpu_utils_test\r\n\r\n### Describe the problem\r\nThis test is passing successfully on X86, however getting failure on ppc64le with following errors:\r\n\r\n### Source code / logs\r\n```\r\n$ bazel test --config=opt --config=cuda  //tensorflow/core:platform_profile_utils_cpu_utils_test\r\n\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n-----------------------------------------------------------------------------\r\nRunning main() from test_main.cc\r\n[==========] Running 5 tests from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 5 tests from CpuUtilsTest\r\n[ RUN      ] CpuUtilsTest.SetUpTestCase\r\n[       OK ] CpuUtilsTest.SetUpTestCase (0 ms)\r\n[ RUN      ] CpuUtilsTest.TearDownTestCase\r\n[       OK ] CpuUtilsTest.TearDownTestCase (0 ms)\r\n[ RUN      ] CpuUtilsTest.CheckGetCurrentClockCycle\r\n[       OK ] CpuUtilsTest.CheckGetCurrentClockCycle (0 ms)\r\n[ RUN      ] CpuUtilsTest.CheckCycleCounterFrequency\r\nF tensorflow/core/platform/profile_utils/cpu_utils_test.cc:56] Check failed: cpu_frequency > 0 (-1 vs. 0)\r\nexternal/bazel_tools/tools/test/test-setup.sh: line 123:  7285 Aborted                 (core dumped) \"${TEST_PATH}\" \"$@\"\r\n```\r\n\r\nAny comments/suggestions ?", "comments": ["What is the output of\r\n`grep '^bogomips' /proc/cpuinfo | head -1`", "Getting something on X86 : \r\n```\r\n$ grep '^bogomips' /proc/cpuinfo | head -1\r\nbogomips        : 4799.99\r\n```\r\nHowever nothing printed on ppc64le : \r\n\r\n`$ grep '^bogomips' /proc/cpuinfo | head -1`", "Unfortunately we don't directly support the PPC platform, so there may be a bunch of issues like this. Marking community support in case somebody is willing to take on the attempt of getting it to work on that platform.", "I went through the file tensorflow/core/platform/profile_utils/cpu_utils_test.cc and found that GetCycleCounterFrequency function returning incorrect result on ppc64le,  expected value is greater than` \"0\"`, however on ppc64le getting `\"-1\"`  (see below for test code)\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.0.1/tensorflow/core/platform/profile_utils/cpu_utils_test.cc#L56\r\n\r\n```\r\nTEST_F(CpuUtilsTest, CheckCycleCounterFrequency) {\r\n  const int64 cpu_frequency = CpuUtils::GetCycleCounterFrequency();\r\n  CHECK_GT(cpu_frequency, 0);\r\n  CHECK_NE(cpu_frequency, CpuUtils::INVALID_FREQUENCY);\r\n  if (DBG) {\r\n    LOG(INFO) << \"Cpu frequency = \" << cpu_frequency;\r\n  }\r\n}\r\n\r\n```\r\n\r\nChecked the output of `CpuUtils::GetCycleCounterFrequency()` using print statement i.e. `const int64 cpu_frequency = CpuUtils::GetCycleCounterFrequency();`\r\n\r\nOn x86 `cpu_frequency` = `239999500`\r\nOn ppc64le `cpu_frequency` = `-1`\r\n\r\nI would like to explore more on this. Please provide comments/suggestions if any. Thanks!", "After investigation found that, \"int64\" value on x86 is \"unsigned\" by default whereas on ppc64le it is \"signed\" by default, that is why this test was failing on ppc64le.\r\n\r\nTo make the results compatible with x86, some code changes were required on ppc64le.\r\nAfter code changes, I ran this test and now it is passing on ppc64le as well as on X86.\r\n\r\nCode changes submitted for review in PR https://github.com/tensorflow/tensorflow/pull/10522  to fix this.\r\nOnce approved I will push the changes to Github so PR https://github.com/tensorflow/tensorflow/pull/10522 should resolve this.\r\n", "Community accepted the changes and merged in master (relevant PR #10522 and merged commit d0d2308). Hence closing this issue as resolved."]}, {"number": 10449, "title": "[Feature]Adding automatic model average parallelism support in TF", "body": "Model Average is a common paradigm for distributed DL training, also there are several papers regarding to it and its variants:\r\n\r\n(https://arxiv.org/pdf/1410.7455v8.pdf)\r\n(http://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/0005880.pdf)\r\n\r\nDo we have any plan to bring this support into TF? Actually we have already implemented model average support and benchmarked it on several in-house models, the speed up is good, about 3X(4 cards) to 9X(16 cards) convergence speed-up. Also we are working on making the model average mechanism as automatic as possible, so modeling guys could easily leverage this nice feature.\r\n\r\nIf TF community is fine with this feature/enhancement, we will be happy to merge our code into community.\r\n\r\nThanks.\r\n\r\n", "comments": ["@sherrym who is a good contact for figuring out how to work on this contribution?", "Interesting work! In the papers you posted it seems that's a new synchronization mechanism to reduce cross device communication cost.\r\n\r\nAs a side note, does your implementation change the core/distributed runtime or it is currently implemented above TF API (e.g. using Python)? Any significant differences from the synchronization methods mentioned [here](https://www.tensorflow.org/performance/performance_models#variable_distribution_and_gradient_aggregation)?\r\n\r\nI'm just a little worried if that brings significant changes to the distributed runtime. The runtime is currently oblivious to the semantic of tensors it send/recv. ", "@byronyi \r\nCurrently, our implementation only involve Python-level change since we formulate this problem as a graph transformation problem so as to ensure our code change is orthogonal to low-level changes.", "I don't know about other members but as for myself I would love to see new synchronisation mechanism beyond the common ones we have right now (parameter server, distributed replicated using all-reduce, etc.). \r\n\r\nLooking forward to your patch.", "@yangjunpro I recently implemented the distributed DL training method based on the paper you mentioned. Here is the details: #11071 .\r\nBut there are some problems with my implementation: the training loss decreases at the begining but will increases dramatically suddenly after the ps params do several updates. \r\nCould you share some ideas of your implementation? Thanks.", "@hellolovetiger You could take a look at the PR in https://github.com/tensorflow/tensorflow/pull/11581", "Cool, thanks, @yangjunpro .", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing this since the referenced PR is closed. Please reopen if there's something that should be done."]}, {"number": 10448, "title": "Updating tag to 1.2.0-rc2.", "body": "", "comments": []}, {"number": 10447, "title": "Tensorflow Errors with uint8 addition", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.1\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n8.0\r\n- **GPU model and memory**:\r\nNvidia Titan X Pascal \r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\na=tf.constant([1],dtype=tf.uint8)\r\nb=tf.constant([2],dtype=tf.uint8)\r\nc=a+b\r\nsession=tf.Session()\r\nwith session.as_default():\r\n    session.run(c)\r\n```\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nIt appears as thought uint8 addition is not supported.  This is odd.  When running the above code I get the error:\r\n\r\n```\r\nInvalidArgumentError: No OpKernel was registered to support Op 'Add' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:\r\n  device='CPU'; T in [DT_STRING]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n\r\n\t [[Node: add = Add[T=DT_UINT8](Const, Const_1)]]\r\n\r\n```\r\n", "comments": ["The documentation claims that uint8 addition is supported.\r\nhttps://www.tensorflow.org/api_docs/python/tf/add", "@cwhipkey do you know why the docs and op registration differ?", "The documentation comes from the op registration in core/ops/math_ops.cc, which uses a macro.  It seems from the history that uint8 was added to the macro as part of enabling uint8 for Div.\r\n\r\nThe kernel registrations are in core/kernels/cwise_op_add_1.cc and core/kernels/cwise_op_add_2.cc (split into separate registrations across of the files to speed up compilation).  None of the REGISTER calls refer to uint8.\r\n\r\nWe can either fix the docs or add the missing registration.", "Is someone working on this issue? If not then I would like to take this up and add the missing registration.", "No one is working on this currently as far as I know, so it would be great if you could take it!  Feel free to take it and add the missing registration (and update the test for the op(s) to include a test of the newly registered type)."]}, {"number": 10446, "title": "How to add a layer about activation function in tensorflow?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Closing since the template was not filled out: please open again with the issue!"]}, {"number": 10445, "title": "Cherrypicks for 1.1.0-rc2", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 10444, "title": "Fix CMD in Dockerfile", "body": "Currently Notebook fails execution because default user for this container is root, and unless explicitly allowed, jupyter notebook will not start.", "comments": ["Can one of the admins verify this patch?", "What changed that this no longer works?", "And does this mean we need to rebuild the existing containers?", "I'm not sure when it changed, however this file already has this change https://github.com/tensorflow/tensorflow/blob/bc4101eb4d1d088faaadb68991e4c177bdf249bd/tensorflow/tools/docker/Dockerfile#L69\r\n\r\nEven if registry has new container that this one inherits FROM, CMD is being overridden so we need to change it here too.\r\n\r\nExisting containers should be fine, if they worked they'll keep working.", "Jenkins, test this please", "I'll need your help guys debugging this gate failure...can't see anything related to my change", "Looks unrelated (a github failure).\r\n\r\nJenkins, test this please"]}, {"number": 10443, "title": "How to do element wise operation for two Tensors?", "body": "Say, I have two Tensors A and B, both with shape [-1, m, d]. How could I get a Tensor C with shape [-1, m*(m-1)/2, d] so than C_ij = A_i + B_j? Where + is an element wise addition. I know there is an element-wise operation named tf.add, but how should I use it? Thanks a lot.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10442, "title": "Branch 158053084", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 10441, "title": "Support for `dtype` keyword in tf.layers.*", "body": "This pull request introduces the `dtype` keyword in functional interfaces for core layers to solve #9898. All the unit-tests seem to run fine on my machine after incorporating the changes. The default value has been kept as tf.float32 in order to ensure compatibility.", "comments": ["Can one of the admins verify this patch?", "We have merged yesterday a change that allows fp16/64 support in all core layers where it applies.", "Ah, yeah, closing for now. The open-source code should have those dtype updates when I push later today.", "As far as I know, for different precision-level dtypes, there are some tricks taking into consideration.\r\nFor example, for fp16, the loss may needs to be adjusted to compensate for the potential accuracy degradation.  \r\nSo I think a straight-forward template-based data type enhancement may not be enough.", "So what's the situation with this now? Not seeing any `dtype` kwargs (in conv layers at least)...\r\n\r\nIs it safe to copy and modify the conv layers to (optionally) use `float16`?", "@yangjunpro Providing this functionality would still be useful.  As often the case, it's up to the user to verify that a change to the model doesn't hurt performance, and many models don't require loss scaling to still be effective with float16 (https://devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/)."]}, {"number": 10440, "title": "Branch 158034419", "body": "", "comments": ["@tensorflow-jenkins test this please", "Closing PR. Please use https://github.com/tensorflow/tensorflow/pull/10442 instead."]}, {"number": 10439, "title": "Different timelines on QueueDequeManyV2 in cpu and gpu installations", "body": "OS Platform and Distribution: Ubuntu 14.04\r\nTensorFlow version: \r\n    cpu: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\r\n    gpu:  ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\r\n\r\nCUDA/cuDNN: 8.0/5.0\r\n\r\nenv: https://pastebin.com/f70fQ9Mw\r\n\r\nI'm trying to fill queue of input data in additional thread in python, but obtain very strange timings for this operation, different for cpu and gpu version in 20 times. as you can see in full script, i forced device to cpu \r\n\r\nfull script i'm using to fill queue: https://pastebin.com/Ny1QVZh2\r\n\r\nkey part:\r\n```\r\n    enqueue_op = queue.enqueue_many([queue_input_data, queue_input_target])\r\n    dequeue_op = queue.dequeue()\r\n \r\n    data_batch, target_batch = tf.train.batch(dequeue_op, batch_size=batch_size, capacity=10 * batch_size)\r\n \r\n    def enqueue(sess):\r\n        while True:\r\n            sess.run(enqueue_op, feed_dict={queue_input_data: data, queue_input_target: target})\r\n\r\n...\r\n\r\n    run_options = tf.RunOptions(timeout_in_ms=4000, trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    sess.run(\r\n        target_batch,\r\n        options=run_options,\r\n        run_metadata=run_metadata)\r\n \r\n    tl = timeline.Timeline(run_metadata.step_stats)\r\n    ctf = tl.generate_chrome_trace_format()\r\n    with open('timeline.json', 'w') as f:\r\n        f.write(ctf)\r\n```\r\nSteps to reproduce:\r\n```\r\n# cpu only version\r\npip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.1.0-cp27-none-linux_x86_64.whl\r\npython read_feed_dict.py\r\nmv timeline.json timeline_cpu_version.json\r\n\r\npip uninstall tensorflow\r\n\r\n# gpu version\r\npip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.1.0-cp27-none-linux_x86_64.whl\r\npython read_feed_dict.py\r\nmv timeline.json timeline_gpu_version.json\r\n```\r\ncpu timeline:\r\n![cpu](https://cloud.githubusercontent.com/assets/1593310/26796368/f636b37e-4a31-11e7-8851-863e74643697.png)\r\n\r\ngpu timeline:\r\n![gpu](https://cloud.githubusercontent.com/assets/1593310/26796369/f640d688-4a31-11e7-989d-197a7f2499e7.png)\r\n\r\n\r\n\r\n\r\n", "comments": ["It is interesting that this script runs much faster on CPU for a build with GPU enabled. \r\n\r\nGunan, are we building the CPU build with different compiler options? ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I am so sorry for missing this issue for so long.\r\nCPU and GPU build both use the same compiler flags. So I am not sure what may be causing the timing difference.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @zheng-xq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 10438, "title": "Android TF Classify: Reduce input size ", "body": "I'm trying to reduce input size in ClassifierActivity, but I get ` java.lang.IllegalArgumentException: Input to reshape is a tensor with 128 values, but the requested shape requires a multiple of 2048`\r\n\r\n```\r\n  private static final int INPUT_SIZE = 96; // original 224\r\n  private static final int IMAGE_MEAN = 117;\r\n  private static final float IMAGE_STD = 1;\r\n  private static final String INPUT_NAME = \"input\";\r\n  private static final String OUTPUT_NAME = \"output\";\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/ClassifierActivity.java#L61\r\n\r\nWhat should I do to reduce it? (my cropped image is also 96x96)\r\n\r\nAlso do I have to change `IMAGE_MEAN` ? Not sure what it means", "comments": ["_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Ovefrlow._\r\n\r\nCan I confirm that you are using a new model? You aren't just trying to change in input size? Can you post some more information about what you are doing?", "model downloaded by default: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/download-models.gradle\r\n\r\nI wanted to know how to reduce input image to speed-up processing?\r\n\r\nI did something like this for tiny yolo (TF Yolo Detector), I reduced input resolution (96x96) to get more fps (of course it reduces quality for recognition but I also use different methods to crop interested parts of image for processing, not just whole frame from camera)", "Yeah that won't work if you're using the default model. You have to give it the correct image size otherwise it will fail. CNNs carefully construct their architectures for a certain image size, you can't give them any size usually without some form of resize of reshape happening. If you read up CNNs hopefully you'll better understand the issue and ways to get around it. Sorry.", "It's ok, thanks. I expected that it would be something like this. So this is why I prefer to use more yolo detector than other demos for android devices", "I'm pretty sure yolo just scales the image back up/down to the correct size. ", "Hm.. I guess for yolo it can be any px size that can be divided by 32 as I know (64x64, 96x96, 416x416)\r\nand when reducing this input size we get more fps\r\nso there is difference (the smaller images the faster but less accuracy of course)", "Hmm fair enough.", "@anonym24 That's correct, the default Inception model requires a 224x224 input. Closing as this is neither a bug nor feature request."]}]