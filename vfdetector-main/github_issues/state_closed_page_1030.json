[{"number": 22430, "title": "TensorFlow C++ API on ARM processor issue", "body": "System information\r\n***Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n   No\r\n***OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n   Linux Kernel 4.4.103, Linaro 5.4.0\r\n***TensorFlow installed from (source or binary):\r\n   Cross compilation in Ubuntu 16.04 and install the C++ library (libtensorflow.so + protobuf + nsync) in \r\n   the ARM development board\r\n***TensorFlow version (use command below):\r\n   1.10.1\r\n***Python version:\r\n  3.5\r\n***CUDA/cuDNN version:\r\n  Only CPU no GPU\r\n***Exact command to reproduce:\r\n  Session->Run(feed_dict, {\"num_detections:0\", \"detection_boxes:0\", \r\n              \"detection_scores:0\", \"detection_classes:0\"}, {}, &outputs)\r\n\r\nDescribe the problem\r\n   Trained Detection API mobileNetSSD model and saved as a 'pb' file.  Cross compiled the TensorFlow on the PC and installed the binary  (libtensorflow.so + protobuf + nsync) to the ARM board.  The code can be successfully built on the ARM side.  The pb file can be successfully read and loaded into a graph in a session.  However, when trying to run the session by feeding a tensor, the following error message came up:\r\n\r\n2018-09-20 18:28:38.852657: F tensorflow/core/framework/tensor.cc:657] Check failed: in_size != 0 (0 vs. 0)\r\n\r\nIt refers to Tensor::UnsafeCopyFromInternal function Line 3:  CHECK_NE(in_size, 0), and in_size =DataTypeSize(other.dtype()), which is unlikely to be zero.  \r\n\r\nBTW, these message also came up when loading the 'pb' file:\r\n2018-09-20 18:28:37.864057: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"DecodeProtoV2\" device_type: \"CPU\"') for unknown op: DecodeProtoV2\r\n2018-09-20 18:28:37.865515: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"EncodeProto\" device_type: \"CPU\"') for unknown op: EncodeProto", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nGPU model and memory\nMobile device", "@RothLuo  sounds like there is data conversion issue related to precision when feeding a tensor. You may want to check floating point precision on your ARM, which may not be compatible with TensorFlow. \r\n\r\nThe error messages that came up when loading the 'pb' file can be ignored. ", "Closing this for now, feel free to open a new issue if encountering any errors."]}, {"number": 22429, "title": "Add MatchingFilesDatasetOp to improve performance of Dataset.list_files", "body": "This PR aims to add `MatchingFilesDatasetOp` to improve the performance of `Dataset.list_files`, which needs to scan the entire directory tree and load the filenames into memory before the next operation starts. More details can be found at this issue (#17810).\r\n\r\n`MatchingFilesDatasetOp` could yield the matching files in a sorted order.\r\n\r\nThe performance experiments run using [the script](https://gist.github.com/darrengarvey/ff05fbe28ab2061c101fe64353b467ff) posted by @darrengarvey , where `width` is the number of dirs at each level, and `depth` is the number of nested dirs.\r\n\r\nHere are the initial experiment results:\r\n\r\nwhen `width` = 1000 and `depth` = 2:\r\n-- MatchingFilesDataset:\r\n```\r\nread first filename: time: 16.06 ms\r\nread second filename: time: 0.52 ms\r\nread 998 more filenames: time: 336.51 ms (0.34 ms per iteration)\r\n```\r\n-- Dataset.list_files():\r\n```\r\nread first filename: time: 253.91 ms\r\nread second filename: time: 0.27 ms\r\nread 998 more filenames: time: 135.77 ms (0.14 ms per iteration)\r\n```\r\nwhen `width` = 1000 and `depth` = 20:\r\n\r\n-- MatchingFilesDataset:\r\n```\r\nread first filename: time: 22.16 ms\r\nread second filename: time: 3.09 ms\r\nread 998 more filenames: time: 2269.14 ms (2.27 ms per iteration)\r\n```\r\n-- Dataset.list_files():\r\n```\r\nread first filename: time: 1860.96 ms\r\nread second filename: time: 0.38 ms\r\nread 998 more filenames: time: 127.23 ms (0.13 ms per iteration)\r\n```\r\n\r\nThe performance test-related functions (e.g., `testPerformance`) in `tensorflow/python/data/kernel_tests/matching_files_dataset_op_test.py` will be removed later.", "comments": ["Thank you for the contribution @feihugis. \r\n\r\nIs there a particular reason you chose to implement your logic as a dataset (as opposed to providing a more efficient implementation of the `MatchingFilesOp` kernel)? It makes more sense to me to provide an efficient MatchingFilesOp kernel. That way, your solution will support shuffling (which it currently does not) and will benefit other users of the `MatchingFilesOp` kernel.", "@jsimsa Thanks for your comments.\r\n\r\nThe issue #17810 is that `Dataset.list_files` needs to wait until the `MatchingFilesOp` scans all the files (@darrengarvey @rohan100jain). To solve this issue, this implementation adopted the idea from this [comment](https://github.com/tensorflow/tensorflow/issues/17810#issuecomment-379080460) by @mrry. In my understanding, the reason for implementing it as a dataset is that the iterator can be used over the dataset to yield the results without waiting.  Regarding the shuffling, `ShuffleDataset` Op could be used to shuffle the dataset proposed in this solution. Added a shuffling test case as an example. \r\n\r\nI thought about how to implement this solution in `MatchingFilsOp` kernel without using `Dataset`, but did not find a good solution. \r\n\r\nDo you have any ideas, suggestions, or comments?     ", "(Removing myself as reviewer, since @jsimsa is on the case!)", "@jsimsa Thanks very much for your detailed comments, which are really helpful! This PR has been revised. Could you have a look at the revisions?\r\n\r\nFor the comments about changing the existing code (e.g. variable names and iterator construction) in `list_files_dataset_op_test.py`, may I submit another PR and also add a benchmark test to `list_files_dataset_op_test.py`?", "@jsimsa Thanks for your comments! This is just a kindly reminder that this PR has been revised, in case you missed my updates two days ago. Could you please have a look when you have time? ", "@jsimsa Thanks very much for your comments! The serialization test is added and the issues in parallelizing the call of IsDirectory are resolved. The coding style issues are resolved as well. Could you please help review these changes?", "@jsimsa Thanks for your help on this PR! If @mrry agrees to change the implementation of list_files to use the new `MatchingFilesDataset`, I would like to submit a PR for it!", "I ran the presubmit tests and some of them are failing. For instance, the Python 2 lint check is failing (https://source.cloud.google.com/results/invocations/3b3d88eb-f0ae-4461-9840-2529e2c554b7/log)", "@jsimsa I rebase my branch and am fixing these failing tests. \r\n\r\nI could reproduce the Python 2 lint check problem now on my laptop. One thing I missed previously is that in the virtual environment, the installation path for `pylint` is required to run `pylint`; otherwise, it will still use the default `pylint` in the system. \r\n\r\n", "The failed tests seem to be caused by 1) the compatibility issue of `os.makedirs` in Python 2 and Python 3; 2) the missing of `def _inputs(self)` in `MatchingFilesDataset`. These two issues have been resolved. Could @jsimsa or @qlzh727 please help run the tests again?", "rerunning tests now", "Ubuntu Sanity fails with:\r\n```\r\nFAIL: Found 1 non-whitelited pylint errors:\r\ntensorflow/python/data/kernel_tests/matching_files_dataset_op_test.py:144: [W0622(redefined-builtin), MatchingFilesDatasetTest.testNestedDirectories] Redefining built-in 'file'\r\n```\r\n\r\nand the Ubuntu CC presubmit still has build failures.\r\n\r\n", "@jsimsa Sorry about the Ubuntu Sanity failure. I missed that one on my local test. Have submitted a commit to fix it.\r\n\r\nFor the [Ubuntu CC](https://source.cloud.google.com/results/invocations/1b2886fb-fc09-4abe-bfb2-adccd305ad35/targets), I did not find the fails/broken targets related to this PR. Do you have any ideas?", "@jsimsa  Ubuntu CC test passed this time. For the Ubuntu Sanity check, I forgot to keep the line inside 80 characters when renaming the variable name. Sorry about my bad. A PR has been submitted to fix it.", "@jsimsa Thanks very much for your help on this PR!", "@feihugis the following comments were raised during an internal review of your PR:\r\n\r\n```\r\n========================================================================\r\nFile tensorflow/core/kernels/data/matching_files_dataset_op.cc (snapshot 1)\r\n------------------------------------\r\nLine 49 (MatchingFilesDatasetOp.MakeDataset):\r\n    for (int i = 0; i < num_patterns; i++) {\r\n\r\nThis `int` will create compiler warnings on some platforms... `size_t` is recommended here.\r\n------------------------------------\r\nLine 103 (Iterator.GetNextInternal):\r\n        Status ret;\r\n\r\nWhat purpose is `ret` serving here? It never seems to be returned, or indeed read.\r\n\r\n(This suggests we might be missing some tests for error cases....)\r\n------------------------------------\r\nLine 113 (Iterator.GetNextInternal):\r\n            // We can also use isDectory() here. But IsDirectory call can be\r\n\r\ns/isDectory/IsDirectory/\r\n------------------------------------\r\nLine 117 (Iterator.GetNextInternal):\r\n              filepath_tensor.scalar<string>()() = current_file;\r\n\r\nOpportunity for a std::move here.\r\n------------------------------------\r\nLine 132 (Iterator.GetNextInternal):\r\n                0, current_pattern_.find_first_of(\"*?[\\\\\"));\r\n\r\nThis seems like it could be a string_view.\r\n------------------------------------\r\nLine 207 (Iterator.UpdateIterator):\r\n        string fixed_prefix =\r\n\r\nAnother opportunity for string_view?\r\n------------------------------------\r\nLine 218 (Iterator.UpdateIterator):\r\n        // there isn't any point in exploring that child path).\r\n\r\nCan this comment move to where children_dir_status is defined?\r\n------------------------------------\r\nLine 226 (Iterator.UpdateIterator):\r\n          ret.Update(s);\r\n\r\nShould we just return immediately if GetChildren() fails?\r\n------------------------------------\r\nLine 237 (Iterator.UpdateIterator):\r\n              return ret;\r\n\r\nThis seems like it could be a return Status::OK();\r\n------------------------------------\r\nLine 254 (Iterator.UpdateIterator):\r\n              children_dir_status[i] =\r\n\r\nuse errors::Cancelled(\"Operation not needed\") instead of Status(...)\r\n------------------------------------\r\nLine 268 (Iterator.UpdateIterator):\r\n          counter.Wait();\r\n\r\nApparently we have a new utility method to help with code like this: tensorflow/core/lib/core/threadpool.h?l=62-69\r\n------------------------------------\r\nLine 273 (Iterator.UpdateIterator):\r\n            const Status child_dir_status = children_dir_status[i];\r\n\r\nconst Status& to avoid copying the error message.\r\n------------------------------------\r\nLine 277 (Iterator.UpdateIterator):\r\n            }\r\n\r\nShould we return child_dir_status? It looks like we swallow it and when we return `ret` on L291 we're returning the status of fs->GetChildren() (which may well be OK, even though we're \"bailing\" with an error here).\r\n------------------------------------\r\nLine 288 (Iterator.UpdateIterator):\r\n            }\r\n\r\nWould it be worth storing a bit in filepath_queue_ that indicates whether a path is known to be a file (the else branch) and thus doesn't need to be re-expanded? IIUC, I think we're doing 2x the necessary RPCs for the leaves on remote file systems.\r\n------------------------------------\r\nLine 298 (Dataset.Iterator):\r\n      string current_pattern_ GUARDED_BY(mu_);\r\n\r\nDoes this need to be part of the iterator state? (Instead accessing `dataset()->patterns_[current_pattern_index_]` would save a copy, and slightly simplify the iterator state.)\r\n========================================================================\r\n```\r\n\r\nIn addition, the Python tests for your implementation are failing. Please check to make sure that you are not assuming that path separators are `/` in your tests.\r\n\r\nOnce these comments are addressed (ideally please address them all in a single commit for ease of review), we will re-run the tests to make sure the Windows issue is fixed and there are no new issues.\r\n\r\nThanks!", "@jsimsa For the comment at Line 268, if we use the threadpool, does it mean a thread pool needs to be created for each call? I attach some sample code here. Will it be efficient? Do you have any suggestion? \r\n```C++\r\nint num_threads = std::min(8, children.size());\r\nthread::ThreadPool threads(Env::Default(), \"ForEach\", num_threads);\r\nthreads.TransformRangeConcurrently(1, children.size(), is_directory_fn);\r\n```\r\n\r\n```\r\n------------------------------------\r\nLine 268 (Iterator.UpdateIterator):\r\n          counter.Wait();\r\n\r\nApparently we have a new utility method to help with code like this: tensorflow/core/lib/core/threadpool.h?l=62-69\r\n------------------------------------\r\n```", "good point about the threadpool, I would stick to using the `ctx->runner()` then", "@jsimsa All the comments are addressed except the following three. Please find my explanation in below. Could you help review these changes (two commits) and run the tests?\r\n\r\n```\r\n------------------------------------\r\nLine 268 (Iterator.UpdateIterator):\r\n          counter.Wait();\r\n\r\nApparently we have a new utility method to help with code like this: tensorflow/core/lib/core/threadpool.h?l=62-69\r\n------------------------------------\r\n```\r\n**Reply**: If we use the threadpool, the thread pool needs to be created several times. The current implementation may be better. \r\n\r\n```\r\n------------------------------------\r\nLine 298 (Dataset.Iterator):\r\n      string current_pattern_ GUARDED_BY(mu_);\r\n\r\nDoes this need to be part of the iterator state? (Instead accessing `dataset()->patterns_[current_pattern_index_]` would save a copy, and slightly simplify the iterator state.)\r\n========================================================================\r\n```\r\n**Reply**: `current_pattern_` is necessary here because `current_pattern_` may be different from `dataset()->patterns_[current_pattern_index_]` in some cases. For example, [these code](https://github.com/tensorflow/tensorflow/blob/156ae3e2a85e77f842edb0e18373c8d2b903230c/tensorflow/core/kernels/data/matching_files_dataset_op.cc#L137-L140) will make them different but we could not directly update `patterns_` as it needs to be same with users' input patterns.\r\n\r\n```\r\n------------------------------------\r\nLine 277 (Iterator.UpdateIterator):\r\n            }\r\n\r\nShould we return child_dir_status? It looks like we swallow it and when we return `ret` on L291 we're returning the status of fs->GetChildren() (which may well be OK, even though we're \"bailing\" with an error here).\r\n------------------------------------\r\n```\r\n**Reply**: For the `child_dir_status` is `errors::Cancelled(\"Operation not needed\")`, it is actually not an error. It indicates that we can ignore this child file/directory to avoid the unnecessary search. I think the current `bailing` implementation is OK.\r\n\r\n ", "@jsimsa Thanks for your comments. A commit has been submitted to address them. I saw the `Windows Bazel` and `Windows Bazel GPU` tests fail, but could not get the details. Do you know what's the issue in these two tests?", "Thanks for checking! Do you think it will make sense to switch the implementation of `list_files` to use `MatchingFilesDataset`?", "Let's leave that for a follow up PR>", "> If that's the case, I would not make the change I suggested because I think that we should return `NotFoundError` iff there is no match for any of the patterns.\r\n> \r\n> Looking at your code, I we currently return non-ok status when either `env()->GetFileSystemForFile` fails (line 216) or `fs->GetChildren` fails (line 236).\r\n> \r\n> I suggest you add temporary log statements that tells us what the returned status at these lines is which will likely help us identify the root cause of the failures.\r\n> \r\n> I think that you will ultimately need to treat `NotFoundError` differently from other non-ok statuses. For all non-ok status other that `NotFoundError` we should propagate this to the caller right away (as opposed to continue doing other work which can also result in an error that will be discarded). For `NotFoundError`, we should skip over these and only return `NotFoundError` to the caller if all patterns evaluated to an empty set of files.\r\n\r\n@jsimsa Thanks very much for your comments and suggestions! The temporary logs have been added for debugging. Could you help run the tests?\r\n\r\nYes, a better way is needed to handle different kinds of non-ok statuses as you suggest. Will do it after figuring out the root cause of the current failures.\r\n\r\nSorry about the commits. I rebased my code and then I was required to push the commit with `force`, which makes all the commit history display again here. The last four commits are created after the internal review.\r\n\r\n---\r\nIt looks like the failures are caused by [these code](https://github.com/tensorflow/tensorflow/blob/597f04e949285f7e72682c7c3a6ed656a5aedb1e/tensorflow/core/kernels/data/matching_files_dataset_op.cc#L129-L132). It works wrong on Windows. The right implementation for Windows may need to be like [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/windows/windows_file_system.cc#L489-L505), which first converts `\\` to `/` and then utlizes the general helper function at [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system_helper.cc#L55).  \r\n", "Does the Windows error log surface what the problem is: https://source.cloud.google.com/results/invocations/607587c1-c2b1-4185-bf91-060cf97249c8/log?", "@jsimsa Yes. The problem is related to the backslash in Windows path. Here is some sample log info. The child paths are not right, and no mached files are pushed to the heap.\r\n```\r\nInput pattern: ./T:\\tmp\\tmpt35u2cfw\\**\\**\\*.txt; Current dir: .\r\nGetFileSystemForFile status: OK\r\nGetChildren status: OK; Children size: 3; Heap size: 0\r\nChild dir path: ./py_test_dir\r\nChild dir path: ./tensorflow\r\nChild dir path: ./__init__.py\r\n```\r\n [These code](https://github.com/tensorflow/tensorflow/blob/597f04e949285f7e72682c7c3a6ed656a5aedb1e/tensorflow/core/kernels/data/matching_files_dataset_op.cc#L129-L132) do not work right for Windows path. One more step is needed to convert the `\\\\` in Windows path to `/` as [WindowsFileSystem::GetMatchingPaths](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/windows/windows_file_system.cc#L489-L505) does. ", "@jsimsa I am trying to fix the backslash problem by adding the following code right before the [line 129](https://github.com/tensorflow/tensorflow/blob/597f04e949285f7e72682c7c3a6ed656a5aedb1e/tensorflow/core/kernels/data/matching_files_dataset_op.cc#L129). Are there any existing API in the FileSytem class that can be used to do it?\r\n\r\n```C++\r\nif (dynamic_cast<WindowsFileSystem*>(fs) != nullptr) {\r\n              std::replace(current_pattern_.begin(), current_pattern_.end(),\r\n                           '\\\\', '/');\r\n}\r\n```\r\n\r\nIf using the above code, I am not clear how to add WindowsFileSystem dependency to `data/BUILD` file. The following build configuration fails.\r\n\r\n```\r\ntf_kernel_library(\r\n    name = \"matching_files_dataset_op\",\r\n    srcs = [\"matching_files_dataset_op.cc\"],\r\n    deps = [\r\n        \":dataset\",\r\n        \"//tensorflow/core:dataset_ops_op_lib\",\r\n        \"//tensorflow/core:framework\",\r\n        \"//tensorflow/core:lib\",\r\n        \"//tensorflow/core:lib_internal\",\r\n        \"//tensorflow/core:platform_file_system\",\r\n    ],\r\n)\r\n```\r\n\r\nOr, is it OK to use the backslash as the indicator of Windows path?\r\n```C++\r\nif (current_pattern_.find('\\\\') != std::string::npos) {\r\n              std::replace(current_pattern_.begin(), current_pattern_.end(),\r\n                           '\\\\', '/');\r\n}\r\n```\r\n\r\n\r\n", "@jsimsa I submit a commit to fix the Windows path issue. Could you please help run the test again?", "@jsimsa @mrry Thanks for running the test! Several tests passed on Windows (e.g.`testSimpleDirectory` and `testFileMiddles`), but `testNestedDirectories` and `MatchingFilesDatasetSerializationTest` still failed. It seems to be caused by the different behaviors of Windows and Linux on the search pattern with nested directories. For example, \r\n- for the input pattern `TMP_DIR/*/*.txt` and the file `TMP_DIR/a/b/c.txt`, Windows will treat this file as a match, but Linux will not; \r\n- for the input pattern `TMP_DIR/*/*/*/*.txt` and the file `TMP_DIR/a/b/c/d.txt`, both Windows and Linux will treat them as the match.    ", "If your test results in a different behavior on Linux and Windows, then it is not a good test and you should change it.", "@jsimsa The test cases have been revised to work for both Windows and Linux. Could you help run the test to see if it works? Thanks!\r\n", "- The Windows tests have passed. \r\n- The non-ok status is handled as 1) for all the error statuses except `NOT_FOUND`, return them to the caller immediately; 2) for `NOT_FOUND` error, skip them to continue searching and only return them to the caller when all the patterns/the last pattern refers to a non-exist path.\r\n\r\n@jsimsa Could you help review these changes when you have time?   ", "Please investigate and fix the current test failures (e.g. Python2 and Python3).", "@jsimsa Sorry about the error in the last commit. `std::max(size_t(0), current_pattern_index_ - 1)` will trigger the error when `current_pattern_index_` is `0` because `current_pattern_index_` is size_t type. It has been changed now. Could you help run the test again?", "@jsimsa The Google internal checks failed, but the link for the details is invalid. Could you see the internal checks from your side?", "The `MatchingFilesDatasetTest.testNonExistingDirectory` test fails -- it encounters `OutOfRange` before encountering `NotFound`.", "@jsimsa Are there any environment difference between the normal tests and the Google internal tests (e.g. the file system)?  In the `MatchingFilesDatasetTest.testNonExistingDirectory`, the test directory has been deleted at the beginning by this [code](https://github.com/tensorflow/tensorflow/blob/579155d915bd1fe2cfcff9927ca9af996aca1b72/tensorflow/python/data/kernel_tests/matching_files_dataset_op_test.py#L51). I'm guessing that the line [260](https://github.com/tensorflow/tensorflow/blob/579155d915bd1fe2cfcff9927ca9af996aca1b72/tensorflow/core/kernels/data/matching_files_dataset_op.cc#L260) in the internal test did not return an error status for the non-existing path.  ", "I am certain that there are differences between the Google and OS environment. Can you add some logs to the tests to check for what the possible problem could be and I will re-run the test.", "@jsimsa Some logs are added. Could you re-run the test?", "@jsimsa Could you help paste the failure logs of the internal check here? ", "```\r\nChildren Num: 0; Status: OK; Current dir: /tmp/tmp4eu2yriu\r\nChildren Num: 4; Status: OK; Current dir: /tmp/tmpz5arw61i\r\nChildren Num: 4; Status: OK; Current dir: /tmp/tmp83qx9jia\r\nChildren Num: 8; Status: OK; Current dir: /tmp/tmp5qsipscw\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/0/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/0/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/0/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/1\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/1/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/1/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/1/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/2/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/2/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/2/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/3\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/3/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/3/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/3/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/4\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/4/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/4/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/4/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/5\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/5/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/5/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/5/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/6\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/6/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/6/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/6/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/7\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/7/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/7/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/7/0/1/2\r\nChildren Num: 8; Status: OK; Current dir: /tmp/tmp5qsipscw\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/0/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/0/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/0/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/1\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/1/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/1/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/1/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/2/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/2/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/2/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/3\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/3/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/3/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/3/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/4\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/4/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/4/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/4/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/5\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/5/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/5/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/5/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/6\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/6/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/6/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/6/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/7\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/7/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp5qsipscw/7/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmp5qsipscw/7/0/1/2\r\nChildren Num: 0; Status: OK; Current dir: /tmp/tmpoltn3n5n\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpt7rllyxw\r\n[ RUN      ] MatchingFilesDatasetTest.testEmptyDirectory\r\n[       OK ] MatchingFilesDatasetTest.testEmptyDirectory\r\n[ RUN      ] MatchingFilesDatasetTest.testFileMiddles\r\n[       OK ] MatchingFilesDatasetTest.testFileMiddles\r\n[ RUN      ] MatchingFilesDatasetTest.testFileSuffixes\r\n[       OK ] MatchingFilesDatasetTest.testFileSuffixes\r\n[ RUN      ] MatchingFilesDatasetTest.testNestedDirectories\r\n[       OK ] MatchingFilesDatasetTest.testNestedDirectories\r\n[ RUN      ] MatchingFilesDatasetTest.testNonExistingDirectory\r\n[  FAILED  ] MatchingFilesDatasetTest.testNonExistingDirectory\r\n[ RUN      ] MatchingFilesDatasetTest.testSimpleDirectory\r\n[       OK ] MatchingFilesDatasetTest.testSimpleDirectory\r\n[ RUN      ] MatchingFilesDatasetTest.test_session\r\n[       OK ] MatchingFilesDatasetTest.test_session\r\n```", "I suspect you will need to add logging to the C++ code to understand why OutOfRange is occurring in place of NotFound.", "@jsimsa Based on the log, there is no `Status: Not found` (one should be there for `testNonExistingDirectory`), and only two records with `Children Num: 0` in below:\r\n```\r\nChildren Num: 0; Status: OK; Current dir: /tmp/tmp4eu2yriu\r\n.......\r\nChildren Num: 0; Status: OK; Current dir: /tmp/tmpoltn3n5n\r\n.\r\n```\r\n- The first one comes from `MatchingFilesDatasetTest.testEmptyDirectory`. We can ignore it.\r\n- The second one comes from `MatchingFilesDatasetTest.testNonExistingDirectory `. It is expected to be `Status: Not found`. There will be two possible scenarios: 1) [self.tearDown()](https://github.com/tensorflow/tensorflow/blob/879a5020f0b05026951d463bad47c00d94da6879/tensorflow/python/data/kernel_tests/matching_files_dataset_op_test.py#L51) did not sucessfully delete the tmp dir; 2) if the tmp dir is removed, that means the filesystem return `Status::OK()` for the non-exisiting path (Will it be possible?). \r\n\r\nWill add more logs. Do you know which filesystem is used in the internal check?  ", "@jsimsa I have changed the testing code to make sure the temp dir doesn't exist in `MatchingFilesDatasetTest.testNonExistingDirectory`. Could you help re-run the test?\r\n", "@jsimsa Could you help paste the logs of the internal checks here? Thanks!", "```\r\nChildren Num: 0; Status: OK; Current dir: /tmp/tmpujrfbk95\r\nChildren Num: 4; Status: OK; Current dir: /tmp/tmpzsm5ljyk\r\nChildren Num: 4; Status: OK; Current dir: /tmp/tmp5f2h0jcm\r\nChildren Num: 8; Status: OK; Current dir: /tmp/tmpe4du34wx\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/0/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/0/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/0/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/1\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/1/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/1/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/1/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/2/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/2/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/2/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/3\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/3/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/3/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/3/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/4\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/4/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/4/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/4/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/5\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/5/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/5/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/5/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/6\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/6/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/6/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/6/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/7\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/7/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/7/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/7/0/1/2\r\nChildren Num: 8; Status: OK; Current dir: /tmp/tmpe4du34wx\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/0/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/0/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/0/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/1\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/1/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/1/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/1/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/2/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/2/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/2/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/3\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/3/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/3/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/3/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/4\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/4/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/4/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/4/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/5\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/5/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/5/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/5/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/6\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/6/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/6/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/6/0/1/2\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/7\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/7/0\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpe4du34wx/7/0/1\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpe4du34wx/7/0/1/2\r\nChildren Num: 0; Status: OK; Current dir: /tmp/tmpfwujok86/nonexistingdir\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmprex2dz9y\r\n[ RUN      ] MatchingFilesDatasetTest.testEmptyDirectory\r\n[       OK ] MatchingFilesDatasetTest.testEmptyDirectory\r\n[ RUN      ] MatchingFilesDatasetTest.testFileMiddles\r\n[       OK ] MatchingFilesDatasetTest.testFileMiddles\r\n[ RUN      ] MatchingFilesDatasetTest.testFileSuffixes\r\n[       OK ] MatchingFilesDatasetTest.testFileSuffixes\r\n[ RUN      ] MatchingFilesDatasetTest.testNestedDirectories\r\n[       OK ] MatchingFilesDatasetTest.testNestedDirectories\r\n[ RUN      ] MatchingFilesDatasetTest.testNonExistingDirectory\r\n[  FAILED  ] MatchingFilesDatasetTest.testNonExistingDirectory\r\n[ RUN      ] MatchingFilesDatasetTest.testSimpleDirectory\r\n[       OK ] MatchingFilesDatasetTest.testSimpleDirectory\r\n[ RUN      ] MatchingFilesDatasetTest.test_session\r\n[       OK ] MatchingFilesDatasetTest.test_session\r\n```", "Thank you, @jsimsa! This log `Children Num: 0; Status: OK; Current dir: /tmp/tmpfwujok86/nonexistingdir` indicates that the internal test returned `Status: OK` instead of `Status: Not found` for the non-existing directory at line 260 [Status s = fs->GetChildren(current_dir, &children)](https://github.com/tensorflow/tensorflow/blob/988fae336c9146b1534a750edcd3b4905f207814/tensorflow/core/kernels/data/matching_files_dataset_op.cc#L260), where `current_dir = \"/tmp/tmpfwujok86/nonexistingdir\"`. It looks like a bug in the corresponding filesystem code. What do you think of it? If you know which filesystem the internal check uses, I can check the implementation code in the TensorFlow repo.", "On some platforms, `fs->GetChildren()` returns an empty list instead of erring if the base path isn't found. It is not a bug as `GetChildren()` does not promise to return `NotFound` if the base path does not exist. You need to use `fs->FileExists(...)` to check if the path exists, which is guaranteed to return `NotFound` is the base path does not exist.", "@jsimsa Thanks for your suggestions! `fs->FileExists()` has been added to make different platforms return the same status `NOT_FOUND` for the non-existing path. Could you help re-run the tests?\r\n\r\n", "@jsimsa The internal check still failed. Could you help paste the log here?", "```\r\nChildren Num: 0; Status: OK; Current dir: /tmp/tmpr_4r6_b3; FileExist status: OK\r\nChildren Num: 4; Status: OK; Current dir: /tmp/tmp37teep66; FileExist status: OK\r\nChildren Num: 4; Status: OK; Current dir: /tmp/tmp322cjwk4; FileExist status: OK\r\nChildren Num: 8; Status: OK; Current dir: /tmp/tmpfoyyep4_; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/0/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/0/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/0/0/1/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/1; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/1/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/1/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/1/0/1/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/2/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/2/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/2/0/1/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/3; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/3/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/3/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/3/0/1/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/4; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/4/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/4/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/4/0/1/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/5; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/5/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/5/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/5/0/1/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/6; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/6/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/6/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/6/0/1/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/7; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/7/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/7/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/7/0/1/2; FileExist status: OK\r\nChildren Num: 8; Status: OK; Current dir: /tmp/tmpfoyyep4_; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/0/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/0/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/0/0/1/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/1; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/1/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/1/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/1/0/1/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/2/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/2/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/2/0/1/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/3; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/3/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/3/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/3/0/1/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/4; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/4/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/4/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/4/0/1/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/5; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/5/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/5/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/5/0/1/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/6; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/6/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/6/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/6/0/1/2; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/7; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/7/0; FileExist status: OK\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmpfoyyep4_/7/0/1; FileExist status: OK\r\nChildren Num: 2; Status: OK; Current dir: /tmp/tmpfoyyep4_/7/0/1/2; FileExist status: OK\r\nChildren Num: 0; Status: OK; Current dir: /tmp/tmppd_7qf3k/nonexistingdir; FileExist status: Not found: Stat failed for /tmp/tmppd_7qf3k/nonexistingdir: No such file or directory\r\nChildren Num: 3; Status: OK; Current dir: /tmp/tmp2iu8nzmf; FileExist status: OK\r\n```", "Thanks, @jsimsa! Did you see which test failed?", "Your implementation never returns `NotFound` when all patterns result in an empty match.", "```\r\n[ RUN      ] MatchingFilesDatasetTest.testEmptyDirectory\r\n[       OK ] MatchingFilesDatasetTest.testEmptyDirectory\r\n[ RUN      ] MatchingFilesDatasetTest.testFileMiddles\r\n[       OK ] MatchingFilesDatasetTest.testFileMiddles\r\n[ RUN      ] MatchingFilesDatasetTest.testFileSuffixes\r\n[       OK ] MatchingFilesDatasetTest.testFileSuffixes\r\n[ RUN      ] MatchingFilesDatasetTest.testNestedDirectories\r\n[       OK ] MatchingFilesDatasetTest.testNestedDirectories\r\n[ RUN      ] MatchingFilesDatasetTest.testNonExistingDirectory\r\n[  FAILED  ] MatchingFilesDatasetTest.testNonExistingDirectory\r\n[ RUN      ] MatchingFilesDatasetTest.testSimpleDirectory\r\n[       OK ] MatchingFilesDatasetTest.testSimpleDirectory\r\n[ RUN      ] MatchingFilesDatasetTest.test_session\r\n[       OK ] MatchingFilesDatasetTest.test_session\r\n```", "@jsimsa The implementation is revised to return `NotFound`if the input patterns result in the empty match. Could you re-run the test? ", "@jsimsa This time, all the checks (including the internal check) passed. I submit another commit to remove the logging code and make a minor change to avoid the repeated checking if the FS is Windows for each call `GetNextInternal`. It should not add any issues. Could you re-run the test?", "@jsimsa `import/copybara` failed with the info `An error happened while migrating the change`. Do you know what it means? Also, a commit is added to consider the case that the input patterns contains both the Windows patterns and non-Windows patterns. Could you help run the test again?", "@jsimsa The internal checks failed again. Could you help paste the log here?", "@feihugis internal tests are still running...", "@drpngx I saw a red cross in the front of `feedback/copybara` with the message `Google internal checks FAILED`, so had thought the internal tests failed. Will wait until all the tests finish.\r\n\r\n---\r\n@jsimsa @drpngx It seemed that `Windows Bazel` and `feedback/copybara` tests failed, but could not access the logs. ", "Stuck on flaky test, I think. Re-running.", "Thanks all for the help on this PR (@jsimsa @mrry @drpngx and the internal reviewer)!! Especially a lot of help from @jsimsa \ud83d\udc4d\u2764\ufe0f!!"]}, {"number": 22428, "title": "DO NOT MERGE - Update RELEASE.md", "body": "", "comments": []}, {"number": 22427, "title": "tf.image.resize_image running on CPU when I use tf.float16", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n     yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n    OpenSUSE 12.1\r\n- **TensorFlow installed from (source or binary)**:\r\n    Source\r\n- **TensorFlow version (use command below)**:\r\n    1.1 \r\n- **Python version**:\r\n    3.5\r\n- **CUDA/cuDNN version**:\r\n    8.0\r\n- **GPU model and memory**:\r\n    Tesla M40, 11GB\r\n\r\n- **Exact command to reproduce**:\r\n    tf.image.resize_image(input with float16, shape)\r\n\r\n### Describe the problem\r\nTried to run inference using float16 but I saw a significant inference slow down comparing with float32. I did some further study and found it's running on CPU, which blocks the speed up of using float16.  \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nMobile device", "Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22426, "title": "TensorFlow installs open source Keras", "body": "Installing TensorFlow (1.11.rc1) installs open source Keras. I don't think that's intentional, and things might get confusing.\r\n\r\nIn my bazel build system, that's a problem as there's a circular dependency as my keras depends on tensorflow, and tensorflow now depends on keras. (although it's true that open source keras does not explicitly list TF as a dependency)\r\n\r\nThis happens because of the new dependncy on keras-applications and keras-preprocessing that both depend on `keras` (open source version).\r\n\r\npip dependency tree of `tensorflow-gpu==1.11.0rc1`:\r\n\r\n```\r\ntensorflow-gpu==1.11.0rc1\r\n  (...)\r\n  - keras-applications [required: >=1.0.5, installed: 1.0.5]\r\n    (...)\r\n    - keras [required: >=2.1.6, installed: 2.2.0]\r\n  (...)\r\n  - keras-preprocessing [required: >=1.0.3, installed: 1.0.3]\r\n    - keras [required: >=2.1.6, installed: 2.2.0]\r\n  (...)\r\n(...)\r\n```", "comments": ["@fchollet @martinwicke Is this dependency intentional?", "@fchollet has removed the Keras dependency from applications and preprocessing. Thanks for reporting this."]}, {"number": 22425, "title": "Inference slowdown of 3x with TF-TensorRT integration with C API ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.11\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**:9/7.1\r\n- **GPU model and memory**: 1080ti/11gb\r\n- **Exact command to reproduce**:\r\n1. Clone tensorflow repo, checkout r1.11 branch\r\n2. Build from source as directed from documentation by disabling everything except cuda, tensorrt(4.0.1.6)\r\n3. Create a tensorrt .pb file using following:\r\n```\r\n    trt_graph = trt.create_inference_graph(\r\n    input_graph_def=tf.get_default_graph().as_graph_def(),\r\n    outputs=output_node,\r\n    max_batch_size=1,\r\n    max_workspace_size_bytes=1 << 25,\r\n    precision_mode=\"FP32\",  # TRT Engine precision \"FP32\",\"FP16\" or \"INT8\"\r\n    minimum_segment_size=2  # minimum number of nodes in an engine\r\n    )\r\n    f = open(\"trt.pb\", 'w')\r\n    f.write(trt_graph.SerializeToString())\r\n    f.close()\r\n\r\n```\r\n4. Use Tensorflow C API to run infernce on the protobuf file\r\n\r\nIssue: I see a slowdown of 3x for a model with C API, but this is not reproducible with Python API with the same model.\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n![trt_1](https://user-images.githubusercontent.com/4759327/45907556-e4e35980-bde7-11e8-8d78-bcf84fd0b366.png)\r\n![trt_2](https://user-images.githubusercontent.com/4759327/45907558-e876e080-bde7-11e8-9e8d-863f6b3918d9.png)\r\n", "comments": ["Nagging Assignee @aaroey: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @dhingratul, is it possible for you to provide the .pb file from which you observe this slowdown? Also, could you provide the c API code that you used to load and run the file?\r\n\r\nThanks.", "@aaroey The bug is resolved. Python API and C API now are consistent after increasing the max batch_size parameter from the log information"]}, {"number": 22424, "title": "Native compilation of Tensorflow on ARMv8 platforms fail", "body": "------------------------\r\n\r\n### System information\r\n== cat /etc/issue ===============================================\r\nLinux ad96b38fdef35570b0c421cb8b68608f.lsdk.generic.ls1046ardb.nxp 4.14.47 #1 SMP PREEMPT Sat Jun 23 08:17:00 CST 2018 aarch64 aarch64 aarch64 GNU/Linux\r\nVERSION=\"18.04 LTS (Bionic Beaver)\"                                                                                                                     \r\nVERSION_ID=\"18.04\"                                                                                                                                      \r\nVERSION_CODENAME=bionic                                                                                                                                 \r\n== are we in docker =============================================\r\nNo                                                               \r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu/Linaro 7.3.0-16ubuntu3) 7.3.0                        \r\nCopyright (C) 2017 Free Software Foundation, Inc.                \r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux ad96b38fdef35570b0c421cb8b68608f.lsdk.generic.ls1046ardb.nxp 4.14.47 #1 SMP PREEMPT Sat Jun 23 08:17:00 CST 2018 aarch64 aarch64 aarch64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.6.0)\r\ntensorflow (1.10.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.10.0\r\ntf.GIT_VERSION = v1.10.0-0-g656e7a2b34\r\ntf.COMPILER_VERSION = v1.10.0-0-g656e7a2b34\r\nSanity check: array([1], dtype=int32)\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\nImportError: No module named platform\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tools/tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n### Describe the problem\r\nNative compilation on ARMv8 platform fails, due to tensorflow-lite \r\nPatch is copied below. Please incorporate them in the next release. thanks.\r\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++\r\n+ Patch file is copied below\r\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++\r\ndiff --git a/tensorflow/contrib/lite/build_def.bzl b/tensorflow/contrib/lite/build_def.bzl\r\nindex b735d08b4b..4f333f079d 100644                                                       \r\n--- a/tensorflow/contrib/lite/build_def.bzl                                               \r\n+++ b/tensorflow/contrib/lite/build_def.bzl                                               \r\n@@ -14,8 +14,8 @@ def tflite_copts():                                                     \r\n               \"-O3\",                                                                     \r\n           ],                                                                             \r\n           str(Label(\"//tensorflow:android_arm\")): [\r\n-              \"-mfpu=neon\",\r\n-              \"-mfloat-abi=softfp\",\r\n+              #\"-mfpu=neon\",\r\n+              #\"-mfloat-abi=softfp\",\r\n               \"-std=c++11\",\r\n               \"-O3\",\r\n           ],\r\ndiff --git a/tensorflow/contrib/lite/kernels/internal/BUILD b/tensorflow/contrib/lite/kernels/internal/BUILD\r\nindex 3a855fe3dd..c013313512 100644\r\n--- a/tensorflow/contrib/lite/kernels/internal/BUILD\r\n+++ b/tensorflow/contrib/lite/kernels/internal/BUILD\r\n@@ -21,7 +21,7 @@ HARD_FP_FLAGS_IF_APPLICABLE = select({\r\n NEON_FLAGS_IF_APPLICABLE = select({\r\n     \":arm\": [\r\n         \"-O3\",\r\n-        \"-mfpu=neon\",\r\n+        #\"-mfpu=neon\",\r\n     ],\r\n     \":armeabi-v7a\": [\r\n         \"-O3\",\r\ndiff --git a/third_party/png.BUILD b/third_party/png.BUILD\r\nindex 17c5449cc0..7b095cbc36 100644\r\n--- a/third_party/png.BUILD\r\n+++ b/third_party/png.BUILD\r\n@@ -42,6 +42,7 @@ cc_library(\r\n     ],\r\n     includes = [\".\"],\r\n     linkopts = [\"-lm\"],\r\n+    copts = [\"-DPNG_ARM_NEON_OPT=0\"],\r\n     visibility = [\"//visibility:public\"],\r\n     deps = [\"@zlib_archive//:zlib\"],\r\n )\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@xiaomin05 Good catch and thanks for the contribution. Please submit PR and we will gladly review.", "With bazel >= 0.16, which comes with better support for ARMv8, and my patch (https://github.com/tensorflow/tensorflow/pull/16175), TF Lite related stuff should work.", "Here is the requested info:\r\n=================================\r\nHave I written custom code\r\nAns: No\r\n OS Platform and Distribution\r\nAns: Ubuntu userland 16.04\r\n TensorFlow installed from\r\nAns: https://github.com/tensorflow/tensorflow, tag: v1.10.0\r\n TensorFlow version\r\nAns: v1.10.0\r\n Bazel version\r\nAns: v0.15.2\r\n CUDA/cuDNN version\r\nAns: not used\r\n GPU model and memory\r\nAns: not used\r\n Exact command to reproduce\r\nAns: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n Mobile device\r\nAns: Not used", "@freedomtan Thanks for the patch. @xiaomin05 Feel free to submit PR or patch for older Bazel versions."]}, {"number": 22423, "title": "cluster resolver for slurm workload manager", "body": "This creates a new cluster resolver which allows launching of distributed TensorFlow on clusters using Slurm workload manager, which is typically used on HPC systems. The resolver for now can handle homogeneous job allocation with default plane distribution. The resolver can also implicitly expose GPUs to a process according to configurations and specifications where one or more GPUs can be can be allocated to a process. A method is also provided to all processes to \"realize\" which job and task index they are assigned to.", "comments": ["Can u take a look Frank?", "@frankchn thanks for the feedback, the issues are addressed!", "Please fix the lint issue:\r\n\r\nFAIL: Found 1 non-whitelited pylint errors:\r\ntensorflow/contrib/cluster_resolver/python/training/slurm_cluster_resolver_test.py:21: [W0611(unused-import), ] Unused import subprocess\r\n\r\nAlso the py3  test is failing, see https://source.cloud.google.com/results/invocations/31d6152f-cd75-4ac6-b132-8639e7166b77/targets/%2F%2Fbazel_pip%2Ftensorflow%2Fcontrib%2Fcluster_resolver:slurm_cluster_resolver_py_test/log", "@qlzh727 I fixed the format issue, thanks!", "This has been committed as https://github.com/tensorflow/tensorflow/commit/59a2424cac60f904cf39b56fb413364413c77ed2, not sure why the bot is not marking this as closed automatically, but @steven-chien, if you want to make any additional fixes, can you create a new PR?", "OK checked with the Build team, so this won't automatically close as you made new commits. Closing this manually, and please make another PR and assign me as the reviewer if you need to make any other changes."]}, {"number": 22422, "title": "tensorflow issue on python3.5 running at raspbian 9 stretch", "body": "tensorflow library was downloaded from python wheels to raspberry pi 3 running raspbian 9. Thonny shell log reports a runtime warning while executing \"import tensorflow\" instruction: compile version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I have entered a custom code, just a single line as following:\r\n\r\n   import tensorflow\r\n\r\nOS platform : Raspbian 9\r\nTensorflow installed from python wheels (default for raspberry pi)\r\nRunning on raspberry device", "I am also having the same warnings using Raspberry Pi 2 w/ Raspbian 9:\r\n\r\n```\r\n(keras_tf) pi@raspberrypi:~ $ python -c \"import tensorflow as tf; print(tf.__version__)\"\r\n/home/pi/.virtualenvs/keras_tf/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\r\n  return f(*args, **kwds)\r\n/home/pi/.virtualenvs/keras_tf/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\r\n  return f(*args, **kwds)\r\n1.11.0\r\n(keras_tf) pi@raspberrypi:~ $ \r\n\r\n```", "Thanks for your update... Regarding the issue, can it be fixed?\n\nLooking forward your comments.\nThanks and regards\n\nEl mi\u00e9., 3 de oct. de 2018 17:19, agrutter87 <notifications@github.com>\nescribi\u00f3:\n\n> I am also having the same warnings:\n>\n> (keras_tf) pi@raspberrypi:~ $ python -c \"import tensorflow as tf; print(tf.__version__)\"\n> /home/pi/.virtualenvs/keras_tf/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n>   return f(*args, **kwds)\n> /home/pi/.virtualenvs/keras_tf/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\n>   return f(*args, **kwds)\n> 1.11.0\n> (keras_tf) pi@raspberrypi:~ $\n>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22422#issuecomment-426820782>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ApakeIfNwITdMV8lalYRG-bFCjHhiQC3ks5uhTgNgaJpZM4WyuI->\n> .\n>\n", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "hello\r\nafter installing tensflow version 1.1.0. this is the error i get when try to import tensorflow as tf\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 305, in _custom_import\r\n    module = self._original_import(*args, **kw)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: wrong ELF class: ELFCLASS64\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/pi/Desktop/opencv_test.py\", line 4, in <module>\r\n    from tensorflow import tf\r\n  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 305, in _custom_import\r\n    module = self._original_import(*args, **kw)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 305, in _custom_import\r\n    module = self._original_import(*args, **kw)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 305, in _custom_import\r\n    module = self._original_import(*args, **kw)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/lib/python3/dist-packages/thonny/backend.py\", line 305, in _custom_import\r\n    module = self._original_import(*args, **kw)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: wrong ELF class: ELFCLASS64\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\nkindly help.", "Question: did you find a solution or is it working right now? As shown in the ticket https://github.com/tensorflow/tensorflow/issues/26459 some other people has the very same problem without any solution. sharing your solution would be fine."]}, {"number": 22421, "title": "Change update_op in tf.metrics.auc", "body": "We do not need to compute the AUC in the update_op, just to update the confusion matrix value.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Can u please sign the CLA if you haven't? Thanks", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 22420, "title": "Add GPU complex support for reciprocal", "body": "This fix tries to address the issue in #22384 where there\r\nwas no GPU complex support for reciprocal. This fix adds\r\nthe complex64/128 support for GPU.\r\n\r\nThis fix fixes #22384.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Looks like there are some python 3 gnu issues. Close for now. Will reopen if issue is resolved."]}, {"number": 22419, "title": "Update version strings to rc2", "body": "", "comments": []}, {"number": 22418, "title": "[WIP] Add tensor forest classification train", "body": "Work in progress", "comments": ["Thanks for the PR. There is a merge conflict for the build file. Can you rebase to the latest code? Thanks", "sorry, i have to close this and create a new one, once this get merged  https://github.com/tensorflow/tensorflow/pull/21803"]}, {"number": 22417, "title": "tf.estimator.train_and_evaluate() return None when trained with only one master node", "body": "### System information\r\n- **Hosting Environment**: GCP ML-Engine\r\n- **TensorFlow version (use command below)**: r1.10\r\n- **Python version**: 3.5\r\n- **GPU model and memory**: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\n- **Exact command to reproduce**: eval_metrics = tf.estimator.train_and_evaluate(my_estimator, my_train_spec, my_eval_spec\r\n\r\n### Describe the problem\r\nAccording to document of Tensorflow r1.10, the function tf.estimator.train_and_evaluate() should return a tuple of the result of the evaluate call to the Estimator and the export results using the specified ExportStrategy. \r\n\r\nThis function returns the correct tuple if trained locally; when using this function on ml-engine (i.e. submit as a job run on Cloud ML Engine runtime version 1.10 using only one node in the cluster), it will return **None**. The corresponding TF_CONFIG environment variable is as follows: \r\n```\r\n{'cluster': {'master': ['127.0.0.1:2222']}, 'job': {'scale_tier': 'BASIC_GPU', 'region': 'europe-west1', 'python_version': '3.5', 'python_module': 'task.train_eval', 'job_dir': 'gs://my_bucket_name/my_job_folder', 'runtime_version': '1.10', 'run_on_raw_vm': True, 'package_uris': ['gs://my_bucket_name/kYspuihk/packages/my_package.tar.gz']}, 'environment': 'cloud', 'task': {'index': 0, 'cloud': 'bdb8f39da23bcb702-ml', 'type': 'master'}}\r\n```\r\n\r\n### Source code / logs\r\nI looked a bit into the training logic, and [this line](https://github.com/tensorflow/tensorflow/blob/ca94990804cf5326c0f6f46d75c96e0f0e240366/tensorflow/python/estimator/training.py#L674) seems suspicious, since it may not return any evaluation metrics. I might also be wrong in spotting the location because I did not too deep into it; and I made it to do an estimator.evaluate() again instead.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nMobile device", "OS Platform and Distribution: GCP ML-engine with scale tier \"BASIC GPU\" and runtime version \"1.10\"\r\nTensorflow installed from: Not sure, since it is automatically installed when the VM is started\r\nBazel Version: N/A\r\nCUDA/cuDNN: unknown\r\nMobile Device: N/A", "Nagging Assignee @ymodak: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Can you please open your issue in [GoogleCloudPlatform/ml-on-gcp repo](https://github.com/GoogleCloudPlatform/ml-on-gcp) since this issue is related to Machine Learning on Google Cloud Platform. Thanks!", "I have seen the issue is closed but still would like to contribute a solution as others may land in this page. \r\nThe problem, in some cases,  could be that for distributed training the result of train_and_evaluate is currently (runtime version 1.10) undefined. This can be read in the last line of the [documentation of the method](https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate), where as of today it reads:\r\n\r\n> Returns:\r\n> A tuple of the result of the evaluate call to the Estimator and the export results using the specified ExportStrategy. **Currently, the return value is undefined for distributed training mode.**"]}, {"number": 22416, "title": "Fix warning in text_classification_character_cnn.py", "body": "`squeeze_dims` was deprecated so switch to `axis`.", "comments": []}, {"number": 22415, "title": "Fix build failure in verbs", "body": "While looking into #22372, the verbs build fails with the following error:\r\n```\r\ntensorflow/contrib/verbs/verbs_server_lib.cc:80:6: error: 'once_call' in namespace 'std' does not name a type\r\n std::once_call reg_mem_visitors_call;\r\n      ^\r\ntensorflow/contrib/verbs/verbs_server_lib.cc: In member function 'tensorflow::Status tensorflow::VerbsServer::Init(tensorflow::ServiceInitFunction, tensorflow::RendezvousMgrCreationFunction)':\r\ntensorflow/contrib/verbs/verbs_server_lib.cc:85:18: error: 'reg_mem_visitors_call' was not declared in this scope\r\n   std::call_once(reg_mem_visitors_call, []() { RdmaMgr::RegMemVisitors(); });\r\n```\r\n\r\nThis fix fixes the build failures with `once_call` -> `once_flag`.\r\n\r\nThis fix fixes #22372.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Paul, can u take a look?\r\n"]}, {"number": 22414, "title": "CUDNN_STATUS_ALLOC_FAILED", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no, it's on examples/tutorial/mnist,  mnist_deep.py\r\n- **Bazel version**: N/A\r\n- **Mobile device**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10 x64 latest build\r\n- **TensorFlow installed from (source or binary)**: binary prebuilt \r\n- **TensorFlow version (use command below)**: 1.11.0rc1\r\n- **Python version**: 3.6.6\r\n- **CUDA/cuDNN version**: 9.0/7.2.1\r\n- **GPU model and memory**: GTX1050Ti GDDR5 4GB\r\n- **Exact command to reproduce**: \r\n1. python mnist_deep.py\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:353] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n\r\n2. other codes are fine-working as before\r\n\r\n3. no problem with my laptop, which has geforce mx150 GDDR5 2GB\r\n\r\n\r\nall this happens after I installed geforce driver 411.63\r\n\r\n### Source code / logs\r\nPS D:\\VSWorkspace\\tensorflow\\tensorflow\\examples\\tutorials\\mnist> python .\\mnist_deep.py\r\nWARNING:tensorflow:From .\\mnist_deep.py:130: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nWARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future versi\r\non.\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nWARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future vers\r\nion.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting /tmp/tensorflow/mnist/input_data\\train-images-idx3-ubyte.gz\r\nWARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future vers\r\nion.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting /tmp/tensorflow/mnist/input_data\\train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data\\t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data\\t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future ve\r\nrsion.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nSaving graph to: C:\\Users\\alanp\\AppData\\Local\\Temp\\tmpxpinc32v\r\n2018-09-20 22:10:32.455130: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-09-20 22:10:32.641187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:\r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.4175\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.30GiB\r\n2018-09-20 22:10:32.659090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-09-20 22:10:33.556406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-20 22:10:33.565294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0\r\n2018-09-20 22:10:33.571150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N\r\n2018-09-20 22:10:33.578487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3013 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus\r\nid: 0000:01:00.0, compute capability: 6.1)\r\n2018-09-20 22:10:36.376573: E tensorflow/stream_executor/cuda/cuda_dnn.cc:353] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nMobile device", "@tensorflowbutler  updated", "@zheng-xq @zhangyaobit @alextp Could you comment or reassign?", "@azaks2 do you know if there is a known issue with this driver version?", "Edit.  NVIDIA was moving 7.2 to the archive page and we ran into some edge cases.  I do not  have evidence there is anything wrong with the latest version of 7.2.x.  With 7.3 being the version supported by CUDA 10, that will be out next jump and 7.3 will work with existing TensorFlow builds 1.10 and anything targeting cuDNN 7.x\r\n\r\nI would upgrade to cuDNN 7.3 The 411 driver is really new and likely more aligned with CUDA 10 but I have no data points even on Linux at this time as to combinations I know work.  \r\n\r\nMy advice (with no data points I am sorry):\r\n-  Upgrade to cuDNN 7.3\r\n-  Consider downgrading to an older Device Driver like 396 but I realized that is really hard on windows as it tries to keep you on the latest possible version.  I would likely do this as a last resort.  \r\n", "@tfboyd \r\nwith cudnn 7.3, same\r\n\r\nbut thank you anyway, I'd rather stick with ubuntu this moment and leave this pc for a while", "For people searching the keyword CUDNN_STATUS_ALLOC_FAILED\r\n and finding this issue. You may check if there is another process allocating the gpu. In my case i got this error on terminal after previously running tensorflow from a jupyter notebook"]}, {"number": 22413, "title": "tf_inspect.py is not future secure due to use of inspect.getargspec()", "body": "### System information\r\nPython 3.6.5\r\nPytest version 3.8.0.\r\n\r\nIssue valid for master of tensorflow on github\r\n\r\n### Describe the problem\r\nThe module `tf_inspect.py` uses deprecated functions of the `inspect` module.\r\n\r\n### Source code / logs\r\nEasiest to see the warning is make a pytest:\r\n\r\n```python3\r\nimport tensorflow\r\n\r\ndef test_me():\r\n    return\r\n```\r\nand run `pytest` selecting that test-file producing the warnings:\r\n\r\n```\r\n../python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\r\n  return _inspect.getargspec(target)\r\n```\r\nIt should be as easy as replacing `getargspec` with `getfullargspec` as the latter is meant as a dropin replacement of be previous according to the docs.\r\n\r\nI would make a PR, but I'm not comfortable, to put it mildly, with the legalisms required.", "comments": ["@local-minimum Hi, Could you provide the following information\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n", "We should deprecate tf_inspect.getargspec and move all remaining uses to tf_inspect.getfullargspec. This feels to me like a good way to get into the codebase.\r\n\r\nDo you feel like submitting a pull request?", "If I don't have to sign some contributor agreement I would gladly do a PR for changing it.", "@local-minimum Great! Please post your PR number once it's submitted. Also feel free to reopen the issue if required.", "I don't think @local-minimum will send a PR since they're not happy signing the CLA. We should leave this open.", "I was just looking for a good first issue, and this looks pretty doable. I'll sign the CLA and put in a PR for this a bit later today.", "I'm sharing my findings for the night in case there's any community input. I'll work on putting this comment into a PR tomorrow.\r\n\r\nThe `tf_inspect.py` module already looks configured to shim python 2 for use with the `tf_inspect.getfullargspec` wrapper, but from what I see the current implementation has two coupled issues:\r\n\r\n1) `tf_inspect.getargspec` is used locally for `_getfullargspec` with python 2, and called for tests in `tf_inspect_test.py`, but it's also exposed for potential misuse without much documentation. \r\n\r\n2) Several modules (beyond unit tests) fall into the trap of calling the exposed `tf_inspect.getargspec` directly. Python 3 calls to any of these modules will call the deprecated python 2 function.\r\n\r\nThe main change that I'd recommend to `tf_inspect.py` would be adding a warning into the `tf_inspect.getargspec` function signature to call `tf_inspect.getfullargspec` instead. \r\n\r\nNon-unit-test files directly calling `tf_inspect.getargspec` should changed to use the existing cross-compatible wrapper.\r\n\r\nAnd (bonus issue), a unit test needs to be written for `tf_inspect.getfullargspec`.", "@alextp  Any suggestions for the above ?", "@IMBurbank 's suggestions look great to me. Let's wait for the pull request and then close this issue.", "Submitted as PR #22517 ", "Let's let the PR's merge close the issue."]}, {"number": 22412, "title": "Tensor2Tensor Intro tensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nwindows 10, x64\r\n- **TensorFlow installed from (source or binary)**:\r\n pip (pip install --upgrade tensorflow)\r\n- **TensorFlow version (use command below)**:\r\n1.10.0\r\n- **Python version**:\r\n3.6\r\n- **GPU model and memory**:\r\nnot using GPU, 12GB memory\r\nMobile device: N/A\r\nBazel version : N/A\r\nCUDA/cuDNN version : N/A\r\n\r\n### Describe the problem \r\nI'm trying to make the Tensor2Tensor intro: English to German translation with a pre-trained model work locally. I have downloaded the checkpoints to the right directory, however it's returning this error:\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested\r\n\r\nAs you can see the checkpoints are in the proper directory:\r\n![image](https://user-images.githubusercontent.com/24248699/45819470-ac0e8c00-bce4-11e8-85bf-fccc4e2e7c22.png)\r\n\r\nI have tried redownloading the checkpoints many times, I can't seem to get the English to German translation working with the provided checkpoints.\r\n@lukaszkaiser \r\n\r\n- **Exact command to reproduce**:\r\n\r\n```\r\n# Imports we need.\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport os\r\nimport collections\r\n\r\nfrom tensor2tensor import models\r\nfrom tensor2tensor import problems\r\nfrom tensor2tensor.layers import common_layers\r\nfrom tensor2tensor.utils import trainer_lib\r\nfrom tensor2tensor.utils import t2t_model\r\nfrom tensor2tensor.utils import registry\r\nfrom tensor2tensor.utils import metrics\r\n\r\n# Enable TF Eager execution\r\ntfe = tf.contrib.eager\r\ntfe.enable_eager_execution()\r\n\r\n# Other setup\r\nModes = tf.estimator.ModeKeys\r\n\r\n# Setup some directories\r\ndata_dir = os.path.expanduser(\"~/t2t-ende/data\")\r\ntmp_dir = os.path.expanduser(\"~/t2t-ende/tmp\")\r\ntrain_dir = os.path.expanduser(\"~/t2t-ende/train\")\r\ncheckpoint_dir = os.path.expanduser(\"~/t2t-ende/checkpoints\")\r\ntf.gfile.MakeDirs(data_dir)\r\ntf.gfile.MakeDirs(tmp_dir)\r\ntf.gfile.MakeDirs(train_dir)\r\ntf.gfile.MakeDirs(checkpoint_dir)\r\ngs_data_dir = \"gs://tensor2tensor-data\"\r\ngs_ckpt_dir = \"gs://tensor2tensor-checkpoints/\"\r\n\r\n# Fetch the problem\r\nende_problem = problems.problem(\"translate_ende_wmt32k\")\r\n\r\n# Copy the vocab file locally so we can encode inputs and decode model outputs\r\n# All vocabs are stored on GCS\r\nvocab_name = \"vocab.translate_ende_wmt32k.32768.subwords\"\r\nvocab_file = os.path.join(gs_data_dir, vocab_name)\r\n\r\n# Get the encoders from the problem\r\nencoders = ende_problem.feature_encoders(data_dir)\r\n\r\n# Setup helper functions for encoding and decoding\r\n\r\n\r\ndef encode(input_str, output_str=None):\r\n    \"\"\"Input str to features dict, ready for inference\"\"\"\r\n    inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\r\n    batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.\r\n    return {\"inputs\": batch_inputs}\r\n\r\n\r\ndef decode(integers):\r\n    \"\"\"List of ints to str\"\"\"\r\n    integers = list(np.squeeze(integers))\r\n    if 1 in integers:\r\n        integers = integers[:integers.index(1)]\r\n    return encoders[\"inputs\"].decode(np.squeeze(integers))\r\n\r\n# Generate and view the data\r\n# This cell is commented out because WMT data generation can take hours\r\n\r\n\r\nende_problem.generate_data(data_dir, tmp_dir)\r\nexample = tfe.Iterator(ende_problem.dataset(Modes.TRAIN, data_dir)).next()\r\ninputs = [int(x) for x in example[\"inputs\"].numpy()]  # Cast to ints.\r\ntargets = [int(x) for x in example[\"targets\"].numpy()]  # Cast to ints.\r\n\r\n# Example inputs as int-tensor.\r\nprint(\"Inputs, encoded:\")\r\nprint(inputs)\r\nprint(\"Inputs, decoded:\")\r\n# Example inputs as a sentence.\r\nprint(decode(inputs))\r\n# Example targets as int-tensor.\r\nprint(\"Targets, encoded:\")\r\nprint(targets)\r\n# Example targets as a sentence.\r\nprint(\"Targets, decoded:\")\r\nprint(decode(targets))\r\n\r\n# Create hparams and the model\r\nmodel_name = \"transformer\"\r\nhparams_set = \"transformer_base\"\r\n\r\nhparams = trainer_lib.create_hparams(\r\n    hparams_set, data_dir=data_dir, problem_name=\"translate_ende_wmt32k\")\r\n\r\n# NOTE: Only create the model once when restoring from a checkpoint; it's a\r\n# Layer and so subsequent instantiations will have different variable scopes\r\n# that will not match the checkpoint.\r\ntranslate_model = registry.model(model_name)(hparams, Modes.EVAL)\r\n\r\n# Copy the pretrained checkpoint locally\r\nckpt_name = \"transformer_ende_test\"\r\ngs_ckpt = os.path.join(gs_ckpt_dir, ckpt_name)\r\nckpt_path = tf.train.latest_checkpoint(os.path.join(checkpoint_dir, ckpt_name))\r\n\r\n# Restore and translate!\r\ndef translate(inputs):\r\n    encoded_inputs = encode(inputs)\r\n    print(encoded_inputs)\r\n    with tfe.restore_variables_on_create(ckpt_path):\r\n        model_output = translate_model.infer(encoded_inputs)[\"outputs\"]\r\n        print(model_output)\r\n    return decode(model_output)\r\n\r\n\r\ninputs = \"The animal didn't cross the street because it was too tired\"\r\noutputs = translate(inputs)\r\n\r\nprint(\"Inputs: %s\" % inputs)\r\nprint(\"Outputs: %s\" % outputs)`\r\n```\r\n\r\n### Source code / logs\r\n>>> def translate(inputs):\r\n...     encoded_inputs = encode(inputs)\r\n...     print(encoded_inputs)\r\n...     with tfe.restore_variables_on_create(ckpt_path):\r\n...         model_output = translate_model.infer(encoded_inputs)[\"outputs\"]\r\n...         print(model_output)\r\n...     return decode(model_output)\r\n...\r\n>>> inputs = \"The animal didn't cross the street because it was too tired\"\r\n>>> outputs = translate(inputs)\r\n{'inputs': <tf.Tensor: id=202, shape=(1, 15, 1), dtype=int32, numpy=\r\narray([[[   28],\r\n        [ 4705],\r\n        [ 6253],\r\n        [   83],\r\n        [   62],\r\n        [ 3444],\r\n        [    4],\r\n        [ 3825],\r\n        [  244],\r\n        [   40],\r\n        [   53],\r\n        [  362],\r\n        [19285],\r\n        [   85],\r\n        [    1]]])>}\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<stdin>\", line 4, in translate\r\n  File \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\n  File \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\contrib\\eager\\python\\saver.py\", line 91, in restore_variables_on_create\r\n    ckpt_var_cache[k] = reader.get_tensor(k)\r\n  File \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 348, in get_tensor\r\n    status)\r\n  File \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested\r\n>>> print(\"Inputs: %s\" % inputs)\r\nInputs: The animal didn't cross the street because it was too tired\r\n>>> print(\"Outputs: %s\" % outputs)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nNameError: name 'outputs' is not defined\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "@davypepe From your screenshot it appears that at least some of the files were not downloaded completely. You may consider switching to GPU or use [Git LFS](https://git-lfs.github.com/). \r\n", "@wt-huang Thanks! I'll try that out and close the issue for now! \ud83d\ude0e "]}, {"number": 22411, "title": "the session->close() is ok,but the error is  stack smashing detected ", "body": "### OS platform:ubuntu:16.04\r\n### ensorflow install from source code\r\n### tensorflow version:1.11(master branch)\r\n### Bazel version:0.17.1\r\n### CPU\r\n## 1\u3001comand:\r\nbazel build --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=monolithic //tensorflow:libtensorflow_cc.so\r\nbazel build --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=monolithic //tensorflow:libtensorflow_framework.so\r\n\r\nsudo mkdir /usr/local/include/tf\r\nsudo mkdir /usr/local/include/tf/tensorflow\r\nsudo cp -r bazel-genfiles/ /usr/include/tf\r\nsudo cp -r tensorflow/cc /usr/include/tf/tensorflow\r\nsudo cp -r tensorflow/core /usr/include/tf/tensorflow\r\nsudo cp -r third_party /usr/include/tf\r\nsudo cp -r bazel-bin/tensorflow/libtensorflow_cc.so /usr/local/lib\r\nsudo cp -r bazel-bin/tensorflow/libtensorflow_framework.so /usr/local/lib\r\n## 2\u3001code:\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n\r\nusing namespace std;\r\nusing namespace chrono;\r\nusing namespace tensorflow;\r\nint main(int argc, char* argv[]) {\r\n// SessionOptions options;\r\n// std::unique_ptrtensorflow::Session session(tensorflow::NewSession(options));\r\nSession *session = NewSession(SessionOptions());\r\nif(session == nullptr)\r\n{\r\nthrow runtime_error(\"Could not create tensorflow session\");\r\n}\r\nStatus status = session->Close();\r\nreturn 0;\r\n}\r\n## 3\u3001the status is status.ok() is right,but the error is \"*** stack smashing detected ***: terminated \u5df2\u653e\u5f03 (\u6838\u5fc3\u5df2\u8f6c\u50a8)\"", "comments": ["I'm facing the same issue with tensorflow 1.12. I'm trying  to compile my project using our own build system and linking against the  tensorflow_cc.so. Is that possible or will I have to find a way to  build my code using bazel?\r\n\r\n\r\n\r\n", "the best way in ubuntu is to use bazel", "check you protobuf with command \"protoc --version\",I think your problem is that the version is wrong.", "@NeuYangxiu \r\nIndeed, I have checked the output of protoc --version command and it shoed version 2.6.1 used. However, since Ive modified the PATH variable to find my local installation of version 3.6.1 first and protoc --version returns 3.6.1 now and yet the same problem still persists. I'm using tensorflow 1.12   ", "Sorry for not following up here in a timely way.\r\nWe do not support any other build system other than bazel at the moment. So I do not have any experience with any other build systems.\r\n\r\nFor the main issue, I have no idea. I do not think I have ever seen stack smashing errors.\r\nMaybe flag mismatches between TF compilation and your c++ code?\r\n\r\n", "@Yangxiu123321 We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is out of support window. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. please refer [**`link`**](https://stackoverflow.com/questions/1345670/stack-smashing-detected) Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22411\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22411\">No</a>\n"]}, {"number": 22409, "title": "ValueError: Cannot set tensor: Got tensor of type 4 but expected type 2 for input 25", "body": "I am trying to run a tensorflow model on device, using tensorflow lite. I was able to convert my .pb graph to a .lite graph. Using python interpreter as below\r\n\r\n`interpreter = interpreter_wrapper.Interpreter(model_path=model_file)\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ninterpreter.set_tensor(input_details[0]['index'], candidates)`\r\n\r\nthe above code throws following error : ValueError: Cannot set tensor: Got tensor of type 4 but expected type 2 for input 25\r\n\r\nI have no clue what tensor of type 4 and tensor of type 2 are, I am not sure if they are same as tensor ranks, in my case, the tensor ranks and types mentioned above don't seem to be the same.", "comments": ["@theavgguy Just executed above code snippet using a simple model without running into any error. Type 2 or 4 is internal TFLite type casted from C++ type.  You would need to check your converted graph, or post your graph here.", "Closing this now, feel free to open a new issue if encountering any errors.", "@theavgguy \r\nHello, I got this problem too.  \r\nAnd I debug and find that type 2 or 4 in your case is the data type of the internal encoding which you can find clearly [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L142~L166). So you just need to cast your tensor with the same data type of your input.  \r\n\r\nHope that help you.", "@oneTaken Yes I forgot to share my findings here. Been a busy month at work.\r\nI found out that the problem was not with the tensor , but with numpy encoding. For each array I was creating I specified the data type to int32 and I was able to debug the issue. It would have been better though if tensorflow documented this error better as the error statement is of no help, it required a lot of reading of the code and hit and trial to debug and finally find the issue.\r\nThanks for your help.\r\n", "@theavgguy It is a remarkably bad error message. I'm fighting a similar one right now.", "got similar problem here, the error message is \r\n\"ValueError: Cannot set tensor: Got tensor of type 2 but expected type 1 for input 0 \"\r\n i checked the input dtype by\r\n \"print(interpreter.get_input_details())\",  \r\nthe output is \r\n\"[{'name': 'Placeholder', 'index': 0, 'shape': array([  1, 160, 160,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\"\r\nbut i still got this error when i set the input dtype to be either int32 or float32", "same error with float32 \r\n```ValueError: Cannot set tensor: Got tensor of type 5 but expected type 1 for input 0```\r\nstrangely when I set a non-converted numpy array it only starts to bark when I invoke the interpreter"]}, {"number": 22408, "title": "Tried to read from index 3 but array size is: 3", "body": "### System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\r\nOS Platform and Distribution : Linux Ubuntu 16.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\nTensorFlow installed from (source or binary): N/A\r\nTensorFlow version (use command below): N/A\r\nPython version: 3.6\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: 8.0\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\n\r\n### Describe the problem\uff1a\r\n\r\nHi, \r\nwhen I am using the tf.data.dataset api, I meet the error, and i am confused. \r\n\r\nthe error is :\r\n`InvalidArgumentError (see above for traceback): Tried to read from index 3 but array size is: 3`\r\n\r\nand my related code is:\r\n\r\n```python\r\ndef train_input(sess, vocab, rcmodel, train_or_valid=True):\r\n    train_dataset = get_dataset(vocab.word2idx, FLAGS.train_tfrecord_file, FLAGS.train_batch_size,\r\n                                repeat_num=FLAGS.num_epochs, shuffle_bufer=500, prefetch=-1)\r\n    valid_dataset = get_dataset(vocab.word2idx, FLAGS.valid_tfrecord_file, FLAGS.valid_batch_size,\r\n                                repeat_num=-1, shuffle_bufer=500)\r\n    train_iterator = train_dataset.make_initializable_iterator()\r\n    valid_iterator = valid_dataset.make_initializable_iterator()\r\n    if train_or_valid:\r\n        data_handle = sess.run(train_iterator.string_handle())\r\n        sess.run(train_iterator.initializer)\r\n    else:\r\n        data_handle = sess.run(valid_iterator.string_handle())\r\n        sess.run(valid_iterator.initializer)\r\n        \r\n    data_iter = tf.data.Iterator.from_string_handle(string_handle=data_handle,\r\n                                                    output_types=train_dataset.output_types,\r\n                                                    output_shapes=train_dataset.output_shapes)\r\n\r\n    batch_data = data_iter.get_next()\r\n    feed_data = {rcmodel.passage: batch_data[\"passage\"].eval(),\r\n                 rcmodel.passage_len: batch_data[\"passage_len\"].eval(),\r\n                 rcmodel.query: batch_data[\"query\"].eval(),\r\n                 rcmodel.query_len: batch_data[\"query_len\"].eval(),\r\n                 # rcmodel.query_id: batch_data[\"query_id\"],\r\n                 rcmodel.answer: batch_data['answer'].eval(),\r\n                 rcmodel.answer_len: batch_data[\"answer_len\"].eval(),\r\n                 rcmodel.alter0: batch_data[\"alter0\"].eval(),\r\n                 rcmodel.alter1: batch_data[\"alter1\"].eval(),\r\n                 rcmodel.alter2: batch_data[\"alter2\"].eval()}\r\n    return feed_data\r\n```\r\n\r\n```python\r\ntrain_feed = train_input(sess, vocab, rcmodel, True)\r\ntrain_feed[rcmodel.dropout_keep_prob] = FLAGS.dropout_keep_prob\r\ntrain_summary, _, train_loss, train_ppl_loss, train_acc, step, predict, logits, answer, embedding = sess.run(\r\n                   [merged, rcmodel.train_op, rcmodel.total_loss, rcmodel.ppl_loss,\r\n                    rcmodel.accuracy, rcmodel.global_step, rcmodel.predict, rcmodel.logits,\r\n                    rcmodel.answer, rcmodel.word_embedding],\r\n                   feed_dict=train_feed)\r\n```\r\n\r\nI know the error is caused by that: when I try to use \r\n```sess.run(fetches, feed_dict={passage:data_iter[\"passage}, query:data_iter['passage_len'])```\r\n\r\n```passage:data_iter[\"passage\"]``` consumed a batch, and ```query:data_iter[\"passage_len\"]``` consumed an other batch. So the data pasage and passage_len is not match.\r\n\r\nSo the reason is that when I sess.run one of the content of batch_iter, it will consume one batch. And I know using the string_handle can solve this problem. \r\n\r\nBut when I need to do this:\r\n```python\r\nself.passage = tf.placeholder(tf.int32, [None, None], name=\"passage\")\r\nself.query = tf.placeholder(tf.int32, [None, None], name=\"query\")\r\nself.answer = tf.placeholder(tf.int32, [None, None], name=\"answer\")\r\nself.passage_len = tf.placeholder(tf.int32, [None], name=\"passage_len\")\r\nself.query_len = tf.placeholder(tf.int32, [None], name=\"query_len\")\r\nself.query_id = tf.placeholder(tf.int32, [None], name=\"query_id\")\r\nself.answer_len = tf.placeholder(tf.int32, [None], name=\"answer_len\")\r\nself.alter0 = tf.placeholder(tf.int32, [None, None], name=\"alter0\")\r\nself.alter1 = tf.placeholder(tf.int32, [None, None], name=\"alter1\")\r\nself.alter2 = tf.placeholder(tf.int32, [None, None], name=\"alter2\")\r\n```\r\n\r\nWhat can I do to feed this?\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@PanXiebit This question is best asked at StackOverflow as this is not a bug nor feature request. You would need to provide more code snippets if seeking help here.", "@wt-huang  thank you, yes, this is not a bug. I had trained the model during the training. But when I am going to test, and restore the model by  reload the `rcmodel`, and  then\r\n```rcmodel.saver.restore(sess, save_path=tf.train.latest_checkpoint(\"./model/train_v2/\"))```\r\nbut I cannot feed the test dataset to the model, because the test_dataset.types is difficult to the train_dataset.shape. \r\n\r\n```\r\ndata_iter = tf.data.Iterator.from_string_handle(string_handle=data_handle,\r\n                                                    output_types=train_dataset.output_types,\r\n                                                    output_shapes=train_dataset.output_shapes)\r\n``` \r\nSo  I want to feed the this input one by one like this:\r\n```\r\nself.passage = tf.placeholder(tf.int32, [None, None], name=\"passage\")\r\nself.query = tf.placeholder(tf.int32, [None, None], name=\"query\")\r\nself.answer = tf.placeholder(tf.int32, [None, None], name=\"answer\")\r\nself.passage_len = tf.placeholder(tf.int32, [None], name=\"passage_len\")\r\nself.query_len = tf.placeholder(tf.int32, [None], name=\"query_len\")\r\nself.query_id = tf.placeholder(tf.int32, [None], name=\"query_id\")\r\nself.answer_len = tf.placeholder(tf.int32, [None], name=\"answer_len\")\r\nself.alter0 = tf.placeholder(tf.int32, [None, None], name=\"alter0\")\r\nself.alter1 = tf.placeholder(tf.int32, [None, None], name=\"alter1\")\r\nself.alter2 = tf.placeholder(tf.int32, [None, None], name=\"alter2\")\r\n```\r\nthen I find the issue above,  ```passage:data_iter[\"passage\"]``` consumed a batch, and ```query:data_iter[\"passage_len\"]``` consumed an other batch. So the data pasage and passage_len is not match.", "@PanXiebit test_dataset.types should be the same as train_dataset.shape, You can use tf.reshape()", "Closing this for now, feel free to open a new one if running into any issues."]}, {"number": 22407, "title": "Distributed training hangs up when I use CollectiveAllReduceStrategy (Python 2)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI slightly updated keras_model_to_estimator_client.py example so that it doesn't require Kubernetes. Updated version is [here](https://github.com/dmitrievanthony/ecosystem/blob/ignite-3/distribution_strategy/keras_model_to_estimator_client.py).\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 17.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.12.0-dev20180919\r\n\r\n- **Python version**:\r\n2.7.13\r\n\r\n- **Exact command to reproduce**:\r\nStart [worker1.py](https://github.com/dmitrievanthony/ecosystem/blob/ignite-3/ignite/worker1.py).\r\nStart [worker2.py](https://github.com/dmitrievanthony/ecosystem/blob/ignite-3/ignite/worker2.py).\r\nStart [keras_model_to_estimator_client.py](https://github.com/dmitrievanthony/ecosystem/blob/ignite-3/distribution_strategy/keras_model_to_estimator_client.py).\r\n\r\nAs result I would expect distributed training started in standalone mode with the following cluster config: `{\"worker\": [\"localhost:1111\", \"localhost:1112\"], \"chief\": [\"localhost:1113\"]}`. After initialization I would expect to see the same output as in local (non-distributed) mode.\r\n\r\n### Describe the problem\r\n\r\nHi, \r\n\r\nIt looks like the process hangs up on initialization step. See logs of processes below.\r\n\r\n### Source code / logs\r\n\r\nLogs of `worker1.py`.\r\n```\r\n2018-09-20 12:23:37.784529: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-09-20 12:23:37.785450: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job chief -> {0 -> localhost:1113}\r\n2018-09-20 12:23:37.785469: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -> {0 -> localhost:1111, 1 -> localhost:1112}\r\n2018-09-20 12:23:37.786408: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:1111\r\n2018-09-20 12:23:37.786477: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:385] Server already started (target: grpc://localhost:1111)\r\n2018-09-20 12:23:37.786586: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:385] Server already started (target: grpc://localhost:1111)\r\n2018-09-20 12:24:02.453219: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session 10324091b7c40099 with config: device_filters: \"/job:worker/task:0\" device_filters: \"/job:worker/task:0\" allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE scoped_allocator_optimization: ON scoped_allocator_opts { enable_op: \"CollectiveReduce\" enable_op: \"CollectiveReduce\" } } } isolate_session_state: true experimental { collective_group_leader: \"/job:chief/replica:0/task:0\" }\r\n2018-09-20 12:24:02.471993: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:355] Starting optimization for grappler item: tf_graph\r\n```\r\n\r\nLogs of `worker2.py`.\r\n```\r\n2018-09-20 12:23:43.680413: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-09-20 12:23:43.681625: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job chief -> {0 -> localhost:1113}\r\n2018-09-20 12:23:43.681664: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -> {0 -> localhost:1111, 1 -> localhost:1112}\r\n2018-09-20 12:23:43.682404: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:1112\r\n2018-09-20 12:23:43.682480: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:385] Server already started (target: grpc://localhost:1112)\r\n2018-09-20 12:23:43.682577: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:385] Server already started (target: grpc://localhost:1112)\r\n2018-09-20 12:24:02.426685: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session 438db1d6d8b24351 with config: device_filters: \"/job:worker/task:1\" device_filters: \"/job:worker/task:1\" allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE scoped_allocator_optimization: ON scoped_allocator_opts { enable_op: \"CollectiveReduce\" enable_op: \"CollectiveReduce\" } } } isolate_session_state: true experimental { collective_group_leader: \"/job:chief/replica:0/task:0\" }\r\n2018-09-20 12:24:02.441035: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:355] Starting optimization for grappler item: tf_graph\r\n```\r\n\r\nLogs of `keras_model_to_estimator_client.py`.\r\n```\r\nUsing /tmp/asdasdasd to store checkpoints.\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense (Dense)                (None, 16)                176       \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 1)                 17        \r\n=================================================================\r\nTotal params: 193\r\nTrainable params: 193\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nINFO:tensorflow:CollectiveAllReduceStrategy with local_devices = ['/device:CPU:0']\r\n2018-09-20 12:30:36.241065: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\nINFO:tensorflow:RunConfig initialized for Distribute Coordinator with STANDALONE_CLIENT mode\r\nINFO:tensorflow:Using the Keras model provided.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f971668a110>, '_model_dir': '/tmp/asdasdasd', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': DistributeConfig(train_distribute=<tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f97166f2ed0>, eval_distribute=<tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f971668a190>, remote_cluster=<tensorflow.python.training.server_lib.ClusterSpec object at 0x7f971668a110>), '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f971668a190>, '_train_distribute': <tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f97166f2ed0>, '_master': '', '_distribute_coordinator_mode': 'standalone_client'}\r\nINFO:tensorflow:Running `train_and_evaluate` with Distribute Coordinator.\r\nINFO:tensorflow:Running Distribute Coordinator with mode = 'standalone_client', cluster_spec = {'chief': ['localhost:1113'], 'worker': ['localhost:1111', 'localhost:1112']}, task_type = None, task_id = None, environment = None, rpc_layer = 'grpc'\r\nINFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['localhost:1113'], 'worker': ['localhost:1111', 'localhost:1112']}, task_type = 'chief', task_id = 0, num_workers = 3, local_devices = ['/job:chief/task:0']\r\nINFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['localhost:1113'], 'worker': ['localhost:1111', 'localhost:1112']}, task_type = 'worker', task_id = 0, num_workers = 3, local_devices = ['/job:worker/task:0']\r\nINFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['localhost:1113'], 'worker': ['localhost:1111', 'localhost:1112']}, task_type = 'worker', task_id = 1, num_workers = 3, local_devices = ['/job:worker/task:1']\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Collective All-reduce invoked with batches size = 4, num_workers = 3\r\nINFO:tensorflow:Collective All-reduce invoked with batches size = 4, num_workers = 3\r\nINFO:tensorflow:Collective All-reduce invoked with batches size = 4, num_workers = 3\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from=u'/tmp/asdasdasd/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\r\nINFO:tensorflow:Warm-starting from: (u'/tmp/asdasdasd/keras/keras_model.ckpt',)\r\nINFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Creating chief session creator with config: device_filters: \"/job:worker/task:0\"\r\nallow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n    scoped_allocator_optimization: ON\r\n    scoped_allocator_opts {\r\n      enable_op: \"CollectiveReduce\"\r\n    }\r\n  }\r\n}\r\nisolate_session_state: true\r\nexperimental {\r\n  collective_group_leader: \"/job:chief/replica:0/task:0\"\r\n}\r\n\r\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from=u'/tmp/asdasdasd/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\r\nINFO:tensorflow:Warm-starting from: (u'/tmp/asdasdasd/keras/keras_model.ckpt',)\r\nINFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from=u'/tmp/asdasdasd/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\r\nINFO:tensorflow:Warm-starting from: (u'/tmp/asdasdasd/keras/keras_model.ckpt',)\r\nINFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Creating chief session creator with config: device_filters: \"/job:worker/task:1\"\r\nallow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n    scoped_allocator_optimization: ON\r\n    scoped_allocator_opts {\r\n      enable_op: \"CollectiveReduce\"\r\n    }\r\n  }\r\n}\r\nisolate_session_state: true\r\nexperimental {\r\n  collective_group_leader: \"/job:chief/replica:0/task:0\"\r\n}\r\n\r\nINFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Creating chief session creator with config: device_filters: \"/job:chief/task:0\"\r\nallow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n    scoped_allocator_optimization: ON\r\n    scoped_allocator_opts {\r\n      enable_op: \"CollectiveReduce\"\r\n    }\r\n  }\r\n}\r\nisolate_session_state: true\r\nexperimental {\r\n  collective_group_leader: \"/job:chief/replica:0/task:0\"\r\n}\r\n\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Graph was finalized.\r\n```", "comments": ["#22321 Related issue for python3", "@harshini-gadige, it's similar only in terms of code, but it's not similar in terms of issue. In case of Python 3 the system throws exception during initialization, but in case of Python 2 it just hangs up after initialization.", "@dmitrievanthony Yes, I've put the reference comment for other users to refer to it for a different python version :)", "I am able to run your code. \r\n\r\nLooks like your client is not able to connect to these workers. Otherwise, you will see messages like: `New connected subchannel at ...`\r\n\r\nYou had specified a `\"chief\"` in your cluster but you didn't mention to start a chief job. Try remove the `\"chief\"` in your `remote_cluster`?", "Yes, looks like it's my mistake. I thought that coordinator starts chief on his own.\r\n\r\nThanks, @yuefengz. I think we can close this issue.", "BTW, @yuefengz, from user perspective it would be great to have a minimalistic example without all these kubernetes-docker-templates things. Do you mind if I prepare such?", "Do you think the example in this doc is good enough: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/README.md? If you can make the example clearer, I would appreciate it!", "Doc is good, @yuefengz, I was talking about examples in `ecosystem`. I've prepared and example, please have a look at this PR https://github.com/tensorflow/ecosystem/pull/101.", "Is chief node necessary in Collective All Strategy? \r\nWhat is the difference between chief and worker?\r\nThanks."]}, {"number": 22406, "title": "Exporting trained TensorFlow models to C++", "body": "\r\n### System information\r\n- **Have I written custom code **: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: win7 X64    Visual Studio 2015\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: source \r\n- **TensorFlow version (use command below)**: r 1.10\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.15\r\n- **CUDA/cuDNN version**: CUDA 9.0  /  CUDNN 7.2.1\r\n- **GPU model and memory**: 1080Ti / 11G\r\n- **Exact command to reproduce**:\r\n\r\nI trained a `graph.pb` file with python \uff0c i want to Exporting trained TensorFlow models to C++\uff0c\r\nI refer to some code on github, but still can't successfully import trained models using C++.Using the MNIST data.\r\n\r\n```\r\nTF_Graph* LoadGraphDef( const char* file )\r\n\t{\r\n\t\tif (file == nullptr)\r\n\t\t{\r\n\t\t\treturn nullptr;\r\n\t\t}\r\n\r\n\t\tTF_Buffer* buffer = ReadBufferFromFile( file );\r\n\t\tif (buffer == nullptr)\r\n\t\t{\r\n\t\t\treturn nullptr;\r\n\t\t}\r\n\r\n\t\tTF_Graph* graph = TF_NewGraph();\r\n\t\tTF_Status* status = TF_NewStatus();\r\n\t\tTF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();\r\n\r\n\t\tTF_GraphImportGraphDef( graph, buffer, opts, status );\r\n\t\tTF_DeleteImportGraphDefOptions( opts );\r\n\t\tTF_DeleteBuffer( buffer );\r\n\r\n\t\tif (TF_GetCode( status ) != TF_OK)\r\n\t\t{\r\n\t\t\tTF_DeleteGraph( graph );\r\n\t\t\tgraph = nullptr;\r\n\t\t}\r\n\r\n\t\tTF_DeleteStatus( status );\r\n\r\n\t\treturn graph;\r\n\t}\r\n\r\nTF_Tensor* CreateTensor( TF_DataType data_type,\r\n\t\tconst std::int64_t* dims, std::size_t num_dims,\r\n\t\tconst void* data, std::size_t len )\r\n\t{\r\n\t\tif (dims == nullptr || data == nullptr)\r\n\t\t{\r\n\t\t\treturn nullptr;\r\n\t\t}\r\n\t\tTF_Tensor* tensor = TF_AllocateTensor( data_type, dims, static_cast<int>(num_dims), len );\r\n\t\tif (tensor == nullptr)\r\n\t\t{\r\n\t\t\treturn nullptr;\r\n\t\t}\r\n\t\tvoid* tensor_data = TF_TensorData( tensor );\r\n\t\tif (tensor_data == nullptr)\r\n\t\t{\r\n\t\t\tTF_DeleteTensor( tensor );\r\n\t\t\treturn nullptr;\r\n\t\t}\r\n\t\tstd::memcpy( tensor_data, data, std::min( len, TF_TensorByteSize( tensor ) ) );\r\n\r\n\t\treturn tensor;\r\n\t}\r\n```\r\n\r\n```\r\n\tTF_Graph* graph = tf_utils::LoadGraphDef( \"graph.pb\" );\r\n\tif (graph == nullptr)\r\n\t{\r\n\t\tstd::cout << \"Can't load graph\" << std::endl;\r\n\t\treturn 1;\r\n\t}\r\n\tTF_Output input_op = { TF_GraphOperationByName( graph, \"input\" ), 0 };\r\n\tif (input_op.oper == nullptr)\r\n\t{\r\n\t\tstd::cout << \"Can't init input_op\" << std::endl;\r\n\t\treturn 2;\r\n\t}\r\n\tconst std::vector<std::int64_t> input_dims = { 1, 784 };\r\n\tconst std::vector<float> input_vals = {\r\n\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 96, 223, 255, 255, 223, 96, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 72, 223, 255, 255, 255, 255, 255, 247, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 231, 255, 255, 255, 255, 255, 255, 151, 0, 72, 223, 223, 96, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 255, 255, 255, 167, 8, 0, 0, 0, 96, 239, 255, 255, 247, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 72, 223, 255, 255, 255, 255, 0, 0, 0, 0, 104, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 231, 255, 255, 255, 255, 183, 0, 0, 72, 223, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 223, 96, 0, 0, 0, 231, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 72, 255, 255, 255, 255, 151, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 104, 255, 255, 255, 0, 0, 0, 96, 239, 255, 255, 255, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 231, 255, 255, 167, 0, 0, 128, 255, 255, 255, 255, 151, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 8, 0, 104, 255, 255, 255, 255, 151, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 231, 255, 255, 183, 8, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 255, 255, 255, 8, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 247, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 120, 255, 255, 255, 255, 255, 255, 151, 0, 0, 0, 0, 0, 0, 247, 255, 255, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 183, 255, 255, 104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 96, 255, 255, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0\r\n\t};\r\n\r\n\tTF_Tensor* input_tensor = tf_utils::CreateTensor( TF_FLOAT,\r\n\t\tinput_dims.data(), input_dims.size(),\r\n\t\tinput_vals.data(), input_vals.size() * sizeof( float ) );\r\n\r\n\tTF_Output out_op = { TF_GraphOperationByName( graph, \"output\" ), 0 };\r\n\tif (out_op.oper == nullptr)\r\n\t{\r\n\t\tstd::cout << \"Can't init out_op\" << std::endl;\r\n\t\treturn 3;\r\n\t}\r\n```\r\ninput_vals is the array corresponding to the minist picture, I want to know how `TF_AllocateTensor( data_type, dims, static_cast<int>(num_dims), len )` should be used and  what `dims` and `static_cast<int>(num_dims)` stand for?\r\n\r\n\r\n[This link](https://github.com/taotaolin/tensorflow_test/) Detailed partial code", "comments": ["@taotaolin Hi, please refer to [this link](https://medium.com/@hamedmp/exporting-trained-tensorflow-models-to-c-the-right-way-cf24b609d183) which has detailed steps to export a trained TF model to C++.", "@taotaolin static_cast<> is used to convert the given expression to the type specified. You may refer [this example ](https://stackoverflow.com/questions/44378764/hello-tensorflow-using-the-c-api)for a better understanding on the usage of TF_AllocateTensor()\r\n\r\nAlso, this question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 22405, "title": "[tf.data] Make `tf.contrib.data.reduce_dataset` work when the internal state is a tuple", "body": "## Issue\r\n\r\nOne issue with the new [`tf.contrib.data.reduce_dataset`][doc] is that the internal state can only have one element.\r\n\r\nThis is because of [this line][line] in the code to create the `sentinel_dataset`:\r\n\r\n```python\r\nsentinel_dataset = dataset_ops.Dataset.from_tensors(\r\n          reducer.finalize_func(reducer.init_func(np.int64(0))))\r\n```\r\n\r\nUsually the `finalize_func` can have multiple input arguments (as many as the number of elements in the state).\r\n\r\nFor instance if we want to compute the average over a dataset:\r\n```python\r\ndataset = tf.data.Dataset.range(5)\r\n\r\ndef init_fn(_):\r\n    return 0.0, 0.0\r\n\r\ndef reduce_fn(state, value):\r\n    average, count = state\r\n    value = tf.cast(value, tf.float32)\r\n    average = (average * count + value) / (count + 1)\r\n    count = count + 1\r\n    return average, count\r\n\r\ndef finalize_fn(average, count):\r\n    return average\r\n\r\nreducer = tf.contrib.data.Reducer(init_fn, reduce_fn, finalize_fn)\r\n\r\nmean = tf.contrib.data.reduce_dataset(dataset, reducer)\r\n```\r\n\r\nThis will return an error:\r\n>TypeError: finalize_fn() missing 1 required positional argument: 'count'\r\n\r\n---\r\n\r\n## Proposed workaround\r\n\r\nA way to pass a tuple as multiple arguments to `finalize_func` is to use `*`:\r\n\r\n```python\r\ninitial_state = reducer.init_func(np.int64(0))\r\nif not isinstance(initial_state, tuple):\r\n    initial_state = (initial_state,)\r\nsentinel_dataset = dataset_ops.Dataset.from_tensors(\r\n        reducer.finalize_func(*initial_state))\r\n```\r\n\r\nwhere I convert the initial state to a tuple if it is not already a tuple.\r\n\r\n\r\n## Test\r\n\r\nI added a `testReduceDatasetAverage` which has a state with two elements.\r\n\r\n\r\n## Comments\r\n\r\nLet me know if this makes sense and if my workaround is correct.\r\n\r\nIt would also be great to have this in release `1.11`.\r\n\r\n\r\n\r\n\r\n\r\n[doc]: https://www.tensorflow.org/versions/r1.11/api_docs/python/tf/contrib/data/reduce_dataset\r\n[line]: https://github.com/tensorflow/tensorflow/blob/da3357ecbdd6772413e8bbceeab8238971be11ce/tensorflow/contrib/data/python/ops/get_single_element.py#L94-L95", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Comment again for the CLA.", "CLAs look good, thanks!\n\n<!-- ok -->", "@mrry: Sorry I left some incorrect indentations in the code and forgot to run pylint first, it should be fixed now !", "@omoindrot, please resolve the merge conflict and we will run the test again.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 22404, "title": "Merge some memory leak fixing changes from master to r1.11", "body": "", "comments": []}, {"number": 22403, "title": "Where is the pywrap_tensorflow_internal.lib after building TensorFlow1.11 On Win10?", "body": "I have build TensorFlow1.11GPU version successfully in Win10 with using bazel, and generate tensorflow-1.11.0rc1-cp36-cp36m-win_amd64.whl file seccessfully. But I can not find the C++ lib(pywrap_tensorflow_internal.lib), where is the lib?  I want to use C++ not  Python.\r\n\r\n![qq 20180920145146](https://user-images.githubusercontent.com/39480728/45800850-bdf02f00-bce4-11e8-9cda-e233ac413387.png)\r\n\r\n\r\n", "comments": []}, {"number": 22402, "title": "Fix typo error in grappler remapper optimizer.", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 22401, "title": "CUDA 10.0 and python 3.7 support", "body": "this is not an issue\r\n\r\nmore of a question, request\r\n\r\ncan tensorflowers give roadmap for CUDA 10.0(and of course, cuDNN 7.3 for CUDA 10.0) and python 3.7 support? and of course Ubuntu 18.04\r\n\r\nCUDA 10.0 and python 3.7 already support Ubuntu 18.04\r\n\r\n\r\nthank you always\r\n", "comments": ["Linked to #20517 : Python 3.7 compatibility", "In addition, [TensorRT 5](https://developer.nvidia.com/tensorrt#tensorrt-whats-new)\r\n\r\n> **What's New in TensorRT 5 and TensorRT inference server**\r\n> TensorRT 5 delivers up to 40x faster inference over CPU-only platforms through support for Turing GPUs, new INT8 APIs and optimizations. It uses multi-precision compute to dramatically speed up recommenders, neural machine translation, speech and natural language processing. With TensorRT 5, you can:\r\n> - Speed up inference by 40x over CPUs for models such as translation using mixed precision on Turing Tensor Cores\r\n> - Optimize inference models with new INT8 APIs and optimizations\r\n> - Deploy applications to Xavier-based NVIDIA Drive platforms and the NVIDIA DLA accelerator (FP16 only)\r\n> \r\n> In addition, TensorRT 5 supports Windows and the CentOS Operating System. TensorRT 5 RC is available for download now to members of the NVIDIA Developer Program.", "@EwoutH \r\n\r\ngood\r\n\r\nTensorRT 5.0.0.10 RC for Ubuntu 1804 and CUDA 10.0 ", "Tensorflow 1.10.1 works with cuda 10.0 and tensorRT5.0.0, Just compiled on Ubuntu 16.04 (python 3.5) and did some tests. But no cudnn7.3 at this point from Nvidia's download site so have to use cudnn7.2 .\r\nEdited: just got updated to cudnn7.3, rebuilt TF1.10.1, still works.\r\n\r\nTensorflow 1.11.0RC1 didn't build but it might have nothing to do with cuda 10", "@alanpurple Hi, if you are looking for compatibility information(correct me if I'm wrong), please find the links below.\r\nFor [Linux and Mac OS](https://www.tensorflow.org/install/source#tested_build_configurations)\r\nFor [Windows ](https://www.tensorflow.org/install/source_windows#tested_build_configurations)", "Just built 1.11.0rc1 with CUDA 10, cuDNN 7.3 on Ubuntu 18.04.", "I've been attempting to find a compatible Tensorflow version with the new RTX cards. Does anyone know if this exists yet? I tried building 1.11.0rc2 on Windows but it throws \"Failed to load the native TensorFlow Runtime\" message. \r\n", "looks like CUDA 10 is GA.\r\nIt is too close to our 1.12 release cut. But we will have 1.13 with CUDA 10.\r\n\r\nFor python 3.7 support, we are ready to move, but we are waiting on our dependencies to cut releases that have python 3.7 compatibility.", "@gunan \r\n\r\nthank you for the precise and complete answer, maybe you can close this issue\r\n\r\nI wish (if you guys do us a favor) prebuilt binaries will also with CUDA10+cuDNN7.3+TensorRT5"]}, {"number": 22400, "title": "[tensorflow ppc64le] Building tensorflow from source fails on power8 architecture machine", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat Enterprise Linux Server 7.4 (Maipo)\r\n- **Mobile devices**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: \r\n- **Python version**: 3.5.6\r\n- **Bazel version (if compiling from source)**: 0.14.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0 (tried with 4.8.x as well)\r\n- **CUDA/cuDNN version**: 8.0/ 7.1.4\r\n- **GPU model and memory**: NVIDIA P100\r\n- **Exact command to reproduce**: bazel build //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Output of the tf_collect_env.sh**\r\n\r\n== cat /etc/issue ===============================================\r\nLinux summitdev-login1 3.10.0-693.21.1.el7.ppc64le #1 SMP Fri Feb 23 14:02:56 EST 2018 ppc64le ppc64le ppc64le GNU/Linux\r\nVERSION=\"7.4 (Maipo)\"\r\nVERSION_ID=\"7.4\"\r\nREDHAT_BUGZILLA_PRODUCT_VERSION=7.4\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7.4\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 5.4.0\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux summitdev-login1 3.10.0-693.21.1.el7.ppc64le #1 SMP Fri Feb 23 14:02:56 EST 2018 ppc64le ppc64le ppc64le GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy      1.15.1   \r\n\r\n== check for virtualenv =========================================\r\nFalse    **(this is not true I created an environment and ran this script from inside that env)**\r\n\r\n== tensorflow import ============================================\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /autofs/nccs-svm1_sw/summitdev/.swci/1-compute/opt/spack/20180406/linux-rhel7-ppc64le/gcc-5.4.0/spectrum-mpi-10.2.0.0-20180110-6keaqyzn74d6t7ilr5wcmd4ovxjkxue7/lib:/sw/summitdev/gcc/5.4.0new/lib64:/sw/summitdev/cuda/8.0.61-1/lib64:/autofs/nccs-svm1_home/shubhankar/magma_install/lib:/autofs/nccs-svm1_home1/shubhankar/magma_install/lib:/autofs/nccs-svm1_home/shubhankar/magma_install/lib:/autofs/nccs-svm1_home1/shubhankar/magma_install/lib:/opt/ibm/spectrumcomputing/lsf/10.1/linux3.10-glibc2.17-ppc64le-csm/lib:/autofs/nccs-svm1_home1/shubhankar/miniconda3/envs/pytorch/lib:/autofs/nccs-svm1_home/shubhankar/miniconda3/envs/pytorch/lib:/autofs/nccs-svm1_home/shubhankar/pytorch/build/lib:/autofs/nccs-svm1_home1/shubhankar/pytorch/build/lib:/autofs/nccs-svm1_home/shubhankar/libcudnn7_7.0.3.11-1+cuda8.0_ppc64el/lib/powerpc64le-linux-gnu:/sw/summitdev/cuda/8.0.61-1/extras/CUPTI/lib64:/autofs/nccs-svm1_home1/shubhankar/miniconda3/envs/pytorch/lib:/autofs/nccs-svm1_home/shubhankar/miniconda3/envs/pytorch/lib:/autofs/nccs-svm1_home/shubhankar/pytorch/build/lib:/autofs/nccs-svm1_home1/shubhankar/pytorch/build/lib:/sw/summitdev/cuda/9.0.69/extras/CUPTI/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed Sep 19 23:25:19 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.46                 Driver Version: 390.46                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P100-SXM2...  Off  | 00000002:01:00.0 Off |                    0 |\r\n| N/A   32C    P0    29W / 300W |     18MiB / 16280MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla P100-SXM2...  Off  | 00000003:01:00.0 Off |                    0 |\r\n| N/A   28C    P0    30W / 300W |     18MiB / 16280MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla P100-SXM2...  Off  | 00000006:01:00.0 Off |                    0 |\r\n| N/A   32C    P0    28W / 300W |     18MiB / 16280MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla P100-SXM2...  Off  | 00000007:01:00.0 Off |                    0 |\r\n| N/A   29C    P0    30W / 300W |     18MiB / 16280MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n\r\n### Describe the problem\r\nBuilding tensorflow from source fails on power architecture machine. I am following the instructions given [here](https://developer.ibm.com/tutorials/install-tensorflow-on-power/)\r\n\r\n### Source code / logs\r\n```\r\nWARNING: Output base '/autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/e74c0b81a4afde0b86e7c039eda6fc5e' is on NFS. This may lead to surprising failures and undetermined behavior.\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n    File \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1166\r\n        _create_local_cuda_repository(repository_ctx)\r\n    File \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 995, in _create_local_cuda_repository\r\n        _get_cuda_config(repository_ctx)\r\n    File \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 750, in _get_cuda_config\r\n        _cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n    File \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 464, in _cudnn_version\r\n        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\r\n    File \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 707, in _find_cudnn_header_dir\r\n        auto_configure_fail((\"Cannot find cudnn.h under %s\" ...))\r\n    File \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n        fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find cudnn.h under /autofs/nccs-svm1_home1/shubhankar/libcudnn7_7.0.3.11-1+cuda8.0_ppc64el/lib/powerpc64le-linux-gnu\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n    File \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1166\r\n        _create_local_cuda_repository(repository_ctx)\r\n    File \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 995, in _create_local_cuda_repository\r\n        _get_cuda_config(repository_ctx)\r\n    File \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 750, in _get_cuda_config\r\n        _cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n    File \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 464, in _cudnn_version\r\n        _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\r\n    File \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 707, in _find_cudnn_header_dir\r\n        auto_configure_fail((\"Cannot find cudnn.h under %s\" ...))\r\n    File \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n        fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find cudnn.h under /autofs/nccs-svm1_home1/shubhankar/libcudnn7_7.0.3.11-1+cuda8.0_ppc64el/lib/powerpc64le-linux-gnu\r\nINFO: Elapsed time: 0.197s\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n```\r\n\r\n\r\n**NOTE:** Don't have sudo access and can't modify /usr/local/* but have all dependencies installed.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "For ppc64, we do not have official support but IBM is working on keeping TF working on ppc64.\r\nFrom what I can see, is it possible you forgot to run `./configure`\r\n\r\nalso cc @wdirons @sandipmgiri ", "@gunan Nope I did.", "the main problem is this \r\n```\r\nCuda Configuration Error: Cannot find cudnn.h under /autofs/nccs-svm1_home1/shubhankar/libcudnn7_7.0.3.11-1+cuda8.0_ppc64el/lib/powerpc64le-linux-gnu\r\n```\r\nIt is looking for the cudnn headers in the wrong place.\r\n", "I guess cudnn.h usually lives inside /usr/local/cuda/ but I have cuda installed separately but I didn't find that file there either. Also I don't have cuDNN installed with CUDA so had to install it separately.", "Hi @ghltshubh , I can help you get TensorFlow compiled on Power (or perhaps point you to a whl file where it is already compiled.)\r\n\r\nSome questions:\r\n\r\nDo you want to use CUDA 8.0 and cuDNN 7.0 as those versions are old, or are you just using those levels because that is what is documented at that website? \r\n\r\nCan you share the contents of .tf_configure.bazelrc (in the root of your tensorflow directory) which will show me the paths you provided with the ./configure script.", "I am using CUDA 8.0 and cuDNN 7.0 just because those are mentioned on the website. I have CUDA 9.0 and cuDNN 7.0 as well. I tried CUDA 9 didn't work either. Contents of the .tf_configure.bazelrc\r\n\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/ccs/home/shubhankar/miniconda3/envs/tensorflow/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/ccs/home/shubhankar/miniconda3/envs/tensorflow/lib/python3.5/site-packages\"\r\nbuild --python_path=\"/ccs/home/shubhankar/miniconda3/envs/tensorflow/bin/python\"\r\nbuild --define with_jemalloc=true\r\nbuild:gcp --define with_gcp_support=true\r\nbuild:hdfs --define with_hdfs_support=true\r\nbuild:s3 --define with_s3_support=true\r\nbuild:kafka --define with_kafka_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild:gdr --define with_gdr_support=true\r\nbuild:verbs --define with_verbs_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/ccs/home/shubhankar/8.0.61-1/\"\r\nbuild --action_env TF_CUDA_VERSION=\"8.0\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/autofs/nccs-svm1_home1/shubhankar/libcudnn7_7.0.3.11-1+cuda8.0_ppc64el/lib/powerpc64le-linux-gnu\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_NCCL_VERSION=\"1\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"3.5,5.2\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/autofs/nccs-svm1_sw/summitdev/.swci/1-compute/opt/spack/20180406/linux-rhel7-ppc64le/gcc-4.8.5/spectrum-mpi-10.2.0.0-20180110-kfmznqmxgfqvpxzgtkuhlaggidefbkjw/lib:/sw/summitdev/cuda/8.0.61-1/lib64:/autofs/nccs-svm1_home/shubhankar/magma_install/lib:/autofs/nccs-svm1_home1/shubhankar/magma_install/lib:/opt/ibm/spectrumcomputing/lsf/10.1/linux3.10-glibc2.17-ppc64le-csm/lib:/autofs/nccs-svm1_home1/shubhankar/miniconda3/envs/pytorch/lib:/autofs/nccs-svm1_home/shubhankar/miniconda3/envs/pytorch/lib:/autofs/nccs-svm1_home/shubhankar/pytorch/build/lib:/autofs/nccs-svm1_home1/shubhankar/pytorch/build/lib:/autofs/nccs-svm1_home/shubhankar/libcudnn7_7.0.3.11-1+cuda8.0_ppc64el/lib/powerpc64le-linux-gnu:/sw/summitdev/cuda/8.0.61-1/extras/CUPTI/lib64\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild --define grpc_no_ares=true\r\nbuild:opt --copt=-mcpu=power8\r\nbuild:opt --copt=-mtune=power8\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --strip=always\r\n\r\n```", "I myself have never built with anything below CUDA 9.0, but it should be fine to build with either version.\r\n\r\nMake sure /autofs/nccs-svm1_home1/shubhankar/libcudnn7_7.0.3.11-1+cuda8.0_ppc64el/lib/powerpc64le-linux-gnu/include/cudnn.h and /autofs/nccs-svm1_home1/shubhankar/libcudnn7_7.0.3.11-1+cuda8.0_ppc64el/lib/powerpc64le-linux-gnu/lib64/libcudnn.so* exists on your system. \r\n\r\nThe build looks for the files in CUDNN_INSTALL_PATH/include and CUDNN_INSTALL_PATH/lib64. If CUDNN_INSTALL_PATH is not correct, correct it and try the build again.", "These are the only contents of my libcudnn7_7.0.3.11-1+cuda8.0_ppc64el\r\ndrwxr-xr-x  0 root   root        0 Sep 16  2017 ./\r\ndrwxr-xr-x  0 root   root        0 Sep 16  2017 ./usr/\r\ndrwxr-xr-x  0 root   root        0 Sep 16  2017 ./usr/share/\r\ndrwxr-xr-x  0 root   root        0 Sep 16  2017 ./usr/share/doc/\r\ndrwxr-xr-x  0 root   root        0 Sep 16  2017 ./usr/share/doc/libcudnn7/\r\n-rw-r--r--  0 root   root        0 Aug 31  2017 ./usr/share/doc/libcudnn7/copyright\r\n-rw-r--r--  0 root   root      158 Sep 16  2017 ./usr/share/doc/libcudnn7/changelog.Debian.gz\r\ndrwxr-xr-x  0 root   root        0 Sep 16  2017 ./usr/share/lintian/\r\ndrwxr-xr-x  0 root   root        0 Sep 16  2017 ./usr/share/lintian/overrides/\r\n-rw-r--r--  0 root   root       45 Sep 16  2017 ./usr/share/lintian/overrides/libcudnn7\r\ndrwxr-xr-x  0 root   root        0 Sep 16  2017 ./usr/lib/\r\ndrwxr-xr-x  0 root   root        0 Sep 16  2017 ./usr/lib/powerpc64le-linux-gnu/\r\n-rw-r--r--  0 root   root 217439272 Sep 16  2017 ./usr/lib/powerpc64le-linux-gnu/libcudnn.so.7.0.3\r\nlrwxrwxrwx  0 root   root         0 Sep 16  2017 ./usr/lib/powerpc64le-linux-gnu/libcudnn.so.7 -> libcudnn.so.7.0.3\r\n\r\n\r\n**NOTE:** I don't have dpkg installed on my system and I don't have permissions to install it either. So I downloaded libcudnn7_7.0.3.11-1+cuda8.0_ppc64el from [nvidia repo](https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/ppc64el/) on my macbook, extracted .deb file and converted it to tar.gz which I then sent to my install machine. ", "I think you also need: libcudnn7-dev_7.0.3.11-1+cuda8.0_ppc64el.deb to get access to cudnn.h\r\n\r\nAlthough I'm not sure if using files from a deb package on a rhel system is the best idea. I'm guessing after this problem, you'll have problems with CUDA.\r\n\r\nYou can download a cuDNN .tar.gz for non-ubuntu systems from nvidia if you register with them: https://developer.nvidia.com/cudnn\r\n\r\nIs it possible to build within a docker container? Do you have docker and nvidia-docker installed ? You can run an ubuntu docker container on your rhel host with all the cuda pre-preqs pre-installed. https://hub.docker.com/r/nvidia/cuda-ppc64le/\r\n\r\nOr you can download a pre-built whl file for CUDA 9.0: \r\nhttps://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/   [New URL on 9/26]", "No I can't build on docker as there is no root access. I will try these solutions and let you know.", "@ghltshubh , Any luck with your build?", "@wdirons Closed the issue by mistake. I tried using _cuDNN v7.3.0 Library for Linux_ but didn't work. \r\nhttps://powerci.osuosl.org/job/TensorFlow_Ubuntu_16.04_PPC64LE_Release_Build/lastSuccessfulBuild/artifact/tensorflow_pkg/ shows me 404 error.", "I renamed the build on Friday to include GPU in the name :) The URL is now:\r\n\r\nhttps://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/", "I tried with cudnn-8.0-linux-ppc64le-v7.tar.gz. I don't get it why I am still getting this error. I even created a symlink `ln -s /autofs/nccs-svm1_home1/shubhankar/cuda/targets/ppc64le-linux/include/cudnn.h /autofs/nccs-svm1_home1/shubhankar/cuda/targets/ppc64le-linux/lib/cudnn.h`\r\n```\r\nWARNING: Output base '/autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/e74c0b81a4afde0b86e7c039eda6fc5e' is on NFS. This may lead to surprising failures and undetermined behavior.\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1166\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 995, in _create_local_cuda_repository\r\n\t\t_get_cuda_config(repository_ctx)\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 750, in _get_cuda_config\r\n\t\t_cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 464, in _cudnn_version\r\n\t\t_find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 707, in _find_cudnn_header_dir\r\n\t\tauto_configure_fail((\"Cannot find cudnn.h under %s\" ...))\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find cudnn.h under /autofs/nccs-svm1_home1/shubhankar/cuda/targets/ppc64le-linux/lib\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1166\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 995, in _create_local_cuda_repository\r\n\t\t_get_cuda_config(repository_ctx)\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 750, in _get_cuda_config\r\n\t\t_cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 464, in _cudnn_version\r\n\t\t_find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 707, in _find_cudnn_header_dir\r\n\t\tauto_configure_fail((\"Cannot find cudnn.h under %s\" ...))\r\n\tFile \"/autofs/nccs-svm1_home1/shubhankar/tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find cudnn.h under /autofs/nccs-svm1_home1/shubhankar/cuda/targets/ppc64le-linux/lib\r\nINFO: Elapsed time: 0.258s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n```\r\n\r\nAnd with the wheel I am getting \r\n`tensorflow_gpu-1.10.1-cp35-cp35m-linux_ppc64le.whl is not a supported wheel on this platform.`", "In .tf_configure.bazelrc\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/autofs/nccs-svm1_home1/shubhankar/cuda/targets/ppc64le-linux/lib\"\r\n\r\nneed to drop the /lib at the end and change to:\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/autofs/nccs-svm1_home1/shubhankar/cuda/targets/ppc64le-linux\"\r\n\r\nIt looks for CUDNN_INSTALL_PATH/include/cudnn,h", "Running wheel tensorflow_gpu-1.10.1-cp36-cp36m-linux_ppc64le.whl  gives the following error:\r\nWith CUDA 9.0 and cuDNN 7.0 gives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/ccs/home/shubhankar/miniconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/ccs/home/shubhankar/miniconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib64/ld64.so.2: version `GLIBC_2.22' not found (required by /ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/ccs/home/shubhankar/miniconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/ccs/home/shubhankar/miniconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib64/ld64.so.2: version `GLIBC_2.22' not found (required by /ccs/home/shubhankar/miniconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n```\r\n\r\nAlso I tried https://github.com/tensorflow/tensorflow/issues/22400#issuecomment-424452557 but I get the following error:\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /sw/summitdev/cuda/8.0.61-1/]:/autofs/nccs-svm1_home1/shubhankar/cuda/targets/ppc64le-linux\r\n\r\n```\r\n\r\nInvalid path to cuDNN 7 toolkit. None of the following files can be found:\r\n/autofs/nccs-svm1_home1/shubhankar/cuda/targets/ppc64le-linux/lib64/libcudnn.so.7\r\n/autofs/nccs-svm1_home1/shubhankar/cuda/targets/ppc64le-linux/libcudnn.so.7\r\nNone.7\r\n\r\n```", "I even copied cudnn.h to /autofs/nccs-svm1_home1/shubhankar/cuda/targets/ppc64le-linux/lib directory but it still can't find it.", "The whl file was compiled on an Ubuntu 16.04 system with glibc 2.22. I'm asking around if there is a way to compile it for an older version of glibc. \r\n\r\nFor the message:\r\n```\r\nInvalid path to cuDNN 7 toolkit. None of the following files can be found:\r\n/autofs/nccs-svm1_home1/shubhankar/cuda/targets/ppc64le-linux/lib64/libcudnn.so.7\r\n```\r\nit is looking for a lib64 (and not the lib directory) for libcudnn.so.7. Maybe symlink lib64 to lib?\r\n", "(Response to the glibc part of the previous comment)\r\nBased on our experience with x86, there is no straightforward way to do it.\r\nYou can statically link glibc and libstdcxx into your binary, but that can cause other problems.\r\nThis is why we still build our binaries on ubuntu 14.", "Thanks @gunan for the comment about building on ububtu 14, I'll see if i'm able to switch our build process to use an ubuntu 14 container. ", "@wdirons Can this solution help. It didn't help me though . https://github.com/tensorflow/tensorflow/issues/53#issuecomment-346976898       ", "@ghltshubh, that comment references compiling with a different level of glibc, the issue you have when using the whl file is at runtime.\r\n\r\nFrom my stand point it will probably be easier to me to compile with an ubuntu 14.04 container then use those instructions. Thanks.", "@wdirons Tried various permutations and combinations of bazel, tf, gcc, cuda and this is the least error I could achieve. Similar error showed up using different versions as well.. The following is using bazel 0.13.1/ 0.14.0, tf 1.8.0 and gcc 4.8.5 and cuda 9.0\r\n\r\n```\r\nERROR: /autofs/nccs-svm1_home1/shubhankar/tensorflow/tensorflow/contrib/framework/BUILD:101:1: output 'tensorflow/contrib/framework/_objs/python/ops/_variable_ops_gpu/tensorflow/contrib/framework/kernels/zero_initializer_op_gpu.cu.pic.o' was not created\r\nERROR: /autofs/nccs-svm1_home1/shubhankar/tensorflow/tensorflow/contrib/framework/BUILD:101:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 232.502s, Critical Path: 97.67s\r\nINFO: 3268 processes, local.\r\nFAILED: Build did NOT complete successfully\r\n```", "Was able to build tf v1.8 397f04acb1faeff451691d7fdc0f754eeb547cc1 as mentioned in https://developer.ibm.com/tutorials/install-tensorflow-on-power/ had to downgrade bazel to 0.11.1.\r\nEdit: cp cuda to local directory in case you don't have sudo access in order to create the symlink mentioned in the error section at the bottom in the documentation page. ", "@wdirons How did you build the tf wheel? Is their any recipe that I could follow to safely build it so that I can distribute it. Thanks.", "I'm happy to hear you got the build to work.\r\n\r\nReferencing the instructions you were using: https://developer.ibm.com/tutorials/install-tensorflow-on-power/\r\n\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package ../tensorflow_pkg\r\n\r\nThat last command will create the whl file in ../tensorflow_pkg\r\n\r\nThe whl file will only work with the CUDA version you compiled against. (And the cuDNN major version and nccl major version)", "oh ok got it. Btw which NCCL version does it use. It never asked for NCCL though. Can I build it using NCCL 2.x?", "From the contents of .tf_configure.bazelrc posted above, I see you answered nccl 1 in ./configure\r\n\r\nNCCL2 will work, just download the os/agnostic package from: https://developer.nvidia.com/nccl/nccl-download and extract the file and use .configure to point at the directory containing the include and lib (or lib64) directory. ", "@wdirons @gunan I tried linking nccl-2.2.13 but it failed I am not sure if it's path issue or legacy bazel 0.11.1 version. However the documentation I followed to link nccl also mentions to symlink\r\n\r\n`sudo ln -s /usr/local/nccl-2.1/include/nccl.h /usr/include/nccl.h` but since I don't have root access I didn't do this step.\r\n\r\n```\r\nERROR: /autofs/nccs-svm1_home1/shubhankar/tensorflow/tensorflow/contrib/nccl/BUILD:23:1: Linking of rule '//tensorflow/contrib/nccl:python/ops/_nccl_ops.so' failed (Exit 1)\r\n/usr/bin/ld: skipping incompatible bazel-out/ppc-opt/bin/_solib_local/_U@local_Uconfig_Unccl_S_S_Cnccl___Uexternal_Slocal_Uconfig_Unccl_Snccl_Slib/libnccl.so.2 when searching for -l:libnccl.so.2\r\n/usr/bin/ld: skipping incompatible bazel-out/ppc-opt/bin/_solib_local/_U@local_Uconfig_Unccl_S_S_Cnccl___Uexternal_Slocal_Uconfig_Unccl_Snccl_Slib/libnccl.so.2 when searching for -l:libnccl.so.2\r\n/usr/bin/ld: cannot find -l:libnccl.so.2\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 988.531s, Critical Path: 771.31s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "We just upgraded the master to NCCL 2.3, which is opensource.\r\nCould you retry?", "@gunan Do I have to change from CUDA 8 to 9 as well? Btw I am not using master I am using: commit: 397f04acb1faeff451691d7fdc0f754eeb547cc1 (June 5, 2018). As told previously my system is little bit old and I could only make Bazel 0.11.1 work on it. \r\n```\r\nLSB Version:\t:core-4.1-noarch:core-4.1-ppc64le\r\nDistributor ID:\tRedHatEnterpriseServer\r\nDescription:\tRed Hat Enterprise Linux Server release 7.4 (Maipo)\r\nRelease:\t7.4\r\nCodename:\tMaipo\r\n```\r\nNCCL 2 requires at least glibc 2.19 whereas I have glibc 2.17 however I did find \r\n[AWS](https://aws.amazon.com/marketplace/pp/B077GFM7L7) providing something similar but it runs NCCL 2 with glibc 2.17 that's pretty odd to me.", "@gunan @wdirons I tried installing tf 1.8 https://github.com/tensorflow/tensorflow/commit/397f04acb1faeff451691d7fdc0f754eeb547cc1 with NCCL 2.3 built from source but got the following error:\r\n\r\n```\r\nERROR: /autofs/nccs-svm1_home1/shubhankar/tensorflow/tensorflow/BUILD:541:1: Executing genrule //tensorflow:python_api_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/e74c0b81a4afde0b86e7c039eda6fc5e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/e74c0b81a4afde0b86e7c039eda6fc5e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/e74c0b81a4afde0b86e7c039eda6fc5e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/ccs/home/shubhankar/miniconda3/envs/tf_p3.5/lib/python3.5/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/ccs/home/shubhankar/miniconda3/envs/tf_p3.5/lib/python3.5/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/e74c0b81a4afde0b86e7c039eda6fc5e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: LLVMInitializePowerPCTargetMC\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/e74c0b81a4afde0b86e7c039eda6fc5e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.util import tf_decorator\r\n  File \"/autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/e74c0b81a4afde0b86e7c039eda6fc5e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/e74c0b81a4afde0b86e7c039eda6fc5e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/e74c0b81a4afde0b86e7c039eda6fc5e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/e74c0b81a4afde0b86e7c039eda6fc5e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/e74c0b81a4afde0b86e7c039eda6fc5e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/ccs/home/shubhankar/miniconda3/envs/tf_p3.5/lib/python3.5/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/ccs/home/shubhankar/miniconda3/envs/tf_p3.5/lib/python3.5/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /autofs/nccs-svm1_home1/shubhankar/.cache/bazel/_bazel_shubhankar/e74c0b81a4afde0b86e7c039eda6fc5e/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: LLVMInitializePowerPCTargetMC\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 324.978s, Critical Path: 159.69s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Edit: Fixed this by turning XLA off.**", "The error \"undefined symbol: LLVMInitializePowerPCTargetMC\" looks like your building with XLA on? I would try building with XLA off.", "I was able to build tf 1.8 with os agnostic NCCL 2.3 built from source with minor changes to NCCL.\r\nSymlink LICENSE.txt in nccl dir to NCCL-SLA.txt"]}]