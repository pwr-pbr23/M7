[{"number": 20171, "title": "Remove a unnecessary dependency to accelerate compilation", "body": "`//tensorflow/contrib/lite/toco:toco` now depends on many kernel implementations when building. It is very annoying on laptops. After some Bazel queries, it looks like they are introduced here and can be safely removed without breaking `toco`.\r\n\r\nI did not do a full build , so I am not sure if it breaks other targets. Can CI help verify this?", "comments": ["Multiple presubmit builds are broken due to this change. ", "Feel free to close this PR if you cannot fix it further."]}, {"number": 20170, "title": "bundled jsoncpp version is ancient and has a security issue", "body": "When I was going through the dependencies in workspace.bzl, I noticed that the bundled version of jsoncpp is super old. it points to https://github.com/open-source-parsers/jsoncpp/commit/11086dd6a7eba04289944367ca82cea71299ed70 which is from 2014. ie before version 0.7.0.\r\n\r\nSome quick searching also turned up these security issues:\r\nhttps://github.com/open-source-parsers/jsoncpp/issues/88\r\nhttps://github.com/open-source-parsers/jsoncpp/issues/56\r\nwhich were fixed only in 0.8.0 at the earliest. Looks like an unbounded stack overflow; I have not looked much into the specifics but a security issue is a security issue so this should be fixed ASAP.\r\n\r\nhttps://github.com/open-source-parsers/jsoncpp/releases\r\nAs of right now the latest version is 1.8.4. When I tried unbundling jsoncpp, using 1.8.4 failed to compile so there is some API change that will need porting too. There appear to have been a few soversion bumps along the way so probably best off to skip straight to the latest 1.8.4.\r\n\r\nI'm filing this dep separately since its a high priority, I will follow up with notes on other not-up-to-date dependencies later on. @martinwicke @gunan @ewilderj ", "comments": ["Added PR #20182 to update jsoncpp.", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce"]}, {"number": 20169, "title": "E1101:Module 'tensorflow.tools.api.generator.api.contrib' has no 'lite' member even though i'm using version 1.8.0", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\nCould you also reference the code for the error.", "Ubuntu 18.0.4 64bit\ni have installed a binary version of tensorflow\n\n*tensorflow-1.8.0*\n\nThanks for looking into this\n\nOn Wed, Jul 4, 2018 at 4:53 AM shivaniag <notifications@github.com> wrote:\n\n> Please provide details about what platform you are using (operating\n> system, architecture). Also include your TensorFlow version. Also, did you\n> compile from source or install a binary?\n> Could you also reference the code for the error.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20169#issuecomment-402318715>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AejTJiTkrNIdGNZYPKDmp9Sb_KkKQ0R1ks5uC_z-gaJpZM4UxVCR>\n> .\n>\n", "On Wed, Jul 4, 2018 at 7:55 AM Anuk Kavishka <anukkavishka94@gmail.com>\nwrote:\n\n> Ubuntu 18.0.4 64bit\n> i have installed a binary version of tensorflow\n>\n> *tensorflow-1.8.0*\n>\n> Thanks for looking into this\n>\n> On Wed, Jul 4, 2018 at 4:53 AM shivaniag <notifications@github.com> wrote:\n>\n>> Please provide details about what platform you are using (operating\n>> system, architecture). Also include your TensorFlow version. Also, did you\n>> compile from source or install a binary?\n>> Could you also reference the code for the error.\n>>\n>> \u2014\n>> You are receiving this because you authored the thread.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/20169#issuecomment-402318715>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AejTJiTkrNIdGNZYPKDmp9Sb_KkKQ0R1ks5uC_z-gaJpZM4UxVCR>\n>> .\n>>\n>\n\n\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n#code above here let's you import the tf sites repository for datasets for tutorials\r\nsess=tf.InteractiveSession()#in this Deep nueral network we use InteractiveSession()\r\n#we use the TF helper function to pull down the data from the MNIST site\r\nmnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\r\n\r\n#we create a placeholder for 28*28 image data to be passed to our neural network\r\nx=tf.placeholder(tf.float32,shape=[None,784]) \r\ny_=tf.placeholder(tf.float32,shape=[None,10])\r\n\r\n#changing the MNIST input data from a list of values to a 28*28*1 grayscale valued cube\r\n#which the Convolution NN can use:\r\n\r\nx_image=tf.reshape(x,[-1,28,28,1],name=\"x_image\")\r\n\r\n#we are going to use RELU as our activation function and if the value of x<0 y=0;x>0 y=x\r\n\r\n#define helper functions to created the weights and baises variables and convolution and pooling layers\r\n\r\n#these must be initialized to a small positive number\r\n#and with some noise you don't end up going to zero when comparing diffs\r\n\r\ndef weight_variable(shape):\r\n    initial = tf.truncated_normal(shape,stddev=0.1)\r\n    return tf.Variable(initial)\r\n\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.1,shape=shape)\r\n    return tf.Variable(initial)\r\n    \r\n#convolution and the pooling we do Convolution,and then pooling to control overfitting\r\n\r\ndef conv2d(x,W):\r\n    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\r\n\r\ndef max_pool_2x2(x):\r\n    return tf.nn.max_pool(x , ksize=[1,2,2,1],strides=[1,1,1,1],padding='SAME')\r\n\r\n\r\n#define layers in the NN\r\n\r\n#1st CNN\r\n#32 features for each 5x5 patch of the image\r\n\r\nW_conv1=weight_variable([5,5,1,32]) #1 para represents the input channels since it's gray scale \r\n                                    #we have only 1,if we were using RGB we will have 3\r\nb_conv1=bias_variable([32])\r\n#Do convolution on images,add bias and push through RELU activation\r\nh_conv1=tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)\r\n#take the results and run through max_pool\r\nh_pool1=max_pool_2x2(h_conv1)\r\n\r\n#2nd convolutional layer \r\n#process the 32 features from the first convolutional layer in 5x5 patch.\r\n#Return 64 features weights and biases.\r\n\r\nW_conv2=weight_variable([5,5,32,64]) \r\nb_conv2=bias_variable([64])\r\n\r\n#Do convolution of the output of the 1st convolution latyer pool results\r\nh_conv2=tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)\r\n#take the results and run through max_pool\r\nh_pool2=max_pool_2x2(h_conv2)\r\n\r\n#defining fully connected layer\r\n\r\nW_fulconnc1=weight_variable([7 * 7 * 64,1024]) \r\nb_fulconnc1=bias_variable([1024])\r\n\r\n#connect output of pooling layer 2 as input for the fully connected layer\r\nh_pool2_flat=tf.reshape(h_pool2, [-1 , 7 * 7 * 64] )\r\nh_fulconnc1=tf.nn.relu(tf.matmul(h_pool2_flat,W_fulconnc1) + b_fulconnc1)\r\n\r\n#since we are using backProp algorithm alos the model might overfit the testing data\r\n#and the real world data will be predicted incorrectly,so we drop out few nuerons from the\r\n#fully connected layer\r\n#to solve this we use the below code\r\nkeep_prob=tf.placeholder(tf.float32) # get the drop out probability as a training input\r\nh_fulconnc1_drop=tf.nn.dropout(h_fulconnc1,keep_prob)\r\n\r\n#Readout layer\r\nW_fulconnc2=weight_variable([1024,10]) \r\nb_fulconnc2=bias_variable([10])\r\n\r\n#define model\r\ny_conv=tf.matmul(h_fulconnc1_drop,W_fulconnc2) +b_fulconnc2\r\n\r\n\r\n\r\n#now we have to create a function to measure the loss just like in previous models\r\ncross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv,labels=y_))\r\n\r\n#y_ is the result given value and the y_conv is the predicted value\r\n\r\n#now let's optimize \r\ntrain_step=tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\n\r\n#what is the correct prediction\r\n\r\ncorrect_prediction=tf.equal(tf.argmax(y_conv,1),tf.argmax(y_,1))\r\n\r\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n\r\n#initialize all of the variables\r\nsess.run(tf.global_variables_initializer())\r\n\r\n#train the model\r\nimport time\r\n\r\n#define num of steps and how often we display the progress\r\n\r\nnum_steps =3000\r\ndisplay_every=100\r\n\r\n#starting the timer\r\n\r\nstart_time=time.time()\r\nend_time=time.time()\r\n\r\nfor i in range(num_steps):\r\n    batch=mnist.train.next_batch(50)\r\n    train_step.run(feed_dict={x:batch[0] , y_:batch[1] , keep_prob:0.5})\r\n\r\n    #periodic status display\r\n    if i%display_every == 0 :\r\n        train_accuracy=accuracy.eval(feed_dict={\r\n             x:batch[0] , y_:batch[1] , keep_prob :1.0})\r\n        end_time=time.time()\r\n        print(\"step {0},elapsed time {1:.2f} seconds,training accuracy{2:.3f}% \".format(i,end_time-start_time,train_accuracy*100))\r\n\r\n\r\n#display the summary\r\n\r\nend_time=time.time()\r\n\r\nprint(\"Total training time for {0} batches : {1:.2f} seconds\".format(i+1,end_time-start_time))\r\n\r\n#test the accuracy on the test data\r\n\r\nprint(\"Test Accuracy {0:.3f}%\".format(accuracy.eval(feed_dict={\r\nx:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0})*100.0))\r\n\r\n\r\n", "can you provide the full backtrace. I also don't see anything touching a \"lite\" namespace in your code?\r\nDoes this still occur on 1.9, which was just released?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20168, "title": "Exception in thread \"main\" java.lang.UnsatisfiedLinkError: C:\\Users\\Luke\\AppData\\Local\\Temp\\tensorflow_native_libraries-1529547871141-0\\tensorflow_jni.dll:  \u00ad\u0019U", "body": "I have search on SO what could be the reason why i am having this issue, but have not reached on the right answer. \r\nMy System Info:\r\nWindows 7 6bit\r\nJdk 8u111\r\n\r\nI have created a Maven Project and added dependencies  for tensorflow \r\n`\t<dependency>\r\n\t\t<groupId>org.tensorflow</groupId>\r\n\t\t<artifactId>tensorflow</artifactId>\r\n\t\t<version>1.8.0</version>\r\n\t</dependency>   \r\n\t\t<dependency>\r\n\t\t    <groupId>org.tensorflow</groupId>\r\n\t\t    <artifactId>libtensorflow_jni</artifactId>\r\n\t\t    <version>1.8.0</version>\r\n     </dependency>`\r\n\r\nand when i clean and build the application\r\ni run the program\r\n`  public static void main(String[] args) throws Exception {\r\n\r\n    try (Graph g = new Graph()) {\r\n\r\n      final String value = \"Hello from \" + TensorFlow.version();\r\n\r\n      // Construct the computation graph with a single operation, a constant\r\n\r\n      // named \"MyConst\" with a value \"value\".\r\n\r\n      try (Tensor t = Tensor.create(value.getBytes(\"UTF-8\"))) {\r\n\r\n        // The Java API doesn't yet include convenience functions for adding operations.\r\n\r\n        g.opBuilder(\"Const\", \"MyConst\").setAttr(\"dtype\", t.dataType()).setAttr(\"value\", t).build();\r\n\r\n      }\r\n\r\n      // Execute the \"MyConst\" operation in a Session.\r\n\r\n      try (Session s = new Session(g);\r\n\r\n           Tensor output = s.runner().fetch(\"MyConst\").run().get(0)) {\r\n\r\n        System.out.println(new String(output.bytesValue(), \"UTF-8\"));\r\n\r\n      }\r\n\r\n    }\r\n\r\n  }`\r\nthe error i am having is this:\r\n`Exception in thread \"main\" java.lang.UnsatisfiedLinkError: C:\\Users\\Luke\\AppData\\Local\\Temp\\tensorflow_native_libraries-1529547871141-0\\tensorflow_jni.dll:  \u00ad\u0019U`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "> the issue template.\r\nSir you can help with the statement above.\r\nI really understand what you mean,\r\nOS Platform and Distribution :  I am using Windows 7 Professional 64bit mode.\r\nTensorFlow installed from: I have tried  the Tensorflow basic HelloTF  example by creating a maven project in eclipse oxygen and have added Tensorflow 1.8.0 version in the dependencies and i end up getting Main Class HelloTF not found error.\r\nI also tried adding the DLL  file in my MV Options in a same Netbeans project and putting the jar file in my classpath and got the error which is the subject of this issue.\r\nBazel Version: I don't have it installed on my system\r\nCUDA/cuDNN version:  I don't have it installed on my system\r\n GPU model and memory: No idea about it\r\nExact command to reproduce: No command \r\n\r\nI am sorry if i have not answered correctly though.", "Nagging Assignee @rohan100jain: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry for this delay.\r\nI was yet to understand how to update the labels.\r\nI have filed a similar issue again with this code: #20440\r\nPlease let this issue be close and i will update on the new issue for more response \r\nthanks.", "Nagging Assignee @rohan100jain: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20167, "title": "Improvements to the 1st tutorial ", "body": "Regarding the very first tutorial: https://www.tensorflow.org/get_started/eager\r\n\r\nYou should add the import: \r\n\r\nimport tensorflow.contrib.eager as tfe;\r\n\r\nand change the types for example_default to:\r\n\r\nexample_defaults = [[0.], [0.], [0.], [0.], [0]]  # set field types (the labels are integers: https://stackoverflow.com/questions/50184145/tensorflow-sparsesoftmaxcrossentropywithlogits-error)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "All of these fields are N/A. This is a general improvement of the code for people who copy/paste the code from the tutorial.", "I don't quite follow your suggestion. ", "I see that it was updated."]}, {"number": 20166, "title": "Updating TensorFlow", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 20165, "title": "Removed tfe", "body": "Removed tfe", "comments": []}, {"number": 20164, "title": "[Intel MKL] Adding support for MKL to docker infrastructure", "body": "- MKL container support added to parameterized_docker_build.sh\r\n- MKL containers use `--build-args` rather than `sed` commands\r\n- Old MKL Dockerfile removed; MKL Dockerfiles now follow existing naming convention\r\n- build-dev-container.sh script generates python2 and python3 dev containers\r\n\r\n@case540 and @gunan Hopefully this PR is more in line with expectations.", "comments": []}, {"number": 20163, "title": "Move external/ directory in pip package.", "body": "Moving external/ directory in the pip packages (which is currently\r\ninstalled directly into site-packages directory). Moving external/ \r\nto tensorflow/include/external/.\r\n\r\nAlso, removing all python files from external/ (since we should really only\r\nbe packaging header and license files.)\r\n\r\nRELNOTES: Moving external header files in pip package from site-packages/external to site-packages/tensorflow/includes/external/", "comments": []}, {"number": 20161, "title": "Update doc and add tests about multi_hot labels with vocabulary", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "CLA singed via Spotify, also signed my personal just now.", "CLAs look good, thanks!\n\n<!-- ok -->", "Right now when label vocabulary is set, the doc says that:\r\n\r\n```\r\nIf `label_vocabulary` is given, a string `SparseTensor`. The `dense_shape`\r\nmust be `[D0, D1, ... DN, ?]` and the values within `label_vocabulary`.\r\n```\r\n\r\nbut if you pass the label vocabulary and labels as multi hot it works fine too - see the test. In fact if you trace the code, in such case `label_vocabulary` stays unused afaiu. I don't have a strong opinion about failing in such case - this PR fixes the doc and adds tests for that case. But I can also try to fail in such case if that is what we want to do. What's your opinion?", "@qlzh727 @ispirmustafa thanks for the feedback. The error in the tests doesn't seem related?", "Seems to be a flaky test, let me rerun it."]}, {"number": 20160, "title": "TF Debug Session Wrapper has no attribute '_make_callable_from_options'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 18\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\nv1.8.0-3410-g79d7e11f3e, 1.10.0-dev20180616\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0, cuDNN 7.0\r\n- **GPU model and memory**:\r\nNVIDIA GeForce 1080\r\n- **Exact command to reproduce**:\r\nRunning the provided script should produce the error. The attached file shows the output.\r\n\r\n### Describe the problem\r\nIt's not possible to debug Keras models since `BaseDebugWrapperSession` in `tensorflow/python/debug/wrappers/framework.py` does not implement the `_make_callable_from_options` method.\r\n\r\n### Source code / logs\r\n- **Script**:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow import keras\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\n\r\ninputs = np.zeros((10, 3))\r\ntargets = np.zeros((10, 4))\r\ndataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\r\ndataset = dataset.repeat(100)\r\ndataset = dataset.batch(5)\r\n\r\nwith tf_debug.LocalCLIDebugWrapperSession(tf.Session()) as sess:\r\n    keras.backend.set_session(sess)\r\n\r\n    x = keras.layers.Input(shape=(3,), name='input')\r\n    flat = keras.layers.Flatten()(x)\r\n    y = keras.layers.Dense(4, name='dense')(flat)\r\n\r\n    model = keras.Model(x, y)\r\n    model.compile(loss='mse', optimizer='rmsprop')\r\n\r\n    model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\r\n```\r\n- **Output**:\r\n[tfdbg_output.txt](https://github.com/tensorflow/tensorflow/files/2120950/tfdbg_output.txt)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Thanks for the report. Indeed, seems like a little bit of work may be helpful here.\r\n\r\n@caisq : Would you have the cycles to take this on? If not, let's discuss options.\r\n\r\nFYI @fchollet @mrry ", "@dillondaudert Thank you for reporting this issue.\r\n@asimshankar Thanks for cc'ing me on this issue. I will look into it."]}, {"number": 20159, "title": "Cast: support casting to and from quantized types", "body": "This adds support for casting to and from quantized data types using the Cast operator. It pretty much just changes the types to the non quantized version before calling cast and then changing them back since the quantized types are just a semantic difference and not an underlying one.\r\n\r\nIssue #20150.\r\n\r\nTested with\r\n```\r\nbazel test //tensorflow/core/kernels:cast_op_test -c dbg --test_output=errors\r\n```", "comments": ["I'm not entirely sure about that test failure. I've been digging into it, but I'm not seeing where the Fill operation calls into Cast.", "Oh, it's happening under eager execution modes: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/pywrap_tensor.cc\r\n\r\nI'm not sure why fill can't find a device, but the main difference under eager mode is that casting works with qint16 now. Graph mode appears to be unchanged for that test.\r\n\r\nExpected error under eager:\r\n```\r\n======================================================================\r\nERROR: testUnsupportedDtype (__main__.EmbedCheckCategoricalEventShapeTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/rice/.cache/bazel/_bazel_rice/d01f73017c98ae6e2acf29590b690a6c/execroot/org_tensorflo\r\nw/bazel-out/k8-dbg/bin/tensorflow/python/kernel_tests/distributions/util_test.runfiles/org_tensorfl\r\now/tensorflow/python/framework/test_util.py\", line 647, in decorated\r\n    run_eagerly(self, **kwargs)\r\n  File \"/home/rice/.cache/bazel/_bazel_rice/d01f73017c98ae6e2acf29590b690a6c/execroot/org_tensorflo\r\nw/bazel-out/k8-dbg/bin/tensorflow/python/kernel_tests/distributions/util_test.runfiles/org_tensorfl\r\now/tensorflow/python/framework/test_util.py\", line 639, in run_eagerly\r\n    f(self, **kwargs)\r\n  File \"/home/rice/.cache/bazel/_bazel_rice/d01f73017c98ae6e2acf29590b690a6c/execroot/org_tensorflo\r\nw/bazel-out/k8-dbg/bin/tensorflow/python/kernel_tests/distributions/util_test.runfiles/org_tensorfl\r\now/tensorflow/python/kernel_tests/distributions/util_test.py\", line 374, in testUnsupportedDtype\r\n    param = array_ops.ones([int(2**11+1)], dtype=dtypes.qint16)\r\n  File \"/home/rice/.cache/bazel/_bazel_rice/d01f73017c98ae6e2acf29590b690a6c/execroot/org_tensorflo\r\nw/bazel-out/k8-dbg/bin/tensorflow/python/kernel_tests/distributions/util_test.runfiles/org_tensorfl\r\now/tensorflow/python/ops/array_ops.py\", line 1687, in ones\r\n    output = fill(shape, constant(one, dtype=dtype), name=name)\r\n  File \"/home/rice/.cache/bazel/_bazel_rice/d01f73017c98ae6e2acf29590b690a6c/execroot/org_tensorflo\r\nw/bazel-out/k8-dbg/bin/tensorflow/python/kernel_tests/distributions/util_test.runfiles/org_tensorfl\r\nd 2 of 3):\r\n..2018-06-21 01:03:37.801623: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU support\r\ns instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n...2018-06-21 01:03:38.108347: W tensorflow/core/framework/op_kernel.cc:1305] OP_REQUIRES failed at\r\n cast_op.cc:77 : Unimplemented: Cast int64 to qint16 is not supported\r\nE.............................\r\n======================================================================\r\nERROR: testUnsupportedDtype (__main__.EmbedCheckCategoricalEventShapeTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/rice/.cache/bazel/_bazel_rice/d01f73017c98ae6e2acf29590b690a6c/execroot/org_tensorflo\r\now/tensorflow/python/framework/constant_op.py\", line 167, in constant\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/home/rice/.cache/bazel/_bazel_rice/d01f73017c98ae6e2acf29590b690a6c/execroot/org_tensorflo\r\nw/bazel-out/k8-dbg/bin/tensorflow/python/kernel_tests/distributions/util_test.runfiles/org_tensorfl\r\now/tensorflow/python/framework/constant_op.py\", line 109, in convert_to_eager_tensor\r\n    t = ops.EagerTensor(value, context=handle, device=device, dtype=dtype)\r\nTypeError: Error while casting from DataType 9 to 15. Cast int64 to qint16 is not supported\r\n```\r\n\r\nUnder graph mode there's a different expected error:\r\n```\r\nERROR: testUnsupportedDtype (__main__.EmbedCheckCategoricalEventShapeTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/rice/.cache/bazel/_bazel_rice/d01f73017c98ae6e2acf29590b690a6c/execroot/org_tensorflo\r\nw/bazel-out/k8-dbg/bin/tensorflow/python/kernel_tests/distributions/util_test.runfiles/org_tensorfl\r\now/tensorflow/python/framework/test_util.py\", line 625, in decorated\r\n    f(self, **kwargs)\r\n  File \"/home/rice/.cache/bazel/_bazel_rice/d01f73017c98ae6e2acf29590b690a6c/execroot/org_tensorflo\r\nw/bazel-out/k8-dbg/bin/tensorflow/python/kernel_tests/distributions/util_test.runfiles/org_tensorfl\r\now/tensorflow/python/kernel_tests/distributions/util_test.py\", line 374, in testUnsupportedDtype\r\n    param = array_ops.ones([int(2**11+1)], dtype=dtypes.qint16)\r\n  File \"/home/rice/.cache/bazel/_bazel_rice/d01f73017c98ae6e2acf29590b690a6c/execroot/org_tensorflo\r\nw/bazel-out/k8-dbg/bin/tensorflow/python/kernel_tests/distributions/util_test.runfiles/org_tensorfl\r\now/tensorflow/python/ops/array_ops.py\", line 1687, in ones\r\n    output = fill(shape, constant(one, dtype=dtype), name=name)\r\n  File \"/home/rice/.cache/bazel/_bazel_rice/d01f73017c98ae6e2acf29590b690a6c/execroot/org_tensorflo\r\nw/bazel-out/k8-dbg/bin/tensorflow/python/kernel_tests/distributions/util_test.runfiles/org_tensorfl\r\now/tensorflow/python/framework/constant_op.py\", line 196, in constant\r\n    value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/home/rice/.cache/bazel/_bazel_rice/d01f73017c98ae6e2acf29590b690a6c/execroot/org_tensorflo\r\nw/bazel-out/k8-dbg/bin/tensorflow/python/kernel_tests/distributions/util_test.runfiles/org_tensorfl\r\now/tensorflow/python/framework/tensor_util.py\", line 436, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/home/rice/.cache/bazel/_bazel_rice/d01f73017c98ae6e2acf29590b690a6c/execroot/org_tensorflo\r\nw/bazel-out/k8-dbg/bin/tensorflow/python/kernel_tests/distributions/util_test.runfiles/org_tensorfl\r\now/tensorflow/python/framework/tensor_util.py\", line 347, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected qint16, got 1 of type 'int' instead.\r\n```", "Sorry for the reviewer run-around, but I think @suharshs is the best reviewer for this.", "No worries! I realize this PR is kind of out of the blue. I'm pretty new to the C++ part of the codebase so thanks for taking the time to review it", "@suharshs @andrewharp any ETA on a code review?", "Nudge viewers for review/approval.", "@skye Do you see any issues with the access to set_dtype in tensor.h?", "Looks ok to me, I'm not sure how else you would do it. FYI @gunan, note the use of Tensor::set_dtype (which is private), this might be relevant when thinking about kernel APIs.", "I just pushed a fix for the broken //tensorflow/python/kernel_tests/distributions:util_test  test. It was testing the wrong thing anyways. Instead of testing embed_check_categorical_event_shape correctly threw a TypeError, ones was throwing the TypeError instead.", "I think this code should be pretty ready to check in unless you guys have any comments.", "Nudge viewers for final review/approval.", "@suharshs @andrewharp Any update on this? Don't think there's anything blocking it as far as I'm aware", "I believe it was because we needed to do a copy and set the data type\ninstead of just copying. I suppose we could have done a safe copy and then\nused set_dtype as an alternative though.\n\nOn Mon, Aug 20, 2018 at 11:09 AM Suharsh Sivakumar <notifications@github.com>\nwrote:\n\n> Hi @d4l3k <https://github.com/d4l3k> , can you remind me why the\n> UnsafeCopyFromInternal was needed in you PR? Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/20159#issuecomment-414409632>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA3fMGC07zsnyYAAOb4pAfP6PcTw-tj9ks5uSvtlgaJpZM4Uv7Yr>\n> .\n>\n"]}, {"number": 20158, "title": "Feature Request: Initialize tables when passing a Dataset as input to a Keras model", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 18\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\nv1.8.0-3410-g79d7e11f3e, 1.10.0-dev20180616\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0, cuDNN 7.0\r\n- **GPU model and memory**:\r\nNVIDIA GeForce 1080\r\n- **Exact command to reproduce**:\r\nRunning the provided script should produce this error.\r\n\r\n### Describe the problem\r\ntf.keras.Model will throw a `FailedPreconditionError: Table not initialized.` error when passing a Dataset that includes a lookup table to Model.fit(). This can be worked around by creating a Session and running `tf.tables_initializer()` and `keras.backend.set_session`, but it would be nice if Keras could check for tables and initialize them automatically.\r\n\r\n### Source code / logs\r\n- **Script**:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.ops import lookup_ops\r\nfrom tensorflow import keras\r\n\r\nalphabet = ['A', 'B', 'C']\r\ntable = lookup_ops.index_table_from_tensor(tf.constant(alphabet))\r\n# generate samples of strings of different lengths\r\ninputs = [''.join([np.random.choice(alphabet) for _ in range(5)]) for _ in range(10)]\r\ntargets = np.zeros((10, 4))\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\r\ndef map_fn(x, y):\r\n    x = tf.string_split([x], delimiter=\"\").values\r\n    x = table.lookup(x)\r\n    x = tf.nn.embedding_lookup(tf.eye(3), x)\r\n    return x, y\r\ndataset = dataset.map(lambda x, y: map_fn(x, y))\r\ndataset = dataset.repeat(100)\r\ndataset = dataset.batch(5)\r\n\r\nx = keras.layers.Input(shape=(5, 3), name='input')\r\nflat = keras.layers.Flatten()(x)\r\ny = keras.layers.Dense(4, name='dense')(flat)\r\n\r\nmodel = keras.Model(x, y)\r\nmodel.compile(loss='mse', optimizer='rmsprop')\r\n\r\nmodel.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\r\n```\r\n\r\n- **Output**:\r\n```bash\r\nScript started on 2018-06-20 16:09:12-0400\r\ndillon@dillon-linux:~/github/dillondaudert$ python keras_table_init_ex.py \r\n2018-06-20 16:09:20.610136: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-06-20 16:09:20.717933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:883] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-06-20 16:09:20.718316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8475\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.17GiB\r\n2018-06-20 16:09:20.788286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:883] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-06-20 16:09:20.788649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8475\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.81GiB\r\n2018-06-20 16:09:20.789264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\r\n2018-06-20 16:09:21.112863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-20 16:09:21.112890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \r\n2018-06-20 16:09:21.112895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \r\n2018-06-20 16:09:21.112899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \r\n2018-06-20 16:09:21.113086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6917 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-06-20 16:09:21.169182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7537 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nEpoch 1/1\r\n2018-06-20 16:09:21.490711: W tensorflow/core/framework/op_kernel.cc:1328] OP_REQUIRES failed at lookup_table_op.cc:675 : Failed precondition: Table not initialized.\r\nTraceback (most recent call last):\r\n  File \"keras_table_init_ex.py\", line 29, in <module>\r\n    model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\r\n  File \"/home/dillon/.conda/envs/tf1.10-nightly/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1353, in fit\r\n    validation_steps=validation_steps)\r\n  File \"/home/dillon/.conda/envs/tf1.10-nightly/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 195, in fit_loop\r\n    outs = f(ins)\r\n  File \"/home/dillon/.conda/envs/tf1.10-nightly/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 2897, in __call__\r\n    fetched = self._callable_fn(*array_vals)\r\n  File \"/home/dillon/.conda/envs/tf1.10-nightly/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1382, in __call__\r\n    run_metadata_ptr)\r\n  File \"/home/dillon/.conda/envs/tf1.10-nightly/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 519, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Table not initialized.\r\n\t [[Node: hash_table_Lookup = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT64](hash_table_lookup_placeholder, StringSplit:1, hash_table_lookup_placeholder_1)]]\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,4]], output_types=[DT_FLOAT, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n\t [[Node: IteratorGetNext/_17 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_18_IteratorGetNext\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\ndillon@dillon-linux:~/github/dillondaudert$ exit\r\nexit\r\n\r\nScript done on 2018-06-20 16:09:29-0400\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "As suggested by the poster, the following Callback is a (temporary) fix for the problem:\r\n\r\n```\r\nclass TableInitializerCallback(Callback):\r\n    \"\"\" Initialize Tables \"\"\"\r\n    def on_train_begin(self, logs=None):\r\n        K.get_session().run(tf.tables_initializer())\r\n```", "It has been 34 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "@marco-willi \r\nThis will work for training. How do I make it work with the `predict` method which does not take any callbacks?", "Any update with the predict methid? @dhruvdcoder ", "> @marco-willi\r\n> This will work for training. How do I make it work with the `predict` method which does not take any callbacks?\r\n\r\nso is there any solution (for tf 1.14) ?"]}, {"number": 20157, "title": "TensorRT integration doesn't optimize conv2d_transpose", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource. Build toward TensorRT 3.0.4.\r\n- **TensorFlow version (use command below)**:\r\n1.7\r\n- **Python version**: \r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0, cuDNN 7.1.4\r\n- **GPU model and memory**:\r\nNVIDIA 1080 Ti\r\n- **Exact command to reproduce**:\r\nFirst train a example model use [mnist_conv_deconv.py](https://gist.github.com/qinyao-he/573254f8e225a09114b7408cc1c984aa)\r\nThen use tensorflow built-in tools to freeze the graph:\r\n```\r\npython -m tensorflow.python.tools.freeze_graph --input_graph log/graph.pbtxt --input_checkpoint log/model.ckpt-20000 --output_node_names L2Loss --output_graph log/freeze_graph.pb\r\n```\r\nFinally use [tensorrt.py](https://gist.github.com/qinyao-he/28ddedb7f561bb3cb4ba880833f14a89) to optimize the graph use TensorRT engine.\r\n\r\n### Describe the problem\r\nThe original graph contains 4 convolution and 4 deconvolution layers.\r\n<img width=\"179\" alt=\"snipaste_2018-06-20_12-39-15\" src=\"https://user-images.githubusercontent.com/6523975/41680987-1e912d1e-7488-11e8-8a25-6f8e112cc2e6.png\">\r\n\r\nAfter optimizing, convolution layers was converted into trt_op (successfully optimized by TensorRT), while all deconvolution layers (conv2d_transpose) remains unchanged.\r\n<img width=\"244\" alt=\"snipaste_2018-06-20_12-39-46\" src=\"https://user-images.githubusercontent.com/6523975/41681041-4bba6a08-7488-11e8-970c-2773fedea98a.png\">\r\n\r\nI think TensorRT support deconvolution as shown in their official guide [TensorRT Developer Guide](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html)\r\n\r\n> TensorFlow\r\n> The following list describes the operations that are supported in a TensorFlow framework.\r\n> Placeholder\r\n> Const\r\n> Add, Sub, Mul, Div, Minimum and Maximum\r\n> BiasAdd\r\n> Negative, Abs, Sqrt, Rsqrt, Pow, Exp and Log\r\n> FusedBatchNorm\r\n> ReLU, TanH, Sigmoid\r\n> SoftMax\r\n> Mean\r\n> ConcatV2\r\n> Reshape\r\n> Transpose\r\n> Conv2D\r\n> DepthwiseConv2dNative\r\n> ConvTranspose2D\r\n> MaxPool\r\n> AvgPool\r\n> Pad is supported if followed by one of these TensorFlow layers: Conv2D, DepthwiseConv2dNative, MaxPool, and AvgPool\r\n\r\nSo is there any problem in tensorflow integrations?", "comments": ["@samikama ", "@qinyao-he There could be many reasons but it is not possible to figure out what is wrong from the pictures since only parent scopes are visible in the plots but not the nodes themselves. Would it be possible to share the network? At least some logs? Also for the time being, TFTRT requires shapes to be statically inferable at the conversion time. Are your non-batch input ranks fixed or are the free?", "https://gist.github.com/qinyao-he/573254f8e225a09114b7408cc1c984aa\r\nhttps://gist.github.com/qinyao-he/28ddedb7f561bb3cb4ba880833f14a89\r\n@samikama These are the two code snippet to reproduce as I stated in the issue description.\r\n\r\nAll my non-batch input ranks are fixed.", "@zheng-xq ", "@samikama could someone take a look at this and help debug this issue?", "Do you still see the issue with recent commits?", "@samikama I've just tried the most recent master branch. Seems there is error during the conversion.\r\n\r\n> 2018-06-26 15:20:51.327926: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1                                                                                 \r\n> 2018-06-26 15:20:51.527677: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:726] MULTIPLE tensorrt candidate conversion: 8                                                                  \r\n> 2018-06-26 15:20:51.527853: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2390] Segment @scope '', converted to graph                                                                     \r\n> 2018-06-26 15:20:51.533387: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2390] Segment @scope 'dense/', converted to graph                                                               \r\n> 2018-06-26 15:20:51.533767: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2390] Segment @scope 'conv2d_transpose/', converted to graph                                                    \r\n> 2018-06-26 15:20:51.534064: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2390] Segment @scope 'dropout/dropout/', converted to graph                                                     \r\n> 2018-06-26 15:20:51.534425: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2390] Segment @scope 'conv2d_transpose_2/', converted to graph                                                  \r\n> 2018-06-26 15:20:51.534882: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2390] Segment @scope '', converted to graph                                                                     \r\n> 2018-06-26 15:20:51.535612: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2390] Segment @scope '', converted to graph                                                                     \r\n> 2018-06-26 15:20:51.536245: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2390] Segment @scope 'conv2d_transpose_1/', converted to graph                                                  \r\n> 2018-06-26 15:20:51.537682: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:690]  Can't find a GPU device to work with. Please instantiate a session to initialize devices                  \r\n> 2018-06-26 15:20:51.537710: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:812] Can't identify the cuda device. Running on device 0                                                        \r\n> 2018-06-26 15:20:52.558412: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:690]  Can't find a GPU device to work with. Please instantiate a session to initialize devices                  \r\n> 2018-06-26 15:20:52.558439: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:812] Can't identify the cuda device. Running on device 0                                                        \r\n> 2018-06-26 15:20:52.559444: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger Parameter check failed at: Network.cpp::addInput::281, condition: isIndexedCHW(dims) && volume(dims) < MAX_TENSOR_SIZE\r\n> 2018-06-26 15:20:52.559459: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:825] Engine creation for segment 1, composed of 3 nodes failed: Invalid argument: Failed to create Input layer tensor InputPH_0 rank=1. Skipping...\r\n> 2018-06-26 15:20:52.559486: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:690]  Can't find a GPU device to work with. Please instantiate a session to initialize devices                  \r\n> 2018-06-26 15:20:52.559491: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:812] Can't identify the cuda device. Running on device 0                                                        \r\n> 2018-06-26 15:20:52.582179: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:690]  Can't find a GPU device to work with. Please instantiate a session to initialize devices                  \r\n> 2018-06-26 15:20:52.582202: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:812] Can't identify the cuda device. Running on device 0                                                        \r\n> 2018-06-26 15:20:52.583201: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger Parameter check failed at: Network.cpp::addInput::281, condition: isIndexedCHW(dims) && volume(dims) < MAX_TENSOR_SIZE\r\n> 2018-06-26 15:20:52.583219: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:825] Engine creation for segment 3, composed of 4 nodes failed: Invalid argument: Failed to create Input layer tensor InputPH_0 rank=1. Skipping...\r\n> 2018-06-26 15:20:52.583249: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:690]  Can't find a GPU device to work with. Please instantiate a session to initialize devices                  \r\n> 2018-06-26 15:20:52.583256: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:812] Can't identify the cuda device. Running on device 0                                                        \r\n> 2018-06-26 15:20:52.605704: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:690]  Can't find a GPU device to work with. Please instantiate a session to initialize devices                  \r\n> 2018-06-26 15:20:52.605730: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:812] Can't identify the cuda device. Running on device 0                                                        \r\n> 2018-06-26 15:20:53.000096: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:690]  Can't find a GPU device to work with. Please instantiate a session to initialize devices                  \r\n> 2018-06-26 15:20:53.000124: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:812] Can't identify the cuda device. Running on device 0                                                        \r\n> 2018-06-26 15:20:53.001103: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger Parameter check failed at: Network.cpp::addInput::281, condition: isIndexedCHW(dims) && volume(dims) < MAX_TENSOR_SIZE\r\n> 2018-06-26 15:20:53.001119: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:825] Engine creation for segment 6, composed of 3 nodes failed: Invalid argument: Failed to create Input layer tensor InputPH_0 rank=1. Skipping...\r\n> 2018-06-26 15:20:53.001161: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:690]  Can't find a GPU device to work with. Please instantiate a session to initialize devices                  \r\n> 2018-06-26 15:20:53.001166: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:812] Can't identify the cuda device. Running on device 0                                                        \r\n> [1]    20854 segmentation fault (core dumped)  python tensorrt.py", "@qinyao-he @xiaocai00 I think the segfault comes from shape inference, when PR #20350 is merged it should go away (I tested on my local machine about that). Now looking at your problem about the conversion.", "I ran your script with PR #20350 built with TRT 4.0.1.6, I think all conv layers are now converted:\r\n![0oym5krjj7d](https://user-images.githubusercontent.com/31743510/42596032-18d7f2cc-8509-11e8-941e-aaadd2f3ab48.png)\r\n", "@qinyao-he @xiaocai00 Would you please wait for PR #20350 to be merged, and build against TRT 4.0.1.6 and try again? Thanks.", "@aaroey I've just tried your PR branch but is still the same as before. Will this need to be merged to master first?", "@aaroey I think your result is just the same as mine before. Although there are some trt_op, all conv2d_transpose ops are remains unchanged. So TensorRT still not optimize conv2d_transpose at all.", "@qinyao-he  are you talking about transpose ops? Transposes are never included in TRT, they are to convert data layouts.", "@samikama There are no transpose ops. conv2d_transpose is deconv.", "@qinyao-he the op type of conv2d_transpose ops is Conv2DBackpropInput, which is currently not supported in the [op list](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/convert/convert_nodes.cc#L2547) of the TF-TRT integration.", "@jjsjann123 to check if we would support that op in the future.", "@aaroey The TensorRT developer guide stated that it support ConvTranspose2D in TensorFlow. https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#support_op\r\n\r\n> TensorFlow\r\n> The following list describes the operations that are supported in a TensorFlow framework.\r\n> Placeholder\r\n> Const\r\n> Add, Sub, Mul, Div, Minimum and Maximum\r\n> BiasAdd\r\n> Negative, Abs, Sqrt, Rsqrt, Pow, Exp and Log\r\n> FusedBatchNorm\r\n> ReLU, TanH, Sigmoid\r\n> SoftMax\r\n> Mean\r\n> ConcatV2\r\n> Reshape\r\n> Transpose\r\n> Conv2D\r\n> DepthwiseConv2dNative\r\n> ConvTranspose2D\r\n> MaxPool\r\n> AvgPool\r\n> Pad is supported if followed by one of these TensorFlow layers: Conv2D, DepthwiseConv2dNative, MaxPool, and AvgPool\r\n\r\nSo you mean that the ConvTranspose2D op is not added into this TF-TRT integration?", "@qinyao-he yes that's what I meant. But, you kept mentioning ConvTranspose2D, which nodes have that type? It seems I cannot find any in the graph.", "@aaroey I think in tensorflow Conv2DTranspose operation (i.e. deconvolution) is implemented as Conv2DBackpropInput type, but in TensorRT documentation they mentioned as ConvTranspose2D. I think is should be a documentation error, since I think tensorflow doesn't have ConvTranspose2D type. But they should refer to the same thing.", "@qinyao-he make sense. I'll leave this to @jjsjann123 to check if/when we can support Conv2DTranspose.", "tensorrt does have \"deconvolution\" layer support: https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/classnvinfer1_1_1_i_deconvolution_layer.html , which presumably should map to the \"Conv2DBackpropInput\" op in tensorflow.", "TRT does support deconvolution (Conv2DTranspose). We should take a look at this and strat adding support @benbarsdell @pooyadavoodi", "Have there been any updates on this? It would be very helpful to have deconv supported in tensorflow-tensorrt conversion.", "We are working on this. Stay tuned.\r\n@trevor-m ^^", "PR from Trevor to support conv2d_transpose is merged. Thanks, Trevor!\r\nhttps://github.com/tensorflow/tensorflow/commit/ca2a1cb4cad2dba0d7c4bde662f36de592f5668f\r\n\r\nClosing this issue."]}, {"number": 20156, "title": "Branch 201392512", "body": "", "comments": ["@mdanatg do you mind taking a look at the tensorflow/contrib/autograph/converters:decorators_test failure?", "@yifeif Looking now. Looks like a missing dependency that somehow I missed.", "Thanks @mdanatg and @yifeif \r\n@mdanatg, this is blocking our push, could you prioritize it and keep me posted? Thanks!"]}, {"number": 20155, "title": "Find NCCL2 debians in configure.py", "body": "The Tensorlfow configure doesn't work with NCCL2 when it is installed via the debians.\r\nThis PR adds code that will handle the following scenarios:\r\n - NCCL2 installed via unpacked .tar file and not listed in ldconfig\r\n - NCCL2 installed via unpacked .tar file and listed properly in ldconfig\r\n - NCCL2 installed via debian file and not listed in ldconfig\r\n - NCCL2 installed via debian file and listed properly in ldconfig\r\n\r\nThe debian places the lib and header in the system paths (/usr/lib/*-linux-gnu and /usr/include) and the license files are placed in /usr/share/ and gzipped, so third_party/nccl/nccl_configure.bzl had to change in order to be flexible enough to find those, but changes here were deliberately kept to a minimum.\r\n\r\nThis avoids having to add a new config option for the NCCL2 header location by assuming /usr as the install location when NCCL is installed via debian.\r\n", "comments": ["@jayfurmanek Please resolve the merge conflict", "Sorry -I was out. Looking at this now.", "So the merge conflict was because some basic checking for the existence of the NCCL license was added recently. I added a check for the license for each of the scenarios listed in the description. I also reformatted a bit for consistency.", "So, the license checking that caused the initial code conflict is now removed again, causing another conflict. I'm inclined to leave it in for this PR as this is attempting to be comprehensive. I'd like some feedback, however. With the move of using NCCL2 as the default, I think we want more comprehensive support in configure.py.\r\nAny thoughts/preferences before I resolve the conflict?", "I resolved the conflict:\r\n- I already had the expanduser calls in there so ~home dirs should work\r\n- I left in the license checking\r\n\r\nI think this PR improves NCCL2 configuration in configure.py, which is currently not that great.", "I can't see the internal build failures here. Let me know if there is anything I can help with here to get this one going.", "I think the problem is due to nccl path not having a meaningful default?", "Well, when looking for `libnccl.so.2` there are two scenarios:\r\n - ldconfig is properly set up and knows the location\r\nIf ldconifg knows the location, this PR just retrieves it and continues. There is no need to prompt the user and therefore no need for a default.\r\n\r\n - ldconfig is not aware of nccl\r\nIf ldconfig does not know the location, then the user is prompted and the default location is `CUDA_TOOLKIT_PATH` which should give us a good chance of finding libnccl assuming it was put in the proper location.\r\n\r\n@gunan can you share any error messages from the internal builds?", "The error messages I see are as early as configure script, unless we explicitly specify nccl locations, the configure script crashes and build fails. Currently, not sure how we resolve nccl location, but maybe @aaroey can weigh in.", "@tfboyd @smit-hinsu ", "I am not totally positive the license is the issue.  I suspect the issue is the following and I hope to make sometime to dig in or get Smit to do it.  :-)\r\n\r\n-  Our build machine for normal builds has the .tgz install method and copied into /user/local/cuda or something like that or not and we depend on the ./configure hard coded dir structure.\r\n-  Our Docker builds install with apt-get", "The \"proper\" place for NCCL is to unpack it into the cuda directory like:\r\n```/usr/local/cuda-9.2/targets/ppc64le-linux/lib/libnccl.so.2.2.13\r\n/usr/local/cuda-9.2/targets/ppc64le-linux/lib/libnccl.so.2\r\n/usr/local/cuda-9.2/targets/ppc64le-linux/lib/libnccl.so\r\n/usr/local/cuda-9.2/targets/ppc64le-linux/lib/libnccl_static.a\r\n/usr/local/cuda-9.2/targets/ppc64le-linux/include/nccl.h\r\n```\r\nor the corresponding `targets` locations on other platforms. This PR should find it there whether or not ldconfig knows where it is.  It also should find it if installed via apt-get, which should be:\r\n\r\n```\r\n/usr/include/nccl.h\r\n/usr/lib/powerpc64le-linux-gnu/libnccl.so\r\n/usr/lib/powerpc64le-linux-gnu/libnccl_static.a\r\n/usr/lib/powerpc64le-linux-gnu/libnccl.so.2\r\n/usr/lib/powerpc64le-linux-gnu/libnccl.so.2.2.12\r\n```\r\n\r\nOn the other hand, this PR would fail if the NCCL-SLA file was not there (or removed from where it was unpacked/installed to). I just pushed a commit to remove the license checking, so we'll see.\r\n", "It may have been fixed but the header files was ending up in something like /usr/local or lib or something/header/nccl.h.  I think on a call with NVIDIA they mentioned working to fix that as it was not helpful.  I am excited about this PR it may take some days to run through the scenarios and update the back-end build system.  I also like the move that the build system assumes the files are stored where installed by apt-get when possible.  ", "It looks like it failed, so the license wasn't the issue. I can't see the results for the internal CI build fails.\r\n@tfboyd are you able to post the error message from one of the builds? thx!", "Here is the error: \r\n```\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nAssuming header is at /usr/local/cuda-9.0/include/nccl.h\r\nInvalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib64/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h is not found. Please use the O/S agnostic package of NCCL 2\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: \r\n\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nAssuming header is at /usr/local/cuda-9.0/include/nccl.h\r\nInvalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib64/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h is not found. Please use the O/S agnostic package of NCCL 2\r\nTraceback (most recent call last):\r\n  File \"configure.py\", line 1654, in <module>\r\n    main()\r\n  File \"configure.py\", line 1593, in main\r\n    set_tf_nccl_install_path(environ_cp)\r\n  File \"configure.py\", line 1221, in set_tf_nccl_install_path\r\n    _DEFAULT_PROMPT_ASK_ATTEMPTS)\r\n__main__.UserInputError: Invalid TF_NCCL setting was provided 10 times in a row. Assuming to be a scripting mistake.\r\n```\r\n\r\nThe build script assumes that nccl can be found without extra additions (I think) @aaroey I think you made the build script changes for nccl.\r\nWhat is the mechanism to find where nccl is?", "You will want to see my comment.  I updated all of the builds for NCCL2, it is a mess because we mixed tgz and apt-get and NVIDIA packaged NCCL oddly (put the header in /usr/local/header or something crazy like that) and maybe since has fixed it.  We can 100% make this PR work and it will be better.  It will just take some testing.  I am in the middle of a bunch of benchmark tests or I would have jumped in already.  \r\n\r\nIf you want to work through it yourself:\r\n   * try to do a build where you unzip the package to a directory and have the ./configure still work if you point to the root directory where you unzipped that has /lib /header.  \r\n   * Then try it with the apt-get install where it just finds it in the path.  \r\n\r\nAnd the person that did the build script was Christian Sigg, but we do not need him to fix this, he left the comment about wanting to do ldconfig, which I found amusing as not having it was killing me while trying to roll this out.  ", "@jayfurmanek   I see the logic this way, as a strawperson:\r\n\r\n- Can I find it in the path, check ld or whatever\r\n- If not user provides ./configure with the path which would either have to be individually to the .so and .h or to a parent directory that assumes a structure underneath like the .tgz.\r\n\r\nI am very interested in other ideas.  For CUDA I can just point to another folder and off I go. While I would not want to build with multiple NCCL versions on a single system I would want to be able to do that.  That is where option #2 comes in and providing the path. Do you ask for the path to both .so and .h or a parent with a known structure.  Thoughts?\r\n\r\n**I also apologize that this is so confusing.**", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@smit-hinsu   I am sorry I dropped this item.  Can you pick it up?  The build process should work with NCCL 2.x installed from debian.  Make sure to try the latest package as I think NVIDIA has been making changes.  Since NCCL is only on linux we could also just look by default in the debian locations and then ask for the original folder where we expect library/header to exist as the fall back.   Jay may already being doing something similar or cooler I did not look closely.  Thankyou very much. ", "Also sorry to @jayfurmanek .  This will get sorted out.", "Chatted with @gunan a bit this week. I may have an idea whats wrong here (beyond the new conflicts).", "I like the new build template files in `third_party/nccl`. They should help simplify this PR a bit.", "@jayfurmanek Let us know when the tests are fixed and this PR is ready for review.", "Will do. I'll be working on this today. I've had to change a bit to resolve the conflict. Mostly all for the better. I hope to push a commit today.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Dang, I updated my branch to resolve the merge conflicts and it now I have a ton of commits on PR. I'll close and reopen a new one and reference this.", "replaced with https://github.com/tensorflow/tensorflow/pull/22519"]}, {"number": 20154, "title": "TF.RECORDS IMAGES TENSORFLOW", "body": "I'm doing a tensorflow cnn, at the time of reading my tf.records I do not know what type of data is appropriate when I decode my images, and I saw in multiple tutorials that a reshape should be done before entering the model , I'd like to know if I'm doing it the right way, this is my code\r\n\r\n    `def read_file(filename_queue):\r\n \r\n        #Funcion para leer el archivo tf.record, y retornamos el next recrod\r\n         reader=tf.TFRecordReader()\r\n         _,serialized_example=reader.read(filename_queue)\r\n\r\n        #Se decodifica el tf.record retornando un diccionario \r\n        feature={'train/image':tf.FixedLenFeature([],tf.string),\r\n                        'train/label':tf.FixedLenFeature([],tf.int64)}\r\n        features=tf.parse_single_example(serialized_example,features=feature)\r\n\r\n        #Convertimos el string a numeros de los decodificados features\r\n         image=tf.decode_raw(features['train/image'],tf.float32)* (1 / 255.0)\r\n\r\n        #Convertimos a datos\r\n        label=tf.cast(features['train/label'],dtype=tf.int32)\r\n\r\n         #Reshape data\r\n          image=tf.reshape(image,[224,224,3])   \r\n\r\n          return image,label`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20153, "title": "Fix OOB check for result_index in header generation", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Can you please sign the CLA? Thanks.", "Ah, sorry about the delay. I'm waiting for the contact in my org to add me\nto the approved list. Will update when I get on the list.\n\nOn Wed, Jun 20, 2018 at 2:38 PM, Qianli Scott Zhu <notifications@github.com>\nwrote:\n\n> Can you please sign the CLA? Thanks.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/20153#issuecomment-398905153>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AH0NX0WDwVvinfpZ43qwxFpC9tW4HhQ5ks5t-sDigaJpZM4UvvRD>\n> .\n>\n\n\n\n-- \n*Learn about Life Inside Dropbox <http://bit.ly/lifeinside>*\n", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks for the fix. Can you update the test case as well?", "@qlzh727 Do you really want a new test just for this change? This patch fixes a slightly lax OOB check against `CpuAotCompilationResult` which is generated data concerning temp buffer sizes. Looking at `//tensorflow/compiler/aot/codegen_test.cc`, there seems to be a manually crafted `CpuAotCompilationResult` for the golden example (that's expected to be accepted), but no existing test for \"badly\" crafted ones as far as I can see. Let me know how you want to proceed. Thanks!", "This seems to be a straight forward fix. I am just surprise we didn't catch it in the unit test in the first place."]}, {"number": 20152, "title": "Remove duplicate imports in dynamic_stitch_op_test.py", "body": "There is a duplicate `from tensorflow.python.framework import dtypes`\r\nin dynamic_stitch_op_test.py (See Line 24 above).\r\n", "comments": []}, {"number": 20151, "title": "fixed typo in docstring", "body": "`init_from_checkpoint` does not accept a set as `assignment_map`", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 20150, "title": "unable to cast to quantized data types uint8 -> quint8", "body": "The `Cast` operator doesn't support casting to quantized data types such as `quint8`. This makes it a pain to implement custom quantization logic in Tensorflow. Since both quint8 and uint8 have the same underlying data type it should be fairly easy to implement conversion between them.\r\n\r\n```\r\ntensor_test.go:33: Cast uint8 to quint8 is not supported\r\n  [[Node: QuantizeProbabilistic/Quint8/Cast = Cast[DstT=DT_QUINT8, SrcT=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](QuantizeProbabilistic/Cast)]]\r\n```\r\n\r\nYou can work around it by casting to uint8, export from tensorflow in to the host language and then create a new tensor with the specified data type, but that's certainly less than ideal.\r\n\r\nOn a side note: I ran into this problem while trying to implement a randomized quantize that when averaged returns the original number. Maybe that's something that should be in the core tensorflow library?\r\n\r\nTemplate:\r\nHave I written custom code? No\r\nOS Platform and Distribution? Linux, Arch\r\nTensorFlow installed from? Master\r\nTensorFlow version? Master\r\nBazel version? 0.14.1- (@non-git)\r\nCUDA/cuDNN version? N/A\r\nGPU model and memory? N/A\r\nExact command to reproduce?\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yes.", "Nagging Assignee @rohan100jain: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20149, "title": "Tensorflow on GCP instance", "body": "I'm trying to setup my GCP instance for working with TF and Keras. I installed Cuda-toolkit, libcupti-dev and cudnn.\r\nI also installed the *tensorflow-gpu* but when I import Tensorflow, I get the following.\r\n\r\nOperating System : Ubuntu 16.04 LTS\r\nTensorflow Installation Method : ```pip install tensorflow-gpu```\r\nTensorflow Version: 1.8\r\nPip Version: 10.0\r\nCuda version 9.2\r\nCudnn  version: 7\r\nGPU: Nvidia Tesla K80 - 12 GB\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n```\r\n\r\n\r\n cudnn and CUDA are setup as precribed in the documentation. Could you help me with this?\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\n```\r\ngives me the above error on server", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Can you try installing Cuda version 9.0?", "```\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n```\r\nThis means TF is looking for cuda 9.0 and failing to find.\r\nprebuilt TF packages are built on top of cuda 9.0. To use cuda 9.2, you need to rebuild TF from sources.", "Thanks!\r\n\r\nOn Fri, Jun 29, 2018 at 11:30 AM Gunhan Gulsoy <notifications@github.com<mailto:notifications@github.com>> wrote:\r\n\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nThis means TF is looking for cuda 9.0 and failing to find.\r\nprebuilt TF packages are built on top of cuda 9.0. To use cuda 9.2, you need to rebuild TF from sources.\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/20149#issuecomment-401255648>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ARPwgXGeoZkDXvUG0CP-yACG2MWQZcHFks5uBcKTgaJpZM4Uvpjr>.\r\n"]}, {"number": 20148, "title": "Retraining explanation with tf-hub", "body": "Trying to go through this tutorial below to retrain ssd_inception_v2_coco_11_06_2017 to fine tune and retrain on coco + additional set of images:\r\nhttps://www.tensorflow.org/tutorials/image_retraining#creating_a_set_of_training_images\r\n\r\nThis tutorial seems to be different from others I've seen where labelling images and creation of tfrecord are necessary steps. However, this tutorial seems to accept a directory of images and retrains the model with the image dir. \r\nQuestion: Is the tf-hub automating the tfrecord creation process using the directory name. If so, is it using the full image size and not tagging the bounding boxes of contained boxes?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "The example is for classification not detection", "Is this still an open issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20147, "title": "Fix MPI build issue with bazel", "body": "\r\nWhile trying to build tensorflow with MPI through:\r\n```\r\nDo you wish to build TensorFlow with MPI support [y/N] y\r\n...\r\n$ bazel build -s --verbose_failures --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n, the following failure happens:\r\n```\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:76:18: error: 'se' does not name a type\r\n using StatusOr = se::port::StatusOr<T>;\r\n                  ^\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:139:28: error: 'StatusOr' was not declared in this scope\r\n typedef std::function<void(StatusOr<Tensor>)> CommunicationDoneCallback;\r\n...\r\n```\r\n\r\nThis fix addresses the build failures for MPI build.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Assigning to @jlebar since the code was recently migrated to use new namespace.", "I won't have my laptop until Monday, so apologies for the somewhat vague comment, but rather than depend on XLA to get StatusOr, I'd recommend depending on the relevant header and BUILD dep within StreamExecutor. That's less of a layering violation.", "@jlebar Thanks for the review. I updated the PR and used `stream_executor_headers_lib` instead. Please take a look and let me know if there are any issues.", "@qlzh727, can you help get this landed?"]}, {"number": 20146, "title": "tf.extract_image_patches() gradient very slow at graph construction time", "body": "### System information\r\n- **Have I written custom code**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Fedora 28\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-1-g8753e2e\r\n- **Python version**: 3.5\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: see below\r\n\r\n### Problem\r\nWhen building the graph, taking the gradient with respect to the input of `tf.extract_image_patches` is unusually slow (see minimal example below, where it takes ~2min). \r\n\r\n### How to reproduce\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.reset_default_graph()\r\n\r\nbatch_size = 4\r\nheight = width = 1024\r\nksize = 5\r\n\r\ninputs = tf.get_variable('inputs', (batch_size, height, width, 1))\r\npatches = tf.extract_image_patches(inputs, \r\n                                   ksizes=[1,ksize,ksize,1], \r\n                                   strides=[1,1,1,1], \r\n                                   rates=[1,1,1,1], \r\n                                   padding='SAME')\r\nprint(patches.shape)\r\n%time gradients = tf.gradients(patches, inputs)\r\n```\r\nResult:\r\n```\r\n(4, 1024, 1024, 25)\r\nCPU times: user 1min 42s, sys: 2.07 s, total: 1min 44s\r\nWall time: 1min 44s\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Yes.\n\nOp do 26 jul. 2018 om 21:19 schreef Alfred Sorten Wolf <\nnotifications@github.com>:\n\n> It has been 14 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20146#issuecomment-408205210>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADNkJlMwozqHyxy_uTMmDsHDlwltP6ECks5uKhZBgaJpZM4UvMHW>\n> .\n>\n", "@nikhilmishra000 this looks like it's due to a very inefficient for loop inside the CreateImagePatchesGrad.  Can you suggest way to speed this up?  For example, can that for loop be converted to a tf.while_loop?", "Perhaps the idx array can be pre-allocated instead of being extended at every iteration of the double-for loop.  Or perhaps there's another way to calculate the indices that doesn't require a double for loop?", "`tf.while_loop` doesn't make sense here but you could speed this up through better use of numpy functions.\r\n\r\nI would suggest using `np.meshgrid` to create the initial set of indices (this would give you an array of shape `(cols_out, rows_out, 2)` corresponding to the values taken by `i` and `j`). Then the body of the loop could be expressed via elementwise operations on that array.", "Would you be up for making the change?  I see you as the author on this code.", "Alas, I don't think I have the bandwidth right now...", "@ebrevdo @nikhilmishra000 I create a PR #21218 to reduce the constructing time from 138s to 3.6s. Could you take a look? Thanks.", "Nagging Assignee @ebrevdo: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20145, "title": "Tf lite: EXC_BAD_ACCESS when invoking the interpreter in iOS", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS X High Sierra\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.5\r\n\r\n### Describe the problem\r\nI created a feedforward neural network in Python and then exported the tflite model using:\r\n\r\n```\r\nbazel run --config=opt \\\r\n  //tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=frozen_graphdef.pb \\\r\n  --output_file=model.tflite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --inference_type=FLOAT \\\r\n  --input_shape=1,200 \\\r\n  --input_array=inputs \\\r\n  --output_array=prediction  \r\n```\r\n\r\nI'm now trying to use it in iOS following the guidelines of your examples. This is my code:\r\n\r\n```\r\n\r\n  float* _fInput;\r\n  std::unique_ptr<tflite::Interpreter> _interpreter;\r\n\r\n  std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(sModel_path.c_str());\r\n  \r\n  if (!model) {\r\n    std::cout << \"Failed to map model \" << sModel_path << std::endl;\r\n    exit(-1);\r\n  }\r\n  std::cout << \"Loaded model \" << sModel_path << std::endl;\r\n  \r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  \r\n  tflite::InterpreterBuilder(*model, resolver)(&_interpreter);\r\n  \r\n  if (!_interpreter) {\r\n    std::cout << \"Failed to construct interpreter.\" << std::endl;\r\n    exit(-1);\r\n  }\r\n  \r\n  int input_idx = _interpreter->inputs()[0];\r\n  \r\n  std::vector<int> sizes = {1, NUM_FEAT};\r\n  std::string input_layer_type = \"float\";\r\n  \r\n  if (input_layer_type != \"string\") {\r\n    _interpreter->ResizeInputTensor(input_idx, sizes);\r\n  }\r\n\r\n  if (_interpreter->AllocateTensors() != kTfLiteOk) {\r\n    printf(\"Failed to allocate tensors!\\n\");\r\n  }\r\n  \r\n  _fInput = _interpreter->typed_tensor<float>(input_idx);\r\n\r\n  // receive vFeatures, an std::vector<float> of size NUM_FEAT\r\n\r\n  for(int i = 0; i < NUM_FEAT; ++i)\r\n    _fInput[i] = vFeatures[i];\r\n  \r\n  if (_interpreter->Invoke() != kTfLiteOk) {\r\n    std::cout << \"Failed to invoke!\" << std::endl;\r\n    exit(-1);\r\n  }\r\n\r\n```\r\n\r\nFor some reason, the execution fails on the line _interpreter->Invoke(), I get an \"EXC_BAD_ACCESS\" error. \r\n\r\nOn a sidenote, do I need the ResizeInputTensor and AllocateTensors lines? It seems to me that the resizing is not necessary since I already stated the input size when exporting the tflite model, and I've seen some examples in which these lines were not present.\r\n", "comments": []}, {"number": 20144, "title": "lack using namespace std", "body": "/opt/tensorflow/tensorflow/core/graph/default_device.h:29:36: error: 'string' does not name a type; did you mean 'stdin'?\r\n inline void SetDefaultDevice(const string& device, GraphDef* graph_def) {\r\n                                    ^~~~~~\r\n                                    stdin\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20143, "title": "Estimator.export_savedmodel problem", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI have attached a ZIP file\r\n[CaseStudy.zip](https://github.com/tensorflow/tensorflow/files/2118856/CaseStudy.zip)\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\n1.8\r\n- **TensorFlow version (use command below)**:\r\nb'v1.8.0-0-g93bc2e2072' 1.8.0\r\n- **Python version**: \r\n3.6(64bit)\r\n- **CUDA/cuDNN version**:\r\n9.0\r\n- **GPU model and memory**:\r\nGtx660 2GB\r\n- **Exact command to reproduce**:\r\nCaseStudy.py\r\n\r\n\r\n i Try to use Estimator.export_savedmodel() to export the model, but i receive the error: AttributeError(\"'dict' object has no attribute 'dtype'\",)\r\nThis is happening because export_savedmodel() calls model_fn() with the dict of the features and not the features themselves. If this is not a bug then i am missing something very fundamental. What is this?\r\n\r\nI have attached a VS solution for you to run\r\n\r\n[CaseStudy.zip](https://github.com/tensorflow/tensorflow/files/2118856/CaseStudy.zip)\r\n\r\n\r\nThank you\r\n\r\n", "comments": ["sorry, found my error. Closing..."]}, {"number": 20142, "title": " use the stream builder to invoke FFT library fail in custom op ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nCentos /Linux\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.8.0\r\n- **Python version**: \r\n3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nCUDA Version 9.0.176  cuDNN 7.0.5\r\n- **GPU model and memory**:\r\nGeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\n- **Exact command to reproduce**:\r\nI'm writing a custom op performing image alignment. The module is performing several FFT operations on GPU .  I created a simple custom op to reproduce the issue with the stream builder method.\r\nI didn't find any real example, I followed the comments given in the code  stream_executor/fft.h  \r\n\r\nThe portion of the code that is causing the error  is bellow  ( Create1dPlan ...)  with the corresponding trace log.\r\nI also attached the full code  to produce the custom op at the bottom.\r\nThanks for any help !\r\n``` javascript\r\nvoid launchFFT(\r\n  OpKernelContext* context,\r\n  const Tensor& x,\r\n   Tensor* y)\r\n{\r\nauto dev_ctx = context->op_device_context();\r\nOP_REQUIRES(context, dev_ctx->stream(), errors::Internal(\"No stream available.\"));\r\nauto stream_exec=dev_ctx->stream()->parent();\r\nperftools::gputools::DeviceMemory<std::complex<float>>X=stream_exec>AllocateArray<std::complex<float>>(1024);\r\nperftools::gputools::DeviceMemory<std::complex<float>>Y=stream_exec>AllocateArray<std::complex<float>>(1024);\r\n /* ... populate x and y ... TBD*/\r\nStream stream{stream_exec};\r\nprintf(\"Create1dPlan and ... die\\n\");\r\nstd::unique_ptr<Plan> plan =stream_exec->AsFft()->Create1dPlan(&stream, 1024,Type::kC2CForward,false);\r\nstream.Init().ThenFft(plan.get(), X, &Y);\r\nSE_CHECK_OK(stream.BlockHostUntilDone());\r\n}\r\n```\r\n\r\n\r\n018-06-20 02:12:43.373297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-06-20 02:12:43.524101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-20 02:12:43.524119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-06-20 02:12:43.524123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-06-20 02:12:43.524320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11431 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:06:00.0, compute capability: 5.2)\r\nlaunchFFT\r\nCreate1dPlan and ... die\r\n**2018-06-20 02:12:43.603606: F tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:112] Check failed: cuda_exec != nullptr** \r\n\r\nProcess finished with exit code 134 (interrupted by signal 6: SIGABRT) \r\n\r\nThe sources of the custom op to reproduce the problem are bellow:\r\n**dummyFFT.cc**\r\n```javascript\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/register_types.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n\r\nnamespace tensorflow\r\n{\r\ntypedef Eigen::GpuDevice GPUDevice;\r\n\r\nusing shape_inference::ShapeHandle;\r\nusing shape_inference::InferenceContext;\r\nusing shape_inference::DimensionHandle;\r\n\r\nREGISTER_OP(\"DummyFft\")\r\n.Input(\"float: float32\")\r\n.Output(\"output: float32\")\r\n.SetShapeFn([](InferenceContext* ctx) {\r\n    // Get shapes and ensure correct dimensionality\r\n    ShapeHandle in_shape;\r\n    TF_RETURN_IF_ERROR(ctx->WithRank(ctx->input(0), 2, &in_shape));\r\n    // Construct and set the output shape\r\n   ctx->set_output(0, in_shape);\r\n    return Status::OK();\r\n});\r\n\r\nvoid launchFFT(\r\n  OpKernelContext* ctx,\r\n  const Tensor& x,\r\n  Tensor* y);\r\n\r\nclass launchFFTOp : public OpKernel\r\n{\r\n public:\r\n  explicit launchFFTOp(OpKernelConstruction* ctx) : OpKernel(ctx) { }\r\n\r\n  void Compute(OpKernelContext* ctx) override\r\n  {\r\n    // Get inputs\r\n    const Tensor& input = ctx->input(0);\r\n    // Setup output shape\r\n    const TensorShape& input_shape(input.shape());\r\n    TensorShape output_shape(input.shape());\r\n    // Allocate output tensor\r\n    Tensor* output = nullptr;\r\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &output));\r\n    printf(\"launchFFT\\n\");\r\n    launchFFT(ctx, input,output);\r\n\r\n  }\r\n};\r\n```\r\n**dummyFFT.cu.cc**\r\n```javascript\r\n#if GOOGLE_CUDA\r\n#define EIGEN_USE_GPU\r\n\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n#include \"tensorflow/core/framework/register_types.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/util/cuda_kernel_helper.h\"\r\n#include \"tensorflow/core/platform/stream_executor.h\"\r\n\r\nusing namespace perftools::gputools::fft;\r\n\r\nusing namespace perftools::gputools;\r\n\r\n\r\n\r\ntypedef Eigen::GpuDevice GPUDevice;\r\nnamespace tensorflow\r\n{\r\nnamespace {\r\n\r\ntemplate <typename T>\r\n\tperftools::gputools::DeviceMemory<T> AsDeviceMemory(const T* cuda_memory) {\r\n\tperftools::gputools::DeviceMemoryBase wrapped(const_cast<T*>(cuda_memory));\r\n\tperftools::gputools::DeviceMemory<T> typed(wrapped);\r\n  return typed;\r\n}\r\n\r\n}  // end namespace\r\n\r\n\r\nvoid launchFFT(\r\n  OpKernelContext* context,\r\n  const Tensor& x,\r\n   Tensor* y)\r\n{\r\nauto dev_ctx = context->op_device_context();\r\nOP_REQUIRES(context, dev_ctx->stream(), errors::Internal(\"No stream available.\"));\r\nauto stream_exec=dev_ctx->stream()->parent();\r\nperftools::gputools::DeviceMemory<std::complex<float>>X=stream_exec>AllocateArray<std::complex<float>>(1024);\r\nperftools::gputools::DeviceMemory<std::complex<float>>Y=stream_exec>AllocateArray<std::complex<float>>(1024);\r\n /* ... populate x and y ... TBD*/\r\nStream stream{stream_exec};\r\nprintf(\"Create1dPlan and ... die\\n\");\r\nstd::unique_ptr<Plan> plan =stream_exec->AsFft()->Create1dPlan(&stream, 1024,Type::kC2CForward,false);\r\nstream.Init().ThenFft(plan.get(), X, &Y);\r\nSE_CHECK_OK(stream.BlockHostUntilDone());\r\n}\r\n\r\n\r\n}  // namespace TensorFlow\r\n#endif\r\n```\r\n\r\n**makefile**\r\n```\r\nTF_CFLAGS := $(shell python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))')\r\nTF_LIB := $(shell python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')\r\n\r\nCUDA_LIB=/usr/local/cuda/lib64\r\n\r\nall: dummyFFT.cu.o dummyFFT.cu.cc dummyFFT.cc \r\n\tnvcc -std=c++11 -c -o dummyFFT.cu.o dummyFFT.cu.cc  $(TF_CFLAGS) -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC --expt-relaxed-constexpr -ltensorfow_framework -I /usr/local -I /usr/local/cuda/include -O3\r\n\tg++ -std=c++11 -shared -o dummyFFT_op.so dummyFFT.cc dummyFFT.cu.o   $(TF_CFLAGS) -fPIC -O3 -L$(CUDA_LIB) -lcudart -L$(TF_LIB) -ltensorflow_framework\r\n```\r\nThe Python functions:\r\n\r\n**test.py** The main python  file to run the custom op\r\n```javascript\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom dummyFFT import dummy_fft\r\n\r\n\r\ndef test():\r\n\r\n    x = np.random.rand(10,10).astype(np.float32)\r\n    x_ph = tf.placeholder(tf.float32, (10,10))\r\n    out=dummy_fft(x_ph)\r\n\r\n    with tf.Session() as sess:\r\n        _out = sess.run(out, feed_dict={x_ph: x})\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    test()\r\n```\r\n\r\n**dummyFFT.py**  used to load the library \r\n```javascript\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops\r\n\r\nop_module = tf.load_op_library('dummyFFT_op.so')\r\n\r\n\r\ndef dummy_fft(x):\r\n    return op_module.dummy_fft(x)\r\n```\r\n\r\n\r\n\r\n\r\n", "comments": ["@laurentk67 I apologize for slow reply. Do you still have a problem?", "Hi, \nThe general problem is how cuFFT or cuBlas should be used inside a custom op.\nThe code I reported is crashing.\nThanks\nLaurent\n\nSent from my iPad\n\n> On Jul 13, 2018, at 10:16 AM, Tatiana Shpeisman <notifications@github.com> wrote:\n> \n> @laurentk67 I apologize for slow reply. Do you still have a problem?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "I'm still trying to get some help on this topic:\r\nHow to call one of the cuda library (cuFFT,Cublas...) within a custom op ?\r\nI will appreciate any help or example \r\nThanks", "@laurentk67 We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. refer [link](https://github.com/tensorflow/tensorflow/issues/40119) .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20142\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20142\">No</a>\n"]}, {"number": 20141, "title": "Resnet20: add fake quantization nodes", "body": "System information\r\n------------------------------------------\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):CentOS Linux release 7.2\r\nTensorFlow installed from (source or binary):Anaconda python 3.6.5(conda install)\r\nTensorFlow version (use command below):1.8.0\r\nPython version: 3.6.5\r\nBazel version (if compiling from source): 0.7.0\r\nGCC/Compiler version (if compiling from source): 4.8.5\r\nCUDA/cuDNN version:cuda-9.0\r\nGPU model and memory: P40\r\nPhone: N/A\r\n\r\nHi~ \r\nWhen I use 'tf.contrib.quantize.experimental_create_training_graph'  it will add fake quantization in the graph.  reference this picture(find positions to insert fake quantization nodes). \r\n![image](https://user-images.githubusercontent.com/9084403/41646572-c5a48d8a-74a6-11e8-81bf-d1fee67d5c4a.png)\r\n\r\nI think it will add a fake quantization node in the bypass, but I can not find it in the tensorboard graph(in the red box, it is the bypass). In other convs, I can find the quantization node. \r\n![image](https://user-images.githubusercontent.com/9084403/41649732-d6f538d4-74ae-11e8-87b5-66b2b49904a7.png)\r\n\r\nMy network architecture is ResNet20,  tensorflow version is 1.8.0\r\n\r\npls help me. thank you.\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Is this ResnetV2. If that is the case, there are issues with batchnorm that messes up quantization, that we still need to resolve before we are able to quantize this.", "@suharshs  When will this problem be solved?", "This may be a bit involved to resolve, but it is something we plan to look into this upcoming quarter.", "I'm getting same type of error for xception model.\r\n\r\n_Array Cast, which is an input to the ResizeBilinear operator producing the output array ResizeBilinear, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation._\r\n\r\nThe model is downloaded from tensorflow deeplab repo.\r\n\r\nThe command that generated the error was\r\n\r\n```\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n--input_file=frozen_inference_graph.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--output_file=xception.lite \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--input_arrays=ImageTensor \\\r\n--output_arrays=SemanticPredictions \\\r\n--input_shapes=1,299,299,3\r\n```", "The issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet.\r\n\r\nIt can be complicated and sometimes not possible to fully quantize certain models due to the available fused operations at any given time , if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a floating point version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case. Check it out here: https://www.tensorflow.org/performance/post_training_quantization", "Based on feedback that the contrib/quantize quantization-aware training tool is a bit brittle and hard to use on some model architectures, we have released a [post-training integer quantization tool](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba), that requires a small calibration dataset. Please take a look at the [tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_integer_quant.ipynb) and give it a try, it should work much better! And let us know if you run into any issues.\r\n\r\nClosing this issue, since we are rethinking and working on an api to replace contrib/quantize quantization-aware-training (although post-training quantization above should be sufficient for the majority of use cases).\r\n\r\nThanks!\r\n-Suharsh\r\n"]}]