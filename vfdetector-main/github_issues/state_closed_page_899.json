[{"number": 26507, "title": "updated documentation for tf.math.argmin and tf.math.argmax operations", "body": "PR for issue #26530 and #26532\r\n\r\nAdded usage examples for `tf.math.argmin` and `tf.math.argmax` functions with this patch. Modified the `api_def_ArgMin.pbtxt` and  ` api_def_ArgMax.pbtxt`. Also added usage summary to `/tensorflow/core/ops/math_ops.cc`.", "comments": []}, {"number": 26506, "title": "Simple bug fix when building tf2.0 with verbs", "body": "Include alarm.h to fix the compiling bug \"::grpc::Alarm not found\" in tf 2.0.", "comments": ["TF 2 shouldn't build contrib at all. Can you take this to TensorFlow/networking, assuming it's not fixed there?", "Ok, I'll keep following."]}, {"number": 26505, "title": "tf.function decorator converts ragged tensors to their dense equivalent", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: Windows 10 (will also test on Ubuntu)\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.0 preview\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: GTX 1070 8gb\r\n\r\n**Describe the current behavior**\r\nWhen using `@tf.function` a function with a ragged input it will return the dense equivalent of the ragged tensor.  When the decorator is omitted it returns a ragged tensor.\r\n\r\n**Describe the expected behavior**\r\nReturn a ragged tensor regardless of the use of `@tf.function`\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\n@tf.function\r\ndef do_nothing(x):\r\n    return x\r\n\r\nrt = tf.ragged.constant([[[1, 2, 3], [4]],\r\n                         [[5], [], [6]],\r\n                         [[7]],\r\n                         [[8, 9], [10]]])\r\n\r\ndo_nothing(rt)\r\n```\r\n```\r\n<tf.Tensor: id=51310, shape=(10,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])>\r\n```\r\n\r\n```\r\n# @tf.function\r\ndef do_nothing(x):\r\n    return x\r\n\r\nrt = tf.ragged.constant([[[1, 2, 3], [4]],\r\n                         [[5], [], [6]],\r\n                         [[7]],\r\n                         [[8, 9], [10]]])\r\n\r\ndo_nothing(rt)\r\n```\r\n```\r\ntf.RaggedTensor(values=<tf.RaggedTensor [[1, 2, 3], [4], [5], [], [6], [7], [8, 9], [10]]>, row_splits=tf.Tensor([0 2 5 6 8], shape=(5,), dtype=int64))\r\n```\r\n\r\nThe ragged tensor documentation hints that this might be intended or necessary, is it?\r\nhttps://www.tensorflow.org/guide/ragged_tensors#rank_and_ragged_rank", "comments": ["@edloper I thought we supported raggedtensors in tf.function due to the changes in nest. Can you take a look?", "@alextp I want to work on this issue, can you please guide me on this?", "@shashvatshahi1998 the right way to fix this is to instrument calls to the ragged tensor to tensor function and see when they fire in the tf.function codepath. Whatever's triggering that probably needs to call nest.flatten with expand_composites=True", "Sorry, @alextp i am not getting what you are trying to say.", "In third_party/tensorflow/python/eager/function.py, nest is used in several\nplaces to unpack and repack collections of tensors.  Most likely, the\nappropriate fix is to add \"expand_composites=True\" to some (or all?) of\nthese nest.xyz(...) calls.  This option causes composite tensors to be\nexpanded into their components.  E.g.:\n\n>>> x = {'a': t1, 'b': RaggedTensor(t2, t3), 'c': tf.SparseTensor(t4, t5,\nt6)}\n>>> nest.flatten(x)\n(t1, RaggedTensor(t2, t3), SparseTensor(t4, t5, t6)\n>>> nest.flatten(x, expand_composites=True)\n(t1, t2, t3, t4, t5, t6)\n\nSimilarly, when expand_composites=True is used with nest.pack_sequence_as,\nit will reconstruct the composite tensors from their components.\n\nOn Wed, Apr 3, 2019 at 3:02 PM Shashvat Chand Shahi <\nnotifications@github.com> wrote:\n\n> Sorry, @alextp <https://github.com/alextp> i am not getting what you are\n> trying to say.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26505#issuecomment-479616856>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFhajDPzTbrtBjXkDa9tdNp8OTti1pYkks5vdPqugaJpZM4bmf0e>\n> .\n>\n", "I took an initial stab at this, and got something basic working.  It ends up being a bit more involved than I thought at first (though the basic approach I suggested is the right one).  I'll work on it a bit more and get back to you with an update.", "@edloper can I also contribute to it.", "I added expand_composites to all nest.flatten() and nest.pack_sequence_as() what about other functions of nest in the file python/eager/function.py, @edloper ", "Fixed by commit 40006c3."]}, {"number": 26504, "title": "Disconnected graph when concatenating two models", "body": "\r\n**System information**\r\n Linux Ubuntu 16.04,\r\nTensorFlow from docker image \r\ntf versionv1.13.1, python v3.5.2, cuda V10.0.130\r\ngpu 1060ti  6GB\r\n\r\nTrying to build a new model based on part of pre-trained model,\r\n\r\nHere's some cleaned out code.\r\n\r\nLet's imagine we got model1 trained, and want to add some layers defined in model2: \r\n\r\n    from tensorflow.keras.layers import  Conv2D, Activation\r\n    from tensorflow.keras.models import Model, Sequential\r\n    \r\n    model1 = Sequential([\r\n        Conv2D(2, (3,3), padding='same', input_shape=(6,6,1)),\r\n        Activation('relu')\r\n    ])\r\n    model2 = Sequential([\r\n        Conv2D(3, (3,3), padding='same', input_shape=(6,6,2)),\r\n        Activation('softmax')\r\n    ])\r\n    \r\n    model_merge = Model(inputs=model1.input, \r\n                        outputs=Activation('softmax')(model2(model1.get_layer('conv2d').output)))\r\n \r\nIt looks a bit messy, but I want to demonstrate it's not disconnected by adding a softmax activation here.\r\n\r\nsummary of model1:\r\n\r\n    _________________________________________________________________\r\n    Layer (type)                 Output Shape              Param #   \r\n    =================================================================\r\n    conv2d (Conv2D)              (None, 6, 6, 2)           20        \r\n    _________________________________________________________________\r\n    activation (Activation)      (None, 6, 6, 2)           0         \r\n    =================================================================\r\n    Total params: 20\r\n    Trainable params: 20\r\n    Non-trainable params: 0\r\n    _________________________________________________________________\r\n\r\n   \r\nsummary of model2:\r\n\r\n    _________________________________________________________________\r\n    Layer (type)                 Output Shape              Param #   \r\n    =================================================================\r\n    conv2d_4 (Conv2D)            (None, 6, 6, 3)           57        \r\n    _________________________________________________________________\r\n    activation_4 (Activation)    (None, 6, 6, 3)           0         \r\n    =================================================================\r\n    Total params: 57\r\n    Trainable params: 57\r\n    Non-trainable params: 0\r\n    _________________________ \r\nAnd summary of model_merge:\r\n\r\n    _________________________________________________________________\r\n    Layer (type)                 Output Shape              Param #   \r\n    =================================================================\r\n    conv2d_input (InputLayer)    (None, 6, 6, 1)           0         \r\n    _________________________________________________________________\r\n    conv2d (Conv2D)              (None, 6, 6, 2)           20        \r\n    _________________________________________________________________\r\n    sequential_2 (Sequential)    (None, 6, 6, 3)           57        \r\n    _________________________________________________________________\r\n    activation_4 (Activation)    (None, 6, 6, 3)           0         \r\n    =================================================================\r\n    Total params: 77\r\n    Trainable params: 77\r\n    Non-trainable params: 0\r\n    _________________________________________________________________\r\n\r\nLet's prove this merged model is not disconnected:\r\n\r\n    layers = [layer.output for layer in model_merge.layers]\r\n    test1 = Model(inputs=model_merge.input, outputs=layers[-1])\r\nEverything works just fine.\r\n\r\ntest1's summary:\r\n\r\n    _________________________________________________________________\r\n    Layer (type)                 Output Shape              Param #   \r\n    =================================================================\r\n    conv2d_input (InputLayer)    (None, 6, 6, 1)           0         \r\n    _________________________________________________________________\r\n    conv2d (Conv2D)              (None, 6, 6, 2)           20        \r\n    _________________________________________________________________\r\n    sequential_2 (Sequential)    (None, 6, 6, 3)           57        \r\n    _________________________________________________________________\r\n    activation_4 (Activation)    (None, 6, 6, 3)           0         \r\n    =================================================================\r\n    Total params: 77\r\n    Trainable params: 77\r\n    Non-trainable params: 0\r\n    _________________________________________________________________\r\n\r\n\r\nHere's the tragedy:\r\n\r\n    test2 = Model(inputs=model_merge.input, outputs=layers[-2])\r\nThe most important feed back:\r\n\r\n    ValueError: Graph disconnected: cannot obtain value for tensor Tensor(\"conv2d_2_input:0\", shape=(?, 6, 6, 2), dtype=float32) at layer \"conv2d_2_input\". The following previous layers were accessed without issue: []\r\n\r\nfull feedback:\r\n\r\n    ValueErrorTraceback (most recent call last)\r\n    <ipython-input-18-946b325081c1> in <module>\r\n    ----> 1 test = Model(inputs=model_merge.input, outputs=layers[-2])\r\n    \r\n    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py in __init__(self, *args, **kwargs)\r\n        119 \r\n        120   def __init__(self, *args, **kwargs):\r\n    --> 121     super(Model, self).__init__(*args, **kwargs)\r\n        122     # Create a cache for iterator get_next op.\r\n        123     self._iterator_get_next = weakref.WeakKeyDictionary()\r\n    \r\n    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py in __init__(self, *args, **kwargs)\r\n         79         'inputs' in kwargs and 'outputs' in kwargs):\r\n         80       # Graph network\r\n    ---> 81       self._init_graph_network(*args, **kwargs)\r\n         82     else:\r\n         83       # Subclassed network\r\n    \r\n    /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpointable/base.py in _method_wrapper(self, *args, **kwargs)\r\n        440     self._setattr_tracking = False  # pylint: disable=protected-access\r\n        441     try:\r\n    --> 442       method(self, *args, **kwargs)\r\n        443     finally:\r\n        444       self._setattr_tracking = previous_value  # pylint: disable=protected-access\r\n    \r\n    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py in _init_graph_network(self, inputs, outputs, name)\r\n        219     # Keep track of the network's nodes and layers.\r\n        220     nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\r\n    --> 221         self.inputs, self.outputs)\r\n        222     self._network_nodes = nodes\r\n        223     self._nodes_by_depth = nodes_by_depth\r\n    \r\n    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py in _map_graph_network(inputs, outputs)\r\n       1850                              'The following previous layers '\r\n       1851                              'were accessed without issue: ' +\r\n    -> 1852                              str(layers_with_complete_input))\r\n       1853         for x in node.output_tensors:\r\n       1854           computable_tensors.append(x)\r\n    \r\n    ValueError: Graph disconnected: cannot obtain value for tensor Tensor(\"conv2d_2_input:0\", shape=(?, 6, 6, 2), dtype=float32) at layer \"conv2d_2_input\". The following previous layers were accessed without issue: []\r\n\r\nIt's really driving me crazy,\r\n\r\nAny ideas?\r\n\r\ncode to reproduce error:\r\n\r\n    from tensorflow.keras.layers import Conv2D, Activation\r\n    from tensorflow.keras.models import Model, Sequential\r\n    model1 = Sequential([\r\n        Conv2D(2, (3,3), padding='same', input_shape=(6,6,1)),\r\n        Activation('relu')\r\n    ])\r\n    model2 = Sequential([\r\n        Conv2D(3, (3,3), padding='same', input_shape=(6,6,2)),\r\n        Activation('softmax')\r\n    ])\r\n    \r\n    model_merge = Model(inputs=model1.input, \r\n                        outputs=Activation('softmax')(model2(model1.get_layer('conv2d').output)))\r\n    layers = [layer.output for layer in model_merge.layers]\r\n    test = Model(inputs=model_merge.input, outputs=layers[-2])", "comments": ["It seems after using Model() merge 2 models, the second model will be summarized as a layer.\r\nAnd this layer can not be used for building another model using Model().\r\nIs it a bug or something?", "Solved.\r\n[Disconnected graph when concatenating two models @ stackoverflow](https://stackoverflow.com/questions/55073785/disconnected-graph-when-concatenating-two-models)"]}, {"number": 26503, "title": "Exception in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: linux, architecture: i386.", "body": "java -cp libtensorflow-1.12.0.jar:. -Djava.library.path=./jni HelloTensorFlow\r\n\r\nJava HotSpot(TM) Server VM warning: You have loaded library /home/administrator/soft/jni/libtensorflow_jni.so which might have disabled stack guard. The VM will try to fix the stack guard now.\r\nIt's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.\r\nException in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: linux, architecture: i386. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.\r\n\tat org.tensorflow.NativeLibrary.load(NativeLibrary.java:77)\r\n\tat org.tensorflow.TensorFlow.init(TensorFlow.java:66)\r\n\tat org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)\r\n\tat org.tensorflow.Graph.<clinit>(Graph.java:361)\r\n\tat HelloTensorFlow.main(HelloTensorFlow.java:8)\r\n", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 26502, "title": "IDE cannot resolve module tf.keras", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.10\r\n- TensorFlow installed from (source or binary):\r\nbinary (pip)\r\n- TensorFlow version (use command below):\r\n2.0_alpha\r\n- Python version:\r\n3.6\r\n\r\n**Describe the current behavior**\r\n`import tensorflow as tf`\r\nthen pyCharm cannot resolve module tf.keras and report an error:\r\n`Cannot find reference 'keras' in '__init__.py'`\r\nBut when running program, everything works well.\r\n\r\n**Describe the expected behavior**\r\ntf.keras imported successfully with autocomplete of pyCharm.\r\n\r\n**Code to reproduce the issue**\r\n`import tensorflow as tf`\r\n\r\n**Other info / logs**\r\nIt seems that there is no import command for `keras` module in `__init_.py` of tensorflow package.\r\nWhen I added `from tensorflow.python import keras` to `__init__.py`manually, everything work well.\r\nMaybe there are some problem for package importing after `keras` was moved from `_api` to `python`.\r\n\r\n\r\n", "comments": ["https://stackoverflow.com/a/47306203/6108843", "Not very familiar with how these modules work. @annarev would you know what's up?", "https://intellij-support.jetbrains.com/hc/en-us/community/posts/360002486739/comments/360000407199", "@knaydenov none of your solutions is pythonic. It should work like @tensorflow import keras in its examples.", "@annarev any solution to this issue? any commit for the next dev or stable release?", "Yes, this is not a bug from pyCharm but tensorflow itself.\r\nThe bug is caused by missing `tensorflow.python import keras` in `__init__.py` of tensorflow package.", "@ctrysbita do you think whether they resolve the issue in future releases?", "keras is imported dynamically in the `__init__.py` file. \r\nI am not familiar with the way pyCharm looks for autocomplete symbols, but I will see if there is any workaround (may be adding 'keras' to `__all__`).\r\n", "> keras is imported dynamically in the `__init__.py` file.\r\n> I am not familiar with the way pyCharm looks for autocomplete symbols, but I will see if there is any workaround (may be adding 'keras' to `__all__`).\r\n\r\nAs we discussed here, It's a serious problem which has been created by tensorflow implementation. Could you please kindly consider it to resolve as soon as possible for next versions?", "seems this issue hasn't been resolved yet, any idea to fix it temporarily? thx! ", "> seems this issue hasn't been resolved yet, any idea to fix it temporarily? thx!\r\n\r\nAt least for code completion to find it, a quick and dirty fix is to throw `from tensorflow.python import keras` at the top of `<python_site_packages_dir>/tensorflow/__init__.py`\r\nIn pycharm you can get there quickly by just going to the declaration of tensorflow (ctrl+b default)", "@jaingaurav @jvishnuvardhan @annarev  Hello, could you please let us when this bug will be fixed?", "> At least for code completion to find it, a quick and dirty fix is to throw `from tensorflow.python import keras` at the top of `<python_site_packages_dir>/tensorflow/__init__.py`\r\n> In pycharm you can get there quickly by just going to the declaration of tensorflow (ctrl+b default)\r\n\r\nDoing this solved the code completion issue but led to other errors, namely API incompatibilities. I tried creating my own Model by subclassing and I got strange errors from the _tensorflow_backend.py_ functions.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26502\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26502\">No</a>\n", "For folks hitting this issue, the fix has merged. Please try the latest 2.0 nightly and let us know if it is not yet resolved.", "Not resolved for me in pycharm", "@DecentGradient which version of TensorFlow are you using?", "@annarev tf-nightly-2.0-preview-2.0.0.dev20190430 ", "I just installed latest tf-nightly-2.0-preview and can't repro the issue. For example, I see autocompletion suggestions in PyCharms when typing `tf.keras.applications`.\r\n\r\nCan you describe the issue you are seeing in more detail? Which name doesn't autocomplete?\r\n", "@annarev when i import tensorflow.keras the ide says `Cannot find reference 'keras' in '__init__.py' ` and cannot autocomplete anything under keras @jaingaurav ", "Interesting , @annarev \r\nwhen using ` import tensorflow as tf ` then `tf.keras.*` auto completes \r\nBut  `from tensorflow.keras import layers` will still not \"work\"", "There is a difference between the two: `tf.keras.*` uses the attribute `keras` of module imported as `tf` but `import tensorflow.keras` looks for module `keras` in package `tensorflow`.", "@annarev @jaingaurav \r\nSorry, but it seems it doesn't work.\r\n when using `import tensorflow as tf` then `tf.keras.*` autocompletes\r\nBut `from tensorflow.keras import layers` will still not \"work\"!", "> There is a difference between the two: `tf.keras.*` uses the attribute `keras` of module imported as `tf` but `import tensorflow.keras` looks for module `keras` in package `tensorflow`.\r\n\r\nIt doesn't make sense to me. \r\nwhy `import tensorflow.keras` doesn't work properly? I would like to have `import tensorflow.keras as tfk` \r\nbut it doesn't work.", "I will reopen the bug. I suspect getting `import tensorflow.keras` would be much harder to fix. Especially, if it depends on having `keras` under `tensorflow` directory (which is not possible since these are installed as separate pip packages and live in different directories).", "Problem still existing in the lastest tensorflow-gpu-2.0.0a0, Please fix this.\r\n\r\nInsight normal in the terminal, but not in pycharm", "I think the issue comes from the 3 different ways we are doing imports:\r\n\r\n* normal Python import (`import foo` and `from foo import bar`)\r\n* imports by extending `sys.path` and `sys.modules` and `__path__`\r\n* using `LazyLoader` to make a name available, loading the corresponding module lazily\r\n\r\nI think that the last two are the ones causing issues and this will be hard to fix at the moment. There's another issue caused by the same root cause, just linking it here for completeness of the image: #28214 ", "@phdsky tensorflow-gpu-2.0.0a0 was released before the fix for `import tensorflow as tf` went in. That fix is only in nightly. I don't know if you are using `import tensorflow as tf`, but if you are, you can try nightly.\r\nIf you are using other ways of importing tensorflow, then pycharms suggestions won't work in any version.", "@ErmiaAzarkhalili to workaround `import tensorflow.keras as tfk`, you can use `from tensorflow import keras as tfk`", "On Pycharm, instead of `from tensorflow import keras`, try to use` from tensorflow.python import keras`  will help autocomplete. I tested with Tensorlow 2.0 alpha and it is working.\r\n", "As advised by @PhanDuc, it works for me on Tensorflow 2.0 alpha. Thanks.", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26502\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26502\">No</a>\n", "I believe this is unresolved as the current working suggestion is a temporary workaround", "I would say the original issue is resolved since it talks about `import tensorflow as tf` specifically.\r\nHowever, `from tensorflow.keras import losses` and similar are not resolved yet. May be it is better to open a new issue?", "I had the same problem and it bugged me for a good couple of hours :( . I used `from tensorflow.python.keras import ***` for importing the libraries and it worked fine for me.", "> @ErmiaAzarkhalili to workaround `import tensorflow.keras as tfk`, you can use `from tensorflow import keras as tfk`\r\n\r\nThanks, but is it pythonic solution? It seems circumvention to me, not a real solution.", "Well, for loss functions ` from tensorflow.python.keras.losses import *` or simple keras `from tensorflow.python import keras` should work.", "This occurs also in 1.14, and renders PyCharm completely useless with TensorFlow. It used to be OK, what happened?", "I am also still having this issue running PyCharm 2019.2 with Python 3.7 and `tf-2.0.0-beta1`. If I run `from tensorflow import keras` in a Python interpreter it works fine, but tf seems to have done something that renders PyCharm unable to find many of its submodules, which makes it difficult to work with due to the lack of autocomplete and false import warnings.", "> I had the same problem and it bugged me for a good couple of hours :( . I used `from tensorflow.python.keras import ***` for importing the libraries and it worked fine for me.\r\n\r\nThanks for the temporary solution! It had been working for a while; but now with `2.0.0-dev20190805`, even this solution does not work: no way to make PyCharm(2019.2) find the correct reference inside TensorFlow.", "Using `from tensorflow import keras` works but since almost all documentations / examples / books for TF 2.0 are using `tf.keras`, that's still a problem.\r\n\r\nAn bug tracking ticket for PyCharm is opened here: https://youtrack.jetbrains.com/issue/PY-34174", "I am still having this issue with pycharm 2019.1.3\r\nI cannot import keras from tensorflow. \"from tensorflow import keras\" doesn't work either.\r\nI am using the nightly build of TF2.0: tf-nightly-gpu-2.0-preview 2.0.0.dev20190821", "Same issue with the new tensorflow==2.0.0-rc0 release. If in beta release I could just use the tensorflow.python.keras workaround, now keras.layers module doesn't see the layers classes (Dense, Embedding, etc) but packages in which every layer is grouped. If I import them separately from each module the call of a layer over an input doesn't seem to work. This behaviour might be only a problem in PyCharm (in cmd works just fine) but as the majority of developers are using PyCharm as IDE it would be nice to not struggle to use tensorflow library. In addition the tensorflow namespace doesn't contain anything at import, all the packages are grouped in tensorflow_core. I understand that the backend grouping of modules must be done that way, the problem is that the importing step doesn't match the guides and tutorials on the official site and makes everything hard to use.\r\n\r\nIf it is any workaround on this issue until a fix is released (from tensorflow team or PyCharm) I would be grateful to use it.", "Someone should reopen this since neither `tensorflow-gpu-2.0.0rc0` nor `tf-nightly-gpu-2.0-preview` enables IDE to resolve `tf.keras`. I don't really understand why tf2 still uses such a complex importing method. tf1 has reinvented logging and argparse, now tf2 is still reinventing import.", "\ud83e\udd15 I just upgraded to 2.0rc ..... and my code is full of yellows\r\nEarlier for 2.0b I used to use the hack for keras and other stuff\r\n```\r\nimport tensorflow.python as tfpy\r\ntfpy.keras\r\n```\r\nNow that doesn't work .... but nonetheless code runs ....", "As `tensorflow_core` was mentioned above, please note that that is an undocumented experimental layout which might change in the future, so don't depend on it.\r\n\r\nIt exists now in order to allow us to break some component packages, to modularize TensorFlow and reduce bloat. But at one point the future it will get replaced (with proper release notes)", "I added an update on this issue on the freshly opened #31973 : https://github.com/tensorflow/tensorflow/issues/31973#issuecomment-524928419\r\n\r\nTL;DR: we are working on this but the fix is quite complex", "> I added an update on this issue on the freshly opened #31973 : [#31973 (comment)](https://github.com/tensorflow/tensorflow/issues/31973#issuecomment-524928419)\r\n> \r\n> TL;DR: we are working on this but the fix is quite complex\r\n\r\nThanks for your great comment. We seriously need this feature; otherwise, pycharm will be completely useless!!!", "Out of curiosity, what IDEs do the core developers use? I think most heavy-duty IDEs for software development would be affected by this, but I also would have assumed the core developers would use such IDEs. Seeing that this was left in, I'm guessing the developers are using an IDE that wasn't affected.", "what is going on here? i suffering this error from moths to now", "Does not work with the stable version of tensorflow.keras(stable release 1st October 2019)!!!!!", "Hey everyone, if you still struggle, please check my stackoverflow post in which I solve this issue with\r\nTensorFlow 2.0.0 stable release version.\r\n\r\nhttps://stackoverflow.com/questions/58188704/tensorflow-2-0-unable-to-import-keras-in-pycharm", "At least with PyCharm IDE (EAP release), it will work. I have tried it and it works. [Check here](https://stackoverflow.com/a/58192020/5681083). \r\n\r\nAvoid importing `tensorflow_core` if you want to keep code compatible.", "> I had the same problem and it bugged me for a good couple of hours :( . I used `from tensorflow.python.keras import ***` for importing the libraries and it worked fine for me.\r\n\r\nThat is kind of dangerous since you may end up importing different modules.\r\n\r\ne.g.\r\n\r\n`from tensorflow.python.keras.callbacks import TensorBoard; print(TensorBoard)` gets\r\n`<class 'tensorflow.python.keras.callbacks.TensorBoard'>`\r\nbut, `from tensorflow.keras.callbacks import TensorBoard; print(TensorBoard)` gets `<class 'tensorflow.python.keras.callbacks_v1.TensorBoard'>`\r\n\r\n", "This problem is annoying, i end up coding on Google Colab, there the module is recognized and the autocompletion works good (On experimental Colab Gui). I will stay this way while this is solved, but its a pain.", "With PyCharm 2019.3 (Early Access Preview or later) the issue disappears.", "why is this issue closed while it's still not fixed?", "Apparently they closed the issue because with \"PyCharm 2019.3 (Early Access Preview\" the issue disappears. However, at the time speaking, PyCharm 2019.3 still has not been released (it is R.C. now)", "Same issue with stable Intellij IDEA 2019.2.4 and tf2 in Match 2020...", "@fzyzcjy which import doesn't work for you? Can you try tensorflow 2.2.0rc0 ?", "@annarev All does not work... I will first try to update to IDEA 2019.3.4, and then report the situation to you. Thanks!", "Sounds good. Either updating Intellij IDEA to newer version or updating TensorFlow to 2.2.0rc0 should help. Let me know if you still see issues.", "@annarev After updating IDEA to 2019.3.4, it works! Thanks! :)", "tensorflow-version:'2.0.0' \r\nCUDA:10.0 Cudnn:7.6.4\r\n(I have tried update` tensorflow` to `2.2.0rc0` but I get `import error:Failed to load the native TensorFlow runtime` ) \r\n\r\npycharm version: 2019.2.4 -> 2019.3.4\r\n\r\nProblem solved!\r\nthanks @annarev and @fzyzcjy ", "tensorflow 1.13.1\r\nPyCharm 2019.3.4\r\n![image](https://user-images.githubusercontent.com/17675462/77259152-40660000-6c88-11ea-80ba-4260dffafced.png)\r\n![image](https://user-images.githubusercontent.com/17675462/77259157-44921d80-6c88-11ea-98fa-0b29b8c1a969.png)\r\n\r\n\r\n\r\nsame problem...", "@elisim I'm using TF2.0 + PyCharm 19.3. Your code works for me.\r\nMay be update to latest version can resolve the issue?\r\n", "@ctrysbita It's works with TF2.0, but I need TF1.3 for my needs.", "1.3 is too old, we no longer support it or bring fixes to it. Please update to 1.15 or 2.1 (long term releases)", "still not solved for TF 2.1 (Pylance 2020.9.6)", "i had such problem with tensorflow keras.leyer dn it doesnt work \r\nthe cod was  from keras.layer import DENSE nd i solve it by running this command in pycharm command erea \r\n\r\n\"pip install tensorflow\""]}, {"number": 26501, "title": "How to use libtensorflow-lite.a and tflite model on Raspi 3?", "body": "1. I want to install TensorFlow Lite on the Raspi. \r\nI'm reading the instructions to cross compile TensorFlow Lite https://www.tensorflow.org/lite/guide/build_rpi?hl=ru, but I have no idea what to do after generating libtensorflow-lite.a .\r\n\r\n2. Also, I have downloaded and converted mobilenet_v1_1.0_224.pb to .fb format.\r\nBut how do I use the .fb model to detect any object?\r\n", "comments": ["@shubham9436  I've the same Question. Writing this comment to get a notification when someone answers.", "Hi, I am also kinda curious about the workflow after successfully compiling the static library for Rasp Pi (Armv7l) architecture. This static library should also support (tensorflow-lite) *.tflite file according to the Tensorflow documentation on getting-started-tflite. \r\n\r\nhttps://www.tensorflow.org/lite/guide/get_started#3_use_the_tensorflow_lite_model_for_inference_in_a_mobile_app", "Same here. The documentation on this is very light, to say the least.", "same problem here.\r\nit's like doing halfway and suddenly stuck.\r\nhow exactly are we going to do with this libtensorflow-lite.a file on raspberriy pi?", "same here", "**Edit: Changing for a more appropriate linking syntax**\r\n\r\nI have been able to compile [minimal.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc) with libtensorflow-lite.a on my raspberry pi using:\r\n\r\n`g++ -I${TENSORFLOW_ROOT_DIR}/tensorflow/lite/tools/make/downloads/flatbuffers/include -I${TENSORFLOW_ROOT_DIR} -pthread -Wall -Wextra -pedantic -o minimal minimal.cc -L${TENSORFLOW_ROOT_DIR}/tensorflow/lite/tools/make/gen/rpi_armv7l/lib -ltensorflow-lite`\r\n\r\nwhere `${TENSORFLOW_ROOT_DIR}` is the path to the root of the cloned tensorflow repository.\r\nThe -pthread tag appears to be required by the eigen library.", "and thken you get minimal.so? what do you do then ?\r\n\r\n> **Edit: Changing for a more appropriate linking syntax**\r\n> \r\n> I have been able to compile [minimal.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc) with libtensorflow-lite.a on my raspberry pi using:\r\n> \r\n> `g++ -I${TENSORFLOW_ROOT_DIR}/tensorflow/lite/tools/make/downloads/flatbuffers/include -I${TENSORFLOW_ROOT_DIR} -pthread -Wall -Wextra -pedantic -o minimal minimal.cc -L${TENSORFLOW_ROOT_DIR}/tensorflow/lite/tools/make/gen/rpi_armv7l/lib -ltensorflow-lite`\r\n> \r\n> where `${TENSORFLOW_ROOT_DIR}` is the path to the root of the cloned tensorflow repository.\r\n> The -pthread tag appears to be required by the eigen library.\r\n\r\n", "> and thken you get minimal.so? what do you do then ?\r\n> \r\n> > **Edit: Changing for a more appropriate linking syntax**\r\n> > I have been able to compile [minimal.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc) with libtensorflow-lite.a on my raspberry pi using:\r\n> > `g++ -I${TENSORFLOW_ROOT_DIR}/tensorflow/lite/tools/make/downloads/flatbuffers/include -I${TENSORFLOW_ROOT_DIR} -pthread -Wall -Wextra -pedantic -o minimal minimal.cc -L${TENSORFLOW_ROOT_DIR}/tensorflow/lite/tools/make/gen/rpi_armv7l/lib -ltensorflow-lite`\r\n> > where `${TENSORFLOW_ROOT_DIR}` is the path to the root of the cloned tensorflow repository.\r\n> > The -pthread tag appears to be required by the eigen library.\r\n\r\nThen you have compiled your program `minimal` and linked it to `libtensorflow-lite.a` and can execute it: `./minimal /path/to/your/model.tflite`", "I find 2 demos use .a files here #24194 and here is his example [complete-procedure-to-train-and-recognize-you-by-raspberry-pi-3-and-tensorflow-lite](https://github.com/huaxiaozhong1/complete-procedure-to-train-and-recognize-you-by-raspberry-pi-3-and-tensorflow-lite). Hope it might help(", "Assigning to @aselle to track the RPi TF Lite progress.", "Any update on this please - feel like I went through all this effort and left hanging at the end not understandfing whats the next step with libtensorflow-lite.a???", "Waiting for the same.", "Ok Thanks for response. Disappointing\n\nOn Thu, 26 Sep 2019 at 11:13, Dilip Lilaramani <notifications@github.com>\nwrote:\n\n> Waiting for the same.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26501?email_source=notifications&email_token=ADXDB6HDE5JX7XEGNGMUEQLQLSDN7A5CNFSM4G4Z2VWKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7VBP2Y#issuecomment-535435243>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADXDB6C46O6PM3RT6QPODNTQLSDN7ANCNFSM4G4Z2VWA>\n> .\n>\n\n\n-- \nBest regards\n\nStephen Devane\n", "libtensorflow-lite.a is just the compiled static library of tensorflow lite that you can use in your own custom c++ code to inference a neural network. The minimal.cc is an example code that shows you how you can actually use the tensorflow-lite static library. So you should have a look into the minimal.cc source file to get an understanding.\r\n\r\nFor using your own network you need to train your own model and convert it to a .tflite file with the tensorflow lite converter. Or you can use a pretrained model with .tflite format.", "@marcel1991 When I look through the minimal.cc code, there is no reference to the static library. Only the following includes:\r\n\r\n```\r\n#include <cstdio>\r\n#include \"tensorflow/lite/interpreter.h\"\r\n#include \"tensorflow/lite/kernels/register.h\"\r\n#include \"tensorflow/lite/model.h\"\r\n#include \"tensorflow/lite/optional_debug_tools.h\"\r\n```", "The static library must be provided to the linker after you compiled your code. For example when you use a build tool like Cmake you must provide the path to the static library in the cmake file. In the code itself you only reference the header files of the static library. In the minimal.cc these are the #include \"tensorflow/lite/*\" files.\r\n\r\nOr are you wondering where to find the actual lib file?", "So you need both the static library and the header files from the includes? Could you share an example on how to compile minimal.cc on linux?", "@4p0pt0Z already showed a complete example in his reply:\r\n\r\n> g++ -I${TENSORFLOW_ROOT_DIR}/tensorflow/lite/tools/make/downloads/flatbuffers/include -I${TENSORFLOW_ROOT_DIR} -pthread -Wall -Wextra -pedantic -o minimal minimal.cc -L${TENSORFLOW_ROOT_DIR}/tensorflow/lite/tools/make/gen/rpi_armv7l/lib -ltensorflow-lite\r\n> \r\n> where `${TENSORFLOW_ROOT_DIR}` is the path to the root of the cloned tensorflow repository.\r\n> The -pthread tag appears to be required by the eigen library.\r\n\r\nFor example you can use g++ to compile the minimal.cc file and then link the static library with the -L flag. I think its best if you read something about general compilation and linking of libraries in linux. It's just a general compilation pipeline nothing special related to tensorflow.\r\n\r\n", "There is no hint about how to use the static library (or any other solution) for a python package.\r\nright now, there is no way to run python TF lite on the beaglebone. (or any other board arm board it seems)", "Type above command for running on pi.\r\nexport LD_LIBRARY_PATH=/your .a file path/libtensorflow-lite.a\r\n./minimal  /your model file path/yolov3-tiny_int8.tflite\r\n\r\nThank you.\r\n", "Hey all, I also struggled with this for a while. I've written this repo that have a single cmake file rules that will compile a minimal example of tflite in c++ :)\r\nhttps://github.com/Namburger/edgetpu-minimal-example\r\n\r\nIt also has code plugin for using with the [edgeptu](https://coral.ai/products/accelerator) for faster inference but that's optional. Building should be as easy as this on the rpi.\r\n```\r\n$ git clone https://github.com/Namburger/edgetpu-minimal-example.git && cd edgetpu-minimal-example\r\n$ ./scripts/install_cmake.sh // install cmake if needed\r\n$ ./scripts/make2gbswap.sh //  [Optional] increase swap to avoid OOM killer\r\n$ mkdir build && cd build\r\n$ cmake ..\r\n$ make\r\n```\r\nRun (from the build directory):\r\n```\r\n$ ../out/aarch64/minimal ../test_data/mobilenet_v1_1.0_224_quant.tflite ../test_data/resized_cat.bmp ../test_data/imagenet_labels.txt\r\n```\r\n\r\nWould appreciate a star!", "Could you Please refer to [**`link1`**](https://stackoverflow.com/questions/52637206/how-to-use-libtensorflow-lite-a-on-raspi-3), [**`link2`**](https://github.com/tensorflow/tensorflow/pull/24194), [**`link3`** ](https://github.com/huaxiaozhong1/complete-procedure-to-train-and-recognize-you-by-raspberry-pi-3-and-tensorflow-lite) , [**`link4`**](https://www.tensorflow.org/lite/api_docs/cc/class/tflite/interpreter) and comment: https://github.com/tensorflow/tensorflow/issues/26501#issuecomment-644816894 . I hope it helps, Hence moving this to closed status. Please open a new issue in case you face any errors. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26501\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26501\">No</a>\n"]}, {"number": 26500, "title": "LSTM recurrent kernel ( hidden state ) makes Host to GPU transaction for every sequence, which looks quite inefficient.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): build from source\r\n- TensorFlow version (use command below): v1.13.0-7-g9ca8321 1.12.0\r\n- Python version: Python 3.6.1 :: Anaconda 4.4.0 (64-bit)\r\n- Bazel version (if compiling from source): Build label: 0.19.2\r\n- GCC/Compiler version (if compiling from source): gcc version 5.4.0 (GCC)\r\n- CUDA/cuDNN version: \r\nI got cuda version by checking nvcc verison. it is Cuda compilation tools, release 10.0, V10.0.130.\r\ncudnn 7.5.0\r\n- GPU model and memory: v100\r\n\r\nI am evaluating the inference performance of 2-level stacked LSTM model. \r\nAs you could see in the code below, my model has 2 LSTM cell and one time-distributed (FC) layer.\r\nBecause i want to make close investigation of how inference works, i utilize google timeline tool.\r\n\r\n**Describe the current behavior**\r\nWhenever new sequence starts, the recurrent kernel ( hidden state of lstm cell ) makes Host to GPU data transaction. Because this data transfer is initiated by ReadVariableOp which is connected to matmul op of a lstm hidden state, i am pretty sure that a data is the weight of the hidden state.\r\nWhat i am curious is that why the weight has to move from H2D? if right, where is the D2H transaction? Doesn't hidden state be renewed every new sequence?\r\n\r\n**Describe the expected behavior**\r\nI think there should be two kinds of correct behavior.\r\n1. For every sequence of lstm, GPU renew the weight, and this is moved to Host memory (D2H transaction - which doesn't exist at the timeline result). And then the weight goes to GPU again at the next sequence. \r\n2. No Transaction. the hidden state is renewed and reused only inside the GPU memory. In this case, i could see the H2D transaction (which is observed at the timeline result)\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nimport time\r\n\r\nfrom tensorflow.keras.models import Sequential, Model\r\nfrom tensorflow.keras.layers import Dense, Activation, LSTM, RepeatVector, Input, TimeDistributed\r\nfrom tensorflow.python.client import timeline\r\n\r\n# configuration\r\niteration = 1\r\nbatch = 1024\r\nseq_len = 26\r\nfvec = 256#1024\r\nunit = 256 \r\ndtype = tf.float32\r\nnp_dtype = 'float32'\r\n\r\n# set precision of model\r\nK.set_floatx(np_dtype)\r\nprint(K.floatx())\r\n\r\n# session setting\r\nK.clear_session()\r\nconfig = tf.ConfigProto() \r\nsess = tf.Session(config=config)\r\nK.set_session(sess)\r\n\r\n# make random input data\r\ninp_dims = ( batch, seq_len, fvec)\r\ninput_data = np.random.random_sample(inp_dims).astype(np_dtype)\r\n\r\nwith tf.device('/device:GPU:0'):\r\n\tinput_box = tf.placeholder(dtype, shape=(None, seq_len,fvec))\r\n\tout = LSTM(unit  , activation='sigmoid', input_shape=(seq_len, fvec),  implementation=2)(input_box) # 1st LSTM\r\n\tout = RepeatVector(seq_len)(out)\r\n\tout = LSTM(unit, activation='sigmoid', return_sequences=True, implementation=2)(out)\r\n\tout = TimeDistributed(Dense(fvec))(out)\r\n\r\n# variable initialization\r\ninit = tf.global_variables_initializer()\r\nsess.run(init)\r\n\r\n# for tracing\r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\nrun_metadata = tf.RunMetadata()\r\n\r\n# check time lapse\r\nstart_time = time.time()\r\nfor i in range(iteration):\r\n  output = sess.run(out, feed_dict={input_box:input_data} , options = run_options , run_metadata= run_metadata)\r\n  print (type(output), output.shape)\r\nend_time = time.time()\r\n\r\n# get trace information\r\ntl= timeline.Timeline(run_metadata.step_stats)\r\nctf = tl.generate_chrome_trace_format(show_memory=True)\r\n\r\n# store the trace information\r\nwith open (\"./all_model_trace_\" + np_dtype +\"_infer_0306_cpumem.json\", 'w') as f:\r\n  f.write(ctf)\r\n\r\n#save graph file..\r\nsess = K.get_session()\r\ngraph_writer = tf.summary.FileWriter(\"./output_4/graph\", sess.graph)\r\ntf.train.write_graph(sess.graph, './output_4/', 'saved_model.pb', as_text=True)\r\n\r\n# print time lapse\r\ntime_lapse = end_time - start_time\r\nprint(time_lapse)\r\n```\r\n", "comments": ["You should try [CuDNNLSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/CuDNNLSTM) ", "Fast Evaluation reveals that CuDNNLSTM makes performance three times better than a simple LSTM layer. I'll compare the performance of FP32(Cudacore) and FP16(Tensorcore) and  post it later. ", "Closing this issue due to lack of updates. Feel free to reopen if have any follow up questions. Thanks!"]}, {"number": 26499, "title": "Providing a Prebuilt Binary for TFLite Benchmark", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): latest\r\n- Are you willing to contribute it (Yes/No): Y\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThere are many people benchmarking their tflite models using the https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark . However, it seems that this does not provide a pre-built binary, so we have to build it by ourselves. Since there are many people needing to benchmark, I think the pre-built binary can benefit a lot of people and save their time. I would appreciate it if it could be done.\r\n\r\n**Will this change the current api? How?**\r\nN\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who uses tflite benchmark\r\n\r\n**Any Other info.**\r\nN/A\r\n", "comments": ["In addition, it does not support windows compile \ud83d\ude22", "Thanks for the suggestion.\r\n\r\nThe problem is one of hosting and deployment. We don't store binaries in the actual GitHub repo, so what would be the distribution model for the binary? We could potentially bundle it in the TensorFlow pip, but then you're getting *host* performance, not on-device performance.", "Sorry, I do not quite get your point... I think, if the binaries are bundles in pip, everything will be fine and I can get on-device performance!\r\n\r\nLooking at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark , we see these steps:\r\n\r\n(0) ndk install\r\n(1) Build for your specific platform\r\n(2) Connect your phone. Push the binary to your phone with adb push (make the directory if required):\r\n(3) Make the binary executable.\r\n(4) Push the compute graph that you need to test.\r\n(5) Run the benchmark. For example:\r\n\r\nThe step (1) is the only step which requires building from source. If the binary is bundled and given in pip, then I think I can get **on-device** performace, because: In step (2)-(5), I push _my own model(compute graph)_, and I execute the binary on _my own mobile phone_. Thus I think it is _on device_, not _hosted_.\r\n\r\nPlease correct me if I am wrong, and thank you very much for your reply!!\r\n\r\n@jdduke ", ">  If the binary is bundled and given in pip\r\n\r\nFor better or worse, we cannot bundle *Android* binaries in a pip compiled for a desktop PC.\r\n\r\nWhat we can do is provide a single shell script which automates steps (1)-(5). Would that help?", "> For better or worse, we cannot bundle _Android_ binaries in a pip compiled for a desktop PC.\r\n\r\nOh it does not matter in _pip for PC_. It would be excellent if the binary can be put **somewhere**! ", "@multiverse-tf can you link to the prebuilts?", "For benchmark binary - https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary", "Nice to see it is now there! (Though 2.5 yr has passed and I am no longer doing tflite...)", "Closing this issue now since the feature is added. Thanks!"]}, {"number": 26497, "title": "[TF 2.0 API Documentation Issue] `tf.lookup.StaticHashTable` usage example is incorrect", "body": "**System information**\r\n- TensorFlow version: 2.0 preview\r\n- Doc Link: [`tf.lookup.StaticHashTable`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/lookup/StaticHashTable#class_statichashtable)\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nThe example usage describes using `StaticHashTable.init.run()`, which is not possible since there is no `init` attribute (and it's missing from the documentation). [This comment](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/lookup_ops.py#L321) in `lookup_ops.py` indicates that it's definitely not the correct way to initialize the table in TF 2.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n\r\nI don't think so, because I really have no idea what the correct way to initialize the table is :)\r\n\r\nI've tried a few things [as shown here](https://gist.github.com/zmjjmz/81138b0d62764fabe00085cd7091dd14#file-tokenize_layer_tf2-py-L50) but to no avail, usually ending in a `FailedPreconditionError` :(\r\n", "comments": ["I am not getting any such example, please can anybody guide me", "The example I'm referring to is in the doc link, and is (as of writing) \r\n```\r\ntable = tf.lookup.StaticHashTable(\r\n    tf.KeyValueTensorInitializer(keys, values), -1)\r\nout = table.lookup(input_tensor)\r\ntable.init.run()\r\nprint(out.eval())\r\n```", "This issue also causing me some headaches -- any updates?", "@zmjjmz,\r\nThe documentation and usage example for [tf.lookup.StaticHashTable](https://www.tensorflow.org/api_docs/python/tf/lookup/StaticHashTable) have been updated from TF v2.0. \r\n\r\nAnd I was able to run the usage example without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/0b912a9f4c1c2f5b67e046a89733b67a/26497.ipynb). Could you please take a look at it and let us know if this is still an issue. Thanks!", "Hey @amahendrakar \r\n\r\nThis works. Thanks!", "@zachary-jablons-okcupid,\r\nThank you for the update.\r\n\r\nClosing the issue as it is resolved. Please feel free to re-open if necessary."]}, {"number": 26496, "title": "tflite_convert -- Unknown layer: VladPooling", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OSX 10.14.3\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): 1.13.1\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nValueError: Unknown layer: VladPooling\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\nhttps://drive.google.com/open?id=1M_SXoW1ceKm3LghItY2ENKKUn3cWYfZm\r\n(from: https://github.com/WeidiXie/VGG-Speaker-Recognition)\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nCommand line: tflite_convert --output_file=weights_keras.tflite --keras_model_file=weights_keras.h5 \r\n\r\nResult:\r\nTraceback (most recent call last):\r\n  File \"/anaconda3/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 442, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 438, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 122, in _convert_model\r\n    converter = _get_toco_converter(flags)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 109, in _get_toco_converter\r\n    return converter_fn(**converter_kwargs)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 370, in from_keras_model_file\r\n    keras_model = _keras.models.load_model(model_file)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/saving.py\", line 234, in load_model\r\n    model = model_from_config(model_config, custom_objects=custom_objects)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/saving.py\", line 324, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py\", line 74, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 192, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1263, in from_config\r\n    process_layer(layer_data)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1249, in process_layer\r\n    layer = deserialize_layer(layer_data, custom_objects=custom_objects)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py\", line 74, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 181, in deserialize_keras_object\r\n    config, module_objects, custom_objects, printable_module_name)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 166, in class_and_config_for_serialized_keras_object\r\n    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\nValueError: Unknown layer: VladPooling", "comments": ["This model requires the `custom_object` parameter when loading the model. This is only available through the Python API. The approach for using custom objects in 1.13 without Eager enabled is similar to the following (where `VladPooling` needs to be imported by your code):\r\n\r\n```\r\n# Load the Keras model into a session.\r\nkeras_model = tf.keras.models.load_model(\r\n    keras_file, custom_objects={'VladPooling': VladPooling})\r\nsess = tf.keras.backend.get_session()\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_session(sess, keras_model.inputs,\r\n                                                 keras_model.outputs)\r\nconverter.convert()\r\n```\r\n\r\nSupport for `custom_objects` will be added to `from_keras_model_file` shortly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26496\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26496\">No</a>\n", "Has this been solved?"]}, {"number": 26495, "title": "softmax_cross_entropy_with_logits_v2 doesn't support hypergradients of labels", "body": "Current implementation of tf.nn.softmax_cross_entropy_with_logits_v2 doesn't correctly implement the  hypergradients w.r.t labels. I implemented a hypergradient demo where I used softmax_cross_entropy_with_logits_v2 to compute meta_loss, and a custom function to compute meta_loss2. We should either expect that:\r\n```\r\ntf.gradients(meta_loss, [a]) == tf.gradients(meta_loss2, [a])\r\n```\r\n\r\nOr that \r\n```\r\ntf.gradients(meta_loss, [a]) == None\r\n```\r\n\r\nInstead, TensorFlow incorrectly returns 0s for hypergradients of softmax_cross_entropy_with_logits_v2 w.r.t labels.\r\n\r\n\r\nCode to reproduce:\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\na = tf.get_variable(\"a\", (2, 2)) * 10\r\nb = tf.get_variable(\"b\", (2, 2)) * 10\r\na2 = tf.get_variable(\"a2\", (2, 2)) * 10\r\n\r\nlabels = tf.nn.softmax(a, -1)\r\nlabels2 = tf.nn.softmax(a2, -1)\r\nloss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=b))\r\ngrads = tf.gradients(loss, [b])\r\n\r\nb_tp1 = b + grads[0]\r\nmeta_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels2, logits=b_tp1)\r\n\r\nlogits = tf.nn.log_softmax(b, -1)\r\nloss2 = tf.reduce_sum(-tf.reduce_sum(labels * logits))\r\ngrads2 = tf.gradients(loss2, [b])\r\nb2_tp1 = b + grads2[0]\r\n\r\nmeta_loss2 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels2, logits=b2_tp1)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nprint(sess.run([tf.gradients(meta_loss, [a]), tf.gradients(meta_loss2, [a])]))\r\n```\r\n\r\n**System information**\r\n- TensorFlow-gpu 1.10.1\r\n", "comments": ["@fps7806 Could you check whether the bug persists with latest TF? Thanks!", "> @fps7806 Could you check whether the bug persists with latest TF? Thanks!\r\n\r\nStill present on the latest release (1.13.1)", "@fps7806 Is this still an issue for you. I ran your code with recent `tf-nightly`. Please check the [gist Here](https://colab.research.google.com/gist/jvishnuvardhan/0d7a17908038ac233ea47f3f886692c7/untitled.ipynb).\r\n\r\nCurrently `TF1.x` is not supported. Can you please check with `TF2.x` and let us know whether the issue persists with `TF2.x`. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26495\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26495\">No</a>\n"]}, {"number": 26494, "title": "doc improvement for tf.math.asin and tf.math.atan", "body": "PR for issue #26492 ", "comments": ["> Do we not have acos?\r\n> \r\n> Assuming we do, the same change needs to be made there as well I assume.\r\n\r\nActually, I have fixed acos in #26017. Would you be able to look at that PR as well? It has been pending for a long time.\r\n\r\nAs for the changes, will writing something like...\r\n**\"Note: The output of `tf.math.asin` will lie within the invertible range of sine i.e. [-pi/2, pi/2] \"**\r\n...be enough or should I add another usage example that demonstrates this?", "I think the wording changes you propose are good. \r\n\r\nThis comment also applies to #26017, can you make the same change to cos (except of course the range is different)?\r\n\r\n", "Alright, I've specified the range for all the three operations. ", "@martinwicke will you be able to approve #26017 as well if that PR is fine?"]}, {"number": 26492, "title": "[TF 2.0 API Docs] tf.math.atan and tf.math.asin", "body": "**System information**\r\n- TensorFlow version: 2.0\r\n- Doc Links:\r\n   tf.math.atan: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/atan\r\n   tf.math.asin: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/asin\r\n\r\n**Describe the documentation issue**\r\nDocumentation for `tf.math.asin` and `tf.math.atan` is created from a generated file, `python/ops/gen_math_ops.py`. The documentation can be modified by editing the appropriate `.pbtxt` files within the `tensorflow/tensorflow/core/api_def/base_api` directory of the source repository.\r\n\r\nBoth of these math operations could use a clear description, usage examples, and a list specifying the errors raised by these operations.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nDefinitely :smile: ", "comments": ["@Sudeepam97, Can we close the issue since the associated PR has been merged. Thanks!"]}, {"number": 26491, "title": "Making memory allocation exception safe", "body": "Used make_unique instead of malloc", "comments": ["@jdduke , i get your point, but why to use this inside also if other options are available. Anyway it was great talking to you, hope for more such interactions in the future. I am closing the PR."]}, {"number": 26490, "title": "Using Custom Loss in Keras ", "body": "**System information**\r\n- Uses a basic CNN MNIST Keras example\r\n- CentOS7\r\n- TensorFlow installed from: Anaconda\r\n- TensorFlow version: Both 1.12-gpu and 2.0.0-alpha0-cpu\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nI am trying to use a custom loss function in Keras. Ive simplified its use below, and discover that if I try to personally \"touch\" `y_pred` and `y_true` in anyway (even trivially as seen below), the model:\r\n\r\n- Does **not** achieve high accuracy/categorgical accuracy/etc.\r\n- **Does** achieve \"low-loss\"\r\n\r\nIn the code below I try to basic approaches:\r\n\r\n1. `customLossThatWorks` is just an alias for a built in loss - This gets me to a low loss value, and high accuracy\r\n2. Any of the other losses that actually \"touch\" the `y_` arguments, even one that is directly copied and pasted, achieve the same low loss, but not accuracy.\r\n\r\n**Describe the expected behavior**\r\nWrite a custom loss that works, as per the documentation.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.keras import datasets, layers, models\r\n\r\n(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\r\n\r\ntrain_images = train_images.reshape((60000, 28, 28, 1))\r\ntest_images = test_images.reshape((10000, 28, 28, 1))\r\n\r\n# Normalize pixel values to be between 0 and 1\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\ndef customLossThatWorks():\r\n    return tf.keras.losses.sparse_categorical_crossentropy\r\n\r\n# def customLoss(y_true, y_pred):\r\n#     return K.sparse_categorical_crossentropy(y_true, y_pred)\r\n\r\n# def customLoss():\r\n#     def loss(y_true,y_pred):\r\n#         return K.sparse_categorical_crossentropy(y_true, y_pred)\r\n#     return loss\r\n    \r\n# def customLoss():\r\n#     def loss(y_true,y_pred):\r\n#         return tf.keras.losses.sparse_categorical_crossentropy(y_true,y_pred)\r\n#     return loss\r\n\r\n# copied from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/losses.py#L867\r\ndef customLoss(y_true, y_pred, from_logits=False, axis=-1):\r\n    return K.sparse_categorical_crossentropy(\r\n      y_true, y_pred, from_logits=from_logits, axis=axis)\r\n\r\nmodel = models.Sequential()\r\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(64, activation='relu'))\r\nmodel.add(layers.Dense(10, activation='softmax'))\r\nmodel.compile(loss=customLoss, optimizer='adam', metrics=['accuracy', 'categorical_accuracy'])\r\nmodel.fit(train_images, train_labels, epochs=1)\r\n```\r\n\r\n**Other info / logs**\r\n* **Note**: If the custom loss function handles `y_pred` and `y_true` directly, you just pass the function without invoking it: `=customLoss` vs `=customLoss()`\r\n\r\n\r\n* Run the above, commenting out different versions of the loss and changing the compilation line accordingly: `model.compile(loss=customLoss(), optimizer='adam', metrics=['accuracy', 'categorical_accuracy'])` to observe the behavior\r\n* The answer in #16721 and the [SO question](https://stackoverflow.com/questions/48654851/customized-loss-in-tensorflow-with-keras) generated from it, don't address this problem.", "comments": ["Your code does not even run, as you have to feed customLoss() with y_true and y_pred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 43, in <module>\r\n    model.compile(loss=customLoss(), optimizer='adam', metrics=['accuracy', 'categorical_accuracy'])\r\nTypeError: customLoss() missing 2 required positional arguments: 'y_true' and 'y_pred'\r\n", "@timudk I've updated the code - specifically changing this one line:\r\n\r\nFROM:\r\n```\r\nmodel.compile(loss=customLoss(), optimizer='adam', metrics=['accuracy', 'categorical_accuracy'])\r\n```\r\n\r\nTO:\r\n```\r\nmodel.compile(loss=customLoss, optimizer='adam', metrics=['accuracy', 'categorical_accuracy'])\r\n```\r\n\r\nWhen the loss function handles `y_pred` and `y_true` directly, you just pass the function without invoking it.\r\n\r\nI've updated the original comment with this information as well. ", "@diego898 Thanks, I got it.\r\n\r\nSo I replaced:\r\n`def customLoss(y_true, y_pred, from_logits=False, axis=-1):\r\n    return K.sparse_categorical_crossentropy(\r\n      y_true, y_pred, from_logits=from_logits, axis=axis)`\r\n\r\nwith\r\n\r\n`def customLoss(y_true, y_pred, from_logits=False, axis=-1):\r\n\treturn tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)`\r\n\r\nand even this does not work, which seems really weird.\r\n", "Hey @timudk, according to the docs I only have to handle `y_true` and `y_pred` directly and it should work. For example, see one of the other customLosses that are commented out. \r\n\r\nAlso, when you say \"works fine\" what do you mean? One of my problems is that it reports low loss, but also low accuracy in certain cases.", "@diego898 \r\n\r\nI have figured out what the problem is. When you use your custom loss, Keras things that accuracy=categorical_accuracy. Just change \r\n`model.compile(loss=customLoss, optimizer='adam', metrics=['accuracy', 'categorical_accuracy'])`\r\n\r\nto \r\n\r\n`model.compile(loss=customLoss, optimizer='adam', metrics=['sparse_categorical_accuracy', 'categorical_accuracy'])`\r\n\r\nand you should not see a difference between your custom loss and tf.keras.losses.sparse_categorical_crossentropy.\r\n\r\nI hope this helps :)", "@timudk - thank you I thought TF was doing something behind the scenes automatically that was failing in one case. Im not sure this is the intended behavior, so I will open a new issue to discuss it."]}, {"number": 26489, "title": "Tried - Error Logging for GradientTape #26143", "body": "i have added isinstance() method to check variable type and raise according exception message", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26489) for more info**.\n\n<!-- need_sender_cla -->", "@shashvatshahi1998 please sign CLA", "i am trying again and again why this is not working.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26489) for more info**.\n\n<!-- ok -->", "ok its done", "i have commited once again can you please review it", "Did you mean to close this PR?"]}, {"number": 26488, "title": "TOCO reports SymbolAlreadyExposedError:  Symbol AttrValue is already exposed as ().", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nArch Linux 4.20.1-arch1-1-ARCH\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version (use command below):\r\n`master` (reproduction instructions below)\r\n- Python version:\r\n3.7.2\r\n- Bazel version (if compiling from source):\r\n0.21.0-1\r\n- GCC/Compiler version (if compiling from source):\r\n8.2.1\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nRunning `toco` with or without arguments throws this traceback:\r\n```\r\n$ toco\r\nTraceback (most recent call last):\r\n  File \"/auxiliary//home/ajarthurs/Development/data/python/bin/toco\", line 6, in <module>\r\n    from tensorflow.lite.python.tflite_convert import main\r\n  File \"/auxiliary/home/ajarthurs/Development/data/python/tensorflow/__init__.py\", line 33, in <module>\r\n    from tensorflow_estimator import __path__ as _new_path\r\n  File \"/auxiliary/home/ajarthurs/Development/data/python/tensorflow_estimator/__init__.py\", line 8, in <module>\r\n    from tensorflow_estimator._api.v1 import estimator\r\n  File \"/auxiliary/home/ajarthurs/Development/data/python/tensorflow_estimator/_api/v1/estimator/__init__.py\", line 8, in <module>\r\n    from tensorflow_estimator._api.v1.estimator import experimental\r\n  File \"/auxiliary/home/ajarthurs/Development/data/python/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\", line 8, in <module>\r\n    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\r\n  File \"/auxiliary/home/ajarthurs/Development/data/python/tensorflow_estimator/python/estimator/canned/dnn.py\", line 23, in <module>\r\n    from tensorflow.python.feature_column import feature_column\r\n  File \"/auxiliary/home/ajarthurs/Development/data/python/tensorflow_core/python/__init__.py\", line 157, in <module>\r\n    tf_export(v1=['AttrValue'])(AttrValue)\r\n  File \"/auxiliary/home/ajarthurs/Development/data/python/tensorflow_core/python/util/tf_export.py\", line 334, in __call__\r\n    self.set_attr(undecorated_func, api_names_attr, self._names)\r\n  File \"/auxiliary/home/ajarthurs/Development/data/python/tensorflow_core/python/util/tf_export.py\", line 346, in set_attr\r\n    (func.__name__, getattr(func, api_names_attr)))  # pylint: disable=protected-access\r\ntensorflow_core.python.util.tf_export.SymbolAlreadyExposedError: Symbol AttrValue is already exposed as ().\r\n```\r\n\r\n**Describe the expected behavior**\r\n```\r\n$ toco\r\nusage: toco [-h] --output_file OUTPUT_FILE\r\n            (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE)\r\n            [--output_format {TFLITE,GRAPHVIZ_DOT}]\r\n            [--inference_type {FLOAT,QUANTIZED_UINT8}]\r\n            [--inference_input_type {FLOAT,QUANTIZED_UINT8}]\r\n            [--input_arrays INPUT_ARRAYS] [--input_shapes INPUT_SHAPES]\r\n            [--output_arrays OUTPUT_ARRAYS]\r\n            [--saved_model_tag_set SAVED_MODEL_TAG_SET]\r\n            [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]\r\n            [--std_dev_values STD_DEV_VALUES] [--mean_values MEAN_VALUES]\r\n            [--default_ranges_min DEFAULT_RANGES_MIN]\r\n            [--default_ranges_max DEFAULT_RANGES_MAX]\r\n            [--post_training_quantize] [--drop_control_dependency]\r\n            [--reorder_across_fake_quant]\r\n            [--change_concat_input_ranges {TRUE,FALSE}] [--allow_custom_ops]\r\n            [--target_ops TARGET_OPS] [--dump_graphviz_dir DUMP_GRAPHVIZ_DIR]\r\n            [--dump_graphviz_video]\r\ntoco: error: the following arguments are required: --output_file\r\n```\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThis issue first appeared when I switched to TF commit 13f022a946c7fbfeb1786488d278b675bd027a87. TOCO worked fine when I was on TF commit 12606ff846566dd842444f27f7fed4f911a58580.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Narrowed it down. This issue first appears in TF commit fc85579a2b4b689aa9c56d0434c3433479750d23.", "Reverted in 61f5a61d25f2d8ef422f14331584fb49af97e493. Fixed."]}, {"number": 26487, "title": "Using \"for input_image in tf.data.Dataset: \" TypeError: zip argument #1 must support iteration ", "body": "I try using the tf,data.Dataset to load my own images from local directory.\r\n\r\nBut when I want to feed batch_size data to model, I got a TypeError: zip argument #1 must support iteration\r\n\r\nHere is my code\r\n\r\n```\r\nImport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\n\r\nimport os\r\nimport time\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport PIL\r\nfrom IPython.display import clear_output\r\n\r\ntrain_dataset = tf.data.Dataset.list_files(image_path + 'train_imgs/*png')\r\ntrain_dataset = train_dataset.shuffle(200)\r\ntrain_dataset = train_dataset.map(lambda x: load_image(x))\r\ntrain_dataset = train_dataset.batch(16)\r\n\r\nval_dataset = tf.data.Dataset.list_files(image_path + 'val_imgs/*.png')\r\nval_dataset = val_dataset.shuffle(200)\r\nval_dataset = val_dataset.map(lambda x: load_image(x))\r\nval_dataset = val_dataset.batch(16)\r\n\r\ntest_dataset = tf.data.Dataset.list_files(image_path + 'test_imgs/*.png')\r\ntest_dataset = test_dataset.shuffle(200)\r\ntest_dataset = test_dataset.map(lambda x: load_image(x))\r\ntest_dataset = test_dataset.batch(16)\r\n\r\n# train/test dataset batch shape: (16, 256, 256, 1)\r\n\r\nclass Downsample(tf.keras.Model):\r\n    def __init__(self, filters, size, apply_batchnorm=True):\r\n        super(Downsample, self).__init__()\r\n        self.apply_batchnorm = apply_batchnorm\r\n        \r\n        initializer = tf.random_normal_initializer(0., 0.02)\r\n        \r\n        self.conv1 = tf.keras.layers.Conv2D(filters, \r\n                                            (size, size), \r\n                                            strides=2, \r\n                                            padding='same', \r\n                                            kernel_initializer=initializer)\r\n        if self.apply_batchnorm:\r\n            self.batchnorm = tf.keras.layers.BatchNormalization()\r\n    \r\n    def call(self, x, training):\r\n        x = self.conv1(x)\r\n        if self.apply_batchnorm:\r\n            x = self.batchnorm(x, training=training)\r\n        x = tf.nn.leaky_relu(x)\r\n        return x\r\n\r\nclass Upsample(tf.keras.Model):\r\n    def __init__(self, filters, size, apply_dropout=False):\r\n        super(Upsample, self).__init__()\r\n        self.apply_dropout = apply_dropout\r\n        initializer = tf.random_normal_initializer(0., 0.02)\r\n        \r\n        self.up_conv = tf.keras.layers.Conv2DTranspose(filters,\r\n                                                      (size, size),\r\n                                                      strides=2,\r\n                                                      padding='same',\r\n                                                      kernel_initializer=initializer,\r\n                                                      use_bias=False)\r\n        self.batchnorm = tf.keras.layers.BatchNormalization()\r\n        if self.apply_dropout:\r\n            self.dropout = tf.keras.layers.Dropout(0.5)\r\n    \r\n    def call(self, x, training):\r\n        x = self.up_conv(x)\r\n        x = self.batchnorm(x, training=training)\r\n        if self.apply_dropout:\r\n            x = self.dropout(x, training=training)\r\n        x = tf.nn.relu(x)\r\n        return x\r\n\r\nclass Network(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Network, self).__init__()\r\n        initializer = tf.random_normal_initializer(0., 0.02)\r\n        \r\n        self.down1 = Downsample(64, 4, apply_batchnorm=False)\r\n        self.down2 = Downsample(128, 4)\r\n        self.down3 = Downsample(256, 4)\r\n        self.down4 = Downsample(512, 4)\r\n        self.down5 = Downsample(512, 4)\r\n        self.down6 = Downsample(512, 4)\r\n        self.down7 = Downsample(512, 4)\r\n        self.down8 = Downsample(512, 4)\r\n        \r\n        self.up1 = Upsample(512, 4, apply_dropout=True)\r\n        self.up2 = Upsample(512, 4, apply_dropout=True)\r\n        self.up3 = Upsample(512, 4, apply_dropout=True)\r\n        self.up4 = Upsample(512, 4)\r\n        self.up5 = Upsample(256, 4)\r\n        self.up6 = Upsample(128 ,4)\r\n        self.up7 = Upsample(64, 4)\r\n        \r\n        self.last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS,\r\n                                                   (4, 4),\r\n                                                   strides=2,\r\n                                                   padding='same',\r\n                                                   kernel_initializer=initializer)\r\n        \r\n        @tf.contrib.eager.defun\r\n        def call(self, x, training):\r\n            # x shape == (bs, 256, 256, 3)    \r\n            x1 = self.down1(x, training=training) # (bs, 128, 128, 64)\r\n            x2 = self.down2(x1, training=training) # (bs, 64, 64, 128)\r\n            x3 = self.down3(x2, training=training) # (bs, 32, 32, 256)\r\n            x4 = self.down4(x3, training=training) # (bs, 16, 16, 512)\r\n            x5 = self.down5(x4, training=training) # (bs, 8, 8, 512)\r\n            x6 = self.down6(x5, training=training) # (bs, 4, 4, 512)\r\n            x7 = self.down7(x6, training=training) # (bs, 2, 2, 512)\r\n            x8 = self.down8(x7, training=training) # (bs, 1, 1, 512)\r\n\r\n            x9 = self.up1(x8, x7, training=training) # (bs, 2, 2, 1024)\r\n            x10 = self.up2(x9, x6, training=training) # (bs, 4, 4, 1024)\r\n            x11 = self.up3(x10, x5, training=training) # (bs, 8, 8, 1024)\r\n            x12 = self.up4(x11, x4, training=training) # (bs, 16, 16, 1024)\r\n            x13 = self.up5(x12, x3, training=training) # (bs, 32, 32, 512)\r\n            x14 = self.up6(x13, x2, training=training) # (bs, 64, 64, 256)\r\n            x15 = self.up7(x14, x1, training=training) # (bs, 128, 128, 128)\r\n\r\n            x16 = self.last(x15) # (bs, 256, 256, 3)\r\n            x16 = tf.nn.tanh(x16)\r\n            \r\n            return x16\r\n\r\ndef train(dataset, epochs):\r\n    for epoch in range(epochs):\r\n        start = time.time()\r\n        \r\n        for input_image in dataset:\r\n#             print(input_image.shape)\r\n            with tf.GradientTape() as net_tape:\r\n                output = net(input_image, training=True)\r\n                loss = net_loss(input_image, output)\r\n                \r\n            net_gradients = net_tape.gradient(loss, net.variables)\r\n            optimizer.apply_gradients(zip(net_gradients, net.variables))\r\n        \r\n        if epoch % 1 == 0:\r\n            clear_output(wait=True)\r\n            for inp, tar in test_dataset.take(1):\r\n                generate_images(net, inp, tar)\r\n        \r\n        # saving checkpoint evey 20 epochs\r\n        if (epoch + 1) % 20 == 0:\r\n            checkpoint.save(file_prefix=checkpoint_prefix)\r\n        \r\n        print('Time taken for epoch {} is {} sec\\n'.format(epoch+1, time.time()-start))\r\n\r\ntrain(train_dataset, epochs)`\r\n\r\nNow I got a Error:\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-36-d0e3ab41d250> in <module>()\r\n----> 1 train(train_dataset, epochs)\r\n\r\n<ipython-input-35-58c1c7be4acb> in train(dataset, epochs)\r\n      6 #             print(input_image.shape)\r\n      7             with tf.GradientTape() as net_tape:\r\n----> 8                 output = net(input_image, training=True)\r\n      9                 loss = net_loss(input_image, output)\r\n     10 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    701 \r\n    702       if not in_deferred_mode:\r\n--> 703         outputs = self.call(inputs, *args, **kwargs)\r\n    704         if outputs is None:\r\n    705           raise ValueError('A layer\\'s `call` method should return a Tensor '\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py in call(self, inputs, training, mask)\r\n    718     outputs, _ = self._run_internal_graph(inputs,\r\n    719                                           training=training,\r\n--> 720                                           mask=masks)\r\n    721     return outputs\r\n    722 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)\r\n    853     # does not return a list the same size as `call`\r\n    854     tensor_map = {}\r\n--> 855     for x, y, mask in zip(self.inputs, inputs, masks):\r\n    856       tensor_map[str(id(x))] = (y, mask)\r\n    857 \r\n\r\nTypeError: zip argument #1 must support iteration\r\n\r\n\r\ninput_image Tensor shape is (16, 256, 256, 1)", "comments": ["Tensorflow : 1.9", "As far as I can tell, the `tf.data` part of your program is working correctly, and something is failing inside Keras. At a guess, some part of the internal code is expecting multiple inputs, but it's raising an error when it only receives one input. I'm not sure if that's a potential user error or a bug, so forwarding this to @fchollet for triage.", "Can you clarify what version of TF you are using? If 1.9, please upgrade to a more recent version, and see if the issue still occurs. If you are interested in using eager execution, I would encourage you to use the [TensorFlow 2.0 Beta](https://www.tensorflow.org/beta).", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26487\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26487\">No</a>\n"]}, {"number": 26486, "title": "tf_upgrade_v2  fails if the file contains f-strings and gives pasta.base.annotate.AnnotationError", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0-alpha0\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\n`tf_upgrade_v2` fails if the file contains f-strings\r\n\r\n**Describe the expected behavior**\r\n`tf_upgrade_v2` does not fails if the file contains f-strings\r\n\r\n**Code to reproduce the issue**\r\nFile `foo.py`:\r\n```\r\nprint(f'tf_upgrade_v2 fails to convert f-strings, like this one: {42}')\r\n```\r\nCommand that produces the error: `tf_upgrade_v2 --infile foo.py --outfile foo_tf20.py`\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 1161, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 127, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 47, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 1213, in visit_Num\r\n    self.attr(node, 'content', contentargs, deps=('n',), default=str(node.n))\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 1352, in attr\r\n    attr_parts.append(attr_val())\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 1210, in <lambda>\r\n    contentargs = [lambda: self.tokens.next_of_type(token_number_type).src]\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/token_generator.py\", line 347, in next_of_type\r\n    self.lines[token.start[0] - 1]))\r\nValueError: Expected 'NUMBER' but found ')'\r\nline 1: print(f'tf_upgrade_v2 fails to convert f-strings, like this one: {42}')\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/anaconda3/envs/tf2.0/bin/tf_upgrade_v2\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/tools/compatibility/tf_upgrade_v2_main.py\", line 110, in main\r\n    args.input_file, output_file, upgrade)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/tools/compatibility/tf_upgrade_v2_main.py\", line 33, in process_file\r\n    upgrader.process_file(in_filename, out_filename)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/tools/compatibility/ast_edits.py\", line 494, in process_file\r\n    temp_file)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/tools/compatibility/ast_edits.py\", line 548, in process_opened_file\r\n    self.update_string_pasta(\"\".join(lines), in_filename))\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/tools/compatibility/ast_edits.py\", line 510, in update_string_pasta\r\n    t = pasta.parse(text)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/__init__.py\", line 25, in parse\r\n    annotator.visit(t)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 1161, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 127, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 47, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 211, in visit_Module\r\n    self.generic_visit(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 261, in generic_visit\r\n    self.visit(item)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 1161, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 127, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 47, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 574, in visit_Expr\r\n    self.visit(node.value)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 1161, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 127, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 47, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 703, in visit_Call\r\n    any_args = self.visit_Call_arguments35(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 770, in visit_Call_arguments35\r\n    self.visit(arg)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 1161, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 127, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 261, in generic_visit\r\n    self.visit(item)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 1161, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 127, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 263, in generic_visit\r\n    self.visit(value)\r\n  File \"/anaconda3/envs/tf2.0/lib/python3.6/site-packages/pasta/base/annotate.py\", line 1163, in visit\r\n    raise AnnotationError(e)\r\npasta.base.annotate.AnnotationError: Expected 'NUMBER' but found ')'\r\nline 1: print(f'tf_upgrade_v2 fails to convert f-strings, like this one: {42}')\r\n```", "comments": ["I'm having exactly the same problem when trying to migrate to tensorflow==2.0.0-alpha0 using the `tf_upgrade_v2` script.", "I've encountered the same bug. A test file:\r\n\r\n#! /usr/bin/env /usr/bin/python3\r\n  \r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\nprint(f\"tf.executing_eagerly() = {tf.executing_eagerly()}\")\r\n\r\nI've attached the Traceback.\r\n[tf2_fstring_traceback.txt](https://github.com/tensorflow/tensorflow/files/2989900/tf2_fstring_traceback.txt)\r\n\r\n", "This is an issue in google_pasta, a library we use to write that script in. I've escalated it to the maintainers and will ping back here soon.", "Sorry for the trouble here, this will be addressed as part of google/pasta#58.", "The fstring failures should be fixed as of pasta `0.1.5` ([pasta#59](https://github.com/google/pasta/issues/59) + [pasta#61](https://github.com/google/pasta/issues/61)).", "same here, waiting for this to upgraded, so I can upgrade it as well for http://tf2up.ml/", "@soupytwist Should this problem be fixed now? I am using google_pasta 0.1.6 now and am still experiencing this issue.", "@kriskorrel-cw yes, it's fixed in nightly builds for quite some time", "I think we can also close the issue", "I have the following (minimal) code example that tf_upgrade_v2 can't pass.\r\n```\r\nimport os\r\nimport Tensorflow\r\npath = \"/a/b/c/d/e\"\r\nc = f'/{(os.sep).join(path.split(os.sep)[2:])}'\r\n```\r\n\r\nI believe I have the correct pasta version:\r\n```\r\n$ pip3 install google_pasta\r\n\r\nCollecting google_pasta\r\n  Using cached https://files.pythonhosted.org/packages/f9/68/a14620bfb042691f532dcde8576ff82ee82e4c003cdc0a3dbee5f289cee6/google_pasta-0.1.6-py3-none-any.whl\r\nInstalling collected packages: google-pasta\r\nSuccessfully installed google-pasta-0.1.6\r\n```\r\n\r\nDo you also experience this error @lc0 ? In that case, I don't think it can be closed", "@kriskorrel-cw I think the truth is somewhere in the middle. It does work for simpler cases like \r\n\r\n```python\r\nfoo = 'bar'\r\na, b = 1, 2\r\nprint(f\"{foo} : {a+b}\")\r\n```\r\n\r\nBut does fail for your case. For more examples please see the colab example below \r\n\r\nhttps://colab.research.google.com/drive/1_hWyz2x_SwN0R0Jo1G0OCBeybmPkLlzC#scrollTo=a9l0KRPYu0JK\r\n\r\nI think in this case would make sense to reference this case for `pasta`", "Thank you for the test case to reproduce this. Revisiting this now.", "It should be [fixed](https://github.com/google/pasta/commit/f26107d1f27848ab67387c7f4697de2393b3e4d9) in `0.1.7`.", "I just tested with todays nightly and the issue is resolved - I can convert the file that failed before. Thanks @soupytwist!\r\n\r\nI think if @MaximilianProll does not have any objections we can close this issue\r\n\r\ncc @alextp ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26486\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26486\">No</a>\n", "I received the `pasta.base.annotate.AnnotationError` but I used `str.format()` instead of f-string. What should I do? \r\n\r\nThe detailed error is:\r\n```\r\npasta.base.annotate.AnnotationError: Expected '_flatten' but found 'yield'\r\nline 119:                 yield from _flatten(tensor)\r\n```\r\nThe current code was written in TF-1.14. `google-pasta` version is 0.1.8.\r\n\r\n**update**: I fixed this by removing all code that includes `yield`...", "Hi @richardwth,\r\nThanks for reporting that. The error you posted isn't what I would expect, but it did reveal that the `yield from <expression>` syntax is unsupported in pasta. Fixed this in [43cc7c6](https://github.com/google/pasta/commit/43cc7c64125dd56d3019bbc598fa9394b192e5bf).", "I have tried to upgrade to tf2 : \r\n`tf_upgrade_v2 --intree tf2/ --outtree tf2/upgraded/ --reportfile report.txt` \r\nwhere tf2 is the directory with all my codes ready for upgrade and tf2/upgraded/ the subdirectory where I'd like to get the update codes.\r\nI also tried on single .py files with \r\n`tf_upgrade_v2 --intree tf2/util_model.py --outtree tf2/upgraded/util_model.py --reportfile report_utile_model.txt`\r\n\r\nI am working with with Windows 10 - anaconda - spyder 4.1.5 - python 3.7 and\r\ngoogle-pasta 0.2.0 then 0.1.5 then 0.1.7 then 0.1.8.... I always get the same message \r\n```Traceback (most recent call last):\r\n  File \"D:\\Anaconda3\\Scripts\\tf_upgrade_v2-script.py\", line 6, in <module>\r\n    from tensorflow.tools.compatibility.tf_upgrade_v2_main import main\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\tools\\compatibility\\tf_upgrade_v2_main.py\", line 24, in <module>\r\n    from tensorflow.tools.compatibility import ast_edits\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\tools\\compatibility\\ast_edits.py\", line 30, in <module>\r\n    import pasta\r\nModuleNotFoundError: No module named 'pasta'\r\n```\r\nI don't know what to do anymore."]}, {"number": 26484, "title": "Android / TFlite call results in a NPE", "body": "**System information**\r\n\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**\r\nLinux Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary):**\r\nSource\r\n- **TensorFlow version training (or github SHA if from source):**\r\n1.11.0 + GPU support\r\n- **custom model**\r\nbased on ssdlite_mobilenet_v2_coco (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)\r\n- **TensorFlow version convert (or github SHA if from source):**\r\ntried with 1.11 and 1.12\r\n\r\nwe are trying to do object detection in an android app. To do this we used a ssdlite_mobilenet_v2_coco pretrained network and continued training our own dataset.\r\n\r\nWe created the tflite model using these scripts:\r\n\r\n**tflite_convert**\r\n\r\n```\r\npython3 ~/tensorflow/models/research/object_detection/export_tflite_ssd_graph.py \\\r\n--pipeline_config_path input/ssdlite_mobilenet_v2_coco/pipeline.config \\\r\n--trained_checkpoint_prefix input/ssdlite_mobilenet_v2_coco/model.ckpt-381700 \\\r\n--output_directory output/ \\\r\n--add_postprocessing_op=true\r\n\r\ntflite_convert \\\r\n--output_file=output/ssdlite_mobilenet_v2_coco.tflite \\\r\n--graph_def_file=input/ssdlite_mobilenet_v2_coco.pb \\\r\n--input_arrays=FLOAT \\\r\n--output_arrays=concat,concat_1 \\\r\n--input_shape=1,300,300,3\r\n```\r\nWhat this app basically does, it takes a pre-recorded video, decodes it frame-by-frame using FFmpegMediaMetadataRetriever and passes the bitmap into tflite to detect objects there. The app is built with gradle and we are using 'org.tensorflow:tensorflow-lite:1.12.0', but we basically get the same error with 1.11.\r\n\r\nWe scale the bitmap down to 300x300 and convert it from ARGB into 3 float channels and call tflite like this:\r\n\r\n```\r\nog.v(TAG, \"Feeding TFLite\")\r\noutputLocations = Array(1) { Array(NUM_DETECTIONS) { FloatArray(4) } }\r\noutputClasses = Array(1) { FloatArray(NUM_DETECTIONS) }\r\noutputScores = Array(1) {FloatArray(NUM_DETECTIONS)}\r\nnumDetections = FloatArray(1)\r\n\r\nval inputArray = arrayOf<Any>(imgData!!)\r\nval outputMap = HashMap<Int, Any>()\r\noutputMap.put(0, outputLocations!!)\r\noutputMap.put(1, outputClasses!!)\r\noutputMap.put(2, outputScores!!)\r\noutputMap.put(3, numDetections!!)\r\n\r\n\r\nLog.v(TAG, \"Running TFLite\")\r\ntflite!!.runForMultipleInputsOutputs(inputArray, outputMap)\r\nLog.v(TAG, \"Returning from TFLite\")\r\n\r\nval recognitions = ArrayList<Recognition>(NUM_DETECTIONS)\r\n```\r\nThe error we are getting is:\r\n\r\n```\r\n2019-02-28 11:52:33.486 26807-26879/com.package.xxxxxx A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0 in tid 26879 (.xxxxxx), pid 26807 (.xxxxxx)\r\n2019-02-28 11:52:33.600 26890-26890/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n2019-02-28 11:52:33.601 26890-26890/? A/DEBUG: Build fingerprint: 'google/sdk_gphone_x86/generic_x86:9/PSR1.180720.075/5124027:userdebug/dev-keys'\r\n2019-02-28 11:52:33.601 26890-26890/? A/DEBUG: Revision: '0'\r\n2019-02-28 11:52:33.601 26890-26890/? A/DEBUG: ABI: 'x86'\r\n2019-02-28 11:52:33.604 26890-26890/? A/DEBUG: pid: 26807, tid: 26879, name: .xxxxxx  >>> com.package.xxxxxx <<<\r\n2019-02-28 11:52:33.604 26890-26890/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0\r\n2019-02-28 11:52:33.604 26890-26890/? A/DEBUG: Cause: null pointer dereference\r\n2019-02-28 11:52:33.604 26890-26890/? A/DEBUG:     eax 00000000  ebx 00000000  ecx 00000000  edx 00000000\r\n2019-02-28 11:52:33.604 26890-26890/? A/DEBUG:     edi c75094a8  esi 00000000\r\n2019-02-28 11:52:33.604 26890-26890/? A/DEBUG:     ebp c75090f8  esp c7509070  eip c757c77b\r\n2019-02-28 11:52:33.606 26890-26890/? A/DEBUG: backtrace:\r\n2019-02-28 11:52:33.606 26890-26890/? A/DEBUG:     #00 pc 0007277b  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so\r\n2019-02-28 11:52:33.606 26890-26890/? A/DEBUG:     #01 pc 00074fe0  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so\r\n2019-02-28 11:52:33.606 26890-26890/? A/DEBUG:     #02 pc 0007590d  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so\r\n2019-02-28 11:52:33.606 26890-26890/? A/DEBUG:     #03 pc 000755b0  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so\r\n2019-02-28 11:52:33.606 26890-26890/? A/DEBUG:     #04 pc 00076322  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so\r\n2019-02-28 11:52:33.606 26890-26890/? A/DEBUG:     #05 pc 0013389c  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so\r\n2019-02-28 11:52:33.607 26890-26890/? A/DEBUG:     #06 pc 00132fa7  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so\r\n2019-02-28 11:52:33.607 26890-26890/? A/DEBUG:     #07 pc 00132e37  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so\r\n2019-02-28 11:52:33.607 26890-26890/? A/DEBUG:     #08 pc 0016550e  /data/app/com.package.xxxxxx-yGil4a0ylttBBhPIo26SWA==/lib/x86/libtensorflowlite_jni.so\r\n2019-02-28 11:52:33.607 26890-26890/? A/DEBUG:     #09 pc 0008f065  /system/lib/libc.so (__pthread_start(void*)+53)\r\n2019-02-28 11:52:33.607 26890-26890/? A/DEBUG:     #10 pc 0002485b  /system/lib/libc.so (__start_thread+75)\r\n2019-02-28 11:52:34.197 1761-1761/? E//system/bin/tombstoned: Tombstone written to: /data/tombstones/tombstone_35\r\n```\r\nAs you can see the NPE occurs deep in the libtensorflow and we are basically running out of ideas what we could do to fix it, so any help is appreciated. It happens on both a physical device and the android sandbox (API 28)\r\n\r\nWe used https://github.com/baxterai/tfliteSSDminimalWorkingExample/blob/master/android/tfliteSSDminimalWorkingExample/app/src/main/java/com/example/user/tflitessdminimalworkingexample/TFLiteObjectDetectionAPIModel.java as a starting point and also the Tensorflow tflite demo form the tensorflow repository.\r\n", "comments": ["1. This error is probably caused by the fact that the input array is not associated with the output, you probably made a mistake when converting the model by entering the parameter \"--input_arrays=FLOAT\" here you should write name array, not type. Maybe it should be \"--input_arrays=image_tensor\"\r\n2. And when converting the full model, you cut it to the node \"concat\", \"concat_1\", but use nodes after post processing and they are not in the model.  \r\nFor convert .pb to .tflite use script: \r\n```\r\ntflite_convert \\\r\n--graph_def_file=input/ssdlite_mobilenet_v2_coco.pb \\\r\n--output_file=output/ssdlite_mobilenet_v2_coco.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--std_dev_values=128 \\\r\n--mean_values=128 \\\r\n--input_arrays=image_tensor \\\r\n--output_arrays='detection_boxes','detection_classes','detection_scores','num_detections' \\\r\n--allow_custom_ops\r\n```", "Hey,\r\nI am a colleague of nmicieli.\r\nThanks so much for your input. \r\n\r\nWe tried your script and unfortunately there are just 2 small things that do not fit fully:\r\nNaming of input-arrays and output-arrays. This is how we managed to actually succeed in passing the tflite call without the NullPointer:\r\n\r\n```\r\n tflite_convert \\ \r\n--graph_def_file=tflite_graph.pb \\ \r\n--output_file=output.tflite \\ \r\n--input_shapes=1,300,300,3 \\ \r\n--std_dev_values=128 \\ \r\n--mean_values=128 \\ \r\n--input_arrays=normalized_input_image_tensor \\ \r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\ \r\n--allow_custom_ops \r\n```\r\n", "I didn\u2019t know what the graph looked like **nmichel**, so I assumed possible input / output nodes. I hope your problem is solved. ", "I assume tflite assumes these values as default?", "The names of the Input / Output arrays are assigned in the graph architecture. You are using SSDLite Mobilenet V2 from the Tensorflow Object Detection API with post-processing, your graphics will have the correct output arrays that you specified.", "Do you by chance have a link to the converted .tflite model?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26484\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26484\">No</a>\n"]}, {"number": 26483, "title": "Fix freeze_graph_test with correct parameters", "body": "This will fix _testFreezeGraph to call freeze_graph() with correct parameters\r\n- input_meta_graph is passed in position of variable_names_blacklist that should be empty string\r\n\r\nAlso remove input_meta_graph that is not used with this fix", "comments": []}, {"number": 26482, "title": "ImportError: cannot import name 'Activation'", "body": "Hi,\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo. Just importing.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nManjaro\r\n- TensorFlow installed from (source or binary):\r\nvia pip install tensorflow\r\n- TensorFlow version (use command below):\r\n1.13.1\r\n- Python version:\r\n3.6\r\n\r\n**Describe the current behavior**\r\n\r\nwith tensorflow-1.13.1 one cannot import keras.Model and keras.layer with\r\n\r\nfrom tensorflow.keras import Model\r\n\r\n-> this gives the following error:\r\n\r\n    from tensorflow.python.keras.applications.densenet import Activation; ImportError: cannot import name 'Activation'\r\n\r\n**Describe the expected behavior**\r\nExpected that I can do the following import:\r\nfrom tensorflow.keras import Model\r\n\r\n\r\n**Other info / logs**\r\n\r\nHowever, importing with\r\n\r\nfrom tensorflow import keras \r\n\r\nand then use keras.Model or keras.layer works fine", "comments": ["I was able to execute ``` from tensorflow.keras import Model``` successfully in TF 1.13.1\r\nCan you please try updating your keras version?"]}, {"number": 26481, "title": "Tensorflow error, sess.run(); Iris dataset for logistics regression", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Windows 10:\r\n- TensorFlow installed from conda-forge:\r\n- TensorFlow version 1.5.0:\r\n- Python version 3.6\r\n\r\n\r\nI am trying to perform logistic regression on the iris dataset that I got from UCL, i am using tensorflow and i feel it is giving me the error in the feed_dict part of the error line. I tried changing the shape of the the x_input and y_input according to the error that occurred using np.reshape() but it didn't help.\r\n\r\n\r\n```\r\n`#training data\r\nx_input=data.loc[0:105, ['SEPAL LENGTH','SEPAL WIDTH','PETAL LENGTH','PETAL WIDTH']]\r\ntemp=data['FLOWER']\r\ny_input=temp[0:106]\r\n#test data\r\nx_test=data.loc[106:149, ['SEPAL LENGTH','SEPAL WIDTH','PETAL LENGTH','PETAL WIDTH']]\r\ny_test=temp[106:150]\r\n\r\n#placeholders and variables. input has 4 features and output has 3 classes\r\nx=tf.placeholder(tf.float32,shape=[None, 4])\r\ny_=tf.placeholder(tf.float32,shape=[None, 3])\r\n#weight and bias\r\nW=tf.Variable(tf.zeros([4,3]))\r\nb=tf.Variable(tf.zeros([3]))\r\n\r\n\r\n# X is placeholdre for iris features. We will feed data later on\r\nX = tf.placeholder(tf.float32, [None, 4])\r\n# y is placeholder for iris labels. We will feed data later on\r\nY = tf.placeholder(tf.float32, [3, None])\r\n\r\nw = tf.Variable(tf.zeros([4,3]))\r\nb = tf.Variable(tf.zeros([3]))\r\n\r\n\r\n# model \r\n#softmax function for multiclass classification\r\ny = tf.nn.softmax(tf.matmul(x, W) + b)\r\n#loss function\r\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\r\n#optimiser -\r\ntrain_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)\r\n#calculating accuracy of our model \r\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\r\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n#session parameters\r\nsess = tf.InteractiveSession()\r\n#initialising variables\r\ninit = tf.initialize_all_variables()\r\nsess.run(init)\r\n#number of interations\r\nepochs=2000\r\n\r\nfor step in range(epochs):\r\n           _, c=sess.run([train_step,cross_entropy], feed_dict={x: x_input.as_matrix(), y_:y_input.as_matrix()}) #error line\r\n           if step%500==0:\r\n               print(c)`\r\n```\r\n\r\nError stack looks like this\r\n\r\n```\r\n`ValueError                                Traceback (most recent call last)\r\n<ipython-input-13-aa781a046f65> in <module>\r\n      1 for step in range(epochs):\r\n----> 2            _, c=sess.run([train_step,cross_entropy], feed_dict={x: x_input.as_matrix(), y_:y_input.as_matrix()})\r\n      3            if step%500==0:\r\n      4                print(c)\r\n\r\nc:\\users\\hp\\anaconda3\\envs\\maskrcnn\\lib\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    893     try:\r\n    894       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 895                          run_metadata_ptr)\r\n    896       if run_metadata:\r\n    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nc:\\users\\hp\\anaconda3\\envs\\maskrcnn\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1102                 'Cannot feed value of shape %r for Tensor %r, '\r\n   1103                 'which has shape %r'\r\n-> 1104                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\r\n   1105           if not self.graph.is_feedable(subfeed_t):\r\n   1106             raise ValueError('Tensor %s may not be fed.' % subfeed_t)\r\n\r\nValueError: Cannot feed value of shape (106,) for Tensor 'Placeholder_1:0', which has shape '(?, 3)'`\r\n\r\n\r\n```", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "I have the same error!! Help Me Please"]}, {"number": 26480, "title": "Where is gen_experimental_dataset_ops?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.0.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nfrom tensorflow.python.ops import gen_experimental_dataset_ops\r\n```\r\nin `tensorflow/python/data/experimental/ops/batching.py`\r\nIs it the `gen_experimental_dataset_ops` disappeared? I can not find where the `gen_experimental_dataset_ops` is defined.\r\n", "comments": ["It's a generated file. The easiest way to see its contents might be to download and install a PIP package, and look at the library source."]}, {"number": 26479, "title": "[TF2.0] Histogram summary unexpected keyword 'values' in summary_v2.py", "body": "\r\n**System information**\r\n- OS Platform and Distribution: Linux 18.04\r\n- TensorFlow version 2.0-alpha\r\n- Python 3.6.8\r\n\r\n**Describe the current behavior**\r\n\r\nTrying to write any tensor as histogram summary:\r\n\r\n> /tmp/cpu-env/lib/python3.6/site-packages/tensorboard/plugins/histogram/summary_v2.py in _buckets(data, bucket_count)\r\n     89   if bucket_count is None:\r\n     90     bucket_count = DEFAULT_BUCKET_COUNT\r\n---> 91   with tf.name_scope('buckets', values=[data, bucket_count]):\r\n     92     tf.debugging.assert_scalar(bucket_count)\r\n     93     tf.debugging.assert_type(bucket_count, tf.int32)\r\nTypeError: __init__() got an unexpected keyword argument 'values'\r\n\r\n**Code to reproduce the issue**\r\n`tf.summary.histogram('SampleKernel', tf.random.normal(shape=[3,3,3,32], dtype=tf.float32))`\r\n", "comments": ["@Uture Could you provide a code to reproduce the bug? Thanks!", "> @Uture Could you provide a code to reproduce the bug? Thanks!\r\n\r\nIt's right there in the last point of the issue. This line suffices for me to get the error (besides tensorflow import).", "Duplicate of https://github.com/tensorflow/tensorboard/issues/1993\r\n\r\nThis is fixed in `tb-nightly`, which you can try out via `pip install --upgrade tb-nightly`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26479\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26479\">No</a>\n"]}, {"number": 26478, "title": "Met unsupported operator of type Cast when quantize model get from quantize aware training", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes, i tried to use quantize aware training, post training quantization and tflite converter to train and convert a custom mobilenetv2 model\r\n- OS Platform and Distribution :\r\nmacos 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNone\r\n- TensorFlow installed from (source or binary):\r\nconda install tensorflow\r\n- TensorFlow version (use command below):\r\ntensorflow 1.13\r\n- Python version:\r\npython 3.6.8\r\n- Bazel version (if compiling from source):\r\nnone\r\n- GCC/Compiler version (if compiling from source):\r\nnone\r\n- CUDA/cuDNN version:\r\nnone\r\n- GPU model and memory:\r\nnone\r\n\r\n\r\ni was training a mobilenetv2 model, using quantize aware training defined in tf.contrib.quantize, following the direction in [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize](url)\r\n\r\n tflite convert was used to convert the quantize aware trained model to 8bit fixed point. everything was fine until i saved frozen graph. i then use toco command line tool, and got this error massage:\r\n\"2019-03-08 16:13:24.208096: F ./tensorflow/lite/toco/toco_tooling.h:38] Check failed: s.ok() Unimplemented: this graph contains an operator of type Cast for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\nFatal Python error: Aborted\"\r\n\r\ni also tried tflite converter python api, convert from session, convert from frozen graph and convert from saved models, and all got the same error message. by printing out ops in the gragh, i found some \"Cast\" ops in the graph, in batch norm layers mostly. i guess these cast operators were used building training graphs and folding batch norms, and should be deleted when building eval graph or freezing the graph. but i still get them in my frozen graph, so i would like to ask for some help to get rid of them (and other unsupported ops) and convert the model to 8bit\r\n\r\nthe nodes in graph_def is saved here [https://www.dropbox.com/s/mev825dzrzh7hrv/log.txt?dl=0](url) \r\nsaved frozen graph is available here [https://www.dropbox.com/s/8xbropujwwxm5tb/frozen_graph.pb?dl=0](url)\r\n\r\n\r\n\r\n", "comments": ["hello, have you solved this problem of cast op?", "> hello, have you solved this problem of cast op?\r\n\r\nThe contrib.quantize is no longer maintained. Please try the new keras based quantization aware training [API](https://www.tensorflow.org/model_optimization/guide/quantization/training_example) and let us know if you run into any issues. Thanks!", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26478\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26478\">No</a>\n"]}, {"number": 26477, "title": "ImportError", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: 1.13\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.5\r\n- GPU model and memory: Nvidia GeForce 1060 6GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen I run my code, I get the error included here.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n**Any other info / logs**\r\nException has occurred: ImportError\r\nTraceback (most recent call last):   File \"C:\\Users\\Jonny\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>     from tensorflow.python.pywrap_tensorflow_internal import *   File \"C:\\Users\\Jonny\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>     _pywrap_tensorflow_internal = swig_import_helper()   File \"C:\\Users\\Jonny\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)   File \"C:\\Users\\Jonny\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module     return load_dynamic(name, filename, file)   File \"C:\\Users\\Jonny\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic     return _load(spec) ImportError: DLL load failed: The specified module could not be found.   Failed to load the native TensorFlow runtime.  See https://www.tensorflow.org/install/errors  for some common reasons and solutions.  Include the entire stack trace above this error message when asking for help.\r\n", "comments": ["I solved it by using 10.0 instead of 10.1. Duh!", "I encountered this problem today, please help.\r\n\r\nImportError                               Traceback (most recent call last)\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-0b6b89de8d85> in <module>()\r\n     18 ##The next set of Libraries will be used for Deep Learning\r\n     19 \r\n---> 20 import keras\r\n     21 from keras.layers import Dense\r\n     22 from keras.models import Sequential\r\n\r\n~\\anaconda3\\lib\\site-packages\\keras\\__init__.py in <module>()\r\n      1 from __future__ import absolute_import\r\n      2 \r\n----> 3 from . import utils\r\n      4 from . import activations\r\n      5 from . import applications\r\n\r\n~\\anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py in <module>()\r\n      4 from . import data_utils\r\n      5 from . import io_utils\r\n----> 6 from . import conv_utils\r\n      7 \r\n      8 # Globally-importable utils.\r\n\r\n~\\anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py in <module>()\r\n      7 from six.moves import range\r\n      8 import numpy as np\r\n----> 9 from .. import backend as K\r\n     10 \r\n     11 \r\n\r\n~\\anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py in <module>()\r\n     87 elif _BACKEND == 'tensorflow':\r\n     88     sys.stderr.write('Using TensorFlow backend.\\n')\r\n---> 89     from .tensorflow_backend import *\r\n     90 else:\r\n     91     # Try and load external backend.\r\n\r\n~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in <module>()\r\n      3 from __future__ import print_function\r\n      4 \r\n----> 5 import tensorflow as tf\r\n      6 from tensorflow.python.framework import ops as tf_ops\r\n      7 from tensorflow.python.training import moving_averages\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 try:\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 from tensorflow.python.tools import component_api_helper\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\no270\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\no270\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\no270\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\no270\\anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\no270\\anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help."]}, {"number": 26476, "title": "Code cifar10 uses CPU instead of GPU", "body": "`\r\nimport time\r\ntime.sleep(1.5)\r\nfrom keras.datasets import cifar10\r\nfrom matplotlib import pyplot\r\nfrom scipy.misc import toimage\r\n\r\nimport numpy\r\nfrom keras.datasets import cifar10\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.layers import Dropout\r\nfrom keras.layers import Flatten\r\nfrom keras.constraints import maxnorm\r\nfrom keras.optimizers import SGD\r\nfrom keras.layers.convolutional import Conv2D\r\nfrom keras.layers.convolutional import MaxPooling2D\r\nfrom keras.utils import np_utils\r\nfrom keras import backend as K\r\nK.set_image_dim_ordering('th')\r\n\r\nseed = 7\r\nnumpy.random.seed(seed)\r\n\r\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\r\n\r\n\r\nX_train = X_train.astype('float32')\r\nX_test = X_test.astype('float32')\r\nX_train = X_train / 255.0\r\nX_test = X_test / 255.0\r\n\r\ny_train = np_utils.to_categorical(y_train)\r\ny_test = np_utils.to_categorical(y_test)\r\nnum_classes = y_test.shape[1]\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes, activation='softmax'))\r\n\r\nepochs = 25\r\nlrate = 0.01\r\ndecay = lrate/epochs\r\nsgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\r\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\nprint(model.summary())\r\n\r\n\r\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\r\n\r\nscores = model.evaluate(X_test, y_test, verbose=0)\r\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))`\r\n\r\n\r\n**OUTPUT OF CODE**\r\n\r\nUsing TensorFlow backend.\r\n2019-03-08 13:22:50.598025: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2019-03-08 13:22:50.617421: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2019-03-08 13:22:50.617482: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: linuxbox-1\r\n2019-03-08 13:22:50.617489: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: linuxbox-1\r\n2019-03-08 13:22:50.617516: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 390.116.0\r\n2019-03-08 13:22:50.617533: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 390.77.0\r\n2019-03-08 13:22:50.617539: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:308] kernel version 390.77.0 does not match DSO version 390.116.0 -- cannot find working devices in this configuration\r\n\r\n\r\ncan anyone help me with performance?@tensorflow", "comments": ["@usernamemissing Could you check running the following code to see whether TF is detecting GPU or not\r\nfrom tensorflow.python.client import device_lib \r\nprint(device_lib.list_local_devices())\r\n\r\nWhat do you see as output? Please fill the requested details in the [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Without these details it is difficult to support. Thanks!", "I got the same output (`kernel version 390.77.0 does not match DSO version 390.116.0`). When I applied the last Ubuntu updates, and I restarted the system this message disappeared.", "@usernamemissing Could you follow @grzegorz700 suggestion and let me know how it progresses. Please close the issue if it was resolved already. Thanks!", "I think it was resolved. I am closing the issue. Please open a new ticket if you see similar issue again. Thanks!"]}, {"number": 26475, "title": "Add skip_empty argument to tf.strings.split Issue #26368", "body": "There were two split functions defined in TensorFlow 1.13:\r\n\r\ntf.string_split\r\nstring_split_v2\r\nWe have consolidated these two ops into tf.strings.split in TF 2.0:\r\n\r\ntf.strings.split(\r\n    source,\r\n    sep=None,\r\n    maxsplit=-1\r\n)\r\nand wish to deprecate tf.string_split. However, tf.strings.split is missing the skip_empty functionality included in string_split:\r\n\r\ntf.string_split(\r\n    source,\r\n    delimiter=' ',\r\n    skip_empty=True\r\n)\r\nThis feature request would be to include skip_empty functionality for tf.strings.split.\r\n\r\n\r\nThe code and unit tests are running.  I am currently running End to end tests and ensuring it adheres to clean standards.", "comments": ["#26475", "#26368", "Code has been put through python and C++ coding standards. \r\nThere is pylint fix I have been dealing with but I am not sure if it is part of tensorflow's standards or a part of pylint not related to tensorflow as I am having trouble removing it E:292,27: Unexpected keyword argument 'skip_empty' in the function call (unexpected-keyword-arg).  This seems to be correct by all things I can see and it should be noted that the code runs and passes unit tests so the function, so I am not sure why it is throwing that?  \r\nOtherwise, as i mentioned testing is passing on my end.  I am continuing to look for improvements but let me know if there are any thoughts\r\n", "Changed tests and uses to have the skip empty default to true as was the case in the previous version.  Fixed linting issue", "We had a question on if the skip empty argument should default to true or falses, previous unit tests had it default to false, but the previous version defaulted to true.  We went with defaulting to true for the time being but are looking for feedback.  This would potentially change the results by removing empty results for anyone using this", "(I'm not a good reviewer for this change, unassigning.)", "@lhendre could you please re base your branch, @alextp do you want to review this change ?", "Ill re base it, as I am about to enter transit, to clarify @alextp you mean reviewing the change would be better done after re base?\r\n", "No, this is not what I meant. What I meant is that I think filtering away the empty strings is better done as a post-processing step, say with boolean_mask, instead of another flag to split.", "@alextp So to clarify I should make this a Post-processing step, my understanding then is to use a filter(maybe a boolean mask) in file python/ops/string_ops.py after the call to gen_string_ops.string_split_v2?  I am fine with this and can start looking into this.  \r\n\r\nWould you like me to bring this back up at the issue itself?  This setup was based on how the previous skip_empty so this would be a departure from previous work.  Ill start looking into it ", "I have the mask working, I am working on updating the indices and shapes since those are set during processing @alextp ", "Im having some trouble getting the indices and shape values to be correct if you have any suggestions", "@alextp I am not sure how to go about setting the indices and shapes.  The values are correct and I have removed the unit tests for indices and shapes.  They were previously set in the earlier portion of the code so I am not sure if we want to leave them at their previously values or change them(I have issues both figuring out how to change them and what to change them to).  Im leaving this as is for feedback  ", "I'm sorry I wasn't clear; I meant instead that we should let users filter out empty strings as post processing if they do want to.\r\n\r\nRe indices and shapes, the right answer is to have proper ragged and sparse tensor support to tf.boolean_mask. Can you file an issue?", "Ill Open up an issue and post about this in the current open issue at #26368, would you like to close that one?"]}]