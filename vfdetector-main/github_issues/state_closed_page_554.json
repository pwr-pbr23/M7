[{"number": 37094, "title": "I have to install TF 1.8!", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows (10 or 8.1)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:1.8\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:both\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:9.1\r\n- GPU model and memory:NVIDIA GeForce GTX 760\r\n\r\n\r\n\r\n**Describe the problem**\r\nHello, the computing capabilities of my GPU is 3.0 and TF doesn't work in this case. I've read that it's possible to run the 1.8 version here https://medium.com/@naarkie/using-tensorflow-gpu-on-a-compute-3-0-graphics-card-in-windows-4184f4228fe9\r\nBut impossible to find the good package.\r\nI need it to use the Visions of Chaos software (fractal art).\r\nMay somebody help me?\r\nThanks for all.\r\nBest regards,\r\nFran\u00e7ois\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@FB43 \r\n\r\nCan you go through the[ link ](https://www.deciphertechnic.com/install-tensorflow-gpu-18-on-windows/)and see if it helps you.Also, see tested builded configuration from [here](https://www.tensorflow.org/install/source_windows#gpu).Thanks!", "@ravikyram thanks, but in the link, there is: \"pip install --ignore-installed --upgrade tensorflow-gpupip install --ignore-installed --upgrade tensorflow-gpu\". And, of course, it will install the last tensoflow version, not the 1.8 which seems not to be available on the repository. That's my problem! If I can find the 1.8 package (source or build), it will be fine but it seems to have disappeared!\r\nAny idea to find it?\r\nThanks for your help,\r\nFran\u00e7ois\r\n", "@ravikyram hello, I'm going on but I'm stuck there, any idea to continue?\r\nThanks, Fran\u00e7ois\r\nPS C:\\windows\\system32\\tensorflow> ./bazel build --config=v1 //tensorflow/tools/pip_package:build_pip_package\r\nExtracting Bazel installation...\r\n.............\r\nWARNING: Config values are not defined in any .rc file: v1\r\nERROR: in target '//external:cc_toolchain': no such package '@local_config_cc//': Traceback (most recent call last):\r\n        File \"C:/users/fbour/appdata/local/temp/_bazel_fbour/nseddbsr/external/bazel_tools/tools/cpp/cc_configure.bzl\", line 37\r\n                configure_windows_toolchain(repository_ctx)\r\n        File \"C:/users/fbour/appdata/local/temp/_bazel_fbour/nseddbsr/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 323, in configure_windows_toolchain\r\n                _find_env_vars(repository_ctx, vc_path)\r\n        File \"C:/users/fbour/appdata/local/temp/_bazel_fbour/nseddbsr/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 194, in _find_env_vars\r\n                _find_vcvarsall_bat_script(repository_ctx, vc_path)\r\n        File \"C:/users/fbour/appdata/local/temp/_bazel_fbour/nseddbsr/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 188, in _find_vcvarsall_bat_script\r\n                auto_configure_fail((vcvarsall + \" doesn't exist, pl...\"))\r\n        File \"C:/users/fbour/appdata/local/temp/_bazel_fbour/nseddbsr/external/bazel_tools/tools/cpp/lib_cc_configure.bzl\", line 30, in auto_configure_fail\r\n                fail((\"\\n%sAuto-Configuration Error:%...)))\r\n\r\nAuto-Configuration Error: C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\VCVARSALL.BAT doesn't exist, please check your VC++ installation\r\nINFO: Elapsed time: 21,381s\r\nFAILED: Build did NOT complete successfully (3 packages loaded)\r\n", "@ravikyram VC++ seems OK. Now, it's Python and I Don't know why!!!\r\nPYTHON_BIN_PATH is configured and the path variable too.\r\nThanks for your help.\r\nPS C:\\WINDOWS\\system32\\tensorflow> bazel build --config=v1 //tensorflow/tools/pip_package:build_pip_package\r\n.\r\nWARNING: Config values are not defined in any .rc file: v1\r\nWARNING: C:/users/fbour/appdata/local/temp/_bazel_fbour/nseddbsr/external/protobuf_archive/WORKSPACE:1: Workspace name in C:/users/fbour/appdata/local/temp/_bazel_fbour/nseddbsr/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nERROR: C:/windows/system32/tensorflow/third_party/py/numpy/BUILD:11:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n        File \"C:/windows/system32/tensorflow/third_party/py/python_configure.bzl\", line 291\r\n                _create_local_python_repository(repository_ctx)\r\n        File \"C:/windows/system32/tensorflow/third_party/py/python_configure.bzl\", line 254, in _create_local_python_repository\r\n                _get_python_include(repository_ctx, python_bin)\r\n        File \"C:/windows/system32/tensorflow/third_party/py/python_configure.bzl\", line 210, in _get_python_include\r\n                _execute(repository_ctx, [python_bin, \"-c\",...())\"], <2 more arguments>)\r\n        File \"C:/windows/system32/tensorflow/third_party/py/python_configure.bzl\", line 54, in _execute\r\n                _fail(\"\\n\".join([error_msg.strip() if ... \"\"]))\r\n        File \"C:/windows/system32/tensorflow/third_party/py/python_configure.bzl\", line 27, in _fail\r\n                fail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: Problem getting python include path.\r\njava.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(239): CreateProcessW(\"C:\\Python27\" -c \"from __future__ import print_function;from distutils import sysconfig;print(sysconfig.get_python_inc())\"): Acc\u00des refus\u00da.\r\nIs the Python binary path set up right? (See ./configure or PYTHON_BIN_PATH.) Is distutils installed?\r\n and referenced by '//third_party/py/numpy:headers'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed\r\nINFO: Elapsed time: 77,898s\r\nFAILED: Build did NOT complete successfully (79 packages loaded)\r\n    currently loading: tensorflow/core ... (11 packages)\r\n    Fetching https://mirror.bazel.build/github.com/LMDB/lmdb/archive/LMDB_0.9.19.tar.gz\r\n    Fetching https://mirror.bazel.build/github.com/jemalloc/jemalloc/archive/4.4.0.tar.gz\r\n    Fetching https://mirror.bazel.build/github.com/google/re2/archive/26cd968b735e227361c9703683266f01e5df7857.tar.gz\r\n    Fetching https://mirror.bazel.build/github.com/google/snappy/archive/1.1.7.tar.gz\r\n    Fetching https://mirror.bazel.build/.../abseil/abseil-cpp/archive/720c017e30339fd1786ce4aac68bc8559736e53f.tar.gz\r\n    Fetching https://mirror.bazel.build/www.kurims.kyoto-u.ac.jp/~ooura/fft.tgz; 6,980b\r\n    Fetching https://github.com/google/highwayhash/archive/dfcb97ca4fe9277bf9dc1802dd979b071896453b.tar.gz; 21,098b\r\n    Fetching https://github.com/google/snappy/archive/1.1.7.tar.gz", "@ravikyram I'm trying other thing in the link you sent me:\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\nMaybe that's better but it doesn't works too I've checked, the file cudnn.h is really there!\r\nThanks for your help,\r\nFran\u00e7ois\r\nPS C:\\WINDOWS\\system32\\tensorflow> bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/windows/system32/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1142\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/windows/system32/tensorflow/third_party/gpus/cuda_configure.bzl\", line 995, in _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \"C:/windows/system32/tensorflow/third_party/gpus/cuda_configure.bzl\", line 750, in _get_cuda_config\r\n                _cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n        File \"C:/windows/system32/tensorflow/third_party/gpus/cuda_configure.bzl\", line 466, in _cudnn_version\r\n                find_cuda_define(repository_ctx, cudnn_header_dir, \"c...\", ...)\r\n        File \"C:/windows/system32/tensorflow/third_party/gpus/cuda_configure.bzl\", line 422, in find_cuda_define\r\n                auto_configure_fail((\"Error reading %s: %s\" % (str(h...)))\r\n        File \"C:/windows/system32/tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n                fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Error reading C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1/include/cudnn.h: java.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(239): CreateProcessW(\"grep\" --color=never -A1 -E \"#define CUDNN_MAJOR\" \"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1/include/cudnn.h\"): Le fichier sp\u00dacifi\u00da est introuvable.\r\n\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/windows/system32/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1142\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/windows/system32/tensorflow/third_party/gpus/cuda_configure.bzl\", line 995, in _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \"C:/windows/system32/tensorflow/third_party/gpus/cuda_configure.bzl\", line 750, in _get_cuda_config\r\n                _cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n        File \"C:/windows/system32/tensorflow/third_party/gpus/cuda_configure.bzl\", line 466, in _cudnn_version\r\n                find_cuda_define(repository_ctx, cudnn_header_dir, \"c...\", ...)\r\n        File \"C:/windows/system32/tensorflow/third_party/gpus/cuda_configure.bzl\", line 422, in find_cuda_define\r\n                auto_configure_fail((\"Error reading %s: %s\" % (str(h...)))\r\n        File \"C:/windows/system32/tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n                fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Error reading C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1/include/cudnn.h: java.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(239): CreateProcessW(\"grep\" --color=never -A1 -E \"#define CUDNN_MAJOR\" \"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1/include/cudnn.h\"): Le fichier sp\u00dacifi\u00da est introuvable.\r\n\r\nINFO: Elapsed time: 0,544s", "@ravikyram hello, I think the 1.8 tensorflow-gpu is correctly buid and installed with Python 2.7.\r\nas you can see:\r\nName: tensorflow-gpu\r\nVersion: 1.8.0\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: c:\\python27\\lib\\site-packages\r\nRequires: astor, tensorboard, numpy, termcolor, six, backports.weakref, absl-py, wheel, gast, protobuf, enum34, mock, grpcio\r\n\r\nNow, I have a problem in the first Python script which is testing it:\r\nCode:\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport tensorflow as tf\r\nprint(\"TF_Version {}\".format(tfSession.__version__))\r\nprint(\"Keras_Version {}\".format(tf.keras.__version__))\r\nResult:\r\ntf_version_check.py\", line 5, in <module>\r\n    print(\"TF_Version {}\".format(tfSession.__version__))\r\nNameError: name 'tfSession' is not defined\r\nMay you help me?\r\nThanks for all.\r\nBest regards,\r\nFran\u00e7ois", "@angerson Hello, things are going a little bit more but still not succesful, now I have what you can see below. Forgot the previous messages, I've solved it. Thanks for your help. Fran\u00e7ois\r\nP:\\Anaconda3\\tensorflow>bazel build --config=v1 //tensorflow/tools/pip_package:b\r\nuild_pip_package\r\nWARNING: The following rc files are no longer being read, please transfer their\r\ncontents or import their path into one of the standard rc files:\r\np:\\anaconda3\\tensorflow/tools/bazel.rc\r\nWARNING: Ignoring JAVA_HOME, because it must point to a JDK, not a JRE.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=p:/Anaconda3/python.exe\r\nINFO: Reading rc options for 'build' from p:\\anaconda3\\tensorflow\\bazel.rc:\r\n  'build' options: --define framework_shared_object=true --define=use_fast_cpp_p\r\nrotos=true --define=allow_oversize_protos=true --define=grpc_no_ares=true --spaw\r\nn_strategy=standalone --genrule_strategy=standalone -c opt\r\nINFO: Reading rc options for 'build' from p:\\anaconda3\\tensorflow\\.tf_configure.\r\nbazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=p:/Anaconda3/python.exe --action\r\n_env PYTHON_LIB_PATH=p:/Anaconda3/lib/site-packages --force_python=py3 --host_fo\r\nrce_python=py3 --python_path=p:/Anaconda3/python.exe --define with_xla_support=t\r\nrue --define with_gdr_support=true --define with_verbs_support=true --action_env\r\n TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_CUDA=1 --action_env CUDA_TOOLKIT_PAT\r\nH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1 --action_env TF_CUDA_V\r\nERSION=9.1 --action_env CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing\r\n Toolkit/CUDA/v9.1 --action_env TF_CUDNN_VERSION=9 --action_env TF_CUDA_COMPUTE_\r\nCAPABILITIES=3.0 --action_env TF_CUDA_CLANG=0 --action_env CUDA_PATH=C:/Program\r\nFiles/NVIDIA GPU Computing Toolkit/CUDA/v9.1 --action_env CUDA_COMPUTE_CAPABILIT\r\nIE=None --action_env NO_WHOLE_ARCHIVE_OPTION=1 --config=win-cuda --define grpc_n\r\no_ares=true --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK --host_copt=-DGEMMLOWP_\r\nALLOW_SLOW_SCALAR_FALLBACK --config monolithic --copt=-w --host_copt=-w --verbos\r\ne_failures\r\nINFO: Found applicable config definition build:win-cuda in file p:\\anaconda3\\ten\r\nsorflow\\bazel.rc: --define=using_cuda=true --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:monolithic in file p:\\anaconda3\\t\r\nensorflow\\bazel.rc: --define framework_shared_object=false\r\nERROR: Config value v1 is not defined in any .rc file\r\n\r\nP:\\Anaconda3\\tensorflow>", "Hi @FB43 ,  Could you please look at this[ link ](https://www.tensorflow.org/install/source_windows)for installing from source in windows.\r\n\r\nWe also see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37094\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37094\">No</a>\n"]}, {"number": 37093, "title": "Reading TFRecord through tf.data extremely slow in comparison to Pandas (MWE)", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from (source or binary): binary pip\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: Python 3.7.6\r\n- CUDA/cuDNN version: 7.6.4.38-1\r\n- GPU model and memory: Quadro RTX 6000 22752 MB\r\n\r\n**Describe the current behavior**\r\n\r\nIn the example below, first `tf.Data` is used to read data from dummy `TFRecord` files. Then pandas is used to read the same data from serialized `pandas.DataFrame` pickle-files. Both pipelines read chunks of `batch_size=512` elements from the data to the end (in the example, 10 files of 210083x135 floats).\r\n\r\nIn comparison, the performance of the `tf.Dataset` is very slow. Pandas reads the data  in ~**1.86 s** (~4.52 MB/s) vs `tf.data` in ~**51.3 s** (~0.16 MB/s).\r\n\r\nThe pipeline (in my understanding) does nothing more than to read the files from disk into batched junks in memory. Please help me understand and resolve this performance gap. \r\n\r\nThis issue induces a huge performance hit in my model training, which uses a slightly more complicated pipeline.\r\n\r\n**Describe the expected behavior**\r\n\r\nProcessing TFRecord files should be equally fast if not faster than processing pickle-files with pandas.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```\r\nimport math\r\nimport time\r\nimport pandas\r\nimport random\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(\"TensorFlow {}\".format(tf.__version__))\r\n\r\nnum_features = 130\r\nnum_labels = 5\r\nbatch_size = 512\r\nwin_size = 2048\r\nlen_max = 210083\r\n\r\n#########################################\r\n# dummy data\r\n#########################################\r\n\r\nl1 = len_max\r\nX1, Y1 = (np.ones((l1, num_features), dtype=np.float32), np.zeros((l1, num_labels), dtype=np.float32))\r\n\r\nk = 10\r\n\r\n#########################################\r\n# create dummy tfrecord file(s)\r\n#########################################\r\n\r\ndef write_tfrecord(X, Y, fn):\r\n    with tf.Graph().as_default():\r\n        ds = tf.data.Dataset.from_tensor_slices((X, Y))\r\n\r\n        sstring = ds.map(lambda *x: \r\n           tf.reshape(tf.py_function(lambda *v:\r\n               tf.train.Example(features=tf.train.Features(feature={\r\n                   \"features\": tf.train.Feature(float_list=tf.train.FloatList(value=v[0].numpy())),\r\n                   \"label\": tf.train.Feature(float_list=tf.train.FloatList(value=v[1].numpy())),\r\n               })).SerializeToString(), x, tf.string\r\n           ), ())\r\n        )\r\n\r\n        writer = tf.data.experimental.TFRecordWriter(fn)\r\n        writer_op = writer.write(sstring)\r\n\r\n        sess = tf.compat.v1.Session()\r\n        sess.run(tf.compat.v1.global_variables_initializer())\r\n        sess.run(writer_op)\r\n        sess.close()\r\n\r\nfiles_base_tf = [\"./temp3.tfrecord\"]\r\n\r\nwrite_tfrecord(X1, Y1, files_base_tf[0])\r\n\r\n#########################################\r\n# create dummy pandas pickle file(s)\r\n#########################################\r\n\r\ndf = pandas.DataFrame(np.concatenate((X1, Y1), axis=1))\r\n\r\nfiles_base_pd = [\"./temp3.pkl\"]\r\n\r\ndf.to_pickle(files_base_pd[0])\r\n\r\n#########################################\r\n# take k random files as tf.Dataset\r\n#########################################\r\n\r\n_files = random.choices(files_base_tf, k=k)\r\n\r\nds_fs = tf.data.Dataset.list_files(_files, shuffle=False, seed=1)\r\nfs_len = 0\r\nfor f in ds_fs:\r\n    fs_len += 1\r\nprint(\"Reading {} tfrecord files...\".format(fs_len))\r\n\r\n# prepare the tf.Dataset\r\n\r\ndef prep_ds_file(file):\r\n    _ds = tf.data.TFRecordDataset(file)\r\n    _ds = _ds.map(lambda x: tf.io.parse_single_example(x, {\r\n        \"features\": tf.io.FixedLenFeature([num_features], tf.float32),\r\n        \"label\": tf.io.FixedLenFeature([num_labels], tf.float32),\r\n    #}), num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    }), num_parallel_calls=1)\r\n    #print(_ds)\r\n    \r\n    _ds = _ds.flat_map(lambda v: tf.data.Dataset.from_tensors((v[\"features\"], v[\"label\"])))\r\n    #print(_ds)\r\n\r\n    _ds = _ds.batch(batch_size, drop_remainder=False)\r\n    print(_ds)\r\n    \r\n    return _ds\r\n\r\n\r\ndef prep_ds(fs):\r\n    _ds = fs.interleave(prep_ds_file, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    print(_ds)\r\n\r\n    return _ds\r\n\r\n\r\nds = prep_ds(ds_fs)\r\n\r\n#########################################\r\n# read/use the tf.Dataset\r\n#########################################\r\n\r\nshapes_tf = []\r\n\r\nts = time.perf_counter()\r\ni = 0\r\nfor e in ds:\r\n    v = (lambda: [x.numpy() for x in e])()\r\n    #shapes_tf.append([x.shape for x in v])\r\n    i += 1\r\nte = time.perf_counter()\r\nprint(\"Duration TFRecord ({} rounds): {} s\".format(i, (te - ts)))\r\nprint(\"Speed: {} MB/s\".format((k*l1*4/1000/1000/(te - ts))))\r\n\r\n#########################################\r\n# take k random files as pandas DataFrame\r\n#########################################\r\n\r\n_files = random.choices(files_base_pd, k=k)\r\n\r\n#########################################\r\n# read/use the pandas DataFrame\r\n#########################################\r\n\r\nshapes_pd = []\r\n\r\nts = time.perf_counter()\r\ni = 0\r\nfor f in _files:\r\n    df = pandas.read_pickle(f)\r\n    n = math.ceil(len(df.index) / batch_size)\r\n    for b in range(n):\r\n        #print(df.iloc[(b*batch_size):((b+1)*batch_size)].shape)\r\n        v = (lambda: df.iloc[(b*batch_size):((b+1)*batch_size)].to_numpy())()\r\n        #shapes_pd.append(v.shape)\r\n        i += 1\r\nte = time.perf_counter()\r\nprint(\"Duration Pandas ({} rounds): {} s\".format(i, (te - ts)))\r\nprint(\"Speed: {} MB/s\".format((k*l1*4/1000/1000/(te - ts))))\r\n```\r\n\r\n**Other info / logs**\r\n\r\nOutput:\r\n```\r\nTensorFlow 2.1.0\r\nReading 10 tfrecord files...\r\n<BatchDataset shapes: ((None, 130), (None, 5)), types: (tf.float32, tf.float32)>\r\n<ParallelInterleaveDataset shapes: ((None, 130), (None, 5)), types: (tf.float32, tf.float32)>\r\nDuration TFRecord (4110 rounds): 51.2759778869804 s\r\nSpeed: 0.1638841489970629 MB/s\r\nDuration Pandas (4110 rounds): 1.8584379029925913 s\r\nSpeed: 4.521711479554074 MB/s\r\n```\r\n", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/983e36d410aa52fedaab9dc19135019c/37093.ipynb). Thanks!", "The overhead of parsing an example record is rather high compared to copying the small features and label fields. Changing the dataset to just passing the records speeds up the reads to 1/2 of the speed of the pandas.read_pick based solution. (Using @amahendrakar 's colab as a base )\r\nThis seems reasonable as reading dynamic sized records, calculating the record checksum, .. takes effort.\r\nFor larger feature and label tensors the example record parsing overhead should be a smaller percentage.\r\nIf example records are required - maybe storing batches of features/labels inside of a single example record (and unbatching them on read) could be a workaround for small tensors?\r\n( I would also recommend looking at tf.data.experimental.ThreadingOptions if not all cores are busy )\r\n", "Thank you for your reply. I have two questions if you allow:\r\n\r\n1. I don't know if example records are required. Is there a better way to represent the data given in my MWE? If so, could you provide a code snippet or web link illustrating this?\r\n\r\n2. Could you additionally or alternatively present a code snippet or point me to an example that shows how to properly use batches during storing/retrieving? It would help a lot.\r\n\r\nThanks again!", "I found that using `FixedLenSequenceFeature` and storing/reading whole columns instead of rows mitigates the issue. It requires to use `ds = ds.map(lambda x : tf.transpose(x, [1, 0]))` after reading but works much faster. I guess that is what you meant? Anyway, thanks, I am closing this issue.", "@mimxrt - thank you for raising this issue and for mentioning the way you resolved it. Is there any chance you can post the code of the solution that makes use of `FixedLenSequenceFeature`? would be quite grateful", "@marwan116 It has been a long time but I think this was the code snippet that mitigated this issue:\r\n\r\n```\r\ndef prep_ds_file(file):\r\n    _ds = tf.data.TFRecordDataset(\r\n        file,\r\n        #num_parallel_reads=tf.data.experimental.AUTOTUNE\r\n    )\r\n    \r\n#     _ds = _ds.map(lambda x: tf.io.parse_single_example(x, {\r\n#         \"features\": tf.io.FixedLenFeature([num_features], tf.float32),\r\n#         \"label\": tf.io.FixedLenFeature([num_labels], tf.float32),\r\n#     }), num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n#     #}), num_parallel_calls=1)\r\n#     #print(_ds)\r\n\r\n    _ds = _ds.batch(l1)\r\n    print(_ds)\r\n    _ds = _ds.map(\r\n        lambda x: tf.io.parse_example(x, {\r\n            \"features\": tf.io.FixedLenFeature([num_features], tf.float32),\r\n            \"label\": tf.io.FixedLenFeature([num_labels], tf.float32),\r\n        }),\r\n        #num_parallel_calls=tf.data.experimental.AUTOTUNE\r\n    )\r\n    print(_ds)\r\n    \r\n    #_ds = _ds.flat_map(lambda v: tf.data.Dataset.from_tensors((v[\"features\"], v[\"label\"])))\r\n    _ds = _ds.interleave(\r\n        lambda v: tf.data.Dataset.from_tensors((v[\"features\"], v[\"label\"])),\r\n        #num_parallel_calls=tf.data.experimental.AUTOTUNE\r\n    )\r\n    print(_ds)\r\n\r\n    _ds = _ds.unbatch()\r\n    \r\n    _ds = _ds.batch(batch_size, drop_remainder=False)\r\n    print(_ds)\r\n    \r\n    return _ds\r\n\r\n\r\ndef prep_ds(fs):\r\n#     _ds = fs.interleave(\r\n#         prep_ds_file,\r\n#         #num_parallel_calls=tf.data.experimental.AUTOTUNE\r\n#     )\r\n    _ds = fs.flat_map(prep_ds_file)\r\n    \r\n    _ds = _ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n    \r\n    return _ds\r\n\r\n\r\nds = prep_ds(ds_fs)\r\n```\r\n\r\nNote the commented-out parts that were replaced from the original MWE. IIRC the solution was to read bigger chunks at once instead of \"line by line\" (using `parse_example` instead of `parse_single_example` with the concept _batch_ -> _read_ -> _unbatch_). Hope that helps!", "@mimxrt thank you for following up on this - I can see how employing `parse_example` to read a chunk of examples at a time will help optimize the read operation. \r\n\r\nBut If I understand it correctly, this is different from your last comment on writing and reading the full columns which is the approach that I think would be the fastest for reading and writing", "I don't exactly remember what I meant by that but I double checked and could not find any other example in connection to this issue. However, the way I ended up creating and using these datasets is as follows:\r\n\r\n```\r\ndef _write(df, fn):\r\n    # df is a Pandas DataFrame, fn is the filename\r\n\r\n    def tf_serialize_example(*x):\r\n        tf_string = tf.py_function(\r\n            lambda *v : tf.train.Example(features=tf.train.Features(\r\n                feature = {\r\n                    l: tf.train.Feature(float_list=tf.train.FloatList(value=f)) for l, f in zip(df.columns, v)\r\n                }\r\n            )).SerializeToString(),\r\n            x,\r\n            tf.string\r\n        )\r\n        return tf.reshape(tf_string, ())\r\n\r\n    ds = tf.data.Dataset.from_tensor_slices(\r\n        tuple(df[col].values.reshape(1, -1) for col in df.columns)\r\n    )\r\n\r\n    ds_ser = ds.map(tf_serialize_example)\r\n\r\n    writer = tf.data.experimental.TFRecordWriter(fn)\r\n    writer.write(ds_ser)\r\n```\r\n\r\nFor reading this, I actually end up using `parse_single_example`:\r\n\r\n```\r\nfor i, filename in enumerate(filenames):\r\n    path = my_path + filename\r\n    ds_fs = tf.data.TFRecordDataset(\r\n        path,\r\n        num_parallel_reads=tf.data.experimental.AUTOTUNE,\r\n    )\r\n    #print(ds_fs)\r\n\r\n    # add or remove features to **read** from the TFRecord here\r\n    ds = ds_fs.map(lambda x : tf.io.parse_single_example(\r\n        x, features={\r\n#             \"Ft1\": tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\r\n#             **{str(i) : tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True) for i in range(num_features-3)},\r\n#             \"Ft2\": tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\r\n#             \"Ft3\": tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\r\n            \"Label\": tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\r\n        }\r\n    ))\r\n\r\n    # add or remove features to **include** in the Dataset here\r\n    ds = ds.interleave(\r\n        lambda v: tf.data.Dataset.from_tensors([v[\"Label\"]]),\r\n        #num_parallel_calls=tf.data.experimental.AUTOTUNE\r\n    )\r\n    #print(ds)\r\n```"]}, {"number": 37092, "title": "Entity could not be transformed error", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code - YES\r\n- OS Platform and Distribution - Mac OS 10.15.2 Catalina\r\n- TensorFlow installed from (source or\r\nbinary): - Conda\r\n- Python version: - 3.6.7\r\nUsing CPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nUsing Tensorflow 2.0.0\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to implement custom layers and I keep getting the following errors: \r\n\r\nWARNING:tensorflow:Entity <bound method BinaryConv2D.call of <binary_layers.BinaryConv2D object at 0x1ac37a9d50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nWARNING: Entity <bound method BinaryConv2D.call of <binary_layers.BinaryConv2D object at 0x1ac37a9d50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nWARNING:tensorflow:Entity <function streak at 0x1ac34def80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nWARNING: Entity <function streak at 0x1ac34def80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1ac8e150e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nWARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1ac8e150e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nModel: \"model_8\"\r\n\r\nI dont care about the lambda layers as those should be non-trainable. However, I want the bin_conv2d to be trainable and it appears that something is preventing that but I cannot figure out why. I also dont understand the last warning of Entity function initialize_unitialized_variables is. Any help with this would be greatly appreciated.\r\n\r\nTo give an idea for what I'm trying to do, I generate a binary kernel in bin_conv2d and then element-wise multiply that by each input. I am wrapping everything in TimeDistributed as my inputs are videos and I want the outputs to be evaluated as such. The original implementation of bin_conv2d used the standard convolution and that compiled fine. It seems that once I changed it to be element-multiply it breaks down. The weird thing is that it seems that the binarize function is what breaks when I change the code even though I am not changing that at all.\r\n\r\nHere is a link to my [gist](https://gist.github.com/matthewp14/dd77eea3464d39d49c618f4398723ce5)\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nHere is the output when I set \r\n\r\ntf.autograph.set_verbosity(3, True)\r\n\r\nINFO:tensorflow:Converted call: <bound method BinaryConv2D.call of <binary_layers.BinaryConv2D object at 0x1ac6957150>>\r\n    args: (<tf.Tensor 'time_distributed_280/Reshape:0' shape=(60, 32, 32, 1) dtype=float32>,)\r\n    kwargs: {}\r\n\r\nConverted call: <bound method BinaryConv2D.call of <binary_layers.BinaryConv2D object at 0x1ac6957150>>\r\n    args: (<tf.Tensor 'time_distributed_280/Reshape:0' shape=(60, 32, 32, 1) dtype=float32>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Not whitelisted: <method-wrapper '__call__' of method object at 0x1ac5c17140>: default rule\r\nNot whitelisted: <method-wrapper '__call__' of method object at 0x1ac5c17140>: default rule\r\nINFO:tensorflow:Not whitelisted: <class 'binary_layers.BinaryConv2D'>: default rule\r\nNot whitelisted: <class 'binary_layers.BinaryConv2D'>: default rule\r\nINFO:tensorflow:Not whitelisted: <bound method BinaryConv2D.call of <binary_layers.BinaryConv2D object at 0x1ac6957150>>: default rule\r\nNot whitelisted: <bound method BinaryConv2D.call of <binary_layers.BinaryConv2D object at 0x1ac6957150>>: default rule\r\nINFO:tensorflow:Cache hit for entity <bound method BinaryConv2D.call of <binary_layers.BinaryConv2D object at 0x1ac6957150>> key <code object call at 0x1ac9f12030, file \"/Users/matthew/Desktop/Desktop - Matthew\u2019s MacBook Pro (74)/CUP-Net/src/binary_layers.py\", line 158> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x1acb755dd0>, frozenset()): _ConvertedEntityFactoryInfo(tf__call in tmppbrtju_8)\r\nCache hit for entity <bound method BinaryConv2D.call of <binary_layers.BinaryConv2D object at 0x1ac6957150>> key <code object call at 0x1ac9f12030, file \"/Users/matthew/Desktop/Desktop - Matthew\u2019s MacBook Pro (74)/CUP-Net/src/binary_layers.py\", line 158> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x1acb755dd0>, frozenset()): _ConvertedEntityFactoryInfo(tf__call in tmppbrtju_8)\r\nINFO:tensorflow:Error transforming entity <bound method BinaryConv2D.call of <binary_layers.BinaryConv2D object at 0x1ac6957150>>\r\nTraceback (most recent call last):\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 324, in convert\r\n    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 266, in _instantiate\r\n    factory = converted_entity_info.get_factory()\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 92, in get_factory\r\n    assert self.module_name in sys.modules\r\nAssertionError\r\nError transforming entity <bound method BinaryConv2D.call of <binary_layers.BinaryConv2D object at 0x1ac6957150>>\r\nWARNING:tensorflow:Entity <bound method BinaryConv2D.call of <binary_layers.BinaryConv2D object at 0x1ac6957150>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nWARNING: Entity <bound method BinaryConv2D.call of <binary_layers.BinaryConv2D object at 0x1ac6957150>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nINFO:tensorflow:Converted call: <function streak at 0x1ac9e22320>\r\n    args: (<tf.Tensor 'x:0' shape=(2, 30, 32, 32, 1) dtype=float32>,)\r\n    kwargs: {}\r\n\r\nConverted call: <function streak at 0x1ac9e22320>\r\n    args: (<tf.Tensor 'x:0' shape=(2, 30, 32, 32, 1) dtype=float32>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Cache hit for entity <function streak at 0x1ac9e22320> key <code object streak at 0x1ac9f12db0, file \"/Users/matthew/Desktop/Desktop - Matthew\u2019s MacBook Pro (74)/CUP-Net/src/lambda_layers.py\", line 20> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x1acb837b50>, frozenset()): _ConvertedEntityFactoryInfo(tf__streak in tmpqw9832gt)\r\nCache hit for entity <function streak at 0x1ac9e22320> key <code object streak at 0x1ac9f12db0, file \"/Users/matthew/Desktop/Desktop - Matthew\u2019s MacBook Pro (74)/CUP-Net/src/lambda_layers.py\", line 20> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x1acb837b50>, frozenset()): _ConvertedEntityFactoryInfo(tf__streak in tmpqw9832gt)\r\nINFO:tensorflow:Error transforming entity <function streak at 0x1ac9e22320>\r\nTraceback (most recent call last):\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 324, in convert\r\n    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 266, in _instantiate\r\n    factory = converted_entity_info.get_factory()\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 92, in get_factory\r\n    assert self.module_name in sys.modules\r\nAssertionError\r\nError transforming entity <function streak at 0x1ac9e22320>\r\nWARNING:tensorflow:Entity <function streak at 0x1ac9e22320> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nWARNING: Entity <function streak at 0x1ac9e22320> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nTraceback (most recent call last):\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 324, in convert\r\n    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 266, in _instantiate\r\n    factory = converted_entity_info.get_factory()\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 92, in get_factory\r\n    assert self.module_name in sys.modules\r\nAssertionError\r\nTraceback (most recent call last):\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 324, in convert\r\n    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 266, in _instantiate\r\n    factory = converted_entity_info.get_factory()\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 92, in get_factory\r\n    assert self.module_name in sys.modules\r\nAssertionError\r\nINFO:tensorflow:Converted call: <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd33c320>\r\n    args: ()\r\n    kwargs: {}\r\n\r\nConverted call: <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd33c320>\r\n    args: ()\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Cache hit for entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd33c320> key <code object initialize_variables at 0x14d357660, file \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 603> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x1acd3638d0>, frozenset({'initializer_map'})): _ConvertedEntityFactoryInfo(tf__initialize_variables in tmphhcwjhr8)\r\nCache hit for entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd33c320> key <code object initialize_variables at 0x14d357660, file \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 603> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x1acd3638d0>, frozenset({'initializer_map'})): _ConvertedEntityFactoryInfo(tf__initialize_variables in tmphhcwjhr8)\r\nINFO:tensorflow:Error transforming entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd33c320>\r\nTraceback (most recent call last):\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 324, in convert\r\n    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 266, in _instantiate\r\n    factory = converted_entity_info.get_factory()\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 92, in get_factory\r\n    assert self.module_name in sys.modules\r\nAssertionError\r\nError transforming entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd33c320>\r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd33c320> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nWARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd33c320> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nINFO:tensorflow:Converted call: <function streak at 0x1ac9e22320>\r\n    args: (<tf.Tensor 'x:0' shape=(2, 30, 32, 32, 1) dtype=float32>,)\r\n    kwargs: {}\r\n\r\nConverted call: <function streak at 0x1ac9e22320>\r\n    args: (<tf.Tensor 'x:0' shape=(2, 30, 32, 32, 1) dtype=float32>,)\r\n    kwargs: {}\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 324, in convert\r\n    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 266, in _instantiate\r\n    factory = converted_entity_info.get_factory()\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 92, in get_factory\r\n    assert self.module_name in sys.modules\r\nAssertionError\r\nModel: \"model_9\"\r\n_____________________\r\n\r\nINFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x1acd0b20e0>\r\n    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\r\n    kwargs: {}\r\n\r\nConverted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x1acd0b20e0>\r\n    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x1acd0b20e0>: DoNotConvert rule for tensorflow\r\nWhitelisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x1acd0b20e0>: DoNotConvert rule for tensorflow\r\nINFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x1acd0b2f80>\r\n    args: (<tf.Tensor 'args_0:0' shape=(16,) dtype=int64>,)\r\n    kwargs: {}\r\n\r\nConverted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x1acd0b2f80>\r\n    args: (<tf.Tensor 'args_0:0' shape=(16,) dtype=int64>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x1acd0b2f80>: DoNotConvert rule for tensorflow\r\nWhitelisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x1acd0b2f80>: DoNotConvert rule for tensorflow\r\nINFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x1acd0b2440>\r\n    args: (<tf.Tensor 'args_0:0' shape=(2,) dtype=int64>, (<tf.Tensor 'args_1:0' shape=(16, 30, 32, 32, 1) dtype=float32>, <tf.Tensor 'args_2:0' shape=(16, 30, 32, 32, 1) dtype=float32>))\r\n    kwargs: {}\r\n\r\nConverted call: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x1acd0b2440>\r\n    args: (<tf.Tensor 'args_0:0' shape=(2,) dtype=int64>, (<tf.Tensor 'args_1:0' shape=(16, 30, 32, 32, 1) dtype=float32>, <tf.Tensor 'args_2:0' shape=(16, 30, 32, 32, 1) dtype=float32>))\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x1acd0b2440>: DoNotConvert rule for tensorflow\r\nWhitelisted: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x1acd0b2440>: DoNotConvert rule for tensorflow\r\nTrain on 16 samples\r\nEpoch 1/10\r\nWARNING:tensorflow:Gradients do not exist for variables ['time_distributed_280/kernel:0'] when minimizing the loss.\r\nINFO:tensorflow:Converted call: <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd133950>\r\n    args: ()\r\n    kwargs: {}\r\n\r\nConverted call: <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd133950>\r\n    args: ()\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Cache hit for entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd133950> key <code object initialize_variables at 0x14d357660, file \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 603> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x1ace7741d0>, frozenset({'initializer_map'})): _ConvertedEntityFactoryInfo(tf__initialize_variables in tmphhcwjhr8)\r\nCache hit for entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd133950> key <code object initialize_variables at 0x14d357660, file \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 603> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x1ace7741d0>, frozenset({'initializer_map'})): _ConvertedEntityFactoryInfo(tf__initialize_variables in tmphhcwjhr8)\r\nINFO:tensorflow:Error transforming entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd133950>\r\nTraceback (most recent call last):\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 324, in convert\r\n    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 266, in _instantiate\r\n    factory = converted_entity_info.get_factory()\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 92, in get_factory\r\n    assert self.module_name in sys.modules\r\nAssertionError\r\nError transforming entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd133950>\r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd133950> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nWARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1acd133950> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nTraceback (most recent call last):\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 324, in convert\r\n    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 266, in _instantiate\r\n    factory = converted_entity_info.get_factory()\r\n  File \"/Users/matthew/opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 92, in get_factory\r\n    assert self.module_name in sys.modules\r\nAssertionError\r\nWARNING:tensorflow:Gradients do not exist for variables ['time_distributed_280/kernel:0'] when minimizing the loss.\r\n", "comments": ["@matthewp14 \r\nI do not find any gist, as mentioned by you [also please share the tensorflow version].\r\nCan you please try installing gast ( pip install gast==0.2.2), refer to [link](https://github.com/tensorflow/tensorflow/issues/32859) where the workaround is avaiable.\r\nlet us know if that helps.\r\n", "@Saduf2019 thank you that did help with the initial warnings but I am still having the following problem. In this [gist](https://gist.github.com/matthewp14/dd77eea3464d39d49c618f4398723ce5) I can train the first model fine. However, when I add the two lambda layers in the second model it breaks and I am not sure why. I intended for there to be no trainable weights in the lambda layers but I dont understand why the model stops working as soon as I add those. it seems as though it stops tracking BinaryConv2D when I add those. If you could lend a hand with this that would be great. \r\n\r\nThis is the output that I get: \r\n\r\nValueError: No gradients provided for any variable: ['time_distributed/kernel:0'].\r\n\r\nI added all the code that you need to run. To be clear, I redefined the lambda functions within the binary test file to product dummy validation data but the real functions are located within the lambda_layers.py file. ", "@matthewp14 \r\nCould you please confirm which tensor flow version should i replicate this issue.", "@Saduf2019 i am using Tensorflow 2.0.0 installed using conda", "@matthewp14 \r\nI ran the code shared by you and face a different error, please refer to this [link](https://colab.sandbox.google.com/gist/Saduf2019/2e75f9009b71f100a5e48179c590f813/37092.ipynb) for the same.", "@Saduf2019 thats because you are not importing those modules. If you look at the gist that I linked in the previous comment, everything except for \"binary_test.py\" is a module and should be loaded as such. The only file to be executed is binary_test. Here is a link to the updated [gist](https://gist.github.com/matthewp14/dd77eea3464d39d49c618f4398723ce5) again. Sorry about that. I dont know how to share a collab notebook with files loaded as modules otherwise I would share that link for you. Please use the files from this most recent gist as they are updated to ensure that the error is the same. ", "@gowthamkpr I was able to figure this out and modify my code accordingly. I am closing this issue. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37092\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37092\">No</a>\n"]}, {"number": 37091, "title": "LookupError when calculating gradient of gradient with RNN on GPU", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from (source or\r\nbinary): binary (Anaconda/PIP)\r\n - TensorFlow version (use command below): \r\nGIT_VERSION v1.12.1-25080-gca585e7\r\nVERSION  2.2.0-dev20200218\r\n- Python version: 3.6.10\r\n- CUDA/cuDNN version: CUDA 10.2, cuDNN 7.6.2\r\n- GPU model and memory: GeForce RTX 2080 Ti, 12GB VRAM\r\n\r\n\r\n**Describe the current behavior**\r\nWhen trying to implement a gradient penalty for a WGAN-GP, which requires to calculate a gradient of a tensor, which itself depends on a gradient, the program terminates with a LookupError and tells me \r\n\r\n> LookupError: gradient registry has no entry for: CudnnRNNBackprop\r\n\r\nThis occurs **only on the GPU version** of Tensorflow and **only if a recurrent layer is used** (GRU or LSTM, doesn't matter). On the CPU, the provided minimum working example runs as expected. The error does not occur when training a \"normal\" model with a recurrent layer on the GPU (no gradient penalty). \r\n\r\nI tried with tensorflow-gpu 2.0 (installed via conda), tensorflow-gpu 2.1 (installed via pip) and tf-nightly-gpu 2.2 (more specific: _2.2.0-dev20200218_) (installed via pip). The error is the same for all GPU versions.\r\n\r\nThe operation **works with tensorflow-gpu, if the parameter _unroll_ of the GRU-Layer is set to True**. Since this disables the usage of the cuDNN implementation of the GRU-Layer (deviation from default parameters), this does not solve the actual problem, but might indicate that there is just a small bug in the interface to cuDNN.\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\n# this code does not make much sense, but is shorter than providing a full optimization loop for a WGAN_GP and produces the same error\r\nimport tensorflow as tf\r\nimport tensorflow.keras as k\r\nimport tensorflow.keras.layers as kl\r\nimport numpy as np\r\n\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\ndef gradient_penalty(model, input_data):\r\n    # get gradient\r\n    input_data = tf.convert_to_tensor(input_data)\r\n    with tf.GradientTape() as t:\r\n        t.watch(input_data)\r\n        pred = model(input_data)\r\n    grad = t.gradient(pred, [input_data])[0]\r\n    # define gradient penalty\r\n    slopes = tf.sqrt(tf.reduce_sum(tf.square(grad), axis=[1, 2]))\r\n    gp = tf.reduce_mean((slopes - 1.) ** 2)\r\n    return gp\r\n\r\nif __name__ == \"__main__\":\r\n    # model with recurrent layer\r\n    model = k.Sequential([kl.InputLayer(input_shape=(50, 20)), kl.GRU(100), kl.Dense(1)])\r\n    # Optimizer\r\n    opt = tf.optimizers.Adam()\r\n    # Dummy data\r\n    data = np.random.normal(0, 1, (8, 50, 20)).astype(np.float32)\r\n    # Optimize\r\n    with tf.GradientTape() as tape:\r\n        gp = gradient_penalty(model=model, input_data=data)\r\n    grad = tape.gradient(gp, model.trainable_variables)\r\n    opt.apply_gradients(zip(grad, model.trainable_variables))\r\n\r\n```\r\n\r\n**Other info / logs**\r\nFull log:\r\n```\r\n/home/hendrik/anaconda3/envs/MLS22/bin/python /home/hendrik/PycharmProjects/GAN/MinimumMinimumWorkingExample.py\r\n2020-02-26 13:30:08.149540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-02-26 13:30:08.172329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-26 13:30:08.172601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-02-26 13:30:08.172716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-02-26 13:30:08.173554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-26 13:30:08.174413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-02-26 13:30:08.174551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-02-26 13:30:08.175425: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-02-26 13:30:08.175952: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-02-26 13:30:08.177916: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-26 13:30:08.177978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-26 13:30:08.178270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-26 13:30:08.178512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-02-26 13:30:08.187368: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-02-26 13:30:08.208283: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz\r\n2020-02-26 13:30:08.208513: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cac97d38c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-02-26 13:30:08.208523: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-02-26 13:30:08.267483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-26 13:30:08.267804: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cac97f6830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-02-26 13:30:08.267814: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-02-26 13:30:08.267915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-26 13:30:08.268226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-02-26 13:30:08.268246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-02-26 13:30:08.268253: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-26 13:30:08.268259: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-02-26 13:30:08.268265: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-02-26 13:30:08.268271: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-02-26 13:30:08.268276: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-02-26 13:30:08.268282: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-26 13:30:08.268308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-26 13:30:08.268554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-26 13:30:08.268778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-02-26 13:30:08.268795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-02-26 13:30:08.269304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-26 13:30:08.269310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 \r\n2020-02-26 13:30:08.269314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N \r\n2020-02-26 13:30:08.269362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-26 13:30:08.269613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-26 13:30:08.269856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9813 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-02-26 13:30:08.667776: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-26 13:30:09.284739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\nTraceback (most recent call last):\r\n  File \"/home/hendrik/PycharmProjects/GAN/MinimumMinimumWorkingExample.py\", line 31, in <module>\r\n    grad = tape.gradient(gp, model.trainable_variables)\r\n  File \"/home/hendrik/anaconda3/envs/MLS22/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 1048, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \"/home/hendrik/anaconda3/envs/MLS22/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\", line 77, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \"/home/hendrik/anaconda3/envs/MLS22/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 145, in _gradient_function\r\n    grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access\r\n  File \"/home/hendrik/anaconda3/envs/MLS22/lib/python3.6/site-packages/tensorflow/python/framework/registry.py\", line 97, in lookup\r\n    \"%s registry has no entry for: %s\" % (self._name, name))\r\nLookupError: gradient registry has no entry for: CudnnRNNBackprop\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nThanks in advance for your time!", "comments": ["I have tried on colab with TF version 2.2.0-dev20200226 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/d4f32851c12e3bb4329030a0dd0d297b/untitled681.ipynb). Thanks!", "Hi @HendrikLaux, Thanks for reporting the issue.\r\n\r\nThe root cause is that cudnnRNN kernel doesn't have higher order gradient support, which is the issue you are hitting. I am not sure if Nvidia is going to add high order gradient support the cudnn kernel, for now we can only work around the issue (eg use unroll=True).", "Note that unroll requite the input to have a static time sequence. If you want to build your model with dynamic time sequence length, you can disable the cudnn backend by:\r\n\r\n```\r\ngru_layer = tf.keras.layers.GRU(10)\r\ngru_layer._could_use_gpu_kernel = False.\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37091\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37091\">No</a>\n", "Hey qlzh727,\r\n\r\nthank you for the clarification of this issue! Do you think the issue is worth passing to Nvidia, or is the problem too minor and the use cases too limited for this? ", "@houtoms from Nvidia for this.", "@houtoms @qlzh727 \r\nAny updates on this issue? :slightly_smiling_face: ", "Thanks for the update. Let me sync with our team and put some update here.", "@HendrikLaux We don't have it on our plan in the near term to support higher order gradient RNN. But I can help issue a RFE if you think this is needed because, for example, the performance of it is the bottleneck.", "This support would be very nice to have. "]}, {"number": 37090, "title": "Update docstring for consine_similarity", "body": "Addresses some of the issues discussed in tensorflow#33820 by editing the docstring for `consine_similarity`. The edited docstring correctly indicates the `y_true` and `y_pred` need not be normalized, and that passing a zero vector as one of the arguments will return 0 regardless of the proximity between true and predicted labels.", "comments": []}, {"number": 37089, "title": "Tensorflow Lite Hexagon delegate support for QCOM SDM765G", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (\r\nLinux Ubuntu 16.04): \r\n- Mobile device (OPPO reno3 Pro) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below):  14417096dbebbaf46043972029f4bf721e5364cb\r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nHello,\r\nI am using tensorflow based on commit ID: 14417096dbebbaf46043972029f4bf721e5364cb\r\nAnd I use tflite benchmark_model for testing (tensorflow/lite/tools/benchmark),\r\nbenchmark_model run failed when enable TFlite hexagon delegate on my phone.\r\nhexagon delegate is not supportted on my phone.\r\nCould you please help add this CPU in the supportted list?\r\nbelow is my phone innfo:\r\n```\r\n#adb shell getprop ro.product.device\r\nOP4A9D  \r\n\r\n#adb shell getprop ro.board.platform\r\nlito\r\n\r\nCPU info :  Qualcomm Technologies, Inc SDM765G 5G\r\n```\r\n\r\nAnd from https://source.codeaurora.org/quic/hexagon_nn/nnlib, \r\nI see that the latest hexagon_nn version is 0x00021401, Could you please update libhexagon_nn_skel.so  to the latest version ?\r\n\r\nthanks very much\r\n\r\n", "comments": ["Hi,\r\n\r\nCan you share more details about this SoC, which version it is using V65/66/something else ?\r\nDo you know the soc id ? (you can run \"adb shell cat /sys/devices/soc0/soc_id\")\r\n\r\nregarding the version, stay tuned on it's way :)\r\nBTW, is there something you need that is specifically in the new version ?\r\n\r\nThanks", "@karimnosseir\r\nhello,\r\nI am not sure the hexagon version of this SoC,  it is Hexagon 696 DSP Processor, I think it is V66.\r\nand the SoC id is 400\r\n```\r\n $ cat /sys/devices/soc0/soc_id\r\n400\r\n```\r\nyou can find more info at https://www.qualcomm.com/media/documents/files/qualcomm-snapdragon-765-5g-mobile-platform-product-brief.pdf\r\n \r\n  \r\n \r\nabout the libhexagon_nn_skel.so version question, I see that:\r\ntensorflow/lite/experimental/delegates/hexagon/builders/concat_builder.cc: line 112\r\nOP_Requantize_8to8 is added after concat,\r\n```\r\n   auto* requantize_op = graph_builder_->AddNode(GetTFLiteNodeID());\r\n    requantize_op->SetOpType(OP_Requantize_8to8);\r\n    requantize_op->AddInput(concat_out);\r\n```\r\nbut I checked the latest version nnlib, https://source.codeaurora.org/quic/hexagon_nn/nnlib/tree/docs/ops.txt  line 778\r\n```\r\nQuantizedConcat_8:\r\n    3n+1 inputs:\r\n        0: Dimension (int32 scalar)\r\n        1-n: Input data tensors (quint8 tensor)\r\n        n+1-2n: Input min (float scalar)\r\n        2n+1-3n: Input max (float scalar)\r\n    3 outputs:\r\n        0: Output data (quint8 tensor)\r\n        1: Output min (float scalar)\r\n        2: Output max (float scalar)\r\n    The operation concatenate tensors. Currently we only support along the depth dimension. The \r\n        operation also requantizes values. \r\n    Dimension - Currently only depthwise concatenation (3) is supported.\r\n```\r\n**\"The operation also requantizes values. \"**\r\nThe OP_Requantize_8to8 is not needed after concat.\r\nSo, I guess maybe it is  libhexagon_nn_skel.so version issue.\r\n\r\n\r\n", "Thanks.\r\nWill enable it - hopefully in the next couple of days will be available.\r\n\r\nFor the Requantize, no it is not related to the version. We still need the Requantize anyway.\r\nBasically, we want to make sure that the data is quantized with the same scale as in the tflite output tensor of the concat op.\r\nHTH", "New version is released which Should enable SDM765G and also includes version 1.14 \r\nYou will need to use the compatible libhexagon_nn_skel* (you can find them here\r\nhttps://www.tensorflow.org/lite/performance/hexagon_delegate)\r\n\r\n", "@karimnosseir\r\nI tried to download the libhexagon_nn_skel* from \"https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_v1.14.run\",\r\nbut failed to download, shows\r\n\"Forbidden\r\nError 403\"\r\n\r\nCould you please help check this ?", "Can you try again \r\n\r\nThanks", "It is ok.\r\nThank you very much"]}, {"number": 37088, "title": "network_tester test failed with NN with 28x28 px input image but succeded with NN with 14x14 px input", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):  source\r\n- Tensorflow version (commit SHA if source): 1881d32ef8e998e6f1a403856dd44acbba2aad7c\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP32, x86\r\n\r\n**Describe the problem**\r\nI am implementing the Fashion MNIST classification project on ESP32.\r\nI have successfully implemented NN with input image size 14x14 pixels.\r\nThen I changed only the Input image size to 28x28 in NN without changing the structure of NN and was not able to obtain correct predictions with TFL Micro.\r\nThen I found [network_tester example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/network_tester) in the TF repository and decided to use it to test my NN with TFL Micro on x86.\r\n\r\nI have refactored original **network_tester example** to:\r\n1. feed the input image to NN as a 2-dimensional array.\r\n2. be able to compare float-type predictions.\r\n\r\nI have attached here both .tflite trained models and sources with output results.\r\n\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n1. Be sure all TFL Micro unit tests are succeeded. Run command `make -f tensorflow/lite/micro/tools/make/Makefile test`. Expected result: all tests passed successfully.\r\n\r\n2. Unzip `network_tester.zip` file.\r\n\r\n3. Replace original `network_tester_test.cc` file in TensorFlow source directory `tensorflow/tensorflow/lite/micro/examples/network_tester/` by file contained in archive.\r\n\r\n4. Build a test with 14x14 input NN. See parameters to build in `make_14x14.sh` file.\r\n\r\n5. Run test from `tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin` directory. See success.\r\n\r\n6. Build a test with 28x28 input NN. See parameters to build in `make_28x28.sh` file.\r\n\r\n7. Run test from `tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin` directory. **Test failed**.\r\n\r\nAll source files added to provided archive.\r\n\r\n[network_tester.zip](https://github.com/tensorflow/tensorflow/files/4255010/network_tester.zip)\r\n", "comments": ["@mr-goldhands \r\nIn order to expedite the trouble-shooting process, please provide a simple stand alone code with correct indentation and supporting documents to reproduce the issue reported here. Thanks!\r\n\r\nAlso please share the error logs of the issue faced.\r\n", "@Saduf2019 \r\nHi!\r\n\r\n**Before investigating this bug**, I recommend being familiar with the original [network_tester](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/network_tester) example from TensorFlow repository because of my test sample is based on it and utilize a similar approach.\r\n\r\nAll source code and logs I have attached in [network_tester.zip](https://github.com/tensorflow/tensorflow/files/4255010/network_tester.zip) file.\r\n\r\n1. Unzip `network_tester.zip` file.\r\n\r\n2. Test source code placed in: `network_tester/network_tester_test.cc`\r\n\r\n3. The Model, input data and the expected result placed in `network_tester/size_14x14` directory in `network_model.h`,  `input_data.h`, `expected_output_data.h` files respectively for 14x14 px input.\r\n\r\n4. The Model, input data and the expected result placed in `network_tester/size_28x28` directory in similar files for 28x28 px input.\r\n\r\n5. The test output logs for 14x14 px input placed in: `network_tester/size_14x14/bin/output_log.txt` file.\r\n\r\n6. The test output logs for 28x28 px input placed in: `network_tester/size_28x28/bin/output_log.txt` file.\r\n\r\n\r\n**To run the test for 14x14 px input Neural Network do the following:**\r\n\r\n1. Unzip attached network_tester.zip file (for example to `home` directory).\r\n\r\n2. Replace original `network_tester_test.cc` file in TensorFlow source directory `tensorflow/tensorflow/lite/micro/examples/network_tester/` by file contained in archive.\r\n\r\n3. Run the Linux console and then change directory (use `cd` command) to the root of the TensorFlow source directory (repository).\r\n\r\n4. Build the test by entering the command `make -f tensorflow/lite/micro/tools/make/Makefile network_tester_test NETWORK_MODEL=~/network_tester/size_14x14/network_model.h INPUT_DATA=~/network_tester/size_14x14/input_data.h OUTPUT_DATA=~/network_tester/size_14x14/expected_output_data.h ARENA_SIZE=50000 COMPARE_OUTPUT_DATA=yes`\r\n\r\n5. Run the Test by entering the command `./tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/network_tester_test`. See success.\r\n\r\n**Then, to run the test for 28x28 px input Neural Network do the following:**\r\n\r\n1. Build the test by entering the command `make -f tensorflow/lite/micro/tools/make/Makefile network_tester_test NETWORK_MODEL=~/network_tester/size_28x28/network_model.h INPUT_DATA=~/network_tester/size_28x28/input_data.h OUTPUT_DATA=~/network_tester/size_28x28/expected_output_data.h ARENA_SIZE=150000 COMPARE_OUTPUT_DATA=yes`\r\n\r\n2. Run the Test by entering the command `./tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/network_tester_test`. **Test failed**.", "I'm seeing  the same issue on ESP32, using the Arduino IDE with the Arduino TensorFlowLite library.\r\nThe hello-world example (sine_model) works well, a Fashion FMNIST model with 14x14 or 28x28 input also works, but ONLY if i do NOT use any optimizer in the TFLite converter.\r\nIf i do use the OPTIMIZE_FOR_SIZE on 14x14 model, or on the 28x28 model, the ESP32 crashes at inference, with the same backtrace decoder result in both cases:\r\n```\r\nDecoding 8 results\r\n0x4008b8ec: invoke_abort at /Users/ficeto/Desktop/ESP32/ESP32/esp-idf-public/components/esp32/panic.c line 707\r\n0x4008bb1d: abort at /Users/ficeto/Desktop/ESP32/ESP32/esp-idf-public/components/esp32/panic.c line 707\r\n0x400ded55: tflite::reference_integer_ops::FullyConnected(tflite::FullyConnectedParams const&, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, int const*, tflite::RuntimeShape const&, signed char*) at /home/tom/CODE/sketchbook/libraries/Arduino_TensorFlowLite/src/tensorflow/lite/kernels/internal/reference/integer_ops/fully_connected.h line 36 (discriminator 1)\r\n0x400df0b9: tflite::ops::micro::fully_connected::Eval(TfLiteContext*, TfLiteNode*) at /home/tom/CODE/sketchbook/libraries/Arduino_TensorFlowLite/src/tensorflow/lite/micro/kernels/fully_connected.cpp line 102\r\n:  (inlined by) tflite::ops::micro::fully_connected::Eval(TfLiteContext*, TfLiteNode*) at /home/tom/CODE/sketchbook/libraries/Arduino_TensorFlowLite/src/tensorflow/lite/micro/kernels/fully_connected.cpp line 187\r\n0x400d441e: tflite::MicroInterpreter::Invoke() at /home/tom/CODE/sketchbook/libraries/Arduino_TensorFlowLite/src/tensorflow/lite/micro/micro_interpreter.cpp line 190\r\n0x400d1ceb: setup() at /home/tom/CODE/sketchbook/libraries/Arduino_TensorFlowLite/src/tensorflow/lite/micro/micro_mutable_op_resolver.h line 105\r\n0x400e9d1b: loopTask(void*) at /home/tom/.arduino15/packages/esp32/hardware/esp32/1.0.3/cores/esp32/main.cpp line 14\r\n0x40087935: vPortTaskWrapper at /Users/ficeto/Desktop/ESP32/ESP32/esp-idf-public/components/freertos/port.c line 355 (discriminator 1)\r\n```\r\nActually this seems more related to [#36914](https://github.com/tensorflow/tensorflow/issues/36914)\r\n", "I confirmed @tomtobback notification.\r\n\r\nIf I do NOT apply TF Lite model optimization, the model works as expected with input 28x28 px.\r\n\r\nIf I convert model to TF Lite with either of `tf.lite.Optimize.OPTIMIZE_FOR_SIZE` or `tf.lite.Optimize.OPTIMIZE_FOR_LATENCY` I just receive wrong predictions.\r\n\r\nTested on ESP32, built with ESP IDF.\r\n\r\n", "@mr-goldhands Can you try optimizing your model again using the [updated hello_world example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb) and let us know if you face this issue.\r\n\r\nWe recommend testing with `tf.float32` input and output type as given in the example and then update it to `tf.int8` as given in [this code snippet](https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only)\r\n\r\n\r\n", "Closing issue due to user inactivity. Feel free to re-open it if the issue persists.", "@MeghnaNatraj , I have tried to perform testing according to your propositions [here](https://github.com/tensorflow/tensorflow/issues/37088#issuecomment-642369484).\r\nConfirmed. It works well.\r\n**Software environment:**\r\nCNN trained and converted with TF 2.2.0 installed from binaries.\r\nSources for MCU (TF micro component) has taken from TF sources SHA commit: c004b03ab7\r\n\r\n**But** I have no success using TF 2.3.0 binaries with the latest TF sources SHA commit: c60a66c4ee.", "@mr-goldhands could you provide more information (code + errors). We highly recommend using installing the TensorFlow binary via pip and then use the Python API for conversion. You can also use https://colab.research.google.com/ to convert models to TFLite for free.\r\n\r\nAnother thing to note: The input/output type can be modified only in TF2.3 (and not TF2.2). However it looks like you're facing issues in TF 2.2 - so we would need more information to debug", "@MeghnaNatraj , I tried again with TF 2.3.0 in the Google Collab:\r\nI was not able to convert and optimize the model because 'tanh' activation was the matter of the error. I was able to convert the model only with 'relu' activations in the model.\r\nI did not apply 8-bit quantization to Inputs and Outputs because ESP32 is not an 8-bit microcontroller. But I will use these options for future optimization.\r\nI have created and shared with you the collab notebook with the train and the Model conversion code.\r\n\r\nReceived error message:\r\n`RuntimeError: Quantization not yet supported for op:`", "@mr-goldhands if the error is not related to the current issue \"network_tester test failed with NN with 28x28 px input image but succeded with NN with 14x14 px input\" could you file a new bug and post a link to it here? We can look into it in a separate thread as this a converter related issue.", "@MeghnaNatraj , at the moment  I have two issues in different stages.\r\n1. I have an issue during the conversion of my model to TF Lite format. I have registered an [Issue](https://github.com/tensorflow/tensorflow/issues/42390).\r\n\r\n2. Also, I faced an issue during building the \"TF Hello World\" example. As I see the API for microcontrollers has slite changed since v2.1.0. Consequently, I need an actual TFMicro component for ESP32 for TF v2.3.0. So I have updated still open [Issue](https://github.com/tensorflow/tensorflow/issues/36189)."]}, {"number": 37087, "title": "docs: add tip to prefer tf.shape(x) over x.shape in custom layers/models", "body": "See #36991 for details.", "comments": ["@jvishnuvardhan, Can you please take a look on this PR ? Thanks!"]}, {"number": 37086, "title": "Misleading convert error when no concrete functions are given", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\n- TensorFlow installed from source:\r\n- TensorFlow version 2.1.0:\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_path)\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\r\n```\r\n\r\n**Failure details**\r\nWhen attempting to convert a model in which no ConcreteFunctions have been defined, the error message implies that there are multiple.\r\n\r\nAs someone new coming to TF and TFLite, I found this very confusing as to where my concrete functions were being defined. \r\n\r\nInspecting the code from https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/lite/python/lite.py in the ```convert``` method at line 417 the Value Error is thrown when anything but 1 concrete function is defined in the model (rightly so) but the error message implies that more than one have been defined.\r\n\r\n```\r\nif len(self._funcs) != 1:\r\n      raise ValueError(\"This converter can only convert a single \"\r\n                       \"ConcreteFunction. Converting multiple functions is \"\r\n                       \"under development.\")\r\n```", "comments": ["@PRDrum5 \r\n\r\nCan you please share the simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "Sure\r\n\r\nHere I've made a simple model but not defined the input_signature for the tf.function for the `__call__` of the model. In doing so there is **no**  concrete functions at all. But the error message which is returned implies that there are multiple and that this is the issue.\r\n\r\nMy suggestion is that the error thrown should be clear that in this scinario the user has not defined any concrete functions rather that suggesting that there exists more than one which is not the case.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense\r\n\r\n\r\n# Define very simple classification model\r\nclass Model(tf.Module):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n\r\n        self.d1 = Dense(2, activation='relu')\r\n        self.d2 = Dense(2, activation='softmax')\r\n    \r\n    @tf.function\r\n    def __call__(self, x):\r\n        print(\"Tracing the model\")\r\n        x = self.d1(x)\r\n        return self.d2(x)\r\n\r\nmodel = Model()\r\n\r\nexample_data = tf.constant([[1.0, 2.0]])\r\npreds = model(example_data)\r\ntf.print(preds)\r\n\r\n# Save the model\r\ntf.saved_model.save(model, './model_example')\r\n\r\n\r\n# Load the saved model and convert to TFLite\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('./model_example')\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n\r\n```", "I have tried on colab with TF version 2.1.0,2.2.0-dev20200227  and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/39a05cacd98c1c561c6c2b94f986db69/untitled683.ipynb). Thanks!", "@gargn for this case is it possible to provide the signature w/ the `from_saved_model` method? Or do they have to load the model and then explicitly call `from_concrete_functions`?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37086\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37086\">No</a>\n"]}, {"number": 37085, "title": "Everything's OK... but still can't import", "body": "I'm trying to use TF for some data processing.\r\nSo I installed tensorflow with pip.py\r\nand then CUDA and cuDNN.\r\n\r\nWindows : 7\r\nPython : 3.6.8\r\nPip version : 20.0.2\r\nSetuptools version : 45.2.0\r\nTensorflow : 2.1.0 I think\r\nCUDA : 10.2\r\ncuDNN : 6.0 (I also tried with 7.6.4.38)\r\nVisual Studio : Visual Studio Community, Microsoft Visual C++ Redistributable 14.24, Build Tools\r\n\r\nI formerly used the script tensorflow_self_check.py below to know what was wrong, but now when I run it this script says that the import fails but that everything is here.\r\n\r\n![040](https://user-images.githubusercontent.com/61496017/75330672-bcd01380-5881-11ea-8cfb-a77a824f7647.png)\r\n\r\n\r\nand when I import \r\n\r\n![039](https://user-images.githubusercontent.com/61496017/75330642-b3df4200-5881-11ea-9f56-f75aab199613.png)\r\n\r\n\r\nWhat do I have to do for the import to be successful now ?\r\n\r\n\r\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\"\"\"A script for testing that TensorFlow is installed correctly on Windows.\r\nThe script will attempt to verify your TensorFlow installation, and print\r\nsuggestions for how to fix your installation.\r\n\"\"\"\r\n\r\nimport ctypes\r\nimport imp\r\nimport sys\r\n\r\ndef main():\r\n  try:\r\n    import tensorflow as tf\r\n    print(\"TensorFlow successfully installed.\")\r\n    if tf.test.is_built_with_cuda():\r\n      print(\"The installed version of TensorFlow includes GPU support.\")\r\n    else:\r\n      print(\"The installed version of TensorFlow does not include GPU support.\")\r\n    sys.exit(0)\r\n  except ImportError:\r\n    print(\"ERROR: Failed to import the TensorFlow module.\")\r\n\r\n  candidate_explanation = False\r\n\r\n  python_version = sys.version_info.major, sys.version_info.minor\r\n  print(\"\\n- Python version is %d.%d.\" % python_version)\r\n  if not (python_version == (3, 5) or python_version == (3, 6)):\r\n    candidate_explanation = True\r\n    print(\"- The official distribution of TensorFlow for Windows requires \"\r\n          \"Python version 3.5 or 3.6.\")\r\n\r\n  try:\r\n    _, pathname, _ = imp.find_module(\"tensorflow\")\r\n    print(\"\\n- TensorFlow is installed at: %s\" % pathname)\r\n  except ImportError:\r\n    candidate_explanation = False\r\n    print(\"\"\"\r\n- No module named TensorFlow is installed in this Python environment. You may\r\n  install it using the command `pip install tensorflow`.\"\"\")\r\n\r\n  try:\r\n    msvcp140 = ctypes.WinDLL(\"msvcp140.dll\")\r\n  except OSError:\r\n    candidate_explanation = True\r\n    print(\"\"\"\r\n- Could not load 'msvcp140.dll'. TensorFlow requires that this DLL be\r\n  installed in a directory that is named in your %PATH% environment\r\n  variable. You may install this DLL by downloading Microsoft Visual\r\n  C++ 2015 Redistributable Update 3 from this URL:\r\n  https://www.microsoft.com/en-us/download/details.aspx?id=53587\"\"\")\r\n\r\n  try:\r\n    cudart64_80 = ctypes.WinDLL(\"cudart64_80.dll\")\r\n  except OSError:\r\n    candidate_explanation = True\r\n    print(\"\"\"\r\n- Could not load 'cudart64_80.dll'. The GPU version of TensorFlow\r\n  requires that this DLL be installed in a directory that is named in\r\n  your %PATH% environment variable. Download and install CUDA 8.0 from\r\n  this URL: https://developer.nvidia.com/cuda-toolkit\"\"\")\r\n\r\n  try:\r\n    nvcuda = ctypes.WinDLL(\"nvcuda.dll\")\r\n  except OSError:\r\n    candidate_explanation = True\r\n    print(\"\"\"\r\n- Could not load 'nvcuda.dll'. The GPU version of TensorFlow requires that\r\n  this DLL be installed in a directory that is named in your %PATH%\r\n  environment variable. Typically it is installed in 'C:\\Windows\\System32'.\r\n  If it is not present, ensure that you have a CUDA-capable GPU with the\r\n  correct driver installed.\"\"\")\r\n\r\n  cudnn5_found = False\r\n  try:\r\n    cudnn5 = ctypes.WinDLL(\"cudnn64_5.dll\")\r\n    cudnn5_found = True\r\n  except OSError:\r\n    candidate_explanation = True\r\n    print(\"\"\"\r\n- Could not load 'cudnn64_5.dll'. The GPU version of TensorFlow\r\n  requires that this DLL be installed in a directory that is named in\r\n  your %PATH% environment variable. Note that installing cuDNN is a\r\n  separate step from installing CUDA, and it is often found in a\r\n  different directory from the CUDA DLLs. You may install the\r\n  necessary DLL by downloading cuDNN 5.1 from this URL:\r\n  https://developer.nvidia.com/cudnn\"\"\")\r\n\r\n  cudnn6_found = False\r\n  try:\r\n    cudnn = ctypes.WinDLL(\"cudnn64_6.dll\")\r\n    cudnn6_found = True\r\n  except OSError:\r\n    candidate_explanation = True\r\n\r\n  if not cudnn5_found or not cudnn6_found:\r\n    print()\r\n    if not cudnn5_found and not cudnn6_found:\r\n      print(\"- Could not find cuDNN.\")\r\n    elif not cudnn5_found:\r\n      print(\"- Could not find cuDNN 5.1.\")\r\n    else:\r\n      print(\"- Could not find cuDNN 6.\")\r\n      print(\"\"\"\r\n  The GPU version of TensorFlow requires that the correct cuDNN DLL be installed\r\n  in a directory that is named in your %PATH% environment variable. Note that\r\n  installing cuDNN is a separate step from installing CUDA, and it is often\r\n  found in a different directory from the CUDA DLLs. The correct version of\r\n  cuDNN depends on your version of TensorFlow:\r\n\r\n  * TensorFlow 1.2.1 or earlier requires cuDNN 5.1. ('cudnn64_5.dll')\r\n  * TensorFlow 1.3 or later requires cuDNN 6. ('cudnn64_6.dll')\r\n\r\n  You may install the necessary DLL by downloading cuDNN from this URL:\r\n  https://developer.nvidia.com/cudnn\"\"\")\r\n\r\n  if not candidate_explanation:\r\n    print(\"\"\"\r\n- All required DLLs appear to be present. Please open an issue on the\r\n  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues\"\"\")\r\n\r\n  sys.exit(-1)\r\n\r\nif __name__ == \"__main__\":\r\n  main()", "comments": ["isn't CUDA10.1 should be use and cudnn for right version?", "@Seliotna \r\ncould you please use CUDA : 10.1\r\nLlet us know if it helps resolve your issue", "I uninstalled CUDA 10.2 and installed CUDA 10.1 and the output is still the same.\r\nBut before tensorflow_self_check.py said that I  needed the file \"cudart64_80.dll\", and I solved this by downloading just this file and copying it in the right directory. Should I have installed CUDA 8.0 ?", "@Seliotna \r\nPLease set the CUDA %PATH% environmental variable as guided in this [link](https://www.tensorflow.org/install/gpu#windows_setup)", "Thanks, but still no difference...", "@Seliotna \r\nPlease refer to this [comment](https://github.com/tensorflow/tensorflow/issues/18503#issuecomment-485803790) and let us know if it helps.", "The fact is that on this computer I cannot use the conda file because whatever I try to install with itn the output is \r\n![07](https://user-images.githubusercontent.com/61496017/75552578-e0888a80-5a36-11ea-970b-e1c7fbbad2a9.png)\r\n", "And it's the same with pip.py from Anaconda Scripts", "@Seliotna \r\nIn that case please refer to this [link](https://github.com/conda/conda/issues/6064#issuecomment-463219171)", "The link to the patch doesn't seem to be available anymore. And with the database of the second link, I downloaded the file conda-4.8.2-py38_0.tar.bz2, and now it puts me \r\n![08](https://user-images.githubusercontent.com/61496017/75558096-8ccf6e80-5a41-11ea-80e8-f02b8d9cbda4.png)\r\nand I tried 4 or 5 times it changes nothing\r\n", "@Seliotna \r\nconda install fabio is not related to tensorflow, please check in the relevant repository.\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nHence moving this to resolved status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37085\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37085\">No</a>\n"]}, {"number": 37084, "title": "Reintroduced Typo in tf.keras.layers.Attention docs example", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention\r\n\r\n## Description of issue (what needs changing):\r\nTypo was reintroduced in v2.1\r\nSee https://github.com/tensorflow/tensorflow/issues/34809 \r\n\r\nmust be \r\n\r\n```\r\nvalue_embeddings = token_embedding(value_input)\r\n```\r\n\r\n@rabitt", "comments": ["@ulf1 , It is exactly same as you are telling. See the nightly version of the doc [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention?version=nightly)", "@ashutosh1919 Ok."]}, {"number": 37083, "title": "differentiaing layer and it's test code is implemented.", "body": "A keras layer which differentiate 1D temporal data with respect to step.", "comments": ["Adding @fchollet here as the API owner.\r\n\r\n@shmoon-kr, thanks for sending the PR. Since this is extension to the current Keras layer API, we want to make sure it is adding value and align with other existing layers. For the current PR you sent, its unclear that my whether it is a good fit for a new layer, it more just like a ops that does the diff, and doesn't necessary to be a layer.", "Also the weights of the layer are just constants, which probably isn't a good usage of variable as they never gets updated.", "Thanks for a quick and detailed review. In my recent research, I found that sometimes it's better to use a differentiated vector instead. (https://www.mdpi.com/2077-0383/8/9/1419/htm) I thought the idea could be leveraged in other applications (especially for time series data) so I prepared for a contribution to the project.\r\nI implemented differentiating by multiplying a constant matrix to a tensor as you said. The reason why I wrote a new layer (instead of just defining a constant matrix and multiplying it) is that the size of the constant matrix varies with the shape of an input tensor. Without a layer I should calculate the shape of an input tensor, define an appropriate constant matrix, permute an input tensor and multiply it by the constant matrix for each layer to differentiate. In the beginning I did so but the code is very complicated so I used a custom layer later. With the new layer I proposed, all of the above is done in just a line. It's the value I want to add to the project.", "> Thanks for a quick and detailed review. In my recent research, I found that sometimes it's better to use a differentiated vector instead. (https://www.mdpi.com/2077-0383/8/9/1419/htm) I thought the idea could be leveraged in other applications (especially for time series data) so I prepared for a contribution to the project.\r\n> I implemented differentiating by multiplying a constant matrix to a tensor as you said. The reason why I wrote a new layer (instead of just defining a constant matrix and multiplying it) is that the size of the constant matrix varies with the shape of an input tensor. Without a layer I should calculate the shape of an input tensor, define an appropriate constant matrix, permute an input tensor and multiply it by the constant matrix for each layer to differentiate. In the beginning I did so but the code is very complicated so I used a custom layer later. With the new layer I proposed, all of the above is done in just a line. It's the value I want to add to the project.\r\n\r\nIf we step back, I think the same functionality can also be done without being a layer. If you defined a function with the input tensor as input param, the body of the function will just be all the actions u described above. I don't see any value being added as layer, comparing to a just a normal python function.", "> > Thanks for a quick and detailed review. In my recent research, I found that sometimes it's better to use a differentiated vector instead. (https://www.mdpi.com/2077-0383/8/9/1419/htm) I thought the idea could be leveraged in other applications (especially for time series data) so I prepared for a contribution to the project.\r\n> > I implemented differentiating by multiplying a constant matrix to a tensor as you said. The reason why I wrote a new layer (instead of just defining a constant matrix and multiplying it) is that the size of the constant matrix varies with the shape of an input tensor. Without a layer I should calculate the shape of an input tensor, define an appropriate constant matrix, permute an input tensor and multiply it by the constant matrix for each layer to differentiate. In the beginning I did so but the code is very complicated so I used a custom layer later. With the new layer I proposed, all of the above is done in just a line. It's the value I want to add to the project.\r\n> \r\n> If we step back, I think the same functionality can also be done without being a layer. If you defined a function with the input tensor as input param, the body of the function will just be all the actions u described above. I don't see any value being added as layer, comparing to a just a normal python function.\r\n\r\nOK. If you're asking \"Is a new layer the only way to do it?\" The answer is no and I agree with you. But last year when I tried to find a way to differentiate a tensor, there was no such an example. So it took a few days to implement it. If there was a Keras layer I proposed by that time, I could have done it in a few minutes. If there's somebody else who takes a similar approach, we can save his/her time a lot by adding the new layer to Keras APIs. And if it is listed in Keras APIs, more people will see it and try in their problems and it's a good contribution to our society. Now it seems totally up to you. I understand your point but I think it still have a value though it's not essential.", "I think the functionality proposed in this PR is a not common use case that is widely used, and Keras layers are designed to be common building blocks for DL model, and this PR is not fit for the core Keras API.\r\n\r\nYou could try propose this to tensorflow/addons, which is a place for extended functionality for tensorflow and Keras. See contribution guild line at https://github.com/tensorflow/addons/blob/master/CONTRIBUTING.md.\r\n\r\nI am closing this PR for now since we can't accept at the moment. "]}, {"number": 37082, "title": "Merge (#1)", "body": "* Raise error if we hit an unknown exception while reading file from s3\r\n\r\n* Add forbidden error check\r\n\r\n* Added relevant changes for retrying operations in the s3 file system\r\nAdd header as in master for building the test, and change error type for delete dir\r\nRegister retrying file system for s3\r\nCompile retrying s3 file system\r\nAdd forbidden errors, add mtime to the stat function, add if check around creating file in createDir\r\nMove read error bug to separate PR for easier reviews\r\n\r\nLint fix with buildifier\r\n\r\n* Refactored error handling to separate methods for reuse\r\n\r\n* Make function static\r\n\r\n* Fix link failure for tests\r\n\r\n* update ragged_string_ops.py comment\r\n\r\n* [Intel Mkl] Upgrade Sqlite3 to fix CVE-2019-19880 CVE-2019-19244 and CVE-2019-19645\r\n\r\n* Use CreateStatusFromAwsError for the read error\r\n\r\n* make gpu delegate build\r\n\r\nit seems this line was forgotten when adding space_to_depth\r\n\r\n* Depthwise convolution 3x3 per-channel int8 for dot-product ARM (17x).\r\n\r\nDisable dot-product path temporarily.\r\n\r\nPiperOrigin-RevId: 297134519\r\nChange-Id: Ib00acb656471143fd31ca39e6762d46e9dec994a\r\n\r\n* Add method that returns the version for hexagon_interface.\r\nThis will be used to compare version between hexagon_interface and hexagon_skel libraries.\r\n\r\nPiperOrigin-RevId: 297138592\r\nChange-Id: I23e1279fa5baa9cb262308f00d5f7f55f1256221\r\n\r\n* Use strategy properties to check if a worker should checkpoint instead of use the TF_CONFIG. Using TF_CONFIG will cause errors if we use a cluster resolver to initialize the strategy.\r\n\r\nPiperOrigin-RevId: 297139161\r\nChange-Id: I7f13c7b7ea6588e020011569b26eb0bf49893bbf\r\n\r\n* Enabling non trivial fast tuning for all vendors.\r\n\r\nPiperOrigin-RevId: 297142417\r\nChange-Id: I948e0ae458fbf59841d83d3c087c35c286daf269\r\n\r\n* Update Eigen to:\r\nhttps://gitlab.com/libeigen/eigen/-/commit/52a2fbbb008a47c5e3fb8ac1c65c2feecb0c511c\r\n\r\nPiperOrigin-RevId: 297148140\r\nChange-Id: I71fa876fcf492dc0051be3c3eecd12407770220c\r\n\r\n* Qualify uses of std::string\r\n\r\nPiperOrigin-RevId: 297148457\r\nChange-Id: Id91f1da7d30e9a9880d8610c36ef8657f12817f0\r\n\r\n* Improving snapshot read performance (under snappy compression) by reducing the\r\nnumber of copies by 2.\r\n\r\nWe do the following in this CL.\r\n\r\n1) Get rid of the snappy input and output buffers that were previously being\r\nused to do the compression. This saved one copy.\r\n2) Directly decompress the compressed bytes into the TensorBuffer for simple\r\ntypes (not string, variant, resource). This saves another copy during Tensor\r\ncreation. For complex types, we still continue to use the TensorProto encoding\r\nand pay a copy there.\r\n3) As a result, we end up changing the on-disk format for Snapshot. For a group\r\nof tensors that make up one element of an IteratorGetNext output, we first\r\nwrite out a metadata proto that describes the types, shapes and sizes of\r\ntensors. After that we lay out the Tensor data (TensorBuffers for simple types\r\nand TensorProtos serialized for complex ones) and compress them via snappy.\r\n4) Add a version to the SnapshotMetadata. If it isn't set its assumed to be 0\r\nand the old code path runs. We now set it to 1 while writing so that all new\r\nsnapshots are written in this data format.\r\n\r\nPiperOrigin-RevId: 297149479\r\nChange-Id: I2c9a35c5a254189a5fad946b2995f25cdc452308\r\n\r\n* update_bazel_macos is not used any more and has been replaced with install_bazelisk\r\n\r\nPiperOrigin-RevId: 297149906\r\nChange-Id: Ibcc1a114468a57cdf5314a7499e5a4ad32b8e941\r\n\r\n* Replaced tools with exec_tools in genrules.\r\n\r\nPiperOrigin-RevId: 297150637\r\nChange-Id: I4b807e07a6b9072d53191ee7c2c3634cc0ca8e98\r\n\r\n* Check for TENSORFLOW_LITE_PROTOS instead of __ANDROID__ when checking whether the full/lite protobuf implementation is being used.\r\n\r\nPiperOrigin-RevId: 297150726\r\nChange-Id: I5b7930811ef4410bfb85465736749558d6d630f1\r\n\r\n* [tfdbg2] Fix a copy-paste error in unit test code\r\n\r\nFixes https://github.com/tensorflow/tensorflow/issues/36934\r\n\r\nPiperOrigin-RevId: 297162227\r\nChange-Id: Idcc10fdf84cfeabd6f136725923ef39ea27c4e04\r\n\r\n* Fix an issue with out of order execution on a corner cases when there is multiple remote pending requests and remote functions with remote inputs.\r\n\r\nSpecifically:\r\n1. We keep track the\u00a0last device used to execute a remote function with remote inputs\r\n2. Before executing a remote function, if the remote device is different than the last tracked device, we insert a sync point.\r\nPiperOrigin-RevId: 297163787\r\nChange-Id: I944669b92dd1b50cdf213c73e1cdf990c9011e00\r\n\r\n* Fix the deprecation_test for py3.8\r\n\r\nPiperOrigin-RevId: 297168125\r\nChange-Id: I441c6cd7d85fc8e8f8aedb53d1b2ec69594d9bcd\r\n\r\n* Add tf.MatrixBandPartOp ODS definition and corresponding verifier.\r\n\r\nPiperOrigin-RevId: 297174451\r\nChange-Id: I77400245d526dc647dd17f532b935bf6f942db62\r\n\r\n* Fix device placement logic in ConvertToEagerTensor.\r\n\r\nThe fix consists in always creating a host tensor: the inputs to\r\nConvertToEagerTensor are host Python objects, so it makes sense that\r\nthe created tensor should be a host tensor too. The user can control\r\nGPU copies by using tf.identity.\r\n\r\nPiperOrigin-RevId: 297174740\r\nChange-Id: I01f2aa9be3eb29fd49c7d81823e044db292b2d7c\r\n\r\n* *In dilated conv pass, set shape for expand_dims result correctly.\r\n*Put more checks on input conv2d's dilation attributes, padding type, etc.\r\n\r\nPiperOrigin-RevId: 297174875\r\nChange-Id: Ibc59cc80a091e98d3eb1f7a855a4ee08402d7d96\r\n\r\n* Experimental prototype of BertTokenizer  for tflite support library\r\n\r\nPiperOrigin-RevId: 297176134\r\nChange-Id: I4fea84735d1686f1ae1e98f4433bd0e29a642d35\r\n\r\n* Remove ops.device(None).__enter__ from eager:core_test.\r\n\r\nThose statements are not necessary for the test to consistently pass.\r\n\r\nPiperOrigin-RevId: 297180465\r\nChange-Id: I7dd6798b56fa7f80b8170ddc92ed811370bffd2f\r\n\r\n* Bump open source llvm revision to fee41517fe0f7ff9f0e204dd9200ebf32ca03cb8\r\n\r\nPiperOrigin-RevId: 297184332\r\nChange-Id: Ide9c91a6171350d9eadb406f3ed028bcdbc33cc9\r\n\r\n* Add `shape_signature` to visualize.py.\r\n\r\nPiperOrigin-RevId: 297194460\r\nChange-Id: I9b08d418bf75b6b5f4e2e2f7b05c84932d8bab46\r\n\r\n* Expose quantization parameters in the TFLite Java API\r\n\r\nPiperOrigin-RevId: 297207151\r\nChange-Id: Iff5a1e720df750d0c90710e5209bcabc53054150\r\n\r\n* Go: Update generated wrapper functions for TensorFlow ops.\r\n\r\nPiperOrigin-RevId: 297207526\r\nChange-Id: I92b95973a96d2c8641b5c13d4c8addf7687df1c9\r\n\r\n* Update hexagon_nn_headers to v1.10.3.1.3\r\nChanges Includes:\r\n* Support soc_id:371\r\n* New method exposed that returns the version of hexagon_nn used in libhexagon_interface.so\r\n\r\nPiperOrigin-RevId: 297212018\r\nChange-Id: I5c3396f04b305ea6197e8c693c5cd06c789e8d8d\r\n\r\n* Qualify uses of std::string\r\n\r\nPiperOrigin-RevId: 297212802\r\nChange-Id: Ic65150e7ab418be034f48d45ce25ef5d19105836\r\n\r\n* Internal refactoring for metrics.\r\n\r\nPiperOrigin-RevId: 297214293\r\nChange-Id: I9ef1510677669bc9c21de5839fde7ed42421fb6d\r\n\r\n* Add test for connecting to v2-32 Cloud TPU.\r\n\r\nPiperOrigin-RevId: 297219348\r\nChange-Id: I6072853736fc98badbca598850f850b9980cc7b0\r\n\r\n* Fix Concat behavior in Inception V3 quant\r\n\r\nPiperOrigin-RevId: 297220987\r\nChange-Id: Icd045f9dac3a7cd6aa42bf5070c206bc7b375046\r\n\r\n* Minor change in update script\r\n\r\nPiperOrigin-RevId: 297221831\r\nChange-Id: Ibc864c21ad7bd277ffe00caecb9751629f89d97b\r\n\r\n* Extend the optimize global tensors pass with resource analysis\r\n\r\nInter-procedural resource analysis for identifying resource as mutable or not.\r\n\r\nPiperOrigin-RevId: 297225621\r\nChange-Id: I12690ddab584660792c216347efa7a265fa6ffee\r\n\r\n* Add check for hexagon version between interface and skel.\r\nlibhexagon_interface and libhexagon_nn_skel should be used with matching versions. Users shouldn't use mixed versions as it will not work correctly.\r\n\r\nPiperOrigin-RevId: 297239641\r\nChange-Id: I4e82626fc395eaba23fd51f84fbb402e24b2fd8a\r\n\r\n* Improves documentation for keras.preprocessing.pad_sequence\r\n\r\nPiperOrigin-RevId: 297247197\r\nChange-Id: I4c85e8ba6d4ae43d4c249442ef9c47bb6a5805c6\r\n\r\n* Go: Update generated wrapper functions for TensorFlow ops.\r\n\r\nPiperOrigin-RevId: 297248087\r\nChange-Id: Ibc3efb37f0f855fb86f8d0915c17c5bfb26847d3\r\n\r\n* Upgrade nanopb to latest release.\r\n\r\nFixes security vulnerability reported in #37011\r\n\r\nPiperOrigin-RevId: 297248686\r\nChange-Id: Ib24b109c8a36b8673842b2f2ebb771406e5fa022\r\n\r\n* Split the source files into ESP IDF components by their paths.\r\n\r\nPiperOrigin-RevId: 297249461\r\nChange-Id: I16c81c416ab803d4ba82cc4ae801b35a00be1acc\r\n\r\n* Special-case variable reads in auto control deps.\r\n1. Variable reads, with no write in between, are no longer performed in a sequence. This included reads happening inside functional ops e.g. nested functions or control flow.\r\n2. Variable reads are no longer added to the function's control outputs.\r\n\r\nPiperOrigin-RevId: 297253117\r\nChange-Id: I833891f644cf6f8c1b733401ed2e3de3adb9ef0e\r\n\r\n* Add GPU kernel stats to cloud tools.\r\n\r\nPiperOrigin-RevId: 297267405\r\nChange-Id: Ia3f0ac52ccb1db07f0b6230f9b181e8f2e316c04\r\n\r\n* Set model.history after `on_epoch_end` in `History` class.\r\n\r\nPiperOrigin-RevId: 297267664\r\nChange-Id: I0cf97412913bdfe3599d74f162d172f02e7ceac9\r\n\r\n* Switch ConcatOp, PackOp, SelectOp, and SelectV2Op to access inputs by index (not name).\r\n\r\nThese kernels are heavily used, and the name resolution on each invocation causes non-trivial overhead.\r\n\r\nPiperOrigin-RevId: 297267975\r\nChange-Id: Id69de0e2cff3622e992389c16e020a5da3141462\r\n\r\n* Add an experimental_hints to batch all reduce\r\n\r\nThis contains all performance hints to the API. Currently there's only bytes_per_pack, which splits large batches into multiple packs allows overlapping communication and computation.\r\n\r\nCurrently we can only pack if all Tensors in the batch have known shapes.\r\n\r\nPiperOrigin-RevId: 297269428\r\nChange-Id: Iaf7d7d3adf7c6cad59aa6079fbcd36b31e92c4b5\r\n\r\n* Set resource as mutable when unknown op is encountered in the optimize global tensors pass\r\n\r\nPiperOrigin-RevId: 297274143\r\nChange-Id: Ia33df38d23875a1a4021c52e70fca17b8aeb9c96\r\n\r\n* [OpKernel] Implement `OpKernelContext::output_required(int)`.\r\n\r\nThis long-pending feature enables kernels to specialize for the case where not all of their outputs may need to be produced. As an example, this change uses `OpKernelContext::output_required()` in `SparseFillEmptyRowsOp`, which often produces unused outputs for the purpose of backpropagation; using `output_required()` we can save the related allocations in inference workloads.\r\n\r\nI did consider implementing this as a Grappler rewrite pass, but the example of SparseFillEmptyRowsOp convinced me that (given the present state) a runtime test was better. The rewrite-based alternative would require me to have four similar op registrations for SparseFillEmptyRows (since we now have two optional outputs that may be present or absent, so there are 2^2 possible signatures), and to devise a mapping/registration scheme that the rewriter might use to substitute different implementations. By contrast, the runtime check is (i) pretty cheap compared to op dispatch, and (ii) easy to implement gradually and locally.\r\n\r\nPiperOrigin-RevId: 297274989\r\nChange-Id: I23b5207017921ba118bd4fc31dc54ff53fe4332d\r\n\r\n* Include FreeRTOS.h before other dependencies demending this prior inclusion.\r\n\r\nPiperOrigin-RevId: 297275456\r\nChange-Id: I624bc648276a4cad3ca481cc2433fd29754ddfa6\r\n\r\n* Inject keras.preprocessing modules into the doctest.\r\n\r\nPiperOrigin-RevId: 297276571\r\nChange-Id: I76c18e90f02ea6d0ef7ad69dae16e231734efc8d\r\n\r\n* Fix the BatchNorm v2 performance issue wrt \"training\".\r\n\r\nChange the logic to skip the math_ops.logical_and() when layer.trainable is True, since the \"training\" value is only going to be changed if layer.trainable is False.\r\n\r\nThis apparently fix the regression issue (1.33x). I think the root cause might be the logical_and() with a value that is not the GPU device (layer.trainable).\r\n\r\nPiperOrigin-RevId: 297277503\r\nChange-Id: I9e3b6650bfa58c3d2ea8e3f023a3bd1a5160cc8d\r\n\r\n* Add warmup in keras tb callback to improve profiling accuracy.\r\n\r\nPiperOrigin-RevId: 297277543\r\nChange-Id: Ic78a286400fffa73d583f3bb27bfce0dc1e275dc\r\n\r\n* Go: Update generated wrapper functions for TensorFlow ops.\r\n\r\nPiperOrigin-RevId: 297278005\r\nChange-Id: I7ae629b04ffb9c44c732dac221b0821f86b282e8\r\n\r\n* Consolidate via CallOpInterface the various TF call ops handling in optimize global tensors pass\r\n\r\nPiperOrigin-RevId: 297283013\r\nChange-Id: Ibd5a306b662d61ef2644a6cf59f78ac0119aa02e\r\n\r\n* Add go_backwards support for keras fused lstm\r\n\r\nPiperOrigin-RevId: 297287353\r\nChange-Id: Idaebe3d0c84fc8be03651233a4af7c9cd46a23ca\r\n\r\n* Go: Update generated wrapper functions for TensorFlow ops.\r\n\r\nPiperOrigin-RevId: 297288470\r\nChange-Id: I86fd6955e320399697a6f1082c8a98efdd09d639\r\n\r\nCo-authored-by: Rahul Huilgol <rahulhuilgol@gmail.com>\r\nCo-authored-by: HUAN-PING SU <pingsutw@gmail.com>\r\nCo-authored-by: Clayne Robison <clayne.b.robison@intel.com>\r\nCo-authored-by: \"freedom\" Koan-Sin Tan <koansin.tan@gmail.com>\r\nCo-authored-by: TensorFlower Gardener <gardener@tensorflow.org>\r\nCo-authored-by: Alex Stark <539273+jalexstark@users.noreply.github.com>\r\nCo-authored-by: Karim Nosseir <44206880+karimnosseir@users.noreply.github.com>\r\nCo-authored-by: anj-s <32556631+anj-s@users.noreply.github.com>\r\nCo-authored-by: Rohan Jain <rohan100jain@gmail.com>\r\nCo-authored-by: Brian Atkinson <nairb774@gmail.com>\r\nCo-authored-by: Mihai Maruseac <mihai.maruseac@gmail.com>\r\nCo-authored-by: Shanqing Cai <cais@google.com>\r\nCo-authored-by: Bramandia Ramadhana <bramandia@gmail.com>\r\nCo-authored-by: lucyrfox <lucyfox@google.com>\r\nCo-authored-by: Haoliang Zhang <haoliang@google.com>\r\nCo-authored-by: thuang513 <556234+thuang513@users.noreply.github.com>\r\nCo-authored-by: River Riddle <riddleriver@gmail.com>\r\nCo-authored-by: Nupur Garg <nupurgarg@gmail.com>\r\nCo-authored-by: lu-wang-g <47436172+lu-wang-g@users.noreply.github.com>\r\nCo-authored-by: Revan Sopher <rsopher@gmail.com>\r\nCo-authored-by: Sachin Joglekar <srjoglekar@google.com>\r\nCo-authored-by: sun51 <yanh.sun@gmail.com>\r\nCo-authored-by: Ashwin Murthy <ashwinm@google.com>\r\nCo-authored-by: Fr\u00e9d\u00e9ric Rechtenstein <fred.rec@gmail.com>\r\nCo-authored-by: Saurabh Saxena <saxenasaurabh@users.noreply.github.com>\r\nCo-authored-by: Situ Yi <60493024+yisitu@users.noreply.github.com>\r\nCo-authored-by: Yash Katariya <yashkatariya@google.com>\r\nCo-authored-by: Derek Murray <derek.murray@gmail.com>\r\nCo-authored-by: Ran Chen <crccw@google.com>\r\nCo-authored-by: Qianli Scott Zhu <scottzhu@google.com>\r\nCo-authored-by: renjie-liu <36247193+renjie-liu@users.noreply.github.com>", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to determine that you authored the commits in this PR.  Maybe you used a different email address in the git commits than was used to sign the CLA?  If someone else authored these commits, then please add them to this pull request and have them confirm that they're okay with them being contributed to Google.  If there are co-authors, make sure they're formatted properly.\n\nIn order to pass this check, please resolve this problem and then comment`@googlebot I fixed it.`.. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37082) for more info**.\n\n<!-- unknown_author -->", "Sincere apologies for the mistake. "]}, {"number": 37081, "title": "[Intel MKL] Compilation fixes for DNNL 1.0", "body": "This PR makes following changes:\r\n\r\n1. Enables MklLayoutPass for DNNL 1.0 integration\r\n2. Makes certain changes to MKL kernels to fix compilation issues introduced by DNNL 1.0\r\n3. Ports remaining features such as weight caching to DNNL 1.0 API\r\n4. Adds `exponential_avg_factor` attribute to MklFusedBatchNorm ops\r\n5. Adds BUILD support for DNNL 1.0\r\n\r\n**NOTE**: Changes in `mkl_fused_batchnorm_op.cc` file reflect most of the changes made by PR #36767. Once this PR is merged to master, most of the changes in this file would go away.", "comments": ["Hi @penpornk, pls take a look whenever you get chance.", "> There are two reformattings that shouldn't happen. Are you using `clang-format` 8.0.1?\r\n\r\nYes, we are using clang+llvm-8.0.1. These changes were suggested by that tool.", "Got it. I'll try to find the version that works some time later then T-T. Please revert by hand for now.", "@penpornk Thanks for quick review and approval. I've submitted another PR #37106 for making DNNL 1.2 default."]}, {"number": 37080, "title": "Update docstring for Conv3DTranspose", "body": "As mentioned in #29841, `dillation_rate` is not a supported feature for `Conv3DTranspose` as of yet. Until this feature is supported, it is misleading to show `dilation_rate` as an available parameter in the docstring.", "comments": ["Thanks for the PR. This is fixed, i.e., conv3d_transpose should support [dilation_rate](https://github.com/tensorflow/tensorflow/commit/46c79db67db33d7a14e124fc0b5cf23e0910d039). Closing it for now. Let us know if any other issues come up"]}, {"number": 37079, "title": "Cannot use tape.gradient to calculate two networks with one loss simultaneously", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): none\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n\r\nIn a minimize problem, Ithe purpose is to minimize a function(loss function) which is calculated by two networks. I try to train two networks with one custom loss function. When calculating the gradient, I found that `tape.gradient(target, sources)` usually use `model.trainable_variables` as source. But in my case, there are two models to be trained simultaneously, they should be updated together. SoI write `sources= pModel.trainable_variables+gModel.trainable_variables ` and use the same way to write `apply_gradients` as below:\r\n```\r\nfor i_batch in tf.range(n_batch):\r\n    learning_rate = 0.01\r\n    opt = tf.keras.optimizers.Adam(learning_rate)\r\n    with tf.GradientTape() as tape:\r\n        Loss = calculate_loss(pModel, gModel, V, RP, lam_1, lam_2, i_batch, batchsize)\r\n        #Loss is a custum function, gModel and pModel are two sequential model written by Keras\r\n    d = tape.gradient(target=Loss, sources= pModel.trainable_variables+gModel.trainable_variables)         \r\n    opt.apply_gradients(zip(d, pModel.trainable_variables+gModel.trainable_variables))\r\n```\r\nCan I use `sources= pModel.trainable_variables+gModel.trainable_variables` as sources? Does the simple + work as I think? \r\n(I tried , it can work but the training results are not good. )", "comments": ["@nsssss \r\n\r\nWill it be possible to share the colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "> @nsssss\r\n> \r\n> Will it be possible to share the colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!\r\n\r\nI attatched my code. It can be run but the results are not as expected. I'm not sure if `sources= pModel.trainable_variables+gModel.trainable_variables` is correct. ", "@nsssss \r\n\r\nI have tried on colab with TF version 2.1.0 Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/c18418d5ce9f3654157ae874ed63b0b5/untitled678.ipynb). Is this the expected behavior?Thanks!", "> @nsssss\r\n> \r\n> I have tried on colab with TF version 2.1.0 Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/c18418d5ce9f3654157ae874ed63b0b5/untitled678.ipynb). Is this the expected behavior?Thanks!\r\n\r\nThanks! The format looks right but the values are not expected. I guess the problem was on `sources= pModel.trainable_variables+gModel.trainable_variables`. I'm not sure if I can simply add (\"+\") two list of trainable variables in two models when calculate and apply gradients . Do you think it is correct?", "Does anyone know the answer?", "Your question reminds me of a project. The link is [here](https://github.com/zhaoyingjun/chatbot). I hope it's helpful.", "> Your question reminds me of a project. The link is [here](https://github.com/zhaoyingjun/chatbot). I hope it's helpful.\r\n\r\nThank you so much. It's really helpful.", "Simply add the variables lists in `tape.gradient` and `opt.apply_gradients` is really work. \r\nTest as below: \r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\ni = 0\r\nbatchsize = 16\r\nQ = 1000\r\nn_batch = Q//batchsize #total batch number\r\nX = tf.constant(tf.ones([Q,1]))\r\n#y = np.random.uniform(low=-10.0, high=10.0, size=(Q,1))\r\nY = tf.constant(tf.ones([Q,1]))\r\n\r\ngModel = tf.keras.Sequential([\r\nlayers.Dense(2, input_shape=(1,), use_bias = False),\r\nlayers.Dense(1 , use_bias = False)\r\n])\r\n\r\npModel = tf.keras.Sequential([\r\nlayers.Dense(2, input_shape=(1,), use_bias = False),\r\nlayers.Dense(1, use_bias = False),\r\n])\r\n\r\ndef Loss_function(gModel, pModel, i):\r\n    loss = tf.square(tf.subtract(tf.multiply(gModel(X[i:i+1]), pModel(X[i:i+1])) , 2))\r\n    return loss\r\n\r\nfor j in range(Q):\r\n    learning_rate = 0.0001\r\n    opt = tf.keras.optimizers.Adam(learning_rate)\r\n    # calculate gradient\r\n    with tf.GradientTape() as tape:\r\n        Loss = Loss_function(gModel, pModel, j)\r\n    d = tape.gradient(target=Loss,  sources= pModel.trainable_variables+gModel.trainable_variables)  \r\n    opt.apply_gradients(zip(d, pModel.trainable_variables+gModel.trainable_variables))\r\n    if j % 100 == 0:\r\n        print(Loss)\r\n```", "I have similar problem. When I add the variables then I get following error in apply_gradiants:\r\nDo you have any idea? I have two models to train but the dimension of inputs are different.\r\n\r\n\r\nValueError: in user code:\r\n\r\n    <ipython-input-170-a184f0a52f78>:37 ts_train_step  *\r\n        optimizer.apply_gradients(zip(gradients, history_model.trainable_variables+future_model.trainable_variables))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:519 apply_gradients  **\r\n        self._create_all_weights(var_list)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:702 _create_all_weights\r\n        _ = self.iterations\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:709 __getattribute__\r\n        return super(OptimizerV2, self).__getattribute__(name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:836 iterations\r\n        aggregation=tf_variables.VariableAggregation.ONLY_FIRST_REPLICA)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1040 add_weight\r\n        aggregation=aggregation)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py:750 _add_variable_with_custom_getter\r\n        **kwargs_for_getter)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py:145 make_variable\r\n        shape=variable_shape if variable_shape else None)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:260 __call__\r\n        return cls._variable_v1_call(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:221 _variable_v1_call\r\n        shape=shape)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:702 invalid_creator_scope\r\n        \"tf.function-decorated function tried to create \"\r\n\r\n    ValueError: tf.function-decorated function tried to create variables on non-first call.", "@shdeldari Can you please create a new issue with a simple standalone code to reproduce your issue? Thanks!", "Can I ask what is the shapes of the Loss, I'm sorry I'm not able to help, but what shape did you do for it?"]}, {"number": 37078, "title": "Documentation issue about tf.math.xlog1py (nightly-only APIs)", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/math/xlog1py\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThis documentation describes `tf.math.xlog1py`, shown as a part of \"stable\" version of TF 2.1 but I believe this API has not been released yet. `tf.math.xlog1py` is only available on nightly at this point, added in https://github.com/tensorflow/tensorflow/commit/19986377f2a3c560418f42f2323733564a7303eb (Jan 2020).\r\n\r\nFYI: I ran into this issue as I was using `tfp-nightly` which depends on `tf-nightly` (module 'tensorflow_core._api.v2.math' has no attribute 'xlog1py').\r\n\r\nThe documentation should have not published under \"TensorFlow Core v2.1.0\". Why was it the case? If it was due to a mistake, could we improve on the process so we can have a \"nightly\" doc and a \"stable\" doc?", "comments": ["@wookayin Agree. This is not part of `TF2.1`. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/6aa619234a38c34150e4b447d4037c66/untitled834.ipynb) is the gist for our reference. With `tf-nightly` it works as expected. But with the `TF2.1`, it throws the following error. Thanks!\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-564ca226252b> in <module>()\r\n----> 1 tf.math.xlog1py(0., 1.)\r\n\r\nAttributeError: module 'tensorflow_core._api.v2.math' has no attribute 'xlog1py'\r\n```\r\n", "Hi, This is an artifact of how the nightly diff are generated.\r\n\r\nLike here:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/math/?version=nightly#for_example\r\n\r\nThis page has no left-nav, because it's only generated in nightly. \r\nWe should make the whole thing green, or add a a sort of [NEW] banner so it's clear what the status is. \r\n", "Thanks -- I see. Yes, I agree that for the pages that should be only available in nightly, more clear status needs to be displayed. (Please feel free to change the issue title to a more general one if you think appropriate)\r\n\r\nWhat actually confused me is the header:\r\n\r\n> TensorFlow > API > **TensorFlow Core v2.1.0** > Python\r\n\r\nwhich sounds like it is a part of TF 2.1.0. FYI, I directly got to the page from google search results so wasn't aware of \"See Stable\" v.s. \"See Nightly\" modes (which is nice BTW).\r\n\r\nSo one of my suggestions is to have a different version name in this navigation banner (e.g., *TensorFlow Core Nightly*).", "This should get fixed tomorrow. There will be a note at the top of the page saying that the API is only available in nightly.", "I see this is now [fixed](https://www.tensorflow.org/api_docs/python/tf/math/xlog1py). Thank you for the support!\r\n\r\n> Note: This API is new and only available intf-nightly.\r\n![image](https://user-images.githubusercontent.com/1009873/75590304-e760ce80-5a31-11ea-8f33-b16930ffff81.png)\r\n\r\n\r\n", "The space between `in` and `tf-nightly` should get fixed by tomorrow."]}, {"number": 37077, "title": "Tensorflow Workspace not configured for android build", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: \r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 1.2.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am trying to build tensorflow lite locally. I am following the steps mentioned here : https://www.tensorflow.org/lite/guide/android\r\nWhen I run ./configure script in root directory, there should be an option for interactively configuring the ./workspace for android builds. But I don't get that option and I am facing a lot of errors during build time. Is there any way I can know the variables that are required to be set? Or any way to fix this ? \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nC:\\Users\\nikhil\\tensorflow>.\\configure                                                                                                                                   Extracting Bazel installation...                                                                                                                                                                           \r\nYou have bazel 1.2.1 installed.                                                                                                                                           Please specify the location of python. [Default is C:\\Users\\nikhil\\AppData\\Local\\Programs\\Python\\Python37\\python.exe]:                                                                                                                                                                                                                                                                                                                                                                                                       Found possible Python library paths:                                                                                                                                        C:\\Users\\nikhil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages                                                                                               Please input the desired Python library path to use.  Default is [C:\\Users\\nikhil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages]                                                                                                                                                                                                        Do you wish to build TensorFlow with XLA JIT support? [y/N]:                                                                                                              No XLA JIT support will be enabled for TensorFlow.                                                                                                                                                                                                                                                                                                  Do you wish to build TensorFlow with ROCm support? [y/N]:                                                                                                                 No ROCm support will be enabled for TensorFlow.                                                                                                                                                                                                                                                                                                     Do you wish to build TensorFlow with CUDA support? [y/N]:                                                                                                                 No CUDA support will be enabled for TensorFlow.                                                                                                                                                                                                                                                                                                     Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:                                                                                                                                                                                                                                                                                                                                                                                             Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:                                                            \r\nEigen strong inline overridden.                                                                                                                                                                                                                                                                                                                     Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.                                     \r\n--config=mkl            # Build with MKL support.                                                                                                                         --config=monolithic     # Config for mostly static monolithic build.                                                                                                      --config=ngraph         # Build with Intel nGraph support.                                                                                                                --config=numa           # Build with NUMA support.                                                                                                                        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.                                                                              --config=v2             # Build TensorFlow 2.x instead of 1.x.                                                                                                    Preconfigured Bazel build configs to DISABLE default on features:                                                                                                                 --config=noaws          # Disable AWS S3 filesystem support.                                                                                                              --config=nogcp          # Disable GCP support.                                                                                                                            --config=nohdfs         # Disable HDFS support.                                                                                                                           --config=nonccl         # Disable NVIDIA NCCL support.                                                                                                            Configuration finished\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@t-niroc \r\nWe see that the link shared by you has the option to [interactively configure](https://www.tensorflow.org/lite/guide/android#configure_workspace_and_bazelrc) android work space.\r\nplease refer to this comment where it confirms the [option to interactively configure](https://github.com/tensorflow/tensorflow/issues/35377#issuecomment-569890899) work-space is present\r\n\r\nplease share the tensor flow version used, and in case you have done the configuration interactively and still face error then please share the error log, else please configure from using the interactive option as mentioned.", "Also, how do I check the tensor flow version?\r\n@Saduf2019 According to link, there should be an option for configuring workspace for android. But when I run the ./configure script, I do not get that option if you see log shared below.\r\n\r\n`C:\\Users\\t-niroc\\tensorflow>.\\configure\r\n\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\n\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".      \r\n\r\nYou have bazel 1.2.1 installed.   \r\n\r\nPlease specify the location of python. [Default is C:\\Users\\t-niroc\\AppData\\Local\\Programs\\Python\\Python37\\python.exe]:                                                                                                                                                     \r\n\r\nFound possible Python library paths:                                                                                                                         C:\\Users\\t-niroc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages  \r\n\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\t-niroc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages]                                                                                                                                                                          Do you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.     \r\n                                                                                                                                                                                                                                                               Do you wish to build TensorFlow with ROCm support? [y/N]:                                                                                                  No ROCm support will be enabled for TensorFlow.            \r\n                                                                                                                                                                                                                                                           Do you wish to build TensorFlow with CUDA support? [y/N]:                                                                                                  No CUDA support will be enabled for TensorFlow.    \r\n                                                                                                                                                                                                                                                                   Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:   \r\n                                                                                                                                                                                                                                                                                                                                             Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:                                             \r\n\r\nEigen strong inline overridden.                                                                                                                                                                                                                                                                                       \r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.           \r\n           --config=mkl            # Build with MKL support.       \r\n                                                                                                   --config=monolithic     # Config for mostly static monolithic build.                  \r\n                                                                     --config=ngraph         # Build with Intel nGraph support.                                                                                                 --config=numa           # Build with NUMA support.   \r\n                                                                                                      --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects. \r\n                                                              --config=v2             # Build TensorFlow 2.x instead of 1.x.      \r\n                                                                               Preconfigured Bazel build configs to DISABLE default on features:          \r\n                                                                                        --config=noaws          # Disable AWS S3 filesystem support.        \r\n                                                                                       --config=nogcp          # Disable GCP support.   \r\n                                                                                                          --config=nohdfs         # Disable HDFS support.              \r\n                                                                                              --config=nonccl         # Disable NVIDIA NCCL support.                      \r\n                                                                       Configuration finished    `              \r\n                     \r\n\r\n", "@t-niroc \r\nto check tensorflow version:\r\nimport tensorflow as tf \r\n`tf.__version__ `", "I have not been able to build tensor flow and so I am unable to see the version. I cloned its repository from the master branch @Saduf2019 ", "@t-niroc Sorry for missing this issue. Is this still an issue for you or was it resolved already. \r\nIf this was resolved for you, please close the issue. Thanks!", "@jvishnuvardhan It was not resolved at the time. I tried 3 fresh installations, 2 times on same device and once on different device. Every time, the option to interactively configure workspace for android was not coming. ( This was on Windows 10 every time).\r\n\r\nAs a side note, I tried this on mac and I got this on first try. So maybe it is platform specific problem.\r\n", "Did you set environment variables ANDROID_NDK_HOME and ANDROID_SDK_HOME ?", "@t-niroc \r\nCould you please update as per above comment and confirm if this is still an issue on later tf versions.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37077\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37077\">No</a>\n"]}, {"number": 37076, "title": "v2.1.0 build fails with TensorRT 6 and 7", "body": "**System information**\r\n- Inside Docker container, base image `ubuntu:bionic`\r\n- Ubuntu 18.04, x86_64\r\n- TensorFlow from-source build, checked out @ tag `v2.1.0`\r\n- Python 3.7.5\r\n- Bazel 0.29.1\r\n- GCC 7.4.0\r\n- CUDA 10.1.243, CuDNN 7\r\n- TensorRT-6.0.1.5\r\n- Quadro GV100 32gb\r\n\r\nTrying to build TensorFlow 2.1.0 in the above environment is failing consistently. I have tried TensorRT 6 and 7, CUDA 10.1.243 and 10.2.89, and it always fails with the message below. <em>**If I try the build without TensorRT, it succeeds**</em>.\r\n\r\n<b>Build invocation</b>\r\n```\r\nbazel build --jobs=36 --config=opt --config=cuda --config=v2 //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n<b>Error output</b>\r\n```\r\nERROR: /tensorflow/tensorflow/python/keras/api/BUILD:115:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 85, in <module>\r\n    from tensorflow.python.ops.standard_ops import *\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/ops/standard_ops.py\", line 117, in <module>\r\n    from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/__init__.py\", line 22, in <module>\r\n    from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 28, in <module>\r\n    from tensorflow.compiler.tf2tensorrt import wrap_py_utils\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 28, in <module>\r\n    _wrap_py_utils = swig_import_helper()\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_wrap_py_utils', fp, pathname, description)\r\n  File \"/usr/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_\r\n----------------\r\nNote: The failure of target //tensorflow/python/keras/api:create_tensorflow.python_api_1_keras_python_api_gen_compat_v1 (with exit code 1) may have been caused by the fact that it is a Python 2 program that was built in the host configuration, which uses Python 3. You can change the host configuration (for the entire build) to instead use Python 2 by setting --host_force_python=PY2.\r\n\r\nIf this error started occurring in Bazel 0.27 and later, it may be because the Python toolchain now enforces that targets analyzed as PY2 and PY3 run under a Python 2 and Python 3 interpreter, respectively. See https://github.com/bazelbuild/bazel/issues/7899 for more information.\r\n----------------\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /tensorflow/tensorflow/python/tools/BUILD:80:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed (Exit 1)\r\nINFO: Elapsed time: 3470.734s, Critical Path: 462.21s\r\nINFO: 26665 processes: 26665 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["@eugeneswalker \r\n\r\nCan you try with bazel with 0.27.1 and let us know how it progresses. Please, see [Tested build configurations](https://www.tensorflow.org/install/source#gpu).Also, please refer [link](https://github.com/tensorflow/tensorflow/issues/35584#issuecomment-570920203) and see if it helps you. Thanks!", "@ravikyram I tried using 0.27.1 just now, and it failed with the following message:\r\n```\r\nERROR: /tensorflow/tensorflow/BUILD:881:1: Executing genrule //tensorflow:tf_python_api_gen_v2 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_2_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_2_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 85, in <module>\r\n    from tensorflow.python.ops.standard_ops import *\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_2_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/ops/standard_ops.py\", line 117, in <module>\r\n    from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_2_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/__init__.py\", line 22, in <module>\r\n    from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_2_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 28, in <module>\r\n    from tensorflow.compiler.tf2tensorrt import wrap_py_utils\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_2_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 28, in <module>\r\n    _wrap_py_utils = swig_import_helper()\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_2_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_wrap_py_utils', fp, pathname, description)\r\n  File \"/usr/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_2_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_\r\n----------------\r\nNote: The failure of target //tensorflow:create_tensorflow.python_api_2_tf_python_api_gen_v2 (with exit code 1) may have been caused by the fact that it is a Python 2 program that was built in the host configuration, which uses Python 3. You can change the host configuration (for the entire build) to instead use Python 2 by setting --host_force_python=PY2.\r\n\r\nIf this error started occurring in Bazel 0.27 and later, it may be because the Python toolchain now enforces that targets analyzed as PY2 and PY3 run under a Python 2 and Python 3 interpreter, respectively. See https://github.com/bazelbuild/bazel/issues/7899 for more information.\r\n----------------\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2740.531s, Critical Path: 565.23s\r\nINFO: 26992 processes: 26992 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "Why is it trying to build a python 2 target when I am using python 3? Is this normal?", "Use \"--noincompatible_do_not_split_linking_cmdline\", when you build tf2.1.0. \r\nSuch as: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package  --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --noincompatible_do_not_split_linking_cmdline", "@jxhekang is `--cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"` necessary? I was able to successfully build v2.1.0 by just adding `--noincompatible_do_not_split_linking_cmdline`.", "@eugeneswalker,\r\nIs this still an issue?\r\n\r\nPlease take a look at @jxhekang's comment and check if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37076\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37076\">No</a>\n"]}, {"number": 37075, "title": "Documentation needs to upgrade to Python3", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/README.md\r\n\r\n## Description of issue (what needs changing):\r\n\r\nSInce , PSF has officially stopped it's support for Python2, the documentation needs to be upgraded to Python3.\r\npip2 -> pip3 \r\n### Clear description\r\n\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\nYes, I'll\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["It seems this could be closed since #36817 is merged", "@Ir1d  I was told that many users use windows where pip3 is not required and python3 too. For them pip and python do the work so i closed my PR. If you suggest me to do the changes, i'll do it.", "@aymericdamien oops, I thought the PR was merged. In my opinion there are many cases where python is default aliased to python3, so I think there's no need to explicitly speficy to use python3 instead of python..", "Yes. So you may consider closing this issue. Could you suggest me some issues to work on? I am a GSOC 2020 aspirant and looking forward to work with Tensorflow.", "Sorry I can't help you. I'm also interested in GSOC 2020 and now wandering around in the issues.", "Closing this issue as per the threads discussion. Thanks!"]}, {"number": 37074, "title": "Add different pattern to show op in timeline", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\nWhen using timeline in tensorflow, we often observe large blanks in the \"/job\" row, which should be showing the consecutive execution of operators. \r\n\r\nThe following are timelines for transformer and transformer using XLA:\r\nTransformer:\r\n![transformer](https://i.ibb.co/NVPV5W6/transformer-before.png)\r\nXLA:\r\n![XLA](https://user-images.githubusercontent.com/10428324/75305719-e0935980-5881-11ea-81a4-f1dda66c2767.png)\r\nThe timeline is somehow confusing and those blanks may leads to misunderstanding that  there are gpus hanging freely.\r\n\r\nThe reason for this issue is that the \"/job\" row only shows the scheduling time for each op. As a result, the async kernels may not start to compute by the time the scheduling of its op is over. For the transformer example, the matmul in embedding takes long and blocks other kernels, which results in the large blank in the middle. And for XLA, those fused kernels are scheduled early but have to wait for execution, which results in the large gap between the ending in \"/job\" and \"/stream:all\".\r\n\r\nTherefore, we propose 2 new pattern to show the op execution time.\r\n- `\"gpu\"` pattern will align op with the execution span of all its kernels\r\n- `\"all\"` pattern will only change the ending time of the op to  the ending of its last kernel. \r\n\r\nwe added a new argument `op_time` to function `generate_chrome_trace_format` to let user select how op execution time will be shown. The default value is `\"schedule\"` which behaves the same as before. And other possible values are `\"gpu\"` and `\"all\"` as explained above.\r\n\r\n```python\r\n  def generate_chrome_trace_format(self, show_dataflow=True, show_memory=False, op_time=\"schedule\"):\r\n```\r\n\r\nThe result of \"gpu\" pattern is:\r\nTransformer:\r\n![transformer](https://i.ibb.co/m4ztqsm/transformer-gpu.png)\r\nXLA:\r\n![XLA](https://i.ibb.co/Tgf2wC6/xla-gpu.png)\r\n\r\nAnd the result of \"all\" pattern is\r\nTransformer:\r\n![transformer](https://i.ibb.co/4Jpfkg9/transformer-all.png)\r\nXLA: \r\n![XLA](https://i.ibb.co/71FxsmF/XLA-all.png)\r\nNotice that the above illustrations have only shown part of the \"all\" pattern since it may induce large parallel (many op are waiting to be executed at the same time.)\r\n\r\nAdditionally, the kernel name of an XLA fusion kernel is not parsed correctly, and because we do not have plans on changing the C++ part of the profiler, we reparsed it using the timeline_label attribute provided in RunMetadata.\r\n\r\nThank you for your time on this review.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37074) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37074) for more info**.\n\n<!-- ok -->", "Thanks for the PR @zhuzilin. @prb12, would you mind taking a look?", "@prb12 do you see this pull request?", "@xinan-jiang, sorry, looks like @prb12 no longer works on TF anymore. @qiuminxu do you know if anybody on the performance side can take a look at this PR? Thank you!", "> Thanks for sending the PR.\r\n\r\nThank you for your review. I have modified the file according to the advice. Could you have a second look? Thank you!", "@qiuminxu Thank you for your quick review. I've changed the lint errors and run pylint on it. Can you have another look?", "@google-admin @googlebot Could you help me check what's wrong with the cla check on this pr? The same github account and email address are used on #37813 and that pr passed the cla check.", "Can you run the check again and see if it passes?", "@googlebot I signed it!", "@qiuminxu Could you help me check what's the migration error in the check \"import/copybara\"? Thank you!", "@yifeif I couldn't see the details of the test error, can someone help take a look?", "@mihaimaruseac Can you please take a look on cla/google test failure? Thanks!"]}, {"number": 37073, "title": "Update README to add new resources and fix grammar", "body": "In this commit, I added new resources to learn TensorFlow and also fixed some grammar stuff(capitalizing words that should be capitalized). Thanks, TensorFlow team!", "comments": ["@gbaned Is this commit ready to be merged into @TensorFlow? Thanks!"]}, {"number": 37072, "title": "Custom object detection API for tensorflow 2.0 not working", "body": "The current Object detection API still has graphs and many code structures which are working only until version 1.15\r\n\r\nEverywhere including colab it says support will only be for 2.0\r\n\r\nIs there a timeline for the object detection API to be useful for tensorflow 2.0 for us to do custom object detection on our own datasets?\r\n\r\n\r\n", "comments": ["@sreeni5493 You can follow the following [tutorial1](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html) and [tutorial2](https://towardsdatascience.com/how-to-train-a-tensorflow-2-object-detection-model-25d4da64b817) to train your custom model using object detection API. If you have more questions, please post it in tensorflow/models repo as this issue is more apt there. Thanks!"]}, {"number": 37071, "title": "[Intel MKL] Adding support for DNNL1 builds to public CI.", "body": "@penpornk This is also not required for r2.2", "comments": ["Got it. Thanks for the feedback, @penpornk!"]}, {"number": 37070, "title": "[Intel Mkl] minor fix to MKL test script", "body": "@penpornk This is not required for r2.2", "comments": []}, {"number": 37069, "title": "tensor_bundle: Supply size hint to input buffer", "body": "This improves performance for random reads on string and variant tensors. Both rely on input buffering currently. If the buffer size is too large, it ends up reading too much data and throw most of them away doing next out-of-range Seek.\r\n\r\nThis is part of a patch series aiming to improve the performance for CacheDatasetV2. It does not support external shuffle or random reads now, but we plan to add it in the future (subject to the API review).", "comments": ["Also, the new API should have unit tests.", "> Can you please provide an explanation in the PR description as to why does this change improve the TensorBundle performance or even better describe how you benchmarked the performance and the results you have observed.\r\n\r\nI mentioned that in the commit message.\r\n\r\n> EDIT: In particular, when I looked at the code path, it seemed like without the hint, if a read does not have data in the internal buffer, it will refill the buffer with new data. Is the reason why hint is faster that we the original code path is reading in too much data?\r\n\r\nThat is exactly the point of this patch.", "> Also, the new API should have unit tests.\r\n\r\nAdded and tested offline.", "@gbaned Anything I could do to roll this PR forward?", "Gently ping @gbaned again."]}, {"number": 37068, "title": "Can't load Keras model with multiple MetaGraphs, can't pass tags", "body": "- Have I written custom code: Yes\r\n- OS Platform and Distribution: Amazon Linux AMI 2018.03\r\n- TensorFlow installed from: Preinstalled by Amazon\r\n- TensorFlow version: 1.15.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: Nvidia Tesla V100\r\n\r\n**Describe the current behavior**\r\nWhen trying to load a saved model using `keras.models.load_model`, an error occurs stating that `tf.saved_model.load requires a 'tags='`. There is also no way to pass tags through the `load_model` function.\r\n\r\n**Describe the expected behavior**\r\nModel should load.\r\n\r\n**Standalone code to reproduce the issue** \r\nWith `\"albert/\"` being a path to this model locally: https://tfhub.dev/google/albert_xlarge/3:\r\n\r\n```python3\r\nfrom tensorflow import keras\r\n\r\nmodel = keras.models.load_model(\"albert/\")\r\n```\r\n\r\nError:\r\n```\r\nValueError: Importing a SavedModel with tf.saved_model.load requires a 'tags=' argument if there is more than one MetaGraph. Got 'tags=None', but there are 2 MetaGraphs in the SavedModel with tag sets [['train'], []]. Pass a 'tags=' argument to load this SavedModel.\r\n```\r\n\r\n\r\n**Other info / logs**\r\n```\r\nFile \"albert_entrypoint.py\", line 50, in <module>\r\n    model = keras.models.load_model(MODEL_EXTRACT_DIR)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\", line 147, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 86, in load\r\n    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/load.py\", line 550, in load_internal\r\n    root = load_v1_in_v2.load(export_dir, tags)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/load_v1_in_v2.py\", line 239, in load\r\n    return loader.load(tags=tags)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/load_v1_in_v2.py\", line 168, in load\r\n    meta_graph_def = self.get_meta_graph_def_from_tags(tags)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/load_v1_in_v2.py\", line 80, in get_meta_graph_def_from_tags\r\n    .format(len(self._saved_model.meta_graphs), tag_sets))\r\n\r\nValueError: Importing a SavedModel with tf.saved_model.load requires a 'tags=' argument if there is more than one MetaGraph. Got 'tags=None', but there are 2 MetaGraphs in the SavedModel with tag sets [['train'], []]. Pass a 'tags=' argument to load this SavedModel.\r\n```\r\n", "comments": ["@uchua \r\n\r\nThis issue is more suitable for TensorFlow Hub repo. Please post it on hub repo from [here.](https://github.com/tensorflow/hub/issues) Thanks!", "@ravikyram outside of the model (which I don't believe the TensorFlow Hub team would have any control of), the issue doesn't involve TensorFlow Hub. Any model with multiple MetaGraphs (where `tf.saved_model.load` requires the `tags=` argument), won't be able to be load correctly.", "I have tried on colab with TF version 1.15 , 2.2.0-dev20200226 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/bb6967034ebefb8752f33de0378484f0/untitled680.ipynb). Thanks!", "You can load the module as a Keras layer to use it. Heres the [gist](https://colab.sandbox.google.com/gist/gowthamkpr/b20285706b9fb5152adf2050016268aa/untitled25.ipynb). \r\n\r\nI was also able to lad the model using `tf.saved_model.load()` which has tags argument. Please find the gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/eaceca0db447e1ed7025d0bd07c68d8a/untitled26.ipynb). \r\n\r\n`keras.models.load_model ` doesnt have tags argument explicitly defined and thats the reason why you are not able to load this model.", "Is there a way to load a model using `tf.saved_model.load()` and then add it to a Keras model?", "You can take the outputs of the loaded model and create a new model with those outputs @uchua \r\nYou can take a look at this[ issue](https://stackoverflow.com/questions/41668813/how-to-add-and-remove-new-layers-in-keras-after-loading-weights)", "Upgrade your tensorflow version to 2.0.", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this issue as it has been inactive for 3 weeks. Please add additional comments for us to open this issue again.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37068\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37068\">No</a>\n"]}, {"number": 37067, "title": "Usage Documentation Needed for tf.image.yuv_to_rgb", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/image/yuv_to_rgb\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe tf.image.yuv_to_rgb method specifies a YUV input of shape (H,W,3) and an RGB output of the same shape. It also notes:\r\n\r\n> The output is only well defined if the Y value in images are in [0,1], U and V value are in [-0.5,0.5].\r\n\r\nHowever YUV is natively encoded in HxWx1.5 Bytes, with values ranging from 0-255. Considering that multiple YUV-RGB conversion standards exist, it is unclear what pre-processing steps need to be done by a user who wants to pass YUV inputs to his network (https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion).\r\n\r\n### Usage example\r\n\r\nMore documentation on the proper usage of this method would be highly helpful. Specifically:\r\n\r\n- An example showing how to pre-process a raw YUV image of size HxWx1.5B to the expected shape of (H,W,3) with normalized values Y: [0,1], UV: [-0.5,0.5].\r\n- An example of how one might append this method to an RGB-trained model to enable it to accept YUV inputs during inference. A likely scenario might be exporting a frozen model to an Android device that natively captures in YUV.", "comments": ["@alexdwu13 Thanks for creating the issue. Would you like to create a PR to update the doc? Thanks!", "@jvishnuvardhan I'd be happy to create a PR -- but I myself am not clear about the usage. I've tried munging YUV images to the input format expected by `tf.image.yuv_to_rgb` but have so far not been successful https://stackoverflow.com/questions/60067415/what-is-the-correct-usage-of-tf-image-yuv-to-rgb-output-is-distorted.\r\nWould you be able to grab ahold of the original authors of this method?", "I left a comment on the PR 37565. I think the documentation has been incorrect on the [-0.5, 0.5] range for the UV channels. And there are bigger issues with the function itself. \r\n\r\n1. Prior to this issue, the documentation specified that the U and V channels need to be in the range [-0.5, 0.5]. That is incorrect. The kernels in the source show that the correct ranges are \r\n\r\nu: -0.43601035 to + 0.43601035\r\nv: -0.61497538 to + 0.61497538\r\n\r\n2. The defined ranges for U and V per the ITU spec are in fact \r\n\r\n[-0.436, +0.436]\r\n[-0.615, +0.615]\r\n\r\n3. \r\n\r\nThe PR 37565 created a linear interpolation example for the (incorrectly documented) range of [-0.5, 0.5] for the U and V channels. \r\n\r\n4.\r\n\r\nPR 37565  implies that a TF developer might end up with a YUV range of [0, 256). That is extremely unlikely because the YUV space is not \"square\", it is \"diamond\" and corners like YUV= (1,1,1) are in fact impossible colors. YUV=(1,1,1) would mean a \"maximum bright white color with a lot of red and blue\". This results in a color with more red and blue than an LCD display can render, or the human eye can perceive. \r\n\r\n>>> tf.image.yuv_to_rgb(tf.constant([1.0, 0.436, 0.615]))\r\n<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1.7010281, 0.4708535, 1.8859789], dtype=float32)>\r\n\r\nThe conceivable sources of a YUV image would be directly pulled from an analog CRT display, or, via transforms on a tensor that came from the tf.image.rgb_to_yuv. The former seems very unlikely, the latter is also unlikely because manipulating YUV directly without running into clipping against the \"diamond shaped\" visible color space is very difficult. \r\n\r\n5. \r\n\r\nFinally, the rgb_to_yuv and yuv_to_rgb functions are problematic because they assume a linear, non-gamma corrected RGB color values. The vast majority of images on the web or captured by a camera will be rendered in a gamma corrected color-space, namely sRGB. \r\n\r\n--\r\n\r\nTo fix the documentation, I'd suggest a quick update to the correct UV ranges.  I'd also suggest a strong caveat that this function is likely not what people are looking for - a better transform for gamma corrected RGB (namely, sRGB) would be YCbCr. I'd also suggest we remove the example recently added of the linear transform, since it's unlikely a developer would ever get the raw YUV values from something other than the rgb_to_yuv function. \r\n\r\nHere is the start of some example code for yuv_to_rgb... if these suggestions make sense, I'll finish this up and make it a PR.\r\n\r\ndef yuv_to_rgb(images):\r\n  \"\"\"Converts one or more images from YUV to RGB.\r\n\r\n  Outputs a tensor of the same shape as the `images` tensor, containing the RGB\r\n  value of the pixels.\r\n  The output is only well defined if the Y values in images are in [0,1],\r\n  U values are in [-0.43601035, 0.43601035], and V value are in \r\n  [-0.61497538,0.61497538].\r\n\r\n\r\n  # Test approximate range of Y, U, and V. \r\n  >>> rgb_images = tf.random.uniform(shape=[60000, 32, 32, 3], minval=0, maxval=256, dtype=tf.int32)\r\n  >>> rgb_images = tf.cast(rgb_images, tf.uint8)\r\n  >>> rgb_float_images = tf.image.convert_image_dtype(rgb_images, tf.float32)\r\n  >>> yuv_images = tf.image.rgb_to_yuv(rgb_float_images)\r\n\r\n  >>> # Approximate min and max for Y channel\r\n  >>> tf.reduce_min(yuv_images[...,0])\r\n  <tf.Tensor: shape=(), dtype=float32, numpy=0.0>\r\n  >>> tf.reduce_max(yuv_images[...,0])\r\n  <tf.Tensor: shape=(), dtype=float32, numpy=1.0>\r\n\r\n  # Approximate min and max for U channel\r\n  >>> tf.reduce_min(yuv_images[...,1])\r\n  <tf.Tensor: shape=(), dtype=float32, numpy=-0.43543333>\r\n  >>> tf.reduce_max(yuv_images[...,1])\r\n  <tf.Tensor: shape=(), dtype=float32, numpy=0.43601036>\r\n\r\n  # Approximate min and max for V channel\r\n  >>> tf.reduce_min(yuv_images[...,2])\r\n  <tf.Tensor: shape=(), dtype=float32, numpy=-0.6149754>\r\n  >>> tf.reduce_max(yuv_images[...,2])\r\n  <tf.Tensor: shape=(), dtype=float32, numpy=0.6149754>\r\n\r\n  This function is likely not appropriate for images typically found on the web or generated by\r\n  digital cameras. Those typically adjust the RGB values to be gamma corrected, meaning they are rendered in the sRGB color space. The YCbCr colorspace was designed for the \r\n  sRGB colorspace and may be more appropriate. The YUV colorspace\r\n  was based on analog, non-gamma corrected RGB values. \r\n"]}, {"number": 37066, "title": "Model class API / predict / gradient update", "body": "## URL(s) with the issue:\r\nhttps://keras.io/models/model/\r\nand \r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#predict\r\n\r\n## Description of issue (what needs changing):\r\nDocumentation for model predict says \"batch_size: Integer or None. Number of samples per gradient update.\", but unless I'm missing something, there are no gradient updates while predicting.\r\n\r\n", "comments": ["Thanks for the report @johngrabner .", "Thanks! This is already updated in the [nightly version](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#predict)."]}, {"number": 37065, "title": "Slice of a tensor is created on CPU not GPU when dtype=tf.int16", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: NA\r\n- **TensorFlow version (use command below)**: version 2.x\r\n- **Python version**: 3\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: Google Colab GPU\r\n- **Exact command to reproduce**: \r\nhttps://colab.research.google.com/drive/1gbVxZgaesqxOLuoF-D9yuwOahCNTDcEi\r\n\r\n### Describe the problem\r\nWhen slicing a tensor of dtype tf.int16 on GPU, the operation is created on CPU, not GPU. If the tensor is tf.int32, the operation is created on GPU.\r\n\r\n### Source code / logs\r\nhttps://colab.research.google.com/drive/1gbVxZgaesqxOLuoF-D9yuwOahCNTDcEi\r\n", "comments": ["@NicMaq Slicing operation on gpu does not supported int16  and this is the reason why the operation is not created on GPU. ", "Thank you for your reply. \r\n\r\nMaybe, I would suggest modifying the documentation of tf.slice as it indicates: input: A tensor. \r\n\r\nThank you. \r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37065\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37065\">No</a>\n"]}]