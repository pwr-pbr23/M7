[{"number": 5333, "title": "Problem compiling tensorflow for iOS on Sierra/Xcode 8", "body": "with xcode-select pointed to Xcode 8:\r\n\r\n```\r\n$ xcode-select -p\r\n/Applications/Xcode.app/Contents/Developer\r\n```\r\n\r\nI get the following compile error with build_all_ios.sh on Sierra:\r\n\r\n```\r\nchecking whether we are cross compiling... configure: error: in `/Users/serkan/tensorflow/tensorflow/contrib/makefile/downloads/protobuf':\r\nconfigure: error: cannot run C compiled programs.\r\nIf you meant to cross compile, use `--host'.\r\nSee `config.log' for more details\r\n```\r\n\r\nNot sure if this is related to Xcode 8 or to code signing on Sierra\r\n\r\n", "comments": ["Does your c compiler work i.e. try\n\n```\ncat >foo.c <<EOF\n#include <stdio.h>\nint main(){printf(\"hello\\n\");return 0;}\nEOF\ngcc foo.c\n./a.out\n```\n", "Got the same issue too.\n\nThe c compiler works.\nTested by @aselle  's test code.\n\nP.S. my env: macOS 10.12 and XCode8.1 \n", "Same error: macOS 10.12.1, XCode 8.1\n", "I\u2019m not sure why Xcode 8.1 could not support iPhone simulator with CNN function, but Apple original CNN example does not work on simulators and could not build app. Then it\u2019s working on real device only.\n\nSo, I wonder Tensorflow also could not run on simulators.\nI modified the following sh files for disabling simulator build.\n\ncompile_ios_protobuf.sh\ncompile_ios_tensorflow.sh\n\nI deleted \u201cmake\u201d part for build and \u201clipo\u201d part for archive lib file to the following directory related. \n${LIBDIR}/iossim_386/\n${LIBDIR}/iossim_x86_64/\n\nAfter modified I run \u201cbuild_all_ios.sh\u201d for building to  ${LIBDIR}/ libtensorflow-core.a .\nThat sh called two modified sh files.\n\nThe modified sh build libtensorflow-core.a without simulator libs.\nBut It run on the real iPhone device.\n \u2026I run camera ios_example on iPhone 6 plus iOS 10.1.1\n    used environment : macOS Sierra 10.12.1, and Xcode 8.1 with command line tools 8.1\n\nPlease try above for fixing configure issue in build phase.\n", "I got the same error(macOS Sierra 10.12, XCode 8.1\uff09,and don\u2018t know how to fix it.Does somebody have solutions?\n", "follow #4640 which is similar issue as this. I fixed it by removing all occurences of X86_64 bits from the build scripts compile_ios_protobuf.sh & compile_ios_tensorflow.sh and then running build_all_ios.sh\nworks great on device. \n\nNote the above fix will not run on simulator\n", "Closing out as a duplicate of #4640."]}, {"number": 5332, "title": "about how to conduct the tensor like the numpy or theao", "body": "I can get the result by numpy or theano,but for tensorflow it may not support this\r\nHow can I do to get the result that I want\r\n>>>import tensorflow as tf\r\n>>> x = [[1,2,3],[4,5,6]]\r\n>>> y = [0,1]\r\n>>> z = [1,2]\r\n>>> x = tf.constant(x)\r\n>>> y = tf.constant(y)\r\n>>> z = tf.constant(z)\r\n>>> m = x[y,z]  # m = [2,6] is what I expect ,I just make a simple example ,Thanks\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/users3/htleng/.local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 383, in _SliceHelper\r\n    name=name)\r\n  File \"/users3/htleng/.local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 537, in strided_slice\r\n    shrink_axis_mask=shrink_axis_mask)\r\n  File \"/users3/htleng/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2750, in strided_slice\r\n    shrink_axis_mask=shrink_axis_mask, name=name)\r\n  File \"/users3/htleng/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\r\n    op_def=op_def)\r\n  File \"/users3/htleng/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2382, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/users3/htleng/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1783, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/users3/htleng/.local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1645, in _DelegateStridedSliceShape\r\n    return common_shapes.call_cpp_shape_fn(op, input_tensors_needed=[1, 2, 3])\r\n  File \"/users3/htleng/.local/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py\", line 596, in call_cpp_shape_fn\r\n    raise ValueError(err.message)\r\nValueError: Shape must be rank 1 but is rank 2\r\n", "comments": ["This is almost assuredly something you do not want to do for performance reasons. You should be trying to write operations that do a lot of work per statement (i.e. in a batch-centric or simd fashion). If you are doing this for diagnostic purposes that is one thing, but model building, training and inference should not use anything like this.\n\nIf you really must do this you can use eval() or run() on a tensorflow tensor to get a numpy array back which supports individual index querying. \n\nFor futher guidance on using TensorFlow write StackOverflow questions. This forum is for bugs and feature requests. Thank you!\n", "@aselle \uff0cI then use  tf.pack() and tf.gather_nd to get the result that I want ,but I can't get the gradient.\nAny advice for me, thanks.\n", "A gradient for gather_nd will be added soon, and we will also be adding tensor indirect indexing moving forward that will make the foo[tensor1] technique work.\n", "Automatically closing due to lack of recent activity, we will reopen if further information becomes available. Thanks!\n"]}, {"number": 5331, "title": "Branch 137837591", "body": "", "comments": ["@vrv, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @charlesnicholson and @dsmilkov to be potential reviewers.\n", "Jenkins, test this please.\n"]}, {"number": 5330, "title": "Implementing WriteFile Op", "body": "Hello,\r\nI created a pull request which implements `tf.write_file` OP requested [here](https://github.com/tensorflow/tensorflow/issues/4471). Please take a look.\r\n\r\nThanks,\r\nMoustafa\r\n\r\nFixes #4471", "comments": ["@malzantot, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @concretevitamin to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "PS: I just submitted the CLA\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Hey @concretevitamin, \n\nThanks for your review. PTAL!\n", "Hi @concretevitamin ,\n\nThanks for your review. PTAL !\n", "Jenkins test this please\n"]}, {"number": 5329, "title": "Hessian with respect to one-dimensional tensors", "body": "This PR adds an operation to compute the Hessian with respect to one-dimensional tensors.", "comments": ["@tillahoffmann, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @yuanbyu to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "@goodfeli, could I ask you to take a look at this one, since you wrote _hessian_vector_product a while ago?   I'm not sure what the difference between this and calling _hessian_vector_product with v = 1 is (other than the kwargs, which we'd be fine adding).  As is, I'm worried about adding new, restricted functionality without a good reason, since this is just a composition of two functions that any user can write themselves.  (In contrast, _hessian_vector_product does a bit more subtle work with stop_gradient to make things efficient).\n\n(I also don't fully understand why you say the hessian can only be computed for 1-D vectors, but I admit I don't understand the details).\n", "Looking at it\n", "The `HessianVectorProduct`, if I understand correctly, computes the inner product of the Hessian with some vector which is useful for some optimizers that make use of the Hessian for more efficient steps. It does not allow for the evaluation of the Hessian itself but I may have misunderstood.\n\nI have restricted to the hessian with respect to one dimension because the reshaping necessary to do it for arbitrary-shape tensors would be quite involved.\n", "@vrv: tl;dr This looks like the correct way to implement the full Hessian matrix. You wouldn't want to use _hessian_vector_product to do it.\n\nLong version:\nFor most problems, the Hessian matrix would be too big to store, so we use _hessian_vector_product to compute only a small Krylov subspace of the Hessian matrix. However, it can be useful to compute the full Hessian matrix for a small problem, for debugging or research purposes.\n\n_hessian_vector_product takes the product between the Hessian \"matrix\" and a \"vector\". I use scare quotes here because the \"vector\" can actually be a list of tensors of arbitrary shape. We imagine that we reshape and concatenate these tensors to get one large 1-D tensor. It's not possible to call _hessian_vector_product with just a scalar `1` or an identity matrix as its argument; the argument needs to match the shapes of the parameter list.\n\nTo compute a Hessian matrix of shape [n, n] using _hessian_vector_product, you would need to call _hessian_vector_product n times. On call `i`, you pass a \"vector\" that is all zeros except for position `i`, and is 1 at position `i`. This is not very efficient because you have to construct those one-hot vectors.\n\nComputing the full Hessian is not a very common operation, but if someone wants to do it, this PR looks like the right way to do it.\n", "@vrv : let me know if you need anything more from me---I wasn't sure if you wanted me to do a full TensorFlow code review or just to comment on algorithmic correctness.\n", "@tensorflow-jenkins test this please\n", "Hello to all. \r\nI don't knwo if this is the right place to ask, but I was wondering if there is any specific reason why the` _hessian_vector_product` function is not present in the \"public\" api.\r\nThanks, Luca", "I imagine the method exists for optimisation algorithms like [this one](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_Martens10.pdf) which are not (yet) implemented in tensorflow."]}, {"number": 5328, "title": "Is there any way to use `gather_nd` more simpler?", "body": "Hi all, \r\n\r\nNow I'm using [tf.gather_nd](https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#gather_nd). \r\n\r\nWhen batched indexing into a matrix:\r\n```\r\nindices = [[[0, 0]], [[0, 1]]]\r\nparams = [['a', 'b'], ['c', 'd']]\r\noutput = [['a'], ['b']]\r\n```\r\n\r\nI wonder if there any simpler solution if my `indices.shape[0] = params.shape[0]`. That means I want to take values from **EVERY** row of params. \r\n\r\nSo if I want to get \r\n`output = [['a'], ['c']]` ('a' is from params[0], 'c' is from params[1])\r\n I can code like `indices = [[0], [0]]` instead of `indices = [[[0, 0]], [[1, 0]]]` since people hardly have the row info in `indices`\r\n\r\nFor example,\r\n```\r\nimport tensorflow as tf\r\n\r\nindices = [[[0, 4], [0, 1], [0, 6], [0, 2]],\r\n          [[1, 1], [1, 4], [1, 0], [1, 9]],\r\n          [[2, 5], [2, 1], [2, 9], [2, 6]]]\r\n\r\nparams = [[4,6,3,6,7,8,4,5,3,8], [9,5,6,2,6,5,1,9,6,4], [4,6,6,1,3,2,6,7,1,8]]\r\noutput = tf.gather_nd(params, indices)\r\n\r\nsess = tf.Session()\r\nprint sess.run(output)\r\n```\r\nThere output is \r\n```\r\n[[7 6 4 3]\r\n [5 6 9 4]\r\n [2 6 8 6]]\r\n```\r\n\r\nI want to take out 7, 6, 4, 3 from params[0]. However, I have to set row index 0 in `indices` because params[0][4] = 7, params[0][1] = 6, params[0][6] = 4, params[0][2] = 3.\r\nBut what I already know is just \r\n```\r\nraw_indices = [[4, 1, 6, 2],\r\n               [1, 4, 0, 9],\r\n               [5, 1, 9, 6]]\r\n```\r\nHow can I add the \"row index\" in each elem of `raw_indices` to get\r\n```\r\nindices = [[[0, 4], [0, 1], [0, 6], [0, 2]],\r\n          [[1, 1], [1, 4], [1, 0], [1, 9]],\r\n          [[2, 5], [2, 1], [2, 9], [2, 6]]]\r\n```\r\n\r\n\r\n\ufffc\r\n\r\n\r\n\ufffc\r\n\ufffc\ufffc\ufffc", "comments": ["This question is more appropriate to ask on StackOverflow, since it is not a specific feature request or bug and is concerned with best practices. Thanks!\n"]}, {"number": 5327, "title": "Build error: Eigen/src/Core/arch/CUDA/PacketMathHalf.h(45): error: identifier \"__half2half2\" is undefined", "body": "I'm getting the following errors when I try to build TensorFlow:\r\n\r\n```\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/PacketMathHalf.h(45): error: identifier \"__half2half2\" is undefined                                                                                                                   [136/14011]\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/PacketMathHalf.h(53): error: identifier \"__halves2half2\" is undefined\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/PacketMathHalf.h(57): error: identifier \"__halves2half2\" is undefined\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/PacketMathHalf.h(65): error: identifier \"__low2half\" is undefined\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/PacketMathHalf.h(66): error: identifier \"__high2half\" is undefined\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/PacketMathHalf.h(81): error: identifier \"__halves2half2\" is undefined\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/PacketMathHalf.h(88): error: identifier \"__halves2half2\" is undefined\r\n```\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNothing.\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 14.04.1\r\ngcc-4.8.4\r\nbazel 0.3.2\r\nTensorFlow: Master 6f7cf60a4158a6d\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\ncuda: 7.5\r\ncudnn: 5.1.5\r\n```\r\n \u276f ls -lh /usr/local/cuda-7.5/lib64/libcud*\r\n-rw-r--r-- 1 root root 316K Aug  7  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root   16 Aug  7  2015 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root   18 Aug  7  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.7\r\n-rwxr-xr-x 1 root root 375K Aug  7  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.7\r\n-rw-r--r-- 1 root root 704K Aug  7  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a\r\n \u276f ls -lh /usr/local/cudnn-5.1/lib64 \r\nlrwxrwxrwx 1 root root  13 Oct 21 18:33 libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root  17 Oct 21 18:33 libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-r-xr-xr-x 1 root root 76M Oct 21 18:33 libcudnn.so.5.1.5\r\n-r-xr-xr-x 1 root root 67M Oct 21 18:33 libcudnn_static.a\r\n\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n6f7cf60a4158a6d7861dc1b41a6446b575153a2e\r\n2. The output of `bazel version`\r\n```\r\nBuild label: 0.3.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\r\nBuild timestamp: 1475861110\r\nBuild timestamp as int: 1475861110\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Maybe @benoitsteiner might help.\n", "`\u276f grep half2half2 /usr/local/cuda-7.5/include/cuda_fp16.h -2`\n\n``` c++\n\n__CUDA_FP16_DECL__ __half __high2half(const __half2 h);\n__CUDA_FP16_DECL__ __half2 __halves2half2(const __half l, const __half h);\n__CUDA_FP16_DECL__ __half2 __half2half2(const __half lh);\n__CUDA_FP16_DECL__ __half2 __floats2half2_rn(const float f1, const float f2);\n__CUDA_FP16_DECL__ __half2 __float22half2_rn(const float2 f);\n--\n   return val;\n}\n__CUDA_FP16_DECL__ __half2 __half2half2(const __half lh)\n{\n   __half2 val;\n```\n", "Same on 0.11 branch.\n", "@gokceneraslan: it looks like bazel is picking up an older version of CUDA that doesn't support fp16. Do you have cuda 7.0 (or older) installed in  /usr/local/cuda by any chance ?\n", "No, I have only cuda 7.5 installed. In bazel cache, I can also see that it picks up the right cuda:\n\n```\n \u276f pwd\n.../.bazel-cache/107ff11f0584ee207e19ea91624e574d/external/local_config_cuda/cuda\n \u276f ls\nbin  extras  include  lib64  nvvm  BUILD  build_defs.bzl  cuda_config.h  platform.bzl\n \u276f ls -alh include/cuda_fp16.h\nlrwxrwxrwx 1 user users 39 Nov  1 10:20 include/cuda_fp16.h -> /usr/local/cuda-7.5/include/cuda_fp16.h\n \u276f grep half2half2 include/cuda_fp16.h -2\n__CUDA_FP16_DECL__ __half __high2half(const __half2 h);\n__CUDA_FP16_DECL__ __half2 __halves2half2(const __half l, const __half h);\n__CUDA_FP16_DECL__ __half2 __half2half2(const __half lh);\n__CUDA_FP16_DECL__ __half2 __floats2half2_rn(const float f1, const float f2);\n__CUDA_FP16_DECL__ __half2 __float22half2_rn(const float2 f);\n--\n   return val;\n}\n__CUDA_FP16_DECL__ __half2 __half2half2(const __half lh)\n{\n   __half2 val;\n```\n", "@gokceneraslan Which .cc file(s) trigger these error messages ?\n", "I think `contrib/rnn/kernels/gru_ops_gpu.cu.cc`. The output is as follows:\n\n```\nINFO: From Compiling tensorflow/contrib/rnn/kernels/gru_ops_gpu.cu.cc:\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/PacketMathHalf.h(45):error: identifier \"__half2half2\" is undefined\n\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/PacketMathHalf.h(53):error: identifier \"__halves2half2\" is undefined\n\n... (several of these error messages)\n\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/TypeCasting.h(100): error: identifier \"__floats2half2_rn\" is undefined\n\n71 errors detected in the compilation of \"/tmp/user/tmpxft_00002ecc_00000000-10_gru_ops_gpu.cu.compute_52.cpp1.ii\".\nERROR: /users/user/Code/tensorflow/tensorflow/contrib/rnn/BUILD:73:1: output 'tensorflow/contrib/rnn/_objs/python/ops/_gru_ops_gpu/tensorflow/contrib/rnn/kernels/gru_ops_gpu.cu.pic.o' was not created.\nERROR: /users/user/Code/tensorflow/tensorflow/contrib/rnn/BUILD:73:1: not all outputs were created.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nERROR: /users/user/Code/tensorflow/tensorflow/tools/pip_package/BUILD:23:1 not all outputs were created.\nINFO: Elapsed time: 12.422s, Critical Path: 11.16s\n```\n", "Sometimes another cc file triggers the error, like `tensorflow/core/kernels/depthtospace_op_gpu.cu.cc`\n", "@gokceneraslan I can't reproduce the issue locally, so it's kind of a shot in the dark, but can you check if https://github.com/tensorflow/tensorflow/commit/0b730a0d9351a89bf0abff8b18ff382f4559d239 fixed the issue ?\n", "No, it didn't fix it. But it seems there is something weird about my setup, because I also cannot reproduce it on another machine with same CUDA and cudnn.\n", "Thanks for the update. I'm closing this issue.\n"]}, {"number": 5326, "title": "Feature Request: Optimizer that can perform Sparse Updates", "body": "context:\r\nhttps://www.reddit.com/r/MachineLearning/comments/5abcd4/r161009027_scaling_memoryaugmented_neural/d9fz62u/\r\n\r\nFeature request to add optimizer that can perform sparse updates so that [1610.09027] \"Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes\" from DeepMind can be implemented in Tensorflow.\r\n\r\n@alrojo", "comments": ["We don't have any plans right now to do this. Thanks!\n", "@aselle given you support [sparse variable updates](https://www.tensorflow.org/versions/r0.11/api_docs/python/state_ops.html#sparse-variable-updates) and your optimizers have [_apply_sparse](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/adam.py#L141) functionality.\n\nIt would be much appreciated if you could elaborate on the current state of sparse support for optimizers in TensorFlow and which challenges exists for implementing fully sparse support?\n\nThanks\n", "I don't think optimizers are taking advantage of sparse updates yet. @alextp, do you have anything to add on the subject? It's an area of some interest of course, so thanks for bringing it up.\n", "Currently optimizers in TF do sparse updates, as you pointed out, only if the variable access was made with a gather (as the gradient of gather is defined to be indexedslices which triggers sparse updates).\n\nThat said what this paper implemented is not sparse updates as part of the optimization process but sparse updates as part of unrolling an RNN (wherein each state has a reference to a very large memory tensor and does scatter_add on it). Currently tensorflow has no support for efficient sparse mutations on arbitrary tensors.\n\nI see two ways of supporting this: adding a new type of mutable resource like tensorarray which allows for these operations, or adding a graph-rewriting pass which checks which sparse tensor mutations are valid and can be done with overwriting instead of copying.\n", "This issue is quite old and hasn't had recent activity. If there's a well-scoped feature requests in here, please open a new issue. Thank you."]}, {"number": 5325, "title": "Add optimizer that can perform sparse updates", "body": "context:\r\nhttps://www.reddit.com/r/MachineLearning/comments/5abcd4/r161009027_scaling_memoryaugmented_neural/d9fz62u/\r\n\r\nPull request to add optimizer that can perform sparse updates so that [1610.09027] \"Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes\" from DeepMind can be implemented in Tensorflow.\r\n\r\n@alrojo ", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 5324, "title": "Small documentation error in iOS Example", "body": "I'm following the wonderfully comprehensive instructions here (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/ios_examples/README.md)  for building an iOS project with tensorflow with the makefile system. \r\n\r\nThe instructions, in the section \"creating your own app\" suggest adding \r\ntensorflow/contrib/makefile/downloads/eigen-latest\r\nto the header search paths; however this folder does not exist after running the make scripts.\r\nI believe it should be changed to \r\ntensorflow/contrib/makefile/downloads/eigen\r\n\r\n\r\n(I have not seen anything suggesting tensorflow can be built for iOS using bazel, so I assume that this remains the recommended approach.)", "comments": ["@petewarden, can you confirm this fix is correct?\n", "Yes, I believe it is correct.\n"]}, {"number": 5323, "title": "Memoization", "body": "I have a particle physics optimization problem where my graph contains a large subgraph that only needs to be calculated once every couple of iterations. The code I'm currently using for this uses memoization to create a significant increase in performance for these types of problems. I've been looking into porting the graph calculation part of my current code to TensorFlow, but it seems like the absence of memoization might become a show-stopper.\r\n\r\nAre there plans to include support for memoization? If not, do you think this would be a good addition and do you have ideas on how to do this? I would be willing to contribute, if possible.\r\n\r\nIdeally, one would be able to activate memoization per node/operation, so as to avoid performance hits due to overhead in nodes that need to be recalculated every time (i.e. that receive different arguments every time).\r\n\r\nPerhaps this could be implemented using partial_run (#672), but I suspect a native c++ implementation would be vastly superior.", "comments": ["@vrv might have more insight, but I have a few things.\n1. This is a great question to ask on StackOverflow, because you can ask \"how do people deal with caching data and/or memoizing? do you use explicit memoization?\"\n2. A concrete example of what kind of data you are memoizing might make it clearer exactly what you want.\n3. Dataflow and traditional memoization are pretty much at odds in the current architecture, since you really have no way to have a function that causes upstream dependencies to run. You need to be able to schedule a static computation. Perhaps you mean memoization in the sense of if you see the same input, you have an internal cache per node.\n", "I think using partial_run, or perhaps saving your subgraph output state in a variable to be consumed downstream is probably the best ways to go about doing this for now.\n\nNote that partial_run is also supported at the C++ interface, though I'd like to see proof that it would be 'superior'. :)\n", "> memoization in the sense of if you see the same input, you have an internal cache per node.\n\nYes, that's what I mean. It seems to me that implementing such functionality would be a lot more expensive when done at the Python level than at the C++ level.\n\n> Note that partial_run is also supported at the C++ interface, though I'd like to see proof that it would be 'superior'. :)\n\nSee above, by \"c++ implementation\" I meant the implementation of memoization in C++ vs in Python, not the partial_run implementation :)\n", "partial_run's C++/python interface does do most of its implementation in C++, fortunately.\n", "I'm closing this for now, since there really isn't a concrete feature request here. If you want to formalize or suggest a particular design, we are happy to reopen and discuss.\n"]}, {"number": 5322, "title": "Remove unnecessary GCS customized python code", "body": "We have a GCS-like file system (blob store) and added the support without any customized python code (event summary input for tensorboard is not needed either).\r\n\r\nIt's better to clean them up. At least, refactor and clean up the following code because wen shouldn't change any python code when we adding a new file system:\r\n```python\r\n    if (io_wrapper.IsGCSPath(specification) or                                  \r\n        specification.startswith('hdfs://')): \r\n```\r\n\r\nI can contribute if no one is working on this.", "comments": ["@jhseu, since you are doing HDFS work, do you have any thoughts on this or know who might be a good person to consult. It seems like these checks could be encapsulated into some kind of filesystem registry, but maybe we already have plans to do that?  If not, please mark this contributions welcome if you agree with the premise.\n", "@llhe Yeah, if you have any ideas about what to do there, contributions are welcome. Perhaps we should just check if it's a URI?\n\nThis is very tensorboard-specific code, so I'm not sure it's worth plumbing through filesystem registry from C++ to make it work.\n", "If you send a pull request, mention me on it.\n", "@aselle @jhseu I think just check the URI is ok, check if it's start with xyz://, which won't conflict with the event file lists like group_name:/path/to/directory (Looks like the current code has a bug for group_name:xys://?). So it can be fixed without touching the C++ code.\n"]}, {"number": 5321, "title": "Filesystem FileExists interface is broken", "body": "The FileSystem define the following interface, which can't handle temporary runtime errors properly but just swallow them (like GCS, HDFS and any customized DFS),\r\n```c++\r\nvirtual bool FileExists(const string& fname) = 0;\r\n```\r\nIt should be changed to\r\n```c++\r\nvirtual Status FileExists(const string& fname) = 0;\r\n```\r\nI can provide a patch if no one is working on this.", "comments": ["@jhseu, I'm marking this contributions welcome, assuming you agree with that change.\n", "Yeah, I agree that's an issue. I can make the change, though, because we also have to change the implementation for our internal filesystem.\n", "This is more friendly (less error handling code) for the caller to handle unexpected errors:\n\n``` c++\nvirtual Status FileExists(const string& fname, bool* result) = 0;\n```\n\nAnd IsDirectory should also be changed to (both for API semantics consistency and making the caller easier to handle the runtime error, most of the current caller code just ignored them):\n\n``` c++\nvirtual Status IsDirectory(const string& fname, bool* result) = 0;\n```\n\nMay be this can be addresses by another change.\nI'm creating a patch for FileExists first.\n", "Fixed internally. It'll show up during the next sync.\n\nAs mentioned in your pull request, we decided on:\n\n``` c++\nvirtual Status FileExists(const string& fname);\n```\n\nfor consistency with how we've been using these error codes elsewhere.\n"]}, {"number": 5320, "title": "How to Read tensorflow source code with an IDE ?", "body": "Is there any IDE support bazel so that I can use IDE to read the source code ? ", "comments": ["Most IDEs have options to setup \"builders\" that can be arbitrary commands. Just make a new \"builder\" that runs \"bazel build <whatevertarget you want>\"... You can ask on stackoverflow is you want to query a larger audience about what they use as an IDE. Closing, as this is not a bug. Thanks!\n"]}, {"number": 5319, "title": "Object of type 'path' has no field \"realpath\".", "body": "I upgraded to 0.11rc1 as you have advised, @tatatodd in issue #4841 . I did a `sudo ./configure`, and this was the result:\r\n\r\n```\r\n$ sudo ./configure\r\n~/tensorflow ~/tensorflow\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] N\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.5/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]\r\n\r\n/usr/local/lib/python3.5/dist-packages\r\nDo you wish to build TensorFlow with GPU support? [y/N] Y\r\nGPU support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda-8.0\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \r\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda-8.0]: \r\nlibcudnn.so resolves to libcudnn.5\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: \r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n.\r\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/darth/tensorflow/third_party/gpus/cuda_configure.bzl\", line 517\r\n\t\t_create_cuda_repository(repository_ctx)\r\n\tFile \"/home/darth/tensorflow/third_party/gpus/cuda_configure.bzl\", line 432, in _create_cuda_repository\r\n\t\t_cuda_toolkit_path(repository_ctx, cuda_version)\r\n\tFile \"/home/darth/tensorflow/third_party/gpus/cuda_configure.bzl\", line 148, in _cuda_toolkit_path\r\n\t\tstr(repository_ctx.path(cuda_toolkit...)\r\n\tFile \"/home/darth/tensorflow/third_party/gpus/cuda_configure.bzl\", line 148, in str\r\n\t\trepository_ctx.path(cuda_toolkit_path).realpath\r\nObject of type 'path' has no field \"realpath\".\r\n```\r\n\r\nThanks in advance for your response.", "comments": ["Already fixed this by download the `tar.gz` file. The problem now is when I'm doing `bazel build`, my computer freezes. \n", "Which tar.gz file exactly?\n", "Do you have bazel 0.3.2 by the way? Might be related to this bug: https://github.com/bazelbuild/bazel/issues/1685\n", "Thanks, @gokceneraslan \n\nReplacing Bazel 0.3.0 with 0.3.2 resolves this issue on my side.\n\nMy system info:\nNvidia driver: 367.48-0ubuntu1\nCUDA: cuda-repo-ubuntu1604_8.0.44-1_amd64.deb\ncuDNN: cuDNN 5.1\n", "@AFAgarap, did you try upgrading to 0.3.2 as @willSapgreen suggests? Thanks!\n", "@gokceneraslan The 0.11rc1 tar.gz file.\n@aselle I have 0.3.1, and yes, I shall upgrade to 0.3.2. Will get back to you on this. Thanks!\n", "@aselle @willSapgreen @gokceneraslan I have upgraded bazel to 0.3.2, and yet, when I'm doing `bazel build`, my computer still crashes. It has the following hardware specs: Intel Core i5-6300HQ (2.3GHz to 3.2GHz), 8GB DDR3 RAM 1600MHz, 1TB Hybrid Hard Drive + 8GB cache, Nvidia GeForce GTX 960M 4GB DDR5. I'm using Ubuntu 16.04 LTS, with kernel 4.4.0-45 generic. As for CUDA, it's 8.0; cuDNN, 5.1; `nvidia-smi`: 367.58\n", "This bug is about `Object of type 'path' has no field \"realpath\".` error and it can be fixed by upgrading bazel to 0.3.2. I think we can close this now.\n", "@gokceneraslan I already did upgrade to bazel 0.3.2\n", "After upgrading, did you try starting from a clean repository sandbox (i.e. delete your old directory, reclone and build again)?\n", "and remove `~/.cache/bazel`.\n", "i meet similar errors and update to **bazel 0.3.2** does not fix the issue.\n\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\n    File \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/third_party/gpus/cuda_configure.bzl\", line 517\n        _create_cuda_repository(repository_ctx)\n    File \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/third_party/gpus/cuda_configure.bzl\", line 432, in _create_cuda_repository\n        _cuda_toolkit_path(repository_ctx, cuda_version)\n    File \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/third_party/gpus/cuda_configure.bzl\", line 148, in _cuda_toolkit_path\n        str(repository_ctx.path(cuda_toolkit...)\n    File \"/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/third_party/gpus/cuda_configure.bzl\", line 148, in str\n        repository_ctx.path(cuda_toolkit_path).realpath\nObject of type 'path' has no field \"realpath\".\n", "@aselle @gokceneraslan Okay, thanks, guys. I shall try your suggestions. Will get back to you.\n", "@aselle @gokceneraslan Still, the same problem. It crashes.\n\nEarly screenshot:\n\n![screenshot from 2016-11-03 19-16-36](https://cloud.githubusercontent.com/assets/11130276/19964141/0cb0241c-a1fb-11e6-8d24-cbc09af282b0.png)\n\nPhoto of my screen when `bazel build` froze my computer:\n\n![img_0744](https://cloud.githubusercontent.com/assets/11130276/19964167/320d9b72-a1fb-11e6-92ad-330e4e95ab51.JPG)\n", "Can you try something like that http://stackoverflow.com/questions/34756370/is-there-a-way-to-limit-the-number-of-cpu-cores-bazel-uses ? \n", "Sure, will do. Thanks\n", "I did `bazel build -c opt --config=cuda --local_resources 5120,2.0,1.0 //tensorflow/cc:tutorials_example_trainer`. Looks like it's still using all four cores of my computer. Though my computer is not yet crashing right now.\n\n![screenshot from 2016-11-03 19-43-39](https://cloud.githubusercontent.com/assets/11130276/19964672/157974e2-a1fe-11e6-8fd1-4dbdb817e294.png)\n", "This is the screenshot of my computer after `bazel build -c opt --config=cuda --local_resources 5120,2.0,1.0 //tensorflow/cc:tutorials_example_trainer` was done. I suppose, it's successful? I'lll try to build the pip package now.\n\n![screenshot from 2016-11-03 20-10-39](https://cloud.githubusercontent.com/assets/11130276/19965490/288ac74e-a202-11e6-96e1-4220c58f0ae0.png)\n", "@gokceneraslan @aselle This is the result of my `bazel build -c opt --config=cuda --local_resources 5120,2.0,1.0 //tensorflow/tools/pip_package:build_pip_package`:\n\n```\nINFO: Found 1 target...\nINFO: From Compiling tensorflow/core/kernels/matrix_solve_op.cc:\ntensorflow/core/kernels/matrix_solve_op.cc:78:7: warning: multi-line comment [-Wcomment]\n       // Make sure to backport: https://bitbucket.org/eigen/eigen/commits/ \\\n       ^\ntensorflow/core/kernels/matrix_solve_op.cc:98:5: warning: multi-line comment [-Wcomment]\n     // https://bitbucket.org/eigen/eigen/pull-requests/174/ \\\n     ^\nINFO: From Compiling tensorflow/core/kernels/tile_ops_gpu.cu.cc:\nKilled\nERROR: /home/darth/tensorflow/tensorflow/core/kernels/BUILD:422:1: output 'tensorflow/core/kernels/_objs/tile_ops_gpu/tensorflow/core/kernels/tile_ops_gpu.cu.pic.o' was not created.\nERROR: /home/darth/tensorflow/tensorflow/core/kernels/BUILD:422:1: not all outputs were created.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n```\n", "Okay, I re-run `bazel build -c opt --config=cuda --local_resources 5120,2.0,1.0 //tensorflow/tools/pip_package:build_pip_package`, and it went fine. Though I was not able to see its output since I added a `| less`. But, when I did `bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`, then `sudo pip install /tmp/tensorflow_pkg/tensorflow-0.10.0-py2-none-any.whl`. I got it installed!\n\n![screenshot from 2016-11-03 21-05-42](https://cloud.githubusercontent.com/assets/11130276/19967011/5b48bf72-a209-11e6-9601-87e8aa178b20.png)\n\n@aselle @gokceneraslan This is fine now, right?\n\nI ran `python3 ~/tensorflow/tensorflow/models/image/mnist/convolutional.py`, and got the following results:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Found device 0 with properties: \nname: GeForce GTX 960M\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.176\npciBusID 0000:02:00.0\nTotal memory: 3.95GiB\nFree memory: 3.63GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:889] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:899] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:958] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:02:00.0)\nInitialized!\nStep 0 (epoch 0.00), 110.4 ms\nMinibatch loss: 8.334, learning rate: 0.010000\nMinibatch error: 85.9%\nValidation error: 84.6%\nStep 100 (epoch 0.12), 16.2 ms\nMinibatch loss: 3.264, learning rate: 0.010000\nMinibatch error: 7.8%\nValidation error: 7.3%\nStep 200 (epoch 0.23), 16.3 ms\nMinibatch loss: 3.376, learning rate: 0.010000\nMinibatch error: 10.9%\nValidation error: 4.5%\nStep 300 (epoch 0.35), 16.5 ms\nMinibatch loss: 3.172, learning rate: 0.010000\nMinibatch error: 3.1%\nValidation error: 3.1%\nStep 400 (epoch 0.47), 16.3 ms\nMinibatch loss: 3.218, learning rate: 0.010000\nMinibatch error: 6.2%\nValidation error: 2.9%\nStep 500 (epoch 0.58), 16.3 ms\nMinibatch loss: 3.167, learning rate: 0.010000\nMinibatch error: 4.7%\nValidation error: 2.5%\nStep 600 (epoch 0.70), 16.6 ms\nMinibatch loss: 3.092, learning rate: 0.010000\nMinibatch error: 3.1%\nValidation error: 2.1%\nStep 700 (epoch 0.81), 16.9 ms\nMinibatch loss: 2.961, learning rate: 0.010000\nMinibatch error: 1.6%\nValidation error: 2.2%\nStep 800 (epoch 0.93), 16.4 ms\nMinibatch loss: 3.088, learning rate: 0.010000\nMinibatch error: 6.2%\nValidation error: 1.8%\nStep 900 (epoch 1.05), 16.5 ms\nMinibatch loss: 2.905, learning rate: 0.009500\nMinibatch error: 1.6%\nValidation error: 1.5%\nStep 1000 (epoch 1.16), 16.7 ms\nMinibatch loss: 2.882, learning rate: 0.009500\nMinibatch error: 1.6%\nValidation error: 1.7%\nStep 1100 (epoch 1.28), 16.6 ms\nMinibatch loss: 2.822, learning rate: 0.009500\nMinibatch error: 0.0%\nValidation error: 1.4%\nStep 1200 (epoch 1.40), 16.9 ms\nMinibatch loss: 2.964, learning rate: 0.009500\nMinibatch error: 3.1%\nValidation error: 1.5%\nStep 1300 (epoch 1.51), 16.5 ms\nMinibatch loss: 2.795, learning rate: 0.009500\nMinibatch error: 1.6%\nValidation error: 1.6%\nStep 1400 (epoch 1.63), 16.1 ms\nMinibatch loss: 2.810, learning rate: 0.009500\nMinibatch error: 1.6%\nValidation error: 1.5%\nStep 1500 (epoch 1.75), 16.1 ms\nMinibatch loss: 2.887, learning rate: 0.009500\nMinibatch error: 3.1%\nValidation error: 1.3%\nStep 1600 (epoch 1.86), 16.2 ms\nMinibatch loss: 2.714, learning rate: 0.009500\nMinibatch error: 1.6%\nValidation error: 1.3%\nStep 1700 (epoch 1.98), 16.5 ms\nMinibatch loss: 2.661, learning rate: 0.009500\nMinibatch error: 0.0%\nValidation error: 1.7%\nStep 1800 (epoch 2.09), 16.4 ms\nMinibatch loss: 2.669, learning rate: 0.009025\nMinibatch error: 1.6%\nValidation error: 1.3%\nStep 1900 (epoch 2.21), 15.8 ms\nMinibatch loss: 2.621, learning rate: 0.009025\nMinibatch error: 0.0%\nValidation error: 1.2%\nStep 2000 (epoch 2.33), 16.3 ms\nMinibatch loss: 2.603, learning rate: 0.009025\nMinibatch error: 1.6%\nValidation error: 1.2%\nStep 2100 (epoch 2.44), 16.0 ms\nMinibatch loss: 2.573, learning rate: 0.009025\nMinibatch error: 0.0%\nValidation error: 1.2%\nStep 2200 (epoch 2.56), 16.3 ms\nMinibatch loss: 2.564, learning rate: 0.009025\nMinibatch error: 0.0%\nValidation error: 1.1%\nStep 2300 (epoch 2.68), 16.4 ms\nMinibatch loss: 2.565, learning rate: 0.009025\nMinibatch error: 1.6%\nValidation error: 1.1%\nStep 2400 (epoch 2.79), 16.1 ms\nMinibatch loss: 2.501, learning rate: 0.009025\nMinibatch error: 0.0%\nValidation error: 1.1%\nStep 2500 (epoch 2.91), 16.1 ms\nMinibatch loss: 2.472, learning rate: 0.009025\nMinibatch error: 0.0%\nValidation error: 1.1%\nStep 2600 (epoch 3.03), 16.1 ms\nMinibatch loss: 2.463, learning rate: 0.008574\nMinibatch error: 0.0%\nValidation error: 1.3%\nStep 2700 (epoch 3.14), 16.1 ms\nMinibatch loss: 2.512, learning rate: 0.008574\nMinibatch error: 1.6%\nValidation error: 1.1%\nStep 2800 (epoch 3.26), 16.1 ms\nMinibatch loss: 2.458, learning rate: 0.008574\nMinibatch error: 3.1%\nValidation error: 1.2%\nStep 2900 (epoch 3.37), 16.2 ms\nMinibatch loss: 2.489, learning rate: 0.008574\nMinibatch error: 1.6%\nValidation error: 1.1%\nStep 3000 (epoch 3.49), 16.4 ms\nMinibatch loss: 2.405, learning rate: 0.008574\nMinibatch error: 3.1%\nValidation error: 0.9%\nStep 3100 (epoch 3.61), 16.1 ms\nMinibatch loss: 2.407, learning rate: 0.008574\nMinibatch error: 3.1%\nValidation error: 1.0%\nStep 3200 (epoch 3.72), 16.0 ms\nMinibatch loss: 2.345, learning rate: 0.008574\nMinibatch error: 1.6%\nValidation error: 1.1%\nStep 3300 (epoch 3.84), 16.1 ms\nMinibatch loss: 2.326, learning rate: 0.008574\nMinibatch error: 0.0%\nValidation error: 1.0%\nStep 3400 (epoch 3.96), 16.6 ms\nMinibatch loss: 2.300, learning rate: 0.008574\nMinibatch error: 1.6%\nValidation error: 1.2%\nStep 3500 (epoch 4.07), 16.3 ms\nMinibatch loss: 2.278, learning rate: 0.008145\nMinibatch error: 0.0%\nValidation error: 1.0%\nStep 3600 (epoch 4.19), 16.5 ms\nMinibatch loss: 2.250, learning rate: 0.008145\nMinibatch error: 0.0%\nValidation error: 0.9%\nStep 3700 (epoch 4.31), 16.2 ms\nMinibatch loss: 2.229, learning rate: 0.008145\nMinibatch error: 0.0%\nValidation error: 1.0%\nStep 3800 (epoch 4.42), 16.1 ms\nMinibatch loss: 2.218, learning rate: 0.008145\nMinibatch error: 0.0%\nValidation error: 0.9%\nStep 3900 (epoch 4.54), 16.0 ms\nMinibatch loss: 2.255, learning rate: 0.008145\nMinibatch error: 3.1%\nValidation error: 1.0%\nStep 4000 (epoch 4.65), 16.3 ms\nMinibatch loss: 2.243, learning rate: 0.008145\nMinibatch error: 3.1%\nValidation error: 1.0%\nStep 4100 (epoch 4.77), 16.2 ms\nMinibatch loss: 2.165, learning rate: 0.008145\nMinibatch error: 0.0%\nValidation error: 0.9%\nStep 4200 (epoch 4.89), 16.4 ms\nMinibatch loss: 2.160, learning rate: 0.008145\nMinibatch error: 1.6%\nValidation error: 1.0%\nStep 4300 (epoch 5.00), 16.1 ms\nMinibatch loss: 2.188, learning rate: 0.007738\nMinibatch error: 1.6%\nValidation error: 1.0%\nStep 4400 (epoch 5.12), 16.2 ms\nMinibatch loss: 2.120, learning rate: 0.007738\nMinibatch error: 0.0%\nValidation error: 1.0%\nStep 4500 (epoch 5.24), 16.1 ms\nMinibatch loss: 2.202, learning rate: 0.007738\nMinibatch error: 4.7%\nValidation error: 1.0%\nStep 4600 (epoch 5.35), 16.3 ms\nMinibatch loss: 2.088, learning rate: 0.007738\nMinibatch error: 0.0%\nValidation error: 0.9%\nStep 4700 (epoch 5.47), 16.0 ms\nMinibatch loss: 2.086, learning rate: 0.007738\nMinibatch error: 1.6%\nValidation error: 0.9%\nStep 4800 (epoch 5.59), 16.7 ms\nMinibatch loss: 2.062, learning rate: 0.007738\nMinibatch error: 1.6%\nValidation error: 1.0%\nStep 4900 (epoch 5.70), 16.1 ms\nMinibatch loss: 2.061, learning rate: 0.007738\nMinibatch error: 1.6%\nValidation error: 1.0%\nStep 5000 (epoch 5.82), 16.7 ms\nMinibatch loss: 2.084, learning rate: 0.007738\nMinibatch error: 3.1%\nValidation error: 0.8%\nStep 5100 (epoch 5.93), 17.7 ms\nMinibatch loss: 2.006, learning rate: 0.007738\nMinibatch error: 1.6%\nValidation error: 1.1%\nStep 5200 (epoch 6.05), 16.3 ms\nMinibatch loss: 2.072, learning rate: 0.007351\nMinibatch error: 3.1%\nValidation error: 0.8%\nStep 5300 (epoch 6.17), 16.4 ms\nMinibatch loss: 1.970, learning rate: 0.007351\nMinibatch error: 0.0%\nValidation error: 0.8%\nStep 5400 (epoch 6.28), 16.6 ms\nMinibatch loss: 1.957, learning rate: 0.007351\nMinibatch error: 0.0%\nValidation error: 0.9%\nStep 5500 (epoch 6.40), 16.3 ms\nMinibatch loss: 1.997, learning rate: 0.007351\nMinibatch error: 1.6%\nValidation error: 1.0%\nStep 5600 (epoch 6.52), 16.3 ms\nMinibatch loss: 1.943, learning rate: 0.007351\nMinibatch error: 1.6%\nValidation error: 0.8%\nStep 5700 (epoch 6.63), 16.2 ms\nMinibatch loss: 1.915, learning rate: 0.007351\nMinibatch error: 0.0%\nValidation error: 0.8%\nStep 5800 (epoch 6.75), 16.2 ms\nMinibatch loss: 1.898, learning rate: 0.007351\nMinibatch error: 0.0%\nValidation error: 0.8%\nStep 5900 (epoch 6.87), 16.2 ms\nMinibatch loss: 1.890, learning rate: 0.007351\nMinibatch error: 0.0%\nValidation error: 0.9%\nStep 6000 (epoch 6.98), 16.2 ms\nMinibatch loss: 1.912, learning rate: 0.007351\nMinibatch error: 1.6%\nValidation error: 0.9%\nStep 6100 (epoch 7.10), 16.0 ms\nMinibatch loss: 1.864, learning rate: 0.006983\nMinibatch error: 0.0%\nValidation error: 0.9%\nStep 6200 (epoch 7.21), 16.1 ms\nMinibatch loss: 1.843, learning rate: 0.006983\nMinibatch error: 0.0%\nValidation error: 0.8%\nStep 6300 (epoch 7.33), 16.1 ms\nMinibatch loss: 1.855, learning rate: 0.006983\nMinibatch error: 1.6%\nValidation error: 0.9%\nStep 6400 (epoch 7.45), 16.1 ms\nMinibatch loss: 1.836, learning rate: 0.006983\nMinibatch error: 1.6%\nValidation error: 0.8%\nStep 6500 (epoch 7.56), 16.9 ms\nMinibatch loss: 1.806, learning rate: 0.006983\nMinibatch error: 0.0%\nValidation error: 0.9%\nStep 6600 (epoch 7.68), 16.6 ms\nMinibatch loss: 1.825, learning rate: 0.006983\nMinibatch error: 1.6%\nValidation error: 0.8%\nStep 6700 (epoch 7.80), 16.5 ms\nMinibatch loss: 1.783, learning rate: 0.006983\nMinibatch error: 0.0%\nValidation error: 0.8%\nStep 6800 (epoch 7.91), 16.4 ms\nMinibatch loss: 1.773, learning rate: 0.006983\nMinibatch error: 0.0%\nValidation error: 0.9%\nStep 6900 (epoch 8.03), 16.5 ms\nMinibatch loss: 1.759, learning rate: 0.006634\nMinibatch error: 0.0%\nValidation error: 0.9%\nStep 7000 (epoch 8.15), 16.5 ms\nMinibatch loss: 1.757, learning rate: 0.006634\nMinibatch error: 0.0%\nValidation error: 0.9%\nStep 7100 (epoch 8.26), 15.9 ms\nMinibatch loss: 1.734, learning rate: 0.006634\nMinibatch error: 0.0%\nValidation error: 0.8%\nStep 7200 (epoch 8.38), 16.1 ms\nMinibatch loss: 1.728, learning rate: 0.006634\nMinibatch error: 0.0%\nValidation error: 0.9%\nStep 7300 (epoch 8.49), 16.1 ms\nMinibatch loss: 1.779, learning rate: 0.006634\nMinibatch error: 3.1%\nValidation error: 0.8%\nStep 7400 (epoch 8.61), 16.4 ms\nMinibatch loss: 1.699, learning rate: 0.006634\nMinibatch error: 0.0%\nValidation error: 0.7%\nStep 7500 (epoch 8.73), 16.4 ms\nMinibatch loss: 1.690, learning rate: 0.006634\nMinibatch error: 0.0%\nValidation error: 0.7%\nStep 7600 (epoch 8.84), 16.4 ms\nMinibatch loss: 1.775, learning rate: 0.006634\nMinibatch error: 1.6%\nValidation error: 0.9%\nStep 7700 (epoch 8.96), 16.3 ms\nMinibatch loss: 1.666, learning rate: 0.006634\nMinibatch error: 0.0%\nValidation error: 0.9%\nStep 7800 (epoch 9.08), 16.3 ms\nMinibatch loss: 1.665, learning rate: 0.006302\nMinibatch error: 0.0%\nValidation error: 0.8%\nStep 7900 (epoch 9.19), 15.8 ms\nMinibatch loss: 1.647, learning rate: 0.006302\nMinibatch error: 0.0%\nValidation error: 0.8%\nStep 8000 (epoch 9.31), 17.0 ms\nMinibatch loss: 1.648, learning rate: 0.006302\nMinibatch error: 0.0%\nValidation error: 0.8%\nStep 8100 (epoch 9.43), 16.9 ms\nMinibatch loss: 1.634, learning rate: 0.006302\nMinibatch error: 0.0%\nValidation error: 0.9%\nStep 8200 (epoch 9.54), 16.5 ms\nMinibatch loss: 1.619, learning rate: 0.006302\nMinibatch error: 0.0%\nValidation error: 0.8%\nStep 8300 (epoch 9.66), 16.4 ms\nMinibatch loss: 1.614, learning rate: 0.006302\nMinibatch error: 0.0%\nValidation error: 0.7%\nStep 8400 (epoch 9.77), 16.5 ms\nMinibatch loss: 1.597, learning rate: 0.006302\nMinibatch error: 0.0%\nValidation error: 0.8%\nStep 8500 (epoch 9.89), 16.4 ms\nMinibatch loss: 1.617, learning rate: 0.006302\nMinibatch error: 1.6%\nValidation error: 0.9%\nTest error: 0.7%\n```\n", "OK, let's close the bug then.\n", "Thanks a lot, @gokceneraslan @aselle !\n", "I also got tile_ops_gpu.cu.pic.o' was not created.\r\n\r\nWhat have you done to fix it ??", "Did you try re-running `bazel build`, @marcosmoura91 ? That's what I did.", "@AFAgarap \r\nI actually quitted installing from sources...  I don't know why most of tutorials advise that.  So I just installed using pip install and it went great and quick.  So now it is working 100%."]}, {"number": 5318, "title": "Consistently use bytes for git version tracking for Python3 compatibi\u2026", "body": "\u2026lity\r\n\r\nI'm getting the same TypeError as in #4701 on current master.\r\nThis patch fixes it by making sure that `write_version_info` always receives `bytes` for `git_version` param.", "comments": ["@nsuke, thanks for your PR! By analyzing the history of the files in this pull request, we identified @aselle, @Phhere and @meteorcloudy to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "I would personally make a minor edit using\n\n``` python\nimport builtins\nval = builtins.bytes(subprocess.check_output...\n```\n\nbecause the word `bytes` might get overridden by some custom defined method. [Source](https://docs.python.org/3/library/builtins.html)\n", "Is builtins available in python2? I  couldn't find the docs in the 2.7 version.  https://docs.python.org/2/library/functions.html\n", "Oh yeah -- I stand corrected, my bad! Python 2.x doesn't have `builtins`. \n", "Looks like it should work on Windows (AIUI, `subprocess.check_output()` returns the same type on Windows and Linux, and the previous Windows failures were because we were hitting an untested failure case). The Windows presubmit should reveal whether it works or not....\n", "@tensorflow-jenkins test this please\n", "Cool, kicking off tests.  Will merge if it's all passing.\n"]}, {"number": 5317, "title": "Build - leftover bazel processes after build complete", "body": "After running a successful build, 2 or 3 bazel processes remain along with many child processes for each one - they don't take up any CPU resources. Have to kill the processes manually to get rid of them.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone!\r\n\r\n### Environment info\r\nOperating System:\r\n```Slackware 64 14.2```\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\nlibcudadevrt.a\r\nlibcudart.so -> libcudart.so.8.0\r\nlibcudart.so.8.0 -> libcudart.so.8.0.44\r\nlibcudart.so.8.0.44\r\nlibcudart_static.a\r\nlibcudnn.so -> libcudnn.so.5\r\nlibcudnn.so.5 -> libcudnn.so.5.1.5\r\nlibcudnn.so.5.1.5\r\nlibcudnn_static.a\r\n```\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n```6f7cf60a4158a6d7861dc1b41a6446b575153a2e```\r\n\r\n2. The output of `bazel version`\r\n```\r\nBuild label: 0.3.2- (@non-git)\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Sun Oct 16 21:50:19 2016 (1476654619)\r\nBuild timestamp: 1476654619\r\nBuild timestamp as int: 1476654619\r\n```\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\no Configure with GPU support and run a standard build.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n```\r\nps aux | grep bazel\r\n```\r\n```\r\nroot     21080  0.5  0.3 50920612 649256 ?     Ssl  19:00   0:09 bazel(tensorflow) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/root/.cache/bazel/_bazel_root/d63fa4ac02658c8e5f9c2e0f1f523a93 -Xverify:none -Djava.util.logging.config.file=/root/.cache/bazel/_bazel_root/d63fa4ac02658c8e5f9c2e0f1f523a93/javalog.properties -Djava.library.path=/root/.cache/bazel/_bazel_root/install/26cccb4705ab94af88df4f6dfb1e20c4/_embedded_binaries/ -Dfile.encoding=ISO-8859-1 -jar /root/.cache/bazel/_bazel_root/install/26cccb4705ab94af88df4f6dfb1e20c4/_embedded_binaries/A-server.jar --max_idle_secs 10800 --install_base=/root/.cache/bazel/_bazel_root/install/26cccb4705ab94af88df4f6dfb1e20c4 --install_md5=26cccb4705ab94af88df4f6dfb1e20c4 --output_base=/root/.cache/bazel/_bazel_root/d63fa4ac02658c8e5f9c2e0f1f523a93 --workspace_directory=/usr/local/src/tensorflow/tensorflow-git --deep_execroot --experimental_oom_more_eagerly_threshold=100 --nofatal_event_bus_exceptions --product_name=Bazel --option_sources=\r\nroot     23518 10.0  1.1 51042312 2339964 ?    Ssl  19:03   2:31 bazel(tensorflow) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/root/.cache/bazel/_bazel_root/d63fa4ac02658c8e5f9c2e0f1f523a93 -Xverify:none -Djava.util.logging.config.file=/root/.cache/bazel/_bazel_root/d63fa4ac02658c8e5f9c2e0f1f523a93/javalog.properties -Djava.library.path=/root/.cache/bazel/_bazel_root/install/26cccb4705ab94af88df4f6dfb1e20c4/_embedded_binaries/ -Dfile.encoding=ISO-8859-1 -jar /root/.cache/bazel/_bazel_root/install/26cccb4705ab94af88df4f6dfb1e20c4/_embedded_binaries/A-server.jar --max_idle_secs 10800 --install_base=/root/.cache/bazel/_bazel_root/install/26cccb4705ab94af88df4f6dfb1e20c4 --install_md5=26cccb4705ab94af88df4f6dfb1e20c4 --output_base=/root/.cache/bazel/_bazel_root/d63fa4ac02658c8e5f9c2e0f1f523a93 --workspace_directory=/usr/local/src/tensorflow/tensorflow-git --deep_execroot --experimental_oom_more_eagerly_threshold=100 --nofatal_event_bus_exceptions --product_name=Bazel --option_sources=\r\n```", "comments": ["@jart, this looks a little like the bug you filed against bazel concerning persistent workers https://github.com/bazelbuild/bazel/issues/1868?\n\nCould it be related to this https://github.com/bazelbuild/bazel/issues/614?\n(did you interrupt it?)\n\nDo you find that if you clean and run another build you keep getting an accumulation of more processes?\n", "Thanks for following up - as a workaround I'm currently doing `pkill -f \"bazel\\(tensorflow\\)\"` after each build. Also I'm deleting /root/.cache/bazel/_bazel_root/ after the build since on most Linux systems the root partition is quite small and these gigabyte+ caches are going to fill up /root rather quickly.\n", "Sounds like it could be related to https://github.com/bazelbuild/bazel/issues/614 yes but I'm seeing the issue even without interrupting anything using CTRL-C. I just ran a successful ./configure and the following process is left over:\n\n```\nroot     29914  167  0.6 51021036 1312840 ?    Ssl  10:43   1:23 bazel(tensorflow) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/root/.cache/bazel/_bazel_root/d63fa4ac02658c8e5f9c2e0f1f523a93 -Xverify:none -Djava.util.logging.config.file=/root/.cache/bazel/_bazel_root/d63fa4ac02658c8e5f9c2e0f1f523a93/javalog.properties -Djava.library.path=/root/.cache/bazel/_bazel_root/install/26cccb4705ab94af88df4f6dfb1e20c4/_embedded_binaries/ -Dfile.encoding=ISO-8859-1 -jar /root/.cache/bazel/_bazel_root/install/26cccb4705ab94af88df4f6dfb1e20c4/_embedded_binaries/A-server.jar --max_idle_secs 10800 --install_base=/root/.cache/bazel/_bazel_root/install/26cccb4705ab94af88df4f6dfb1e20c4 --install_md5=26cccb4705ab94af88df4f6dfb1e20c4 --output_base=/root/.cache/bazel/_bazel_root/d63fa4ac02658c8e5f9c2e0f1f523a93 --workspace_directory=/usr/local/src/tensorflow/tensorflow-git --deep_execroot --experimental_oom_more_eagerly_threshold=100 --nofatal_event_bus_exceptions --product_name=Bazel --option_sources=\n```\n", "Bazel keeps a d\u00e6mon process in the background in order to make subsequent\ninvocations of the Bazel command go blazing fast. If this is not what you\nwant, run `bazel shutdown` after the build, or use the `--batch` flag. This\nissue can be closed. The issue I reported regarding persistent workers\nshouldn't apply here, because those are opt in.\n\nOn Nov 1, 2016 10:40 AM, \"Eduard Rozenberg\" notifications@github.com\nwrote:\n\n> Thanks for following up - as a workaround I'm currently doing pkill -f\n> \"bazel(tensorflow)\" after each build. Also I'm deleting\n> /root/.cache/bazel/_bazel_root/ after the build since on most Linux\n> systems the root partition is quite small and these gigabyte+ caches are\n> going to fill up /root rather quickly.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5317#issuecomment-257635144,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AADAbpbFCtnvKy498vE6E1V-EUIKyG-lks5q53lrgaJpZM4Klueu\n> .\n", "Thanks I'll try `bazel shutdown` from now on - a build tool that spawns a daemon was unexpected. I did see up to 2-3 leftover processes at least a couple of times, not just one.\n", "I would recommend filing an issue with Bazel asking them to display a log message indicating when it's starting a d\u00e6mon, which can be shutdown with bazel shutdown. I would support that feature.\n"]}, {"number": 5316, "title": "Invalid argument when accessing HDFS", "body": "We have installed pseudo-distributed HDFS in local and use the default port, 9000. We follow the example code from `https://github.com/tensorflow/tensorflow/issues/2218` but it throws `InvalidArgumentError`.\r\n\r\nOur code looks like this and it works if we change to local filesystem.\r\n\r\n```\r\nhdfs_path = \"hdfs://127.0.0.1:9000/cancer_train.csv.tfrecords\"\r\nfilename_queue = tf.train.string_input_producer(\r\n    tf.train.match_filenames_once(hdfs_path),\r\n    num_epochs=epoch_number)\r\nlabel, features = read_and_decode(filename_queue)\r\nbatch_labels, batch_features = tf.train.shuffle_batch(\r\n    [label, features],\r\n    batch_size=batch_size,\r\n    num_threads=thread_number,\r\n    capacity=capacity,\r\n    min_after_dequeue=min_after_dequeue)\r\n```\r\n\r\nAnd the error log looks like this.\r\n\r\n```\r\nhdfsGetPathInfo(): constructNewObjectOfPath error:\r\njava.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:126)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:134)\r\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: hdfs://127.0.0.1:9000\r\nhdfsGetPathInfo(): constructNewObjectOfPath error:\r\njava.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:126)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:134)\r\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: hdfs://127.0.0.1:9000\r\nhdfsGetPathInfo(): constructNewObjectOfPath error:\r\njava.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:126)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:134)\r\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: hdfs://127.0.0.1:9000\r\nhdfsGetPathInfo(): constructNewObjectOfPath error:\r\njava.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:126)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:134)\r\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: hdfs://127.0.0.1:9000\r\nTraceback (most recent call last):\r\n  File \"./hdfs.py\", line 237, in <module>\r\n    sess.run(init_op)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 717, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 915, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 985, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors.InvalidArgumentError: hdfs://127.0.0.1:9000\r\n         [[Node: matching_filenames/MatchingFiles = MatchingFiles[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](matching_filenames/MatchingFiles/pattern)]]\r\n         [[Node: matching_filenames_1/Assign/_6 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send\r\n_device_incarnation=1, tensor_name=\"edge_117_matching_filenames_1/Assign\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n```\r\n\r\n### Environment info\r\nOperating System: CentOS 7.0\r\nTensorFlow: 0.11.0rc1\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["@jhseu, could you take a quick a look at this please?\n", "This is now fixed internally and will be pushed during the next sync. In the meanwhile, a workaround is to put your data somewhere in a subdirectory so it's not in your HDFS root.\n", "Thanks @jhseu and that works for me.\n\nI'm sure if it supports kerberos now and can @jhseu help to verify that?\n", "We have tested and confirm that it doesn't support HDFS with kerberos. I'm gonna open another issue to track that if you don't mind. Hope Google would help to work for it :)\n"]}, {"number": 5315, "title": "Add synthetic datasets", "body": "This is only one possible synthetic dataset. Following methods added + sample code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Create synthetic data:\r\n## Generic make_synthetic wrapper routine\r\ncircles = tf.contrib.learn.datasets.make_synthetic(name='circles', n_samples=1000, noise=0.1)\r\n```\r\n\r\nFiles modified/added:\r\n- `/.gitignore`: **/tools/git/gen is not necessary to commit (for dev only)\r\n- `/tensorflow/contrib/learn/python/learn/datasets/__init__.py`: Added the support for `make_synthetic` and `split_data`\r\n- `/tensorflow/contrib/learn/python/learn/datasets/base.py`: Added new method `split_into_train_and_test`\r\n- `/tensorflow/contrib/learn/python/learn/datasets/synthetic.py`: Created a file to hold all synthetic generators", "comments": ["Can one of the admins verify this patch?\n", "@zafartahirov, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ilblackdragon, @tensorflower-gardener and @keveman to be potential reviewers.\n", "Sorry about two commits -- should have rebased it before pushing\n", "BTW, an unrelated question, what is TFs and Google's policy on rebasing the commits -- do I have to rebase to the current origin/HEAD before submitting a PR? Also, do I need to squash intermediate (minor) commits? Sorry, I am new to TF contributions, and don't know where to find these answers\n", "@ilblackdragon -- Will fix the CL over the weekend\n", "I have gone through the comments by @ilblackdragon and fixed most of them. The one I didn't touch is in the inline comments for the code.\n\nI have also included some unittests\n", "@ilblackdragon I have covered your reviews (all except one), also included a new routine -- spiral generation, and placed unittests.\n", "Done with all changes\n\n@ilblackdragon I have removed the `train_test_split` routine -- I don't think it is required for this PR as it is not used anywhere.\n", "@ilblackdragon Can you please check it again, I submitted the CL", "@tensorflow-jenkins Test this please", "Oh snap! I am on it!", "For some reason it passes all the tests on my machine -- going to rebase it to the HEAD, and see if anything new is breaking it. Is there any way I could get the Travis' logs to see what exactly fails?", "It's a python3 error, and an easy fix. Look at the full log here: \r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/2439/consoleFull\r\n\r\nIf you want to check locally, make sure to configure and run using python3.\r\n", "@martinwicke Thanks for pointing out the Python 3 issue, it was a minor `iteritems` problem -- fixed it, hope Jenkins has `six` installed.\r\n\r\n@martinwicke @ilblackdragon That actually raises a question -- is it possible to have `tensorflow-devel` compiled for both Py2 and Py3 in the same location? Or do I have to maintain two different locations on my machine if I want to test both? In my case I had it compiled for Py2 before, and had to reconfigure and rebuild for Py3 because `bazel test --force_python=py3 --host_force_python=py3 --define PYTHON_BIN_PATH=/usr/local/bin/python3 //tensorflow/contrib/learn/python/learn/datasets:synthetic_test` was throwing a `*.so Symbol not found` error. Now I am rebuilding it back to Py2 because all my other scripts are using Py2.", "@tensorflow-jenkins Test this please", "OK, I don't think [this](https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/2803/) is something I can fix -- there is a problem with `git fetch` on the remote machine. Wild guess -- a problem with web-hook?", "Restarted the failing build (only) at http://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/2815/\r\n\r\n@danmane, if that passes, it's fine to merge.", "@danmane I think this once could be merged -- Thank you!", "Thanks for this!", "I got `ImportError: cannot import name synthetic` when start tensorboard, I'm using the latest code. `synthetic.py` should be declared in BUILD file."]}, {"number": 5314, "title": "Add more \"synthetic\" datasets in the `contrib.learn`", "body": "The feature request is as in the title -- having synthetic datasets for demo purposes, and to see how different algorithms behave would be beneficial. It is fairly straightforward to generate two-dimensional multi-class set for a lot of synthetic problems.", "comments": ["@martinwicke @ilblackdragon, do you have any comments on this. I could mark this contributions welcome.\n", "See https://github.com/tensorflow/tensorflow/pull/5315\n", "This should be reopened -- there was accidental contamination which caused the commit comment include the reference to the current issue. The actual solution to the current issue is #5315 \n", "Sorry, accidentally closed.\n"]}, {"number": 5313, "title": "Branch 137767898", "body": "", "comments": ["@vrv, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @RenatoUtsch and @keveman to be potential reviewers.\n"]}, {"number": 5312, "title": "Add dynamic_pad flag to shuffle_batch as in batch ( ~python/training/input.py)", "body": "The changed have been made as per #5147 ", "comments": ["Can one of the admins verify this patch?\n", "Where's the c++ implementation of the new queue?\n", "Please resubmit if/when you have time to implement the C++ PaddingShuffleQueue object.  As I mentioned in the #5147 that is the nontrivial part.\n"]}, {"number": 5311, "title": "Branch 137743425", "body": "", "comments": []}, {"number": 5310, "title": "Makefile downloads/ breaks ./configure", "body": "I'm rebuilding tf on Sierra and with CUDA 8.0 and Xcode 8.0 but ./configure fails with:\r\n\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n........\r\nERROR: /Users/serkan/tensorflow/tensorflow/contrib/makefile/downloads/protobuf/BUILD:544:1: no such target '//external:gson': target 'gson' not declared in package 'external' defined by /Users/serkan/tensorflow/WORKSPACE and referenced by '//tensorflow/contrib/makefile/downloads/protobuf:protobuf_java_util'.\r\nERROR: /Users/serkan/tensorflow/tensorflow/contrib/makefile/downloads/protobuf/BUILD:544:1: no such target '//external:gson': target 'gson' not declared in package 'external' defined by /Users/serkan/tensorflow/WORKSPACE and referenced by '//tensorflow/contrib/makefile/downloads/protobuf:protobuf_java_util'.\r\nERROR: /Users/serkan/tensorflow/tensorflow/contrib/makefile/downloads/protobuf/BUILD:544:1: no such target '//external:guava': target 'guava' not declared in package 'external' (did you mean 'java'?) defined by /Users/serkan/tensorflow/WORKSPACE and referenced by '//tensorflow/contrib/makefile/downloads/protobuf:protobuf_java_util'.\r\nERROR: Evaluation of query \"deps((//tensorflow/... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\r\n\r\nperhaps bazel has changed something about how it handles dependencies?", "comments": ["Unfortunately we have not built or tested on Mac OS sierra yet. I will keep this open and let you know. We're not sure if it is a underlying Bazel not supporting MacOS sierra or if it is a tensorflow. @gunan, @jart any thoughts? Another issue is having Sierra issues too (http://github.com/tensorflow/models/issues/599). \n", "This is unrelated to Sierra. It's a flaw in the design of contrib/makefile. What most likely happened is you tried out the Makefile build system. Then you decided to go back to using Bazel. Because `bazel fetch //tensorflow/...` crawled the protobuf build files in the downloads directory left over by the makefile, and those BUILD files defined external dependencies we don't specify, things didn't work.\n\nTo work around this issue, `rm -rf tensorflow/contrib/makefile/downloads` before running `./configure`. The download_dependencies.sh script can always recreate that directory as needed.\n\nI'm not sure if this is a bug we can fix, without a nontrivial redesigning contrib/makefile. I want to thank you for bringing this to our attention. But I'm leaning towards closing this issue for now. Although @martinwicke or @gunan may re-open it if they disagree.\n", "@jart removing the makefile downloads worked. Thank you!\n", "Thanks @jart for the quick response.\n", "Always happy to be of service. Enjoy TensorFlow.\n", "@petewarden do you think we can easily fix this issue by modifying makefile?\n", "Thanks to some help from @jart I have a better understanding of what's happening here. It sounds like we could avoid this problem by removing the BUILD file as part of the download_dependencies script, and that would avoid this issue. I'll reopen this and take a look at it.\n", "@petewarden, is this resolved?", "I think it is fixed now, @andrewharp put in a change recently to remove the BUILD files from the download folders. Closing.", "Thanks!"]}, {"number": 5309, "title": "Pylint `disable` should be followed by `enable`", "body": "There is a problem, which might have high severity:\r\n\r\nA lot of files disable `pylint` messages, but don't enable them in the end -- this potentially might cause a missed warning if several files are imported, because `# pylint disable` is a global disable.\r\n\r\n[Here are all the files that modify the `pylint` related to the `wildcard-import`](https://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=pylint+disable+wildcard-import&type=Code)\r\n[Here is an example file that doesn't revert the modifications](https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/contrib/learn/python/learn/preprocessing/__init__.py)\r\n\r\nThere might be other wildcards that are disabled without reverting the changes", "comments": ["Do you have a PR that you could send?\n", "I don't -- I might be able to look into the fix over the weekend\n", "Feel free to send CC me on the PR when you're ready! Thanks.\n"]}, {"number": 5308, "title": "[Readability Issue] Using __all__ vs. wildcards", "body": "I have noticed that a lot of `__init__.py` files in the repo include \r\n\r\n```python\r\n# pylint: disable=wildcard-import\r\nfrom <...> import *\r\n# pylint: enable=wildcard-import\r\n```\r\n\r\nI believe a more appropriate approach would be to use `__all__`. According to PEP 8 documentation:\r\n\r\n> To better support introspection, modules should explicitly declare the names in their public API using the __all__ attribute. Setting __all__ to an empty list indicates that the module has no public API.", "comments": ["@drpngx is in the process of sealing our public interface via `__all__` and some other mechanisms.\n", "Actually, there is another problem, and I am not sure if I should open a new issue (in case the introduction of `__all__` fixes it) -- it is a little more severe:\n\nA lot of files disable `pylint` messages, but don't enable them in the end -- this potentially might cause a missed warning if several files are imported, because `# pylint disable` is a global disable.\n\n[Here are all the files that modify the `pylint`](https://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=pylint+disable+wildcard-import&type=Code)\n[Here is an example file that doesn't revert the modifications](https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/contrib/learn/python/learn/preprocessing/__init__.py)\n", "Probably a separate issue for that one.\n\nIf you want to try to help contribute fixes for those, that would be awesome.\n", "Yes, it's a long story :-)\n\nWe are using all, but this has issues, too. So we're starting to roll out `remove_undocumented`.\n\nHere's how it works in a nutshell:\n- `__all__` only works one level down\n- `remove_undocumented` actually deletes symbols\n- Splitting between interface and implementation\n\nSo, for instance, in `tensorflow/python/__init_.py`, we do\n\n```\nfrom ...standard_ops import *\nfrom ...platform import app\n```\n\nThe `__all__` in `standard_ops` works to filter out anything that comes using the `*`. But `app` doesn't, so you still have `tf.app.print_function` and a whole slew of other things.\n\nWe're rolling out these changes. Stay tuned.\n", "@zafartahirov nice find, please send a PR fixing the missing ones\n", "Yeah, I would love to contribute to the fixes, but I have a paper due the end of the month -- will try to do it over the weekend, no promises though \ud83d\ude04 . If this issue is still open by Saturday, I will send a PR for the current issue as well as #5309\n", "To be clear, I think we have a solution for the `__all__`, but feel free to send a PR for just one file if you have ideas and we can take it from there.\n", "I was trying to see how this issue might get resolved without hurting the maintainability and without breaking the package integrity, and nothing comes to mind -- sorry, have nothing to contribute in here.\n\nI think I have found the two solutions that are being \"tried\" in the [current master](https://github.com/tensorflow/tensorflow/tree/163357b7f3e02ef9b68cd5f107285e27a7f16c8c), I am not sure which one you guys are inclined to use:\n1. [`make_all`](https://github.com/tensorflow/tensorflow/blob/163357b7f3e02ef9b68cd5f107285e27a7f16c8c/tensorflow/python/util/all_util.py#L28)\n2. resolution in the [`tensorflow/python/__init__.py`](https://github.com/tensorflow/tensorflow/tree/163357b7f3e02ef9b68cd5f107285e27a7f16c8c)\n\nPersonally I would just make sure the files that are not supposed to be imported start with `_` to indicate the protection, and use `glob.glob('./[a-zA-Z]*.py')` to get all the public files. After small preprocessing the paths could be converted to the import strings. Protected `_` and private `__` files would be a special case, and would have to be imported/included manually.\n\nThe problem with this approach it is non-standard and requires a lot of rewriting. What is the current standard at TF for in-package imports?\n", "Thanks for looking into this!\n\nWe want to retain some of the ease and flexibility that we have, while disallowing accidental imports.\n\nHere's what we do:\n1. We document symbols that we should use, with `@@foo`\n2. We use `remove_undocumented` to delete all symbols from \"selected\" modules/packages\n3. We have a whitelist\n\n\"Selected\" modules/packages are those that are imported into the `tf` namespace, for instance: `app`, `image`, `nn`, etc., transitively on down. For instance, `tf.app.flags`.\n\nThese packages have a choice of (first one chosen to be the predominant solution):\n1. Have no implementation, just `from .. import *`, or `from .. import Foo`. They typically have a counterpart, called `_impl`, for instance, `gradients_impl.py`\n2. Have implementation, but use underscore for all packages used solely for implementation as you suggest\n\nHere is an example module using the first variant, dubbed dual API/implementation:\n\n`gradients.py`: (API module)\n\n``` python\n\"\"\"Gradients doc\n@@some_func\n\"\"\"\nfrom .. import remove_undocumented\nfrom ..gradients_impl import *\nremove_undocumented(__name__, [\"some_other_func\"])\n```\n\n`gradients_impl.py`: (implementation module)\n\n``` python\n\nimport some_module\nfrom ... import some_useful_module\n\ndef some_func(): pass\ndef some_other_func(): pass\n# No remove_undocumented.\n```\n\n`foo.py`: (internal implementation module which requires some functions defined in `gradients_impl`. We make sure that `foo.py` is never imported in the `tf` namespace, directly or transitively:\n\n``` python\n\nfrom tensorflow...import gradients_impl\n\ndef foo(x):\n   return gradients_impl.my_internally_useful_function(x + 1)\n```\n\nThere are no `import *` in `gradients_impl`.\n\nHere's an example module using the second variant (dubbed the fused api/implementation).\n\n`logging.py`:\n\n``` python\n\"\"\"Logging things\n@@print_stderr\n\"\"\"\nimport sys as _sys\n\ndef print_stderr(msg):\n   _sys.stderr.write(msg)\n\nremove_undocumented(__name__, [])\n```\n\nTo recap, (after the change is fully complete)\n1. As a tensorflow user, (minus a handful of exceptions such as `tfdbg`), you are only allowed to use `import tensorflow as tf`\n2. As a tensorflow developer, you are encouraged to use `from tensorflow... import ...` in your modules\n3. As a tensorflow developer, you are not allowed to use `import *` in your implementation modules, only in API modules\n4. So, more or less our internal code remains the same, except that we have to be careful in a handful of modules (perhaps 20 or so)\n5. As a tensorflow user, you no longer have access to internal modules that have not been explicitly exported via a whitelist or a documentation reference.\n\nHTH\n", "Thank you very much for a detailed explanation. Is this information anywhere in the documentation?\n", "No, we don't have design docs out there. It might be good to have somewhere close to the style guide, though. Ultimately, if you mimic our code, then it should work; if you don't, then we should be able to catch that automatically, at some point.\n", "I think we can close this issue due to a) No new comments in the past 2+ months b) This is not really an issue, and the documentation is work in progress."]}, {"number": 5307, "title": "Changed reference from 'eigen-latest/' to 'eigen/' in pi_examples", "body": "With 'eigen-latest/' following error occurs:\r\n./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory\r\n\r\nwhen changed to 'eigen/' build completes without errors.\r\n\r\nOther mentions of this error:\r\nhttps://github.com/tensorflow/tensorflow/issues/4680\r\nhttps://github.com/tensorflow/tensorflow/issues/5255", "comments": ["Can one of the admins verify this patch?\n", "@drag0, thanks for your PR! By analyzing the history of the files in this pull request, we identified @petewarden to be a potential reviewer.\n", "Thanks for the fix! This looks good to me.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 5306, "title": "Branch 137721344", "body": "", "comments": ["@vrv, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @ebrevdo to be potential reviewers.\n", "Known pip install tutorial breakage, fix was just pushed.   Pushing the rest.\n"]}, {"number": 5305, "title": "More programmatically configurable TensorBoard graph visualization", "body": "I'm repeatedly making small tweaks to graphs and visualizing them in TensorBoard.\r\nHere are two feature requests for TensorBoard that would make this workflow significantly smoother:\r\n\r\n(1)\r\nAllow the user to specify that a node should always be drawn in the main graph or as an auxiliary node.  Alternatively, add an option to draw all nodes in main graph.\r\n\r\nCurrently, TensorBoard chooses to put nodes in a subgraph as auxiliary nodes (within the subgraph). Adding a node to the main graph consists of \"open subgraph, click auxiliary node to add to main graph\", which needs to be repeated for each auxiliary node to move because \"add to main graph\" collapses all subgraphs. This pain could also be partially alleviated by an option to \"Shift + Click\" to add/remove multiple nodes from the main graph at once.\r\n\r\n(2)\r\nAllow the user to force edges to be drawn when both nodes are in the main graph.\r\n\r\nExample case:\r\nLet op ``X`` be in the top level graph. Let ``SG`` be a subgraph and let ``SG`` contain 10 subgraphs ``F_i`` for ``i=1,...,10`` that all take ``X`` as input.\r\nTensorBoard will draw an edge from ``X`` to ``SG``, but each ``F_i`` will have it's dependence on ``X`` noted as an auxiliary input. There is no way to add ``X`` to the main graph because it is already in the main graph (but visualized as an aux input). \r\nIt would nice for there to be a way for there to be one edge from ``X`` to ``SG``, and then 10 edges connecting the entry point of ``X`` into ``SG`` with each of the ``F_i``. Ideally this option would also have a programmatic interface or at least a default to set.", "comments": ["Thanks @lightcatcher for the suggestion. @danmane, please take a look at this issue. If it is something we will not do in the short-term maybe mark it contributions welcome. If it is something you are planning to include in your plugin API anyways, please close the issue to keep the backlog clear.\n", "Thanks for the suggestion! I've migrated this to our new repository at https://github.com/tensorflow/tensorboard/issues/74. Feel free to continue the discussion or submit a PR there!"]}, {"number": 5304, "title": "Small header changes and a Doxygen tweak to shape the C++ API a bit b\u2026", "body": "\u2026etter.\r\n\r\nChange: 135747447\r\n\r\n(cherry picked from commit 120c3f11f857ce6da7ae90b3eab943896499dca2)", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @josh11b, @keveman and @tensorflower-gardener to be potential reviewers.\n"]}]