[{"number": 5062, "title": "Change test size of gradient_checker_test from small to medium.", "body": "", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @vrv to be potential reviewers.\n"]}, {"number": 5061, "title": "Feature Request - expose protobuf on syntaxnet as 'first class' interface", "body": "Clearly syntaxnet will parse sentences out of the box to a terminal with / without CoNLL format.\n\necho I saw the dude with glasses|./syntaxnet/demo.sh\n<img width=\"298\" alt=\"screen shot 2016-10-18 at 20 10 02\" src=\"https://cloud.githubusercontent.com/assets/289994/19502649/725582b6-957c-11e6-8c94-e2230f6146ec.png\">\n\nI'm using a tensorflow in docker container - and now I want to interface to it from some other software via an api. I'd like to use some server side swift code. \n\nI spent time looking at following (parsey flask wrapper) \nhttps://github.com/JoshData/parsey-mcparseface-server\n\n(parsey django wrapper) \nhttps://github.com/dan-nadler/ParseyAPI\n\nand https://github.com/dmansfield/parsey-mcparseface-api\n\nThe last repo exposes parsey via protobufs - \nhttps://github.com/dmansfield/parsey-mcparseface-api/blob/160f3cbb5f96eff5fb8b73624228ceb7c8208b71/parsey_api/parsey_api.proto\n\nusing tensorflow serving but he had to fork the project and patch the files. \n\nWhile there's been help from other engineers to assist to get project working\nhttps://github.com/dmansfield/parsey-mcparseface-api/issues/1\n\nI suggest supporting protobufs out of the box as an api\n", "comments": ["@calberti might have more to say on this, but I suspect that we won't have the bandwidth to do this. The conll format is what syntaxnext produces and users are encouraged to build any higher level API that suits their needs.\n", "it appears that there are respective proto files created for syntax net - \r\nalthough - there doesn't appear to be any service layer grpc created. \r\n \r\n<img width=\"433\" alt=\"screen shot 2017-04-28 at 2 42 37 pm\" src=\"https://cloud.githubusercontent.com/assets/289994/25542529/f61b9dde-2c20-11e7-8c81-044b405ba9d1.png\">\r\n", "https://github.com/tensorflow/models/tree/master/syntaxnet/syntaxnet\r\n", "This is a models issue, not a tensorflow core issue.", "seems like someone actually coded this \r\nhttps://github.com/dsindex/syntaxnet/blob/master/api/parsey_api.proto\r\n\r\n@calberti  - would you consider cherry picking this if a PR is made?\r\nhttps://github.com/dsindex/syntaxnet/tree/master/api"]}, {"number": 5060, "title": "Rename examples test dir to tflearn", "body": "- SKFLOW_EXAMPLE_BASE_DIR -> TFLEARN_EXAMPLE_BASE_DIR\n- Removed extra echo\n", "comments": ["@terrytangyuan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @yifeif and @vrv to be potential reviewers.\n", "One flaky test had timeout [here](https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/2197/). Submitted a [separate PR](https://github.com/tensorflow/tensorflow/pull/5063) to bump up the test size. @tensorflow-jenkins Test this please\n"]}, {"number": 5059, "title": "Problems with TF_GraphImportGraphDef while loading certain types of graphs", "body": "I have experienced problems when using TF_GraphImportGraphDef to import\ncertain graphs with gradients from Python, and I suspect the function might have a bug.\n\nThe simplest example of a problematic graph is:\n\n``` python\nx = tf.placeholder(tf.float64, [1, 2], name=\"x\")\ny = tf.placeholder(tf.float64, [1, 2], name=\"y\")\nz = tf.add(x, y, name=\"z\")\n\ntf.gradients(z, [x, y])\n```\n\nWhen running TF_GraphImportGraphDef, I get the following error message:\n_Cannot infer multiple unknown dimensions in shape [?,?]_. \n\nSurprisingly, if I change the shape of one of the inputs ([1, 2] => [None, 2]):\n\n``` python\nx = tf.placeholder(tf.float64, [None, 2], name=\"x\")\ny = tf.placeholder(tf.float64, [1, 2], name=\"y\")\nz = tf.add(x, y, name=\"z\")\n\ntf.gradients(z, [x, y])\n```\n\neverything works fine and I can successfully load the graph. \nI don't see a reason why I am not allowed to load the graph in the first example.\n\nI've made all the necessary checks and I am sure the graphs I create in Python \nare correct (I can run them, and I get correct values for outputs and gradients).\nAlso, when I run the second graph (that was created in Python, and that I am able to load) \nusing C API, everything works fine.\n", "comments": ["Thanks for the detailed report. This does seem to be a bug.\n"]}, {"number": 5058, "title": "fixed issue #4903", "body": "as mentioned in #4903,\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L47\nit directly run main function which doesn't run the argument parser in if __name__==\"__main__\".\nI am not sure if FLAGS is used in somewhere else, so I moved the parser part into a function to update FLAGS in main() function.\n\nNot sure if this is the right way to fix, but it works for both running through\n`python tesnsorboard.py --logdir=AAA`\nand\n`tensorbaord --logdir=AAA`\n", "comments": ["Can one of the admins verify this patch?\n", "@willdzeng, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @vrv to be potential reviewers.\n", "@elibixby PTAL\n", "Jenkins, test this please.\n", "Looks good to fix the issue.\n", "Can one of the admins verify this patch?\n", "For the tests, PR #5018 is in review.\n", "While those tests look great, I was actually referring to integration tests that verify the CLI can be run properly. (which I don't think I see there). \n", "Ah, didn't realize we didn't have those.\n\nOn Thu, Oct 20, 2016 at 7:59 AM, Eli Bixby notifications@github.com wrote:\n\n> While those tests look great, I was actually referring to intergration\n> tests that verify the CLI can be run properly. (which I don't think I see\n> there).\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5058#issuecomment-255130674,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AT_SbVxwwKbvj5h1Mm_0Clh1iiqJxerUks5q14HDgaJpZM4KaZMc\n> .\n", "We don't I was saying we need them =)\n"]}, {"number": 5057, "title": "CMake: Set the /MP flag correctly on Windows", "body": "This should fix the \"invalid option\" warning we were seeing in the CMake logs.\n", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ageron, @tensorflower-gardener and @lilac to be potential reviewers.\n", "This builds on the nice work in #5055.\n", "GPU issue is a flake, so force merging this one.\n"]}, {"number": 5056, "title": "Protobuf incompatible with google-cloud-python?", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n- [Tensoflow Installation from source README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#installing-from-sources)\n- [Old google-cloud-sdk question of code.google.com](https://code.google.com/p/google-cloud-sdk/issues/detail?id=24)\n### Environment info\n\nUbuntu 14.04\n### Installed version of CUDA and cuDNN:\n\nCuda Toolkit 7.5 and cuDNN v5\n### pip packages installed from source, provide\n\nhttps://google-cloud-python.readthedocs.io/en/stable/#\n\n```\n$ git clone git://github.com/GoogleCloudPlatform/google-cloud-python.git\n$ cd google-cloud-python\n$ python setup.py install\n```\n\nTensorflow source\n1. The commit hash (`git rev-parse HEAD`): \n\n```\n2ed280b4f553e6aa1c7fe0bb41d52456a0ae452c\n```\n1. The output of `bazel version`\n\n```\nBuild label: 0.3.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\nBuild timestamp: 1475861110\nBuild timestamp as int: 1475861110\n```\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nTrying to import google.cloud\n\n```\npython -c \"from google.cloud import logging as gclogging;\"\n```\n\nresults in the following error\n\n```\nImportError: No module named cloud\n```\n### What other attempted solutions have you tried?\n\nWhenever I install google protobuf using either of the following commands\n\n```\nsudo pip install /tmp/tensorflow_pkg/tensorflow-0.11.0rc0-py2-none-any.whl\n# OR\npip install dist/<wheel file name>\n```\n\nI can no longer import google.cloud. \n### Logs or other output that would be helpful\n\nI'm aware of the fact that you can build TensorFlow with Google Cloud Platform support comes with integration when running  `./configure`. Howoever it's not clear how to integrate with Google Stackdriver.\n", "comments": ["Could you please fill in the answers to the template and also describe the problem a bit more. From what you've listed (\"Protobuf incompatible with google-cloud-python\"), I'm unable to figure out what your precise question is. Did you observe some incompatibility? What was the error?\n", "Hi @asimshankar. Sorry, posted this while I was still working on the question. Perhaps we can loop in the folks from google-cloud-python?\n", "A few things:\n\n(a) You mention \"google protobuf\", but it seems you're talking about TensorFlow and not protobuf? Or is there something about protobuf that I'm missing?\n\n(b) Does `python -c \"from google.cloud import logging as gclogging;\"` succeed if you do not install TensorFlow?\n\n(c) I'm not sure how source installations interact, but can you `pip install google-cloud` (from https://google-cloud-python.readthedocs.io/en/stable/#) instead of installing `google-cloud` from sources. Does that work?\n", "TensorFlow pip package depends on protobuf pip package version 3.1.0 according to the [installation guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#installing-from-sources). In the **common problems** section.\n\n> Protobuf's pip package downloaded from PyPI (when running pip install protobuf) is a Python only library, that has Python implementations of proto serialization/deserialization which can be 10x-50x slower than the C++ implementation. \n\nSo essentially, the protobuf that comes with PyPI is slow and a c++ based one is much faster.\n\nYeah, `python -c \"from google.cloud import logging as gclogging;\"` succeeds if I don't install Protobuf with either the following command line\n\n```\n$ sudo pip install /tmp/tensorflow_pkg/tensorflow-0.11.0rc0-py2-none-any.whl\n# OR\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0-cp27-none-linux_x86_64.whl\n# OR\npip install dist/protobuf-3.1.0-cp27-cp27mu-linux_x86_64.whl\n```\n\nLastly, using `pip install google-cloud` doesn't work. In fact, I had to install from source since I was experiencing other problems with the google-cloud.\n\nAlso, @asimshankar, what exactly does the config option when running `$ ./configure`\n\n```\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\n```\n\nDo?\n", "This issue was referenced and solved here\nhttps://github.com/GoogleCloudPlatform/google-cloud-python/issues/2563\n"]}, {"number": 5055, "title": "Suppress all warnings that appear >100 times in windows cmake build.", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @ageron and @tensorflower-gardener to be potential reviewers.\n", "Nice, thanks!\n", "We will see right now, after the above tests run :)\n", "Roughly: \n- build 134 (which includes other commits), 33MB, 107k lines\n- build 139, 3.7MB, 21k lines\n", "That's a nice improvement :). It looks like there's a lot of other chaff arising from an unknown compiler option - looking at the vcxproj files it seems as if two of the flags are being concatted together... I'll see what I can do in another PR.\n", "ah, I need to resolve conflicts now.\nWill do that\n"]}, {"number": 5054, "title": "Branch 136515806", "body": "Merged by hand:\n- tensorflow.bzl: make sure windows rule is preserved\n- remove extra rules (incl resnet and text_classification_save_restore)\n\nFrom these PRs that have not been merged back:\n- 136476925 conflicts with #4684\n- 136410307 conflicts with #5009\n", "comments": ["@drpngx, thanks for your PR! By analyzing the history of the files in this pull request, we identified @petewarden, @tensorflower-gardener and @vrv to be potential reviewers.\n", "@gunan could you check this?\n", "Jenkins, test this please.\n", "Sorry triggered the build again by accident. This PR passed on Jenkins http://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/2232/. Merging it.\n"]}, {"number": 5053, "title": "eigen patch to enable gpu support on windows", "body": "compiling some kernels with nvcc on windows fails. A few minor changes to eigen to get pass this. I'll send a pull request to the eigen repo. \nThis change is required to make the following 2 work:\nhttps://github.com/tensorflow/tensorflow/pull/5051\nhttps://github.com/tensorflow/tensorflow/pull/5052\n", "comments": ["Can one of the admins verify this patch?\n", "@schmuell, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ebrevdo and @ibab to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "corporate cla: Microsoft\n", "re-submitted with a different user to clear cla:\nhttps://github.com/tensorflow/tensorflow/pull/5076\n"]}, {"number": 5052, "title": "cmake changes to build gpu support for windows", "body": "cmake changes to build gpu support for windows. \nFor this to work you need to have the code changes from:\nhttps://github.com/tensorflow/tensorflow/pull/5051\n", "comments": ["Can one of the admins verify this patch?\n", "@schmuell, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @tensorflower-gardener and @ageron to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "corporate cla: Microsoft\n", "re-submitted with a different user to clear cla:\nhttps://github.com/tensorflow/tensorflow/pull/5074\n"]}, {"number": 5051, "title": "gpu support on windows", "body": "code changes to enable gpu support under windows. Tested on windows 10, cuda 8 sdk and cudnn 5.1.\nCMake changes to build this come in a 2nd pull request.\n", "comments": ["Can one of the admins verify this patch?\n", "@schmuell, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @davidzchen and @mrry to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "corperate cla: microsoft\n", "@schmuell  please provide a url link for the instruction to compile with GPU support. Thank you.\n\nThe link I used to compile for [CPU only is](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md) \n", "It is in\nhttps://github.com/schmuell/tensorflow/blob/b11ef5128f7341cfc7018ac0771d8939b8535cfc/tensorflow/contrib/cmake/README.md\n\nyou need to instal the cuda 8.0 SDK + cudnn 5.1 and add 2 options to cmake\n-Dtensorflow_ENABLE_GPU=ON  -DCUDNN_HOME=c:\\local\\cudnn\n\nI normally do:\ncd tensorflow\\contrib\\cmake\\\nmkdir build\ncd build\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=RelWithDebInfo\n-DPYTHON_EXECUTABLE=C:\\local\\Anaconda2\\envs\\py3\\python.exe\n-DPYTHON_LIBRARIES=C:\\local\\Anaconda2\\envs\\py3\\libs\\python35.lib\n-DSWIG_EXECUTABLE=c:\\local\\swigwin-3.0.10\\swig.exe\n-Dtensorflow_ENABLE_GPU=ON  -DCUDNN_HOME=\"C:\\local\\cudnn\"\nmsbuild /p:Configuration=RelWithDebInfo tf_python_build_pip_package.vcxproj\n\nOn Wed, Oct 19, 2016 at 6:00 AM, JimSw2016 notifications@github.com wrote:\n\n> @schmuell https://github.com/schmuell please provide a url link for the\n> instruction to compile with GPU support. Thank you.\n> \n> The link I used to compile for CPU only is\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5051#issuecomment-254804807,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ACYzfqTmyWXm9Pw1TpJxxC7afrZ-VlHwks5q1hSGgaJpZM4KaRCY\n> .\n", "I close this pull request and did a new one under a different user to deal with the cla: https://github.com/tensorflow/tensorflow/pull/5071\n"]}, {"number": 5050, "title": "tf.Session.reset crashes after starting the chief queue runner", "body": "Code to reproduce:\n\n``` python\nimport tempfile\nimport time\n\nimport tensorflow as tf\n\n\ndef main(_):\n    sync = True\n    start_chief_queue_runners = True\n\n    is_chief = True\n    server = tf.train.Server.create_local_server()\n    logdir = tempfile.mkdtemp()\n\n    graph = tf.Graph()\n\n    device_setter = tf.train.replica_device_setter(worker_device='/job:worker/task:0')\n    with graph.as_default(), tf.device(device_setter):\n        # Build loss op.\n        x = tf.random_normal([100, 128])\n        y = tf.Variable(initial_value=tf.random_normal([100, 128]))\n        global_step = tf.Variable(0)\n        loss = tf.reduce_sum(tf.squared_difference(x, y), name='loss')\n\n        # Set up optimizer.\n        optim = tf.train.GradientDescentOptimizer(0.001)\n        if sync:\n            optim = tf.train.SyncReplicasOptimizerV2(optim, 1)\n        minimize = optim.minimize(loss, global_step=global_step, name='train_op')\n\n        ready_for_local_init = None\n        local_step_init = None\n        if sync:\n            init_tokens = optim.get_init_tokens_op()\n            chief_qr = optim.get_chief_queue_runner()\n            ready_for_local_init = optim.ready_for_local_init_op\n            local_step_init = optim.local_step_init_op\n        init_op = tf.initialize_all_variables()\n\n    sv = tf.train.Supervisor(graph=graph,\n                             is_chief=is_chief,\n                             ready_for_local_init_op=ready_for_local_init,\n                             init_op=init_op,\n                             local_init_op=local_step_init,\n                             recovery_wait_secs=1,\n                             global_step=global_step,\n                             logdir=logdir)\n\n    config = server.server_def.default_session_config\n    sess = sv.prepare_or_wait_for_session(server.target, config=config,\n                                          start_standard_services=False)\n\n    with sess.as_default():\n        if is_chief and sync and start_chief_queue_runners:\n            sv.start_queue_runners(sess, [chief_qr])\n            init_tokens.run()\n\n    # Wait for the queue runner to start.\n    time.sleep(2)\n\n    tf.Session.reset(server.target)\n\n    time.sleep(1)\n    print('restarted session')\n\n    server.join()\n\n\nif __name__ == '__main__':\n    tf.app.run(main)\n```\n\nWhat I see:\n\n```\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job local -> {0 -> localhost:33954}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:211] Started server with target: grpc://localhost:33954\nI tensorflow/core/distributed_runtime/master_session.cc:869] Start master session 3d7c628e8fd8d707 with config: \n\n*** Error in `/home/daeyun/anaconda3/bin/python': double free or corruption (fasttop): 0x00007f2000087ca0 ***\n======= Backtrace: =========\n/lib/x86_64-linux-gnu/libc.so.6(+0x77725)[0x7f20f0d35725]\n/lib/x86_64-linux-gnu/libc.so.6(+0x7ff4a)[0x7f20f0d3df4a]\n/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f20f0d41abc]\n/home/daeyun/anaconda3//lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x11ae503)[0x7f20c471b503]\n/home/daeyun/anaconda3//lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow26ConditionalAccumulatorBase16TryAttemptLockedEPSt6vectorINS0_7CleanUpESaIS2_EE+0x6c)[0x7f20c471be2c]\n/home/daeyun/anaconda3/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow26ConditionalAccumulatorBase13FlushUnlockedEv+0x7b)[0x7f20c471c0fb]\n/home/daeyun/anaconda3/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow26ConditionalAccumulatorBase6CancelEPNS_19CancellationManagerEx+0xde)[0x7f20c471c33e]\n/home/daeyun/anaconda3/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow19CancellationManager11StartCancelEv+0x28b)[0x7f20c59a5bab]\n/home/daeyun/anaconda3/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0xc51a16)[0x7f20c41bea16]\n/home/daeyun/anaconda3/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0xc5d743)[0x7f20c41ca743]\n/home/daeyun/anaconda3/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0xc62704)[0x7f20c41cf704]\n/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80)[0x7f20c3045c80]\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x76fa)[0x7f20f179e6fa]\n/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7f20f0dc4b5d]\n```\n\nThis seems to happen after `SyncReplicasOptimizerV2`'s chief queue runner starts.\nThis does not happen when `start_chief_queue_runners` or `sync` variable is `False`. \n\nSystem info: Nightly build (Python 3.4), Anaconda, Ubuntu 16.04\n", "comments": ["@daeyun : Thanks so much for the instructions to reproduce\n@jmchen-g : Could you take a look (since this seems to be `SyncReplicasOptimizerV2` related?)\n", "Jianmin is gone; sorry this has fallen through the cracks.  @daeyun Do you still get this crash?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 5049, "title": "Branch 136508257", "body": "Merged by hand:\n- tensorflow.bzl: make sure windows rule is preserved\n- remove extra rules (incl resnet and text_classification_save_restore)\n\nFrom these PRs that have not been merged back:\n- 136476925 conflicts with #4684\n- 136410307 conflicts with #5009\n", "comments": ["@drpngx, thanks for your PR! By analyzing the history of the files in this pull request, we identified @petewarden, @tensorflower-gardener and @vrv to be potential reviewers.\n"]}, {"number": 5048, "title": "Arbitrary sized inputs for FCNs are slow.", "body": "Hi,\n\nOS and version: TF r0.11.0rc0, Linux 64bit, cudnn-7.5-v5.1\n\nI am using TF-Slim and a FCN-style architecture based on ResNets. I experience extremely slow training times: 5-10x slower compared to an equivalent Caffe implementation.\n\nI train fully-convolutionally, and my images are of arbitrary sizes and aspect ratios. The training code uses FIFOQueue and preloads data in a separate thread. I use batch_size=1 as all images are of different sizes.\n\nIf I feed dummy random numpy tensors of fixed size, it works very fast. I tried to generate input tensors of 10 predefined sizes and fed them sequentially, the first 10 iterations were slow, but then it speeds back up. Looks like it does some extra work for each input size. I only used Caffe before, and there it was possible to resize all tensors per batch efficiently, somehow.\n\nAm I missing some simple trick or is it a bug?\n", "comments": ["It's hard to come up up with any suggestions here without seeing how you're using the APIs. Is it possible to write up a small snippet of code that demonstrates the problem?\n", "Ah, I actually had a conversation with @vrv and have some more useful information for you.\n\nShort story: Can you re-run after setting the environment variable `TF_CUDNN_USE_AUTOTUNE=0`?\n\nLong story: For some ops, such as `Conv2D` (on GPUs), TensorFlow [supports \"auto-tuning\"](https://github.com/tensorflow/tensorflow/blob/5a566a7701381a5cf7f70fce397759483764e482/tensorflow/core/kernels/conv_ops.cc#L529), which means that it profiles multiple algorithms for the operation before selecting one. This selection is cached based on the shapes of the tensors and some other parameters, but the first time new shapes are encountered, it has to re-profile. This is likely what is causing the slowdown when you provide arbitrary shapes and speeds things up after it has profiled the 10 predefined sizes you had provided.\n\nDisabling auto-tuning with the environment variable will mean that you might not end up using the best algorithm, but should get consistent results.\n\nHope that helps, let us know. Thanks!\n", "@asimshankar Hey, thanks! This was spot on, now it works very fast :) The issue bugged me for several days before I posted it here. I understand that my case can be quite rare, but perhaps to save other people trouble down the road it would be nice to make this feature somehow more explicit. Also, is it possible to manually choose those algorithms it tries to profile automatically?\n", "Glad it worked out! We'll try to figure out a good place to make this more discoverable (suggestions welcome). The set of algorithms is hardcoded in [cuda_dnn.cc](https://github.com/tensorflow/tensorflow/blob/2407f11e53a14958307d367afcbeab5f08f65e94/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1999) right now, so to modify them you'll need to modify that source file.\n\nSince your issue has been resolved, closing this. Happy training.\n", "@asimshankar perhaps a config class like [RunConfig](https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.learn.html#RunConfig) could be a way to handle this type of detail?\r\n\r\nAdditionally could default to a decaying or learned (turtles all the way down...) flag enabling/disabling of autotune so no config is necessary for the typical use cases, including this one, then the config class mentioned above could be used in specific cases where other behavior is desired.", "There is a [stackoverflow question](http://stackoverflow.com/q/39774449/99379) for this for people who come across this in the future.\r\n\r\n@asimshankar to set the environment variable, do I literally in bash run:\r\n```bash\r\nexport TF_CUDNN_USE_AUTOTUNE=0\r\npython myscript.py\r\n```", "@ahundt We do have ConfigProto where generally useful options are added, similar to a RunConfig class.  Config options often start as an environment variable, and get promoted to ConfigProto when enough people in the public use them for it to be better supported / documented, etc.  Operationally, it's a lot easier to remove environment variables (they will just be ignored), and impossible to remove ConfigOptions (because then code will break), if, for example, one day we make auto-tuning fast enough that it doesn't matter.\r\n\r\nIn any case, it's still not clear to us that we want to promote the disabling of auto-tuning into an option we can never take away, but hopefully this issue + the StackOverflow question will suffice for now -- thanks for doing that!\r\n\r\nAnd yes, that's how you set the env var.  Let us know if it that didn't work for some reason.\r\n\r\n", "@vrv thanks for the detailed explanation! Would the environment variable also work if the model was run via the C++ API?", "Yeah, we read the environment variable in C++, so it doesn't matter how it's set, as long as it's set on the binary that's running Cudnn.", "@vrv great, thanks again!", "@asimshankar  thanks ", "@asimshankar \r\nGreat! I now found the culprit of the huge differences in training time between fixed and dynamic input image sizes.\r\nI know I'm late to the party, but I was wondering whether you or anybody else knows how this auto-tuner works internally and what effect it would have on the long term.\r\nWould the auto-tuning stop and find the optimal algorithm after some predefined number of tries, or could it theoretically re-profile indefinitely provided that input image size are random (in a large enough domain).\r\n\r\nLet's say that the input image sizes of my model are (`h`, `w`, 3) where `h` and `w` are uniformly sampled between 500 and 1500, and I should train this model for multiple days anyways. Would it make sense to enable the auto-tuner, or should it basically always be disabled in such a setting?\r\n", "There are 2 options:\r\n\r\n1) Enable auto-tuning (default) - The auto-tuning algorithm is executed for EACH new tuple (batch_size, h,w, color) for EACH cnn layer. It runs the layer against all plans, defined in CuDNN (~10 plan), where the processing time may highly vary between 200nano to 200milli seconds (in my case). So \"heating\" the model with a single batch size can easily take several seconds. Once again, it depends on a model (how many layers, density, etc.) It then chooses the best plan (least execution time) for each layer.\r\nIf you run again with the same dimension tuple (batch_size, h,w, color), best plans are already chosen and you will get the best execution/processing time. If you restart the process, the mapping config_tuple-best_plan is gone and the auto-tuning process will start from the beginning.\r\n\r\n2) Disable auto-tuning (TF_CUDNN_USE_AUTOTUNE=0) - you overcome the tuning phase and the default plan is chosen. But is can be not optimal.\r\n\r\nIn my case tuned model performed ~30% better than without tuning..."]}, {"number": 5047, "title": "Branch 136497385", "body": "See merge comments about conflicts with #4684 and #5009.\n", "comments": []}, {"number": 5046, "title": "Multi-core not being utilized?", "body": "I have a 8 vCPUs 30GB Debian instance on the Google Cloud Platform, running the TensorFlow implementation of the WaveNet from https://github.com/ibab/tensorflow-wavenet  \n\nI noticed that CPU usage stays around 13%, as if only one of the 8 vCPUs is being used. I have found this issue https://github.com/tensorflow/tensorflow/issues/583 which seems to indicate that such problem should have been fixed. \n\nAny advice on what I need to do in order for all of the vCPUs on my instance to be full utilized in such a case? Thanks!\n", "comments": ["Is this a TensorFlow problem or a Google Cloud problem? You can tell by\ntrying to run this model locally and seeing the CPU usage\n\nOn Tue, Oct 18, 2016 at 11:08 AM, kaihuchen notifications@github.com\nwrote:\n\n> I have a 8 vCPUs 30GB Debian instance on the Google Cloud Platform,\n> running the TensorFlow implementation of the WaveNet from\n> https://github.com/ibab/tensorflow-wavenet\n> \n> I noticed that CPU usage stays around 13%, as if only one of the 8 vCPUs\n> is being used. I have found this issue #583\n> https://github.com/tensorflow/tensorflow/issues/583 which seems to\n> indicate that such problem should have been fixed.\n> \n> Any advice on what I need to do in order for all of the vCPUs on my\n> instance to be full utilized in such a case? Thanks!\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5046, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHEUiOQecHfEn_gVDry0gTva65uCFks5q1Qs0gaJpZM4KaHMk\n> .\n", "You can try the timelines profiler e.g.\nhttp://stackoverflow.com/questions/34293714/tensorflow-can-i-measure-the-execution-time-of-individual-operations/37774470#37774470\n\nClosing for now since it is not clear that it is a bug. You might try asking on StackOverflow since this question is more \"how to get good performance on TensorFlow\" rather than \"TensorFlow is broken.\" Please reopen if you find that a bug is causing performance to be worse than it should be.\n"]}, {"number": 5045, "title": "Branch 136481222", "body": "See merge comments, proper merge of the tensorflow/examples/skflow/BUILD wrt resnet.\n", "comments": ["@drpngx, thanks for your PR! By analyzing the history of the files in this pull request, we identified @petewarden, @tensorflower-gardener and @vrv to be potential reviewers.\n", "Superceded by #5047, which includes the cmake fix.\n"]}, {"number": 5044, "title": "Branch 136476925", "body": "", "comments": ["@drpngx, thanks for your PR! By analyzing the history of the files in this pull request, we identified @petewarden, @tensorflower-gardener and @vrv to be potential reviewers.\n"]}, {"number": 5043, "title": "Sudden OOM error with Tensorflow embedding_attention_seq2seq after successful runs", "body": "I've already found plenty of issues on the particular topic of memory issues with TensorFlow [here](https://github.com/tensorflow/tensorflow/issues/352), [here](https://github.com/tensorflow/tensorflow/issues/136), [here](https://github.com/tensorflow/tensorflow/issues/138), [here](https://github.com/tensorflow/tensorflow/issues/1355) and [here](https://github.com/tensorflow/tensorflow/issues/492) and we are still awaiting fixes for the same.\n\nI have implemented the RNN embedding Encoder Attention Decoder model as described by this [tutorial](https://www.tensorflow.org/versions/r0.11/tutorials/seq2seq/index.html) using a custom language pair.\n\nI **successfully implemented** it and the training started after I determined the maximum parameter values of:\n\nSize of NN: 340\nBatch Size: 32\nTotal Vocabulary:\nEnglish: 50000 words\nTamil: 40000 words\n\nand trained on it quite a few times.\n\n```\nPreparing WMT data in /home/hans/Documents/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/\nCreating 3 layers of 325 units.\nReading model parameters from /home/hans/Documents/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/checkpoints/translate.ckpt-9500\nReading development and training data (limit: 0).\n  reading data line 100000\n   Word2Vec English Encoding Successful!\n   Word2Vec Tamil Encoding Successful!\nglobal step 10000 learning rate 0.5000 step-time 1.56 perplexity 33.47\n  eval: bucket 0 perplexity 10.47\n  eval: bucket 1 perplexity 14.57\n  eval: bucket 2 perplexity 20.74\n  eval: bucket 3 perplexity 35.82\n  eval: bucket 4 perplexity 42.77\n\n.......................................\n.......................................\n.......................................\n\nglobal step 109000 learning rate 0.4522 step-time 1.04 perplexity 3.79\n  eval: bucket 0 perplexity 20.10\n  eval: bucket 1 perplexity 18.52\n  eval: bucket 2 perplexity 17.32\n  eval: bucket 3 perplexity 28.79\n  eval: bucket 4 perplexity 18.48\n  eval: bucket 5 perplexity 17.07\n  eval: bucket 6 perplexity 15.35\n  eval: bucket 7 perplexity 29.22\n  eval: bucket 8 perplexity 26.05\n  eval: bucket 9 perplexity 28.58\n\n```\n\nI'm using Ubuntu 16.04 LTS with a 2GB Nvidia 650M GeForce card with:\n\n```\nnvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Tue_Aug_11_14:27:32_CDT_2015\nCuda compilation tools, release 7.5, V7.5.17\n```\n\n```\nroot@hans-Lenovo-IdeaPad-Y500:/opt/cuda/lib64# ls\nlibcudnn.so  libcudnn.so.4  libcudnn.so.4.0.7  libcudnn_static.a\n```\n\n```\nroot@hans-Lenovo-IdeaPad-Y500:/opt/cuda/lib64# pip list | grep tensorflow\ntensorflow (0.8.0)\n\n```\n\n```\nroot@hans-Lenovo-IdeaPad-Y500:/opt/cuda/lib64# ls -l /usr/lib/x86_64-linux-gnu/libcud*\n-rw-r--r-- 1 root root   322936 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Mar 30  2016 /usr/lib/x86_64-linux-gnu/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Mar 30  2016 /usr/lib/x86_64-linux-gnu/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rw-r--r-- 1 root root   383336 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudart_static.a\nlrwxrwxrwx 1 root root       12 Apr 14  2016 /usr/lib/x86_64-linux-gnu/libcuda.so -> libcuda.so.1\nlrwxrwxrwx 1 root root       17 Aug 14 16:25 /usr/lib/x86_64-linux-gnu/libcuda.so.1 -> libcuda.so.367.35\n-rw-r--r-- 1 root root 16881416 Mar 23  2016 /usr/lib/x86_64-linux-gnu/libcuda.so.361.42\n-rwxr-xr-x 1 root root  8121032 Aug 14 14:14 /usr/lib/x86_64-linux-gnu/libcuda.so.367.35\nlrwxrwxrwx 1 root root       13 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 root root       17 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxr-xr-x 1 root root 61453024 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn.so.4.0.7\nlrwxrwxrwx 1 root root       17 Oct  1 05:43 /usr/lib/x86_64-linux-gnu/libcudnn.so.5 -> libcudnn.so.5.1.3\n-rwxr-xr-x 1 root root 60696704 Oct  1 05:43 /usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 62025862 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn_static.a\n\n```\n\nSince then, I required the cuDNN package for other purposes and installed cuDNN 5 first. found out that Tensorflow 0.8 works with cudnn 4 only and reinstalled the version 4 and the model testing worked correctly. But when I tried to run the model after the cudnn install:\n\n```\n\n................................\n................................\n...............................\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1640704 totalling 1.56MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1660928 totalling 1.58MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1676544 totalling 1.60MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 235 Chunks of size 1849600 totalling 414.52MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 2 Chunks of size 1893120 totalling 3.61MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 3 Chunks of size 1936640 totalling 5.54MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2023680 totalling 1.93MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2041088 totalling 1.95MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2067200 totalling 1.97MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2151936 totalling 2.05MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2154240 totalling 2.05MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2174208 totalling 2.07MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2328320 totalling 2.22MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2543872 totalling 2.43MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2590208 totalling 2.47MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2610176 totalling 2.49MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 3 Chunks of size 2774272 totalling 7.94MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2861312 totalling 2.73MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2974976 totalling 2.84MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2994176 totalling 2.86MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 3042816 totalling 2.90MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 3445760 totalling 3.29MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 120 Chunks of size 3699200 totalling 423.34MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 2 Chunks of size 3939072 totalling 7.51MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 4348416 totalling 4.15MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 4418816 totalling 4.21MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 4980736 totalling 4.75MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 5232128 totalling 4.99MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 5960960 totalling 5.68MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 54400000 totalling 51.88MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 54545920 totalling 52.02MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] Sum Total of in-use chunks: 1.40GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:694] Stats: \nLimit:                  1534525440\nInUse:                  1506072064\nMaxInUse:               1506336256\nNumAllocs:                  286635\nMaxAllocSize:             92850432\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:270] ****************************************************************************************************\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 903.1KiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:900] Resource exhausted: OOM when allocating tensor with shape[680,340]\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=9028 evicted_count=9000 eviction_rate=0.996899 and unsatisfied allocation rate=0\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 350, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 347, in main\n    train()\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 186, in train\n    target_weights, bucket_id, False)\n  File \"src/seq2seq_model.py\", line 205, in step\n    outputs = session.run(output_feed, input_feed)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[680,680]\n     [[Node: gradients_9/model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/concat, gradients_9/model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/add_grad/Reshape)]]\n     [[Node: clip_by_global_norm_9/clip_by_global_norm_9/_2/_7992 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_624162_clip_by_global_norm_9/clip_by_global_norm_9/_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'gradients_9/model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/MatMul_grad/MatMul_1', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 350, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 347, in main\n    train()\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 122, in train\n    model = create_model(sess, False)\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 98, in create_model\n    forward_only=forward_only)\n  File \"src/seq2seq_model.py\", line 142, in __init__\n    gradients = tf.gradients(self.losses[b], params)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 481, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py\", line 511, in _MatMulGrad\n    math_ops.matmul(op.inputs[0], grad, transpose_a=True))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 1036, in matmul\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 911, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/MatMul', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n[elided 5 identical lines from previous traceback]\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 98, in create_model\n    forward_only=forward_only)\n  File \"src/seq2seq_model.py\", line 133, in __init__\n    softmax_loss_function=softmax_loss_function)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 961, in model_with_buckets\n    decoder_inputs[:bucket[1]])\n  File \"src/seq2seq_model.py\", line 132, in <lambda>\n    lambda x, y: seq2seq_f(x, y, False),\n  File \"src/seq2seq_model.py\", line 96, in seq2seq_f\n    feed_previous=do_decode)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 718, in embedding_attention_seq2seq\n    initial_state_attention=initial_state_attention)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 643, in embedding_attention_decoder\n    initial_state_attention=initial_state_attention)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 556, in attention_decoder\n    cell_output, state = cell(x, state)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell.py\", line 659, in __call__\n    cur_inp, new_state = cell(cur_inp, cur_state)\n\nhans@hans-Lenovo-IdeaPad-Y500:~/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec$ \n\n```\n\nThe pool allocator for my GPU is only going up to 300 from 100 when before, it went up to somewhere north of 6500. I don't know if this happened because of cudnn but this started right after I installed it. Please help.\n", "comments": ["@hanskrupakar : There have been many changes to TensorFlow since 0.8, is it possible for you to work with the latest release (0.11rc0)? That also supports cudnn5 and with 0.11rc1 that will come out shortly, we will also support CUDA8.\r\n", "Please try tcmalloc as well.\n", "Automatically closing due to lack of recent activity, we will reopen if further information becomes available. Thanks!\n", "Hi @aselle  and @asimshankar  I have run the same model yesterday on new 1.0 release of tensor flow . even then its throwing the same error. I did n't use tcmalloc , I will definitely try , but also I would like to mention that I am using legacy_seq2seq instead of new one. So please suggest should I carry on with legacy models or do I need to use new one along with tcmalloc. If I need to use new one can u kindly provide a tutorial like for the legacy one. \r\n\r\n ", "Same error when I run the translate example\r\nhttps://www.tensorflow.org/tutorials/seq2seq#lets_run_it\r\nfrom Tensorflow tutorial on sequence-to-sequence network. \r\nI run Tensorflow 1.0, simpler examples were OK, my GPU is GTX 1050 Ti.\r\nErrors are attached here.\r\n\r\n[errors output.txt](https://github.com/tensorflow/tensorflow/files/786795/errors.output.txt)\r\n\r\n", "@aselle @asimshankar awaiting response.....!", "Same error as @OVasilyev on my GT 730 with CUDA enabled. Waiting on an answer\r\n"]}, {"number": 5042, "title": "onb.26@", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["There seems to be no content in the issue filed above, perhaps it was in error?\nIf not, please feel free to reopen a new issue with your concerns.\n\nThanks!\n"]}, {"number": 5041, "title": "To fix continuous build, install ca-certificates-java in docker images.", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @ebrevdo and @ilblackdragon to be potential reviewers.\n", "DONOTMERGE yet.\n", "Ready for review, running the 2nd test now.\n", "First tests mostly passed with only a flake\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/2217/\nCurrent tests also cleared up the hurdle we have been seeing.\n\npatrick, would you like me to force merge this to clear all the other failures we are seeing?\n", "SG, thanks!\n\nOn Tue, Oct 18, 2016 at 8:13 AM, gunan notifications@github.com wrote:\n\n> First tests mostly passed with only a flake\n> https://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/2217/\n> Current tests also cleared up the hurdle we have been seeing.\n> \n> patrick, would you like me to force merge this to clear all the other\n> failures we are seeing?\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5041#issuecomment-254538652,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AT_SbRs-wEqC1laEounP_qwWxlizJLSIks5q1OIigaJpZM4KZygp\n> .\n", "As in, please force merge.\n\nOn Tue, Oct 18, 2016 at 8:15 AM, Patrick Nguyen drpng@google.com wrote:\n\n> SG, thanks!\n> \n> On Tue, Oct 18, 2016 at 8:13 AM, gunan notifications@github.com wrote:\n> \n> > First tests mostly passed with only a flake\n> > https://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/2217/\n> > Current tests also cleared up the hurdle we have been seeing.\n> > \n> > patrick, would you like me to force merge this to clear all the other\n> > failures we are seeing?\n> > \n> > \u2014\n> > You are receiving this because you were assigned.\n> > Reply to this email directly, view it on GitHub\n> > https://github.com/tensorflow/tensorflow/pull/5041#issuecomment-254538652,\n> > or mute the thread\n> > https://github.com/notifications/unsubscribe-auth/AT_SbRs-wEqC1laEounP_qwWxlizJLSIks5q1OIigaJpZM4KZygp\n> > .\n"]}, {"number": 5040, "title": "possible bug in translate.py", "body": "Hi there,\nI read translate.py (/tensorflow/models/rnn/translate/translate.py), and got confused on the implement of decoder:\nstep1: when decode a new  input sentence, it is ok to chose a proper bucketId to do decode \nstep2: tokenize the inputs(line244): \n       encoder_inputs, decoder_inputs, target_weights = model.get_batch({bucket_id: [(token_ids, [])]}, bucket_id)\nstep3: model.step the same as training procedure did\n\nMy question:  \nThe decoder_inputs are all zeros, except that the first one was assgined GO, Is this ok for a decoder? should't the LSTM's input be assgined with the previous LSTM's output, one by one?\n", "comments": ["@lukaszkaiser might be able to provide insight.\n", "This is ok during inference (not training) becase, when building the graph, we set the forward_only parameter in seq2seq_model to true, which in turns sets feed_previous to true in tf.nn.seq2seq.embedding_attention_seq2seq. In this case, the decoder uses the previous-step output for all steps other than the first one (GO symbol) and doesn't use the decoder inputs at all.\n"]}, {"number": 5039, "title": "Tensorboard delete option for runs", "body": "Since many of us train with Tensorflow on a remote server and use Tensorboard to monitor and evaluate the training I think it would be useful to add a delete option for specific runs on the Tensorboard dashboard.\nWhen I debug a training process I monitor what the network is doing and sometimes restart the process very quickly. Like this my data directory easily gets filled with a lot of fail runs. Deleting them from command is a little tedious cause you always have to check in Tensorboard which ones you might want to keep. And usually you have to close Tensorboard to be able to delete them at all.\n\nMaybe you guys have a suggestion for an improved workflow! Otherwise I think being able to delete runs directly from Tensorboard would be a nice addition. As part of this an 'active' flag for  whether a run is still running might be interesting as well. Obviously this flag would disable the deletion option.\n\nI am looking forward to your feedback!\n\nEdit: If this is really a feature worth integrating I would be happy to look into it and submit a PR.\n", "comments": ["@danmane : Any thoughts on this?\n", "Thank you for your suggestion @timmeinhardt. We have a feature in TensorBoard that hasn't been released yet where the state of the runs checkboxes is persisted to the URL. I think that might actually solve your problem.\n\nWe're really excited that you're volunteering to be a contributor. Although right now we're in the process of revamping our build system. It's our top priority right now, but it's going to take a bit more time. Feature work is currently on hold and we're not able to accept contributions just yet. We'll let you know as soon as that changes. Because the new build is going to be simple and make it much easier to get involved in TensorBoard development.\n", "Thank you for your fast response @jart ! From what I understood the non-released TensorBoard feature would address the 'active' flag for currently running runs but not the pollution of the data directory. Or am I missing something?\n\nI am looking forward to your new build system and maybe contributing something in the future!\n", "Having TensorBoard do file management is a line we're probably not yet ready to cross. If the concern is disk space, there are probably better angles we could take for tackling the problem. @danmane and I have had conversations about changing the way we write events to disk. One thing I was actually wondering about the other day was if we could do reservoir sampling as we write to disk, rather than when we read from disk. Things like that could really help. But at the end of the day, `tensorboard` is still a command line program, so it's assumed that the user will feel comfortable with the fact that little command-line-fu is required to use this fabulous GUI.\n", "I'd be very much interested in this feature as well! My use case is similar: I am often iterating over a prototype network for ten or twenty iterations, tweaking hyperparameters and fixing bugs according to the learning curves. Of course I can - and will - delete the superfluous runs via command line from time to time, but it would be a very elegant feature if this could be done directly in the GUI.", "We've got a few things planned over the course of this year that dovetail with this feature request, so we'll be sure to take the feedback we've received here into consideration.", "I'm going to close this for now; feel free to open an issue in our new repository at https://github.com/tensorflow/tensorboard/issues if you feel that there's more to discuss. Thanks!"]}, {"number": 5038, "title": "Can I install TensorFlow on the Pi zero", "body": "Hi,\n\nCan I install TensorFlow on the Pi zero.\n\nI tried \"sudo pip2 install tensorflow-0.9.0-cp27-none-linux_armv7l.whl\"\n\nbut it returned\ntensorflow-0.9.0-cp27-none-linux_armv7l.whl is not a supported wheel on this platform.\nStoring debug log for failure in /root/.pip/pip.log\n\nI haven't try from Docker image yet. is it possible ?\nor Can I install the C++  code ?\n\nthank you and regards,\nKhoa\n", "comments": ["@shaolinkhoa : The Pi's are supported by contributions at this time. Have you tried building from source using https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples ?\n\n@petewarden may have other thoughts, but since this is community supported perhaps stack overflow would be a better place to ask for help on this. \n", "Hi @asimshankar \nThank you for your link. I didn't know that.\n\nI follow the step in tensorflow/contrib/makefile\n\n```\ntensorflow/contrib/makefile/download_dependencies.sh\nsudo apt-get install -y autoconf automake libtool gcc-4.8 g++-4.8\ncd tensorflow/contrib/makefile/downloads/protobuf/\n./autogen.sh\n./configure\nmake\nsudo make install\nsudo ldconfig  # refresh shared library cache\ncd ../../../../..\n\nmake -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI OPTFLAGS=\"-Os\" CXX=g++-4.8\n\ncurl https://storage.googleapis.com/download.tensorflow.org/models/inception_dec_2015_stripped.zip \\\n-o /tmp/inception_dec_2015_stripped.zip\nunzip /tmp/inception_dec_2015_stripped.zip \\\n-d tensorflow/contrib/pi_examples/label_image/data/\n```\n\nThen I get error when I run this command\n**make -f tensorflow/contrib/pi_examples/label_image/Makefile** \n\nError: \n**gcc --std=c++11 -O0 -I/usr/local/include -I. -I/home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/downloads -I/home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/downloads/eigen-latest/ -I/home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/gen/proto/ -I/home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/gen/proto_text/ -c tensorflow/contrib/pi_examples/label_image/label_image.cc -o /home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o\nIn file included from ./tensorflow/core/framework/tensor.h:19:0,\n                 from tensorflow/contrib/pi_examples/label_image/label_image.cc:32:\n./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory\n #include \"unsupported/Eigen/CXX11/Tensor\"\n                                          ^\ncompilation terminated.\ntensorflow/contrib/pi_examples/label_image/Makefile:79: recipe for target '/home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o' failed\nmake: **\\* [/home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o] Error 1**\n\nWould you mind helping me\n\nThank you\nKhoa\n", "@shaolinkhoa : Some other people have been experiencing this, unfortunately haven't figured out the details yet. But you can follow along in #4680 \n", "Hi @asimshankar \n\nThank you for your support.\nI will check it.\n\nAre there any errors when i run this command and it returns like this:\n\n```\n\npi@raspberrypi:~/tensorflow/tensorflow $ curl https://storage.googleapis.com/download.tensorflow.org/models/inception_dec_2015_stripped.zip \\\n> -o /tmp/inception_dec_2015_stripped.zip\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  1 84.5M    1 1148k    0     0   980k      0  0:01:28  0:00:01  0:01:27  980k\n100 84.5M  100 84.5M    0     0  1727k      0  0:00:50  0:00:50 --:--:-- 2490k\n\npi@raspberrypi:~/tensorflow/tensorflow $ unzip /tmp/inception_dec_2015_stripped.zip \\\n> -d tensorflow/contrib/pi_examples/label_image/data/\nArchive:  /tmp/inception_dec_2015_stripped.zip\n  inflating: tensorflow/contrib/pi_examples/label_image/data/imagenet_comp_graph_label_strings.txt  \n  inflating: tensorflow/contrib/pi_examples/label_image/data/tensorflow_inception_stripped.pb  \n```\n"]}, {"number": 5037, "title": "Feature request: bounds_check header in python package", "body": "I think it would be useful if \"tensorflow/core/kernels/bounds_check.h\" was included in the python package, so that FastBoundsCheck() can be used in a custom Op when building against the binary package.\n\nI patched this manually by adding bounds_check.h to framework_headers in tensorflow/core/BUILD:\n\n``` diff\ndiff --git a/tensorflow/core/BUILD b/tensorflow/core/BUILD\nindex 2c936bd..44210f7 100644\n--- a/tensorflow/core/BUILD\n+++ b/tensorflow/core/BUILD\n@@ -1136,6 +1136,7 @@ filegroup(\n         \"framework/type_traits.h\",\n         \"framework/types.h\",\n         \"framework/unique_tensor_references.h\",\n+        \"//tensorflow/core/kernels:bounds_check.h\",\n         \"lib/core/errors.h\",\n         \"lib/core/notification.h\",\n         \"lib/core/refcount.h\",\n```\n\nI'm not sure if this is the correct way of doing this, so I'm making a feature request instead of a pull request.\n\nThanks!\n", "comments": ["Nice, please submit a pull request.\n", "This has been  superseded by issue #4996 and is no-longer required.\n\nThanks for your consideration.\n"]}, {"number": 5036, "title": "Feature request: Inception v3 MetaGraph", "body": "Would it be possible for the TensorFlow developers to put a tar-ball online with the Inception v3 saved as a MetaGraph? I can't find it anywhere.\n\nI'm currently using the following tar-ball with a frozen graph for Inception v3:\n\nhttp://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\n\nThe problem is that I cannot continue training that graph because it is frozen, so all the variables have been converted to constants before it was saved. I can't find a way to convert the constants back to variables so I don't think that is possible. (There are also some deprecation warnings regarding BatchNormWithGlobalNormalization so it will presumably stop working at some point in the future).\n\nAfter searching for a solution for days, I found that you have released a newer checkpoint-file for Inception v3:\n\nhttp://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz\n\nI downloaded it but it's only the checkpoint-file, not the graph-definition. So in my Python code I would apparently have to create the Inception graph using this function first:\n\nhttps://github.com/tensorflow/models/blob/master/inception/inception/slim/inception_model.py#L52\n\nBut this apparently requires building TensorFlow from source, as far as I could understand from the README. There's also several options for using the function and it apparently has to be wrapped in arg_scopes and what-not:\n\nhttps://github.com/tensorflow/models/blob/master/inception/inception/inception_model.py#L76-87\n\nWould it be possible to update the above tar-ball (dated 2016-03-01) so it also contains the MetaGraph-files, so I can load it more easily and use it in my own Python program? I have another data-set so I replace the softmax-layer of the Inception-graph, and I also want to continue optimizing the rest of the variables of the Inception-graph.\n\nPlease also consider including a small example program in the tar-ball (or a link to some python-code), as it would make it a lot easier for everyone who wants to use it. Or at least make a list of all the relevant tensor-names (input, output, etc.)\n\nThanks!\n", "comments": ["@martinwicke any reason why we don't ship the graphs?\n", "No reason other than finiteness of resources. sguada@ may have\nmore details, but I believe that the code in tensorflow/models produces the\nproper graph, and could serve as the requested code example.\n\nI don't think building from source is a requirement. I believe the (Python)\nfiles would work fine with an installed binary version of tensorflow. You\nmay have to check out tensorflow if the files are not contained in the pip\npackage.\n", "@martinwicke I am trying to do something very similar to what @Hvass-Labs is attempting.\n\nThe inception code in tensorflow/models is doing rather advanced stuff (using multiple GPUs, etc), which goes over the head of a beginner (like me), and it seems rather tightly coupled. What is more, it uses a (custom) version of slim, which not only makes the code look different than mainstream TF code, but also forces you to use blaze (not super pleasant if you aren't already using blaze for everything) \u2013\u00a0you cannot just drop one file into your repo and count it will work.\n\n(Another source of confusion for me is that there's two versions of the slim inception: one in tensorflow/models, and one in https://github.com/tensorflow/models/tree/master/inception/inception/slim, which are very similar, but slightly different). \n\nThe inception related code that's much more beginner friendly (and much less Google idiosyncratic) is the one used in  https://www.tensorflow.org/versions/r0.9/how_tos/image_retraining/index.html. However, adapting it to training the full network has proven quite daunting (issues mentioned by @Hvass-Labs, like being frozen, etc).\n\nSo, getting the Metagraph would actually be very, very helpful. Some non-slim Python code would be even better, but I understand that the rewrite would be far from trivial.\n", "Thanks for the input everyone.\n\nI am using the Inception model in my tutorials and I try to keep them fairly simple. This means limiting each tutorial to only one or a few topics. The official tutorials have 10 topics in each tutorial which is an extremely steep learning curve for beginners. I've put the Inception model inside a Python module so it is encapsulated and can easily be loaded from different tutorials:\n\nhttps://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/inception.py\n\nBy far the easiest thing would be to replace a few lines in my inception.py with the following:\n- Download a newer tar-ball.\n- A few lines of code for loading the MetaGraph and Checkpoint.\n\nThis would be an elegant solution that I could easily explain in a tutorial.\n\nFor the end-user, there really is no reason to build the Inception graph all over again. It is quite messy and you can't make any changes to it, because then you cannot load the checkpoint anymore. Even tiny changes that are logically identical will break the checkpoint.\n\nAs far as I understand, the MetaGraph is already saved when you save the checkpoint. So it should be fairly trivial to update the tar-ball. Although I do understand that if you guys are under a lot of stress then there's a million 'fairly trivial' things that needs to be done.\n\nHopefully you'll see the usefulness of updating the tar-ball as it might help thousands of people who will be watching the tutorials in the following years. I've been stuck for 2 weeks now trying to obtain the MetaGraph so I'm hoping I can get it soon.\n\nCheers!\n", "Same about me. I need to use tar-ball in my project\n", "Hvass-Labs's tutorials are really great. It would be very helpful to have the MetaGraph available.\n", "Hvass-Labs's tutorials are outstanding.  Please provide the MetaGraph etc that he is requesting.  Thanks, Jon", "Hvass-Labs's tutorials are amazing. Would be great if you could implement the MetaGraph so he can make tutorial 10. Thanks, Daniel", "Hvass-Labs's tutorials are very helpful. Would be great if you could implement the MetaGraph.", "Bump for asking for a meta graph. It's become critical for me because the .pb file which is included with the tars of 2015's inception5h model appear to use an API which has since been deprecated", "Hvass-Labs's tutorials are outstanding. Please provide the MetaGraph etc that he is requesting.Everyone deeply appreciates his lessons. Kindly support his request\r\nThank You.", "Hvass-Labs's tutorials are outstanding. Please provide the MetaGraph etc that he is requesting.\r\n", "Hvass-Labs\u7684\u6559\u7a0b\u975e\u5e38\u51fa\u8272\u3002\u8bf7\u63d0\u4f9b\u4ed6\u8981\u6c42\u7684MetaGraph\u7b49\u3002", "Please support. Give the required files\n\nOn Feb 16, 2017 7:36 PM, \"karl596\" <notifications@github.com> wrote:\n\n> Hvass-Labs\u7684\u6559\u7a0b\u975e\u5e38\u51fa\u8272\u3002\u8bf7\u63d0\u4f9b\u4ed6\u8981\u6c42\u7684MetaGraph\u7b49\u3002\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5036#issuecomment-280540948>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AQAuomMNPiYTBbKMeOWPOc1D7fhk4uX6ks5rdRW_gaJpZM4KZgU8>\n> .\n>\n", "@karl596 tensorflow \u521a\u51fa\u4e86 v.1.0. \r\n\u6700\u65b0\u7248\u5e94\u8be5\u6709metagraph", "+1 for the really necessary feature.", "Please support this feature request. So the community surrounding tensor\ncan mature further,thank You.\n\nOn Tue, Mar 14, 2017 at 10:52 AM, Vladyslav Shkola <notifications@github.com\n> wrote:\n\n> +1 for the really necessary feature.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5036#issuecomment-286465258>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AQAuov9ZON-YfH24NLEoBlcuyb3GMCi9ks5rlrfGgaJpZM4KZgU8>\n> .\n>\n", "Hvass-Labs's tutorials are outstanding. Please provide the MetaGraph etc\nthat he is requesting. Thus extending the usage of this great and best\nmachine learning library.\n\nOn Tue, Mar 14, 2017 at 10:53 AM, remario richards <remariorich@gmail.com>\nwrote:\n\n> Please support this feature request. So the community surrounding tensor\n> can mature further,thank You.\n>\n> On Tue, Mar 14, 2017 at 10:52 AM, Vladyslav Shkola <\n> notifications@github.com> wrote:\n>\n>> +1 for the really necessary feature.\n>>\n>> \u2014\n>> You are receiving this because you commented.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/5036#issuecomment-286465258>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AQAuov9ZON-YfH24NLEoBlcuyb3GMCi9ks5rlrfGgaJpZM4KZgU8>\n>> .\n>>\n>\n>\n", "please support!", "please support this feature request.thank you!\n\nOn Sat, Mar 18, 2017 at 8:47 AM, van <notifications@github.com> wrote:\n\n> please support!\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5036#issuecomment-287547328>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AQAuogNLGn3cQqMa-wwDm6U-ftNLNy61ks5rm-BngaJpZM4KZgU8>\n> .\n>\n", "+1", "+1", "+1", "Please, support! +1", "+1", "+1", "+1", "+1", "+1", "+1", "+1", "+1", "+1", "+1", "+1", "\r\n+1", "+1", "+1", "Hvass-Labs tensorflow tutorial is awesome, the best I can find, please support his pull request, so we can learn more about tensorflow from Hvass-Labs, Thank tensorflow team", "+1", "+1", "+1", "Please use [export_inference_graph](https://github.com/tensorflow/models/blob/master/research/slim/export_inference_graph.py) to get the graph (see [Docs](https://github.com/tensorflow/models/tree/master/research/slim#Export))\r\n\r\n", "+1", "Fixed the links", "+1", "+1", "+1", "+1", "+1", "+1", "+1", "+1\r\n", "Thanks to everyone for supporting this issue!\r\n\r\nThe easiest solution is to just use Keras for doing Transfer Learning and Fine-Tuning in TensorFlow.\r\n\r\n* https://keras.io/applications/#usage-examples-for-image-classification-models\r\n* https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.3-using-a-pretrained-convnet.ipynb\r\n* https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/10_Fine-Tuning.ipynb"]}, {"number": 5035, "title": "Fix hdfs scheme parsing issue in tensorboard", "body": "Tensorboard HDFS support is broken because it's treated as a local relative path.\n", "comments": ["@llhe, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @danmane and @RenatoUtsch to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks for the fix! It looks like we have been looking at this: @YuefengZhou \nhttps://github.com/tensorflow/tensorflow/commit/c296ff248153f38df4da52827a64962df1ebbe96\nCould you sync and try?\n", "@drpngx that fix is already in master branch but incomplete and this fix is another place.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "OK, got it.\n"]}, {"number": 5034, "title": "Fix hdfs scheme parsing issue in tensorboard", "body": "Tensorboard HDFS support is broken because it's treated as a local relative path.\n", "comments": ["Can one of the admins verify this patch?\n", "@llhe, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @danmane and @RenatoUtsch to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Sorry, one typo in this patch, resubmit again.\n"]}, {"number": 5033, "title": "Provide manylinux1 wheels on PyPI", "body": "Context: [what is manylinux1 tag](https://github.com/pypa/manylinux).\n\nCurrently, Tensorflow wheels on PyPI suck. As many know, we cannot upload specific Linux wheels on PyPI because the cheese shop prohibits different flavors (though Windows and macOS are OK). However, one can upload universal Linux wheels aka manylinux1 (see the link from above). There is a tool which helps making manylinux1 wheels called [auditwheel](https://github.com/pypa/auditwheel).\n\n```\nauditwheel show tensorflow-0.11.0rc0-cp35-cp35m-linux_x86_64.whl \n\ntensorflow-0.11.0rc0-cp35-cp35m-linux_x86_64.whl is consistent with\nthe following platform tag: \"manylinux1_x86_64\".\n\nThe wheel references no external versioned symbols from system-\nprovided shared libraries.\n\nThe wheel requires no external shared libraries! :)\n```\n\nThis basically means that we can execute `auditwheel repair` and obtain the manylinux1 wheel.\n\nThus I propose to upload CPU-only wheels for Linux for Python 2/3 to PyPI with manylinux1 tag.\n", "comments": ["@martinwicke has a better understanding of PyPi and wheels than I do, perhaps he can comment or redirect to a more appropriate person.\n", "I did not know about this tool. @yifeif can you look into this one and see if this is something that would work for us? It may actually make the pypi story simpler.\n", "Will look into this.\n", "So?..", "This is now available with 0.12rc0 release!", "The Linux 0.12rc0 whl files available on PyPI do not conform to the manylinux1 specification provided in [PEP531](https://www.python.org/dev/peps/pep-0513/).  Specifically, they contain shared libraries which reference versioned symbols which do not meet the GLIBC <= 2.5 and GLIBCXX <= 3.4.9 requirements specified in the PEP.\r\n\r\n```\r\n$ auditwheel show tensorflow-0.12.0rc0-cp35-cp35m-manylinux1_x86_64.whl \r\n\r\ntensorflow-0.12.0rc0-cp35-cp35m-manylinux1_x86_64.whl is consistent\r\nwith the following platform tag: \"manylinux1_x86_64\".\r\n\r\nThe wheel references external versioned symbols in these system-\r\nprovided shared libraries: libc.so.6 with versions {'GLIBC_2.16',\r\n'GLIBC_2.9', 'GLIBC_2.17', 'GLIBC_2.3.4', 'GLIBC_2.10', 'GLIBC_2.3',\r\n'GLIBC_2.2.5', 'GLIBC_2.4', 'GLIBC_2.14', 'GLIBC_2.11', 'GLIBC_2.6',\r\n'GLIBC_2.7', 'GLIBC_2.3.2'}, libstdc++.so.6 with versions\r\n{'GLIBCXX_3.4.9', 'GLIBCXX_3.4.11', 'CXXABI_1.3', 'GLIBCXX_3.4',\r\n'CXXABI_1.3.5', 'GLIBCXX_3.4.14', 'GLIBCXX_3.4.19', 'GLIBCXX_3.4.18',\r\n'GLIBCXX_3.4.15'}, libdl.so.2 with versions {'GLIBC_2.2.5'},\r\nlibpthread.so.0 with versions {'GLIBC_2.3.2', 'GLIBC_2.2.5'},\r\nlibm.so.6 with versions {'GLIBC_2.2.5'}, libgcc_s.so.1 with versions\r\n{'GCC_3.0'}\r\n\r\nThe following external shared libraries are required by the wheel:\r\n{\r\n    \"libc.so.6\": \"/lib/x86_64-linux-gnu/libc-2.23.so\",\r\n    \"libdl.so.2\": \"/lib/x86_64-linux-gnu/libdl-2.23.so\",\r\n    \"libgcc_s.so.1\": \"/lib/x86_64-linux-gnu/libgcc_s.so.1\",\r\n    \"libm.so.6\": \"/lib/x86_64-linux-gnu/libm-2.23.so\",\r\n    \"libpthread.so.0\": \"/lib/x86_64-linux-gnu/libpthread-2.23.so\",\r\n    \"libstdc++.so.6\": \"/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21\"\r\n}\r\n```\r\n\r\nThe [pypa/manylinux repository](https://github.com/pypa/manylinux) includes Docker images which can be used to build manylinux1 compatible wheels.", "@yifeif I guess `auditwheel repair` should be enough to fix this. Just curious, how do you make wheels currently?", "`auditwheel repair` cannot correct versioned symbol outside of the spec.  The command only fixes wheel files which are missing external shared libraries.", "Mmm, building TF on CentOS 5 looks like a joke. Besides, nvidia/cuda:8.0 is already used, it is still Ubuntu 14.04 - the perfect ABI balance IMHO. I would explicitly write in the docs that TF's \"manylinux1\" is actually not manylinux1 in the strict definition and it should be enough.", "Thanks for bring this up guys. We do run our whl files through `auditwheel repair`, but as @jjhelmus mentioned, auditwheel does not fix symbols when they are from a too-recent version. @vmarkovtsev I will add a note on pypi for next release.", "@yifeif Thanks for update and I think the note sounds like the best solution.  \r\n\r\nAny chance you could share a bit about how the released whl files are built?  I help maintain the [conda-forge tensorflow package](https://github.com/conda-forge/tensorflow-feedstock) and we have been running into some issues on Linux.  We uses CentOS 6 as our Linux target which uses an older GLIBC than the whl files support.  I've tried to build tensorflow from source using our build setup but this has proved challenging.  Any hints you could provide would be most appreciated. ", "@jjhelmus you can refer to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/builds/pip.sh and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/build_pip_package.sh for how we build our whls."]}]