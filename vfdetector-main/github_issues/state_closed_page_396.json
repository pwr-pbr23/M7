[{"number": 42072, "title": "[Intel MKL] Supporting native format in Convolution Bwd", "body": "This PR adds support for native(user) format in convolution backward ops. _MklNative* ops will be used in both graph and eager modes. \r\nThis PR needs to be merged after \"DNNL 1.5.1 Upgrade\" PR is merged #42073 ", "comments": ["Thank you for the review. I have made the changes based on your comments. ", "@mahmoud-abuzaina Could you please help resolve the merge conflict? Thank you!", "Actually I did not find any conflict but did merge with master. Can you check now?"]}, {"number": 42071, "title": "[Intel MKL] Supporting native format in Convolution Fwd", "body": "This PR adds support for native(user) format in convolution forward op. _MklNative* ops will be used in both graph and eager modes. \r\nThis PR needs to be merged after \"DNNL 1.5.1 Upgrade\" PR is merged #42073 ", "comments": ["Thank you for your review. I have addressed the comments. "]}, {"number": 42069, "title": "Noisy Networks for Exploration", "body": "Hello, \r\n\r\n  I implemented NoisyNet layers into Keras API. It's useful for users, that experiments with reinforcement learning algorithms (NoisyDQN or NoisyA3C). For more information, I made a description of those layers in the code.\r\n\r\n## Links\r\n[Noisy Networks for Exploration](https://arxiv.org/abs/1706.10295)", "comments": ["Hello, \r\n\r\n&emsp; I tested it on my Bachelor's project and the results are here. I found here small mistake with `self.kernel_epsilon.assign(w_eps)` and `self.bias_epsilon.assign(b_eps)` -- bad spaces before this, which cause problems...\r\n\r\n## Chart\r\n![W B Chart 29  8  2020, 11 06 32](https://user-images.githubusercontent.com/16693449/91633376-21625b80-e9e8-11ea-99ba-efc7711e244a.png)\r\n\r\nNoisyNet is faster finding the optimal policy than Deep Q networks with epsilon-greedy.\r\n\r\n## Code\r\n[Noisy DQN MazeSolver](https://github.com/markub3327/DQN_MazeSolver)", "Hello,\r\n   \r\n> what do you think about this new feature?", "I think this should be added to the Keras layers in TensorFlow add-ons repository. ", "I found it here: https://github.com/tensorflow/addons/blob/master/tensorflow_addons/layers/noisy_dense.py ... Somebody published a similar solution like me, but later than me ... You should have told me earlier that add-ons repo is more suitable than post it here ... Thanks.", "I made new PR here: https://github.com/tensorflow/addons/pull/2206\r\n\r\nI think that this PR should be closed ... Any ideas?"]}, {"number": 42068, "title": "tf.keras.utils.plot_model doesn't work", "body": "**System information**\r\nGoogle colab.\r\n\r\n**Steps to reproduce:**\r\n1. Click \"Run all\" at Colab of https://www.tensorflow.org/tutorials/structured_data/feature_columns\r\n2. Add a cell with the following code:\r\n```tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)```\r\n\r\n\r\n**Describe the current behavior**\r\n1. It displays only 1 block `[sequential]`.\r\n2. If you copy all cells + plot_model and run locally in a separate py script (not Colab), you'll also get a lot of warnings that features v2 are deprecated and will be removed in next releases (tf 2.2).\r\n\r\n**Describe the expected behavior**\r\n1. The model would be displayed.\r\n2. No deprecation warning would be displayed.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://www.tensorflow.org/tutorials/structured_data/feature_columns\r\n", "comments": ["Probably you need to consider https://github.com/tensorflow/tensorflow/issues/27416#issuecomment-565255351. \r\nI don't know if the tutorial could be updated.", "I have tried in colab with TF version 2.3, nightly version(`2.4.0-dev20200805`) and it displays only `1 block [sequential] `when we execute `tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)`.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/ad694d97ade05d0fb7590e23f5ad0a7e/untitled223.ipynb).Thanks!", "@pochtar I think this is due to shape inference. For Sequential API, we need to provide `input_shape` so that it will infer shape of the other layers. When I executed, `model.summary` it shows `multiple` as `output_shape`. \r\n\r\nLooking at the [link](https://github.com/tensorflow/tensorflow/issues/27416#issuecomment-565255351) provided by @bhack , \r\n\r\n> The interaction between feature columns and Keras is painful. To address this issue, we have proposed Keras preprocessing layers.\r\n\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense_features_6 (DenseFeatu multiple                  1328      \r\n_________________________________________________________________\r\ndense (Dense)                multiple                  19328     \r\n_________________________________________________________________\r\ndense_1 (Dense)              multiple                  16512     \r\n_________________________________________________________________\r\ndropout (Dropout)            multiple                  0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              multiple                  129       \r\n=================================================================\r\nTotal params: 37,297\r\nTrainable params: 37,297\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```", "@pochtar, have you seen the [Classify structured data using Keras Preprocessing Layers tutorial](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers)? This is the same PetFinder dataset example but with preprocessing layers, and shows the results of `tf.keras.utils.plot_model`.\r\n\r\nClosing this issue now since using preprocessing layers is preferred, and the feature column tutorial has the following note: `Note: If you are starting a new project to classify structured data, we recommend you use preprocessing layers.`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42068\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42068\">No</a>\n"]}, {"number": 42067, "title": "Fix issue in tf.string.format wheree unicode is not rendered correctly", "body": "This PR tries to address the issue raised in #42001 where\r\nunicode tensor is not rendered correctly in tf.string.format.\r\n\r\nThe issue was that tf.string.format incorrectly escape the tensor.\r\n\r\nThis PR address the issue by adding escape option in tensor print\r\nfunction to selectively control the escape.\r\n\r\nThis PR fixes #42001.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Is it not safe to use `absl::Utf8SafeCEscape`?", "@bhack `absl::Utf8SafeCEscape` will work, thanks. The PR has been updated."]}, {"number": 42065, "title": "tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Tensorflow version: 2.3.0\r\n- TensorFlow installed from (source or binary): Binary (Anaconda) \r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir,signature_keys=['serving_default'])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nquantized_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-08-05 14:53:54.038060: I tensorflow/stream_executor/platform/default/d\r\nso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-05 14:53:56.073223: I tensorflow/stream_executor/platform/default/d\r\nso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-08-05 14:53:58.130905: I tensorflow/stream_executor/cuda/cuda_gpu_exec\r\nutor.cc:982] successful NUMA node read from SysFS had negative value (-1), \r\nbut there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-05 14:53:58.131826: I tensorflow/core/common_runtime/gpu/gpu_device\r\n.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\r\ncoreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryB\r\nandwidth: 223.96GiB/s\r\n2020-08-05 14:53:58.131898: I tensorflow/stream_executor/platform/default/d\r\nso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-05 14:53:58.134264: I tensorflow/stream_executor/platform/default/d\r\nso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-08-05 14:53:58.136456: I tensorflow/stream_executor/platform/default/d\r\nso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-05 14:53:58.136904: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-05 14:53:58.139311: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-05 14:53:58.140510: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-05 14:53:58.145672: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-05 14:53:58.145878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-05 14:53:58.146748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-05 14:53:58.147536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-05 14:53:58.147999: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-05 14:53:58.158009: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\r\n2020-08-05 14:53:58.158281: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558637af4d10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-05 14:53:58.158342: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-05 14:53:58.207301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-05 14:53:58.208313: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558637b08a60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-05 14:53:58.208367: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\r\n2020-08-05 14:53:58.208657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-05 14:53:58.209465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\r\ncoreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\r\n2020-08-05 14:53:58.209534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-05 14:53:58.209670: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-08-05 14:53:58.209798: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-05 14:53:58.209882: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-05 14:53:58.209990: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-05 14:53:58.210115: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-05 14:53:58.210180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-05 14:53:58.210338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-05 14:53:58.211290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-05 14:53:58.212074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-05 14:53:58.212147: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-05 14:53:58.765400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-05 14:53:58.765463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-08-05 14:53:58.765635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-08-05 14:53:58.766139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-05 14:53:58.767126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-05 14:53:58.767958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10618 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\r\n2020-08-05 14:54:09.899095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-05 14:54:09.899586: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-08-05 14:54:09.899811: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-08-05 14:54:09.900545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-05 14:54:09.900959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\r\ncoreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\r\n2020-08-05 14:54:09.901027: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-05 14:54:09.901113: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-08-05 14:54:09.901170: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-05 14:54:09.901247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-05 14:54:09.901349: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-05 14:54:09.901427: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-05 14:54:09.901501: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-05 14:54:09.901648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-05 14:54:09.902096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-05 14:54:09.902468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-05 14:54:09.902524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-05 14:54:09.902550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-08-05 14:54:09.902571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-08-05 14:54:09.902728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-05 14:54:09.903208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-05 14:54:09.903590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10618 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\r\n2020-08-05 14:54:10.067218: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2020-08-05 14:54:10.067284: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 2978 nodes (2644), 3263 edges (2922), time = 94.971ms.\r\n2020-08-05 14:54:10.067309: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 1.906ms.\r\n2020-08-05 14:54:13.144913: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n2020-08-05 14:54:13.144989: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\nloc(\"Func/StatefulPartitionedCall/input/_0\"): error: requires all operands and results to have compatible element types\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/tf-detect/lib/python3.8/site-packages/tensorflow/lite/python/convert.py\", line 196, in toco_convert_protos\r\n    model_str = wrap_toco.wrapped_toco_convert(model_flags_str,\r\n  File \"/opt/conda/envs/tf-detect/lib/python3.8/site-packages/tensorflow/lite/python/wrap_toco.py\", line 32, in wrapped_toco_convert\r\n    return _pywrap_toco_api.TocoConvert(\r\nException: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types\r\n<unknown>:0: note: loc(\"Func/StatefulPartitionedCall/input/_0\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<1x?x?x3x!tf.quint8>) -> tensor<1x?x?x3xui8>\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"scratch.py\", line 8, in <module>\r\n    quantized_model = converter.convert()\r\n  File \"/opt/conda/envs/tf-detect/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 1076, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"/opt/conda/envs/tf-detect/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 899, in convert\r\n    return super(TFLiteFrozenGraphConverterV2,\r\n  File \"/opt/conda/envs/tf-detect/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 629, in convert\r\n    result = _toco_convert_impl(\r\n  File \"/opt/conda/envs/tf-detect/lib/python3.8/site-packages/tensorflow/lite/python/convert.py\", line 569, in toco_convert_impl\r\n    data = toco_convert_protos(\r\n  File \"/opt/conda/envs/tf-detect/lib/python3.8/site-packages/tensorflow/lite/python/convert.py\", line 202, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types\r\n<unknown>:0: note: loc(\"Func/StatefulPartitionedCall/input/_0\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<1x?x?x3x!tf.quint8>) -> tensor<1x?x?x3xui8>\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/5029227/model.zip)\r\n\r\n\r\n\r\n**Any other info / logs**\r\nModel architecture is SSD-Mobilenet V2 with input size of 96x96, created by Object Detection API\r\nModel exported according to [instructions](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#exporting-a-trained-model)\r\n", "comments": ["The fix from https://github.com/tensorflow/tensorflow/issues/41877 did not resolve this", "+1", "@dtch1997 \r\nPlease provide complete code to replicate the issue faced or if possible share a colab gist to replicate the issue reported.", "@dtch1997 @korabelnikov can you install `tf-nightly` and try again? And also update the input shape?\r\n\r\nSpecifically this solution posted here: https://github.com/tensorflow/tensorflow/issues/42114#issuecomment-671593386\r\n\r\n", "Thanks! It worked after I installed tf-nightly. ", "Issue still exists for TF version 2.3.0 and EfficientDet model. Is anyone facing the same problem? ", "@smohan10 We are tracking TFLite support for these models [here](https://github.com/tensorflow/models/issues/9033). SSD-only for now, EfficientDet is work-in-progress.", "For me just adding following line from [this comment](https://github.com/tensorflow/tensorflow/issues/42114#issuecomment-671593386) fixed the issue.\r\n\r\n`   converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]`\r\n\r\nI am using `tensorflow~=2.4.1`"]}, {"number": 42064, "title": "Building Tensorflow Lite Locally :  Unrecoverable error while evaluating node ", "body": "I am building on Linux Ubuntu 16.04, and following the Android quickstart guide:\r\nhttps://www.tensorflow.org/lite/guide/android\r\n\r\nI build the image with the given DockerFile:\r\nhttps://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/dockerfiles/tflite-android.Dockerfile\r\ndocker build . -t tflite-builder -f tflite-android.Dockerfile\r\n\r\n\r\nI run it with the following command:\r\n\r\ndocker run -it -w /tensorflow_src -v $PWD:/mnt -e HOST_PERMS=\"$(id -u):$(id -g)\" \\\r\n    tensorflow/tensorflow:devel bash\r\n\r\nAdditional android tools in the container:\r\nandroid update sdk --no-ui -a --filter tools,platform-tools,android-${ANDROID_API_LEVEL},build-tools-${ANDROID_BUILD_TOOLS_VERSION}\r\n\r\n\r\nand finally the build command in bazel:\r\nbazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  //tensorflow/lite/java:tensorflow-lite\r\n\r\n\r\ngiving error:\r\nFAILED: Build did NOT complete successfully (65 packages loaded, 1078 targets configured)\r\nInternal error thrown during build. Printing stack trace: java.lang.RuntimeException: Unrecoverable error while evaluating node '@bazel_tools//src/tools/android/java/com/google/devtools/build/android/incrementaldeployment:incremental_stub_application BuildConfigurationValue.Key[291c4977886154b7d32d07c6d0dba1da3dce0bb2e69fd7a43e21c0fea58a22d0] false' (requested by nodes '@bazel_tools//tools/android:incremental_stub_application BuildConfigurationValue.Key[291c4977886154b7d32d07c6d0dba1da3dce0bb2e69fd7a43e21c0fea58a22d0] false')\r\n        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:515)\r\n        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:399)\r\n        at java.base/java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(Unknown Source)\r\n        at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)\r\n        at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)\r\n        at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)\r\n        at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)\r\n        at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)\r\nCaused by: java.lang.NullPointerException\r\n        at com.google.devtools.build.lib.rules.android.BusyBoxActionBuilder.addAapt(BusyBoxActionBuilder.java:315)\r\n        at com.google.devtools.build.lib.rules.android.AndroidResourcesProcessorBuilder.createAapt2ApkAction(AndroidResourcesProcessorBuilder.java:279)\r\n        at com.google.devtools.build.lib.rules.android.AndroidResourcesProcessorBuilder.build(AndroidResourcesProcessorBuilder.java:208)\r\n        at com.google.devtools.build.lib.rules.android.AndroidResourcesProcessorBuilder.buildWithoutLocalResources(AndroidResourcesProcessorBuilder.java:179)\r\n        at com.google.devtools.build.lib.rules.android.ResourceApk.processFromTransitiveLibraryData(ResourceApk.java:328)\r\n        at com.google.devtools.build.lib.rules.android.AndroidLibrary.create(AndroidLibrary.java:178)\r\n        at com.google.devtools.build.lib.rules.android.AndroidLibrary.create(AndroidLibrary.java:42)\r\n        at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:484)\r\n        at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:190)\r\n        at com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:888)\r\n        at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:899)\r\n        at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:338)\r\n        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:438)\r\n        ... 7 more\r\njava.lang.RuntimeException: Unrecoverable error while evaluating node '@bazel_tools//src/tools/android/java/com/google/devtools/build/android/incrementaldeployment:incremental_stub_application BuildConfigurationValue.Key[291c4977886154b7d32d07c6d0dba1da3dce0bb2e69fd7a43e21c0fea58a22d0] false' (requested by nodes '@bazel_tools//tools/android:incremental_stub_application BuildConfigurationValue.Key[291c4977886154b7d32d07c6d0dba1da3dce0bb2e69fd7a43e21c0fea58a22d0] false')\r\n        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:515)\r\n        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:399)\r\n        at java.base/java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(Unknown Source)\r\n        at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)\r\n        at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)\r\n        at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)\r\n        at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)\r\n        at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)\r\nCaused by: java.lang.NullPointerException\r\n        at com.google.devtools.build.lib.rules.android.BusyBoxActionBuilder.addAapt(BusyBoxActionBuilder.java:315)\r\n        at com.google.devtools.build.lib.rules.android.AndroidResourcesProcessorBuilder.createAapt2ApkAction(AndroidResourcesProcessorBuilder.java:279)\r\n        at com.google.devtools.build.lib.rules.android.AndroidResourcesProcessorBuilder.build(AndroidResourcesProcessorBuilder.java:208)\r\n        at com.google.devtools.build.lib.rules.android.AndroidResourcesProcessorBuilder.buildWithoutLocalResources(AndroidResourcesProcessorBuilder.java:179)\r\n        at com.google.devtools.build.lib.rules.android.ResourceApk.processFromTransitiveLibraryData(ResourceApk.java:328)\r\n        at com.google.devtools.build.lib.rules.android.AndroidLibrary.create(AndroidLibrary.java:178)\r\n        at com.google.devtools.build.lib.rules.android.AndroidLibrary.create(AndroidLibrary.java:42)\r\n        at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:484)\r\n        at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:190)\r\n        at com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:888)\r\n        at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:899)\r\n        at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:338)\r\n        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:438)\r\nFAILED: Build did NOT complete successfully (65 packages loaded, 1078 targets configured)\r\n\r\n", "comments": ["Additional Information: I was able to successfully build the C++ shared libraries with command:\r\nbazel build -c opt --config=android_arm64 //tensorflow/lite:libtensorflowlite.so\r\n", "Any update?", "Did you run \"./configure\" to configure Android SDK and NDK?", "As illustrated above I am using a dockerfile from the Android quickstart: \r\nhttps://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/dockerfiles/tflite-android.Dockerfile\r\n\r\nTherefore this bit of infrastructure is taken care of without the need for ./configure \r\n\r\nHeres the guide again:\r\nhttps://www.tensorflow.org/lite/guide/android#build_tensorflow_lite_locally", "any sort of input on this", "I got the same issue here, did you fix it?", "I tracked down when the guide was written. It was about [more than a year ago](https://github.com/tensorflow/tensorflow/commit/fce56a775dfde2a4465888d26ab705e713af5968#diff-f03969229b47807f413de9b51492d8312c1f172a25be052fa9ccd412e2beeae6) so the instructions may not work with current latest docker image (`tensorflow/tensorflow:devel`) and codebase. I will try to use an image published around the time of writing to see if we can build TensorFlow Lite with the instructions.", "The instructions for building with Docker was only added around June this year. I tried to pull a Docker image from a year ago but there was no `tensorflow_src` directory in that image.", "You can find updated doc here.\r\nhttps://www.tensorflow.org/lite/guide/build_android\r\nThere is a step to download Docker file.", "@terryheo Was you able to reproduce a successful build with the instructions? Was the Docker file updated recently (during 2 days ago)?\r\n\r\nI just tried to build TensorFlow Lite again with that Docker file but it is still failing as reported in this issue.", "@amitDaMan As I understand, you can build it successfully locally but fail with docker?", "I was able to reproduce this issue. However, the error is gone after running ./configure with configuration for Android builds under /tensorflow_src\r\nThe ./configure add some flags needed for bazel so please run it even you are using docker.\r\n\r\nBesides, if you would like to reduce the binary size, I would recommend to take a look at https://www.tensorflow.org/lite/guide/reduce_binary_size", "Closing this issue here. Feel free to reopen it if you still face this error after running ./configure.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42064\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42064\">No</a>\n"]}, {"number": 42063, "title": "Windows symbol exports for C++ API", "body": "First attempt to resolve #41904 by mimicking the \"symbol filtering\" approach used for `tensorflow.dll`.\r\n\r\nWould prefer an approach that explicitly exports symbols in the C++ source as the filtering approach seems fragile and hacky, but not sure how to export symbols generated by Protobuf this way.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42063) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!\n\nOn Wed, 5 Aug 2020 at 15:43, googlebot <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here with @googlebot\n> I signed it! and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2139\ufe0f *Googlers: Go here\n> <https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42063>\n> for more info*.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/42063#issuecomment-669234542>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AADHBZJZ5OCCD3DLWEHJONLR7FVZZANCNFSM4PVRZNRQ>\n> .\n>\n\n\n-- \nAndrew Marshall\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42063) for more info**.\n\n<!-- ok -->", "@planetmarshall  This PR is in draft, any update on this? Please. Thanks!", "> @planetmarshall This PR is in draft, any update on this? Please. Thanks!\r\n\r\nI'm not 100% happy with the solution as it still feels very hacky to me. However, it does work, and we're now successfully using the C++ API to load and run a saved Keras model in a production system on Windows, based on this branch. I'll publish the PR and see what people think.\r\n\r\nCheers.", "Have you tried this commit: https://github.com/meteorcloudy/tensorflow/commit/4b8e7bef11040c465453e973eb0c6ad43cfd3c96?\r\nI think it will solve you problem.", "> Have you tried this commit: [meteorcloudy@4b8e7be](https://github.com/meteorcloudy/tensorflow/commit/4b8e7bef11040c465453e973eb0c6ad43cfd3c96)?\r\n> I think it will solve you problem.\r\n\r\nI'm afraid not. See my reply in the linked issue.", "@gbaned i don't believe i am the right person to review this PR. Removing myself.", "@meteorcloudy  Could you have a look at this PR? Thank you!", "@planetmarshall  Can you please resolve conflicts? Thanks!", "Closing this so that I can evaluate the proposed change from @meteorcloudy  in the linked issue."]}, {"number": 42062, "title": "multi worker in nccl mode with something wrong", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):N/A\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\nTensorFlow installed from (source or binary): Binary\r\nTensorFlow version (use command below): 2.3.0\r\nPython version: 3.7.4\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: 10.1.243/N\r\nGPU model and memory: V100 32GB\r\n\r\nI just run a sample code with keras training api  in NCCL multi worker mode with 2*8V100gpus.\r\nSometimes I the training process with failed with this error. But sometimes not, and the training process is same as before.\r\n\r\nHere is the error information:\r\n\r\n>`2020-08-05 20:21:04.420413: E tensorflow/core/common_runtime/ring_alg.cc:274] Aborting RingReduce with Invalid argument: [_Derived_]indices[4] = 14 is not in [0, 14)\r\n\t [[{{node GatherV2}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional_3]]\r\n\t [[GroupCrossDeviceControlEdges_3/div_no_nan/_619]]\r\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\r\n:{\"created\":\"@1596630064.420329728\",\"description\":\"Error received from peer ipv4:10.128.98.77:9000\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"[_Derived_]indices[4] = 14 is not in [0, 14)\\n\\t [[{{node GatherV2}}]]\\n\\t [[MultiDeviceIteratorGetNextFromShard]]\\n\\t [[RemoteCall]]\\n\\t [[IteratorGetNextAsOptional_3]]\\n\\t [[GroupCrossDeviceControlEdges_3/div_no_nan/_619]]\",\"grpc_status\":3}\r\n2020-08-05 20:21:04.420665: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at collective_ops.cc:257 : Invalid argument: [_Derived_]indices[4] = 14 is not in [0, 14)\r\n\t [[{{node GatherV2}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional_3]]\r\n\t [[GroupCrossDeviceControlEdges_3/div_no_nan/_619]]\r\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\r\n:{\"created\":\"@1596630064.420329728\",\"description\":\"Error received from peer ipv4:10.128.98.77:9000\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"[_Derived_]indices[4] = 14 is not in [0, 14)\\n\\t [[{{node GatherV2}}]]\\n\\t [[MultiDeviceIteratorGetNextFromShard]]\\n\\t [[RemoteCall]]\\n\\t [[IteratorGetNextAsOptional_3]]\\n\\t [[GroupCrossDeviceControlEdges_3/div_no_nan/_619]]\",\"grpc_status\":3}\r\nTraceback (most recent call last):\r\n  File \"/opt/tiger/mta/scripts/keras/../../scripts/keras/train_model.py\", line 115, in <module>\r\n    main(args)\r\n  File \"/opt/tiger/mta/scripts/keras/../../scripts/keras/train_model.py\", line 110, in main\r\n    trainer.fit(task_index,num_devices)\r\n  File \"/opt/tiger/mta/mta/keras/trainer.py\", line 170, in fit\r\n    steps_per_epoch=self._config.train.steps_per_epoch,verbose=self._config.train.verbose_mode,callbacks=[CusCallBack(self._config,task_index,self._lr_scheduler,num_devices),self._ckpt_callback])\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 117, in _method_wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 860, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 115, in <lambda>\r\n    lambda _: method(self, *args, **kwargs),\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1098, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 807, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2829, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1848, in _filtered_call\r\n    cancellation_manager=cancellation_manager)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1924, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 550, in call\r\n    ctx=ctx)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  [_Derived_]indices[4] = 14 is not in [0, 14)\r\n\t [[{{node GatherV2}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional_3]]\r\n\t [[GroupCrossDeviceControlEdges_3/div_no_nan/_619]]\r\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\r\n:{\"created\":\"@1596630064.420329728\",\"description\":\"Error received from peer ipv4:10.128.98.77:9000\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"[_Derived_]indices[4] = 14 is not in [0, 14)\\n\\t [[{{node GatherV2}}]]\\n\\t [[MultiDeviceIteratorGetNextFromShard]]\\n\\t [[RemoteCall]]\\n\\t [[IteratorGetNextAsOptional_3]]\\n\\t [[GroupCrossDeviceControlEdges_3/div_no_nan/_619]]\",\"grpc_status\":3}\r\n\t [[allreduce/CollectiveReduce]]\r\n\t [[replica_4/Size_3/_270]] [Op:__inference_train_function_116225]\r\n\r\nFunction call stack:`\r\nI will be grateful if anyone who help me save the problem.", "comments": ["@ImMrMa,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42062\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42062\">No</a>\n"]}, {"number": 42061, "title": "Fail to export function containing tf.lookup.StaticHashTable", "body": "**System information**\r\n- Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary (`pip install tensorflow)\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\n\r\nTrying to save a SavedModel with `tf.saved_model.save`, `tf.Module` fails if it includes `tf.lookup.StaticHashTable`.\r\n\r\n**Describe the expected behavior**\r\n\r\nNo failure.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThe following code \r\n\r\n```\r\nimport tensorflow as tf\r\nclass M(tf.Module):\r\n    @tf.function\r\n    def __call__(self, a):       \r\n        keys_tensor = tf.constant([1, 2])\r\n        vals_tensor = tf.constant([3, 4])\r\n        input_tensor = tf.constant([1, 5])\r\n        tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor), -1)\r\n\r\n        return a\r\n\r\nm = M()\r\ncf = m.__call__.get_concrete_function(a=tf.TensorSpec([None], tf.float32))\r\n\r\ntf.saved_model.save(m, \"~/output_dir\", signatures={\"serving_default\": cf})\r\n```\r\n\r\nyields the error:\r\n\r\n```\r\nAssertionError: Tried to export a function which references untracked object Tensor(\"178:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\r\n```\r\n\r\nNote that the following modification (assigning the return value of `tf.lookup.StaticHashTable` to a class attribute) fails with the same Exception\r\n\r\n```\r\nimport tensorflow as tf\r\nclass M(tf.Module):\r\n    def __init__(self, **kwargs):\r\n        super(M, self).__init__()\r\n        self.table = None\r\n        \r\n    @tf.function\r\n    def __call__(self, a):       \r\n        keys_tensor = tf.constant([1, 2])\r\n        vals_tensor = tf.constant([3, 4])\r\n        input_tensor = tf.constant([1, 5])\r\n        self.table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor), -1)\r\n        \r\n        return a\r\n    \r\nm = M()\r\nconcrete_fn = m.__call__.get_concrete_function(a=tf.TensorSpec([None], tf.float32))\r\n\r\ntf.saved_model.save(m, \"~/some_output_dir\", signatures={\"serving_default\": concrete_fn})\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nFull trace:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-19-401fea85b3de> in <module>\r\n     17 concrete_fn = m.__call__.get_concrete_function(a=tf.TensorSpec([None], tf.float32))\r\n     18 \r\n---> 19 tf.saved_model.save(m, \"~/some_output_dir\", signatures={\"serving_default\": concrete_fn})\r\n\r\n~/py3env/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n    974 \r\n    975   _, exported_graph, object_saver, asset_info = _build_meta_graph(\r\n--> 976       obj, export_dir, signatures, options, meta_graph_def)\r\n    977   saved_model.saved_model_schema_version = constants.SAVED_MODEL_SCHEMA_VERSION\r\n    978 \r\n\r\n~/py3env/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in _build_meta_graph(obj, export_dir, signatures, options, meta_graph_def)\r\n   1064   asset_info, exported_graph = _fill_meta_graph_def(meta_graph_def,\r\n   1065                                                     saveable_view, signatures,\r\n-> 1066                                                     options.namespace_whitelist)\r\n   1067   if options.function_aliases:\r\n   1068     function_aliases = meta_graph_def.meta_info_def.function_aliases\r\n\r\n~/py3env/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in _fill_meta_graph_def(meta_graph_def, saveable_view, signature_functions, namespace_whitelist)\r\n    651 \r\n    652   with exported_graph.as_default():\r\n--> 653     signatures = _generate_signatures(signature_functions, resource_map)\r\n    654     for concrete_function in saveable_view.concrete_functions:\r\n    655       concrete_function.add_to_graph()\r\n\r\n~/py3env/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in _generate_signatures(signature_functions, resource_map)\r\n    517                                                   signature_key, function.name))\r\n    518     outputs = _call_function_with_mapped_captures(\r\n--> 519         function, mapped_inputs, resource_map)\r\n    520     signatures[signature_key] = signature_def_utils.build_signature_def(\r\n    521         _tensor_dict_to_tensorinfo(exterior_argument_placeholders),\r\n\r\n~/py3env/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in _call_function_with_mapped_captures(function, args, resource_map)\r\n    469   \"\"\"Calls `function` in the exported graph, using mapped resource captures.\"\"\"\r\n    470   export_captures = _map_captures_to_created_tensors(function.graph.captures,\r\n--> 471                                                      resource_map)\r\n    472   # Calls the function quite directly, since we have new captured resource\r\n    473   # tensors we need to feed in which weren't part of the original function\r\n\r\n~/py3env/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in _map_captures_to_created_tensors(original_captures, resource_map)\r\n    392            \"be tracked by assigning them to an attribute of a tracked object \"\r\n    393            \"or assigned to an attribute of the main object directly.\"\r\n--> 394           ).format(interior))\r\n    395     export_captures.append(mapped_resource)\r\n    396   return export_captures\r\n\r\nAssertionError: Tried to export a function which references untracked object Tensor(\"836:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\r\n```\r\n", "comments": ["https://github.com/tensorflow/tensorflow/issues/39064", "Thanks, it seems that the following works:\r\n\r\n```\r\nimport tensorflow as tf\r\nclass M(tf.Module):\r\n    def __init__(self, **kwargs):\r\n        super(M, self).__init__()\r\n        self.table = None\r\n        \r\n    def f(self):\r\n        @tf.function\r\n        def inner(a):\r\n            keys_tensor = tf.constant([1, 2])\r\n            vals_tensor = tf.constant([3, 4])\r\n            input_tensor = tf.constant([1, 5])\r\n            self.table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor), -1)\r\n        \r\n            return a\r\n        return inner\r\n    \r\nm = M()\r\nconcrete_fn = m.f().get_concrete_function(a=tf.TensorSpec([None], tf.float32))\r\n\r\ntf.saved_model.save(m, \"~/some_output_dir\", signatures={\"serving_default\": concrete_fn})\r\n```\r\n\r\nIs there an explanation why this works and my original code does not? I'd like to understand the underlying issue a bit more (if at all possible). Is this expected behavior? The docs seem to encourage decorating the class methods, e.g. https://www.tensorflow.org/guide/saved_model#saving_a_custom_model, thus I'm a bit surprised that this actually makes a difference.\r\nBest regards", "@bergfita \r\nplease move the issue to closed status if resolved.", "@bergfita @bhack : I'm having similar issue. I'm using `StaticHashTable` as in one Lambda layer after the output layer of my tf.keras model. It's quite simple actually: I've a text classification models and I'm adding a simple lambda layer that takes the `model.output` and convert the model_id to more general labels. I can save this version of model with model.save(... as H5 format..) without any issue, and can load it back and use it without any problem.\r\n\r\nIssue is, when I try to export my TF2.2.0 model for TF-Serving, I can't find how I can export it. Here is what I can do with TF1.X or with `TF2.X + tf.compat.v1.disable_eager_execution()`\r\n\r\n```python\r\ntf.compat.v1.disable_eager_execution()\r\nversion = 1\r\nname = 'tmp_model'\r\nexport_path = f'/opt/tf_serving/{name}/{version}'\r\nbuilder = saved_model_builder.SavedModelBuilder(export_path)\r\n\r\nmodel_signature = tf.compat.v1.saved_model.predict_signature_def(\r\n    inputs={\r\n        'input': model.input\r\n    }, \r\n    outputs={\r\n        'output': model.output\r\n    }\r\n)\r\n\r\nwith tf.compat.v1.keras.backend.get_session() as sess:\r\n    builder.add_meta_graph_and_variables(\r\n        sess=sess,\r\n        tags=[tf.compat.v1.saved_model.tag_constants.SERVING],\r\n        signature_def_map={\r\n            'predict': model_signature\r\n        },\r\n        # For initializing Hashtables\r\n        main_op=tf.compat.v1.tables_initializer()\r\n    )\r\n    builder.save()\r\n```\r\n\r\nThis will save my models with TF1.X format for serving and I can use it without any issue. Things is, I'm using LSTM layer and I want to use my model on GPU. By the documentation, if I disable the eager mode, I can't use the GPU-version of LSTM with TF2.2. And without going through above mentioned code, I can't save my model for serving wrt TF2.2 standard and StaticHashTables. \r\n\r\nHere is how I'm trying to export my TF2.2 model which is using StaticHashTables in final layer; and which is giving me same error as above:\r\n\r\n```python\r\nclass MyModule(tf.Module):\r\n\r\n    def __init__(self, model):\r\n        super(MyModule, self).__init__()\r\n        self.model = model\r\n    \r\n    @tf.function(input_signature=[tf.TensorSpec(shape=(None, 16), dtype=tf.int32, name='input')])\r\n    def predict(self, input):\r\n        result = self.model(input)\r\n        return {\"output\": result}\r\n\r\nversion = 1\r\nname = 'tmp_model'\r\nexport_path = f'/opt/tf_serving/{name}/{version}'\r\n\r\nmodule = MyModule(model)\r\ntf.saved_model.save(module, export_path, signatures={\"predict\": module.predict.get_concrete_function()})\r\n```\r\n\r\nAny suggestion or am I missing anything on exporting TF2.2 model which is using the `StaticHashTables` in final Lambda layer for TensorFlow Serving? \r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42061\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42061\">No</a>\n", "More information here on how save model with StatcHashTable: https://github.com/tensorflow/serving/issues/1719", "@bergfita, I have tried your way, but it doesn't work for my text-classification model.\r\n\r\nis it necessary to initialise the `tf.lookup.KeyValueTensorInitializer` inside the function?\r\n\r\nI have used tensorflow 2.7.0 on Ubuntu 18.04(google colab) to train and save the model.\r\n\r\nmy code:\r\n```\r\nclass TFModel(tf.Module):\r\n    def __init__(self, model: tf.keras.Model) -> None:\r\n        self.model = model\r\n\r\n        # this \"key_value_tensor_initializer\" is an instance of 'tf.lookup.KeyValueTensorInitializer', but this key-value initialiser is not build inside this constructor, but an outside global variable\r\n        self.hash_table = tf.lookup.StaticHashTable(initializer=key_value_tensor_initializer,\r\n                                                    default_value=1, name='hash_table')\r\n\r\n    def pre_process(self, comment):\r\n        # this function does all the pre-processing on the input payload to the format acceptable by model for predictions\r\n        # convert the text to lower using \"tf.strings.lower\" -> does 3 regex ops with \"tf.strings.regex_replace\" one after the other -> \r\n        # splits sentences to words using \"tf.strings.split\" -> then look-up those in hash-table -> then pad 0s at the end -> return it\r\n        return preprocessed_comment\r\n        \r\n    def post_process(self, probabilities):\r\n        # this function converts the model predictions(probabilities) to output classes\r\n        # this function will return a RaggedTensor\r\n        return final_labels\r\n\r\n    def f(self):\r\n        @tf.function\r\n        def inner(comment: List[str]) -> Dict[str, Union[List[float], List[ByteString], str]]:\r\n            processed_comment = self.pre_process(comment)\r\n            predicted_probs = self.model(processed_comment)\r\n            predicted_labels = self.post_process(predicted_probs)\r\n            return {'predicted_probabilities': predicted_probs,\r\n                    'predicted_labels':predicted_labels,\r\n                    'description': 'prediction probabilities ranges from 0 (hesitant) to 1 (confident).'}\r\n        return inner\r\n\r\n    \r\n##### serialising part\r\n\r\n## create an instance\r\ntf_model_wrapper = TFModel(model)\r\n# trying to create concrete_function as mentioned in\r\n# https://github.com/tensorflow/tensorflow/issues/42061#issuecomment-669770284 \r\nconcrete_fn = tf_model_wrapper.f().get_concrete_function(comment=tf.TensorSpec([None], tf.string))\r\n## save the model to disk(serialize it)\r\ntf.keras.models.save_model(\r\n    model=tf_model_wrapper.model,\r\n    filepath='/content/complex_nw_v1',\r\n    signatures={'serving_default': concrete_fn})\r\n```\r\n\r\nbut, I get this error:\r\n```\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-76-889bb0fb80ba> in <module>()\r\n    101     model=tf_model_wrapper.model,\r\n    102     filepath='/content/complex_nw_v1',\r\n--> 103     signatures={\"serving_default\": concrete_fn})\r\n\r\n1 frames\r\n/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)\r\n     65     except Exception as e:  # pylint: disable=broad-except\r\n     66       filtered_tb = _process_traceback_frames(e.__traceback__)\r\n---> 67       raise e.with_traceback(filtered_tb) from None\r\n     68     finally:\r\n     69       del filtered_tb\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/save.py in _map_captures_to_created_tensors(original_captures, resource_map)\r\n    530           \"directly.\\n\\n Trackable Python objects referring to this tensor \"\r\n    531           \"(from gc.get_referrers, limited to two hops):\\n{}\".format(\"\\n\".join(\r\n--> 532               [repr(obj) for obj in trackable_referrers])))\r\n    533     export_captures.append(mapped_resource)\r\n    534   return export_captures\r\n\r\nAssertionError: Tried to export a function which references 'untracked' resource Tensor(\"308003:0\", shape=(), dtype=resource). TensorFlow objects (e.g. tf.Variable) captured by functions must be 'tracked' by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\r\n\r\n Trackable Python objects referring to this tensor (from gc.get_referrers, limited to two hops):\r\n<tensorflow.python.ops.lookup_ops.StaticHashTable object at 0x7f71763e1550>\r\n```\r\n\r\nHow do I fix this and save the model with properly?"]}, {"number": 42060, "title": "Fail to load model from ModelCheckpoint in java.", "body": "Fail to load model in java.\r\n\r\n**Python**\r\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \r\n                             save_best_only=True, mode='min', save_weights_only = True)\r\n**Java**\r\nSavedModelBundle model = SavedModelBundle.load(\"C:\\\\workspace\\\\tools\\\\cross-360p-high2Tp1Cl100VpConc.1.09.hdf5\", \"serve\" );\t\r\n\r\n**System information**\r\nlibtensorflow-2.3.0.jar\r\n\r\n\r\n**Other info / logs**\r\n2020-08-05 21:17:29.484400: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: C:\\workspace\\tools\\cross-360p-high2Tp1Cl100VpConc.1.09.hdf5\r\n2020-08-05 21:17:29.484980: I tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: fail. Took 554 microseconds.\r\nException in thread \"main\" org.tensorflow.TensorFlowException: Could not find SavedModel .pb or .pbtxt at supplied export directory path: C:\\workspace\\tools\\cross-360p-high2Tp1Cl100VpConc.1.09.hdf5\r\n\tat org.tensorflow.SavedModelBundle.load(Native Method)\r\n\tat org.tensorflow.SavedModelBundle.access$000(SavedModelBundle.java:27)\r\n\tat org.tensorflow.SavedModelBundle$Loader.load(SavedModelBundle.java:32)\r\n\tat org.tensorflow.SavedModelBundle.load(SavedModelBundle.java:95)\r\n\tat com.TensorFlowEngine.main(TensorFlowEngine.java:9)\r\n\r\n", "comments": ["@jimmy6\r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nCan you please share  complete code snippet to reproduce the issue in our environment.Thanks!", "This link show how ModelCheckpoint coded.\r\nhttps://github.com/jimmy6/j6stock-deeplearning/blob/master/Transformer_1D-CNN_Feature_Extraction/xau_usd_OHLCTail2Tp1Cl100VpConc0_4801Vl-TF2.2.ipynb\r\n\r\nModel is here https://drive.google.com/file/d/1K5aLnYzTPp_FTkAzxFp4M1pY9ZdK_xPX/view?usp=sharing", "From the stack trace it says `Could not find SavedModel .pb or .pbtxt at supplied export`.\r\nCan you confirm if you have those files in your export?\r\nThanks!", "It should be able to read h5 or .hdf5 right?", "You are required to have your model in SavedModel format.\r\nSee https://www.tensorflow.org/guide/saved_model#the_savedmodel_format_on_disk\r\nCurrently you are saving your model in `h5` whereas `SavedModelBundel.load()` is searching for an export directory containing `SavedModel.pb` file.\r\nSee https://www.tensorflow.org/api_docs/java/org/tensorflow/SavedModelBundle#public-static-savedmodelbundle-load-string-exportdir,-string...-tags", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42060\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42060\">No</a>\n"]}, {"number": 42059, "title": "[TFLite] Add int16x8 support for BATCH_MATMUL operator", "body": "Hi,\r\n\r\nThis PR adds int16x8 support for the BATCH_MATMUL operator in TensorFlow Lite.\r\n\r\nThibaut", "comments": ["@talumbau  I introduced a small error when fixing a previous merge conflict, I fixed it in the latest commit. The PR will need re-approval, sorry for that.", "I merged the latest changes from the master branch into the PR as there was an [error in the CI](https://source.cloud.google.com/results/invocations/87d483b1-e13e-434c-8fc3-f8417e716340/targets/%2F%2Ftensorflow%2Fcore:ops_tests/log) coming from a bug in the master that was fixed by 9fe68d5108d8e4aefd7907489cf649da81097516. \r\n\r\n@talumbau the PR will need re-approval, thanks!", "@gbaned It seems the copybara service indirectly merged the PR, should I just close the PR itself manually?", "@Tessil  I will close it. \r\nSeems auto-merge is not happening but the changes are now committed, so we can close this. Thank you for the PR."]}, {"number": 42058, "title": "Import Tensorflow giving error", "body": "Hi,\r\nI am trying to deploy a simple CNN on FLASK. on operating system Windows 10, \r\nPython 3.8.3\r\ntensor flow version is 2.3.0  (installed using pip install)\r\nkeras version 2.4.3 (installed using pip install)\r\n\r\nI uninstalled and reinstalled Anaconda also. \r\nUpgraded  Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019 from https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads\r\n\r\nMy system has CPU (no GPU), 4GB RAM, i5 processor\r\n\r\n\r\nFor the statement \"import tensorflow as tf\" i am getting an error.\r\n\r\nD:\\>python -c \"import tensorflow as tf\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n*******************************************************************************************************\r\n\r\n### If I use TensorFlow 2.2.0, I get the following error\r\n\r\nD:\\>python -c \"import tensorflow as tf\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\amala\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "Since you are using python v3, try installing tensorflow using pip3.\r\nSee this [link](https://www.tensorflow.org/install/pip) for installing tensorflow.\r\n", "@pratapmalani,\r\nYou might be facing this issue because of the following reasons\r\n\r\n- You you running 32-bit Python or 32-bit OS\r\n- Your CPU does not support AVX instructions, please provide the make and model of your CPU in this case.\r\n\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check these similar duplicate issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204.\r\n\r\nThanks!", "@ayush0x00 Thanks for your response. using pip3 instead of pip is giving the same error.\r\n\r\n@amahendrakar\r\nThanks for your response. \r\n\r\n**From System Requirement:**\r\n\r\npip3 version is 20.1.1\r\n\r\nWindows 10 Home Single Language\r\nVersion 1909\r\nOS build 18363.959\r\n\r\nI already installed installed MS Visual C++ (as mentioned in first message)\r\n\r\nI have also installed NVIDIA CUDA 10.2\r\n\r\n**Hardware Details:**\r\n\r\nHP Laptop 15-da1xxx\r\nProcessor: Intel(R) Core(TM) i5-8265U CPU @ 1.60Ghz 1.80GHz\r\nSystem type: 64-bit operating system, x64-based processor\r\n\r\nMy processor supports AVX ( https://en.wikichip.org/wiki/intel/core_i5/i5-8265u )\r\n\r\nPlease suggest if i am missing something here.\r\n", "> @pratapmalani,\r\n> You might be facing this issue because of the following reasons\r\n> \r\n> * You you running 32-bit Python or 32-bit OS\r\n> * Your CPU does not support AVX instructions, please provide the make and model of your CPU in this case.\r\n> \r\n> Please take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n> \r\n> Also, check these similar duplicate issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204.\r\n> \r\n> Thanks!\r\n\r\n\r\n\r\n@amahendrakar\r\nThanks for your response.\r\n\r\nFrom System Requirement:\r\n\r\npip3 version is 20.1.1\r\n\r\nWindows 10 Home Single Language\r\nVersion 1909\r\nOS build 18363.959\r\n\r\nI already installed installed MS Visual C++ (as mentioned in first message)\r\n\r\nI have also installed NVIDIA CUDA 10.2\r\n\r\nHardware Details:\r\n\r\nHP Laptop 15-da1xxx\r\nProcessor: Intel(R) Core(TM) i5-8265U CPU @ 1.60Ghz 1.80GHz\r\nSystem type: 64-bit operating system, x64-based processor\r\n\r\nMy processor supports AVX ( https://en.wikichip.org/wiki/intel/core_i5/i5-8265u )\r\n\r\nPlease suggest if i am missing something here.", "> Since you are using python v3, try installing tensorflow using pip3.\r\n> See this [link](https://www.tensorflow.org/install/pip) for installing tensorflow.\r\n\r\n@ayush0x00 Thanks for your response. using pip3 instead of pip is giving the same error.", "@pratapmalani,\r\nCould you please check if the Anaconda version is also 64 bit? Thanks!", "@amahendrakar \r\n\r\nI have used Anaconda3-2020.07-Windows-x86_64.exe to setup Anaconda.\r\n\r\npython version is 3.8.3 ( i think its installed with anaconda3)\r\n\r\nJupyter notebook is also opening well.\r\n\r\nimport tensorflow in jupyter notebook is also throwing same error.\r\n\r\n*****************************************************\r\n\r\nI also did \r\npip3 install tensorflow==2.2 --no-cache-dir\r\n\r\nso that i get fresh copy and not the one in cache, however, i got the same error.\r\n\r\n*****************************************************\r\n\r\nI tried conda install, and it gave me error. From the error I read that its issue with python version (but i can be wrong)\r\n\r\nC:\\>conda install tensorflow\r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: /\r\nThe environment is inconsistent, please check the package plan carefully\r\nThe following packages are causing the inconsistency:\r\n\r\n  - defaults/win-64::anaconda==2020.07=py38_0\r\n  - defaults/win-64::patsy==0.5.1=py38_0\r\n  - defaults/win-64::scikit-image==0.16.2=py38h47e9c7a_0\r\n  - defaults/win-64::scikit-learn==0.23.1=py38h25d0782_0\r\n  - defaults/noarch::seaborn==0.10.1=py_0\r\n  - defaults/win-64::statsmodels==0.11.1=py38he774522_0\r\nfailed with initial frozen solve. Retrying with flexible solve.\r\nSolving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\r\nCollecting package metadata (repodata.json): done\r\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\r\nSolving environment: -\r\nFound conflicts! Looking for incompatible packages.\r\nThis can take several minutes.  Press CTRL-C to abort.\r\n                                                                                                                       |failed\r\n\r\nUnsatisfiableError: The following specifications were found\r\nto be incompatible with the existing python installation in your environment:\r\n\r\nSpecifications:\r\n\r\n  - tensorflow -> python[version='3.5.*|3.6.*|3.7.*']\r\n\r\nYour python: python=3.8\r\n\r\nIf python is on the left-most side of the chain, that's the version you've asked for.\r\nWhen python appears to the right, that indicates that the thing on the left is somehow\r\nnot available for the python version you are constrained to. Note that conda will not\r\nchange your python version to a different minor version unless you explicitly specify\r\nthat.\r\n\r\n", "@pratapmalani \r\n\r\n> I have also installed NVIDIA CUDA 10.2\r\n\r\nI am not sure why you installed CUDA drivers as you don't have GPU. \r\n\r\nSome time back when i had issues with CPU and windows10 (but not Anaconda), I uninstalled all the TF versions and completely removed the folders related to Tensorflow and then reinstalled TF. Can you also try installing Python3.7/3.6 to check whether that resolves your issue.\r\n\r\nGenerally we don't support Anaconda based Installation issues as Anaconda maintain those builds. It is better if you post this issue in Anaconda repo, then you might get faster resolution so that you can start using TF.\r\n\r\nPlease let me know how it progresses. thanks!", "@jvishnuvardhan \r\n\r\nThanks for your help and guidance. Yes, i uninstalled TF and Anaconda, deleted all related folders and did a fresh installation. Now things are working well.\r\n\r\nThanks to @amahendrakar  @ayush0x00  for your help and guidance.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42058\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42058\">No</a>\n"]}, {"number": 42057, "title": "TensorFlow Lite causes segementation faults in C++ when creating threads if fully statically linked", "body": "**System information**\r\n- Linux Ubuntu 18.04\r\n- TensorFlow Binary (via `pip`)\r\n- Tested on 2.1.0, 2.2.0, 2.3.0 & all recent master branches of `tensorflow/tensorflow:devel` up to my last comment\r\n  - I have a local CI script tracking this issue\r\n- Tested on `x86_64` as well as ARM builds\r\n\r\n**Problem**\r\n- Segmentaion fault on `invoke()` when running any model that spawns threads \r\n  - Only occurs if TensorFlow Lite library has been statically linked\r\n  - Not an issue in older versions (pre-2.0.0)\r\n\r\n**Problem Reproduction**\r\n- Standard Tensorflow Lite library build\r\n  - Spin up `tensorflow/tensorflow:devel` docker container\r\n  - `cd tensorflow_src`\r\n  - `./tensorflow/lite/tools/make/download_dependencies.sh`\r\n  - `./tensorflow/lite/tools/make/build_lib.sh`\r\n  - Grab TensorFlow and Flatbuffer headers, as well as Tensorflow `.a` file\r\n- Compile [tflite c++ minimal example](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_c)\r\n  - GCC 7.5\r\n  - Static\r\n    - `g++ minimal.cc -I../tensorflow_headers -I../flatbuffers_include -Wl,--whole-archive -lpthread -Wl,--no-whole-archive -pthread -g -static -L../tensorflow_lib -ltensorflow-lite -ldl -o tflite_min_static`\r\n  - Dynamic\r\n    - `g++ minimal.cc -I../tensorflow_headers -I../flatbuffers_include -Wl,--whole-archive -lpthread -Wl,--no-whole-archive -pthread -g -L../tensorflow_lib -ltensorflow-lite -ldl -o tflite_min_dynamic`\r\n- Make some test models\r\n  - Convolutional model creates extra threads, non-convolutional model does not\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Conv2D, Flatten, Input, Dense\r\nfrom tensorflow.keras.models import Model\r\n\r\n### With a convolution (creates threads on invoke())\r\ninputs = Input(shape=[10, 5, 1])\r\nx = inputs\r\nx = Conv2D(32, (3, 3))(x)\r\nx = Flatten()(x)\r\nx = Dense(1)(x)\r\n\r\nmodel = Model(inputs, x)\r\nmodel.compile()\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n\r\n# Save the TF Lite model.\r\nwith tf.io.gfile.GFile(\"min_model_with_conv.tflite\", \"wb\") as f:\r\n    f.write(tflite_model)\r\n\r\n### Without convolution (no extra threading)\r\ninputs = Input(shape=[10, 5, 1])\r\nx = inputs\r\n# x = Conv2D(32, (3, 3))(x)\r\nx = Flatten()(x)\r\nx = Dense(1)(x)\r\n\r\nmodel = Model(inputs, x)\r\nmodel.compile()\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n\r\n# Save the TF Lite model.\r\nwith tf.io.gfile.GFile(\"min_model_no_conv.tflite\", \"wb\") as f:\r\n    f.write(tflite_model)\r\n\r\n```\r\n- Run minimal examples\r\n  - `tflite_min_dynamic min_model_no_conv.tflite`: pass\r\n  - `tflite_min_dynamic min_model_with_conv.tflite`: pass\r\n  - `tflite_min_static min_model_no_conv.tflite`: pass\r\n  - `tflite_min_static min_model_with_conv.tflite`: segmentation fault\r\n- From `gdb --args tflite_min_static min_model_with_conv.tflite`\r\n```\r\nNode   2 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 6 2 -1\r\n  Outputs: 7\r\n[New Thread 0x7fffff7b0700 (LWP 2331)]\r\n[New Thread 0x7ffffefa0700 (LWP 2332)]\r\n\r\nThread 2 \"tflite_min_stat\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7fffff7b0700 (LWP 2331)]\r\n0x0000000000000000 in ?? ()\r\n```\r\n\r\n**Bazel Reproduction**\r\nUsing the minimal example above, the problem can be reproduced under Bazel by adding the following build rule:\r\n```\r\ncc_binary(\r\n    name = \"tflite_minimal_bazel\",\r\n    srcs = [\"minimal.cc\"],\r\n    deps = [\r\n        \":framework\",\r\n        \"//tensorflow/lite/kernels:builtin_ops_all_linked\", # For kernels/register.h\r\n    ],\r\n    features = [\"fully_static_link\"], # For full static link of standard library\r\n    linkstatic=True,\r\n)\r\n```\r\n\r\n**Files for Reproduction**\r\nSee [here](https://www.dropbox.com/sh/0ztotvfm5l92aaz/AAAI4-TYwyuddKnb4S2uaKBja?dl=0). \r\n\r\n\r\n", "comments": ["@multiverse-tf can you take a look?", "Hi Adam,\r\n\r\nThx for giving detailed instructions to reproduce the issue! While I am going to take a look at this, I'm just wondering is this reproducible w/ the Bazel build toolchain? Considering you're using the tensorflow:devel docker to do the compilation, I think it's quite simple to use Bazel then.\r\n\r\nFor example, as for the minimal.cc program, you could simply put it under \"tensorflow/lite\" directory and add a BUILD rule like the following to get it compiled later.\r\ncc_binary(\r\n    name = \"tflite_minimal\",\r\n    srcs = [\"minimal.cc\"],\r\n    deps = [\r\n        \":framework\",\r\n        # OTHER DEPS...\r\n    ],\r\n    linkstatic=true,\r\n)\r\n\r\nThen, \"bazel build //tensorflow/lite:tflite_minimal\" cmd should get it compiled.\r\n", "Hey Chao, thanks for getting back to me. I'm not familiar with Bazel, but compiling with the suggested rule only created a static link for TensorFlow Lite itself and not the standard C/C++ library. This compilation is essentially the same as the dynamic case referenced in the issue. I tried adding `features = [\"fully_static_link\"]` for a full static build, but this generated compilation errors (largely undefined symbols); I've added this error and the successful compilation to the dropbox link.\r\n\r\nIt's interesting that `bazel` generates compilation errors while `g++` does not. My guess is the cause is something to do with the weak symbol linking for `pthread`; the standard solution to the symbol problem in `g++`: `-Wl,--whole-archive -lpthread -Wl,--no-whole-archive` (+/- `-lrt -pthread`) doesn't appear to work here.\r\n\r\nI had previously been compiling on the image `tensorflow:nightly-devel` referenced in the ARM build guide [here](https://www.tensorflow.org/lite/guide/build_arm64), but it appears to be quite outdated as it pulls `6798656ce64201afe7d8d9ed5445cd5114bfd9b4` which is from 2018 and `v1.12.1` - this may warrant its own issue. Using my original build process on that image (targetting an RPi rather than `x86_64`) allowed for a complete static compilation without the segmentation issue. That might imply that a change in how the threading is handled between then and now is the underlying cause.\r\n\r\nSmall change for successful bazel compilation (may not be the best/correct way to do the include):\r\n```\r\ncc_binary(\r\n    name = \"tflite_minimal_bazel\",\r\n    srcs = [\"minimal.cc\"],\r\n    deps = [\r\n        \":framework\",\r\n        \"//tensorflow/lite/kernels:builtin_ops_all_linked\", # For kernels/register.h\r\n    ],\r\n    # features = [\"fully_static_link\"], # For static link of standard library\r\n    linkstatic=True,\r\n)\r\n```", "@multiverse-tf did you get a chance to look into this?", "@Saduf2019 it might be relevant to add tags for versions 2.1 & 2.3 as I can confirm this issue is present in those releases. I don't believe I can add those myself.", "Thanks @mihaimaruseac. I just want to confirm that this is still an issue as of the newest `tensorflow/tensorflow:devel`. Any help would be greatly appreciated.", "Bumping since this is still a bug. Any updates @multiverse-tf? I have a CI script replicating the problem and I'm happy to help fix it, but I need some pointers on where to look and what the potential causes might be.", "Is anyone interested in trying to solve this?\r\n\r\n@multiverse-tf @advaitjain @mihaimaruseac @jdduke @ymodak @Saduf2019 ", "@Lif3line do you have a more detailed, symbolized stack trace (from a debug build) that we can use for reference?", "Unfortunately I have a very large queue of bugs so I won't be able to look at this before 2.4 release :(", "Fair enough @mihaimaruseac, I appreciate we're all busy, thanks for getting back to me though.\r\n\r\n@jdduke, I can get one although I'm afraid I don't know exactly how I'd do that. Presumably add `-g` to the Tensorflow library compilation then running through `gdb` again? Or do you have some expanded debug build via Bazel?", "> Presumably add -g to the Tensorflow library compilation then running through gdb again? Or do you have some expanded debug build via Bazel?\r\n\r\nYes, for the makefile build for the tensorflow-lite lib you'll want to include `-g`. Note that we're also working on CMake support, and we should probably also extend the minimal example to also export a CMake-based build variant. @terryheo FYI.\r\n", "Unfortunately I don't seem to get anything extra out: \r\n\r\n```\r\nTensor  75 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor  76 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (n[New Thread 0x7fa70f8a6700 (LWP 361)]\r\n[New Thread 0x7fa70f0a5700 (LWP 362)]\r\n[New Thread 0x7fa70e8a4700 (LWP 363)]\r\nThread 2 \"tflite_minimal\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7fa70f8a6700 (LWP 361)]\r\n0x0000000000000000 in ?? ()\r\n```\r\n\r\nI rebuilt Tensorflow with the following Make target copied into `/tensorflow_src/tensorflow/lite/tools/make/targets/linux_debug_makefile.inc`:\r\n```MAKEFILE\r\nifeq ($(TARGET), linux_debug)\r\n    CXXFLAGS += \\\r\n        -fPIC \\\r\n        -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK \\\r\n        -pthread \\\r\n        -g\r\n\r\n    CFLAGS += \\\r\n        -fPIC \\\r\n        -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK \\\r\n        -pthread \\\r\n        -g\r\n\r\n    LIBS += -ldl\r\n\r\nendif\r\n```\r\n\r\nThen compiled via `./tensorflow/lite/tools/make/build_lib.sh TARGET=linux_debug`. The rest was the same as the above with `gdb -ex run --args ${DEPLOY_SRC}/tflite_minimal_static/tflite_minimal adv_model.tflite` to see what was happening.", "Hi @Lif3line, I've forward this issue internally where we'll try to repro. There have certainly been changes over the past several years with threading that could account for the breakage. Thanks for your patience, I've added this to the hotlist for the next release.", "Great, thanks very much for that. I'll leave the code I used to reproduce available via [the link above](https://www.dropbox.com/sh/0ztotvfm5l92aaz/AAAI4-TYwyuddKnb4S2uaKBja?dl=0), let me know if there's anything else useful I can do to help.", "Just want to bump because this still exists in `2.4.1`. Same symptoms, still reproducible with the above.", "Hi @Lif3line,\r\n\r\nSorry for not looking into this for such a long time.\r\n\r\nI tried to reproduce the issue by following the \"Bazel Reproduction\" part using the tensorflow/tensorflow:devel docker image, but ended up w/ static linking errors as shown below :(\r\n\"\r\n...\r\n/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function\r\nexternal/flatbuffers/src/util.cpp:50: error: undefined reference to 'std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream(char const*, std::_Ios_Openmode)'\r\nexternal/flatbuffers/src/util.cpp:51: error: undefined reference to 'std::basic_ios<char, std::char_traits<char> >::good() const'\r\nexternal/flatbuffers/src/util.cpp:50: error: undefined reference to 'std::basic_ifstream<char, std::char_traits<char> >::~basic_ifstream()'\r\nexternal/flatbuffers/src/util.cpp:50: error: undefined reference to 'std::basic_ifstream<char, std::char_traits<char> >::~basic_ifstream()'\r\nexternal/flatbuffers/src/util.cpp:56: error: undefined reference to 'std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream(char const*, std::_Ios_Openmode)'\r\nexternal/flatbuffers/src/util.cpp:57: error: undefined reference to 'std::basic_ifstream<char, std::char_traits<char> >::is_open()'\r\nexternal/flatbuffers/src/util.cpp:60: error: undefined reference to 'std::istream::seekg(long, std::_Ios_Seekdir)'\r\nexternal/flatbuffers/src/util.cpp:61: error: undefined reference to 'std::istream::tellg()'\r\n...\r\n\"\r\n\r\nJust wondering does this happen to you? My docker image has the id as \"sha256:e6925c167c691d50a61fdf06f3495ccdd6612c8b3da29659e121da7e723459e3\" which was created on 2020-12-30 according to \"docker inspect <IMAGE ID>\". Just wondering whether the docker image could play a role here or not.", "No worries, I had issues (although not exactly the same error) when using `features = [\"fully_static_link\"]`, but I don't know enough about Bazel to debug. The fact that a full static build doesn't compile could well be the underlying issue though. \r\n\r\nThe particular bug you posted looks like the include for `<fstream>` is missing or not linked properly somewhere in the `flatbuffers` library, but I know even less about that.\r\n\r\nI have this issue running in a GitLab instance, if you have access to your own with CI time I could edit down a minimal example and share it? It directly uses the build script `/tensorflow/lite/tools/make/build_lib.sh` to build Tensorflow non-statically in the first instance then a CMAKE version of the above `g++` to create a static version of the minimal example.\r\n\r\n\r\n", "Just want to bump because this still exists in 2.5 (Can someone tag for 2.4 and 2.5?). Same symptoms, still reproducible with the above.\r\n\r\nFor building with `bazel` I'd guess this is issue is tied with `fully_static_link` not working at build time. That error can be replicated using the above example and adding the following at the bottom of the `bazel` script.\r\n```\r\ncc_binary(\r\n    name = \"tflite_minimal_bazel\",\r\n    srcs = [\"minimal.cc\"],\r\n    deps = [\r\n        \":framework\",\r\n        \"//tensorflow/lite/kernels:builtin_ops_all_linked\", # For kernels/register.h\r\n    ],\r\n    features = [\"fully_static_link\"], # For static link of standard library\r\n    linkstatic=True,\r\n)\r\n```", "@Lif3line Could you please try using latest TF v2.7.0 and let us know if it helps?Thanks!", "Looks fixed as of most recent `tensorflow/tensorflow:devel` image! Thanks for checking in^^\r\n\r\nMinimal example [here](https://gitlab.com/Lif3line/tensorflow_static_build_test).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42057\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42057\">No</a>\n"]}, {"number": 42056, "title": "android use XNNPACK happen error", "body": " - android 7\r\n - ssd_mobilenet_v2\r\n\r\n```\r\n    implementation 'org.tensorflow:tensorflow-lite:2.3.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-support:0.1.0-rc1'\r\n```\r\n\r\n\r\n```java\r\nInterpreter.Options options = new Interpreter.Options();\r\noptions.setUseXNNPACK(true);\r\noptions.setNumThreads(NUM_THREADS);\r\nNnApiDelegate delegate = new NnApiDelegate();\r\noptions.addDelegate(delegate);\r\ntflite = new Interpreter(file, options);\r\n```\r\n\r\nerror log:\r\n```\r\nFailed to apply XNNPACK delegate: ModifyGraphWithDelegate is disallowed when graph is immutable.\r\n```", "comments": ["@yeyupiaoling Can you please attach the full stacktrace? Thanks!", "@ymodak hi\uff0cthis is log\uff0cuse nnapi.\r\n\r\n![image](https://user-images.githubusercontent.com/26297768/89478269-1a1da880-d7c2-11ea-9604-09db1854d22e.png)\r\n", "If I close NNAPI, XNNPACK works fine", "@yeyupiaoling Is it working for you now? We fixed it on our end, should be there in the nightly builds.", "@srjoglekar246 Which version is it ?\r\n\r\n```\r\n    implementation 'org.tensorflow:tensorflow-lite:2.3.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-support:0.1.0-rc1'\r\n```", "You would probably need to use the 2.4 nightly. The fix landed after 2.3", "@yeyupiaoling \r\n\r\nCan you please try in TF nightly version and see if the issue still persists. Thanks!", "@srjoglekar246 @ravikyram There is no version of this. \r\nurl: https://bintray.com/google/tensorflow/tensorflow-lite\r\n![image](https://user-images.githubusercontent.com/26297768/93564480-5563e780-f9bc-11ea-88db-04df48210135.png)\r\n"]}, {"number": 42055, "title": "F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: a PTX JIT compilation failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda v10.0/cudnn-10.0-windows10-x64-v7.4.2.24\r\n- GPU model and memory:\r\n\r\n\r\n\r\nI installed tensorflow 2.2.0 in May 2020 and ran the tutorial program successfully. However, I come across this issue using the same tutorial program. I believe that I didn't change python interpreter, cuda, cudnn, and tensorflow.\r\n\r\n```python\r\nimport tensorflow as tf\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n# print('\\n\\n')\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10)\r\n])\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nmodel.compile(optimizer='adam',\r\n              loss=loss_fn,\r\n              metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, epochs=5)\r\nmodel.evaluate(x_test,  y_test, verbose=2)\r\n```\r\nOut:\r\n```\r\n2020-08-05 13:54:52.036310: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 13:55:07.270688: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-08-05 13:55:07.734233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-08-05 13:55:07.797735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 13:55:07.990395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-05 13:55:08.045563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-05 13:55:08.082869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-08-05 13:55:08.199070: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-05 13:55:08.241274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-05 13:55:08.470320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-05 13:55:08.554461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-08-05 13:55:08.566140: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow \r\nbinary was not compiled to use: AVX2\r\n2020-08-05 13:55:08.656817: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x172aaf17030 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-05 13:55:08.694556: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-05 13:55:08.755347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-08-05 13:55:08.951294: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 13:55:09.001277: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-05 13:55:09.056517: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-05 13:55:09.129672: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-08-05 13:55:09.220914: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-05 13:55:09.402285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-05 13:55:09.456762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-05 13:55:09.581936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-08-05 13:55:41.323465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-05 13:55:41.477075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-08-05 13:55:41.617702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-08-05 13:55:41.802239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1465 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-08-05 13:55:42.283031: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x172ccaf7de0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-05 13:55:42.752069: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce 940MX, Compute Capability 5.0\r\n2020-08-05 13:55:43.501455: F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: a PTX JIT compilation failed\r\n```\r\n\r\n\r\nCan someone tell me what happened?\r\n", "comments": ["@nbe000,\r\nI was able to run the code without any issues on both TF v2.2 and v2.3. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/36cec401e6e11a7760133f7ce1de7f22/42055.ipynb).\r\n\r\nCould you please try running the code in a virtual environment or with TF v2.3 and check if you are facing the same issue. Thanks!", "@amahendrakar \r\nAfter installing tensorflow 2.3.0, output of the program becomes:\r\n```\r\n2020-08-05 23:18:56.585083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 23:19:26.710262: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-08-05 23:19:27.612692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-08-05 23:19:27.677846: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 23:19:27.820312: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-05 23:19:27.921464: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-05 23:19:27.986783: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-05 23:19:28.234155: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-05 23:19:28.319137: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-05 23:19:28.505193: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-05 23:19:28.644111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-05 23:19:28.711473: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-05 23:19:29.031579: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2884bbe4ca0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-05 23:19:29.225754: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-05 23:19:29.437001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-08-05 23:19:29.572880: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 23:19:29.699556: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-05 23:19:29.796275: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-05 23:19:29.956181: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-05 23:19:30.070618: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-05 23:19:30.267701: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-05 23:19:30.362888: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-05 23:19:30.571472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-05 23:19:30.966740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-05 23:19:31.006886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-08-05 23:19:31.171530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-08-05 23:19:31.283643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1465 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-08-05 23:19:31.583541: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2884bbe55a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-05 23:19:31.709454: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce 940MX, Compute Capability 5.0\r\nTraceback (most recent call last):\r\n  File \"e:/\u6587\u4ef6\u5939/6/fun/debug.py\", line 7, in <module>\r\n    model = tf.keras.models.Sequential([\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 116, in __init__\r\n    super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 308, in __init__\r\n    self._init_batch_counters()\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 317, in _init_batch_counters\r\n    self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 262, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 244, in _variable_v2_call\r\n    return previous_getter(\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 237, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2633, in default_variable_creator_v2\r\n    return resource_variable_ops.ResourceVariable(\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 264, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1507, in __init__\r\n    self._init_from_args(\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1661, in _init_from_args\r\n    handle = eager_safe_variable_handle(\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 242, in eager_safe_variable_handle        \r\n    return _variable_handle_from_shape_and_dtype(\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 174, in _variable_handle_from_shape_and_dtype\r\n    gen_logging_ops._assert(  # pylint: disable=protected-access\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\", line 49, in _assert\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 6843, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse\r\n```\r\nThe program runs successfully if I disable gpu.", "@nbe000,\r\nPlease check [this comment](https://github.com/tensorflow/tensorflow/issues/38518#issuecomment-619538236) from a similar issue and let us know if it helps. Thanks!", "> @nbe000,\r\n> Please check [this comment](https://github.com/tensorflow/tensorflow/issues/38518#issuecomment-619538236) from a similar issue and let us know if it helps. Thanks!\r\n\r\nI tried that, I have same issue too and both of verions 2.2.0 and 2.3.0. But not seen open anything, i control on nvidia-smi but it's clean. I tried on virtual env but it's still same", "@amahendrakar \r\nI am having trouble with nvidia-smi. The NVSMI folder that nvidia-smi should be in is missing. Downgrading to tf 2.2.0 is also not helping.", "> @amahendrakar\r\n> I am having trouble with nvidia-smi. The NVSMI folder that nvidia-smi should be in is missing. Downgrading to tf 2.2.0 is also not helping.\r\n\r\nhttps://stackoverflow.com/a/59482351\r\n", "@mcagricaliskan \r\n\r\nThanks for your help. nvidia-smi shows no other running processes.\r\n\r\n> > @amahendrakar\r\n> > I am having trouble with nvidia-smi. The NVSMI folder that nvidia-smi should be in is missing. Downgrading to tf 2.2.0 is also not helping.\r\n> \r\n> https://stackoverflow.com/a/59482351\r\n\r\n", "@nbe000,\r\nCould you please check if you are facing the same error in a virtual environment as well?\r\n\r\nAlso if possible, check if a fresh/clean installation of the dependencies fixes the issue. Thanks! ", "I tried with a virtual environment after uninstall tensorflow and reinstalled it (with tf 2.2 and 2.3) and still have this issue.", "@amahendrakar \r\nI still have this issue after uninstalling and reinstalling CUDA 10.0.", "@nbe000 Looks like there is CUDA driver compatibility issue. I see some drivers are related to 10.1 (`cudart64_101.dll`) and most are related to CUDA10.0 (`cublas64_10.dll`, `cufft64_10.dll`). TF2.2, 2.3 pip binaries are built with CUDA10.1. I noticed that you had installed CUDA10.0. Please check the tested build configuration.\r\n\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow_gpu-2.3.0 | 3.5-3.8 | MSVC 2019 | Bazel 3.1.0 | 7.4 | 10.1\r\n\r\n\r\nI would suggest you to completely uninstall CUDA drivers, uninstall TF versions, manually check whether there are any remaining folders related to TF or CUDA and delete them if you find them, restart comp/laptop, then install correct CUDA drivers (10.1 for TF2.2 or 2.3), and finally install TF and test it.\r\n\r\nPlease let us know how it progresses. Thanks! \r\n", "@jvishnuvardhan \r\nThanks for your advice. Due to the hardware constraint, I cannot upgrade CUDA to 10.1. I installed python 3.7.9 and tensorflow 2.0.0, which should match with the requirements. The program ran successfully. Here is the output:\r\n```\r\n2020-08-21 21:53:51.218069: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow \r\nbinary was not compiled to use: AVX2\r\nTrain on 60000 samples\r\nEpoch 1/5\r\n60000/60000 [==============================] - 14s 239us/sample - loss: 0.2964 - accuracy: 0.9131\r\nEpoch 2/5\r\n60000/60000 [==============================] - 14s 239us/sample - loss: 0.1443 - accuracy: 0.9569\r\nEpoch 3/5\r\n60000/60000 [==============================] - 11s 183us/sample - loss: 0.1071 - accuracy: 0.9678\r\nEpoch 4/5\r\n60000/60000 [==============================] - 13s 224us/sample - loss: 0.0888 - accuracy: 0.9724\r\nEpoch 5/5\r\n60000/60000 [==============================] - 13s 212us/sample - loss: 0.0748 - accuracy: 0.9765\r\n10000/1 - 1s - loss: 0.0352 - accuracy: 0.9788\r\n```\r\nHowever, it seems that the GPU was not been used. Here is the result of analyzing GPU using nvidia-smi while running the program:\r\n```\r\nFri Aug 21 21:54:05 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 417.35       Driver Version: 417.35       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce 940MX      WDDM  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   44C    P8    N/A /  N/A |     37MiB /  2048MiB |      1%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nHow can I enable GPU? Thanks.", "@nbe000 Did you install tensorflow GPU or CPU? Did you try !pip install tensorflow-gpu==2.0? or !pip install tensorflow==2.0? \r\n\r\nYou can check whether GPU is visible or not and also you can define ops under `with tf.device`. Please check the gpu guide [here](https://www.tensorflow.org/guide/gpu).  Thanks!", "I used ```python37 -m pip install tensorflow==2.0.0``` to install tensorflow. After \r\n```python37 -m pip install tensorflow-gpu==2.0.0``` and \r\n```\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n# tf.debugging.set_log_device_placement(True)\r\nwith tf.device('/GPU:0'):\r\n...\r\n```\r\n, the output becomes \r\n```\r\n2020-08-22 08:36:33.293209: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2020-08-22 08:36:40.679367: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-08-22 08:36:41.221742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.189\r\npciBusID: 0000:01:00.0\r\n2020-08-22 08:36:41.423270: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2020-08-22 08:36:41.746958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\nNum GPUs Available:  1\r\n2020-08-22 08:36:41.950513: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow \r\nbinary was not compiled to use: AVX2\r\n2020-08-22 08:36:42.408260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.189\r\npciBusID: 0000:01:00.0\r\n2020-08-22 08:36:42.774389: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2020-08-22 08:36:42.991451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n```\r\nHowever, the program have stuck here for 11 minutes (still running until this comment).", "@nbe000 Please share the updated standalone that you have used above. Thanks!", "@jvishnuvardhan \r\nHere is my code:\r\n```python\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n# tf.debugging.set_log_device_placement(True)\r\nwith tf.device('/GPU:0'):\r\n  # tf.config.experimental.set_visible_devices([], 'GPU')\r\n  mnist = tf.keras.datasets.mnist\r\n  (x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n  x_train, x_test = x_train / 255.0, x_test / 255.0\r\n  # print('\\n\\n')\r\n  model = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n    tf.keras.layers.Dense(128, activation='relu'),\r\n    tf.keras.layers.Dropout(0.2),\r\n    tf.keras.layers.Dense(10)\r\n  ])\r\n  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n  model.compile(optimizer='adam',\r\n                loss=loss_fn,\r\n                metrics=['accuracy'])\r\n  model.fit(x_train, y_train, epochs=5)\r\n  model.evaluate(x_test,  y_test, verbose=2)\r\n```\r\nThe program completed after 20min or so for the first time. I tried again with the same code. The stuck issue did not appear again. However, the speed of gpu is much lower than cpu (22s/epoch vs 12s/epoch). Is this normal?\r\nThanks!", "@nbe000 I ran your codes on cpu and GPU and I don't see much difference with this small model (as GPU has some overhead when compared to cpu). With this simple model, GPU is slightly faster than cpu's. Please take a look at the [gist](https://colab.research.google.com/gist/jvishnuvardhan/f0635fb321d2a18b72061b69642c6610/untitled16.ipynb) with GPU. Note that I enabled `tf.debugging.set_log_device_placement(True)` and I can see ops on GPU.\r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/5ea107b1788833c0b44a336877ff6403/untitled17.ipynb) is gist with cpu. \r\n\r\nI have never noticed \"stuck issue\". Can you please try with little bigger model? Thanks!", "@jvishnuvardhan\r\nThanks for your advice. I am not able to reproduce the \"stuck issue\" after the first time. I ran a bigger model both on GPU and CPU. GPU outperformed CPU when the model was big enough. Thanks!", "Hey! Any solution yet? I ran into the same issue now.....", "@hliu549 Can you please open a new issue with a simple standalone code to reproduce the issue? Thanks"]}, {"number": 42054, "title": "How to check whether MKL is supported or not  with the installed tensorflow", "body": "**System information**\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from : binary\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7.0\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nI installed tensorflow with mkl support following [this guide](https://software.intel.com/content/www/us/en/develop/articles/intel-optimization-for-tensorflow-installation-guide.html). Now I want to check whether mkl is properly installed or not. I tried the following as suggested by Intel, but encountered no matching attribute error.\r\n>>>import tensorflow\r\n>>> print(tensorflow.pywrap_tensorflow.IsMklEnabled())\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'pywrap_tensorflow'\r\n\r\nIs there any other way to do the same?\r\n\r\nI installed tensorflow with the following cmd:\r\n` pip install tensorflow-mkl`", "comments": ["Can you try with:\r\n`python -c 'from tensorflow.python import _pywrap_util_port; print(_pywrap_util_port.IsMklEnabled())'`", "It works! Thanks @bhack \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42054\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42054\">No</a>\n"]}, {"number": 42053, "title": "All validation loss reported as 0.0000e+00 in keras / TF", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nThis is running on Google Colab. I am running macOS 10.15.6. Colab is opened in Chrome 84.0.4147.105 .\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nNot installed by me as it is running in Colab. \r\n- TensorFlow version (use command below):\r\nv2.3.0-0-gb36436b087 2.3.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: N/A although this also happens when using Tesla P100 on Colab\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nDuring training, val_loss is reported as 0.0000e+00.\r\nWhen printing model.history.history['val_loss'], an array of [0.0,..., 0.0] is printed.\r\nHowever, when evaluating the model on examples val_x, val_y, a non-zero loss is reported.\r\nThis happens for at least 4 different notebooks, including notebooks that worked properly earlier today.\r\n\r\n**Describe the expected behavior**\r\n\r\nDuring training, a non-zero validation loss should be reported in most cases.\r\nThese non-zero losses should be reported in the model history.\r\nWhen evaluating the model against the validation examples val_x, val_y, a loss similar to that reported in the model history should be reported.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/1YP_dR7O9KsqSeLyrgFTzOSna0EggL9Gs?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n", "comments": ["I have tried in colab with TF version 2.2,2.3 and was able to reproduce the issue.Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/0dc65503474d05f733976aff98c25a48/untitled217.ipynb).\r\nHowever i am seeing the below error message with [TF nightly version](https://colab.research.google.com/gist/ravikyram/9f41ca81a451c2a069f219d200eb729a/untitled218.ipynb)(`2.4.0-dev20200804`).\r\n`ValueError: Layer functional_1 expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(10, 1, 10) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(10, 1, 1) dtype=float32>]`\r\nThanks!", "@becker929 Thanks for creating this issue. \r\nRoot-cause of this issue is that the `validation_data` was provided as a list where as  `validation_data` needs to be a tuple. Please check the argument of `modell.fit`[here](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\r\n\r\n> tuple (x_val, y_val) of Numpy arrays or tensors\r\n> tuple (x_val, y_val, val_sample_weights) of Numpy arrays\r\n\r\nI updated `validation_data` argument within the `model.fit` from `validation_data=[val_x, val_y]` to `validation_data=(val_x, val_y)` . There was no error after this modification. \r\n\r\nPlease check the gist [here](https://colab.research.google.com/gist/jvishnuvardhan/d551d006d57ce271e8c3d396b2d7f439/untitled218.ipynb).\r\n\r\nPlease verify once and close the issue if this resolved your issue. Thanks!\r\n", "Hi, thanks! Indeed, after trying this with some other train/val data, this does solve the problem. I did not have to install the nightly build to solve the problem. I updated the notebook in the issue description demonstrating the change. Thanks again.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42053\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42053\">No</a>\n", "FYI, completely not the same reason, but I got the exact same problem when I failed to specify a loss function (in tensorflow 2.3.0) during the call to .compile() on the model.", "@brethvoice Please open a new issue with a simple standalone code to reproduce the issue. thanks!", "> @brethvoice Please open a new issue with a simple standalone code to reproduce the issue. thanks!\r\n\r\nActually it was my failure to read the API that caused the issue.  This is not something Tensorflow can or should \"fix.\"", "Hello I am also having a similar issue. The validation loss is always 0 even though i use keras and not tf.keras. But i have values for validation accuracy.\r\nI am using tf-version 2.1.0 and keras 2.3.1.\r\nCan anyone help me with this?", "> Hello I am also having a similar issue. The validation loss is always 0 even though i use keras and not tf.keras. But i have values for validation accuracy.\r\n> I am using tf-version 2.1.0 and keras 2.3.1.\r\n> Can anyone help me with this?\r\n\r\nCheck your .compile() call and make sure you have a loss function specified there.", "> > Hello I am also having a similar issue. The validation loss is always 0 even though i use keras and not tf.keras. But i have values for validation accuracy.\r\n> > I am using tf-version 2.1.0 and keras 2.3.1.\r\n> > Can anyone help me with this?\r\n> \r\n> Check your .compile() call and make sure you have a loss function specified there.\r\n\r\nI have a loss function specified but still i have the issue.", "> > > Hello I am also having a similar issue. The validation loss is always 0 even though i use keras and not tf.keras. But i have values for validation accuracy.\r\n> > > I am using tf-version 2.1.0 and keras 2.3.1.\r\n> > > Can anyone help me with this?\r\n> > \r\n> > \r\n> > Check your .compile() call and make sure you have a loss function specified there.\r\n> \r\n> I have a loss function specified but still i have the issue.\r\n\r\n@ArpithaFAU another thing I would check is that your validation data are being presented in a tuple (x, y).  If they are not then the validation calculation will not be possible.", "> > > > Hello I am also having a similar issue. The validation loss is always 0 even though i use keras and not tf.keras. But i have values for validation accuracy.\r\n> > > > I am using tf-version 2.1.0 and keras 2.3.1.\r\n> > > > Can anyone help me with this?\r\n> > > \r\n> > > \r\n> > > Check your .compile() call and make sure you have a loss function specified there.\r\n> > \r\n> > \r\n> > I have a loss function specified but still i have the issue.\r\n> \r\n> @ArpithaFAU another thing I would check is that your validation data are being presented in a tuple (x, y). If they are not then the validation calculation will not be possible.\r\n\r\n@brethvoice i have done that as well but i still have the same issue. If you are aware of any other solution pls let me know.", "I  am facing the same issue.\r\nWIth tf.keras I used validation_data as a  tuple to get\r\n /lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 630, in_is_list_of_scalars\r\n return ListsOfScalarsDataAdapter._is_list_of_scalars(inp[0])\r\nIndexError: list index out of range\r\n\r\nWith keras and on using validation data as a tuple I get the same error\r\n\r\nWith keras and on using validation data as a list I get the 0 validation loss.\r\n\r\nWould greatly appreciate any help.", "What i tried is upgrading to tensorflow and keras to the latest version and\nthat fixed my problem.\n\nOn Wed, Jan 20, 2021 at 9:16 PM Nirav Diwan <notifications@github.com>\nwrote:\n\n> I am facing the same issue.\n> WIth tf.keras I used validation_data as a tuple to get\n> /lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\",\n> line 630, in_is_list_of_scalars\n> return ListsOfScalarsDataAdapter._is_list_of_scalars(inp[0])\n> IndexError: list index out of range\n>\n> With keras and on using validation data as a tuple I get the same error\n>\n> With keras and on using validation data as a list I get the 0 validation\n> loss.\n>\n> Would greatly appreciate any help.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/42053#issuecomment-763905683>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIFKDILDLNFNZVFWV6ROGU3S2422PANCNFSM4PVCRLKQ>\n> .\n>\n"]}, {"number": 42052, "title": "Tensorflow-gpu status: Internal: no kernel image is available for execution on the device", "body": "**I run my code on tensorflow 2.1.0 Anaconda with CUDA Toolkit 10.1 CUDNN 7.6.0 (Windows 10) and it returns a issue**\r\n` F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: no kernel image is available for execution on the device`\r\n**My GPU : GT940MX Compute Capability 5.0**\r\n\r\n**I already run the nvcc -V and it returns :** \r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Fri_Feb__8_19:08:26_Pacific_Standard_Time_2019\r\nCuda compilation tools, release 10.1, V10.1.105\r\n\r\n**This is the full result :**\r\n```\r\n2020-08-05 10:05:48.368012: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 10:06:00.488544: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-08-05 10:06:48.153611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\ncoreClock: 0.8605GHz coreCount: 4 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-08-05 10:06:48.164731: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 10:06:48.245826: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-05 10:06:48.296245: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-05 10:06:48.338860: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-05 10:06:48.439393: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-05 10:06:48.489830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-05 10:06:48.941872: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-05 10:06:48.946651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-05 10:06:48.951881: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-05 10:06:48.979077: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23d29b660d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-05 10:06:48.985680: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-05 10:06:48.990616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\ncoreClock: 0.8605GHz coreCount: 4 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-08-05 10:06:49.003356: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 10:06:49.009869: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-05 10:06:49.014858: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-05 10:06:49.020699: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-05 10:06:49.028876: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-05 10:06:49.033607: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-05 10:06:49.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-05 10:06:49.045288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-05 10:06:49.218497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-05 10:06:49.223536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0\r\n2020-08-05 10:06:49.226857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n2020-08-05 10:06:49.230413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1460 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-08-05 10:06:49.244107: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23d301b8fa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-05 10:06:49.250377: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce 940MX, Compute Capability 5.0\r\n2020-08-05 10:06:49.446601: F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: no kernel image is available for execution on the device\r\n```\r\n**What are the issues and how to fix it?**", "comments": ["@MichaelJamesL,\r\nCould you please provide the complete code and the exact sequence of commands / steps that you executed before running into the error. Thanks!", "> \r\n> \r\n> @MichaelJamesL,\r\n> Could you please provide the complete code and the exact sequence of commands / steps that you executed before running into the error. Thanks!\r\n\r\n```\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\n#from yolov3.yolov3 import Create_Yolov3\r\nfrom yolov3.yolov4 import Create_Yolo\r\nfrom yolov3.utils import load_yolo_weights, detect_image, detect_video, detect_realtime\r\nfrom yolov3.configs import *\r\n\r\nif YOLO_TYPE == \"yolov4\":\r\n    Darknet_weights = YOLO_V4_TINY_WEIGHTS if TRAIN_YOLO_TINY else YOLO_V4_WEIGHTS\r\nif YOLO_TYPE == \"yolov3\":\r\n    Darknet_weights = YOLO_V3_TINY_WEIGHTS if TRAIN_YOLO_TINY else YOLO_V3_WEIGHTS\r\n\r\nimage_path   = \"./IMAGES/kite.jpg\"\r\nvideo_path   = \"./IMAGES/test.mp4\"\r\n\r\nyolo = Create_Yolo(input_size=YOLO_INPUT_SIZE)\r\nload_yolo_weights(yolo, Darknet_weights) # use Darknet weights\r\n\r\n#detect_image(yolo, image_path, '', input_size=YOLO_INPUT_SIZE, show=True, rectangle_colors=(255,0,0))\r\ndetect_video(yolo, video_path, '', input_size=YOLO_INPUT_SIZE, show=True, rectangle_colors=(255,0,0))\r\n#detect_realtime(yolo, '', input_size=YOLO_INPUT_SIZE, show=True, rectangle_colors=(255, 0, 0))\r\n```", "> \r\n> \r\n> **I run my code on tensorflow 2.1.0 Anaconda with CUDA Toolkit 10.1 CUDNN 7.6.0 (Windows 10) and it returns a issue**\r\n> ` F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: no kernel image is available for execution on the device`\r\n> **My GPU : GT940MX Compute Capability 5.0**\r\n> \r\n> **I already run the nvcc -V and it returns :**\r\n> nvcc: NVIDIA (R) Cuda compiler driver\r\n> Copyright (c) 2005-2019 NVIDIA Corporation\r\n> Built on Fri_Feb__8_19:08:26_Pacific_Standard_Time_2019\r\n> Cuda compilation tools, release 10.1, V10.1.105\r\n> \r\n> **This is the full result :**\r\n> \r\n> ```\r\n> 2020-08-05 10:05:48.368012: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-08-05 10:06:00.488544: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> 2020-08-05 10:06:48.153611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\n> coreClock: 0.8605GHz coreCount: 4 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n> 2020-08-05 10:06:48.164731: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-08-05 10:06:48.245826: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-08-05 10:06:48.296245: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-08-05 10:06:48.338860: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-08-05 10:06:48.439393: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-08-05 10:06:48.489830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-08-05 10:06:48.941872: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-08-05 10:06:48.946651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-08-05 10:06:48.951881: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2020-08-05 10:06:48.979077: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23d29b660d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-08-05 10:06:48.985680: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n> 2020-08-05 10:06:48.990616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\n> coreClock: 0.8605GHz coreCount: 4 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n> 2020-08-05 10:06:49.003356: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-08-05 10:06:49.009869: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-08-05 10:06:49.014858: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-08-05 10:06:49.020699: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-08-05 10:06:49.028876: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-08-05 10:06:49.033607: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-08-05 10:06:49.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-08-05 10:06:49.045288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-08-05 10:06:49.218497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-08-05 10:06:49.223536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0\r\n> 2020-08-05 10:06:49.226857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n> 2020-08-05 10:06:49.230413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1460 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n> 2020-08-05 10:06:49.244107: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23d301b8fa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> 2020-08-05 10:06:49.250377: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce 940MX, Compute Capability 5.0\r\n> 2020-08-05 10:06:49.446601: F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: no kernel image is available for execution on the device\r\n> ```\r\n> \r\n> **What are the issues and how to fix it?**\r\n\r\nI tried it again with Tensorflow-gpu 2.3 (Pip) on Anaconda Env. Same CUDA Toolkit and CUDNN version, and it return the same issue", "@MichaelJamesL,\r\nCould you please run the below code snippet and share the output with us.\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n```\r\n\r\nThanks!", "> \r\n> \r\n> @MichaelJamesL,\r\n> Could you please run the below code snippet and share the output with us.\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n> ```\r\n> \r\n> Thanks!\r\n\r\nThis is the result\r\n```\r\n>>> import tensorflow as tf\r\n2020-08-06 22:27:36.533164: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n>>> print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n2020-08-06 22:28:06.605620: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-08-06 22:28:06.646080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\ncoreClock: 0.8605GHz coreCount: 4 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-08-06 22:28:06.657210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-06 22:28:06.863965: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-06 22:28:06.983764: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-06 22:28:07.084227: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-06 22:28:07.189598: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-06 22:28:07.369685: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-06 22:28:07.495038: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-06 22:28:07.500783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\nNum GPUs Available:  1\r\n```", "hello \r\nI have same error once I run your code \r\n** import tensorflow as tf\r\n>>> print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\nI got \r\n\r\nNum GPUs Available:  1\r\n\r\nI was good before but after I upgrade tensorflow version it happened \r\n\r\n", "@tariqghd @MichaelJamesL Which python version you had installed? Thanks!\r\n\r\nLooks like this is an issue with Python 3.8 as mentioned in [this issue](https://github.com/tensorflow/tensorflow/issues/41304). \r\n\r\nIf you have Python3.8, can you please try with other versions 3.6, 3.7 and let us know whether you notice same issue with other python versions. Thanks!", "version 3.7 64 windows @jvishnuvardhan \r\n", "@tariqghd Are you also using Anaconda Env? Thanks for quick response", "@jvishnuvardhan  I'm using microsoft visual studio    Microsoft Visual Studio Enterprise 2019\r\nVersion 16.5.4\r\n", "> **I run my code on tensorflow 2.1.0 Anaconda with CUDA Toolkit 10.1 CUDNN 7.6.0 (Windows 10) and it returns a issue**\r\n> ` F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: no kernel image is available for execution on the device`\r\n> **My GPU : GT940MX Compute Capability 5.0**\r\n> \r\n> **I already run the nvcc -V and it returns :**\r\n> nvcc: NVIDIA (R) Cuda compiler driver\r\n> Copyright (c) 2005-2019 NVIDIA Corporation\r\n> Built on Fri_Feb__8_19:08:26_Pacific_Standard_Time_2019\r\n> Cuda compilation tools, release 10.1, V10.1.105\r\n> \r\n> **This is the full result :**\r\n> \r\n> ```\r\n> 2020-08-05 10:05:48.368012: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-08-05 10:06:00.488544: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> 2020-08-05 10:06:48.153611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\n> coreClock: 0.8605GHz coreCount: 4 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n> 2020-08-05 10:06:48.164731: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-08-05 10:06:48.245826: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-08-05 10:06:48.296245: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-08-05 10:06:48.338860: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-08-05 10:06:48.439393: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-08-05 10:06:48.489830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-08-05 10:06:48.941872: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-08-05 10:06:48.946651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-08-05 10:06:48.951881: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2020-08-05 10:06:48.979077: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23d29b660d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-08-05 10:06:48.985680: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n> 2020-08-05 10:06:48.990616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\n> coreClock: 0.8605GHz coreCount: 4 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n> 2020-08-05 10:06:49.003356: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-08-05 10:06:49.009869: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-08-05 10:06:49.014858: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-08-05 10:06:49.020699: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-08-05 10:06:49.028876: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-08-05 10:06:49.033607: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-08-05 10:06:49.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-08-05 10:06:49.045288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-08-05 10:06:49.218497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-08-05 10:06:49.223536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0\r\n> 2020-08-05 10:06:49.226857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n> 2020-08-05 10:06:49.230413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1460 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n> 2020-08-05 10:06:49.244107: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23d301b8fa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> 2020-08-05 10:06:49.250377: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce 940MX, Compute Capability 5.0\r\n> 2020-08-05 10:06:49.446601: F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: no kernel image is available for execution on the device\r\n> ```\r\n> \r\n> **What are the issues and how to fix it?**\r\n\r\nI think maybe is computeCapability need at least 7.5.", "\r\n\r\n\r\n\r\n> \r\n> \r\n> @tariqghd @MichaelJamesL Which python version you had installed? Thanks!\r\n> \r\n> Looks like this is an issue with Python 3.8 as mentioned in [this issue](https://github.com/tensorflow/tensorflow/issues/41304).\r\n> \r\n> If you have Python3.8, can you please try with other versions 3.6, 3.7 and let us know whether you notice same issue with other python versions. Thanks!\r\n\r\nYeah, it works on me. Thanks a lot for the answer @jvishnuvardhan !\r\nNote : I tried the tensorflow 2.3.0 with python 3.7, but it returns an error with python 3.7 because python3.8.dll (I don't remember exactly the error and i already delete the env), anyway i used python 3.7 on anaconda env and installed tensorflow 2.1.0 again with pip and it works.", "This issue is still there for me with the latest Tensorflow version (2.3.0). The error that I'm getting is almost identical to the error message given in this issue.\r\n\r\nI tried the following cases (All with official python release with `virtualenv` and `pip`)\r\n\r\nPython 3.8.5, Tensorflow 2.3.0 - Not working\r\nPython 3.7.8, Tensorflow 2.3.0 - Not working\r\nPython 3.7.8, Tensorflow 2.1.1 - **Working**\r\n\r\nMy GPU: NVidia GTX 950M, CC: 5.0\r\nCUDA Toolkit 10.1 - Update 2 with CUDNN 7.6.5\r\nOS: Windows 10 Home, the latest update\r\n", "@Ramesh-X,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "@Ramesh-X,\r\nTensorflow 2.1.1 is not in pypi, are you installing from source?\r\n\r\nedit: actually it's available for python < 3.8", "I also have same problem with \r\n\r\n- tensorflow 2.3.1\r\n- cuDNN 7.6\r\n- CUDA 10.1 \r\n\r\n`coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 29.80GiB/s\r\n2021-01-08 09:49:00.163149: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2021-01-08 09:49:00.163687: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2021-01-08 09:49:00.164099: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-08 09:49:00.164724: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2021-01-08 09:49:00.165614: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-08 09:49:00.166978: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2021-01-08 09:49:00.168377: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2021-01-08 09:49:00.170017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2021-01-08 09:49:00.434464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-08 09:49:00.434984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2021-01-08 09:49:00.435406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2021-01-08 09:49:00.457239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1464 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2021-01-08 09:49:00.472651: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x212d874fda0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-01-08 09:49:00.473668: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce 940MX, Compute Capability 5.0\r\n2021-01-08 09:49:00.498994: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-01-08 09:49:00.506147: F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: no kernel image is available for execution on the device\r\n\r\nProcess finished with exit code -1073740791 (0xC0000409)\r\n`", "@MichaelJamesL Did you found any solution.\r\n", "@AnushangaWimalasena,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 42051, "title": "keras meet a problem with sync bn in multi worker", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):N/A\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\nTensorFlow installed from (source or binary): Binary\r\nTensorFlow version (use command below): 2.3.0\r\nPython version: 3.7.4\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\n\r\nI just replace the keras bn layer in office resnet.py with tf.keras.layers.experimental.SyncBatchNormalization.\r\nI find in nccl multi worker mode, the model.fit training process will be stuck without error information, and will not begin the training.\r\nSame code work in ring mode well.\r\nI test the code with 2/~4 hosts, each has 2/~4 gpus. didn't work for all the situations.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@ImMrMa \r\nPlease provide simple stand alone code for us to replicate the issue faced or if possible share a colab gist with the error reported.", "```strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=getattr(tf.distribute.experimental.CollectiveCommunication,'NCCL'))  #RING WORK\r\nwith strategy.scope():\r\n    \r\n    inputs = keras.Input(shape=(256,256,3), name='digits')\r\n    output1=applications.ResNet50()(inputs)\r\n    # ,normalizer=\"l2\",normalize_args={'axis':1})\r\n    outputs = layers.Dense(1, activation='relu', name='dense_1')(output1)\r\n    model = keras.Model(inputs=inputs, outputs=outputs)\r\n    # inputs = keras.Input(shape=(784,), name='digits')\r\n    # num_units = 4096\r\n    # dense1 = layers.Dense(num_units, activation='relu', name='dense_1')\r\n    # x = dense1(inputs)\r\n    # x=tf.keras.layers.experimental.SyncBatchNormalization()(x)\r\n    # dense2 = layers.Dense(num_units, activation='relu', name='dense_2')\r\n    # x = dense2(x)\r\n    # outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\r\n    # model = keras.Model(inputs=inputs, outputs=outputs)\r\n    model.compile(loss='sparse_categorical_crossentropy',\r\n                optimizer=keras.optimizers.RMSprop(),\r\n                metrics=['accuracy'])\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\nx_train=np.zeros((2048,64,64,3),dtype=np.float32)\r\nx_test = x_train[:2048]\r\ny_train=y_train[:2048]\r\ny_test=y_test[:2048]\r\nhistory = model.fit(x_train, y_train,\r\n                    batch_size=1024,\r\n                    epochs=5,\r\n                    validation_split=0.2)\r\ntest_scores = model.evaluate(x_test, y_test, verbose=1)\r\nprint('Test loss:', test_scores[0])\r\nprint('Test accuracy:', test_scores[1])\r\n```\r\n\r\nthe training code\r\n\r\nhere is the custom resnet:\r\n\r\n```# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n# pylint: disable=invalid-name\r\n\"\"\"ResNet models for Keras.\r\n\r\nReference:\r\n  - [Deep Residual Learning for Image Recognition](\r\n      https://arxiv.org/abs/1512.03385) (CVPR 2015)\r\n\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom tensorflow.python.keras import backend\r\nfrom tensorflow.python.keras.applications import imagenet_utils\r\nfrom tensorflow.python.keras.engine import training\r\nfrom tensorflow.python.keras.layers import VersionAwareLayers\r\nfrom tensorflow.python.keras.utils import data_utils\r\nfrom tensorflow.python.keras.utils import layer_utils\r\nfrom tensorflow.python.lib.io import file_io\r\nfrom tensorflow.python.util.tf_export import keras_export\r\nfrom tensorflow.keras.layers.experimental import SyncBatchNormalization as BatchNormalization\r\n\r\nBASE_WEIGHTS_PATH = (\r\n    'https://storage.googleapis.com/tensorflow/keras-applications/resnet/')\r\nWEIGHTS_HASHES = {\r\n    'resnet50': ('2cb95161c43110f7111970584f804107',\r\n                 '4d473c1dd8becc155b73f8504c6f6626'),\r\n    'resnet101': ('f1aeb4b969a6efcfb50fad2f0c20cfc5',\r\n                  '88cf7a10940856eca736dc7b7e228a21'),\r\n    'resnet152': ('100835be76be38e30d865e96f2aaae62',\r\n                  'ee4c566cf9a93f14d82f913c2dc6dd0c'),\r\n    'resnet50v2': ('3ef43a0b657b3be2300d5770ece849e0',\r\n                   'fac2f116257151a9d068a22e544a4917'),\r\n    'resnet101v2': ('6343647c601c52e1368623803854d971',\r\n                    'c0ed64b8031c3730f411d2eb4eea35b5'),\r\n    'resnet152v2': ('a49b44d1979771252814e80f8ec446f9',\r\n                    'ed17cf2e0169df9d443503ef94b23b33'),\r\n    'resnext50': ('67a5b30d522ed92f75a1f16eef299d1a',\r\n                  '62527c363bdd9ec598bed41947b379fc'),\r\n    'resnext101':\r\n        ('34fb605428fcc7aa4d62f44404c11509', '0f678c91647380debd923963594981b3')\r\n}\r\n\r\nlayers = None\r\n\r\n\r\ndef ResNet(stack_fn,\r\n           preact,\r\n           use_bias,\r\n           model_name='resnet',\r\n           include_top=True,\r\n           weights='imagenet',\r\n           input_tensor=None,\r\n           input_shape=None,\r\n           pooling=None,\r\n           classes=1000,\r\n           classifier_activation='softmax',\r\n           **kwargs):\r\n  \"\"\"Instantiates the ResNet, ResNetV2, and ResNeXt architecture.\r\n\r\n  Reference:\r\n  - [Deep Residual Learning for Image Recognition](\r\n      https://arxiv.org/abs/1512.03385) (CVPR 2015)\r\n\r\n  Optionally loads weights pre-trained on ImageNet.\r\n  Note that the data format convention used by the model is\r\n  the one specified in your Keras config at `~/.keras/keras.json`.\r\n\r\n  Caution: Be sure to properly pre-process your inputs to the application.\r\n  Please see `applications.resnet.preprocess_input` for an example.\r\n\r\n  Arguments:\r\n    stack_fn: a function that returns output tensor for the\r\n      stacked residual blocks.\r\n    preact: whether to use pre-activation or not\r\n      (True for ResNetV2, False for ResNet and ResNeXt).\r\n    use_bias: whether to use biases for convolutional layers or not\r\n      (True for ResNet and ResNetV2, False for ResNeXt).\r\n    model_name: string, model name.\r\n    include_top: whether to include the fully-connected\r\n      layer at the top of the network.\r\n    weights: one of `None` (random initialization),\r\n      'imagenet' (pre-training on ImageNet),\r\n      or the path to the weights file to be loaded.\r\n    input_tensor: optional Keras tensor\r\n      (i.e. output of `layers.Input()`)\r\n      to use as image input for the model.\r\n    input_shape: optional shape tuple, only to be specified\r\n      if `include_top` is False (otherwise the input shape\r\n      has to be `(224, 224, 3)` (with `channels_last` data format)\r\n      or `(3, 224, 224)` (with `channels_first` data format).\r\n      It should have exactly 3 inputs channels.\r\n    pooling: optional pooling mode for feature extraction\r\n      when `include_top` is `False`.\r\n      - `None` means that the output of the model will be\r\n          the 4D tensor output of the\r\n          last convolutional layer.\r\n      - `avg` means that global average pooling\r\n          will be applied to the output of the\r\n          last convolutional layer, and thus\r\n          the output of the model will be a 2D tensor.\r\n      - `max` means that global max pooling will\r\n          be applied.\r\n    classes: optional number of classes to classify images\r\n      into, only to be specified if `include_top` is True, and\r\n      if no `weights` argument is specified.\r\n    classifier_activation: A `str` or callable. The activation function to use\r\n      on the \"top\" layer. Ignored unless `include_top=True`. Set\r\n      `classifier_activation=None` to return the logits of the \"top\" layer.\r\n    **kwargs: For backwards compatibility only.\r\n  Returns:\r\n    A `keras.Model` instance.\r\n\r\n  Raises:\r\n    ValueError: in case of invalid argument for `weights`,\r\n      or invalid input shape.\r\n    ValueError: if `classifier_activation` is not `softmax` or `None` when\r\n      using a pretrained top layer.\r\n  \"\"\"\r\n  global layers\r\n  if 'layers' in kwargs:\r\n    layers = kwargs.pop('layers')\r\n  else:\r\n    layers = VersionAwareLayers()\r\n  if kwargs:\r\n    raise ValueError('Unknown argument(s): %s' % (kwargs,))\r\n  if not (weights in {'imagenet', None} or file_io.file_exists(weights)):\r\n    raise ValueError('The `weights` argument should be either '\r\n                     '`None` (random initialization), `imagenet` '\r\n                     '(pre-training on ImageNet), '\r\n                     'or the path to the weights file to be loaded.')\r\n\r\n  if weights == 'imagenet' and include_top and classes != 1000:\r\n    raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\r\n                     ' as true, `classes` should be 1000')\r\n\r\n  # Determine proper input shape\r\n  input_shape = imagenet_utils.obtain_input_shape(\r\n      input_shape,\r\n      default_size=224,\r\n      min_size=32,\r\n      data_format=backend.image_data_format(),\r\n      require_flatten=include_top,\r\n      weights=weights)\r\n\r\n  if input_tensor is None:\r\n    img_input = layers.Input(shape=input_shape)\r\n  else:\r\n    if not backend.is_keras_tensor(input_tensor):\r\n      img_input = layers.Input(tensor=input_tensor, shape=input_shape)\r\n    else:\r\n      img_input = input_tensor\r\n\r\n  bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\r\n\r\n  x = layers.ZeroPadding2D(\r\n      padding=((3, 3), (3, 3)), name='conv1_pad')(img_input)\r\n  x = layers.Conv2D(64, 7, strides=2, use_bias=use_bias, name='conv1_conv')(x)\r\n\r\n  if not preact:\r\n    x = BatchNormalization(\r\n        axis=bn_axis, epsilon=1.001e-5, name='conv1_bn')(x)\r\n    x = layers.Activation('relu', name='conv1_relu')(x)\r\n\r\n  x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='pool1_pad')(x)\r\n  x = layers.MaxPooling2D(3, strides=2, name='pool1_pool')(x)\r\n\r\n  x = stack_fn(x)\r\n\r\n  if preact:\r\n    x = BatchNormalization(\r\n        axis=bn_axis, epsilon=1.001e-5, name='post_bn')(x)\r\n    x = layers.Activation('relu', name='post_relu')(x)\r\n\r\n  if include_top:\r\n    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\r\n    imagenet_utils.validate_activation(classifier_activation, weights)\r\n    x = layers.Dense(classes, activation=classifier_activation,\r\n                     name='predictions')(x)\r\n  else:\r\n    if pooling == 'avg':\r\n      x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\r\n    elif pooling == 'max':\r\n      x = layers.GlobalMaxPooling2D(name='max_pool')(x)\r\n\r\n  # Ensure that the model takes into account\r\n  # any potential predecessors of `input_tensor`.\r\n  if input_tensor is not None:\r\n    inputs = layer_utils.get_source_inputs(input_tensor)\r\n  else:\r\n    inputs = img_input\r\n\r\n  # Create model.\r\n  model = training.Model(inputs, x, name=model_name)\r\n\r\n  # Load weights.\r\n  if (weights == 'imagenet') and (model_name in WEIGHTS_HASHES):\r\n    if include_top:\r\n      file_name = model_name + '_weights_tf_dim_ordering_tf_kernels.h5'\r\n      file_hash = WEIGHTS_HASHES[model_name][0]\r\n    else:\r\n      file_name = model_name + '_weights_tf_dim_ordering_tf_kernels_notop.h5'\r\n      file_hash = WEIGHTS_HASHES[model_name][1]\r\n    weights_path = data_utils.get_file(\r\n        file_name,\r\n        BASE_WEIGHTS_PATH + file_name,\r\n        cache_subdir='models',\r\n        file_hash=file_hash)\r\n    model.load_weights(weights_path)\r\n  elif weights is not None:\r\n    model.load_weights(weights)\r\n\r\n  return model\r\n\r\n\r\ndef block1(x, filters, kernel_size=3, stride=1, conv_shortcut=True, name=None):\r\n  \"\"\"A residual block.\r\n\r\n  Arguments:\r\n    x: input tensor.\r\n    filters: integer, filters of the bottleneck layer.\r\n    kernel_size: default 3, kernel size of the bottleneck layer.\r\n    stride: default 1, stride of the first layer.\r\n    conv_shortcut: default True, use convolution shortcut if True,\r\n        otherwise identity shortcut.\r\n    name: string, block label.\r\n\r\n  Returns:\r\n    Output tensor for the residual block.\r\n  \"\"\"\r\n  bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\r\n\r\n  if conv_shortcut:\r\n    shortcut = layers.Conv2D(\r\n        4 * filters, 1, strides=stride, name=name + '_0_conv')(x)\r\n    shortcut = BatchNormalization(\r\n        axis=bn_axis, epsilon=1.001e-5, name=name + '_0_bn')(shortcut)\r\n  else:\r\n    shortcut = x\r\n\r\n  x = layers.Conv2D(filters, 1, strides=stride, name=name + '_1_conv')(x)\r\n  x = BatchNormalization(\r\n      axis=bn_axis, epsilon=1.001e-5, name=name + '_1_bn')(x)\r\n  x = layers.Activation('relu', name=name + '_1_relu')(x)\r\n\r\n  x = layers.Conv2D(\r\n      filters, kernel_size, padding='SAME', name=name + '_2_conv')(x)\r\n  x = BatchNormalization(\r\n      axis=bn_axis, epsilon=1.001e-5, name=name + '_2_bn')(x)\r\n  x = layers.Activation('relu', name=name + '_2_relu')(x)\r\n\r\n  x = layers.Conv2D(4 * filters, 1, name=name + '_3_conv')(x)\r\n  x = BatchNormalization(\r\n      axis=bn_axis, epsilon=1.001e-5, name=name + '_3_bn')(x)\r\n\r\n  x = layers.Add(name=name + '_add')([shortcut, x])\r\n  x = layers.Activation('relu', name=name + '_out')(x)\r\n  return x\r\n\r\n\r\ndef stack1(x, filters, blocks, stride1=2, name=None):\r\n  \"\"\"A set of stacked residual blocks.\r\n\r\n  Arguments:\r\n    x: input tensor.\r\n    filters: integer, filters of the bottleneck layer in a block.\r\n    blocks: integer, blocks in the stacked blocks.\r\n    stride1: default 2, stride of the first layer in the first block.\r\n    name: string, stack label.\r\n\r\n  Returns:\r\n    Output tensor for the stacked blocks.\r\n  \"\"\"\r\n  x = block1(x, filters, stride=stride1, name=name + '_block1')\r\n  for i in range(2, blocks + 1):\r\n    x = block1(x, filters, conv_shortcut=False, name=name + '_block' + str(i))\r\n  return x\r\n\r\n\r\ndef block2(x, filters, kernel_size=3, stride=1, conv_shortcut=False, name=None):\r\n  \"\"\"A residual block.\r\n\r\n  Arguments:\r\n      x: input tensor.\r\n      filters: integer, filters of the bottleneck layer.\r\n      kernel_size: default 3, kernel size of the bottleneck layer.\r\n      stride: default 1, stride of the first layer.\r\n      conv_shortcut: default False, use convolution shortcut if True,\r\n        otherwise identity shortcut.\r\n      name: string, block label.\r\n\r\n  Returns:\r\n    Output tensor for the residual block.\r\n  \"\"\"\r\n  bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\r\n\r\n  preact = BatchNormalization(\r\n      axis=bn_axis, epsilon=1.001e-5, name=name + '_preact_bn')(x)\r\n  preact = layers.Activation('relu', name=name + '_preact_relu')(preact)\r\n\r\n  if conv_shortcut:\r\n    shortcut = layers.Conv2D(\r\n        4 * filters, 1, strides=stride, name=name + '_0_conv')(preact)\r\n  else:\r\n    shortcut = layers.MaxPooling2D(1, strides=stride)(x) if stride > 1 else x\r\n\r\n  x = layers.Conv2D(\r\n      filters, 1, strides=1, use_bias=False, name=name + '_1_conv')(preact)\r\n  x = BatchNormalization(\r\n      axis=bn_axis, epsilon=1.001e-5, name=name + '_1_bn')(x)\r\n  x = layers.Activation('relu', name=name + '_1_relu')(x)\r\n\r\n  x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name=name + '_2_pad')(x)\r\n  x = layers.Conv2D(\r\n      filters,\r\n      kernel_size,\r\n      strides=stride,\r\n      use_bias=False,\r\n      name=name + '_2_conv')(x)\r\n  x = BatchNormalization(\r\n      axis=bn_axis, epsilon=1.001e-5, name=name + '_2_bn')(x)\r\n  x = layers.Activation('relu', name=name + '_2_relu')(x)\r\n\r\n  x = layers.Conv2D(4 * filters, 1, name=name + '_3_conv')(x)\r\n  x = layers.Add(name=name + '_out')([shortcut, x])\r\n  return x\r\n\r\n\r\ndef stack2(x, filters, blocks, stride1=2, name=None):\r\n  \"\"\"A set of stacked residual blocks.\r\n\r\n  Arguments:\r\n      x: input tensor.\r\n      filters: integer, filters of the bottleneck layer in a block.\r\n      blocks: integer, blocks in the stacked blocks.\r\n      stride1: default 2, stride of the first layer in the first block.\r\n      name: string, stack label.\r\n\r\n  Returns:\r\n      Output tensor for the stacked blocks.\r\n  \"\"\"\r\n  x = block2(x, filters, conv_shortcut=True, name=name + '_block1')\r\n  for i in range(2, blocks):\r\n    x = block2(x, filters, name=name + '_block' + str(i))\r\n  x = block2(x, filters, stride=stride1, name=name + '_block' + str(blocks))\r\n  return x\r\n\r\n\r\ndef block3(x,\r\n           filters,\r\n           kernel_size=3,\r\n           stride=1,\r\n           groups=32,\r\n           conv_shortcut=True,\r\n           name=None):\r\n  \"\"\"A residual block.\r\n\r\n  Arguments:\r\n    x: input tensor.\r\n    filters: integer, filters of the bottleneck layer.\r\n    kernel_size: default 3, kernel size of the bottleneck layer.\r\n    stride: default 1, stride of the first layer.\r\n    groups: default 32, group size for grouped convolution.\r\n    conv_shortcut: default True, use convolution shortcut if True,\r\n        otherwise identity shortcut.\r\n    name: string, block label.\r\n\r\n  Returns:\r\n    Output tensor for the residual block.\r\n  \"\"\"\r\n  bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\r\n\r\n  if conv_shortcut:\r\n    shortcut = layers.Conv2D(\r\n        (64 // groups) * filters,\r\n        1,\r\n        strides=stride,\r\n        use_bias=False,\r\n        name=name + '_0_conv')(x)\r\n    shortcut = BatchNormalization(\r\n        axis=bn_axis, epsilon=1.001e-5, name=name + '_0_bn')(shortcut)\r\n  else:\r\n    shortcut = x\r\n\r\n  x = layers.Conv2D(filters, 1, use_bias=False, name=name + '_1_conv')(x)\r\n  x = BatchNormalization(\r\n      axis=bn_axis, epsilon=1.001e-5, name=name + '_1_bn')(x)\r\n  x = layers.Activation('relu', name=name + '_1_relu')(x)\r\n\r\n  c = filters // groups\r\n  x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name=name + '_2_pad')(x)\r\n  x = layers.DepthwiseConv2D(\r\n      kernel_size,\r\n      strides=stride,\r\n      depth_multiplier=c,\r\n      use_bias=False,\r\n      name=name + '_2_conv')(x)\r\n  x_shape = backend.int_shape(x)[1:-1]\r\n  x = layers.Reshape(x_shape + (groups, c, c))(x)\r\n  x = layers.Lambda(\r\n      lambda x: sum(x[:, :, :, :, i] for i in range(c)),\r\n      name=name + '_2_reduce')(x)\r\n  x = layers.Reshape(x_shape + (filters,))(x)\r\n  x = BatchNormalization(\r\n      axis=bn_axis, epsilon=1.001e-5, name=name + '_2_bn')(x)\r\n  x = layers.Activation('relu', name=name + '_2_relu')(x)\r\n\r\n  x = layers.Conv2D(\r\n      (64 // groups) * filters, 1, use_bias=False, name=name + '_3_conv')(x)\r\n  x = BatchNormalization(\r\n      axis=bn_axis, epsilon=1.001e-5, name=name + '_3_bn')(x)\r\n\r\n  x = layers.Add(name=name + '_add')([shortcut, x])\r\n  x = layers.Activation('relu', name=name + '_out')(x)\r\n  return x\r\n\r\n\r\ndef stack3(x, filters, blocks, stride1=2, groups=32, name=None):\r\n  \"\"\"A set of stacked residual blocks.\r\n\r\n  Arguments:\r\n    x: input tensor.\r\n    filters: integer, filters of the bottleneck layer in a block.\r\n    blocks: integer, blocks in the stacked blocks.\r\n    stride1: default 2, stride of the first layer in the first block.\r\n    groups: default 32, group size for grouped convolution.\r\n    name: string, stack label.\r\n\r\n  Returns:\r\n    Output tensor for the stacked blocks.\r\n  \"\"\"\r\n  x = block3(x, filters, stride=stride1, groups=groups, name=name + '_block1')\r\n  for i in range(2, blocks + 1):\r\n    x = block3(\r\n        x,\r\n        filters,\r\n        groups=groups,\r\n        conv_shortcut=False,\r\n        name=name + '_block' + str(i))\r\n  return x\r\n\r\n\r\n@keras_export('keras.applications.resnet50.ResNet50',\r\n              'keras.applications.resnet.ResNet50',\r\n              'keras.applications.ResNet50')\r\ndef ResNet50(include_top=True,\r\n             weights='imagenet',\r\n             input_tensor=None,\r\n             input_shape=None,\r\n             pooling=None,\r\n             classes=1000,\r\n             **kwargs):\r\n  \"\"\"Instantiates the ResNet50 architecture.\"\"\"\r\n\r\n  def stack_fn(x):\r\n    x = stack1(x, 64, 3, stride1=1, name='conv2')\r\n    x = stack1(x, 128, 4, name='conv3')\r\n    x = stack1(x, 256, 6, name='conv4')\r\n    return stack1(x, 512, 3, name='conv5')\r\n\r\n  return ResNet(stack_fn, False, True, 'resnet50', include_top, weights,\r\n                input_tensor, input_shape, pooling, classes, **kwargs)\r\n\r\n\r\n@keras_export('keras.applications.resnet.ResNet101',\r\n              'keras.applications.ResNet101')\r\ndef ResNet101(include_top=True,\r\n              weights='imagenet',\r\n              input_tensor=None,\r\n              input_shape=None,\r\n              pooling=None,\r\n              classes=1000,\r\n              **kwargs):\r\n  \"\"\"Instantiates the ResNet101 architecture.\"\"\"\r\n\r\n  def stack_fn(x):\r\n    x = stack1(x, 64, 3, stride1=1, name='conv2')\r\n    x = stack1(x, 128, 4, name='conv3')\r\n    x = stack1(x, 256, 23, name='conv4')\r\n    return stack1(x, 512, 3, name='conv5')\r\n\r\n  return ResNet(stack_fn, False, True, 'resnet101', include_top, weights,\r\n                input_tensor, input_shape, pooling, classes, **kwargs)\r\n\r\n\r\n@keras_export('keras.applications.resnet.ResNet152',\r\n              'keras.applications.ResNet152')\r\ndef ResNet152(include_top=True,\r\n              weights='imagenet',\r\n              input_tensor=None,\r\n              input_shape=None,\r\n              pooling=None,\r\n              classes=1000,\r\n              **kwargs):\r\n  \"\"\"Instantiates the ResNet152 architecture.\"\"\"\r\n\r\n  def stack_fn(x):\r\n    x = stack1(x, 64, 3, stride1=1, name='conv2')\r\n    x = stack1(x, 128, 8, name='conv3')\r\n    x = stack1(x, 256, 36, name='conv4')\r\n    return stack1(x, 512, 3, name='conv5')\r\n\r\n  return ResNet(stack_fn, False, True, 'resnet152', include_top, weights,\r\n                input_tensor, input_shape, pooling, classes, **kwargs)\r\n\r\n\r\n@keras_export('keras.applications.resnet50.preprocess_input',\r\n              'keras.applications.resnet.preprocess_input')\r\ndef preprocess_input(x, data_format=None):\r\n  return imagenet_utils.preprocess_input(\r\n      x, data_format=data_format, mode='caffe')\r\n\r\n\r\n@keras_export('keras.applications.resnet50.decode_predictions',\r\n              'keras.applications.resnet.decode_predictions')\r\ndef decode_predictions(preds, top=5):\r\n  return imagenet_utils.decode_predictions(preds, top=top)\r\n\r\n\r\npreprocess_input.__doc__ = imagenet_utils.PREPROCESS_INPUT_DOC.format(\r\n    mode='',\r\n    ret=imagenet_utils.PREPROCESS_INPUT_RET_DOC_CAFFE,\r\n    error=imagenet_utils.PREPROCESS_INPUT_ERROR_DOC)\r\ndecode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\r\n\r\nDOC = \"\"\"\r\n\r\n  Reference paper:\r\n  - [Deep Residual Learning for Image Recognition]\r\n  (https://arxiv.org/abs/1512.03385) (CVPR 2015)\r\n\r\n  Optionally loads weights pre-trained on ImageNet.\r\n  Note that the data format convention used by the model is\r\n  the one specified in your Keras config at `~/.keras/keras.json`.\r\n\r\n  Arguments:\r\n    include_top: whether to include the fully-connected\r\n      layer at the top of the network.\r\n    weights: one of `None` (random initialization),\r\n      'imagenet' (pre-training on ImageNet),\r\n      or the path to the weights file to be loaded.\r\n    input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\r\n      to use as image input for the model.\r\n    input_shape: optional shape tuple, only to be specified\r\n      if `include_top` is False (otherwise the input shape\r\n      has to be `(224, 224, 3)` (with `'channels_last'` data format)\r\n      or `(3, 224, 224)` (with `'channels_first'` data format).\r\n      It should have exactly 3 inputs channels,\r\n      and width and height should be no smaller than 32.\r\n      E.g. `(200, 200, 3)` would be one valid value.\r\n    pooling: Optional pooling mode for feature extraction\r\n      when `include_top` is `False`.\r\n      - `None` means that the output of the model will be\r\n          the 4D tensor output of the\r\n          last convolutional block.\r\n      - `avg` means that global average pooling\r\n          will be applied to the output of the\r\n          last convolutional block, and thus\r\n          the output of the model will be a 2D tensor.\r\n      - `max` means that global max pooling will\r\n          be applied.\r\n    classes: optional number of classes to classify images\r\n      into, only to be specified if `include_top` is True, and\r\n      if no `weights` argument is specified.\r\n\r\n  Returns:\r\n    A Keras model instance.\r\n\"\"\"\r\n\r\nsetattr(ResNet50, '__doc__', ResNet50.__doc__ + DOC)\r\nsetattr(ResNet101, '__doc__', ResNet101.__doc__ + DOC)\r\nsetattr(ResNet152, '__doc__', ResNet152.__doc__ + DOC)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42051\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42051\">No</a>\n", "Sorry\uff0cHow can I reopen it. I press the wrong button.\r\n\r\n> nikitamaia", "Hi @ImMrMa, since this appears to be the same as #42111 I'm going to close this issue so we can track progress in one place.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42051\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42051\">No</a>\n"]}, {"number": 42050, "title": "Moving rest of the filesystems to Transactional API", "body": "This PR is complement of  #41946 and enables Transactional API in remaining Filesystems.", "comments": ["Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 42048, "title": "Add test for nested batch accumulator ", "body": "", "comments": ["What's the error / stack trace?", "> What's the error / stack trace?\r\n\r\n```\r\nAssertionError: Tuples differ: (2,) != ()\r\n\r\nFirst tuple contains 1 additional elements.\r\nFirst extra element 0:\r\n2\r\n\r\n- (2,)\r\n+ () : Shape mismatch: expected (2,), got () with contents -0.16209069.\r\n```\r\nThe problem is with using `tape.gradient` afterwards. Results are as expected with `acc.jvp` (tf.Tensor([-0.0841471  -0.16829419], shape=(2,), dtype=float32)", "A scalar seems like the right shape to me, since it's a scalar (`primals`) passed as the second argument to `tape.gradient`. Note that `tape.gradient(y, x)` is equivalent to `tape.gradient(tf.reduce_sum(y), x)` (where the reduce_sum is implicitly watched on the tape). If you want a full Jacobian there's another method for that.", "So the jvp function for the op runs, but something is passing a scalar? It could be related to the tf.function special casing for forwardprop. Maybe this placeholder? https://github.com/tensorflow/tensorflow/blob/247e9bd050e68fa4b055fe6c99144def3fde4e81/tensorflow/python/eager/function.py#L1005\r\n\r\nTo be clear the inner ForwardAccumulator is making a tf.function, and so the outer accumulator runs forwardprop on that function rather than op-by-op. But the test looks fine to me (until the assertions at least; is the whole thing linear in the tangent?).", "@abhichou4 Can you please address Ubuntu Sanity errors? Thanks!"]}, {"number": 42047, "title": "TF with CUDA 11 and cuDNN 8", "body": "Enable TF-nightly with CUDA 11 and cuDNN 8 for Ubuntu and Windows.\r\n", "comments": ["NVIDIA pretty much pushed CUDA 10 out of the view. This team needs to have tf-nightly with CUDA 11. A lots of bugs are being introduced because of this not being to CUDA 11.\r\n\r\nLook at this error. It suddenly started coming from today,\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/42076", "This bug is the tracking bug for converting to CUDA 11.", "But this used to work.", "Accidentally. We never tested it and breakages of something that was accidentally working were not discovered.\r\n\r\nThis bug tracks moving to building with CUDA 11 by default", "As of now, i am building from source for Nvidia 11. But... we have to be careful though, the recent Nvidia update upended my system. :-( and had to spend lots of time figuring out.", ">  the recent Nvidia update upended my system\r\n\r\n@summa-code I recommend using [docker](https://www.tensorflow.org/install/docker#download_a_tensorflow_docker_image) if that works for your use case.", "Well, i know i would have used docker if there are frequent issues in building in the source. But the problem was in the system level nvidia kernal. Reinstalling fixed the issue.", "Hi all: We made the changes for Ubuntu last night. Changes for Windows should land soon. ", "The changes for Windows have landed. Let us know if you encounter any issues. ", "Closing as nightly now uses CUDA 11", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42047\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42047\">No</a>\n"]}, {"number": 42046, "title": "Keras custom model shape inference does not work with model.fit() for hub modules", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.11\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nKeras appears to do some things under the hood when calling `model.fit(...)` that don't work with TensorFlow Hub modules. Minimal script included below, which produces the following error:\r\n\r\n```\r\nValueError: Shape must be rank 1 but is rank 2 for '{{node text_preprocessor/tokenize/StringSplit/StringSplit}} = StringSplit[skip_empty=true](text_preprocessor/StaticRegexReplace_1, text_preprocessor/tokenize/StringSplit/Const)' with input shapes: [?,1], [].\r\n```\r\n\r\nIf I print out the `inputs` argument to `model.call` during `model.fit`, I see that it is actually a `Tensor(\"ExpandDims:0\", shape=(None, 1), dtype=string)`, so if I start `model.call` with `inputs = tf.squeeze(inputs)`, everything works, but this is a bit of a hack.\r\n\r\nI'm actually not sure if the error is specifically related to TensorFlow Hub modules, but it is at least sufficient to reproduce the issue.\r\n\r\n**Describe the expected behavior**\r\n`model.fit(x, y)` should work the same as as manually calling the forward pass with `model(x, y)`\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\n\r\nclass MyModel(tf.keras.Model):    \r\n    def __init__(self):\r\n        super().__init__()\r\n        self.embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\r\n        self.dense = tf.keras.layers.Dense(1)\r\n        \r\n    def call(self, inputs, training=False):\r\n        x = self.embed(inputs)\r\n        x = self.dense(x)\r\n        return x\r\n    \r\nmodel = MyModel()\r\nx = ['a sentence', 'b sentence']\r\ny = [0, 1]\r\n\r\nmodel(x, y)  # works fine\r\n\r\nmodel.compile(loss='mse', optimizer='sgd')\r\nmodel.fit(x, y)  # errors\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@dwyatte \r\nI ran the code shared and able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/7b3c6eec76acac4d3b49f1bef8f937fb/untitled320.ipynb\r\n).\r\n\r\nPlease refer to [this issue](https://github.com/keras-team/keras/issues/10648) , [link](https://stackoverflow.com/questions/40366163/tensorflow-valueerror-shape-must-be-rank-1-but-is-rank-2) and let us know if it helps", "@Saduf2019 Thanks for confirming replication. Neither of the linked issues seem related to this which seems to happen due to some assumptions that Keras makes when compiling a static graph", "@dwyatte Have you seen https://github.com/tensorflow/hub/issues/648?", "@bhack Thanks for pointing out that issue -- I certainly think it's related.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\n\r\n# adapted from https://github.com/tensorflow/hub/issues/648\r\nhub_layer = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4', output_shape=(None, 512), input_shape=(None,), trainable=True, dtype=tf.string)\r\ninputs = tf.keras.layers.Input(shape=(None,), dtype='string')\r\n\r\nhub_layer(tf.squeeze(inputs))  # works\r\nhub_layer(inputs)  # errors\r\n```\r\n\r\nI think there are two things going on:\r\n\r\n1.) It looks like `tf.keras.layers.Input` as well as calling `tf.keras.Model.fit` on a model that relies on shape inference (i.e., no explicit input layers) insert an implicit batch dimension on the input, but calling a `tf.keras.Model` like `model(x, y)` does not.\r\n2.) Universal Sentence Encoder expects 1D inputs, so becomes unusable in compiled Keras models without a squeeze operation to remove the implicit batch dimension.\r\n\r\nDo we think 1.) is an issue? Inserting implicit batch dimensions on Input layers is suggested to be expected behavior according to the docs, but I'm not sure it is documented for models that use shape inference.", "Adding the `contributions welcome` label to this issue for further investigation by the community. If you are interested in working on this issue, please leave a comment and I will assign it to you. Thanks!", "@nikitamaia As it was also partially discussed at https://github.com/tensorflow/hub/issues/366 and https://github.com/tensorflow/hub/issues/648 can you verify with the team if it is in in the TF Keras perimeter or it is in TF Hub repo.", "I think it is likely to be a keras issue. In the for any input feed to keras, we will try to expand it to 2D tensor if the input is 1D. In this case, the x was having a shape [2,] in the direct call(), and expand to [2, 1] when fit(). The hub layer was expecting a vector input, and raise error when the input rank doesn't match its expectation.\r\n\r\nI don't think we could change the keras behavior, since it has been there for quite a while. In this case, either hub layer could have a loose check or squeeze the last dim if it is 1, user will have to do this in their subclassed model.", "> I  don't think we could change the keras behavior, since it has been there for quite a while. \r\n\r\nI meant exactly this and it is why I think it could be by an operational point of view a TF hub ticket as here we are quite constrained.", "> In this case, either hub layer could have a loose check or squeeze the last dim if it is 1, user will have to do this in their subclassed model.\r\n\r\nAgree it might be nice to have something in `hub.KerasLayer` or the next version of the relevant TensorFlow Hub SavedModels that currently expect 1D inputs (not sure if limited to Universal Sentence Encoder or there are others) to automatically handle this.\r\n\r\n> I meant exactly this and it is why I think it could be by an operational point of view a TF hub ticket as here we are quite constrained.\r\n\r\n@bhack @qlzh727 would it be appropriate to simply amend the docs of [tf.keras.Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) in the meantime with details about the \"at least 2D\" expansion?", "I added a note in the docs in https://github.com/tensorflow/tensorflow/pull/49516. If merged, I will close this issue and re-open in https://github.com/tensorflow/hub to see if there's anything that can be done in `KerasLayer`.", "Was able to replicate the issue in TF 2.6.0-dev20210530,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/ee50734005e3c5164cd376a722097db7/untitled104.ipynb)..Thanks !", "Given the pending release of TF 2.7.0 which changes this behavior, I think this issue can be closed.\r\n\r\nhttps://github.com/tensorflow/tensorflow/releases/tag/v2.7.0-rc1\r\n```\r\nThe methods Model.fit(), Model.predict(), and Model.evaluate() will no longer uprank input data of shape (batch_size,) to become (batch_size, 1). This enables Model subclasses to process scalar data in their train_step()/test_step()/predict_step() methods.\r\nNote that this change may break certain subclassed models. You can revert back to the previous behavior by adding upranking yourself in the train_step()/test_step()/predict_step() methods, e.g. if x.shape.rank == 1: x = tf.expand_dims(x, axis=-1). Functional models as well as Sequential models built with an explicit input shape are not affected.\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42046\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42046\">No</a>\n"]}, {"number": 42045, "title": "TF website tutorial \"Save and load\" fails on Google Colab for entire models", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab on cloud (run in Safari browser on Mac OSX 10.14.6)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): NA\r\n- TensorFlow version (use command below): TF 2.3.0, Git version v2.3.0-0-gb36436b087\r\n- Python version: Appears to be 3.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: ??\r\n- GPU model and memory: ??\r\n\r\n**Describe the current behavior**\r\nThe TensorFlow tutorial page https://www.tensorflow.org/tutorials/keras/save_and_load fails even when run in Google Colab, your cloud environment. Where it fails exactly is towards the bottom, in the section of the tutorial describing how to save and load entire models with model.save() and load_model(). I find that the restored models (in either SavedModel or HDF5 format) fail to reproduce the original model accuracies, given exactly the provided code. Instead I get around 8-9% accuracies.\r\n\r\nI get the same problem running the downloaded Jupyter notebook on my own laptop (Mac OSX 10.14.6, TF 2.4.0-dev20200727, Git v1.12.1-37595-g9f2e1a7246, TF from pip3, Python 3.8.5).\r\n\r\n**Describe the expected behavior**\r\nThe expected behavior is for the saved and reloaded models to have exactly the same accuracies on the same test data.\r\n\r\n**Standalone code to reproduce the issue**\r\nI've attached a small notebook that replicates the problem on my laptop. \r\n[minimal.ipynb.txt](https://github.com/tensorflow/tensorflow/files/5024470/minimal.ipynb.txt)\r\n\r\nThis was modeled after a similar tutorial (https://machinelearningmastery.com/save-load-keras-deep-learning-models/ under \"Save Model Weights and Architecture Together\"). That tutorial's code runs fine with the given example data on my laptop (the saved and reloaded model has identical accuracy to the original), yet the very similar code in the attached notebook using the MNIST data and slightly different model does not. I'm not sure what this means. Honestly I'm pretty new to Keras. I'm not sure if this is a bug in the Keras code, or just an error in the tutorial. But again, it doesn't work with the provided tutorial in the Google Colab environment, suggesting it's not an error on my part.\r\n\r\nFWIW, I do notice that the reloaded SavedModel model and the reloaded HDF5 model, when saved from the same original model (as in my example notebook), both give the identical bad accuracy.\r\n\r\n**Other info / logs** \r\nNot sure if relevant, but these warning appear the first time model.save() is used (from Google Colab):\r\n\r\n_WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nINFO:tensorflow:Assets written to: saved_model/my_model/assets_\r\n\r\nOn my laptop, I get the same warnings (with a different path to tracking.py: /Users/lilley/Library/Python/3.8/lib/python/site-packages/tensorflow/python/training/tracking/tracking.py).\r\n\r\nThe example run given on the tutorial page suggests that this functionality was originally working in TF 2.2.0.\r\n", "comments": ["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nINFO:tensorflow:Assets written to: resnet_model_combine.model/assets\r\n", "How can I get rid of this?? How to update?\r\n", "I have the same issue please help", "I have tried in colab in TF nightly version and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/344dbdae13ebdbf0eaf032ff506084c8/untitled224.ipynb).Thanks!", "I think the issue I reported some days ago is exactly the same: https://github.com/keras-team/keras/issues/14181", "@ErikEngerd Thank you for directing me to your issue. I'm a bit new with Github and with TF/Keras. I was afraid I submitted my issue in the wrong forum, which appears to be the case, so perhaps the people with the power to address this issue will never see this thread (or perhaps ignore it).\r\n\r\nIn my issue report, I specifically emphasized that I was using their unedited example code in Google Colab, which would seem to rule out issues in my coding and my environment. I see in your thread that you also pointed this out -- thank you! Hopefully the issue will be addressed soon.\r\n", "@jasonlilley Thanks for reporting this issue. I agree this is an issue with `TF2.3` and `tf-nightly`. It was working well with `TF2.2` so this is a regression issue. Thanks!", "This is a bug with using the sparse categorical accuracy. For now, please compile the model with `metrics='sparse_categorical_accuracy'` instead of just `'accuracy'`. ", "@jasonlilley I can confirm that this is working as expected when I followed @k-w-w suggestion. [Here](https://colab.research.google.com/gist/jvishnuvardhan/2cc844c7f00fec232d04af0a83d9bb22/untitled224.ipynb) is the gist for reference. Thanks!", "@k-w-w Thank you for replying. I was also able to confirm that the model compilation suggestion worked in my notebooks on my laptop. I suggest that someone change the tutorial page with the fix.", "This issue has been fixed in commit [9d8947](https://github.com/tensorflow/tensorflow/commit/5adacc88077ef82f6c4a7f9bb65f9ed89f9d8947). Please try the latest tf-nightly if you need the fix immediately, otherwise the next official release will have the fix. Marking this as closed."]}, {"number": 42044, "title": " TF-TRT op converter test variable naming", "body": "This PR fixes the naming convention of the ParameterizedOpConverterTestBase member variables. The issue was raised here: https://github.com/tensorflow/tensorflow/pull/39990#discussion_r457744558\r\n \r\nTagging @bixia1 for review. Tagging @MattConley for visibility. \r\n\r\n", "comments": ["@tfeher Can you please resolve conflicts? Thanks!", "@tfeher  Can you please check @bixia1's comments and resolve conflicts?. Thanks!", "Thanks for the patience, I have updated the PR. "]}, {"number": 42043, "title": "\"logs\" Callback bugs with TF 2.3 on colab with TPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 2.3\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:TPU \r\n- GPU model and memory:\r\n\r\nI got some weird issue, it seems that when using a custom callback for computing a metric, and then save it in the logs, \r\nthe ModelCheckpoint function does not see the metrics saved in logs, so it does not save the model at the end of an epoch. I have seen this issue today. It worked well few days ago, so it seems to be related to TF 2.3. I also see some issue about the memory, I have to lower the batch size to make the training work.\r\n\r\n\r\n`WARNING:tensorflow:Can save best model only with roc_val available, skipping.`\r\n`WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0288s vs `on_train_batch_end` time: 0.9656s). Check your callbacks.`\r\n\r\n\r\n```\r\nclass RocCallback(Callback):\r\n    def __init__(self , dataset_val):\r\n        self.x = dataset_val\r\n        self.y =  np.concatenate([np.array(x[1]) for x in list(dataset_val)]).reshape(-1)\r\n    def on_train_begin(self, logs={}):\r\n        return\r\n\r\n    def on_train_end(self, logs={}):\r\n        return\r\n\r\n    def on_epoch_begin(self, epoch, logs={}):\r\n        return\r\n\r\n    def on_epoch_end(self, epoch, logs={}):\r\n        pred = model.predict(self.x)\r\n        roc_val = roc_auc_score(self.y, pred)\r\n        logs[\"roc_val\"] = roc_val\r\n        print('\\n - %s average: %s' % ('roc_val', str(round(roc_val,4))),end=100*' '+'\\n')\r\n        return\r\n\r\n    def on_batch_begin(self, batch, logs={}):\r\n        return\r\n\r\n    def on_batch_end(self, batch, logs={}):\r\n        return\r\n```\r\n\r\n```\r\ntf.keras.callbacks.ModelCheckpoint(\"model.h5\", monitor='roc_val', verbose=0, save_best_only=True,\r\n        save_weights_only=True, mode='max', save_freq='epoch')\r\n```\r\n\r\nIs there a way to choose the version of TF (2.2) on colab  ?\r\n\r\n", "comments": ["@Shiro-LK see googlecolab/colabtools#1470 for switching the TF+TPU version in colab.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@craigcitro the fix you mentioned is not working for me. If I downgrade tensorflow I get the following error:\r\n`InvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op<name=_Send; signature=tensor:T -> ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>; NodeDef: {{node _Send}}`\r\n\r\nDo you have any ideas how can I overcome this? Thanks", "I don't have any real expertise on the TF side (so hoping someone else jumps in).", "I had a similar error. It is important to follow this https://colab.research.google.com/gist/graf10a/642c143fd80887ac8a69819250f5141b/tf_tpu_issue_submission_fixed.ipynb exactly (the first and the second cells). Not just downgrade. There should be two separate cells, import requests etc. ", "This might be because of a change in how model loading and saving is handled on TPU. Can you try to add the following option to your checkpoint call:\r\n\r\n```\r\n# TPUs need this extra setting to save to local disk, otherwise, they can only save models to GCS (Google Cloud Storage).\r\n# The setting instructs Tensorflow to retrieve all parameters from the TPU then do the saving from the local VM, not the TPU.\r\nsave_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\r\ntf.keras.callbacks.ModelCheckpoint(\"saved_model\", monitor='roc_val', verbose=0, save_best_only=True,\r\n        save_weights_only=True, mode='max', save_freq='epoch', options=save_locally)\r\n```\r\n\r\nPlease notice in the code that I also recommend to change the type of the saved model from .h5 (the legacy Keras format) to \"Tensorflow saved model\", the standard tensorflow format. The format is determined based on the extension. Saving from TPU to local disk now works in TF saved model format, with the /job:localhost save option. This is new in TF 2.3. Previously, saving locally from TPU only worked in the .h5 format. This is not supposed to have changed but as your bug report points out, it looks like it has.", "@Shiro-LK, the behavior change may be a result of a recent code change. Can you try setting `_supports_tf_logs` to True on the custom callback's attr before `model.fit` and see if it then allows you to pass around information in `logs`?", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42043\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42043\">No</a>\n"]}, {"number": 42042, "title": "Add README.md for keras_examples_benchmarks folder", "body": "We place the README.md for keras_examples_benchmarks folder first and we also can add it to keras/benchmarks folder.", "comments": []}, {"number": 42041, "title": "comp:mkl", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Microsoft Windows 10 Pro\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: Tensorflow 2.4\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 5.3\r\n- GPU model and memory: NVIDIA GTX 1050 v451.67, CUDADNN 11\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1\r\ncoreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n\r\n\r\n**Describe the problem**\r\nUnable to build tensorflow 2.3 on windows. I use Anaconda. The objective was to benefit from the AVX2 instructionset available on my laptop for enhanced performance.\r\n\r\n\"This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\"\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI followed the steps here:\r\nhttps://software.intel.com/content/www/us/en/develop/articles/intel-optimization-for-tensorflow-installation-guide.html#Anaconda_main_win\r\n 1. Already installed:  numpy, keras-applications, keras-preprocessing, pip, six, wheel, mock\r\n\r\n2. git clone https://github.com/tensorflow/tensorflow\r\ngit checkout r2.1\r\n\r\nwindows cmd: set PATH=%PATH%;output_dir\\external\\mkl_windows\\lib\r\n\r\n3. tried either of\r\nbazel --output_base=output_dir build --config=mkl --copt=-mavx2 --config=cuda --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package\r\nand\r\nbazel --output_base=output_dir build --config=mkl --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n(base) C:\\Users\\agarw\\tensorflow>bazel --output_base=output_dir build --config=mkl --config=opt --copt=-mavx2 --config=cuda --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\users\\agarw\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/agarw/anaconda3/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\agarw\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:mkl in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_mkl_dnn_v1_only=true -c opt\r\nERROR: Config value opt is not defined in any .rc file\r\n\r\n(base) C:\\Users\\agarw\\tensorflow>bazel --output_base=output_dir build --config=mkl --copt=-mavx2 --config=cuda --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\users\\agarw\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/agarw/anaconda3/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\agarw\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:mkl in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_mkl_dnn_v1_only=true -c opt\r\nINFO: Found applicable config definition build:cuda in file c:\\users\\agarw\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:windows in file c:\\users\\agarw\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at C:/users/agarw/tensorflow/output_dir/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - C:/users/agarw/tensorflow/output_dir/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - C:/users/agarw/tensorflow/WORKSPACE:37:1\r\nERROR: infinite symlink expansion detected\r\n[start of symlink chain]\r\nC:/users/agarw/tensorflow/output_dir/external/org_tensorflow\r\nC:/users/agarw/tensorflow\r\n[end of symlink chain]\r\nINFO: Call stack for the definition of repository 'rules_cc' which is a tf_http_archive (rule definition at C:/users/agarw/tensorflow/third_party/repo.bzl:134:19):\r\n - C:/users/agarw/tensorflow/tensorflow/workspace.bzl:1013:5\r\n - C:/users/agarw/tensorflow/WORKSPACE:19:1\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': no such package '@org_tensorflow//third_party/gpus': Could not access C:/users/agarw/tensorflow/output_dir/external/org_tensorflow: Infinite symlink expansion\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': no such package '@org_tensorflow//third_party/gpus': Could not access C:/users/agarw/tensorflow/output_dir/external/org_tensorflow: Infinite symlink expansion\r\nINFO: Elapsed time: 55.821s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\n    Fetching @local_config_cuda; Restarting.\r\n\r\n(base) C:\\Users\\agarw\\tensorflow>bazel --output_base=output_dir build --config=mkl --config=cuda --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\users\\agarw\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/agarw/anaconda3/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\agarw\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:mkl in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_mkl_dnn_v1_only=true -c opt\r\nINFO: Found applicable config definition build:cuda in file c:\\users\\agarw\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:windows in file c:\\users\\agarw\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Call stack for the definition of repository 'rules_cc' which is a tf_http_archive (rule definition at C:/users/agarw/tensorflow/third_party/repo.bzl:134:19):\r\n - C:/users/agarw/tensorflow/tensorflow/workspace.bzl:1013:5\r\n - C:/users/agarw/tensorflow/WORKSPACE:19:1\r\nERROR: infinite symlink expansion detected\r\n[start of symlink chain]\r\nC:/users/agarw/tensorflow/output_dir/external/org_tensorflow\r\nC:/users/agarw/tensorflow\r\n[end of symlink chain]\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at C:/users/agarw/tensorflow/output_dir/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - C:/users/agarw/tensorflow/output_dir/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - C:/users/agarw/tensorflow/WORKSPACE:37:1\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': no such package '@org_tensorflow//third_party/gpus': Could not access C:/users/agarw/tensorflow/output_dir/external/org_tensorflow: Infinite symlink expansion\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': no such package '@org_tensorflow//third_party/gpus': Could not access C:/users/agarw/tensorflow/output_dir/external/org_tensorflow: Infinite symlink expansion\r\nINFO: Elapsed time: 0.435s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\n    Fetching @local_config_cuda; Restarting.\r\n\r\n(base) C:\\Users\\agarw\\tensorflow>bazel --output_base=output_dir build --config=mkl --config=opt --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\users\\agarw\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/agarw/anaconda3/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\agarw\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:mkl in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_mkl_dnn_v1_only=true -c opt\r\nERROR: Config value opt is not defined in any .rc file\r\n\r\n(base) C:\\Users\\agarw\\tensorflow>bazel --output_base=output_dir build --config=mkl --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\users\\agarw\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/agarw/anaconda3/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\agarw\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:mkl in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_mkl_dnn_v1_only=true -c opt\r\nINFO: Found applicable config definition build:windows in file c:\\users\\agarw\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at C:/users/agarw/tensorflow/output_dir/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - C:/users/agarw/tensorflow/output_dir/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - C:/users/agarw/tensorflow/WORKSPACE:37:1\r\nINFO: Call stack for the definition of repository 'com_google_protobuf' which is a tf_http_archive (rule definition at C:/users/agarw/tensorflow/third_party/repo.bzl:134:19):\r\n - C:/users/agarw/tensorflow/tensorflow/workspace.bzl:601:5\r\n - C:/users/agarw/tensorflow/WORKSPACE:19:1\r\nERROR: infinite symlink expansion detected\r\n[start of symlink chain]\r\nC:/users/agarw/tensorflow/output_dir/external/org_tensorflow\r\nC:/users/agarw/tensorflow\r\n[end of symlink chain]\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: invalid registered execution platform '@local_execution_config_platform//:platform': no such package '@local_execution_config_platform//': no such package '@org_tensorflow//third_party/remote_config': Could not access C:/users/agarw/tensorflow/output_dir/external/org_tensorflow: Infinite symlink expansion\r\nINFO: Elapsed time: 5.345s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (6 packages loaded, 5 targets configured)\r\n    currently loading: tensorflow\r\n    Fetching @local_execution_config_platform; Restarting. 4s\r\n\r\n(base) C:\\Users\\agarw\\tensorflow>bazel --output_base=output_dir build --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\users\\agarw\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/agarw/anaconda3/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\agarw\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:mkl in file c:\\users\\agarw\\tensorflow\\.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_mkl_dnn_v1_only=true -c opt\r\nERROR: Config value opt is not defined in any .rc file\r\n\r\n(base) C:\\Users\\agarw\\tensorflow>\r\n\r\n", "comments": ["@agarwalamit081 \r\nPlease refer to below issues with same \"ERROR: Config value opt is not defined in any .rc file\" and let us know if it helps. \r\n#23613 #38491", "Btw I don't see you execute `configure` from step 3 of Linux, at docs you mentioned.", "I am using Windows with Visual Studio and BAZEL. I executed the commands on\nwindows cmd.\n\n<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail&utm_term=icon>\nVirus-free.\nwww.avast.com\n<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail&utm_term=link>\n<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>\n\nOn Thu, 6 Aug 2020 at 06:19, Antony Kurniawan <notifications@github.com>\nwrote:\n\n> Btw I don't see you execute configure from step 3 of Linux, at docs you\n> mentioned.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/42041#issuecomment-669674240>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKLHGFXEY2M5R2BCC3OBNP3R7IVOLANCNFSM4PUWWT5A>\n> .\n>\n\n\n-- \n\nKind Regards,\n*Amit Agarwal*\n0760527421\n", "@agarwalamit081 \r\n\r\nCould you feedback the solution?\r\n", "@agarwalamit081 \r\n\r\nIs it possible to close this issue?", "@agarwalamit081 \r\nAny feedback?", "Dear Neo,\nI apologise but I am not working on this currently and moreover I am\npreoccupied with some family issues.\nThanks for your understanding.\n\nRegards,\nAmit Agarwal\n\nOn Thu, Dec 17, 2020, 12:01 Neo Zhang Jianyu <notifications@github.com>\nwrote:\n\n> @agarwalamit081 <https://github.com/agarwalamit081>\n> Any feedback?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/42041#issuecomment-747240502>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKLHGFUQID6RKKZVMTZPZ6DSVGQVZANCNFSM4PUWWT5A>\n> .\n>\n", "@agarwalamit081 \r\nIt's no problem.\r\n\r\nDo you think to pending this issue or close it?\r\n\r\nThank you!", "Hi @agarwalamit081 ! \r\nWe are checking to see whether you still need help in this issue . Have you tried these threads on building Tensorflow from [source](https://www.tensorflow.org/install/source_windows) in latest stable version 2.6 yet ? . Thanks!", "Dear All,\nYou can close this issue. I havent checked for a very long time now.\n\nRegards\nAmit\n\nOn Wed, 27 Oct 2021 at 11:25, mohantym ***@***.***> wrote:\n\n> Hi @agarwalamit081 <https://github.com/agarwalamit081> !\n> We are checking to see whether you still need help in this issue . Have\n> you tried these threads on building Tensorflow from source\n> <https://www.tensorflow.org/install/source_windows> in latest stable\n> version 2.6 yet ? . Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/42041#issuecomment-952719854>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKLHGFRIFV3FYRWASG4SNETUI7APLANCNFSM4PUWWT5A>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "ok @agarwalamit081! Closing this issue for now.Please create a new issue if you need further assistance . Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42041\">No</a>\n"]}, {"number": 42040, "title": "[DOC] Fix math_ops.py doc", "body": "- https://www.tensorflow.org/api_docs/python/tf/math/abs: math expr before \"For example\"\r\n- https://www.tensorflow.org/api_docs/python/tf/math/sign: better look in code block instead of plain text\r\n- https://www.tensorflow.org/api_docs/python/tf/math/sigmoid: plain text \\in\r\n- https://www.tensorflow.org/api_docs/python/tf/math/exp: \\times \\cos \\sin break\r\n- https://www.tensorflow.org/api_docs/python/tf/math/polyval: better look in code block instead of plain text", "comments": []}]