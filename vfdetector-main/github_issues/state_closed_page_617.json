[{"number": 35139, "title": "Fix build breaks for S3 unit test", "body": "\r\nThis PR tries to address the issue raised in #35040 where\r\nS3 unit test failed due to `gtl::string_as_array`.\r\n\r\nThis PR change `string_as_array` into getting the pointter of\r\nthe first char of the string. By std standard, `std::string`'s\r\nmemory is continuous so &s[0] will give the right memory location.\r\n\r\nThis PR fixes #35040.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 35138, "title": "Replacement for experimental_run_tf_function after removal from tf.keras.Model.compile ", "body": "It looks like `experimental_run_tf_function` was removed from `tf.keras.Model.compile` in this commit a few days ago: https://github.com/tensorflow/tensorflow/commit/c73c99ca3e0bacf2bca313f270bb3eae28869530#diff-de9b96ac2d81503324cbbbe21732031fR1159\r\n\r\nIn [Horovod](http://horovod.ai/), this flag / graph mode is necessary in order for `Optimizer.get_gradients()` to be called, which aggregates gradients across workers.  Since this flag has been removed, distributed training in Horovod with `tf.keras` is not working in our nightly builds.\r\n\r\nIs there a workaround to achieve the same behavior with the latest changes on master?\r\n\r\nNote that we cannot perform the allreduce aggregation in `apply_gradients` due to interactions with gradient clipping and loss scaling (see https://github.com/horovod/horovod/pull/1347).\r\n", "comments": ["@tgaddair I notice your PR is already merged. Do we still need to keep this open? Thanks!", "Hey @jvishnuvardhan, thanks for the response.  That PR was only to pin our integration tests to an older version of `tf-nightly`.  We are still looking for a long-term fix that will make Horovod compatible with TensorFlow 2.1.0.", "cc @martinwicke, @alsrgv was telling me you might have some thoughts on this as well.  Thanks.", "@tanzhenyu or @robieta would know more details.", "Hey @tanzhenyu and @robieta, any thoughts on this?  Currently we're unable to support TensorFlow 2.1 effectively without this.  But hopefully there's a workaround.", "@tgaddair In my case, `experimental_run_tf_function` still exits as an argument of the `compile` method in TF 2.1 (and I've been using this flag for some months with TF 2.0 and now with TF 2.1), so I don't get this issue.", "@nbro the commit (https://github.com/tensorflow/tensorflow/commit/c73c99ca3e0bacf2bca313f270bb3eae28869530) was made before the release of TF 2.1, but was not included in the TF 2.1 branch, and as such will not go into effect until TF 2.2.", "@tgaddair I am currently using `experimental_run_tf_function` in TF 2.1.", "@nbro I think you misunderstood me.  I'm saying that this functionality will be dropped in TF 2.2, not 2.1.  In the original issue, I assumed it would be in effect in 2.1 because the change was made before the 2.1 release, but the change was never merged into the 2.1 branch.  Does that make sense?\r\n\r\nSee this discussion for more details: https://github.com/tensorflow/tensorflow/issues/36398", "@tgaddair Yes, now it makes sense. ", "2.2 release is on its way, I guess the solution from #37765 (edit: actually #36398) will work.\r\n\r\nSome analysis (still using 2.1):   \r\nThe same issue exists not only with `model.fit` but also with `model.train_on_batch` (which is ultimately called by `fit`) and also occurs when using `tf.config.experimental_run_functions_eagerly(True)` to see what is going on which (despite `experiment_run_tf_function` being set) uses the same method which does NOT call `get_gradients`\r\n\r\nIMO the underlying problem seems to be in the duality of the methods used.\r\n\r\n- Method A uses `optimizer.get_updates()` which calls `get_gradients` and `apply_gradients`\r\n- Method B uses `GradientTape` and calls `apply_gradients`\r\n\r\nHaving multiple seemingly equivalent ways of doing the same thing always complicates (or even breaks) things.\r\n\r\nBut to move forward: (e.g.) Horovod has a `DistributedGradientTape` which works when `get_gradients` is not invoked.\r\n\r\nQuestion: Why is there a `get_gradients` in the optimizer? Isn't that the task for e.g. GradientTape?\r\n\r\n@tgaddair Correct me if I'm mistaken but I think a good way to solve this is using the plain Optimizer (not the Horovod DistributedOptimizer) and tell TensorFlow to use Horovods DistributedGradientTape.\r\n\r\nSo proposal:\r\n\r\nShort-term: Allow users to pass in an optional GradientTape to model.compile or make the training loop query the optimizer to create one.\r\n\r\nLong-term: Clarify why there are at least 2 ways to compute the gradients and decide for one to use. Support that properly. Obviously my call would be on GradientTape to split responsibilities properly.", "I'm not familiar with all, the issues here.\r\n\r\nBut,  have you seen that `train_step` is now a overloadable in keras models: https://twitter.com/fchollet/status/1250622989541838848\r\n\r\nThat lets you affect how the model calculates the gradients. Does that help this use case? \r\n\r\n", "Not really. Still to high-level. The goal is to only replace the way TF gathers gradients. Not change the whole training algorithm", "Hey @Flamefire, following #36398 Horovod will work correctly when calling `model.fit()` with a `hvd.DistributedOptimizer`, so at least for now the problem is resolved.\r\n\r\nBut I agree that there is an API problem where we have two ways of doing the same thing that needs to be addressed.  \r\n\r\nOne solution that's been proposed has been to add an `optimizer.minimize()` function where we could wrap all of this, eliminating the need for the `DistributedGradientTape` (which I prefer, because it is more consistent with every other API we support including PyTorch, MXNet, and TensorFlow v1).\r\n\r\nThe other solution is, as you suggest, to use `DistributedGradientTape` with a plain optimizer.  This is what we do when not using Keras for TensorFlow v2 (see: [tensorflow_mnist.py](https://github.com/horovod/horovod/blob/master/examples/tensorflow2_mnist.py#L64)).  I think it would also be reasonable to use this in place of `optimizer._aggregate_gradients()`, but it requires that TensorFlow allow us to override/inject the gradient tape when using `model.fit()` (not currently supported).\r\n\r\nAs to why `optimizer.get_gradients` exists, it's because that's what was used in TensorFlow v1, and Horovod maintains compatibility across both v1 and v2 using the same `DistributedOptimizer`.", "I think you misunderstood some parts. [optimizer.get_gradients](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#get_gradients) exists in TF. It gets used in 2.1 when `experimental_run_tf_function=False` while `GradientTape` is used for `True`. This is the cause of this whole issue and the current workaround in Horovod (disabling `experimental_run_tf_function`)\r\n\r\n`optimizer.minimize` exists too. It calls `_compute_gradients` which does use `GradientTape` in yet another way to compute the gradients. This makes it 3 ways...\r\n\r\nHence my proposal to unify the whole training stuff in TF so there is exactly one way and place things are done. This can then be extended to provide customization points. Example: Provide a custom GradientTape which would eliminate the need for the DistributedOptimizer.\r\n\r\nI don't think Optimizer should have a method for gathering gradients. An optimizer is a thing like SGD, Adam, ... which defines how gradients are applied. Also the LossScalingOptimizer makes sense. But the DistributedOptimizer is a hack IMO.", "Hey @Flamefire, I think we're on the same page here.  Essentially, these different ways of doing the same thing need to be unified.  The only question is whether it should be done via a custom gradient tape or custom hooks into the optimizer.  Or put differently, how much of the training loop should be managed as internals of the optimizer.\r\n\r\nI don't have a strong preference either way, so long as TensorFlow chooses to be consistent going forward (which has historically been an issue, as you've pointed out with the three different ways of doing the same thing).  I like keeping DistributedOptimizer because it doesn't require Horovod users to relearn the API for TensorFlow 2, but that's secondary to consistency and consolidation.", "What is the actual solution for this? I see the TF2 Keras example in the docs still uses `experimental_run_tf_function=False`, but is the recommended solution for TF>=2.2 to override model `train_step` (which I am doing anyway) and use `DistributedGradientTape` with a plain optimizer?", "Hey @relativeflux, the TF2 Keras example uses `experimental_run_tf_funcion` for backwards compatibility with older versions of TF2. However, with versions >= 2.2, you should be able to use `model.fit()` without it. The `DistributedGradientTape` approach is also fine if you aren't using `model.fit()`.", "@tgaddair Excellent, that's good to know.", "@tgaddair,\r\nWith respect to [your comment](https://github.com/tensorflow/tensorflow/issues/35138#issuecomment-789962430), can you please let us know if we can close this issue? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 35137, "title": "clone_model changing UpSampling2D interpolation from bilinear to nearest neighbor", "body": "**System information**\r\nWindows 10 (Also happens on Ubuntu 18.04)\r\nInstalled from binary\r\nTensorflow-GPU 2.0.0b1\r\nCUDA 10.0 / CuDNN 7.6\r\nPython 3.6.8\r\nGTX 1060 6GB (also GTX 1080 8GB)\r\n\r\nWhen cloning a model using the clone_model function (AND model_from_json), upsampling interpolation switches from bilinear to nearest neighbor.\r\n\r\nHere's some simple code which replicates the issue:\r\n\r\n```\r\nfrom tensorflow.keras.models import *\r\nfrom tensorflow.keras.layers import *\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\n#Make bilinear upsampling model\r\nx = Sequential([UpSampling2D(8, interpolation = 'bilinear', input_shape = [5, 5, 3])])\r\n\r\n#Clone model\r\ny = clone_model(x)\r\n\r\n#Get noise image\r\nz = np.random.uniform(0.0, 1.0, [1, 5, 5, 3])\r\n\r\n#X's upsampling\r\nplt.figure(1)\r\nplt.imshow(x.predict(z)[0])\r\n\r\n#Y's upsampling\r\nplt.figure(2)\r\nplt.imshow(y.predict(z)[0])\r\n\r\nplt.show()\r\n```\r\n\r\nWhich outputs these two figures:\r\n![Figure_1](https://user-images.githubusercontent.com/8140937/70866792-a3325780-1f33-11ea-99e1-53eff1a8f015.png)\r\n![Figure_2](https://user-images.githubusercontent.com/8140937/70866791-a3325780-1f33-11ea-893c-fe95fafe8f0f.png)\r\n\r\n", "comments": ["@manicman1999, I tried with Tensorflow-gpu 2.0 it outputs as expected. Please find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/15cb7ad955be9cd66dfc115c03f5eb3a/untitled309.ipynb). Thanks!"]}, {"number": 35136, "title": "Typo on doc", "body": "Line 194 is missing the square bracket:\r\n\r\n- **wrong**\r\n`Use distribution to create a linear combination of value with shape batch_size, Tq, dim]:`\r\n\r\n\r\n- **correct**\r\n`Use distribution to create a linear combination of value with shape [batch_size, Tq, dim]:`\r\n\r\nLink to the line code:\r\nhttps://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/dense_attention.py#L194\r\n", "comments": ["@guglielmocamporese Are you willing to open a PR to fix the doc?"]}, {"number": 35135, "title": "dataset's map operation performance is O(N) with data shape for O(1) operations, can be orders of magnitude slower than using a dataset from generator.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 18.04, Windows 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\ndataset.map takes 10 times longer when getting 10 times bigger tensors in the dataset, regardless of the function being passed. e.g. `data.map(lambda x: x[0, 0])` would take 100 times longer, if `data` contained tensors of shape `(1000, 1000)` rather than ` (100, 100)`.\r\n\r\n**Describe the expected behavior**\r\nI would expect that if the mapped function has an O(1) performance, mapping it on any dataset would still have O(1) performance not O(N).\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\ntimeit.timeit(\"\"\"\r\ntf.reduce_sum([x for x in data.map(lambda x: x[:10, :10]).repeat().take(1000)])\r\n\"\"\",\r\nsetup=\"\"\"\r\nimport tensorflow as tf\r\ntf.autograph.set_verbosity(0, False)\r\nx = tf.ones((1, 10000,1000))\r\ndata = tf.data.Dataset.from_tensor_slices([x])\r\n\"\"\", number=10)\r\n```\r\ntakes 10 times longer than \r\n\r\n```\r\ntimeit.timeit(\"\"\"\r\ntf.reduce_sum([x for x in data.map(lambda x: x[:10, :10]).repeat().take(1000)])\r\n\"\"\",\r\nsetup=\"\"\"\r\nimport tensorflow as tf\r\ntf.autograph.set_verbosity(0, False)\r\nx = tf.ones((1, 1000,1000))\r\ndata = tf.data.Dataset.from_tensor_slices([x])\r\n\"\"\", number=10)\r\n```\r\n\r\nWhere the only difference is x having 10 times the values in the first example.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["For comparison, \r\n```\r\ntimeit.timeit(\"\"\"\r\ntf.reduce_sum([x for x in data.take(1000)])\r\n\"\"\",\r\nsetup=\"\"\"\r\nimport tensorflow as tf\r\ndef generator(x):\r\n  def gen():\r\n    yield x[:10, :10]\r\n  return gen\r\nx = tf.ones((1000,10000), dtype=tf.float32)\r\ndata = tf.data.Dataset.from_generator(generator(x), tf.float32)\r\n\"\"\", number=10)\r\n```\r\nruns 1000 times faster than the equivalent using map, as written in my initial example. ", "Able to reproduce the issue. Please find the [Gist](https://colab.sandbox.google.com/gist/amahendrakar/16937cdaede037dd39886d1cf07f0f9d/35135.ipynb) here. Thanks!", "Your benchmark is not measuring the speed of `map` execution accurately. `from_tensor_slices` performs a data copy so executing your input pipeline with inputs of different size will result in different amount of data being copied (which would on its own explain the O(1) vs O(n) difference).\r\n\r\nA better way to test whether `map` is O(1) (which it should be for constant-time operations) vs. O(n) is to use `tf.data.Dataset.from_tensors(X).repeat()` varying the size of `X` in your setup.\r\n\r\nNow as for the `from_generator` comparison, Python-based `from_generator` is expected to perform better than tf.data `map` (which incurs about 10us overhead per element to execute the user-defined computation through TensorFlow executor). The benefits of using `map` are realized when you are performing non-trivial computation that can benefit from either the graph optimizations TensorFlow performs and / or parallel execution enabled through the `num_parallel_calls` argument.", "@jsimsa `from_tensor_slices ` is performed in the setup, and isn't counted in the timing loop.. adding .repeat() to it on setup doesn't change the result.\r\n\r\nThis is a toy example, to make it clear that the problem is with the .map operation. \r\n\r\nI've started off with a training of a simple network comparing different sized tensors which are randomly cropped, before I saw that the problem was in .map, I thought it was the random_crop which was the culprit, but I've replaced it with a simple slicing operation, that left .map as the only explanation. The performance in training is proportional to the tensor size, which should not be the case. See [issue #34936 ](https://github.com/tensorflow/tensorflow/issues/34936). \r\n\r\nYou can check this [gist](https://colab.research.google.com/gist/feature-engineer/e5234588ecb1abb02ec05cdecfd18995/34936.ipynb#scrollTo=hFAR84t1XPQY) for the training of a toy network using different sized initial tensors which are cropped to the same size. One is 100 times slower than the second per training step, and the only difference is the size of the initial tensor.\r\n\r\nIf it was just the initial overhead of copying the tensors once, the training steps should have still taken the same time, but training on bigger initial tensors is linear in that initial tensor's size, when the network input is the same size.\r\n\r\nAs for comparing to from_generator, I understand there's a small overhead to using map, but this overhead shouldn't be dependant on the input sizes, and in my example, it is. \r\n", "Your setup only builds the input pipeline graph. The `from_tensor_slices` data copy is performed in each call to `get_next` which only happens in your `tf.reduce_sum`. The following [presentation](https://www.youtube.com/watch?v=kVEOCfBy9uY) explains in detail how tf.data input pipeline construction and execution works.\r\n\r\nYour `gist` is still using `from_tensor_slices`. Can you reproduce the issue by using `from_tensors` with differently sized inputs?", "Ok, from_tensors does indeed perform with O(1), while from_tensor_slices has an O(N) performance.\r\n\r\nIs this the intended behaviour? If so, it should be mentioned in the documentation, as this is a very significant difference between these otherwise quite similar methods.", "I am generally opposed to documenting implementation details (e.g. which methods are implementated using a data copy and which are not).\r\n\r\n@aaudiber what are your thoughts on this? This is related to our discussion of deprecating `from_tensor_slices()` in favor of `from_tensors().unbatch()` -- if we do that, should we document that `unbatch()` performs a data copy?", "I am also generally opposed to exposing implementation details since we don't want people to rely on the specific implementation and then get in trouble when the implementation changes.\r\n\r\nIn cases where the performance is worse than users might expect, I think it's worth including a note along the lines of \r\n\r\nNote: `unbatch()` performs a copy of the input data, so it can be expensive. Try to avoid unnecessary calls to `unbatch()`.\r\n\r\nIf we update the implementation to avoid the copy in the future, we can update the doc and (hopefully) no one will complain that we've saved a copy.", "So do you think we should update the documentation of all tf.data transformations that currently perform a data copy (`batch`, `padded_batch`, `from_tensor_slices`, `unbatch`, ...)?", "I don't think calling out the data copy is important for `batch`, `padded_batch`, or `from_tensor_slices` because those copies usually aren't the root cause of performance issues.  But we have had issues where liberal use of `unbatch` causes performance issues, so we should call out the performance considerations.", "Well, if you've got big images you want to semantically segment, you could be tempted to use `from_tensor_slices` and map to random_crop. \r\n\r\nThat was my scenario. I wasn't aware that it'd be so inefficient when compared to `from_tensors`, since it wasn't mentioned anywhere. I searched the slowdown in other places until I was told here that it's `from_tensor_slices` that is to blame. \r\n\r\nI don't see why anyone would want to use `from_tensor_slices` when it copies the input tensors on each iteration, or why it's the version more commonly used in the examples and guides.\r\n\r\nIn my opinion, either get rid of `from_tensor_slices` completely, or add a note regarding the aberrant performance - correct me if I'm wrong, but no other method of dataset creation entails a copy of its input on each iteration. ", "@aaudiber `from_tensor_slices` is not that different from `from_tensors` followed by `unbatch`. For datasets materialized into memory (e.g. as a NumPy array), `from_tensor_slices` is commonly used in TF tutorials as an example to convert these into a dataset and this will be suboptimal (c.f. this issue).\r\n\r\n@feature-engineer `from_tensor_slices` represents a \"ease-of-use vs. performance\" trade-off and TF tutorials optimize for simplicity over performance. In many cases, the overhead of the data copy is negligible compared to the per-element pre-processing computation. For best performance, not only you want to avoid / minimize data copies and vectorize and parallelize processing. An example of a performant and complex use of tf.data for handling NumPy objects can be found in [keras](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/data_adapter.py#L215) which converts NumPy objects to a tf.data.Dataset that shuffles and batches the original data (in a way that outperforms doing the same in NumPy).\r\n\r\nAs a side note, is there a reason you are materializing your images in memory outside of tf.data as opposed to reading the images (presumably from files) using tf.data ([example](https://www.tensorflow.org/guide/data#consuming_sets_of_files))?", "@jsimsa I get my training data from a stream over the network, so it's already loaded to memory.. If it's too big to include in the graph, I create a temporary TFRecord out of it, and build a dataset from that. But if the training data is small enough, I skip the TFRecord creation and create the dataset straight from the in-memory data. It's a semantic segmentation model, so the images are big, and I randomly crop them during training (after randomly selecting the validation data and masking it out).", "Thank you for the explanation. In the case the data is small and present in memory, you can use `from_generator` to avoid the `from_tensor_slices` data copy.\r\n\r\nI am going to close this issue as the original reason for opening (O(n) time complexity of `map`) has been refuted and the O(n) time complexity of `from_tensor_slices` is expected. \r\n\r\n@aaudiber as per our offline discussion, it makes sense to comment on the `from_tensor_slices` and `unbatch` performance in either the API documentation of the tf.data [performance guide](https://www.tensorflow.org/guide/data_performance).\r\n\r\n@feature-engineer feel free to create a PR updating the performance guide [source](https://github.com/tensorflow/docs/blob/master/site/en/guide/data_performance.ipynb) with information that would help others with a similar use case. Thank you. "]}, {"number": 35134, "title": "how to use different convolutional filter for each data in a minibatch", "body": "Hi,\r\nI want to use a different convolutional filter for each data in a minibatch, that's said:\r\nif the input images have shape: [batch_size, height, width, channel], and I want to use different filters for each image in the minibatch, so the filter shape is: [batch_size, f_width, f_height, in_channel, out_channel].  So each image in the minibatch corresponds to different filters. \r\n\r\nI try to solve it use tf.nn.depthwise_conv2d, but I cannot guarantee it is used in correct way. So I want to know there is some API designed for this situation?\r\n\r\nIf my statement is unclear, here is a link to the question: https://stackoverflow.com/questions/42068999/tensorflow-convolutions-with-different-filter-for-each-sample-in-the-mini-batch", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n"]}, {"number": 35133, "title": "Installing tensorflow go failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): n/a\r\n- TensorFlow version: n/a\r\n- Python version: n/a\r\n- Installed using virtualenv? pip? conda?: n/a\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\nFollowed https://www.tensorflow.org/install/lang_go\r\nbut got this:\r\ngo get github.com/tensorflow/tensorflow/tensorflow/go\r\npackage github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core\" in any of:\r\n\r\n\r\ngo get -d github.com/tensorflow/tensorflow/tensorflow/go\r\nyielded the same result.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I think there is a typo in your ```get``` command there isn't package word in the end.\r\nCan you try again with,\r\n```go get github.com/tensorflow/tensorflow/tensorflow/go```\r\nAlso did you setup https://www.tensorflow.org/install/lang_go#tensorflow_c_library before installing TF Go.", "hi, ymodak,\r\n\r\nThanks for the quick response!\r\n\r\nThe \"package\" is the starting of the output of the command.\r\n\r\nC library is installed. Here is how it works:\r\n$gcc hello_tf.c -ltensorflow -o hello_tf\r\n$./hello_tf \r\nHello from TensorFlow C library version 1.15.0\r\n\r\nI also tried on linux and the result is same.\r\n\r\nNeither does building from source work.\r\n\r\nI also checked github for \r\n\"github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core\". But it stopped at\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/go/genop/internal.\r\n\r\nIs the left part generated automatically? If so, how?\r\n\r\nCheers,\r\nFreeman", "Suspecting my system is in a bad state, I started a tensorflow container and tried to install in it. And it failed in the exact same way. I started to doubt that the whole thing is broken. Can someone spend 10 minutes to try to install it to confirm that it works? (on the other hand, it is really hard to believe that this is broken in such a way) search on google and most of result are from 2 years ago.", "Otherwise I have to switch to the tensorflow java.", "saw https://github.com/tensorflow/tensorflow/issues/23257. Not exactly the same issue. But the common part is that installing tensorflow for go fails. There should be an integration test for this.", "I have this same issue:\r\n\r\n```\u279c  tf gcc hello_tf.c -ltensorflow -o hello_tf\r\n\u279c  tf ./hello_tf\r\nHello from TensorFlow C library version 1.15.0\r\n\u279c  tf cd\r\n\u279c  ~ go get github.com/tensorflow/tensorflow/tensorflow/go\r\npackage github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core\" in any of:\r\n\t/usr/local/Cellar/go@1.12/1.12.13/libexec/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core (from $GOROOT)\r\n\t/Users/wschobeiri/go/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core (from $GOPATH)", "Checking out `36ef4264270e3815ea4b485a52e35e26e43e2112` per #34580 is a viable workaround. ", "Another workaround is Java. It is old and not as fun to work with as go.\nBut kotlin comes to help. Actually I found kotlin has a few features making\nit even more fun to work with than go, e.g., null handling, exception, and\nof course generic. Besides, I can use Gradle to manage packages and\ndependencies. Everything works smoothly so far.\n\nOn Fri, Dec 27, 2019, 2:11 AM Wilfried Schobeiri <notifications@github.com>\nwrote:\n\n> Checking out 36ef4264270e3815ea4b485a52e35e26e43e2112 per the other\n> thread is a viable workaround.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35133?email_source=notifications&email_token=AJKX36DILNVBE5HJCHOEYGTQ2TCQXA5CNFSM4J27VC6KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHVWBWQ#issuecomment-569073882>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJKX36HHN5EWLM34SGIS24DQ2TCQXANCNFSM4J27VC6A>\n> .\n>\n", "> Another workaround is Java. It is old and not as fun to work with as go. But kotlin comes to help. Actually I found kotlin has a few features making it even more fun to work with than go, e.g., null handling, exception, and of course generic. Besides, I can use Gradle to manage packages and dependencies. Everything works smoothly so far.\r\n> [\u2026](#)\r\n\r\nUnfortunately rewriting an entire application into another language is not the most convenient option..", "Yes, I just started. Tf go should have some basic integration test.\n\nOn Fri, Dec 27, 2019, 8:15 AM Wilfried Schobeiri <notifications@github.com>\nwrote:\n\n> Another workaround is Java. It is old and not as fun to work with as go.\n> But kotlin comes to help. Actually I found kotlin has a few features making\n> it even more fun to work with than go, e.g., null handling, exception, and\n> of course generic. Besides, I can use Gradle to manage packages and\n> dependencies. Everything works smoothly so far.\n> \u2026 <#m_7273184393122837671_>\n>\n> Unfortunately rewriting an entire application into another language is not\n> the most convenient option..\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35133?email_source=notifications&email_token=AJKX36AUQU7AQNOFSHS64R3Q2UNF7A5CNFSM4J27VC6KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHWE5JQ#issuecomment-569134758>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJKX36D2VQX5OHVE73LLQZLQ2UNF7ANCNFSM4J27VC6A>\n> .\n>\n", "Same issue with me....\r\n\r\ngo get github.com/tensorflow/tensorflow/tensorflow/go\r\npackage github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core\" in any of:\r\n\t/usr/local/Cellar/go/1.13.6/libexec/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core (from $GOROOT)\r\n\t/Users/mmussett/src/go/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core (from $GOPATH)\r\n\r\nTried go clean", "Same issue. C-API in place, tests OK. go 1.11:\r\n\r\ngo get github.com/tensorflow/tensorflow/tensorflow/go\r\npackage github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core\" in any of:\r\n\t/opt/go/1.11.2/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core (from $GOROOT)\r\n\t/home/martinme/Develop/go/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core (from $GOPATH)\r\n", "same here using docker.\r\n\r\n```\r\nSending build context to Docker daemon  11.26kB\r\n\r\nStep 1/6 : FROM golang:1.13\r\n ---> 272e3f68338f\r\n\r\nStep 2/6 : WORKDIR /go/src/app\r\n ---> Using cache\r\n ---> ab323f30415a\r\n\r\nStep 3/6 : COPY ./go .\r\n ---> Using cache\r\n ---> ceb815a86117\r\n\r\nStep 4/6 : RUN go get -d -v github.com/tensorflow/tensorflow/tensorflow/go\r\n ---> Running in 366c170d1ab3\r\ngithub.com/tensorflow/tensorflow (download)\r\ngithub.com/golang/protobuf (download)\r\npackage github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core\" in any of:\r\n        /usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core (from $GOROOT)\r\n        /go/src/github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core (from $GOPATH)\r\nThe command '/bin/sh -c go get -d -v github.com/tensorflow/tensorflow/tensorflow/go' returned a non-zero code: 1\r\n```\r\n\r\ndockerfile:\r\n```\r\nFROM golang:1.13\r\n\r\nWORKDIR /go/src/app\r\nCOPY ./go .\r\n\r\nRUN go get -d -v github.com/tensorflow/tensorflow/tensorflow/go\r\nRUN go install -v github.com/tensorflow/tensorflow/tensorflow/go\r\n\r\nCMD [\"app\"]\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35133\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35133\">No</a>\n", "Reopening since there's still an issue. Will resolve tomorrow.", "Fixed as of https://github.com/tensorflow/tensorflow/commit/4221d1aa4d20ada495771528bb13ca786d0bdbe0", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35133\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35133\">No</a>\n", "(As a sidenote, please follow the instructions on the Go README which differs slightly from the documentation on tensorflow.org. I'll work on updating the official docs):\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/README.md", "Issue is not fixed:\r\n\r\n`go get github.com/tensorflow/tensorflow/tensorflow/go\r\npackage github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\" in any of:\r\n\t/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto (from $GOROOT)\r\n       /Users/admin/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto (from $GOPATH)`\r\n", "@glarchev Please follow the instructions in the README:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/README.md\r\n\r\nYou need to use `go get -d` to download-only and then use go generate to create the protobuf files.", "The same issue:\r\n\r\n```\r\nRUN go get -d -u -v github.com/tensorflow/tensorflow/tensorflow/go\r\n ---> Running in 52496c3a3564\r\ngithub.com/tensorflow/tensorflow (download)\r\ncreated GOPATH=/root/go; see 'go help gopath'\r\ngithub.com/golang/protobuf (download)\r\npackage github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\" in any of:\r\n\t/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto (from $GOROOT)\r\n\t/root/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto (from $GOPATH)\r\nThe command '/bin/sh -c go get -d -u -v github.com/tensorflow/tensorflow/tensorflow/go' returned a non-zero code: 1\r\n```", "You can actually ignore that error for now and run:\r\n```\r\n$ go generate github.com/tensorflow/tensorflow/tensorflow/go/op\r\n$ go test github.com/tensorflow/tensorflow/tensorflow/go\r\nok  \tgithub.com/tensorflow/tensorflow/tensorflow/go\t0.182s\r\n```", "I cannot ignore it, because `go get -d github.com/tensorflow/tensorflow/tensorflow/go` fails with exit code 1, that interrupts build of Docker image.", "The installation steps in README also fail on the `./configure` step if you don't have bazel version 2.0.0 (I have 2.1.0).", "I know #36523 is still open and waiting for review, but I just wanted to bump.", "UPDATING as I switched machines in the middle, both were with different Go versions, both attempts failed.\r\n\r\n**Attempt#1:**\r\n\r\nWith\r\n```\r\n$ go version \r\ngo1.11.4 darwin/amd64\r\n```\r\nAnd\r\n```\r\n$ bazel version\r\nBuild label: 2.0.0\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Dec 19 12:33:30 2019 (1576758810)\r\nBuild timestamp: 1576758810\r\nBuild timestamp as int: 1576758810\r\n```\r\nInstalled this version by removing current bazel using brew and running\r\n`$ brew install https://raw.githubusercontent.com/bazelbuild/homebrew-tap/1bd39af61f9731fd03ba4dde408a0cb4198bc76d/Formula/bazel.rb`\r\n\r\nThen I tried running my code again:\r\n```\r\n$ go run main.go\r\ngo: finding module for package github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\r\n(in the first attempt:) go: downloading github.com/tensorflow/tensorflow v1.15.2\r\n../../../go/pkg/mod/github.com/tensorflow/tensorflow@v2.2.0+incompatible/tensorflow/go/saved_model.go:24:2: module github.com/tensorflow/tensorflow@latest found (v2.2.0+incompatible), but does not contain package github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\r\n```\r\n\r\nThen switched to following go [Go installation instructions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/README.md):\r\n```\r\n$ go get -d github.com/tensorflow/tensorflow/tensorflow/go\r\ngo: found github.com/tensorflow/tensorflow/tensorflow/go in github.com/tensorflow/tensorflow v2.2.0+incompatible\r\ngo: finding module for package github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\r\n```\r\n\r\n```\r\n$cd ${GOPATH}/src/github.com/tensorflow/tensorflow\r\n$./configure\r\nTraceback (most recent call last):\r\n  File \"./configure.py\", line 1549, in <module>\r\n    main()\r\n  File \"./configure.py\", line 1365, in main\r\n    _TF_MAX_BAZEL_VERSION)\r\n  File \"./configure.py\", line 478, in check_bazel_version\r\n    ['bazel', '--batch', '--bazelrc=/dev/null', 'version'])\r\n  File \"./configure.py\", line 156, in run_shell\r\n    output = subprocess.check_output(cmd)\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py\", line 573, in check_output\r\n    raise CalledProcessError(retcode, cmd, output=output)\r\nsubprocess.CalledProcessError: Command '['bazel', '--batch', '--bazelrc=/dev/null', 'version']' returned non-zero exit status 1\r\n```\r\n\r\nPeople in the thread seem to have succeeded with the bazel v2.0.0 but this doesn't seem to work for me. So bumping as well.\r\n\r\n---\r\n\r\n**Attempt#2:**\r\n\r\nSwitched to a machine with `go1.14.2 darwin/amd64` and downgraded bazel to v2.0.0 and got:\r\n```\r\n\r\n$ ./configure\r\nWARNING: current bazel installation is not a release version.\r\nMake sure you are running at least bazel 2.0.0\r\n...\r\n```\r\nThis succeeded, but the next command didn't:\r\n```\r\n$ bazel build -c opt //tensorflow:libtensorflow.so\r\nERROR: The project you're trying to build requires Bazel 3.0.0 (specified in $GOPATH/src/github.com/tensorflow/tensorflow/.bazelversion), but it wasn't found in /usr/local/bin/../Cellar/bazel/2.0.0/bin.\r\n```\r\n\r\nUpgraded to `bazel v3.0.0` and later to `bazel v3.1.0`, got the same error in both cases, seem to be rooted in `numpy`:\r\n```\r\n$ bazel build -c opt //tensorflow:libtensorflow.so\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=179\r\nINFO: Reading rc options for 'build' from $GOPATH/src/github.com/tensorflow/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from $GOPATH/src/github.com/tensorflow/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from $GOPATH/src/github.com/tensorflow/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/opt/python/bin/python3.7 --action_env PYTHON_LIB_PATH=/usr/local/Cellar/python/3.7.2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages --python_path=/usr/local/opt/python/bin/python3.7 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file $GOPATH/src/github.com/tensorflow/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file $GOPATH/src/github.com/tensorflow/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:macos in file $GOPATH/src/github.com/tensorflow/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at /private/var/tmp/_bazel_$USER/79ab9535b528f2bf6382aeb7c6f1eede/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - <builtin>\r\n - /private/var/tmp/_bazel_$USER/79ab9535b528f2bf6382aeb7c6f1eede/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - $GOPATH/src/github.com/tensorflow/tensorflow/WORKSPACE:37:1\r\nINFO: Call stack for the definition of repository 'local_execution_config_python' which is a local_python_configure (rule definition at $GOPATH/src/github.com/tensorflow/tensorflow/third_party/py/python_configure.bzl:275:26):\r\n - <builtin>\r\n - $GOPATH/src/github.com/tensorflow/tensorflow/third_party/toolchains/remote_config/rbe_config.bzl:158:5\r\n - $GOPATH/src/github.com/tensorflow/tensorflow/third_party/toolchains/remote_config/configs.bzl:6:5\r\n - $GOPATH/src/github.com/tensorflow/tensorflow/tensorflow/workspace.bzl:93:5\r\n - $GOPATH/src/github.com/tensorflow/tensorflow/WORKSPACE:19:1\r\nINFO: Call stack for the definition of repository 'local_config_python' which is a python_configure (rule definition at $GOPATH/src/github.com/tensorflow/tensorflow/third_party/py/python_configure.bzl:294:20):\r\n - <builtin>\r\n - $GOPATH/src/github.com/tensorflow/tensorflow/tensorflow/workspace.bzl:104:5\r\n - $GOPATH/src/github.com/tensorflow/tensorflow/WORKSPACE:19:1\r\nERROR: An error occurred during the fetch of repository 'local_execution_config_python':\r\n   Traceback (most recent call last):\r\n\tFile \"$GOPATH/src/github.com/tensorflow/tensorflow/third_party/py/python_configure.bzl\", line 213\r\n\t\t_get_numpy_include(<2 more arguments>)\r\n\tFile \"$GOPATH/src/github.com/tensorflow/tensorflow/third_party/py/python_configure.bzl\", line 187, in _get_numpy_include\r\n\t\texecute(repository_ctx, <3 more arguments>)\r\n\tFile \"$GOPATH/src/github.com/tensorflow/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n\t\tfail(<1 more arguments>)\r\nProblem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'numpy'\r\nIs numpy installed?\r\nInternal error thrown during build. Printing stack trace: java.lang.NullPointerException: //tensorflow:libtensorflow.so BuildConfigurationValue.Key[e92daf9dedbad5a840c1dcea19bb219b430534f835df513bcb8c6de1073f33ce] false\r\n\tat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:895)\r\n\tat com.google.devtools.build.skyframe.DelegatingWalkableGraph.getDirectDeps(DelegatingWalkableGraph.java:121)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.assertSaneAnalysisError(SkyframeBuildView.java:773)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.processErrors(SkyframeBuildView.java:616)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.configureTargets(SkyframeBuildView.java:454)\r\n\tat com.google.devtools.build.lib.analysis.BuildView.update(BuildView.java:404)\r\n\tat com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.runAnalysisPhase(AnalysisPhaseRunner.java:213)\r\n\tat com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.execute(AnalysisPhaseRunner.java:124)\r\n\tat com.google.devtools.build.lib.buildtool.BuildTool.buildTargets(BuildTool.java:145)\r\n\tat com.google.devtools.build.lib.buildtool.BuildTool.processRequest(BuildTool.java:290)\r\n\tat com.google.devtools.build.lib.runtime.commands.BuildCommand.exec(BuildCommand.java:95)\r\n\tat com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.execExclusively(BlazeCommandDispatcher.java:564)\r\n\tat com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.exec(BlazeCommandDispatcher.java:208)\r\n\tat com.google.devtools.build.lib.server.GrpcServerImpl.executeCommand(GrpcServerImpl.java:603)\r\n\tat com.google.devtools.build.lib.server.GrpcServerImpl.lambda$run$2(GrpcServerImpl.java:659)\r\n\tat io.grpc.Context$1.run(Context.java:595)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\n\r\nINFO: Elapsed time: 19.355s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (12 packages loaded, 17 targets configured)\r\n    Fetching @local_config_cc_toolchains; fetching\r\nInternal error thrown during build. Printing stack trace: java.lang.NullPointerException: //tensorflow:libtensorflow.so BuildConfigurationValue.Key[e92daf9dedbad5a840c1dcea19bb219b430534f835df513bcb8c6de1073f33ce] false\r\n\tat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:895)\r\n\tat com.google.devtools.build.skyframe.DelegatingWalkableGraph.getDirectDeps(DelegatingWalkableGraph.java:121)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.assertSaneAnalysisError(SkyframeBuildView.java:773)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.processErrors(SkyframeBuildView.java:616)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.configureTargets(SkyframeBuildView.java:454)\r\n\tat com.google.devtools.build.lib.analysis.BuildView.update(BuildView.java:404)\r\n\tat com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.runAnalysisPhase(AnalysisPhaseRunner.java:213)\r\n\tat com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.execute(AnalysisPhaseRunner.java:124)\r\n\tat com.google.devtools.build.lib.buildtool.BuildTool.buildTargets(BuildTool.java:145)\r\n\tat com.google.devtools.build.lib.buildtool.BuildTool.processRequest(BuildTool.java:290)\r\n\tat com.google.devtools.build.lib.runtime.commands.BuildCommand.exec(BuildCommand.java:95)\r\n\tat com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.execExclusively(BlazeCommandDispatcher.java:564)\r\n\tat com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.exec(BlazeCommandDispatcher.java:208)\r\n\tat com.google.devtools.build.lib.server.GrpcServerImpl.executeCommand(GrpcServerImpl.java:603)\r\n\tat com.google.devtools.build.lib.server.GrpcServerImpl.lambda$run$2(GrpcServerImpl.java:659)\r\n\tat io.grpc.Context$1.run(Context.java:595)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\njava.lang.NullPointerException: //tensorflow:libtensorflow.so BuildConfigurationValue.Key[e92daf9dedbad5a840c1dcea19bb219b430534f835df513bcb8c6de1073f33ce] false\r\n\tat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:895)\r\n\tat com.google.devtools.build.skyframe.DelegatingWalkableGraph.getDirectDeps(DelegatingWalkableGraph.java:121)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.assertSaneAnalysisError(SkyframeBuildView.java:773)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.processErrors(SkyframeBuildView.java:616)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.configureTargets(SkyframeBuildView.java:454)\r\n\tat com.google.devtools.build.lib.analysis.BuildView.update(BuildView.java:404)\r\n\tat com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.runAnalysisPhase(AnalysisPhaseRunner.java:213)\r\n\tat com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.execute(AnalysisPhaseRunner.java:124)\r\n\tat com.google.devtools.build.lib.buildtool.BuildTool.buildTargets(BuildTool.java:145)\r\n\tat com.google.devtools.build.lib.buildtool.BuildTool.processRequest(BuildTool.java:290)\r\n\tat com.google.devtools.build.lib.runtime.commands.BuildCommand.exec(BuildCommand.java:95)\r\n\tat com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.execExclusively(BlazeCommandDispatcher.java:564)\r\n\tat com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.exec(BlazeCommandDispatcher.java:208)\r\n\tat com.google.devtools.build.lib.server.GrpcServerImpl.executeCommand(GrpcServerImpl.java:603)\r\n\tat com.google.devtools.build.lib.server.GrpcServerImpl.lambda$run$2(GrpcServerImpl.java:659)\r\n\tat io.grpc.Context$1.run(Context.java:595)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\nFAILED: Build did NOT complete successfully (12 packages loaded, 17 targets configured)\r\n    Fetching @local_config_cc_toolchains; fetching\r\n\u279c  tensorflow git:(master) bazel build -c opt //tensorflow:libtensorflow.so\r\n\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=179\r\nINFO: Reading rc options for 'build' from $GOPATH/src/github.com/tensorflow/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from $GOPATH/src/github.com/tensorflow/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from $GOPATH/src/github.com/tensorflow/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/opt/python/bin/python3.7 --action_env PYTHON_LIB_PATH=/usr/local/Cellar/python/3.7.2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages --python_path=/usr/local/opt/python/bin/python3.7 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file $GOPATH/src/github.com/tensorflow/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file $GOPATH/src/github.com/tensorflow/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:macos in file $GOPATH/src/github.com/tensorflow/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nINFO: Call stack for the definition of repository 'local_config_python' which is a python_configure (rule definition at $GOPATH/src/github.com/tensorflow/tensorflow/third_party/py/python_configure.bzl:294:20):\r\n - <builtin>\r\n - $GOPATH/src/github.com/tensorflow/tensorflow/tensorflow/workspace.bzl:104:5\r\n - $GOPATH/src/github.com/tensorflow/tensorflow/WORKSPACE:19:1\r\nINFO: Call stack for the definition of repository 'local_execution_config_python' which is a local_python_configure (rule definition at $GOPATH/src/github.com/tensorflow/tensorflow/third_party/py/python_configure.bzl:275:26):\r\n - <builtin>\r\n - $GOPATH/src/github.com/tensorflow/tensorflow/third_party/toolchains/remote_config/rbe_config.bzl:158:5\r\n - $GOPATH/src/github.com/tensorflow/tensorflow/third_party/toolchains/remote_config/configs.bzl:6:5\r\n - $GOPATH/src/github.com/tensorflow/tensorflow/tensorflow/workspace.bzl:93:5\r\n - $GOPATH/src/github.com/tensorflow/tensorflow/WORKSPACE:19:1\r\nERROR: An error occurred during the fetch of repository 'local_execution_config_python':\r\n   Traceback (most recent call last):\r\n\tFile \"$GOPATH/src/github.com/tensorflow/tensorflow/third_party/py/python_configure.bzl\", line 213\r\n\t\t_get_numpy_include(<2 more arguments>)\r\n\tFile \"$GOPATH/src/github.com/tensorflow/tensorflow/third_party/py/python_configure.bzl\", line 187, in _get_numpy_include\r\n\t\texecute(repository_ctx, <3 more arguments>)\r\n\tFile \"$GOPATH/src/github.com/tensorflow/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n\t\tfail(<1 more arguments>)\r\nProblem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'numpy'\r\nIs numpy installed?\r\nInternal error thrown during build. Printing stack trace: java.lang.NullPointerException: //tensorflow:libtensorflow.so BuildConfigurationValue.Key[e92daf9dedbad5a840c1dcea19bb219b430534f835df513bcb8c6de1073f33ce] false\r\n\tat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:895)\r\n\tat com.google.devtools.build.skyframe.DelegatingWalkableGraph.getDirectDeps(DelegatingWalkableGraph.java:121)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.assertSaneAnalysisError(SkyframeBuildView.java:773)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.processErrors(SkyframeBuildView.java:616)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.configureTargets(SkyframeBuildView.java:454)\r\n\tat com.google.devtools.build.lib.analysis.BuildView.update(BuildView.java:404)\r\n\tat com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.runAnalysisPhase(AnalysisPhaseRunner.java:213)\r\n\tat com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.execute(AnalysisPhaseRunner.java:124)\r\n\tat com.google.devtools.build.lib.buildtool.BuildTool.buildTargets(BuildTool.java:145)\r\n\tat com.google.devtools.build.lib.buildtool.BuildTool.processRequest(BuildTool.java:290)\r\n\tat com.google.devtools.build.lib.runtime.commands.BuildCommand.exec(BuildCommand.java:95)\r\n\tat com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.execExclusively(BlazeCommandDispatcher.java:564)\r\n\tat com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.exec(BlazeCommandDispatcher.java:208)\r\n\tat com.google.devtools.build.lib.server.GrpcServerImpl.executeCommand(GrpcServerImpl.java:603)\r\n\tat com.google.devtools.build.lib.server.GrpcServerImpl.lambda$run$2(GrpcServerImpl.java:659)\r\n\tat io.grpc.Context$1.run(Context.java:595)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\n\r\nINFO: Elapsed time: 2.530s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (12 packages loaded, 17 targets configured)\r\n    Fetching @local_config_cc_toolchains; fetching\r\nInternal error thrown during build. Printing stack trace: java.lang.NullPointerException: //tensorflow:libtensorflow.so BuildConfigurationValue.Key[e92daf9dedbad5a840c1dcea19bb219b430534f835df513bcb8c6de1073f33ce] false\r\n\tat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:895)\r\n\tat com.google.devtools.build.skyframe.DelegatingWalkableGraph.getDirectDeps(DelegatingWalkableGraph.java:121)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.assertSaneAnalysisError(SkyframeBuildView.java:773)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.processErrors(SkyframeBuildView.java:616)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.configureTargets(SkyframeBuildView.java:454)\r\n\tat com.google.devtools.build.lib.analysis.BuildView.update(BuildView.java:404)\r\n\tat com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.runAnalysisPhase(AnalysisPhaseRunner.java:213)\r\n\tat com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.execute(AnalysisPhaseRunner.java:124)\r\n\tat com.google.devtools.build.lib.buildtool.BuildTool.buildTargets(BuildTool.java:145)\r\n\tat com.google.devtools.build.lib.buildtool.BuildTool.processRequest(BuildTool.java:290)\r\n\tat com.google.devtools.build.lib.runtime.commands.BuildCommand.exec(BuildCommand.java:95)\r\n\tat com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.execExclusively(BlazeCommandDispatcher.java:564)\r\n\tat com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.exec(BlazeCommandDispatcher.java:208)\r\n\tat com.google.devtools.build.lib.server.GrpcServerImpl.executeCommand(GrpcServerImpl.java:603)\r\n\tat com.google.devtools.build.lib.server.GrpcServerImpl.lambda$run$2(GrpcServerImpl.java:659)\r\n\tat io.grpc.Context$1.run(Context.java:595)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\njava.lang.NullPointerException: //tensorflow:libtensorflow.so BuildConfigurationValue.Key[e92daf9dedbad5a840c1dcea19bb219b430534f835df513bcb8c6de1073f33ce] false\r\n\tat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:895)\r\n\tat com.google.devtools.build.skyframe.DelegatingWalkableGraph.getDirectDeps(DelegatingWalkableGraph.java:121)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.assertSaneAnalysisError(SkyframeBuildView.java:773)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.processErrors(SkyframeBuildView.java:616)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeBuildView.configureTargets(SkyframeBuildView.java:454)\r\n\tat com.google.devtools.build.lib.analysis.BuildView.update(BuildView.java:404)\r\n\tat com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.runAnalysisPhase(AnalysisPhaseRunner.java:213)\r\n\tat com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.execute(AnalysisPhaseRunner.java:124)\r\n\tat com.google.devtools.build.lib.buildtool.BuildTool.buildTargets(BuildTool.java:145)\r\n\tat com.google.devtools.build.lib.buildtool.BuildTool.processRequest(BuildTool.java:290)\r\n\tat com.google.devtools.build.lib.runtime.commands.BuildCommand.exec(BuildCommand.java:95)\r\n\tat com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.execExclusively(BlazeCommandDispatcher.java:564)\r\n\tat com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.exec(BlazeCommandDispatcher.java:208)\r\n\tat com.google.devtools.build.lib.server.GrpcServerImpl.executeCommand(GrpcServerImpl.java:603)\r\n\tat com.google.devtools.build.lib.server.GrpcServerImpl.lambda$run$2(GrpcServerImpl.java:659)\r\n\tat io.grpc.Context$1.run(Context.java:595)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\nFAILED: Build did NOT complete successfully (12 packages loaded, 17 targets configured)\r\n    Fetching @local_config_cc_toolchains; fetching\r\n```", "you need to checkout to r1.11 branch, as explained [here](https://github.com/tensorflow/tensorflow/issues/23257#issuecomment-433751410), or you can install the master plus the latest version of the C library (1.15) and in your `go.mod` under `require` change the package version to `github.com/tensorflow/tensorflow v1.11.0` so that you wouldn't have to checkout to that branch every time you import the package. ", "Since this issue isn't getting fixed, I decided to maintain a fork: https://github.com/galeone/tensorflow\r\n\r\n```\r\ngo get github.com/galeone/tensorflow/tensorflow/go@r2.4-go\r\n```\r\n\r\nYou can also use [tfgo](https://github.com/galeone/tfgo) that depends on the fork and it allows a simplified usage of the Go bindings:\r\n\r\n```\r\ngo get github.com/galeone/tfgo\r\n```", "@galeone getting error:\r\n\r\n```\r\ngo: downloading github.com/galeone/tensorflow v2.4.0-rc0.0.20210202175351-640a390c2283+incompatible\r\n# github.com/galeone/tensorflow/tensorflow/go\r\n/usr/bin/ld: cannot find -ltensorflow\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\nwhen trying `go get github.com/galeone/tensorflow/tensorflow/go@r2.4-go`", "> @galeone getting error:\r\n> \r\n> ```\r\n> go: downloading github.com/galeone/tensorflow v2.4.0-rc0.0.20210202175351-640a390c2283+incompatible\r\n> # github.com/galeone/tensorflow/tensorflow/go\r\n> /usr/bin/ld: cannot find -ltensorflow\r\n> collect2: error: ld returned 1 exit status\r\n> ```\r\n> \r\n> when trying `go get github.com/galeone/tensorflow/tensorflow/go@r2.4-go`\r\n\r\nThis solution is old, I'm sorry.\r\n\r\nThe solution is to NOT use the official tensorflow go package, but instead use my fork:\r\n\r\n```\r\ngo env -w GONOSUMDB=\"github.com/galeone/tensorflow\"\r\n```\r\n\r\nObviously, you should fir install the TensorFlow C library in your system (see here: https://github.com/galeone/tfgo#tensorflow-installation)\r\n\r\nI highly recommend you to fallback on the galeone/tensorflow + galeone/tfgo solution, which is used by several go devs and it seems to work pretty well, solving all these problems.\r\n\r\nIn short:\r\n\r\n1. Install the TensorFlow C library\r\n2. go env -w GONOSUMDB=\"github.com/galeone/tensorflow\"\r\n3. go get github.com/galeone/tfgo\r\n\r\nAnd in your source code, instead of using the official tensorflow package, use \r\n\r\nimport (\r\n        \"fmt\"\r\n        tg \"github.com/galeone/tfgo\"\r\n        tf \"github.com/galeone/tensorflow/tensorflow/go\"\r\n)\r\n\r\nHave a look at the readme in the tfgo repo to find a working example and a step by step guidance: https://github.com/galeone/tfgo/\r\n\r\nLet me know if it works for you.", "is this correct output? @galeone\r\n\r\n```\r\n# github.com/galeone/tensorflow/tensorflow/go\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 857 (>= sh_info of 2)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 1944 (>= sh_info of 2)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2314 (>= sh_info of 2)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2502 (>= sh_info of 2)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2724 (>= sh_info of 2)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2725 (>= sh_info of 2)\r\ngo get: added github.com/galeone/tfgo v0.0.0-20210204182614-84b9a5e77f79\r\n```", "it seems to work, thanks so much @galeone ", "> it seems to work, thanks so much @galeone\r\n\r\nGreat! Have you fixed the linking problem you posted before?", "Also curious about this", "is work"]}, {"number": 35132, "title": "Why this simple code causes RAM memory leak? tf.shuffle seems not release memory.", "body": "**System information**\r\n- OS Platform and Distribution Linux Ubuntu 16.04\r\n- Python: 2.7.17 / 3.7.5\r\n- Tensorflow: 1.12.0 / 1.15.0\r\n- Numpy: 1.16.5\r\n- GPU: GeForce RTX 2080 Ti\r\n- CUDA: 9.2\r\n\r\n**Describe the current behavior**\r\nCPU memory gradually increase after each epoch until the program restarts, i suspect that dataset.shuffle doesn't release the buffer memory. Tested with tf 1.15, same situation.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport gc\r\nimport numpy as np\r\nimport tensorflow as tf\r\ndef datagenerator():\r\n    for i in range(250):\r\n        for j in range(100):\r\n            yield 'a','b',np.random.randn(300,120)\r\ndef f(s1,s2,feat):\r\n    return s1,s2,feat\r\ndataset = tf.data.Dataset.from_generator(datagenerator, (tf.string,tf.string,tf.float32))\r\ndataset = dataset.shuffle(20000)\r\ndataset = dataset.batch(200, drop_remainder=True)\r\ndataset = dataset.map(f)\r\ntest_iter = dataset.make_initializable_iterator()\r\ntest_next = test_iter.get_next()   \r\n\r\nrun_config = tf.ConfigProto()\r\nrun_config.gpu_options.allow_growth = True\r\nwith tf.Session(config=run_config) as sess:\r\n\r\n    for i in range(100):\r\n\r\n        sess.run(test_iter.initializer)\r\n        \r\n        while True:\r\n            try:\r\n                loss_list = sess.run([test_next])\r\n                print(len(loss_list[0]))\r\n            except tf.errors.OutOfRangeError:\r\n                print(\"train epoch %d finish\" % (i+1))\r\n                break\r\n        gc.collect()\r\n```\r\n\r\n", "comments": ["@kindernerd, Is this similar [#34942](https://github.com/tensorflow/tensorflow/issues/34942) issue.", "@kindernerd, Can you please let us know if you are happy to close if its duplicate of [#34942](https://github.com/tensorflow/tensorflow/issues/34942) issue.", "> @kindernerd, Can you please let us know if you are happy to close if its duplicate of [#34942](https://github.com/tensorflow/tensorflow/issues/34942) issue.\r\n\r\nok, sorry for the  duplication"]}, {"number": 35131, "title": "added usage example to pad_to_bounding_box()", "body": "I added a usage example to tf.image.pad_to_bounding_box() function. I tested it on colab so I know there aren't syntax errors.\ud83d\ude04\ud83d\udd96", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35131) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35131) for more info**.\n\n<!-- ok -->", "Done it!", "Done!", "@mihaimaruseac Do we use `tensor.eval()` to show the values? I tried it once, it gave me errors...", "No need to `tensor.eval()` (though it would have been useful to also see what errors you were getting).\r\n\r\nYou can just do something similar to\r\n\r\n```python\r\n>>> x = tf.constant([2,3,4])\r\n>>> y = tf.constant([4,5,6])\r\n>>> tf.add(x, y)\r\n<Tensor... [6,8,10]...>\r\n```\r\n\r\n(where I ellided a lot of details in the output, but you get the general idea)", "got it!", "@generationXcode thank you, it is still failing doctest can you please check here for [logs](https://source.cloud.google.com/results/invocations/383d702a-33c9-42f2-94a3-0038329704f1/targets/%2F%2Ftensorflow%2Ftools%2Fdocs:tf_doctest/tests).\r\n\r\nPlease run the doctest locally as mentioned here in the [contributor guidelines](https://www.tensorflow.org/community/contribute/docs_ref).\r\n\r\nAlso, can you please help to fix Ubuntu Sanity errors? Thanks!\r\n", "Hi, sorry for such a late reply (its my computer time now). Umm I don't know much about these checks but I am doing what you said and running the doctest locally, presently I'm downloading the tensorflow repo. From the invocations(is that what they're called?) I understood the error though, I'm fixing it now", "@generationXcode  it is still failing doctest can you please check here for [logs](https://source.cloud.google.com/results/invocations/4de04c11-2db2-48f6-8ced-b4847ba81db5/targets/%2F%2Ftensorflow%2Ftools%2Fdocs:tf_doctest/tests). Thanks!\r\n\r\n", "@gbaned is there any way we can run the doctest again? Thanks!", "Unfortunately we can only trigger all tests at once. Retriggered them now", "Thanks!", "it seems like the Ubuntu CPU test is contradicting the other results https://source.cloud.google.com/results/invocations/a46ee8e1-2790-41ad-8f78-216803471602/targets/%2F%2Ftensorflow%2Ftools%2Fdocs:tf_doctest/tests\r\nIf I solve this problem then the other tests will fail...", "Hi, I know this pull request isn't closed, but GCI is and I'd like to thank all of the mentors for helping me during the contest and also the reviewers of the pull requests I made.\ud83d\ude0a"]}, {"number": 35130, "title": "mnist data download failure", "body": "When I use \r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"/tmp/mnist/\", one_hot = True)\r\n\r\nIt seems that I cannot download the gz files from the googleapis? Error as below\r\nurllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)>\r\n\r\nThen I manually visit the source site, also error xml replied as below:\r\n<Error>\r\n<Code>AccessDenied</Code>\r\n<Message>Access denied.</Message>\r\n<Details>\r\nAnonymous caller does not have storage.objects.get access to cvdf-datasets/mnist.\r\n</Details>\r\n</Error>\r\n\r\nSo how can I fix the problem? Thanks a lot.", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nAlso, can you go through the [link1](https://github.com/tensorflow/tensorflow/issues/10779) , [link2 ](https://github.com/ageron/handson-ml/issues/59) and[ link3](https://github.com/tensorflow/models/issues/5871) and see if it helps you. Thanks!", "Sir\u00a0 \u00a0 \u00a0I think it's the problem for China only.\u00a0 I tried to manually download it by whether and everything went well already.\u00a0 So you may close the ticket and thank you for your support.\u00a0Best RegardRoger Yu\u53d1\u81ea\u6211\u7684\u534e\u4e3a\u624b\u673a-------- \u539f\u59cb\u90ae\u4ef6 --------\u53d1\u4ef6\u4eba\uff1a ravikyram <notifications@github.com>\u65e5\u671f\uff1a 2019\u5e7412\u670816\u65e5\u5468\u4e00 \u4e0b\u53482:17\u6536\u4ef6\u4eba\uff1a tensorflow/tensorflow <tensorflow@noreply.github.com>\u6284\u9001\uff1a rogeryuchao <18501790682@163.com>, Author <author@noreply.github.com>\u4e3b    \u9898\uff1a Re: [tensorflow/tensorflow] mnist data download failure (#35130)Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\nAlso, can you go through the link1 , link2  and link3 and see if it helps you. Thanks!\r\n\r\n\u2014You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.", "@rogeryuchao \r\n\r\nI am closing the issue   since the query is been resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35130\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35130\">No</a>\n"]}, {"number": 35129, "title": "Suggestion for tf.image.rgb to grayscale sample tutorial", "body": "", "comments": ["The notebook of the sample tutorial can be found [here](https://github.com/Mbah-Javis/How-to-convert-RGB-image-to-grayscale/blob/master/RBG_to_GrayScale_using_tf_image_rgb_to_grayscale.ipynb)"]}, {"number": 35128, "title": "Fix docstring of padded_batch", "body": "Convert padding_shapes into padded_shapes since it is\r\nthe argument name", "comments": []}, {"number": 35127, "title": "TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key \u2014 when trying to do dictionary mapping inside Dataset.map() function.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSx 10.14.5\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.0.0\r\n- **Python version**: 3.7.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: try to run the Dataset.map(test) function shown below on a Dataset in the formal of a tuple of two tensors (tensor, tensor).\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nThe Dataset.map() function will not allow for the use of dictionary mapping inside of it. It allows for addition/subtraction just fine, but you can't manipulate values inside of a tensor by mapping them to other values. I also tried to use the tf.map_fn() instead of doing it directly, which also returned the same error shown by the stack trace. \r\n\r\n### Source code / logs\r\n\r\nSource code: \r\n\r\nThe first get_mapped() function returns tuple of tensors in a Dataset object. Then, when trying to map to with the test() function, the error is produced. \r\n\r\n      `def test(self, t1, t2):\r\n             d = {0:1}\r\n        \r\n             t2 = d[t2]\r\n\r\n             return t1, t2\r\n\r\n         def redo(self): \r\n             mapped = self.get_mapped()\r\n             return mapped.map(self.test)`\r\n\r\n\r\nStack Trace:\r\n\r\n     `---------------------------------------------------------------------------\r\n      TypeError                                 Traceback (most recent call last)\r\n      <ipython-input-123-b629a3a02152> in <module>\r\n     ----> 1 a = q.redo()\r\n           2 \r\n           3 for i in a.take(1):\r\n           4     print(i)\r\n           5 \r\n\r\n     <ipython-input-121-502779dfc991> in redo(self)\r\n         147     def redo(self):\r\n         148         mapped = self.get_mapped()\r\n      -> 149         return mapped.map(self.test)\r\n         150 \r\n         151 \r\n\r\n     tensorflow_core/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls)\r\n        1209     \"\"\"\r\n        1210     if num_parallel_calls is None:\r\n     -> 1211       return MapDataset(self, map_func, preserve_cardinality=True)\r\n        1212     else:\r\n        1213       return ParallelMapDataset(\r\n\r\n     tensorflow_core/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, \r\n     use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\r\n        3414         self._transformation_name(),\r\n        3415         dataset=input_dataset,\r\n     -> 3416         use_legacy_function=use_legacy_function)\r\n        3417     variant_tensor = gen_dataset_ops.map_dataset(\r\n        3418         input_dataset._variant_tensor,  # pylint: disable=protected-access\r\n\r\n     tensorflow_core/python/data/ops/dataset_ops.py in __init__(self, func, transformation_name, \r\n     dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, \r\n     use_legacy_function, defun_kwargs)\r\n       2693       resource_tracker = tracking.ResourceTracker()\r\n        2694       with tracking.resource_tracker_scope(resource_tracker):\r\n     -> 2695         self._function = wrapper_fn._get_concrete_function_internal()\r\n        2696         if add_to_graph:\r\n        2697           self._function.add_to_graph(ops.get_default_graph())\r\n\r\n     tensorflow_core/python/eager/function.py in _get_concrete_function_internal(self, *args, \r\n     **kwargs)\r\n        1852     \"\"\"Bypasses error checking when getting a graph function.\"\"\"\r\n        1853     graph_function = self._get_concrete_function_internal_garbage_collected(\r\n     -> 1854         *args, **kwargs)\r\n        1855     # We're returning this concrete function to someone, and they may keep a\r\n        1856     # reference to the FuncGraph without keeping a reference to the\r\n\r\n     tensorflow_core/python/eager/function.py in . \r\n      _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n        1846     if self.input_signature:\r\n        1847       args, kwargs = None, None\r\n     -> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n        1849     return graph_function\r\n        1850 \r\n\r\n     tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n        2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n        2149         if graph_function is None:\r\n     -> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n        2151           self._function_cache.primary[cache_key] = graph_function\r\n        2152         return graph_function, args, kwargs\r\n\r\n     tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, \r\n     override_flat_arg_shapes)\r\n        2039             arg_names=arg_names,\r\n        2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n     -> 2041             capture_by_value=self._capture_by_value),\r\n        2042         self._function_attributes,\r\n        2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n     tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, \r\n     python_func, args, kwargs, signature, func_graph, autograph, autograph_options, \r\n     add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, \r\n     override_flat_arg_shapes)\r\n         913                                           converted_func)\r\n         914 \r\n     --> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n          916 \r\n         917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n     tensorflow_core/python/data/ops/dataset_ops.py in wrapper_fn(*args)\r\n        2687           attributes=defun_kwargs)\r\n        2688       def wrapper_fn(*args):  # pylint: disable=missing-docstring\r\n     -> 2689         ret = _wrapper_helper(*args)\r\n        2690         ret = structure.to_tensor_list(self._output_structure, ret)\r\n        2691         return [ops.convert_to_tensor(t) for t in ret]\r\n\r\n     tensorflow_core/python/data/ops/dataset_ops.py in _wrapper_helper(*args)\r\n        2632         nested_args = (nested_args,)\r\n        2633 \r\n     -> 2634       ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\r\n        2635       # If `func` returns a list of tensors, `nest.flatten()` and\r\n        2636       # `ops.convert_to_tensor()` would conspire to attempt to stack\r\n\r\n     tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n         235       except Exception as e:  # pylint:disable=broad-except\r\n         236         if hasattr(e, 'ag_error_metadata'):\r\n     --> 237           raise e.ag_error_metadata.to_exception(e)\r\n         238         else:\r\n         239           raise\r\n\r\n     TypeError: in converted code:\r\n\r\n         <ipython-input-121-502779dfc991>:145 test  *\r\n             t2 = d[t2]\r\n          venvtf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:713 __hash__\r\n             raise TypeError(\"Tensor is unhashable if Tensor equality is enabled. \"\r\n\r\n         TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use . \r\n    tensor.experimental_ref() as the key.`\r\n", "comments": ["@kristian1108 \r\n\r\nLooks like code is incomplete. Can you please help us with minimal standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I'm getting this too, on Colab using tf 2.1.0-dev20191231", "Hi all, sorry for delayed response. You just need to use the\ntf.lookup.StaticHashMap instead of python's native dictionary and it should\nwork just fine. It would be nice to be able to use the native dictionary,\nbut this was the workaround I used barring that.\n\nOn Tue, Dec 31, 2019, 22:51 shaunster0 <notifications@github.com> wrote:\n\n> I'm getting this too, on Colab using tf 2.1.0-dev20191231\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35127?email_source=notifications&email_token=AGKMU6AJQ4QFPC6DXHE6UXLQ3QHKTA5CNFSM4J26WH22YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEH44VAQ#issuecomment-570018434>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AGKMU6D4I22VHCN3QZDSJZLQ3QHKTANCNFSM4J26WH2Q>\n> .\n>\n", "@kristian1108 \r\n\r\nCan we close this issue since the query is been resolved.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35126, "title": "Usage example added to tf.image.transpose", "body": "A small usage example added to tf.image.transpose", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35126) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35126) for more info**.\n\n<!-- ok -->", "Hello I have made the changes", "I'm closing this PR since I've made the comment N times that you should use a small constant and not a random image and should make it a doctest.", "> I'm closing this PR since I've made the comment N times that you should use a small constant and not a random image and should make it a doctest.\r\n\r\nI have done that on this other \r\n[PR](https://github.com/tensorflow/tensorflow/pull/35345/files)"]}, {"number": 35125, "title": "lite/micro: Add feature buffer to micro_speech example.", "body": "This fixes #35117\r\n\r\nAccumulate feature slices in separate buffer.\r\nThe input tensor is not suitable for keeping state across interference\r\nas it has limited lifetime and the buffer space may be reused.", "comments": ["@petewarden Can you please take a look on this PR? Thanks!"]}, {"number": 35124, "title": "Poor performance of model.fit and/or model.predict in TF 2.1.0-rc1", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Kubuntu 18.04, kernel 5.0\r\n- Mobile device: Not verified on mobile devices\r\n- TensorFlow installed from: binary via `pip install tensorflow-gpu`\r\n- TensorFlow version: `2.1.0-rc1`, however affected are also `2.1.0-rc0`, `2.0.0`, `2.0.0-rc2`, `2.0.0-rc1` and `2.0.0-rc0`.\r\n- Python version: 3.6.9\r\n- CUDA version: 10.1 for TF 2.1.0-rc0; 10.0 for the earlier versions of TF\r\n- cuDNN version: 7\r\n- GPU model and memory: Nvidia GeForce GTX 1050 Ti (4GB)\r\n- CPU model: AMD Ryzen 7 1700\r\n\r\n**Describe the current behavior**\r\n\r\nThe code attached below works about 10x slower with TF `2.1.0-rc1` and `2.1.0-rc0` than with `1.14.0`. This code was posted earlier by @ipsec in the issue #33030 in connection with memory leaks in TF `2.0.0`. Now, in `2.1.0-rc*` the memory leaks seem fixed, but the execution of the code in question exhibits poor speed performance. Said that, the performance was even worse in `2.0.0` and pre-releases `2.0.0-rc*`, but the about 10x slowdown in `2.1.0-rc*` compared to `1.14.0` is significant.\r\n\r\nHere I provide some more results obtained on my device:\r\n\r\n* With TF `1.14.0`, one \"episode\", i.e. one iteration of the `for` loop in the below code, takes ca. 8.5-9.5 seconds in the graph execution mode and ca. 7-7.5 seconds with the eager execution enabled. The memory consumption of the Python process associated with this code is ca. 600 MB.\r\n\r\n* With TF from `2.0.0-a0` to `2.0.0-b1` I obtain very similar results.\r\n\r\n* With TF from `2.0.0-rc0` to `2.0.0` one episode takes ca. 120 seconds. There is also a memory leak. The memory usage after the 1st episode is ca 2 GB and is growing ca 1.5 GB per episode.\r\n\r\n* For TF `2.1.0-rc0` and `2.1.0-rc1` the memory usage is ca. 800 MB and doesn't grow significantly from episode to episode, so the memory leak is fixed. Note that this memory usage is higher than in `1.14.0` but the difference is plausible. Nevertheless, for `2.1.0-rc0` and `2.1.0-rc1` I have measured the execution time to be ca. 75 seconds per episode, so the code execution is still very slow compared to `1.14.0`.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe code attached below should be executed in `2.1.0` comparably fast to `1.14.0`. The difference in the execution time shouldn't be 10-fold.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThis code has already been posted by @ipsec in the issue #33030. Here I only add time measuremets, change verbosity details and reduce the default number of episodes.\r\n\r\n```python\r\nimport gym\r\nimport numpy as np\r\nimport matplotlib.pylab as plt\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\nimport time\r\n\r\nenv = gym.make('NChain-v0')\r\n\r\n\r\ndef q_learning_keras(env, num_episodes=10):\r\n    # create the keras model\r\n    model = tf.keras.Sequential()\r\n    model.add(layers.InputLayer(batch_input_shape=(1, 5)))\r\n    model.add(layers.Dense(10, activation='sigmoid'))\r\n    model.add(layers.Dense(2, activation='linear'))\r\n    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\r\n    # now execute the q learning\r\n    y = 0.95\r\n    eps = 0.5\r\n    decay_factor = 0.999\r\n    r_avg_list = []\r\n    for i in range(num_episodes):\r\n        s = env.reset()\r\n        eps *= decay_factor\r\n        #if i % 100 == 0:\r\n        print(\"Episode {} of {}\".format(i + 1, num_episodes))\r\n        t_start = time.time()\r\n        done = False\r\n        r_sum = 0\r\n        while not done:\r\n            if np.random.random() < eps:\r\n                a = np.random.randint(0, 2)\r\n            else:\r\n                a = np.argmax(model.predict(np.identity(5)[s:s + 1]))\r\n            new_s, r, done, _ = env.step(a)\r\n            target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))\r\n            target_vec = model.predict(np.identity(5)[s:s + 1])[0]\r\n            target_vec[a] = target\r\n            model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)\r\n            s = new_s\r\n            r_sum += r\r\n        print(\"Elapsed time: {}\".format(time.time()-t_start))\r\n        r_avg_list.append(r_sum / 1000)\r\n    plt.plot(r_avg_list)\r\n    plt.ylabel('Average reward per game')\r\n    plt.xlabel('Number of games')\r\n    plt.show()\r\n    for i in range(5):\r\n        print(\"State {} - action {}\".format(i, model.predict(np.identity(5)[i:i + 1])))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    q_learning_keras(env) \r\n```\r\n\r\n\r\n", "comments": ["It has to be your system problem as i ran the code on tf2.0 and it ran 1 epoch under 15 seconds which denies your 120 seconds time lapse.\r\n![Screenshot (1)](https://user-images.githubusercontent.com/46572301/70856376-8ed56700-1f01-11ea-8e3a-4d4a6d7a979c.png)\r\n", "@darsh8200, if it happens with official Tensorflow binaries running on an officially supported system, then it is a Tensorflow problem. But thanks for checking this. Would it be a problem for you to check also if you obtain similar results with TF 1.14?", "I Was able to reproduce the error for TF-2.0-rc1 and TF-nightly.Kindly find the [gist](https://colab.sandbox.google.com/gist/gowthamkpr/9f580a418519a218b7a9e30513b7c3bf/copy-of-35124.ipynb) of colab.Thanks!", "I've also noticed disastrous performance drop between TF2.1/2.0 and 1.14/1.15.\r\n\r\nIt's hard to compare pieces of code directly since,  2x advises very differnt style, which I dutifully followed.\r\n\r\nAnyways I've ported 3 vision models from legacy graph mode and with using the same input pipeline based on `tf.data.Dataset` the performance drop is goes from:\r\n1.5 - 2.9step/s  (1.14)\r\nto 0.3-0.5 step/s (2.1)\r\n\r\nI've tried:\r\na)  eager execution (worse performance)\r\nb) partial graph mode (one step of training with @tf.function)\r\nc) full in graph mode (whole training loop in @tf.function), yet it hard to use as you can't make summaries \r\n\r\nAt best the performance is 4 times worse. This is avergage figure for 3 separate models.", "If we take the above example and replace the following lines, the episode time drops from ~70 seconds to 5.5-6 seconds:\r\n\r\nReported:\r\n```\r\n...\r\na = np.argmax(model.predict(np.identity(5)[s:s + 1]))\r\n...\r\ntarget = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))\r\ntarget_vec = model.predict(np.identity(5)[s:s + 1])[0]\r\n...\r\nmodel.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)\r\n...\r\n```\r\n\r\nNew:\r\n```\r\n...\r\na = np.argmax(model.predict_on_batch(np.identity(5)[s:s + 1]))\r\n...\r\ntarget = r + y * np.max(model.predict_on_batch(np.identity(5)[new_s:new_s + 1]))\r\n\r\n# We call `.numpy()` because the next line performs a slice assignment on target_vec\r\ntarget_vec = model.predict_on_batch(np.identity(5)[s:s + 1])[0].numpy()\r\n...\r\nmodel.train_on_batch(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2))\r\n...\r\n```\r\n\r\nFull [gist](https://colab.sandbox.google.com/gist/robieta/580d214431312972d47542b122f44870/copy-of-35124.ipynb) indicates that the new result matches the old result.\r\n\r\nSo what's going on here, and why is there a slowdown from 1.14? There is a detailed discussion in https://github.com/tensorflow/tensorflow/issues/33340 (including my explanation starting at https://github.com/tensorflow/tensorflow/issues/33340#issuecomment-569458253), but the high level overview is that machinery was added to the \"epoch based\" endpoints (`fit/evaluate/predict`) which makes them scale better, but adds some fixed overhead. And when those endpoints are called on a single batch, those instantiation overheads aren't well amortized and performance suffers. There are single batch equivalents which are more suited for the workload that you're running, but they are unfortunately less well known. There is an open action item to add a warning when fit, evaluate, or predict is used on a single batch of data. @goldiegadde for tracking.", "Thank you for taking a look Taylor!", "I can confirm that. With the changes you suggest, @robieta, I get ~13 sec/episode with TF 2.0.0 and ~4.7-4.8 sec/episode for TF 2.1.0. So I think we may close the issue.\r\n\r\nThank you for your explanations and the link to the other issue. It's worth reading. I knew earlier that there were methods like `predict_on_batch` and `train_on_batch`in the Keras API but I didn't know what is their intended use.\r\n\r\nMaybe the main points of the discussion in #33340 or the high level overview that you present here could be included into the official guide? Correct me if I'm wrong but I cannot find `predict_on_batch` or `train_on_batch` even mentioned in the [tutorials](https://www.tensorflow.org/tutorials) or the [guide](https://www.tensorflow.org/guide). I'm having the impression that if one wants to really understand Tensorflow then it is necessary to study Github issues."]}, {"number": 35123, "title": "lite/micro: Fix bug in tensor lifetime calculation.", "body": "Fix for issue #35121\r\n\r\nThe tensor lifetime may be incorrectly calculated in MicroAllocator::FinishTensorAllocation() if the same sensor is used multiple times as inputs to different operations or is used as input/output or variable of the graph.", "comments": ["@petewarden : Looks easy enough to add more nodes to the MockModel and adjust existing tests that use it. \r\n\r\nI will open a new PR for the test code as I assume we want to observe if failing before this PR is pulled.", "Until someone comes to talk to me in person we done I have no idea who\nGoogle and you are up too\n\nOn Wed, Dec 18, 2019, 19:15 Stephan Uphoff <notifications@github.com> wrote:\n\n> @petewarden <https://github.com/petewarden> : Looks easy enough to add\n> more nodes to the MockModel and adjust existing tests that use it.\n>\n> I will open a new PR for the test code as I assume we want to observe if\n> failing before this PR is pulled.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/35123?email_source=notifications&email_token=AOBPXOQ6MVOY2XCTOP4ZM23QZK4KFA5CNFSM4J25DPH2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHH5PDY#issuecomment-567269263>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AOBPXOT23VYHHC3S2EVRMWDQZK4KFANCNFSM4J25DPHQ>\n> .\n>\n", "@petewarden : Tests are in #35294 ", "@suphoff  Can you please resolve conflicts? Thanks!", "@gbaned : Conflicts removed - conflicting changes only addressed a subset of the issues.  ", "@suphoff  Can you please resolve conflicts? Thanks!", "@njeffrie : Test is in #35294 (and should fail without this bug fix )", "Merged the allocation tests from #35294  into this pull request as requested by @petewarden "]}, {"number": 35122, "title": "undeclared inclusion(s) in rule '@nccl_archive//:device_lib'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux centos 6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0.0\r\n- Python version: \r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 5.4\r\n- CUDA/cuDNN version: 10.0.130/7.6.4\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n```\r\nERROR: /home/conda/.cache/bazel/_bazel_conda/6f8a02c56728b792c78882b32cc69e39/external/nccl_archive/BUILD.bazel:53:1: undeclared inclusion(s) in rule '@nccl_archive//:device_lib':\r\nthis rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/collectives/device/max_f64_reduce.cu.cc':\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/cuda_runtime.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/host_config.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/builtin_types.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/device_types.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/host_defines.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/driver_types.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/vector_types.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/surface_types.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/texture_types.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/library_types.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/channel_descriptor.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/cuda_runtime_api.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/cuda_device_runtime_api.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/driver_functions.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/vector_functions.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/vector_functions.hpp'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/common_functions.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/math_functions.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/math_functions.hpp'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/cuda_surface_types.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/cuda_texture_types.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/device_functions.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/device_functions.hpp'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/device_atomic_functions.h'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/device_atomic_functions.hpp'\r\n  '/usr/local/cuda-10.0/targets/x86_64-linux/include/crt/device_double_functions.h'\r\n\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```bash\r\nbazel build //tensorflow:libtensorflow_cc.so\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Inspired by #32776 and https://github.com/tensorflow/tensorflow/commit/58b236b235ce57808c917feb19b9f895f628710c, I apply the following patch to resolve symbolic links, and it works.\r\n```diff\r\ndiff --git a/third_party/gpus/cuda_configure.bzl b/third_party/gpus/cuda_configure.bzl\r\nindex af1bc96f00..2fd0bef286 100644\r\n--- a/third_party/gpus/cuda_configure.bzl\r\n+++ b/third_party/gpus/cuda_configure.bzl\r\n@@ -373,8 +373,8 @@ def _cuda_include_path(repository_ctx, cuda_config):\r\n             )\r\n     inc_entries = []\r\n     if target_dir != \"\":\r\n-        inc_entries.append(target_dir)\r\n-    inc_entries.append(cuda_config.cuda_toolkit_path + \"/include\")\r\n+        inc_entries.append(str(repository_ctx.path(target_dir).realpath))\r\n+    inc_entries.append(str(repository_ctx.path(cuda_config.cuda_toolkit_path + \"/include\").realpath))\r\n     return inc_entries\r\n\r\n def enable_cuda(repository_ctx):\r\ndiff --git a/third_party/gpus/find_cuda_config.py b/third_party/gpus/find_cuda_config.py\r\nindex 39f2c21d3f..57067b3562 100644\r\n--- a/third_party/gpus/find_cuda_config.py\r\n+++ b/third_party/gpus/find_cuda_config.py\r\n@@ -449,7 +449,7 @@ def find_cuda_config():\r\n   cuda_version = os.environ.get(\"TF_CUDA_VERSION\", \"\")\r\n   base_paths = _list_from_env(\"TF_CUDA_PATHS\",\r\n                               _get_default_cuda_paths(cuda_version))\r\n-  base_paths = [path for path in base_paths if os.path.exists(path)]\r\n+  base_paths = [os.path.realpath(path) for path in base_paths if os.path.exists(path)]\r\n\r\n   result = {}\r\n   if \"cuda\" in libraries:\r\n```\r\n\r\nI don't know why others don't have this problem (only saw @shubham769 reported the problem), but the above solution should make sense.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35122\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35122\">No</a>\n"]}, {"number": 35121, "title": "lite/micro: Tensor lifetime incorrectly calculated on multiple use", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): e12ba3de80d9315b7174037081adb482689bc6d6\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): all\r\n\r\n**Describe the problem**\r\n The tensor lifetime may be incorrectly calculated in MicroAllocator::FinishTensorAllocation() if the same sensor is used multiple times as inputs to different operations or is used as input/output or variable of the graph.\r\n\r\nThe relevant code section:\r\n```c\r\n // Figure out when the first and last use of each tensor is.\r\n  for (int i = (operators_->size() - 1); i >= 0; --i) {\r\n    const auto* op = operators_->Get(i);\r\n    for (size_t n = 0; n < op->inputs()->size(); ++n) {\r\n      const int tensor_index = op->inputs()->Get(n);\r\n      TensorInfo* current = &tensor_info[tensor_index];\r\n      if ((current->last_used == -1) || (current->last_used > i)) {\r\n        current->last_used = i;\r\n      }\r\n    }\r\n    for (size_t n = 0; n < op->outputs()->size(); ++n) {\r\n      const int tensor_index = op->outputs()->Get(n);\r\n      TensorInfo* current = &tensor_info[tensor_index];\r\n      if ((current->first_created == -1) || (current->first_created < i)) {\r\n        current->first_created = i;\r\n      }\r\n    }\r\n  }\r\n```\r\nLooks just like the condition to update a valid lifetime is accidentally inverted.\r\nWill verify and submit PR\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nCode review\r\n", "comments": ["Resolved by #35123", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35121\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35121\">No</a>\n"]}, {"number": 35120, "title": "Want to join tensorflow", "body": "Can I join tensorflow for contributions ", "comments": ["Yes you can join TensorFlow for contributions through correcting the documentation and fixing the issues or adding your extra inputs.", "> Yes you can join TensorFlow for contributions through correcting the documentation and fixing the issues or adding your extra inputs.\r\n\r\nOkay, I Want also to be a contributor :),\r\nI have begun by fixing some issues.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/34096#issuecomment-565770710", "Please help me out with it\r\n", "@KumarOjas \r\n\r\nYes you can join as a contributor to Tensorflow. Please, go through the [contibuting guidelines](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md).\r\n", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@KumarOjas \r\n\r\nAny update on this issue please. Thanks!", "This shouldn't be an issue as it is not something related to code not working as expected and is neither a feature request. Please consult [`CONTRIBUTING.md`](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md) and other similar files in the root of the repo."]}, {"number": 35119, "title": "Expose `assign_moving_average` via public API", "body": "Closes #35118 ", "comments": ["@robieta: Does it make sense to export this API?", "My inclination is not to, as it would extend the API surface and it's not clear that it justifies the maintenance cost. (We have lots of symbols already.) However we can ask api-owners. I definitely don't think it should be a top-level symbol; `tf.train.assign_moving_average` might be fine since `tf.train.ExponentialMovingAverage` already exists.\r\n\r\nBy the way, it's already (mostly) exported via `tf.keras.backend.moving_average_update`, so if you just need a public symbol will that suffice?", "@robieta `tf.keras.backend.moving_average_update` would be perfect, if `zero_debias` was a parameter. But it is currently set to `zero_debias = not tf2.enabled()`. ", "@robieta Any update on this?", "I think you'll have to bring it to TF API Owners."]}, {"number": 35118, "title": "Expose assign_moving_average via public API", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): `master`\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`assign_moving_average` does not have a public API (`tf.assign_moving_average`). \r\n\r\n**Will this change the current api? How?**\r\n`tf.assign_moving_average` instead of `tensorflow.python.training.moving_averages.assign_moving_average`\r\n\r\n**Who will benefit with this feature?**\r\n\r\nWe're using it in https://github.com/tensorflow/addons/pull/760, and having a public API would be super useful. \r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 35117, "title": "lite/micro: micro_speech example:  input tensor lifetime assumption invalid", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): e12ba3de80d9315b7174037081adb482689bc6d6\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): All\r\n\r\n**Describe the problem**\r\nThe feature provider accumulates feature slices using the input tensor in the arena as a buffer.\r\nHowever the lifetime of the input buffer is only the first operation of the model.\r\nAs such the feature buffer may be overwritten when the memory is reused for tensors with different lifetimes.\r\nThis is the case with the current model and the current greedy memory planner.\r\nAs only the front of the feature buffer is currently overwritten - and the front feature slice  is  never reused this does not currently impact the example.  \r\nHowever using the example as a base of more complex models would trigger this problem.\r\n\r\nI will submit a PR to add a buffer to the feature provider.\r\nAlternatively the model could be changed to pass the input through to the output. (To keep the tensor alive) \r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nCode review\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35117\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35117\">No</a>\n"]}, {"number": 35116, "title": "Cannot build raspberry pi wheel for python 3.7", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.1.0-rc1\r\n\r\n I'm building using the docker image for a raspberry pi 3 build.\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI'm able to correctly build `tensorflow-2.1.0rc1-cp35-none-linux_armv7l.whl` using:\r\n\r\n```\r\nCI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4\"     tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3     tensorflow/tools/ci_build/pi/build_raspberry_pi.sh \r\n```\r\nI'm trying to do the same but with the python 3.7 docker images `PI-PYTHON37` (`tensorflow/tools/ci_build/Dockerfile.pi-python37`) but it fails with:\r\n\r\n```\r\nERROR: /workspace/tensorflow/lite/python/interpreter_wrapper/BUILD:8:1: C++ compilation of rule '//tensorflow/lite/python/interpreter_wrapper:numpy' failed (Exit 1)\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nIn file included from bazel-out/armeabi-py2-opt/bin/external/local_config_python/python_include/Python.h:8:0,\r\n                 from ./tensorflow/lite/python/interpreter_wrapper/numpy.h:49,\r\n                 from tensorflow/lite/python/interpreter_wrapper/numpy.cc:17:\r\nbazel-out/armeabi-py2-opt/bin/external/local_config_python/python_include/pyconfig.h:13:55: fatal error: arm-linux-gnueabihf/python3.5m/pyconfig.h: No such file or directory\r\n #  include <arm-linux-gnueabihf/python3.5m/pyconfig.h>\r\n                                                       ^\r\ncompilation terminated.\r\nINFO: Elapsed time: 339.221s, Critical Path: 36.02s\r\nINFO: 6204 processes: 6204 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\ncd tensorflow\r\ngit checkout v2.1.0-rc1\r\nCI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4\"     tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37     tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@fgervais Did you ever manage to build raspberry pi wheel for python 3.7?  I am seeing the same behavior, builds for PI-PYTHON3 but not for PI-PYTHON37.  I just booted up Ubuntu 19.10 armhf on my Raspberry Pi 4 device (which has python3.7 installed) and was hoping to give this a spin (I tried the arm64/aarch64 image as well, see below if you're interested).\r\n\r\n@petewarden Do you have any recommendations for getting TensorFlow running on RPI4?  I noticed the [Build from source for the Raspberry Pi](https://www.tensorflow.org/install/source_rpi) instructions only cover armhf and not aarch64, kind of like [Build TensorFlow Lite for Raspberry Pi](https://www.tensorflow.org/lite/guide/build_rpi) covers RPI4 but only in the context of armhf (haven't tried this one just yet).  I saw and tried the [Build TensorFlow Lite for ARM64 boards](https://www.tensorflow.org/lite/guide/build_arm64) instructions, but was unsuccessful at the time (sorry, I don't have the exact error at this moment).  Anyways, any tips would be greatly appreciated.", "@settle I couldn't make it work, I changed my plan and built a docker image based on debian:stretch (which uses python 3.5) and added then tensorflow dependencies on top.\r\n\r\nSee here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36672#issuecomment-587499389", "@fgervais Did you build a devel docker image that can build tensorflow inside it, or you just installed a tensorflow binary inside the docker image?  If the former, could you share you Dockerfile?  I've attempted to do that myself, but run into issues building bazel, see [#37985](https://github.com/tensorflow/tensorflow/issues/37985).\r\n\r\nBtw, what would be your opinion of doing pip install vs. apt-get for the python packages in my Dockerfile.  I ask because it's extremely slow to compile the python dependencies (specifically matplotlib and scipy) when using pip install and would be much faster installing the pre-built python packages and all their dependencies using apt-get.  Some of the messages during pip install make me worried I still don't have all the dependencies.", "@settle I installed the tensorflow binary inside a docker image.\r\n\r\nI prefer to pip to install the dependencies to I get more control on the versions installed. It is slow, yes, but not too bad since I do it through docker `buildx`. Even if it's through emulation, it's still a lot faster than on a raspberry pi.", "@fgervais, you need to update CROSSTOOL_PYTHON_INCLUDE_PATH to point Python 3.7 header files.\r\nPlease use \"CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.7\" at your CI_DOCKER_EXTRA_PARAMS.", "> @fgervais, you need to update CROSSTOOL_PYTHON_INCLUDE_PATH to point Python 3.7 header files.\r\n> Please use \"CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.7\" at your CI_DOCKER_EXTRA_PARAMS.\r\n\r\n@terryheo I tried this today on r2.2 branch, but keep running into build errors related to memory consumption, see below.  I don't know why since my build machine has 128 GB of RAM and I never see it coming near the limit in docker stats.\r\n\r\nERROR: /workspace/tensorflow/compiler/mlir/lite/BUILD:547:1: C++ compilation of rule '//tensorflow/compiler/mlir/lite:flatbuffer_translate_lib' failed (Exit 1)\r\nvirtual memory exhausted: Cannot allocate memory", "@terryheo I've been rerunning the build script above to see if I can glean any more information and this time the error was more helpful, see below.  Question is, how to increase the JRE memory size to this docker cross compilation build script flow?  Or any other fix?\r\n\r\nERROR: /workspace/tensorflow/core/kernels/BUILD:4101:1: C++ compilation of rule '//tensorflow/core/kernels:reduction_ops' failed (Exit 1)\r\n\r\ncc1plus: out of memory allocating 20521 bytes after a total of 1003520 bytes\r\nOpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f66960a2000, 262144, 0) failed; error='Not enough space' (errno=12)\r\n\\#\r\n\\# There is insufficient memory for the Java Runtime Environment to continue.\r\n\\# Native memory allocation (mmap) failed to map 262144 bytes for committing reserved memory.\r\n\\# An error report file with more information is saved as:\r\n\\# /workspace/hs_err_pid92913.log", "@settle interesting. I think you might want to check https://docs.bazel.build/versions/master/memory-saving-mode.html\r\nBut you said that you have 128GB ram, it should be OK.\r\n\r\nJust wondering, what's the output of following commands?\r\n```\r\n$ docker --version\r\n$ docker run tf_ci.pi-python37 cat /proc/meminfo\r\n```", "@terryheo I'm only familiar to using --local_ram_resources=2048 when building with bazel following the [Build from source](https://www.tensorflow.org/install/source) instructions.  How exactly would I pass those memory saving mode like --host_jvm_args=-Xmx2g to this cross compilation flow?\r\n\r\n$ docker --version\r\nDocker version 19.03.2, build 6a30dfc\r\n\r\n$ docker run tf_ci.pi-python37 cat /proc/meminfo\r\nMemTotal:       97398140 kB\r\nMemFree:        84689100 kB\r\nMemAvailable:   94768644 kB\r\nBuffers:         2086196 kB\r\nCached:          7819040 kB\r\nSwapCached:         1592 kB\r\nActive:          4651344 kB\r\nInactive:        5841044 kB\r\nActive(anon):     324812 kB\r\nInactive(anon):   262192 kB\r\nActive(file):    4326532 kB\r\nInactive(file):  5578852 kB\r\nUnevictable:           0 kB\r\nMlocked:               0 kB\r\nSwapTotal:       1998844 kB\r\nSwapFree:        1991408 kB\r\nDirty:               304 kB\r\nWriteback:             0 kB\r\nAnonPages:        579348 kB\r\nMapped:           346624 kB\r\nShmem:             19472 kB\r\nSlab:            1731056 kB\r\nSReclaimable:    1028488 kB\r\nSUnreclaim:       702568 kB\r\nKernelStack:       22176 kB\r\nPageTables:        20692 kB\r\nNFS_Unstable:          0 kB\r\nBounce:                0 kB\r\nWritebackTmp:          0 kB\r\nCommitLimit:    89657168 kB\r\nCommitted_AS:    4151700 kB\r\nVmallocTotal:   34359738367 kB\r\nVmallocUsed:           0 kB\r\nVmallocChunk:          0 kB\r\nHardwareCorrupted:     0 kB\r\nAnonHugePages:         0 kB\r\nShmemHugePages:        0 kB\r\nShmemPmdMapped:        0 kB\r\nCmaTotal:              0 kB\r\nCmaFree:               0 kB\r\nHugePages_Total:       0\r\nHugePages_Free:        0\r\nHugePages_Rsvd:        0\r\nHugePages_Surp:        0\r\nHugepagesize:       2048 kB\r\nDirectMap4k:     1417216 kB\r\nDirectMap2M:    65132544 kB\r\nDirectMap1G:    34603008 kB", "You can have custom Bazel option with .bazelrc.user file.\r\nBefore using it, could you increase your swap? Your SwapTotal is only 2G. Could you try with higher number? ", "@terryheo Just to confirm and for reference, you mean try adding something like --memory=96g --memory-swap=4g to the docker run line in tensorflow/tools/ci_build/ci_build.sh?  I just gave that a try, but noticed the following line printed so doesn't appear to have worked:\r\n\r\n\"WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.\"", "@terryheo Quick follow up, indeed the build continued to fail as before.  After some searching I found how to modify my kernel with grub to allow the swap limit, see [How to limit a docker container\u2019s resources on ubuntu 18.04](https://hostadvice.com/how-to/how-to-limit-a-docker-containers-resources-on-ubuntu-18-04/).  After that I was able to successfully cross-compile build a whl, but it still shows the wrong python version, i.e., tensorflow-2.2.0-cp35-none-linux_armv7l.whl.  Am I still doing something wrong?\r\n\r\n> CI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.7\"     tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37     tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n\r\nIf I try to change the CI_BUILD_PYTHON=python3 above to CI_BUILD_PYTHON=python3.7 then I get this error that didn't appear when I built the above whl file:\r\n\r\nImportError: cannot import name 'multiarray' from 'numpy.core' (/usr/lib/python3/dist-packages/numpy/core/__init__.py)\r\nIs numpy installed?", "@settle Hello,\r\n\r\nI would like to build a wheel file for my Raspberry Pi 3B (Buster) with TensorFlow version 2.3.0rc0.\r\nSo I run the following command :\r\nCI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.7\" tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n\r\nHowever, the output I obtain is the wheel file for Python 3.5 like you... (tensorflow-2.3.0rc0-cp35-none-linux_armv7l.whl) \r\nDid you manage to make it work?", "I've also verified PI-PYTHON37 container doesn't work as expected.\r\nI've investigated a bit and it's not simple to fix.\r\n\r\nPlease use Python 3.5 interpreter instead.", "I've found a solution. Let me prepare PR.", "PR is merged. You can build RPI3 PIP wheel for Python 3.7 as following.\r\n\r\n```\r\nCI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3.7 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.7\" \\\r\n  tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \\\r\n  tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35116\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35116\">No</a>\n"]}, {"number": 35115, "title": "Failed to build TF2.0 with TensorRT: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 1.1.0- (@non-git)\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 8.3.0-6ubuntu1) 8.3.0\r\n- CUDA/cuDNN version: 10.2/7.6.5\r\n- GPU model and memory: \r\n```console\r\nDevice 0: \"GeForce GTX 980M\"\r\n  CUDA Driver Version / Runtime Version          10.2 / 10.2\r\n  CUDA Capability Major/Minor version number:    5.2\r\n  Total amount of global memory:                 4035 MBytes (4231331840 bytes)\r\n  (12) Multiprocessors, (128) CUDA Cores/MP:     1536 CUDA Cores\r\n  GPU Max Clock rate:                            1126 MHz (1.13 GHz)\r\n```\r\n\r\n\r\n```console\r\nWARNING: ....../tensorflow/python/BUILD:107:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: ....../tensorflow/python/keras/api/BUILD:129:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 776, in <module>\r\n    main()\r\n  File \"~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 772, in main\r\n    lazy_loading, args.use_relative_imports)\r\n  File \"~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 629, in create_api_files\r\n    compat_api_versions, lazy_loading, use_relative_imports)\r\n  File \"~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 503, in get_api_init_text\r\n    _, attr = tf_decorator.unwrap(attr)\r\n  File \"~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 219, in unwrap\r\n    elif _has_tf_decorator_attr(cur):\r\n  File \"~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 124, in _has_tf_decorator_attr\r\n    hasattr(obj, '_tf_decorator') and\r\n  File \"~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 28, in <module>\r\n    _wrap_py_utils = swig_import_helper()\r\n  File \"~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_wrap_py_utils', fp, pathname, description)\r\n  File \"/usr/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 696, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 1043, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: ~/.cache/bazel/_bazel_longervision/b29adcaa7335b5e3a52f33ba4b4f5496/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: ....../tensorflow/python/tools/BUILD:98:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)\r\nINFO: Elapsed time: 3.987s, Critical Path: 2.78s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\n", "comments": ["@jiapei100 ,\r\nThank you for reporting the issue, can you please provide code to reproduce the above error ? Thanks!", "I am getting this too building with Ubuntu 18.04, gcc 7.4.0, bazel 1.1.0, CUDA 10.1, CUDNN 7.6.5, Python 3.7.3 conda virtual environment.", "```\r\n$ c++filt  _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_\r\nstream_executor::StreamExecutor::EnablePeerAccessTo(stream_executor::StreamExecutor*)\r\n```\r\n\r\nIt seems `stream_executor::StreamExecutor::EnablePeerAccessTo(stream_executor::StreamExecutor*)` is needed but not there.\r\n", "@freedomtan Any idea on where to add this?", "@sbaktha I tried a bit, but failed. I was able to add `stream_executor::StreamExecutor::EnablePeerAccessTo(stream_executor::StreamExecutor*)` related binary, but that caused other undefined symbols. It seems building TensorFlow 2.x with TensorRT with bazel 1.x doesn't work. I was able to build and 2.0 and master branch with `bazel 0.2x` before the minimum required bazel version bumped (aaea5414317793a3eedc41f23cc5e619ee5941dc) to 1.0. \r\n\r\nShort answer for building 2.0 w/ TensorRT: Use bazel 0.2x\r\n\r\ntag @gunan  who increased bazel version according to `git log`", "@aaroey for tensorrt and @scentini for bazel 1.x", "Any update on this?", "When I try downgrading by the following command \r\n\r\n`sudo apt install bazel-0.29.0`\r\n\r\nI get the following error\r\n\r\n> (base) antpc@ant-pc:~/tensorflow$ sudo apt install bazel-0.29.0\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nE: Unable to locate package bazel-0.29.0\r\nE: Couldn't find any package by glob 'bazel-0.29.0'\r\nE: Couldn't find any package by regex 'bazel-0.29.0'\r\n\r\nHow do I install bazel-0.2x\r\n", "[Use the binary installer](https://docs.bazel.build/versions/master/install-ubuntu.html#install-with-installer-ubuntu)\r\n\r\n[Compiling bazel from the source](https://docs.bazel.build/versions/master/install-compile-source.html)", "Thank you. I did that I am getting error on configuration. Asking to update to bazel 1.0.0\r\n\r\n> (base) antpc@ant-pc: /home/antpc/tensorflow$ which bazel\r\n> /home/antpc/bin/bazel\r\n> (base) antpc@ant-pc:/home/antpc/tensorflow$ bazel --version\r\n> bazel 0.29.1\r\n> (base) antpc@ant-pc:/home/antpc/tensorflow$ ./configure\r\n> WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\n> You have bazel 0.29.1 installed.\r\n> Please upgrade your bazel installation to version 1.0.0 or higher to build TensorFlow!\r\n\r\n", "Please note the workaround for this in [related issue](https://github.com/tensorflow/tensorflow/issues/35584#issuecomment-570836764). ", "Thanks it worked", "For older releases (e.g., r1.14 or r2.0), I think bazel 0.2x should just work.\r\n\r\nIf you are using newer source, e.g., master branch after aaea541, you need workaround @1zoom  mentioned", "I don't think it worked on 1.14. I was trying to build 1.14", "@1zoom bazel 0.2x should work for r1.14. As you may know, r1.14 was released before TensorFlow switched to bazel 1.x. And you can see you don't need changes you made. See https://github.com/tensorflow/tensorflow/blob/r1.14/WORKSPACE#L84", "From the discussion this seems not related to TensorRT. Adding sanjoy@ to followup.", "And yes, I was wrong about 1.14, I thought I'd switched branches.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I encountered this exact same error building master (2.1.0), cuda 10.1, cudnn 7.6.5, and tensorrt support with python 3.8 using the specified bazel 1.2.1.\r\nI am currently running a new build, this time using the [https://github.com/tensorflow/tensorflow/issues/35584#issuecomment-570836764](workaround) mentioned earlier in order to build with bazel 0.26.1.\r\nI'll write another comment to update as to whether this ended up working.", "Yes, you still need to use bazel 0.26.1 to build master (2.1.0), cuda 10.1, cudnn 7.6.5, and tensorrt support with python 3.8.  Using bazel 0.26.1 my build completed successfully.", "@dbonner I had a very similar error a few times and what helped was to run `bazel clean --expunge`, not just `bazel clean` before the configure and build commands.\r\n\r\nThis error occurred when doing multiple builds with different cuda and python versions in a row (and possibly switching between tf 2.0, 2.1 and latest master, which resulted in bazel version switch).", "Thanks, I'll keep that in mind.  However, I think I got this error from a fresh git clone, so there was nothing for bazel to clean.", "Bazel keeps a cache of it's own outside the tensorflow git-cloned directory, but if also bazel was never used before then it's another issue.", "Oh, I see.  I'll try `bazel clean --expunge` and use the bazel version tensorflow recommends (1.2.1) next time I try to build.", "I just tried building after doing `bazel clean --expunge` and I still got the same error:\r\n```\r\nImportError: /home/daniel/.cache/bazel/_bazel_daniel/79db702fc9f94af7d11e11c5d64854d0/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/daniel/tensorflow/tensorflow/python/tools/BUILD:224:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed (Exit 1)\r\nINFO: Elapsed time: 5010.465s, Critical Path: 291.89s\r\nINFO: 18919 processes: 18919 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "@sanjoy: FYI, as @alanpurple mentioned in #35922, TensorRT 7 doesn't have such problem. Unfortunately, not all people can use TensorRT 7, e.g., I have a Jetson Nano board which doesn't have TensorRT 7 (yet).", "@freedomtan, I'm on CUDA 10.1 and TensorRT 7 does not support this.  Do you\nknow if I can build tensorflow with CUDA 10.2?\nCheers,\nDaniel\n\nOn Thu, 30 Jan 2020 at 11:59, freedomtan <notifications@github.com> wrote:\n\n> @sanjoy <https://github.com/sanjoy>: FYI, as @alanpurple\n> <https://github.com/alanpurple> mentioned in #35922\n> <https://github.com/tensorflow/tensorflow/issues/35922>, TensorRT 7\n> doesn't have such problem. Unfortunately, not all people can use TensorRT\n> 7, e.g., I have a Jetson Nano board which doesn't have TensorRT 7 (yet).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35115?email_source=notifications&email_token=AAB26QX7OM2TV2SM7AZCXB3RAIQ65A5CNFSM4J23JGCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEKJKGTA#issuecomment-580035404>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAB26QRQ7AGQY6D53IJCY63RAIQ65ANCNFSM4J23JGCA>\n> .\n>\n", "@dbonner interesting, actually I was using 10.0 + TensorRT 7 on an x86_64 machine (Jetson boards do not have newer CUDA 10.1 or 10.2, so I use 10.0 on x86_64 machine too).", "@freedomtan I parse the comment from alanpurple (\"no problem without tensorrt suppot ( ver 7.0 )\") as it was successful *without* tensorrt. v7 being mentioned simply added color.\n\nAlso https://github.com/tensorflow/tensorflow/issues/35653#issuecomment-576963272 confirms it (not yet confirmed if this issue and #35653 are related).\n\n\n\n\nOn Thu, Jan 30, 2020, at 10:40 AM, freedomtan wrote:\n> @dbonner <https://github.com/dbonner> interesting, actually I was using 10.0 + TensorRT 7 on an x86_64 machine (Jetson boards do not have newer CUDA 10.1 or 10.2, so I use 10.0 on x86_64 machine too).\n\n\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/35115?email_source=notifications&email_token=AABCNGBJDCLRNVZGHPJTZQLRAKOAFA5CNFSM4J23JGCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEKKKXUQ#issuecomment-580168658>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AABCNGA3Z72VXFTZHGURN4TRAKOAFANCNFSM4J23JGCA>.\n\n", "@ahtik oops, you are right. Let me check the configuration I used to build master branch with `bazel 1.2.1` with TensorRT 7 again.", "@ahtik It turns out that it's a Python 2 vs Python 3 problem. Python 2.7 + Cuda 10.[0-2] + TensorRT + Bazel 1.2.1 works fine.\r\n\r\n@sanjoy it seems bazel 1.2.1 + Python 3.x is problematic when TensorRT enabled.", "@freedomtan\r\nThis same error still occurs when building head with Cuda 10.2 and TensorRT 7.\r\n\r\nOn Fri, 31 Jan 2020 at 14:49, freedomtan <notifications@github.com> wrote:\r\n\r\n> @ahtik <https://github.com/ahtik> It turns out that it's a Python 2 vs\r\n> Python 3 problem. Python 2.7 + Cuda 10.[0-2] + TensorRT + Bazel 1.2.1 works\r\n> fine.\r\n>\r\n> @sanjoy <https://github.com/sanjoy> it seems bazel 1.2.1 + Python 3.x is\r\n> problematic when TensorRT enabled.\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/35115?email_source=notifications&email_token=AAB26QR7LRTMBIEKUOHS7VTRAONWPA5CNFSM4J23JGCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEKNMRBQ#issuecomment-580569222>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AAB26QXTOA4YELYBY7GWC5TRAONWPANCNFSM4J23JGCA>\r\n> .\r\n>\r\n", "@dbonner with Python 2.7?", "@dbonner Confirming, also getting the build failed now with the latest master from yesterday, Ubuntu 18.04, python 3.8, cuda 10.1, tensorrt 6.\r\n`\r\nImportError: /home/ak/.cache/bazel/_bazel_ak/8fab1cbad6992500fcb0a636536cab16/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_\r\n`\r\n\r\nTo make the build succeed, have to disable tensorrt, easiest to `export TF_NEED_TENSORRT=0` before running `./configure`.\r\n\r\nI had a suspicion that pybind11 might be involved after skimming https://github.com/tensorflow/tensorflow/issues/35709 and https://github.com/tensorflow/tensorflow/issues/35653 but simply removing `scipy` from the `tensorflow/tools/pip_package/setup.py` did't help, the same failure after 10315seconds.", "@ahtik and @dbonner It seems bazel 1.x + Python 3 is problematic when building TensorRT is enabled. Workarounds:\r\n1. use bazel 0.2.x, e.g., bazel 0.26.1, see comments in #35584\r\n2. use Python 2.7.x instead of Python 3\r\n", "@freedomtan Thanks! Confirmed, linux build TF 2.1.0 works with TensorRT 6 after adjusting bazel requirement from bazel min v0.29.1 to v0.26.1 as described in https://github.com/tensorflow/tensorflow/issues/35584#issuecomment-570920203\r\n\r\nI haven't figured out any set of versions that would build successfully Tensorflow with TensorRT under Windows (neither latest master nor TF 2.1.0).", "> @dbonner with Python 2.7?\r\n\r\nHi @freedomtan I was building with Python 3.8", "\r\nI encountered the same error on the docker (tensorflow/tensorflow:devel-gpu-py3) that I can compile Tensorflow 2 for my own old machine without AVX. \r\n\r\ngcc 7.4.0\r\nbazel 1.2.1\r\nTensorRT 6.0.1\r\nCUDA 10.1\r\nPython 3.6.9\r\n", "I had run into this issue, and adding the bazel flag `--noincompatible_do_not_split_linking_cmdline` when using bazel 0.27.1 allowed the build to complete successfully, with python3 and TensorRT support enabled.", "cc @oquenchil, seems like this failure is related to https://github.com/bazelbuild/bazel/issues/7687", "I'm seeing this using on Ubuntu 18.04, x86_64:\r\n\r\n* Bazel 0.29.1\r\n* Tensorflow checked out at v2.1.0\r\n* TensorRT 7\r\n* CUDA 10.2\r\n* Python 3.7.4\r\n\r\nIs there a fix in the works?", "Can we fix this? It also causes a build break when trying to build tensorflow serving in a container. Until we know the root cause, why not build tensorflow 2.x with bazel 0.26.1 which is known to work (given the normal workarounds in WORKSPACE etc).", "Can you try running with:\r\n`BAZEL_LINKOPTS=\"\" `  and  `BAZEL_LINKLIBS=-lstdc++:-lm`", "Setting those environment variables and proceeding with an otherwise clean build environment <b>did not resolve the issue</b> sadly :( @oquenchil ", "> why not build tensorflow 2.x with bazel 0.26.1\r\n\r\nNote: TensorFlow needs bazel 2.0 since daec6e0af0dfe39aada697275a76570da7f04608.\r\n\r\nI have a general question for the folks on this issue: are you using [docker images](https://www.tensorflow.org/install/source#docker_linux_builds) to build TensorFlow?  If not, why not?", "\r\nAlright everybody... I had Tensorflow 2.1.0 successfully built with bazel 2.0.0 ....\r\n\r\nNow, I'm closing this issue.\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35115\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35115\">No</a>\n", "> > why not build tensorflow 2.x with bazel 0.26.1\r\n> \r\n> Note: TensorFlow needs bazel 2.0 since [daec6e0](https://github.com/tensorflow/tensorflow/commit/daec6e0af0dfe39aada697275a76570da7f04608).\r\n> \r\n> I have a general question for the folks on this issue: are you using [docker images](https://www.tensorflow.org/install/source#docker_linux_builds) to build TensorFlow? If not, why not?\r\n\r\nI am using docker image, yes. What is the deal with Bazel min version then? I thought we were supposed to be using bazel between 0.27.1 and 0.29.1 for v2.1.0 as specified by bazel MIN and MAX version? What is up with that?", "> I am using docker image, yes. \r\n\r\nCC @gunan \r\n\r\nLooks like you're building TF 2.1, but we don't publish docker images capable of building TF 2.1 (we only publish docker images that can build TF @ head).\r\n\r\nI'll try to see if we can change this and also release docker images that can be used to build various releases of TF from source.  That way you don't have to figure out the right build environment to build TF (say) 2.1.  @gunan WDYT?", "@sanjoy \r\n\r\nI've been able to build TensorFlow 2.1 for both x86_64 and ppc64le using CUDA 10.1 and CuDNN 7 inside my own development Docker images. The problem is only when I try to include TensorRT. I've tried both TensorRT 6 and 7 and either way, TF 2.1 fails to build on BOTH x86_64 AND ppc64le, even in the presence of all the requisite libraries. Having the ability to build TF for ppc64le is a primary reason I'm creating my own development docker images. Do you know of any plans to make the devel images available for ppc64le in the future? Thanks for taking the time to help.", "Since the author closed this issue, but since it is not resolved in my case, I have opened a standalone issue for this. https://github.com/tensorflow/tensorflow/issues/37076 . Others are experiencing this problem as well (see https://github.com/tensorflow/tensorflow/issues/35770).", "pip3 install Keras-Preprocessing\r\n\r\nFixed it for me"]}, {"number": 35114, "title": "STM32F7-Disco Hello World example fails to build", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nNAME=\"Ubuntu\"\r\nVERSION=\"18.04.3 LTS\"\r\nID=ubuntu\r\nPRETTY_NAME=\"Ubuntu 18.04.3 LTS\"\r\nVERSION_ID=\"18.04\"\r\nPython versionL 2.7.15+\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): STM32F7-Disco and also tried on K64F based on ARM-M4\r\n\r\n**Describe the problem**\r\nI am attempting to build the Hello World example for the STM32F7-Disco from the following link but it is failing to build\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project\r\n\r\ncd tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed\r\n\r\nmbed config root .\r\n\r\nmbed deploy\r\npython -c 'import fileinput, glob;\r\nfor filename in glob.glob(\"mbed-os/tools/profiles/*.json\"):\r\n  for line in fileinput.input(filename, inplace=True):\r\n    print line.replace(\"\\\"-std=gnu++98\\\"\",\"\\\"-std=c++11\\\", \\\"-fpermissive\\\"\")'\r\n\r\nmbed compile -m DISCO_F746NG -t GCC_ARM\r\n\r\nThis is the error I seen before making a change to :\r\n\r\n`Compile [ 98.7%]: arm_mult_q15.c\r\n[Error] arm_mult_q15.c@101,6: conflicting types for 'arm_mult_q15'\r\n[ERROR] ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.c:101:6: error: conflicting types for 'arm_mult_q15'\r\n void arm_mult_q15(\r\n      ^~~~~~~~~~~~\r\nIn file included from ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.c:29:0:\r\n./mbed-os/cmsis/TARGET_CORTEX_M/arm_math.h:1924:8: note: previous declaration of 'arm_mult_q15' was here\r\n   void arm_mult_q15(\r\n        ^~~~~~~~~~~~\r\n\r\n[mbed] ERROR: \"/usr/bin/python\" returned error.\r\n       Code: 1\r\n       Path: \"/home/pramod/tensorflow-master/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed\"\r\n       Command: \"/usr/bin/python -u /home/pramod/tensorflow-master/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/make.py -D TF_LITE_STATIC_MEMORY -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM\"\r\n       Tip: You could retry the last command with \"-v\" flag for verbose output\r\n`\r\nAny help regarding this is deeply appreciated.\r\n\r\nThank you", "comments": ["i got the error, could anyone help to fix it.\r\nThe problem show below:\r\n(mbed) ross@ross-virtual-machine:~/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/hello_world/mbed$ mbed compile -m DISCO_F746NG -t GCC_ARM\r\n[mbed] Working path \"/home/ross/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/hello_world/mbed\" (library)\r\n[mbed] Program path \"/home/ross/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/hello_world/mbed\"\r\nCould not find executable for GCC_ARM.\r\nCurrently set search path: No path set\r\n[mbed] ERROR: \"/home/ross/anaconda3/envs/mbed/bin/python\" returned error.\r\n       Code: 2\r\n       Path: \"/home/ross/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/hello_world/mbed\"\r\n       Command: \"/home/ross/anaconda3/envs/mbed/bin/python -u /home/ross/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/hello_world/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM\"\r\n       Tip: You could retry the last command with \"-v\" flag for verbose output\r\n---\r\n\r\nso i try the recommendation like this:\r\n/home/ross/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/hello_world/mbed\"\r\n       Command: \"/home/ross/anaconda3/envs/mbed/bin/python -u /home/ross/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/hello_world/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM -v\r\n\r\nthere shows some other problems, below:\r\n(mbed) ross@ross-virtual-machine:~/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/hello_world/mbed$ /home/ross/anaconda3/envs/mbed/bin/python -u /home/ross/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/hello_world/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM -v\r\nCould not find executable for GCC_ARM.\r\nCurrently set search path: No path set\r\n\r\nsince i am new in this. could anyone help me? thnx sincerely ", "Hi,\r\nThe above link which you have provided does not exists anymore, could you please try the same example mentioned in the latest version [here](https://www.tensorflow.org/lite/microcontrollers/get_started_low_level).\r\nFeel free to open new issue if you still face an error.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35114\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35114\">No</a>\n"]}, {"number": 35113, "title": "TensorFlow cannot detect my GPU", "body": "when I run my code, I found it didn't use the GPU, and just used the CPU. \r\nI check the tf and tf-gpu version is same. my system info as following:\r\n\r\nCuda version (nvcc --version result)\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Sat_Aug_25_21:08:01_CDT_2018\r\nCuda compilation tools, release 10.0, V10.0.130\r\n```\r\n\r\ntf version :\r\n```\r\ntensorboard          1.14.0\r\ntensorflow           1.14.0\r\ntensorflow-estimator 1.14.0\r\ntensorflow-gpu       1.14.0\r\n```\r\n\r\nother info:\r\n```\r\npython version: Python 3.6.8\r\nOS version: Ubuntu 16.04.5\r\nkernel version: 4.4.0-131-generic\r\n```\r\n\r\n\r\nGPU model and memory (`nvidia-smi` result)\r\n\r\nps: GPU 1~3 using by pytorch.\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-PCIE...  On   | 00000000:3D:00.0 Off |                    0 |\r\n| N/A   33C    P0    26W / 250W |      0MiB / 32480MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla V100-PCIE...  On   | 00000000:42:00.0 Off |                    0 |\r\n| N/A   60C    P0   219W / 250W |  13324MiB / 32480MiB |     99%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla V100-PCIE...  On   | 00000000:B1:00.0 Off |                    0 |\r\n| N/A   61C    P0   225W / 250W |  13770MiB / 32480MiB |     99%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla V100-PCIE...  On   | 00000000:B5:00.0 Off |                    0 |\r\n| N/A   61C    P0   222W / 250W |  13392MiB / 32480MiB |     99%      Default |\r\n```\r\n\r\nInstalled using virtualenv? yes, use pyenv\r\n\r\n\r\nwhen I use following command, and just have the cpu, not exist GPU.\r\n\r\n```python\r\nfrom tensorflow.python.client import device_lib\r\nprint(device_lib.list_local_devices())\r\n```\r\n![-c400](https://user-images.githubusercontent.com/43985465/70850093-79404d00-1ec1-11ea-8a94-6ef16dcf9f97.png)\r\n\r\n", "comments": ["@Donald-Su, Could you check by installing tenosrflow-gpu==1.14.0. still the error persists or not. Thanks!", " @gadagashwini this time I uninstall **all the tensorflow*** (include tensorboard  and tensorflow-estimator), and install tf-gpu use `pip3 install tensorflow-gpu==1.14.0` again. And It's working now. thanks~\r\n\r\nIs the reason not uninstall tensorboard  and tensorflow-estimator before It can't use GPU ?"]}, {"number": 35112, "title": "Created using Colaboratory", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35112) for more info**.\n\n<!-- need_sender_cla -->", "Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/35112\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n You'll be able to see Jupyter notebook diff and discuss changes. Powered by <a href='https://www.reviewnb.com'>ReviewNB</a>.", "@rollno4 thank you for your contribution, please sign CLA.", "@rollno4 gentle ping to sign CLA. Thanks!", "Sorry, we cannot proceed until CLA reflects YES. Hence closing this. Please open a new PR(which may resolve the CLA issue) and try. Thanks!"]}, {"number": 35111, "title": "TypeError: An op outside of the function building code is being passed a \"Graph\" tensor. In my RNNCell Test.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): Tensorflow2.0-GPU\r\n- Python version: python3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA10 and cudnn\r\n- GPU model and memory: GTX1060 6G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-165-d59a3afffc48> in <module>\r\n      3 for epoch in range(EPOCHS):\r\n      4     for x, y in db_train:\r\n----> 5         train_step(x, y)\r\n      6 \r\n      7     for test_x, test_y in db_test:\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    518         # Lifting succeeded, so variables are initialized and we can run the\r\n    519         # stateless function.\r\n--> 520         return self._stateless_fn(*args, **kwds)\r\n    521     else:\r\n    522       canon_args, canon_kwds = \\\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1824 \r\n   1825   @property\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1139          if isinstance(t, (ops.Tensor,\r\n   1140                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1141         self.captured_inputs)\r\n   1142 \r\n   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1222     if executing_eagerly:\r\n   1223       flat_outputs = forward_function.call(\r\n-> 1224           ctx, args, cancellation_manager=cancellation_manager)\r\n   1225     else:\r\n   1226       gradient_name = self._delayed_rewrite_functions.register()\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    509               inputs=args,\r\n    510               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 511               ctx=ctx)\r\n    512         else:\r\n    513           outputs = execute.execute_with_cancellation(\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     74           \"Inputs to eager execution function cannot be Keras symbolic \"\r\n     75           \"tensors, but found {}\".format(keras_symbolic_tensors))\r\n---> 76     raise e\r\n     77   # pylint: enable=protected-access\r\n     78   return tensors\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     59     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\r\n     60                                                op_name, inputs, attrs,\r\n---> 61                                                num_outputs)\r\n     62   except core._NotOkStatusException as e:\r\n     63     if name is not None:\r\n\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: my_rnn_cell_12/simple_rnn_cell_36/ones_like:0\r\n\r\n```\r\n**Describe the expected behavior**\r\nIt should train fluently.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThis is the error on [Google Cloab](https://colab.research.google.com/drive/1bLig48fgBRkfihxkU7Sn3bklmVyrrSH-#scrollTo=-RS_curvbwVe)\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nBefor I try experts method with @tf.function\uff0cI try model.fit()\uff0cbut it also run error. After I try to find the solutions from the issues, I find that `tf.config.experimental_run_functions_eagerly(False)` can deal this error. But it run very slow. Does it have much better method to deal it?[Google Cloab](https://colab.research.google.com/drive/19waivwxYTpC6S_r5UtPItuks0nW5wufE#scrollTo=HGIY-zFjdnVJ)\r\n\r\nThank you!\r\n", "comments": ["@shazhongcheng, Could you provide a access to view the Google colab link. Thanks! ", "![image](https://user-images.githubusercontent.com/15049049/70891719-c7209680-2022-11ea-8179-6fd7b4b29f2f.png)\r\n\r\nAlthough I had provided\uff0c I paste it following\uff1a\r\nhttps://colab.research.google.com/drive/1bLig48fgBRkfihxkU7Sn3bklmVyrrSH-#scrollTo=-RS_curvbwVe\r\n\r\nhttps://colab.research.google.com/drive/19waivwxYTpC6S_r5UtPItuks0nW5wufE#scrollTo=HGIY-zFjdnVJ\r\n\r\nCan you look it\uff1f\r\n\r\nThank you\uff01", "@shazhongcheng, Still I don't have access to view the Colab links. Could you provide the gist of the colab links. Thanks!", "https://colab.research.google.com/drive/1bLig48fgBRkfihxkU7Sn3bklmVyrrSH-\r\n\r\n\r\nhttps://colab.research.google.com/drive/19waivwxYTpC6S_r5UtPItuks0nW5wufE\r\n", "> @shazhongcheng, Still I don't have access to view the Colab links. Could you provide the gist of the colab links. Thanks!\r\n\r\nsorry , I didn`t share it! I had shared it now! Can you see it now?", "@shazhongcheng, I could see it now. Thanks!", "Thanks for the reporting this issue.\r\n\r\nThe cause of the issue is a combination of directly using RNN cells with dropout and not resetting the dropout mask.\r\n\r\nIn RNN cell, we cache the dropout mask to achieve the variational dropout (same mask within the batch for different timesteps). Between the boundary of batches, the cached dropout mask need to be reset, and we did that for user in the RNN layers. When user directly using cell, the reset action need to be done by users. Otherwise, from TF runtimes perspective, it is trying to access a cached tensor that belongs to different graph.\r\n\r\nYou can quickly add following lines in your call() function, and the code should run fine.\r\n\r\n```\r\nself.rnn_cell0.reset_dropout_mask()\r\nself.rnn_cell1.reset_dropout_mask()\r\nfor word in tf.unstack(x, axis=1): # word: [b, 100]\r\n   ......\r\n```\r\n\r\nOr if you disable the dropout for the rnn cell, it will work as well.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35111\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35111\">No</a>\n", "> Thanks for the reporting this issue.\r\n> \r\n> The cause of the issue is a combination of directly using RNN cells with dropout and not resetting the dropout mask.\r\n> \r\n> In RNN cell, we cache the dropout mask to achieve the variational dropout (same mask within the batch for different timesteps). Between the boundary of batches, the cached dropout mask need to be reset, and we did that for user in the RNN layers. When user directly using cell, the reset action need to be done by users. Otherwise, from TF runtimes perspective, it is trying to access a cached tensor that belongs to different graph.\r\n> \r\n> You can quickly add following lines in your call() function, and the code should run fine.\r\n> \r\n> ```\r\n> self.rnn_cell0.reset_dropout_mask()\r\n> self.rnn_cell1.reset_dropout_mask()\r\n> for word in tf.unstack(x, axis=1): # word: [b, 100]\r\n>    ......\r\n> ```\r\n> \r\n> Or if you disable the dropout for the rnn cell, it will work as well.\r\n\r\nOK OK\uff0cI have fixed it\uff01 Thank you very much!!!!"]}, {"number": 35110, "title": "Error when converting my_frozen_graph.pb to .tflite Google colab", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or GitHub\r\n\r\n SHA if from source):\r\n\r\nThere is an error when trying to converting .pb that build from tensorflow speech recognition freeze.py file for my own custom data. \r\n\r\nThis is the used code for conversion.\r\n```\r\n# Converting a GraphDef from file.\r\nfrom tensorflow import lite\r\n\r\ngraph_def_file = \"conv.pb\"\r\ninput_arrays = [\"wav_data\"]\r\noutput_arrays = [\"labels_softmax\"]\r\ninput_shapes = {\"wav_data\" :[1,160,160,3]}\r\nallow_custom_ops=True\r\nconverter = lite.TFLiteConverter.from_frozen_graph(\r\n  graph_def_file, input_arrays, output_arrays, input_shapes)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n```\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-14-d875e81402bc> in <module>()\r\n      8 converter = lite.TFLiteConverter.from_frozen_graph(\r\n      9   graph_def_file, input_arrays, output_arrays, input_shapes)\r\n---> 10 tflite_model = converter.convert()\r\n     11 open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py in convert(self)\r\n    981           input_tensors=self._input_tensors,\r\n    982           output_tensors=self._output_tensors,\r\n--> 983           **converter_kwargs)\r\n    984     else:\r\n    985       result = _toco_convert_graph_def(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    447       input_data.SerializeToString(),\r\n    448       debug_info_str=debug_info_str,\r\n--> 449       enable_mlir_converter=enable_mlir_converter)\r\n    450   return data\r\n    451 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    198       stdout = _try_convert_to_unicode(stdout)\r\n    199       stderr = _try_convert_to_unicode(stderr)\r\n--> 200       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    201   finally:\r\n    202     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2019-12-14 06:07:40.498969: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: DecodeWav\r\n2019-12-14 06:07:40.499086: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: AudioSpectrogram\r\n2019-12-14 06:07:40.499112: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Mfcc\r\n2019-12-14 06:07:40.499677: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 18 operators, 30 arrays (0 quantized)\r\n2019-12-14 06:07:40.499947: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 18 operators, 30 arrays (0 quantized)\r\n2019-12-14 06:07:40.501698: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 11 operators, 22 arrays (0 quantized)\r\n2019-12-14 06:07:40.506669: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 10 operators, 21 arrays (0 quantized)\r\n2019-12-14 06:07:40.506844: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 9 operators, 19 arrays (0 quantized)\r\n2019-12-14 06:07:40.506961: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 9 operators, 19 arrays (0 quantized)\r\n2019-12-14 06:07:40.507036: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 9 operators, 19 arrays (0 quantized)\r\n2019-12-14 06:07:40.507128: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\r\n2019-12-14 06:07:40.507170: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 1694865\r\n2019-12-14 06:07:40.507516: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, RESHAPE, SOFTMAX. Here is a list of operators for which you will need custom implementations: AudioSpectrogram, DecodeWav, Mfcc.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, RESHAPE, SOFTMAX. Here is a list of operators for which you will need custom implementations: AudioSpectrogram, DecodeWav, Mfcc.\r\n\r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Any help out there?", "You may try converting the model using TFLITE_BUILTINS ,SELECT_TF_OPS options.\r\nSee https://www.tensorflow.org/lite/guide/ops_select#converting_the_model\r\n", "Thank u @ymodak. I'll give a try and let u know.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this issue for now. Feel free to reopen if have any further questions. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35110\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35110\">No</a>\n"]}]