[{"number": 31554, "title": "[tflite]fix output shape check for strided slice always failing when stride != 1", "body": "", "comments": ["@impjdi could you take a look at this change? Thx!"]}, {"number": 31553, "title": "r2.0-rc0 cherry-pick request: Rollback gradients change which broke convergence for NCF", "body": "The original change broken convergence for NCF keras model with run_eagerly=True. This change reverts it.\r\n\r\nAutomated rollback of commit cadb1283346e08ab780e9cc7338916ebf6ebb482. Revert #22231.\r\nPiperOrigin-RevId: 262947974", "comments": []}, {"number": 31552, "title": "r2.0-rc0 cherry-pick request: use NCCL only for all-reduce.", "body": "PiperOrigin-RevId: 262647780\r\n\r\nWithout this change, `MultiWorkerMirroredStrategy` with NCCL would hang on r2.0 branch.", "comments": []}, {"number": 31551, "title": "CherryPick: Only enable graph rewrite for RNN layer in v2 mode (outmost eager con\u2026", "body": "\u2026text).\r\n\r\nThe tf.function approach does not work well in v1 with session since it might try to update /mutate the graph between session. This change will disable the tf function path in v1 session mode. This will prevent any user to use cudnn kernel either with compat.v2 or tf.disable_eager_exeuction().\r\n\r\nNote the estimator in v2 should still have the graph rewrite support (have cudnn kernel on GPU).\r\n\r\nThe graph rewrite tests are now run in v2 only since the rewrite in v1 has been disabled.\r\n\r\nPiperOrigin-RevId: 262577530", "comments": []}, {"number": 31550, "title": "Error: Check failed: dims == sizes.size() (5 vs. 4), when using CPU and MKL instead of Eigen or GPU.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6 High Sierra\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nRunning my code on cpu with tensorflow 1.14 using mkl from anaconda throws the following error:\r\n2019-08-12 17:42:33.158451: F ./tensorflow/core/util/mkl_util.h:636] Check failed: dims == sizes.size() (5 vs. 4)\r\nAbort trap: 6\r\n\r\nThe error trace gives me no hint on how to localize the problem (see below). The issue does not occur when installing a tensorflow build using eigen.\r\n\r\n**Describe the expected behavior**\r\nThe code should work using both MKL or Eigen\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nI was not able to localize the exact issue.\r\nIt occurs when running ndnet.py from https://github.com/sziem/deconv_unet_2d3d\r\n\r\n**Other info / logs**\r\nFull output:\r\n\r\n$ python ndnet.py\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0812 18:24:24.249456 140736187437952 deprecation.py:323] From /Users/Soenke/anaconda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\ntesting training\r\n2019-08-12 18:24:26.506695\r\n2019-08-12 18:24:26.621728: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2019-08-12 18:24:26.691965: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\r\nW0812 18:24:26.750053 140736187437952 deprecation.py:323] From /Users/Soenke/anaconda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nW0812 18:24:26.794230 140736187437952 deprecation.py:323] From /Users/Soenke/Documents/Uni/Master/Masterarbeit/gits/code/public/unet_deconv/dataset_handlers/tfdata_dataset_handlers.py:332: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ntf.py_func is deprecated in TF V2. Instead, there are two\r\n    options available in V2.\r\n    - tf.py_function takes a python function which manipulates tf eager\r\n    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n    being differentiable using a gradient tape.\r\n    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n    stateful argument making all functions stateful.\r\n    \r\nCropping to nearest allowed input image size.\r\nCropping to nearest allowed input image size.\r\nW0812 18:24:27.274060 140736187437952 deprecation.py:323] From /Users/Soenke/Documents/Uni/Master/Masterarbeit/gits/code/public/unet_deconv/dataset_handlers/tfdata_dataset_handlers.py:139: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\r\ninput_shape: (?, 400, 100, 100, 1)\r\nbuilding Unet_v3 for training\r\nnet input shape (?, 398, 98, 98, 1)\r\nW0812 18:24:27.390805 140736187437952 deprecation.py:323] From /Users/Soenke/Documents/Uni/Master/Masterarbeit/gits/code/public/unet_deconv/network_architectures/ops.py:173: conv3d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.keras.layers.Conv3D` instead.\r\ninput_block1_2 (?, 396, 96, 96, 2)\r\nW0812 18:24:27.957751 140736187437952 deprecation.py:323] From /Users/Soenke/Documents/Uni/Master/Masterarbeit/gits/code/public/unet_deconv/network_architectures/ops.py:116: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.dropout instead.\r\nW0812 18:24:28.167316 140736187437952 deprecation.py:323] From /Users/Soenke/Documents/Uni/Master/Masterarbeit/gits/code/public/unet_deconv/network_architectures/ops.py:232: average_pooling3d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.AveragePooling3D instead.\r\ndown_block2_4 (?, 196, 46, 46, 4)\r\nbottom_block4_4 (?, 192, 42, 42, 4)\r\nup_block4_2 (?, 380, 80, 80, 2)\r\noutput_block2_1 (?, 378, 78, 78, 1)\r\nnet output shape (?, 378, 78, 78, 1)\r\noutput_shape: (?, 378, 78, 78, 1)\r\nloss is  l2loss\r\ndetermining number of trainable vars (except batch_norm) for regularization...\r\ndone:  2183\r\nsaving new ckpt and logs in models/unetv3_small_valid_fp0_pp0_bn00_chlast/poisson_n1000_wl520/seed1_bs1_do0.0_loss=l2loss0_weightreg=0.001l2_loss_datareg=1e-08None_example/run8\r\nSaving 2 logs per epoch by default.\r\nSaving checkpoint every 2 epochs by default.\r\nstarting training with start_step 0\r\nepoch 1 / 2\r\n---->saving 0\r\n---->summarizing 0\r\n2019-08-12 18:24:44.145176: F ./tensorflow/core/util/mkl_util.h:636] Check failed: dims == sizes.size() (5 vs. 4)\r\n2019-08-12 18:24:44.145177: F ./tensorflow/core/util/mkl_util.h:636] Check failed: dims == sizes.size() (5 vs. 4)\r\nAbort trap: 6", "comments": ["I got the exact same issue at the moment!\r\n\r\nPS:\r\n>>> print(keras.__version__)\r\n2.2.4\r\n>>> print(tf.__version__)\r\n1.14.0", "We submitted a few fixes to the 1.15 branch, could you please test your use case in 1.15 branch?", "Great!  I was able to run the code without error after compiling the 1.15 branch with mkl.  \r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31550\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31550\">No</a>\n", "And if we can't use the 1.15 branch?"]}, {"number": 31549, "title": "tf.Estimator starts with GPU and switches to CPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04.4 LTS (Xenial Xerus)\r\n- TensorFlow installed from (source or binary): from pip\r\n- TensorFlow version (use command below): 1.14, but earlier also for 1.11\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0.130 / 7.6.2\r\n- GPU model and memory: 1080Ti 11GB\r\n\r\nFew days ago I've asked a SO question: https://stackoverflow.com/questions/57097880/tensorflow-estimator-gradually-decreasing-gpu-utilization - sadly without a response, so I'm trying here, not sure if this is a bug.\r\n\r\nCopied from the question:\r\nI'm training tf.Estimator model (5025056 trainable parameters):\r\n\r\n    # simplified code\r\n    model = create_custom_model() #preparing my model here \r\n    tf.reset_default_graph()\r\n    estimator = tf.estimator.Estimator(model_fn=model.get_model_fn())\r\n    evaluation_hook = tf.contrib.estimator.InMemoryEvaluatorHook(\r\n        estimator=estimator,\r\n        input_fn=lambda: model.eval_input_fn()\r\n    )\r\n    estimator.train(\r\n        input_fn=lambda: model.train_input_fn(),\r\n        hooks=[evaluation_hook]\r\n    )\r\nAnd dataset is prepared here:\r\n\r\n    def train_input_fn():\r\n        dataset = tf.data.TFRecordDataset(\"some_filename\")\r\n        dataset = dataset.shuffle(3000)\r\n        dataset = dataset.repeat()\r\n        dataset = dataset.batch(384)\r\n        dataset = dataset.prefetch(1)\r\n        return dataset\r\n\r\nMy dataset consists of images, 9000 samples, stored in TFRecord (163M). \r\nGPU is `GeForce GTX 1080 Ti` and CPU is `i5-6600 CPU @ 3.30GHz`.\r\nDuring the training, at first, everything looks fine - `htop` shows that each core is working approximately the same way (mostly jumping between `0` - `50%` utilization), and gpu stats (shown using `vidia-smi --query-gpu=timestamp,pstate,temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used --format=csv -l 1`) indicate that card is working on `100%` of its power - temperatures around `85\u00b0C`, `100%` utilization of gpu and memory, also most of the memory is used(10GB).\r\nUp to step `4000` (of `10000`) tensorflow prints `global_step/sec` with values ranging from `1.0` to `1.1`. `15GB` of available `16GB` of RAM is used and no swap.\r\n\r\n\r\nAfter that step, that time `global_step/sec` is lower and lower. For step `5000` it is `0.617275`.\r\nAaround step 2000 it is only `0.0938672` and decreasing (down to `0.0368357` at the end of the training). During this process `nvidia-smi` shows that `utilization.gpu` and `utilization.memory` is `0%` more and more frequently (despite the fact that `memory.used` shows same amount whole time, that is `10GB`), and CPU is working at a `100%` of a single core. RAM is at the same level as when training started, and no swapping occurs. Periodically, mayby once for `20` steps, GPU utilization is slightly higher, but quickly returns to `0`. \r\n\r\n\r\nIt looks like after some epochs tensorflow trains on CPU instead of GPU? What can be possible cause of that? ", "comments": ["Looks like the code is incomplete.Can you please provide full code snippet to reproduce it on our environment.Thanks!", "Full project is available in my repo here: https://github.com/arozans/idenface/tree/IF-NONE_GitHub_GPU_issue.\r\n\r\nEntry point is file `training.py` (training with multiple models in a loop). From my observations -  after first (sometimes second) model finishes training, GPU often goes into low performance P8 state. Attaching also log for this training, grepping with \" sec)\" show diminishing training speed.\r\n[t_e_0821t0100.log](https://github.com/tensorflow/tensorflow/files/3519486/t_e_0821t0100.log)\r\n", "Hey, any news on that one? Any suggestion where to look? I'm attaching one more log, this time with corresponding gpu (from `nvidia-smi`) and cpu (from `ps`). (Usage of cpu with time is also decreasing, because after some time only one core is working on 100%)\r\n[cnn_training_0921t2132.log](https://github.com/tensorflow/tensorflow/files/3639879/cnn_training_0921t2132.log)\r\n[cpu.log](https://github.com/tensorflow/tensorflow/files/3639880/cpu.log)\r\n[gpu.log](https://github.com/tensorflow/tensorflow/files/3639881/gpu.log)\r\n\r\n\r\n", "I'm not familiar with Estimator. @tanzhenyu could you help to find someone to take a look? Thanks.", "Hi @arozans,\r\n\r\nI would suggest poking at your training job with the [TensorBoard profiler](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) to see if you can spot any red flags.  You can using the [profiler APIs](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras#profiler_apis) to profile the program for a few steps only after step `4000` (since that is when the problem starts).", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31549\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31549\">No</a>\n"]}, {"number": 31548, "title": "Add adjust_batch or (re_batch) support for tf.data", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): N/A\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nAt the moment `tf.data` has both `unbatch()` and `batch(n)` support, where you can either change the output of `tf.data` from `any`-batched ones to `non`-batched ones, or from `non`-batched ones to `n`-batched ones.\r\n\r\nWhat is not available is the transformation from `m`-batched ones to `n`-batched ones directly.\r\n\r\nIndirectly it is possible to achieve the above mentioned `m`-batched => `n-batched` through:\r\n```\r\ndataset = dataset.unbatch().batch(n)\r\n```\r\n\r\nHowever, in case each record is small in size (e.g., one record only have one float32) then `.unbatch().batch(n)` could be costly.\r\n\r\nIt would be great to have a way to shortcut with an `adjust_batch` or `re_batch` that achieve the same thing:\r\n```\r\ndataset = dataset.re_batch(n) # from m-batched to n-batched\r\n```\r\n\r\nNote there is already a `RebatchDataset` (which archives different goal) so not sure if `re_batch(n)` could be a good naming.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo.\r\n**Who will benefit with this feature?**\r\n\r\nSee description above.\r\n\r\n**Any Other info.**\r\n", "comments": ["/cc @jsimsa @mrry  I am wondering if the above feature request makes sense? I could create a PR if it makes sense.", "Hi @yongtang, do you have evidence that suggests that a fused implementation of `unbatch().batch()` would be more efficient than the existing one (based on two separate transformations)?\r\n\r\nIf so, I would be open to implementing this fused kernel and introducing it as a static optimization. In other words, users still write `unbatch().batch()` and tf.data would automatically rewrite this to fused implementation (similar to how `map().batch()` is automatically rewritten to the fused one).", "Thanks @jsimsa! Static optimization makes much more sense than introducing another op of rebatch. Let me take a look and see if I can create a PR with it.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 31547, "title": "tensorflow.keras.layers.BatchNormalization deos not accept float64 input data", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9\r\n- GPU model and memory: 1080 GTX\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n**TypeError: Value passed to parameter 'x' has DataType float64 not in list of allowed values: float32**\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow.keras.layers as tks\r\ninput_ = tks.Input(shape=[None, None, 3], dtype='float64')\r\nconv = tks.Conv2D(filters=3, kernel_size=3)(input_)\r\nbatch = tks.BatchNormalization()(conv)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@DeeperDeeper As you can refer to the source code [here](https://github.com/tensorflow/tensorflow/blob/e19c354920c3b246dda6598229210a582caaa1a9/tensorflow/python/keras/layers/normalization.py#L262) and [here](https://github.com/tensorflow/tensorflow/blob/eca3413b03cb4dce83f655de211418d91798f2ce/tensorflow/python/keras/layers/normalization.py#L262), tensorflow.keras.layers.BatchNormalization() only takes float16 or float32 values as input. This is the cause of your error. Please let me know if it helps. Thanks!", "Closing this issue as it has been answered. Please add additional comments and we can open the issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31547\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31547\">No</a>\n"]}, {"number": 31546, "title": "model.fit_generator() multithreading is broken in tf.keras", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Platform-independent\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): From pip\r\n- TensorFlow version (use command below): 1.14.0 and 2.0\r\n- Python version: 3.6.7\r\n\r\n### **Summary**\r\nfit_generator has an option called *workers*, setting this to >1 will use multithreading to queue up batches from a generator.\r\n\r\nIt raises an exception if the generator is not thread-safe. this is expected. However, **it does not accept thread-safe generators.**\r\n\r\n### **Describe the current behavior**\r\nCalling model.fit_generator on a keras model in tf 2.0 or compat.v2 **using a generator object, subclassed from collections.Generator** raises an exception that the given generator object does not have a shape attribute.\r\n\r\nThis is rooted in the calling of model_iteration which then unsuccessfully attempts to find out wether the generator is in fact a generator by using inspect.isgenerator(), which only recognizes native python generators (constructed by a function containing a yield statement)\r\n\r\nhowever, **native python generators cannot be thread-safe**, thus fit_generator with workers>1 and use_multiprocessing=False is broken in tf.keras\r\n\r\n### **Describe the expected behavior**\r\nIn keras 2.2.4, fit_generator simply calls the next(gen) function on the generator provided to fit_generator(). this is working as expected.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```Python\r\nimport numpy as np\r\n\r\n#switch here to switch between working keras and non-working tf.keras code\r\ndo_broken=True \r\n\r\nif do_broken:\r\n    import tensorflow.compat.v2 as tf\r\n    from tensorflow.compat.v2 import keras\r\n    from tensorflow.compat.v2.keras.layers import Dense\r\n    from tensorflow.compat.v2.keras.models import Sequential\r\n    tf.enable_v2_behavior()\r\nelse:\r\n    import keras\r\n    from keras.layers import Dense\r\n    from keras.models import Sequential\r\n\r\nimport threading\r\nfrom collections import Generator\r\n\r\nclass mwe_gen(Generator):\r\n    \r\n    def __init__(self,train_data,train_labels,batch_size):\r\n        self.train_data=train_data\r\n        self.train_labels=train_labels\r\n        self.batch_size=batch_size\r\n        self.batch=0\r\n        self.lock=threading.Lock()\r\n\r\n    def __iter__(self):\r\n        return self\r\n\r\n    def __next__(self):\r\n        return self.next()\r\n    \r\n    def next(self):\r\n        with self.lock:\r\n            batch=self.batch\r\n            batch_size=self.batch_size\r\n            self.batch=self.batch+self.batch_size\r\n            if self.batch>len(self.train_data):\r\n                self.batch=0\r\n        batch_data=self.train_data[batch:batch+batch_size]\r\n        batch_labels=self.train_labels[batch:batch+batch_size]\r\n        return (batch_data,batch_labels)\r\n    \r\n    def send(self,arg):\r\n        return self.next()\r\n    \r\n    def close(self):\r\n        \"\"\"Raise GeneratorExit inside generator.\r\n        \"\"\"\r\n        try:\r\n            self.throw(GeneratorExit)\r\n        except (GeneratorExit, StopIteration):\r\n            pass\r\n        else:\r\n            raise RuntimeError(\"generator ignored GeneratorExit\")\r\n    \r\n    def throw(self, type=None, value=None, traceback=None):\r\n        raise StopIteration\r\n        \r\ntrain_data=np.random.normal(size=(10,1))\r\ntrain_labels=np.random.normal(size=(10,1))\r\n\r\ngen=mwe_gen(train_data,train_labels,5)\r\n\r\nmodel=Sequential()\r\nmodel.add(Dense(1,input_shape=(1,)))\r\n\r\nmodel.compile(loss=\"mse\",optimizer=\"sgd\")\r\n\r\nmodel.fit_generator(gen,steps_per_epoch=2)\r\n\r\n```\r\n\r\n**Other info / logs**\r\n```Python\r\n\r\nTraceback (most recent call last):\r\n  File \"multithreaded_gen.py\", line 72, in <module>\r\n    model.fit_generator(gen,steps_per_epoch=2)\r\n  File \"C:\\Users\\PYRESTONE\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1433, in fit_generator\r\n    steps_name='steps_per_epoch')\r\n  File \"C:\\Users\\PYRESTONE\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 144, in model_iteration\r\n    shuffle=shuffle)\r\n  File \"C:\\Users\\PYRESTONE\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 480, in convert_to_generator_like\r\n    num_samples = int(nest.flatten(data)[0].shape[0])\r\nAttributeError: 'mwe_gen' object has no attribute 'shape'\r\n\r\n```", "comments": ["I have tried on colab with TF version 1.14, nightly versions, 2.0.0-dev20190813 and was able to reproduce the issue.Please, find the [gist ](https://colab.research.google.com/drive/1yAK1iG0wdAo51uDk6YGifdV2-vpiEzqm)here.Thanks!", "I just hit this and am preparing a fix. In case you're curious the issue is that we check for generators but not iterators: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/data_utils.py#L99 It's slightly tricky, because lots of things are iterators but we but aren't legitimate inputs to `.fit_generator()`", "This is now fixed. (Confirmed with the latest nightly.)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31546\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31546\">No</a>\n", "@robieta , Just to confirm, is this fixed with `tf` version `2.1.0`? (aka how do I check?)\r\n\r\nThanks!"]}, {"number": 31544, "title": "Severe lag due to GPU having no memory to allocate to", "body": "Running tensorflow on my laptop through anaconda and the following popped up:\r\n\r\n2019-08-12 09:32:41.378946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3048 MB memory) -> physical GPU (device: 0, name: Quadro M1000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2019-08-12 09:33:14.804467: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n\r\nI am experiencing severe lag that I was originally not.\r\n\r\nAny thoughts?\r\n\r\nI used the following instructions to install my tensorflow: https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue.  If you are unclear what to include see the issue template displayed in the Github new issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "**System information**\r\n- Have I written custom code:\r\nno\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10, 64-bit operating system, x64-based processor.\r\n\r\n- TensorFlow installed from (source or binary):\r\nC=Not sure if compiled from source or binary.... working through anaconda and the actual script is run with IDLE. Hope this helps.\r\n\r\n- TensorFlow version (use command below):\r\nTensorflow 14.0\r\n\r\n- Python version:\r\npython 3.5.6\r\n\r\n- Bazel version (if compiling from source):\r\n\r\n- GCC/Compiler version (if compiling from source):\r\nGCC 4.8\r\n\r\n- CUDA/cuDNN version:\r\ncuDNN64_7\r\n\r\n- GPU model and memory:\r\nWDDM 2.1\r\nmemory 3.9 GB\r\n\r\n**Describe the current behavior**\r\nRan the following command:\r\n(tensorflow1) C:\\tensorflow1\\models\\research\\object_detection>idle Object_detection_webcam.py\r\nThis opened a script which I selected 'run module'\r\nThen the screen would appear and the object detection would begin.\r\n**tensorflow would run but with extreme lag**\r\n\r\n**Describe the expected behavior**\r\ntensorflow runs with less lag\r\n\r\n**Code to reproduce the issue**\r\n(tensorflow1) C:\\tensorflow1\\models\\research\\object_detection>idle Object_detection_webcam.py\r\n\r\n**Other info / logs**\r\nRunning tensorflow on my laptop through anaconda and the following popped up:\r\n\r\n2019-08-12 09:32:41.378946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3048 MB memory) -> physical GPU (device: 0, name: Quadro M1000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2019-08-12 09:33:14.804467: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n\r\n", "@hunterwallace Looks like you 3048 MB memory was allocated to the conda's virtual environment and the `object_detection model`  ran out of memory trying to allocate 3.06GiB. You need to increase memory or use smaller models that requires lower memory. I think there are ways to increase your memory upto 3.9 GB but still lab will be less if you have little more memory for that model. Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31544\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31544\">No</a>\n"]}, {"number": 31542, "title": "Ploting Gradients to Tensorboard and Console", "body": "**System information**\r\n- Windows 10 Pro Version 1903\r\n- TensorFlow installed from pip in Anaconda:\r\n- TensorFlow version 2.0.0-beta1 (gpu)\r\n- Python version: 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]\r\n- CUDA/cuDNN version: Cuda compilation tools, release 10.0, V10.0.130\r\n- GPU model and memory:  GeForce GTX 980 Ti major: 5 minor: 2 memoryClockRate(GHz): 1.2785\r\n\r\n**Describe the current behavior**\r\nProgram ends with an unclear error, while trying to retrieve the bias gradients of the two dense layers in the model.\r\n\r\n* Writing to tensorboard (*console parameter = False*)\r\n```\r\nTrain on 60000 samples\r\nEpoch 1/5\r\n2019-08-12 15:24:48.713962: I tensorflow/core/profiler/lib/profiler_session.cc:174] Profiler session started.\r\n2019-08-12 15:24:48.718362: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library cupti64_100.dll\r\n   32/60000 [..............................] - ETA: 12:48 - loss: 2.2374 - accuracy: 0.18752019-08-12 15:24:48.840661: I tensorflow/core/platform/default/device_tracer.cc:641] Collecting 81 kernel records, 14 memcpy rec\r\nords.\r\n59744/60000 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.9123Traceback (most recent call last):\r\n  File \"C:/Users/Harald Schweiger/PycharmProjects/Gradients/gradient_test.py\", line 42, in <module>\r\n    model.fit(x_train, y_train, epochs=5, callbacks=[gradient_cb, tensorboard_cb])\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 643, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 664, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 439, in model_iteration\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 295, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"C:/Users/Harald Schweiger/PycharmProjects/Gradients/gradient_test.py\", line 32, in on_epoch_end\r\n    tf.summary.histogram(t.name, data=t)\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorboard\\plugins\\histogram\\summary_v2.py\", line 77, in histogram\r\n    tensor = _buckets(data, bucket_count=buckets)\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorboard\\plugins\\histogram\\summary_v2.py\", line 139, in _buckets\r\n    return tf.cond(is_empty, when_empty, when_nonempty)\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 1382, in cond_for_tf_v2\r\n    return cond(pred, true_fn=true_fn, false_fn=false_fn, strict=True, name=name)\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 1177, in cond\r\n    result = false_fn()\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorboard\\plugins\\histogram\\summary_v2.py\", line 137, in when_nonempty\r\n    return tf.cond(is_singular, when_singular, when_nonsingular)\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 1382, in cond_for_tf_v2\r\n    return cond(pred, true_fn=true_fn, false_fn=false_fn, strict=True, name=name)\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 1174, in cond\r\n    if pred:\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 698, in __bool__\r\n    raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\r\nTypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the\r\n value of a tensor.\r\n```\r\n\r\n* Priniting bias gradients to console(*console parameter = True*)\r\n```\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nTrain on 60000 samples\r\nEpoch 1/5\r\n2019-08-12 15:26:01.265400: I tensorflow/core/profiler/lib/profiler_session.cc:174] Profiler session started.\r\n2019-08-12 15:26:01.268877: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library cupti64_100.dll\r\n   32/60000 [..............................] - ETA: 12:53 - loss: 2.4094 - accuracy: 0.09382019-08-12 15:26:01.391432: I tensorflow/core/platform/default/device_tracer.cc:641] Collecting 81 kernel records, 14 memcpy rec\r\nords.\r\n59776/60000 [============================>.] - ETA: 0s - loss: 0.3015 - accuracy: 0.9121Tensor: Adam/gradients_1/dense128/BiasAdd_grad/BiasAddGrad:0\r\nTraceback (most recent call last):\r\n  File \"C:/Users/Harald Schweiger/PycharmProjects/Gradients/gradient_test.py\", line 42, in <module>\r\n    model.fit(x_train, y_train, epochs=5, callbacks=[gradient_cb, tensorboard_cb])\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 643, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 664, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 439, in model_iteration\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 295, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"C:/Users/Harald Schweiger/PycharmProjects/Gradients/gradient_test.py\", line 30, in on_epoch_end\r\n    print('{}\\n'.format(K.get_value(t)[:10]))\r\n  File \"C:\\Users\\Harald Schweiger\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 2981, in get_value\r\n    return x.numpy()\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n* Writing to tensorboard (*console parameter = False*)\r\nTensorboard event file which contains the distribution and histograms of gradients \r\nderived from the total loss that has been accumulated over the last epoch.\r\n\r\n* Priniting to console(*console parameter = True*)\r\nThe program should print the first 10 gradient bias values of each of the two dense layer\r\nto the console.\r\n\r\nIf the exceptions produced here are the expected behavior due to errors in the developers code\r\na more meaningful error message would be appriciated. \r\nIn that case a correction of the code would be useful for me and other people as well who had to update their code as the *write_grads* parameter has been removed from the tensorboard callback in version 2.0.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras import backend as K\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu', name='dense128'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax', name='dense10')\r\n])\r\n\r\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n\r\n\r\nclass GradientCallback(tf.keras.callbacks.Callback):\r\n    console = True\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        weights = [w for w in self.model.trainable_weights if 'dense' in w.name and 'bias' in w.name]\r\n        loss = self.model.total_loss\r\n        optimizer = self.model.optimizer\r\n        gradients = optimizer.get_gradients(loss, weights)\r\n        for t in gradients:\r\n            if self.console:\r\n                print('Tensor: {}'.format(t.name))\r\n                print('{}\\n'.format(K.get_value(t)[:10]))\r\n            else:\r\n                tf.summary.histogram(t.name, data=t)\r\n\r\n\r\nfile_writer = tf.summary.create_file_writer(\"./metrics\")\r\nfile_writer.set_as_default()\r\n\r\n# write_grads has been removed\r\ntensorboard_cb = tf.keras.callbacks.TensorBoard(histogram_freq=1, write_grads=True)\r\ngradient_cb = GradientCallback()\r\n\r\nmodel.fit(x_train, y_train, epochs=5, callbacks=[gradient_cb, tensorboard_cb])\r\n```\r\n", "comments": ["Issue replicating for TF version 2.0beta,please find the [Gist](https://colab.sandbox.google.com/gist/oanush/299fbfb46ea9bb46f1355c3cef042c67/31542.ipynb) of Colab. Thanks!", "I also find it challenging to plot gradients to Tensorboard in TF 2.2.0-rc3 on Colab. My case is different from @SPP3000 in that instead of \r\n```python\r\nif self.console:\r\n    print('Tensor: {}'.format(t.name))\r\n    print('{}\\n'.format(K.get_value(t)[:10]))\r\nelse:\r\n    tf.summary.histogram(t.name, data=t)\r\n```\r\nI simply have `tf.summary.histogram(t.name, data=t)`. The error I came across is:\r\n```python\r\nAttributeError: 'Sequential' object has no attribute 'total_loss'\r\n```\r\nUsing `tf.keras.Model`, the error became `'Model' object has no attribute 'total_loss'`.", "Im also affected by the issue", "Here is a workaround where the gradients are explicitly calculated. It avoids the `total_loss` error I mentioned above. \r\n````python\r\nclass ExtendedTensorBoard(tf.keras.callbacks.TensorBoard):\r\n  def _log_gradients(self, epoch):\r\n    step = tf.cast(tf.math.floor((epoch+1)*num_instance/batch_size), dtype=tf.int64)\r\n    writer = self._get_writer(self._train_run_name)\r\n\r\n    with writer.as_default(), tf.GradientTape() as g:\r\n      # here we use test data to calculate the gradients\r\n      _x_batch = x_te[:100]\r\n      _y_batch = y_te[:100]\r\n\r\n      g.watch(_x_batch)\r\n      _y_pred = self.model(_x_batch)  # forward-propagation\r\n      loss = self.model.loss(y_true=_y_batch, y_pred=_y_pred)  # calculate loss\r\n      gradients = g.gradient(loss, self.model.trainable_weights)  # back-propagation\r\n\r\n      # In eager mode, grads does not have name, so we get names from model.trainable_weights\r\n      for weights, grads in zip(self.model.trainable_weights, gradients):\r\n        tf.summary.histogram(\r\n            weights.name.replace(':', '_')+'_grads', data=grads, step=step)\r\n    \r\n    writer.flush()\r\n\r\n  def on_epoch_end(self, epoch, logs=None):  \r\n    # This function overwrites the on_epoch_end in tf.keras.callbacks.TensorBoard\r\n    # but we do need to run the original on_epoch_end, so here we use the super function. \r\n    super(ExtendedTensorBoard, self).on_epoch_end(epoch, logs=logs)\r\n\r\n    if self.histogram_freq and epoch % self.histogram_freq == 0:\r\n      self._log_gradients(epoch)\r\n````\r\n`ExtendedTensorBoard` can then be used in replace of `tf.keras.callbacks.TensorBoard`.", "Thanks for posting the code. I am getting and error saying: \r\n```NameError: name 'num_instance' is not defined```\r\nThe same goes for batch_size, x_te, y_te. How can I fix this. Thanks in advance. \r\n", "> Thanks for posting the code. I am getting and error saying:\r\n> `NameError: name 'num_instance' is not defined`\r\n> The same goes for batch_size, x_te, y_te. How can I fix this. Thanks in advance.\r\n\r\nyou can try this code\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras import backend as K\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu', name='l_1st'),\r\n  tf.keras.layers.Dense(128, activation='relu', name='l_2nd'),\r\n  tf.keras.layers.Dense(128, activation='relu', name='l_3rd'),\r\n  tf.keras.layers.Dense(128, activation='relu', name='l_4th'),\r\n  tf.keras.layers.Dense(128, activation='relu', name='l_5th'),\r\n  tf.keras.layers.Dense(128, activation='relu', name='l_6th'),\r\n  tf.keras.layers.Dense(128, activation='relu', name='l_7th'),\r\n  tf.keras.layers.Dense(128, activation='relu', name='l_8th'),\r\n  tf.keras.layers.Dense(128, activation='relu', name='l_9th'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax', name='dense10')\r\n])\r\n\r\nl = tf.keras.losses.SparseCategoricalCrossentropy()\r\nopt = tf.keras.optimizers.Adam(0.001)\r\n\r\nmodel.compile(optimizer=opt, loss=l, metrics=['accuracy'])\r\n\r\nclass ExtendedTensorBoard(tf.keras.callbacks.TensorBoard):\r\n\r\n  def _log_gradients(self, epoch):\r\n    step = tf.cast(epoch, dtype=tf.int64)\r\n    writer = self._train_writer\r\n    # writer = self._get_writer(self._train_run_name)\r\n\r\n    with writer.as_default(), tf.GradientTape() as g:\r\n      # here we use test data to calculate the gradients\r\n      _x_batch = x_train[:100]\r\n      _y_batch = y_train[:100]\r\n\r\n      g.watch(tf.convert_to_tensor(_x_batch))\r\n      _y_pred = self.model(_x_batch)  # forward-propagation\r\n      loss = self.model.loss(y_true=_y_batch, y_pred=_y_pred)  # calculate loss\r\n      gradients = g.gradient(loss, self.model.trainable_weights)  # back-propagation\r\n\r\n      # In eager mode, grads does not have name, so we get names from model.trainable_weights\r\n      for weights, grads in zip(self.model.trainable_weights, gradients):\r\n        tf.summary.histogram(\r\n            weights.name.replace(':', '_')+'_grads', data=grads, step=step)\r\n\r\n    writer.flush()\r\n\r\n  def on_epoch_end(self, epoch, logs=None):  \r\n  # def on_train_batch_end(self, batch, logs=None):  \r\n    # This function overwrites the on_epoch_end in tf.keras.callbacks.TensorBoard\r\n    # but we do need to run the original on_epoch_end, so here we use the super function. \r\n    super(ExtendedTensorBoard, self).on_epoch_end(epoch, logs=logs)\r\n    # super(ExtendedTensorBoard, self).on_train_batch_end(batch, logs=logs)\r\n    if self.histogram_freq and epoch % self.histogram_freq == 0:\r\n      self._log_gradients(epoch)\r\n\r\nee = ExtendedTensorBoard(histogram_freq=1, write_images=True, update_freq='batch')\r\nmodel.fit(x_train, y_train, epochs=10, callbacks=[ee], validation_data=(x_test, y_test), )\r\n# model.fit(x_train, y_train, epochs=5, callbacks=[gradient_cb, tensorboard_cb])\r\n```", "I tried to run the code on colab using tf v2.5 and faced attribute error ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/f7daea766d81693c0b8c5dce3c8ee4ce/untitled18.ipynb)..Thanks !", "Now i'm able to get specific error message in the recent Tensorflow version, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/0670ef9b9dfb4f61f0c61cd875598d02/31542.ipynb) and confirm the same. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31542\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31542\">No</a>\n"]}, {"number": 31541, "title": "Bug in IOU calculation of NMS for GPU", "body": "**System information**\r\nUbuntu 18.04, Tensorflow r2.0 from pip\r\n\r\n**Describe the current behavior**\r\nhttps://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/core/kernels/non_max_suppression_op.cu.cc#L96\r\n\r\n**Describe the expected behavior**\r\n```\r\nconst float w = fdimf(xx2, xx1);\r\nconst float h = fdimf(yy2, yy1);\r\nconst float intersection = w * h;\r\n```\r\n\r\n**Code to reproduce the issue**\r\nI believe that the calculation of intersection over union in line 96 of the GPU implementation of non-max suppression is incorrect. \r\n\r\n```\r\nboxes = np.array([[0.2, 0.3, 0.4, 0.5], [0.61, 0.71, 0.81, 0.91], [0.6, 0.7, 0.8, 0.9]], dtype=np.float32)\r\nscore = np.array([0.2, 0.7, 0.6], dtype=np.float32)\r\niou_threshold = 0.5\r\nnms_reference_cpu = tf.image.non_max_suppression(\r\n            boxes=boxes,\r\n            scores=score,\r\n            max_output_size=3,\r\n            iou_threshold=0.5,\r\n        )\r\n= [1, 0]\r\n```\r\n\r\nit will not work for GPU, but it will work for CPU. If you remove the +1 (not sure why you would want to do that in the first place), they match.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the minimal code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the Github new issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "a duplicate of https://github.com/tensorflow/tensorflow/pull/28745#issuecomment-512949342", "Thanks @ppwwyyxx you\u2019re right, did not see this other issue. Will close."]}, {"number": 31540, "title": "Tensorflow 2 creating custom dataset ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0.0-beta\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIMO there is currently not a really clean way to build an own dataset by using/subclassing the tf.data.Dataset-class. Specially for datasets which have more complex structure as [ICDAR](https://rrc.cvc.uab.es/?ch=4), here the labels are made of text-files with bounding-boxes and transcripts. In these cases even tensorflow_datasets can not be used in a straight way as described [here] (https://www.tensorflow.org/datasets/add_dataset).\r\n\r\nSo i would like to know what is actually the correct procedure to use the Dataset-Api for creating a custom dataset? \r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\nMost of object detection tasks\r\n\r\n**Any Other info.**\r\n", "comments": ["Have you taken a look at [TensorFlow Datasets](https://www.tensorflow.org/datasets)? It offers support to users who would like to add a [custom dataset](https://www.tensorflow.org/datasets/add_dataset), rather than just use `tf.data.Dataset`.", "@arashwan Here's an example how you can create a fully custom dataset using `tf.data.Dataset`. I'm reading a celeb_a dataset, that is located at `'dataset'` directory. Here's how the dataset directory looks like...\r\n```\r\ndataset\r\n\u251c\u2500\u2500 img_align_celeba\r\n\u2502   \u2514\u2500\u2500 img_align_celeba\r\n\u251c\u2500\u2500 list_attr_celeba.csv\r\n\u251c\u2500\u2500 list_bbox_celeba.csv\r\n\u251c\u2500\u2500 list_eval_partition.csv\r\n\u2514\u2500\u2500 list_landmarks_align_celeba.csv\r\n```\r\n\r\n```python\r\nimage_paths = sorted(glob.glob(os.path.join('dataset', 'img_align_celeba', 'img_align_celeba', '*.jpg')))\r\n\r\ndf = pd.read_csv('dataset/list_attr_celeba.csv')\r\ndf.replace(to_replace = -1, value = 0, inplace = True)\r\nlabels = df.iloc[:, 1:].values\r\n\r\nprint(image_paths[:2])\r\nprint(labels[:2])\r\n# prints\r\n'''\r\n['dataset/img_align_celeba/img_align_celeba/000001.jpg', 'dataset/img_align_celeba/img_align_celeba/000002.jpg']\r\n[[0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0\r\n  1 0 0 1]\r\n [0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0\r\n  0 0 0 1]]\r\n'''\r\n\r\n# here's the tensorflow part...\r\n@tf.function\r\ndef read_image(image_path):\r\n    image = tf.io.read_file(image_path)\r\n    # tensorflow provides quite a lot of apis for io\r\n    image = tf.image.decode_image(image, channels = 3, dtype = tf.float32)\r\n    return image\r\n\r\n@tf.function\r\ndef normalize(image):\r\n    image = (image - tf.reduce_min(image))/(tf.reduce_max(image) - tf.reduce_min(image))\r\n    image = (2 * image) - 1\r\n    return image\r\n\r\n@tf.function\r\ndef augment(image):\r\n    image = tf.image.random_crop(image, (178, 178, 3))\r\n    image = tf.image.resize(image, (256, 256))\r\n    image = tf.image.random_flip_left_right(image)\r\n    image = tf.image.random_saturation(image, 0.5, 2.0)\r\n    image = tf.image.random_brightness(image, 0.5)\r\n    return image\r\n\r\n@tf.function\r\ndef preprocess(image_path, label):\r\n    image = read_image(image_path)\r\n    image = augment(image)\r\n    image = normalize(image)\r\n    return image, label\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\r\ndataset = dataset.map(preprocess, num_parallel_calls = tf.data.experimental.AUTOTUNE)\r\n# you can do almost all sorts of custom preprocesses....\r\ndataset = dataset.shuffle(buffer_size = 1024)\r\ndataset = dataset.batch(batch_size = 128)\r\ndataset = dataset.prefetch(buffer_size = f.data.experimental.AUTOTUNE)\r\n\r\nfor x, y in dataset:\r\n    break\r\n\r\nprint(x.shape, y.shape)\r\n# prints\r\n# (128, 256, 256, 3) (128, 40)\r\n```\r\n\r\nHope this helps", "@braindotai and @dynamicwebpaige thanks for your suggestions I will try them and report ASAP.\r\n"]}, {"number": 31539, "title": "tf.image.resize_bilinear() outputs garbage pixel values", "body": "I am trying to resize images using bilinear interpolation but unable to get the correct image while saving the image. I am getting the the outputs similar to [#25591](https://github.com/tensorflow/tensorflow/issues/25591). Applying the solution mentioned in that thread is not helping.\r\n\r\nConfig,\r\nCUDA: 10.0\r\ncudNN 7.4.2\r\nTensorflow-gpu 1.14\r\nnumpy 1.16\r\n\r\nBelow is the reproducible code. \r\n\r\n```\r\nimg_path = 'C:/x.bmp'\r\noutput_dir =  'C:/Users/Desktop/output/'\r\nimage_res = (536,640)\r\nimgname_string = img_path.split('/')[-1]\r\nprint(imgname_string)\r\n\r\ndef preprocess(img_path):\r\n        img_read = tf.read_file(img_path)\r\n        img_decode = tf.image.decode_bmp(img_read,channels=0)\r\n        img_f32 = tf.image.convert_image_dtype(img_decode, dtype=tf.float32)\r\n        img_4d = tf.expand_dims(img_f32,axis=0)\r\n        img_new = tf.image.resize_bilinear(img_4d, size=image_res)\r\n        img_final = tf.squeeze(img_new,[0])\r\n        \r\n        return img_final\r\n\r\ninit_op = tf.global_variables_initializer()\r\nwith tf.Session() as sess:\r\n    sess.run(init_op)\r\n    process_img = sess.run(preprocess(img_path))\r\n    print(process_img.dtype)\r\n    p = process_img.astype(dtype=np.uint8)\r\n    img = Image.fromarray(np.squeeze(p,axis=-1),'L')\r\n    relative_path = os.path.join(output_dir,imgname_string)\r\n    print(relative_path)\r\n    img.save(relative_path)\r\n```", "comments": ["Solution is to remove `img_f32 = tf.image.convert_image_dtype(img_decode, dtype=tf.float32)` if using `tf.image.resize_bilinear()` API. It works fine."]}, {"number": 31538, "title": "Cocoapods cannot install TensorFlowLiteC", "body": "I am getting [!] Error installing TensorFlowLiteC when trying to install pods. I am using \r\npod 'TensorFlowLiteSwift' in my podfile. \r\n\r\nError:\r\n[!] Error installing TensorFlowLiteC\r\n[!] /usr/bin/curl -f -L -o /var/folders/8h/w7cb5w9x4m9dg8l9rr3h1qwas0000gn/T/d20208412-2822-n13n01/file.tgz https://dl.google.com/dl/cpdc/9d0ec5e53f4ff34a/TensorFlowLiteC-0.2.0.tar.gz --create-dirs --netrc-optional --retry 2", "comments": ["@berkerbilgi \r\n\r\nI am not able to open the link you have provided.Please, share the correct link.\r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the Github new issue template.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "@ravikyram \r\n\r\nWhen i am trying to install \"TensorFlowLiteSwift\" library with Cocoapods, getting an error. The link is not working and i think this is the problem. Link is provided by google for downloading the TensorFlowLiteC library.\r\n\r\nSo i cannot share the correct link because it is google link. \r\n\r\nThanks.", "@miaout17 can you advise?", "@miaout17 is there any update? Still cannot install the library. It gives 404 not found error", "Are you using the 1.14 TensorFlowLiteSwift CocoaPod? If not, can you try updating to that version?", "The `0.2.0` was an experimental release and it no longer worked. Sorry for your inconvenience. \r\n\r\nTo solve this problem: \r\nIf you specified version in Podfile, update it to `1.14.0`. \r\nRun `pod update`. \r\n\r\nLet us know if it works. ", "@miaout17 @jdduke thanks for your reply. After pod update and specify version to 1.14.0, I installed successfully.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31538\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31538\">No</a>\n", "Thanks again @berkerbilgi for flagging the issue, we've gone ahead and restored the experimental `0.2.0` release to avoid further confusion."]}, {"number": 31537, "title": "Expose ConvLSTM2DCell", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0-beta1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nExpose ConvLSTM2DCell via Keras API (as is the case with LSTMCell and GRUCell).\r\n\r\n**Will this change the current api? How?**\r\nYes, ConvLSTM2DCell will be added (to tf.keras.layers).\r\n\r\n**Who will benefit with this feature?**\r\nAnyone with an interest in machine learning with video sequences (e.g. https://github.com/tensorflow/models/tree/master/research/video_prediction) who needs a way to integrate convolutional LSTM layers into their model that is more flexible than what the current API provides. \r\n", "comments": ["@loffermann ,\r\nIn Github, with Tensorflow Version 2.0, `ConvLSTM2DCell` is in `keras/layers/convolutional_recurrent.py` in this [Github link](https://github.com/tensorflow/tensorflow/blob/d90e521d71b88f469e68eb1a467606ea6d44c733/tensorflow/python/keras/layers/convolutional_recurrent.py) and `LSTMCell` is in `keras/layers/recurrent.py` in [this link](https://github.com/tensorflow/tensorflow/blob/d90e521d71b88f469e68eb1a467606ea6d44c733/tensorflow/python/keras/layers/recurrent.py). \r\n\r\nThat is, I mean to say that both the Methods are exposed via Keras API.\r\n\r\nIn case you have different opinion, please provide more details, or the links from Tensorflow Website or from Github Site. Thanks!", "Thanks for your reply.\r\nAs you described, the class for ConvLSTM2DCell is there \u2013 and I am aware of that. However, a @keras_export('keras.layers.ConvLSTM2DCell') is missing (line 484 [here](https://github.com/tensorflow/tensorflow/blob/d90e521d71b88f469e68eb1a467606ea6d44c733/tensorflow/python/keras/layers/convolutional_recurrent.py)), and thus the class is currently not accessible via the Keras API.\r\n\r\nI checked by calling tf.keras.layers.ConvLSTM2DCell in TF 2.0.0-beta1, which fails with an AttributeError stating that \"'tensorflow.python.keras.api._v2.keras.layers' has no attribute 'ConvLSTM2DCell'\".", "@loffermann ,\r\nI could run the below code without any error, for `Tensorflow` Version, `2.0.0-beta1`.\r\n\r\n```\r\n!pip install tensorflow==2.0.0-beta1\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom keras.layers import ConvLSTM2DCell\r\n```\r\n\r\nPlease try it at your end and let me know if it helps. Thanks!", "This works, thank you.\r\nWouldn't it be nice though to just add the export decorator for consistency? I guess this would also provide an entry and documentation for [the API docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers), making an already implemented feature more visible.\r\nIf this is not possible for whatever reason, feel free to close this issue. ", "@loffermann ,\r\n`ConvLSTM2D` is already present in the link mentioned by you. Closing the issue as it is resolved.", "The import statement proposed by @rmothukuru does not work for me anymore. I am using the following instead:\r\n`from tensorflow.python.keras.layers.convolutional_recurrent import ConvLSTM2DCell`"]}, {"number": 31536, "title": "Call to build function of GRUCell object segfaults if Keras precision is set to float16", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10.0/7.5\r\n- GPU model and memory: Nvidia RTX 2080 Ti (11 GB)\r\n\r\n**Describe the current behavior**\r\nA call to the build() function of an instantiated GRUCell object with recurrent_initializer set to \"orthogonal\" (this is the default) terminates with a segmentation fault if Keras precision is set to float16.\r\n\r\n**Describe the expected behavior**\r\nCall to build() should succeed.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ntf.keras.backend.set_floatx(\"float16\")\r\n\r\ndim = 100\r\ncell = tf.keras.layers.GRUCell(dim, recurrent_initializer=\"orthogonal\")\r\ncell.build([dim])\r\n```\r\nTest: set a different recurrent initializer (e.g. `recurrent_initializer=\"glorot_uniform\"`) or remove `tf.keras.backend.set_floatx(\"float16\")`. The function should now execute successfully.\r\n\r\n**Other info / logs**\r\nn/a", "comments": ["@loffermann ,\r\nPlease be informed that I tried executing the given code and I did not face any error.Can you provide more info?Thanks!", "@oanush I tried another computer (Ubuntu 18.04 and Quadro M1000M) and could not reproduce the issue either.\r\nHowever, the issue is still reproducible on my original machine. I suspect this might be due to an unfortunate combination of CUDA drivers and Ubuntu kernel. I'll check that and come back if the problem still persists.\r\nThanks for helping me out!"]}, {"number": 31535, "title": "Colab TF 2.0 runs out of memory if eager execution is enabled", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (GPU Runtime)\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 3.6 (Google Colab 3.6 Environment)\r\n- CUDA/cuDNN version:  Google Colab (GPU Runtime)\r\n- GPU model and memory:  Google Colab (GPU Runtime)\r\n\r\n**Describe the current behavior**\r\nIf I use a large U-net with an Inception-ResNetv2 backbone and eager execution is enabled,(default with tf 2.0), the machine runs out of memory. If eager execution is disabled (`tf.compat.v1.disable_eager_execution()`), it works fine.\r\n\r\nMaybe this is a bug in Google colab, as the code works fine with eager execution enabled on my machine with 8 GB of RAM and a Geforce 1060.\r\n\r\n**Describe the expected behavior**\r\nI would expect it to work in both modes similar.\r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1-xgkBBUqZw6rS2WhxUMBZuY6j1uxG_e-", "comments": ["@kielnino \r\nI tried to reproduce your code in Google Colab but i am getting the below error.Please, help me to reproduce the issue.\r\n![image](https://user-images.githubusercontent.com/51902062/62949114-3c752400-be03-11e9-91ab-614ced24a032.png)\r\n\r\n", "@ravikyram Thank you for having a look.\r\nI forgot the efficientnet-module in the pip-installs. The colab is updated now.", "@kielnino \r\nI have reproduced the issue in Colab with TF 2.0 beta1 in eager mode i got the below error`OSError: [Errno 12] Cannot allocate memory`.However i tried by disabling eager execution it was stuck for more than 30 mins after 1st epoch and it is till executing. Can you please confirm? Thanks!", "Yes, thats another deadlock-issue with the `tf.keras.utils.Sequence` class and enabled multiprocessing, which I'm investigating at the moment. I've disabled multiprocessing in the Colab now, so that it should work with eager execution disabled.", "I am able to execute colab link provided with eager execution disabled.Please, find the [gist ](https://colab.research.google.com/drive/15MT-9pG2UBiXRrkUDXW5BGoQGzw0BpBS)here.Thanks!", "@kielnino I don't see any issue when I replace `!pip install -q tensorflow-gpu==2.0.0-beta1` with `!pip install -q tf-nightly-gpu-2.0-preview`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/0ef0f98f59b3496bab443546511fac02/tf_31535.ipynb). Thanks!", "Your are right. Thanks for the hint. So I will use the nightly build instead.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31535\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31535\">No</a>\n"]}, {"number": 31534, "title": "Correct typo: ']' has been written twice.", "body": "An error has been found and corrected in the annotation.\r\n\r\nIf you have a problem, let me know.\r\n\r\nThank you.", "comments": []}, {"number": 31533, "title": "In reference with https://github.com/tensorflow/tensorflow/issues/31406", "body": "\r\n@jdduke \r\n\r\nIn reference with the issue\r\nhttps://github.com/tensorflow/tensorflow/issues/31406\r\n\r\nBasically what needs to be identified and what changes needs to be done in accordance with the model.\r\n\r\nIt can be understood that TensorFlowObjectDetectionAPIModel.java will require the changes. But what are those changes. \r\n\r\nAs it is advised to check io shape with graph. Putting its result over here will help in any way possible?\r\n\r\n", "comments": ["> As it is advised to check io shape with graph. Putting its result over here will help in any way possible?\r\n\r\nThat's fine, but I would recommend looking at the way input shapes/types are used in TensorFlowObjectDetectionAPIModel to better understand what changes you need to make.", "@VirRajpurohit ,\r\nCan you please go through that [link](https://www.tensorflow.org/lite/models/object_detection/overview).Thanks!", "@oanush \r\nThanks for sharing link. I have gone through it.\r\n\r\nTried to understand the theory but as I am stepping as beginner in tensorflow,.\r\n\r\nI just want to know that\r\n\r\nCannot copy between a TensorFlowLite tensor with shape [1, 1917, 4] and a Java object with shape [1,10, 4].\r\n\r\nWhat above error actually says, Does anything needs to be changed in my model, if yes than what? ", "1917 refers to the number of detections. How did you convert/obtain your model?\r\n\r\nIn general, it's better to use StackOverflow for this kinds of basic usability questions. Please post there and provide a link here."]}, {"number": 31532, "title": ".", "body": ".\r\n", "comments": []}, {"number": 31531, "title": "Error when using batch renormalisation option under a strategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS: Windows 10\r\n- TensorFlow installed from (source or binary): tensorflow-gpu binary on pip\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen I try to use `renorm=True` with `tf.keras.layers.BatchNormalization` under `tf.distribute.MirroredStrategy` I get the following error: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.\r\n\r\n\r\n**Describe the expected behavior**\r\nI can use batch renormalisation under mirrored strategy.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nstrat = tf.distribute.MirroredStrategy()\r\nwith strat.scope():\r\n  inp = tf.keras.Input((28, 28))\r\n  tf.keras.layers.BatchNormalization(renorm=True)(inp)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThe following is the stack trace:\r\n```\r\n<ipython-input-3-d17cda041220> in <module>()\r\n      2 with strat.scope():\r\n      3   inp = tf.keras.Input((28, 28))\r\n----> 4   tf.keras.layers.BatchNormalization(renorm=True)(inp)\r\n\r\n16 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    632                     outputs = base_layer_utils.mark_as_return(outputs, acd)\r\n    633                 else:\r\n--> 634                   outputs = call_fn(inputs, *args, **kwargs)\r\n    635 \r\n    636             except TypeError as e:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in call(self, inputs, training)\r\n    734       if self.renorm:\r\n    735         r, d, new_mean, new_variance = self._renorm_correction_and_moments(\r\n--> 736             new_mean, new_variance, training, inputs_size)\r\n    737         # When training, the normalized values (say, x) will be transformed as\r\n    738         # x * gamma + beta without renorm, and (x * r + d) * gamma + beta\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in _renorm_correction_and_moments(self, mean, variance, training, inputs_size)\r\n    598     new_mean = _update_renorm_variable(self.renorm_mean,\r\n    599                                        self.renorm_mean_weight, mean,\r\n--> 600                                        inputs_size)\r\n    601     new_stddev = _update_renorm_variable(self.renorm_stddev,\r\n    602                                          self.renorm_stddev_weight, stddev,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in _update_renorm_variable(var, weight, value, inputs_size)\r\n    593       def _fake_update():\r\n    594         return array_ops.identity(var)\r\n--> 595       return tf_utils.smart_cond(training, _do_update, _fake_update)\r\n    596 \r\n    597     # TODO(yuefengz): colocate the operations\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py in smart_cond(pred, true_fn, false_fn, name)\r\n     56         pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n     57   return smart_module.smart_cond(\r\n---> 58       pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n     59 \r\n     60 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)\r\n     57   else:\r\n     58     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\r\n---> 59                                  name=name)\r\n     60 \r\n     61 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    505                 'in a future version' if date is None else ('after %s' % date),\r\n    506                 instructions)\r\n--> 507       return func(*args, **kwargs)\r\n    508 \r\n    509     doc = _add_deprecated_arg_notice_to_docstring(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in cond(pred, true_fn, false_fn, strict, name, fn1, fn2)\r\n   1975     try:\r\n   1976       context_t.Enter()\r\n-> 1977       orig_res_t, res_t = context_t.BuildCondBranch(true_fn)\r\n   1978       if orig_res_t is None:\r\n   1979         raise ValueError(\"true_fn must have a return value.\")\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in BuildCondBranch(self, fn)\r\n   1812     \"\"\"Add the subgraph defined by fn() to the graph.\"\"\"\r\n   1813     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n-> 1814     original_result = fn()\r\n   1815     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n   1816     if len(post_summaries) > len(pre_summaries):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in _do_update()\r\n    582           weight_value = array_ops.constant(1., dtype=weight.dtype)\r\n    583         new_var = self._assign_moving_average(var, value, self.renorm_momentum,\r\n--> 584                                               inputs_size)\r\n    585         new_weight = self._assign_moving_average(weight, weight_value,\r\n    586                                                  self.renorm_momentum,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in _assign_moving_average(self, variable, value, momentum, inputs_size)\r\n    447   def _assign_moving_average(self, variable, value, momentum, inputs_size):\r\n    448     with K.name_scope('AssignMovingAvg') as scope:\r\n--> 449       with ops.colocate_with(variable):\r\n    450         decay = ops.convert_to_tensor(1.0 - momentum, name='decay')\r\n    451         if decay.dtype != variable.dtype.base_dtype:\r\n\r\n/usr/lib/python3.6/contextlib.py in __enter__(self)\r\n     79     def __enter__(self):\r\n     80         try:\r\n---> 81             return next(self.gen)\r\n     82         except StopIteration:\r\n     83             raise RuntimeError(\"generator didn't yield\") from None\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _colocate_with_for_gradient(self, op, gradient_uid, ignore_existing)\r\n   4398   def _colocate_with_for_gradient(self, op, gradient_uid,\r\n   4399                                   ignore_existing=False):\r\n-> 4400     with self.colocate_with(op, ignore_existing):\r\n   4401       if gradient_uid is not None and self._control_flow_context is not None:\r\n   4402         self._control_flow_context.EnterGradientColocation(op, gradient_uid)\r\n\r\n/usr/lib/python3.6/contextlib.py in __enter__(self)\r\n     79     def __enter__(self):\r\n     80         try:\r\n---> 81             return next(self.gen)\r\n     82         except StopIteration:\r\n     83             raise RuntimeError(\"generator didn't yield\") from None\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in colocate_with(self, op, ignore_existing)\r\n   4447       raise ValueError(\"Trying to reset colocation (op is None) but \"\r\n   4448                        \"ignore_existing is not True\")\r\n-> 4449     op = _op_to_colocate_with(op)\r\n   4450 \r\n   4451     # By default, colocate_with resets the device function stack,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _op_to_colocate_with(v)\r\n   6714   # happen soon, perhaps this hack to work around the circular\r\n   6715   # import dependency is acceptable.\r\n-> 6716   if hasattr(v, \"handle\") and hasattr(v.handle, \"op\") and isinstance(\r\n   6717       v.handle.op, Operation):\r\n   6718     return v.handle.op\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/values.py in handle(self)\r\n    641       device = distribute_lib.get_update_device()\r\n    642       if device is None:\r\n--> 643         raise ValueError(\"`handle` is not available outside the replica context\"\r\n    644                          \" or a `tf.distribute.Strategy.update()` call.\")\r\n    645     return self.get(device=device).handle\r\n```", "comments": ["Could reproduce this issue with Tensorflow Version 1.14. Gist is in [this link](https://colab.sandbox.google.com/gist/rmothukuru/1837bcd5350bbe4fb77f4342bdb4f369/dist_strat_31531.ipynb). \r\n\r\nThanks. ", "Thanks, this seems to be real issue.  We are going to try to fix it.", "@rmothukuru could you please update the permissions on that link?", "@isaprykin [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/1dcc12d3ab39170115bc7aa066f4d3e5/tf_31531.ipynb) is the gist. The same error is in `tf-nightly-gpu` along with `tensorflow==1.14.0`. Same error is in TF2.0 and the [gist is here](https://colab.sandbox.google.com/gist/jvishnuvardhan/127973ecc9e4087b2d018c4e52452c35/tf20_31531_batchnorm.ipynb). Thanks!", "I am running into this issue as well. Any update on the fix?", "Any updates on this?", "still existing in tf 2.1", "Hello there,\r\nAny update on this issue?!!", "Is there a system for voting on issue priority? This one is a real showstopper for training large models.", "I tried updating my TensorFlow to the latest version (2.2.0) and it fixed the issue for me", "Still have this issue in TF 2.1.0, 2.2.0, and 2.3.0 all for CPU and GPU (with Intel MKL-DNN/MKL and CUDA 10.1). This is in Debian 9, GCP CAIP Notebook env, python 3.7.6.\r\n\r\nIt seems to be mostly fixed in tf-nightly 2.4.0-dev20200731 using fit, however with a custom training loop the number of gradients fails the below assertion. My guess is gamma and beta aren't being picked up either in the func_graph outputs (LHS) or in the model.trainable_variables (RHS).\r\n\r\n```\r\n        gradients = gradient_tape.gradient(\r\n    /home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:1083 gradient  **\r\n        unconnected_gradients=unconnected_gradients)\r\n    /home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py:77 imperative_grad\r\n        compat.as_str(unconnected_gradients.value))\r\n    /home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:162 _gradient_function\r\n        return grad_fn(mock_op, *out_grads)\r\n    /home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py:123 _IfGrad\r\n        true_graph, grads, util.unique_grad_fn_name(true_graph.name))\r\n    /home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py:386 _create_grad_func\r\n        func_graph=_CondGradFuncGraph(name, func_graph))\r\n    /home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:986 func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n    /home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py:385 <lambda>\r\n        lambda: _grad_fn(func_graph, grads), [], {},\r\n    /home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py:361 _grad_fn\r\n        assert len(func_graph.outputs) == len(grads)\r\n\r\n    AssertionError: \r\n```", "I have the same issue as @ryangillard in TF 2.3.1 using batch normalization in a custom training loop.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31531\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31531\">No</a>\n", "I think this issue is still present in TF2.5. I opened an other issue since I don't know if I'm having the same bug or only a similar one: #52986 "]}, {"number": 31530, "title": "Added tutorial on Neural Machine Translation (English to German)", "body": "The tutorial covers Tokenisation, cleaning, preprocessing, and configuring Cloud TPUs to work with `tf.keras` models. It goes through the process of converting raw text into padded sequences for the LSTM model. The tutorial goes through the initialization of a parallelly-distributed training `strategy`.", "comments": ["Check out this pull request on ReviewNB: https://app.reviewnb.com/tensorflow/tensorflow/pull/31530 \n\n You'll be able to see notebook diffs and discuss changes. Powered by <a href='https://www.reviewnb.com'>ReviewNB</a>.", "I don't believe we're adding any new examples there. @wolffg would be able to comment.", "Hey there, I initially tried to add this to the TF docs repo but I was asked to try adding this into the main TF repo under the examples directory. Are there any other ways I can add this demo inside? I'd be grateful for any guidance or direction in this regard.\r\n\r\nCheers! ", "@jhseu is right, we are not adding new examples to the core repo, so I will close this PR.\r\n\r\nIf I'm not mistaken, the original PR in docs was [919](https://github.com/tensorflow/docs/pull/919).  \r\n\r\nAs noted there, for new notebooks of this kind, we'd recommend A) update this example to TensorFlow 2 (although I understand that's tricky, especially if you're using Colab TPUs), and B) submit the PR this to the [community](https://github.com/tensorflow/examples/tree/master/community) directory of the examples repo. \r\n\r\nThanks!"]}, {"number": 31529, "title": "A bug in the docs of count_nozero", "body": "It is the \"reduction_indices\" that is deprecated instead of \"axis\"\r\nSame bugs appear in tf2.0, too.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31529) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31529) for more info**.\n\n<!-- ok -->", "@mihaimaruseac can we merge this ?", "Oh, just saw the branch number now. No, it should be on master first and then cherry-picked if needed to release branches"]}, {"number": 31528, "title": "How to identify the input and output array of a model for  \".tflite\" conversion process. ", "body": "I am trying to generate quantized .tflite model from .pb file. For the process I require the 'input_arrays' and 'output_arrays' of the model.\r\nI've tried using the below methods to identify the input array and output array. But none of them worked.\r\n\r\nMethod 1:\r\n\r\n```\r\nimport tensorflow as tf \r\n frozen='/output/freeze/frozen_inference_graph.pb'\r\n gf = tf.GraphDef() \r\n gf.ParseFromString(open(frozen,'rb').read()) \r\n [n.name + '=>' +  n.op for n in gf.node if n.op in ('Softmax','Placeholder')]   \r\n [n.name + '=>' +  n.op for n in gf.node if n.op in ( 'Softmax','Mul')]\r\n\r\n```\r\nMethod 2:\r\n```\r\nimport tensorflow as tf\r\ngf = tf.GraphDef()   \r\nm_file = open('/output/freeze/frozen_inference_graph.pb','rb')\r\ngf.ParseFromString(m_file.read())\r\nfor n in gf.node:\r\n    print( n.name )\r\n\r\n```\r\ntflife conversion query:\r\nimport tensorflow as tf\r\ngraph_def_file = \"new/barun/frozen_inference_graph.pb\"\r\ninput_arrays = ['image_tensor']\r\noutput_arrays = ['BoxPredictor_5/ClassPredictor/act_quant/FakeQuantWithMinMaxVars']\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n  graph_def_file, input_arrays, output_arrays,input_shapes={\"image_tensor\":[1,300,300,3]})\r\nconverter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\ntflite_model = converter.convert()\r\nopen(\"frozen_inference_graph_fd2819_2.tflite\", \"wb\").write(tflite_model)`\r\n\r\nHow to find out the input_array and output_array from a tensorflow model(.pb file) ?``", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nIf you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31528\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31528\">No</a>\n", "@vismayaps  Did you got the solution for this? I am facing this the same issue, please help.How to get the input and output arrays from a frozen model?\r\n"]}, {"number": 31527, "title": "Performance Issue in C# Example", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): use code in TF# demo\r\n- OS Platform and Distribution: **Windows 10**\r\n- TensorFlow installed from (source or binary): **source(by pip)**\r\n- TensorFlow version (use command below): **1.13**\r\n- Python version: **3.7**\r\n- CUDA/cuDNN version: run on **CPU**\r\n\r\n**Describe the current behavior**\r\nI implemented a Griffin-Lim algorithm(GL, a speech processing method used for reconstructing wave from mel-spectrogram) with TF-python, and dump it into a pb file.\r\nThen I load graph of Griffin-Lim from the pb file, and using runner.Run() to call the computing graph and get the output of the last Node.\r\n\r\nThe result of output is correct and the voice quality is on par with TF-python generated.\r\nBut runner.Run() in TF# runs 5-10 times slower than session.Run() in TF-python.\r\nFor example, to generate a 3 seconds wave, runner.Run() in TF# need about 2.7 seconds, but session.Run() in TF-python only need 0.3-0.6 seconds.\r\n\r\nhere are the generate key codes in C#:\r\n\r\n            TFTensor input = new TFTensor(melSpectrum);\r\n            var runner = this.session.GetRunner();\r\n            runner.AddInput(this.input, input);\r\n            runner.Fetch(this.output);\r\n            TFTensor[] results = runner.Run();\r\n\r\nI went back and checked the related code I call runner.Run().\r\n\r\n1. I have already kept a TFSession instant.\r\n2. When I initialize the session & graph, I also warm up them by faking a melSpectrum and calling upper codes. If sess& graph weren't warmed up, runner.Run() will cost 6.8-7seconds to generate a 3 seconds wav.\r\n\r\n**Describe the expected behavior**\r\nI think runner.Run() in TF# should run as fast as session.Run() in TF-python\r\n\r\nAny clues?\r\nThanks\r\n", "comments": ["@ys10 ,\r\nIn order to expedite the trouble-shooting process, please provide compelete code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31527\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31527\">No</a>\n"]}, {"number": 31526, "title": "TFLite Metal GPU delegate: Add operator does not support broadcasting", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS 12.3.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone Xr\r\n- TensorFlow installed from (source or binary): binary, but TFLite compiled from source\r\n- TensorFlow version (use command below): b'v1.13.2-5-g04256c89d8' 1.13.2\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): clang-1001.0.46.4\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: iPhone Xr\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nRunning the single-operation model given below using the Metal GPU delegate gives a different output than when executing on CPU or with the full Tensorflow interpreter.\r\n\r\n**Describe the expected behavior**\r\nThe Metal GPU delegate gives the same output as the CPU interpreter.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n[model-broken.tflite.zip](https://github.com/tensorflow/tensorflow/files/3490983/model-broken.tflite.zip)\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@LK \r\n\r\nSorry, but @NikolayChirkov switched teams and is no more working on this.  We currently don't have an owner.  This may take some time until we get a backfill =/", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31526\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31526\">No</a>\n"]}, {"number": 31525, "title": "Tensorflow Estimator InvalidArgumentError", "body": "TF 1.13.1 binary on ubuntu 18.10\r\n\r\nthe same issue with that on stackoverflow: https://stackoverflow.com/questions/54089982/tensorflow-estimator-invalidargumenterror\r\n\r\nThe issue occurs when the categorical feature size is very large and reports bug of `Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint.` \r\n\r\nhowever, when reducing the size of categorical feature size, the issue disappears.\r\n\r\nOne very important detail is that I build the estimatorSpec myself, which is quite important because using the standard `DNNLinearCombinedClassifier` would not have the above error. At the same time, if I reduce the the size of categorical feature size with the `estimatorSpec` I build myself, the issue also disappears. And the above issue poster on the stackoverflow solves this problem by limiting the feature size to 17k features. Unfortunately with the feature size going larger than 17k, the same issue happens.\r\n\r\nSo is there any restrict of the categorical feature size on the `estimatorSpec ` ?", "comments": ["It seems 2GB of embedding matrix is the bottleneck", "@sjtusmartboy Did you try with recent versions of TF (`1.14.0`, `tf-nightly`, `2.0b1`). Thanks!", "I don't fully understand what is being described here-- the linked SO post seems unrelated? Can you clarify, and provide a minimal code sample to reproduce the issue you are describing?", "@sjtusmartboy, Can you provide the minimal code snippet to analyze the reported issue.Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 31524, "title": " how to replace the wrong node definitions in the frozen graph when importing frozen graph with batchnorm", "body": "there is a confusion when i try to do as the answers in the question #3628, and could anyone give me a proper solution for it?\r\nhe says:\r\n''\r\nAn ugly way to get it working:\r\nmanually replace the wrong node definitions in the frozen graph\r\nRefSwitch --> Switch + add '/read' to the input names\r\nAssignSub --> Sub + remove use_locking attributes\r\n''\r\n\r\nbut, I printed the nodes in my net,:\r\n''\r\ninputs\r\nlabels\r\nkeep_prob\r\nMobilenetV2/input\r\nMobilenetV2/Conv/weights/Initializer/truncated_normal/shape\r\nMobilenetV2/Conv/weights/Initializer/truncated_normal/mean\r\nMobilenetV2/Conv/weights/Initializer/truncated_normal/stddev\r\nMobilenetV2/Conv/weights/Initializer/truncated_normal/TruncatedNormal\r\nMobilenetV2/Conv/weights/Initializer/truncated_normal/mul\r\nMobilenetV2/Conv/weights/Initializer/truncated_normal\r\nMobilenetV2/Conv/weights\r\nMobilenetV2/Conv/weights/Assign\r\nMobilenetV2/Conv/weights/read\r\nMobilenetV2/Conv/kernel/Regularizer/l2_regularizer/scale\r\nMobilenetV2/Conv/kernel/Regularizer/l2_regularizer/L2Loss\r\nMobilenetV2/Conv/kernel/Regularizer/l2_regularizer\r\nMobilenetV2/Conv/dilation_rate\r\nMobilenetV2/Conv/Conv2D\r\nMobilenetV2/Conv/BatchNorm/gamma/Initializer/ones\r\nMobilenetV2/Conv/BatchNorm/gamma\r\nMobilenetV2/Conv/BatchNorm/gamma/Assign\r\nMobilenetV2/Conv/BatchNorm/gamma/read\r\nMobilenetV2/Conv/BatchNorm/beta/Initializer/zeros\r\nMobilenetV2/Conv/BatchNorm/beta\r\nMobilenetV2/Conv/BatchNorm/beta/Assign\r\nMobilenetV2/Conv/BatchNorm/beta/read\r\nMobilenetV2/Conv/BatchNorm/moving_mean/Initializer/zeros\r\nMobilenetV2/Conv/BatchNorm/moving_mean\r\nMobilenetV2/Conv/BatchNorm/moving_mean/Assign\r\nMobilenetV2/Conv/BatchNorm/moving_mean/read\r\nMobilenetV2/Conv/BatchNorm/moving_variance/Initializer/ones\r\nMobilenetV2/Conv/BatchNorm/moving_variance\r\nMobilenetV2/Conv/BatchNorm/moving_variance/Assign\r\nMobilenetV2/Conv/BatchNorm/moving_variance/read\r\nMobilenetV2/Conv/BatchNorm/Const\r\nMobilenetV2/Conv/BatchNorm/Const_1\r\nMobilenetV2/Conv/BatchNorm/FusedBatchNorm\r\nMobilenetV2/Conv/BatchNorm/Const_2\r\nMobilenetV2/Conv/BatchNorm/AssignMovingAvg/sub/x\r\nMobilenetV2/Conv/BatchNorm/AssignMovingAvg/sub\r\nMobilenetV2/Conv/BatchNorm/AssignMovingAvg/sub_1\r\nMobilenetV2/Conv/BatchNorm/AssignMovingAvg/mul\r\nMobilenetV2/Conv/BatchNorm/AssignMovingAvg\r\nMobilenetV2/Conv/BatchNorm/AssignMovingAvg_1/sub/x\r\nMobilenetV2/Conv/BatchNorm /AssignMovingAvg_1/sub\r\nMobilenetV2/Conv/BatchNorm/AssignMovingAvg_1/sub_1\r\nMobilenetV2/Conv/BatchNorm/AssignMovingAvg_1/mul\r\nMobilenetV2/Conv/BatchNorm/AssignMovingAvg_1\r\nMobilenetV2/Conv/Relu6\r\nMobilenetV2/expanded_conv/input\r\n''\r\n\r\nbut! the 'nodes.op' of the 'nodes' above donot exist \"RefSwitch\", please do you any suggestions?", "comments": ["btw, the nodes.op of \r\n''\r\nMobilenetV2/Conv/BatchNorm/AssignMovingAvg \r\nMobilenetV2/Conv/BatchNorm/AssignMovingAvg_1\r\n\"\r\nare all \"AssignSub \";\r\n\r\n\r\nthe nodes.op of \r\n''\r\nMobilenetV2/Conv/BatchNorm/moving_mean/Initializer/zeros\r\nMobilenetV2/Conv/BatchNorm/moving_mean\r\nMobilenetV2/Conv/BatchNorm/moving_mean/Assign\r\nMobilenetV2/Conv/BatchNorm/moving_mean/read\r\n''\r\nare 'Const', 'variableV2', 'Assign', 'Identity' \r\n\r\n\r\n\r\nps, AssignMovingAvg_1 here maybe actually point to the moving_variance.", "@bitwanghe,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.", "> @bitwanghe,\r\n> Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\n\r\n' Win 7 '\r\n' tensorflow-1.13.1 ' \r\nI restore the parameters from mobilenetV2 , and rewrite the last layer. Then retrain the whole graph.\r\n\r\nThank you! ", "here is my code:\r\n> @bitwanghe,\r\n> Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    print(\"loading model...\")\r\n    tf.reset_default_graph()  # \u6784\u5efa\u8ba1\u7b97\u56fe\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n    finetune_model_dir = './mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt'\r\n    train_records_dir = './train_records/train_records.txt'\r\n    val_records_dir = './val_records/val_records.txt'\r\n    train_labels_dir = './train_label.txt'\r\n    val_labels_dir = './val_label.txt'\r\n\r\n    batch_size = 16\r\n    num_classes = 223\r\n    train_epoches = 1000\r\n    train_num = get_example_nums(train_labels_dir)\r\n    train_iterations = train_num // batch_size\r\n    val_num = get_example_nums(val_labels_dir)\r\n    val_iterations = val_num // batch_size\r\n    lr = 0.01\r\n    val_log_step = 1\r\n\r\n\r\n    inputs = tf.placeholder(tf.float32, shape=[None, 224, 224, 3], name='inputs')\r\n    labels = tf.placeholder(tf.int32, shape=[None], name='labels')\r\n    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\r\n    #     is_training = tf.placeholder(tf.bool, name='is_training')\r\n    is_training = True\r\n\r\n    with slim.arg_scope(mobilenet_v2.training_scope()):\r\n        net0, endpoints = mobilenet_v2.mobilenet(inputs, num_classes=None, is_training=is_training)\r\n\r\n     with tf.variable_scope('Logits'):\r\n        logits = slim.conv2d(net0, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, biases_initializer=tf.zeros_initializer(), scope='Conv2d_1c_1x1')\r\n        logits = tf.squeeze(logits, axis=[1, 2])\r\n        \r\n\r\n    checkpoint_exclude_scopes = 'Logits'\r\n    exclusions = None\r\n    if checkpoint_exclude_scopes:\r\n        exclusions = [scope.strip() for scope in checkpoint_exclude_scopes.split(',')]\r\n    variables_to_restore = []\r\n    totalvar = slim.get_model_variables()\r\n\r\n    for var in slim.get_model_variables():\r\n        excluded = False\r\n        for exclusion in exclusions:\r\n            if var.op.name.startswith(exclusion):\r\n                excluded = True\r\n        if not excluded:\r\n            variables_to_restore.append(var)\r\n\r\n    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\r\n    loss = tf.reduce_mean(losses)\r\n    logits = tf.nn.softmax(logits, name='probability')\r\n    classes = tf.argmax(logits, axis=1, name='classes')\r\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(classes, dtype=tf.int32), labels), dtype=tf.float32))\r\n\r\n    step = tf.get_variable(\"step\", [], initializer=tf.constant_initializer(0.0), trainable=False)\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\r\n    train_step = slim.learning.create_train_op(loss, optimizer, global_step=step)\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    if update_ops:\r\n        # print(\"BN parameters: \", update_ops)\r\n        updates = tf.group(*update_ops)\r\n        train_step = control_flow_ops.with_dependencies([updates], train_step)\r\n\r\n    saver_restore = tf.train.Saver(var_list=variables_to_restore)\r\n    saver = tf.train.Saver(tf.global_variables(), max_to_keep=2)\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        saver_restore.restore(sess, finetune_model_dir)\r\n\r\n        train_images_tf, train_labels_tf = read_records(train_records_dir)\r\n        train_images_tf_batch, train_labels_tf_batch = get_batch_images(train_images_tf, train_labels_tf, batch_size, num_classes, shuffle=False)\r\n        val_images_tf, val_labels_tf = read_records(val_records_dir)\r\n        val_images_tf_batch, val_labels_tf_batch = get_batch_images(val_images_tf, val_labels_tf, batch_size, num_classes, shuffle=False)\r\n\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n        print(\"start training...\")\r\n        for i in range(train_epoches):\r\n            for j in range(train_iterations):\r\n                train_images_batch, train_labels_batch = sess.run([train_images_tf_batch, train_labels_tf_batch])\r\n                train_dict = {inputs: train_images_batch, labels: train_labels_batch, keep_prob: 0.5}\r\n                classes_, labels_, loss_, acc_, _ = sess.run([classes, labels, loss, accuracy, train_step], feed_dict=train_dict)\r\n\r\n            if i % val_log_step == 0:\r\n                tmp1 = \"%s: train results: Step [%d]  Loss : %f, accuracy :  %g\" % (datetime.now(), i, loss_, acc_)\r\n                print(tmp1)\r\n\r\n                mean_loss, mean_acc = net_evaluation(sess, loss, accuracy, val_images_tf_batch, val_labels_tf_batch, val_iterations)\r\n                tmp2 = \"%s: val results: Step [%d]  Loss : %f,  accuracy :  %g\" % ( datetime.now(), i, mean_loss, mean_acc)\r\n                print(tmp2)\r\n\r\n\r\n            if i % 1 == 0 or (i + 1) == train_epoches:\r\n                savePath = './models/saved_model.ckpt'\r\n                if os.path.exists(savePath) == False:\r\n                    os.mkdir(savePath)\r\n                saver.save(sess, savePath, global_step=i)\r\n\r\n        coord.request_stop()\r\n        coord.join(threads)\r\n\r\n   ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31524\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31524\">No</a>\n"]}, {"number": 31523, "title": "error while building wheel", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:  2.0.0-rc.0\r\n- Python version: 3.7.4 x64\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source):  7.4.0\r\n- CUDA/cuDNN version: 10.1/7.6.2\r\n- GPU model and memory: GTX1080Ti GDDR5x 11GB X 6\r\n\r\n\r\n\r\n**Describe the problem**\r\nerror while building wheels\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package ~/tensorflow_pkgs/\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 308, in <module>\r\n    keywords='tensorflow tensor machine learning',\r\n  File \"/home/wmind/anaconda3/lib/python3.7/site-packages/setuptools/__init__.py\", line 145, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/home/wmind/anaconda3/lib/python3.7/distutils/core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"/home/wmind/anaconda3/lib/python3.7/distutils/dist.py\", line 966, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/home/wmind/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"/home/wmind/anaconda3/lib/python3.7/site-packages/wheel/bdist_wheel.py\", line 230, in run\r\n    impl_tag, abi_tag, plat_tag = self.get_tag()\r\n  File \"/home/wmind/anaconda3/lib/python3.7/site-packages/wheel/bdist_wheel.py\", line 179, in get_tag\r\n    assert tag == supported_tags[0], \"%s != %s\" % (tag, supported_tags[0])\r\nAssertionError: ('cp37', 'cp@pyvernodots@m', 'linux_x86_64') != ('cp37', 'cp@PYVERNODOTS@m', 'linux_x86_64')\r\n```", "comments": ["I downgraded python to 3.7.3 and then no problem.\r\n\r\nlooks like an issue with python 3.7.4", "fixed with conda python update and tensorflow sh update"]}]