[{"number": 33992, "title": "[Intel MKL] Enable TF_NUM_INTEROP_THREADS for MKL-DNN backend.", "body": "Usually we need to modify model to support inter/intra thread setting for CPU Tensorflow.  This PR enables environment to set intra/inter instead of modifying model with session config:\r\n\r\n```TF_NUM_INTEROP_THREADS=1 TF_NUM_INTRAOP_THREADS=28 python model.py```\r\n\r\nTF r1.14 has already supported TF_NUM_INTRAOP_THREADS for MKL-DNN backend, this PR is only to enable TF_NUM_INTEROP_THREADS.\r\n\r\nModified:\r\n- tensorflow/core/common_runtime/process_util.cc\r\n\r\nSigned-off-by: Lu Teng teng.lu@intel.com", "comments": ["What do you mean \"we need to modify model to support inter/intra thread setting\"?", "@fisakhan Usually #inter-threads and #intra-threads are set through session options in the TF code. This PR allows the user to also set #inter-threads and #intra-threads from the environment variables.", "@penpornk Thanks. How can I confine TensorFlow (C or C++ API) not to generate more than one threads? TF_NUM_INTEROP_THREADS=1 TF_NUM_INTRAOP_THREADS=1 generates 1 thread per core. I have 8 cores and thus tensorflow generates 8 threads. I want TensorFlow to generate only 1 thread in total.", "> @penpornk Thanks. How can I confine TensorFlow (C or C++ API) not to generate more than one threads? TF_NUM_INTEROP_THREADS=1 TF_NUM_INTRAOP_THREADS=1 generates 1 thread per core. I have 8 cores and thus tensorflow generates 8 threads. I want TensorFlow to generate only 1 thread in total.\r\n\r\nCould you start a new issue to describe your question then pin me or @penpornk ? It's hard to answer the question due to lack of information.", "Hi @fisakhan, the possible reason is that you used C/C++ API without enabling thread setting. It means that need extra configuration in code:\r\n```c\r\nRunOptions run_options;\r\nrun_options.mutable_experimental()->set_use_run_handler_pool(true);\r\nRunMetadata run_metadata;\r\nStatus status = session->Run(run_options, input, output_node_names, {}, &output, &run_metadata);\r\n```\r\n\r\nThis is because TF has some steps in Python process to initialize thread pool setting, but it will be skipped when uses C/C++ API directly: https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/core/common_runtime/direct_session.cc#L600-L601\r\nAnd TF will use the core number as thread number by default.", "Thanks @Zantares . You you can find more information about the issue here https://github.com/tensorflow/tensorflow/issues/42510", "If there is no solution for reducing the number of threads to 1 or 2, can we reduce the number of thread by changing the source code of tensorflow (particularly C_API)? what to change in that case? ", "For normal python process, the work thread pool is initialized here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/local_device.cc#L76\r\n\r\nBut I'm not sure if C API goes this path or not.  Can you try to integrate this RunOption(https://github.com/tensorflow/tensorflow/pull/33992#issuecomment-679928166) to your case firstly? I used to enable a C case with wrong thread number, but it become correct after I setting that flag. ", "TensorFlow cannot be single-threaded (https://stackoverflow.com/questions/48696900/why-tensorflow-creates-so-many-cpu-threads). Is it right?", "Not exactly. I'm not familiar with GPU, but CPU can be set to single thread mode with correct configuration: set thread number of all pools(intra, inter, OMP) to be 1. The question is, C API may use another thread pool(your own pool) or set the thread number in other place which is different with python API.\r\n\r\nThis option `run_options.mutable_experimental()->set_use_run_handler_pool(true);` forces C API to use the standard thread pool(inter & intra) in TF, so it can be controlled again.\r\n\r\nBTW, the issue you posted on stackoverflow may be caused by below reason:\r\n* wrong OMP setting\r\n* used LocalDevice API before session run, which is a known issue in TF: https://github.com/tensorflow/tensorflow/issues/22229\r\n", "run_options.mutable_experimental()->set_use_run_handler_pool(true); is not available in C API. It is available in C++ API. As soon as I create a graph or load a .pb model, thread count reaches the number of cores.\r\n\r\nThe following is my code to load a savedmodel (.pb) and it increases the number of threads from 1 to 8 (using \"top -H -b -n1 | grep example | wc -l\" command on linux).\r\ntensorflow::GraphDef graph_def;\r\nStatus load_graph_status =\r\n      ReadBinaryProto(tensorflow::Env::Default(), graph_file_name, &graph_def);\r\n  if (!load_graph_status.ok()) {\r\n    return tensorflow::errors::NotFound(\"Failed to load compute graph at '\",\r\n                                        graph_file_name, \"'\");\r\n  }"]}, {"number": 33991, "title": "Autograph error in LSTMCell when using dropout", "body": "**System information**\r\n- Have I written custom code: no\r\n- OS Platform and Distribution: Manjaro linux testing\r\n- TensorFlow installed from: pip binary\r\n- TensorFlow version: v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nTensorFlow returns an error when decorating functions calling `LSTMCell.call` with `@tf.function` if dropout is non-zero (most common scenario).\r\n```\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: lstm_cell_1/ones_like:0\r\n```\r\n\r\n**Describe the expected behavior**\r\nCompile the LSTMCell code correctly\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ngood_cell = tf.keras.layers.LSTMCell(units=2, dropout=0.0, implementation=1)\r\nbad_cell = tf.keras.layers.LSTMCell(units=2, dropout=0.1, implementation=1)\r\ninputs = tf.ones((1, 2))\r\n\r\nstate = good_cell.get_initial_state(inputs)\r\n\r\n@tf.function\r\ndef no_dropout():\r\n    output = good_cell(inputs, state)\r\n    return output\r\n\r\n@tf.function\r\ndef dropout():\r\n    output = bad_cell(inputs, state)\r\n    return output\r\n\r\nprint('='*50)\r\nprint(no_dropout())\r\nprint('='*50)\r\nprint(dropout())\r\n```\r\n\r\n**Other info / logs**\r\nI did a quick investigation and found out that setting `self._dropout_mask = None` [at the top of the call method](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/recurrent.py#L2210) makes the code work again. This forces the reset of the cached dropout mask used at the previous/first call of LSTMCell.\r\n", "comments": ["Issue replicating for both  TF 2.0 and nightly version.Kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/7c9b4739689b5a093a27dbe3bc12c834/33991.ipynb) of colab.Thanks!", "So the story is bit complicated.\r\n\r\nIf you debug the bad cell call with debugger, you will see the the bad_cell.call() is invoked twice. This means the second time when its invoked, its using the cached dropout mask created in the second trace.\r\n\r\nTo walkaround this issue, you can update your code as below. This is mimic what a normal cell will behave to process the inputs 10 times. The LSTM/RNN layer is resetting mask for every batch. User will have to do the same if they want to direct play with the cell.\r\n\r\n```\r\n@tf.function\r\ndef dropout(inputs):\r\n  time_step = 10\r\n  bad_cell.reset_dropout_mask()\r\n  bad_cell.reset_recurrent_dropout_mask()\r\n  state = bad_cell.get_initial_state(inputs)\r\n  for i in range(time_step):\r\n    inputs, state = bad_cell(inputs, state)\r\n  return inputs, state\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33991\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33991\">No</a>\n", "Hi @qlzh727 \r\nJust wanted to thank you for your great response. This is an interesting behaviour of tf.function that I was not aware of, so I learnt something new :)\r\n\r\n> This means the second time when its invoked, its using the cached dropout mask created in the second trace.\r\n\r\nWould you be able to help me with a high-level technical explanation ? I don't fully understand this yet, although it seems related to the specific implementation of tf.function.", "Ah sorry, I made a typo in the original message:\r\n\r\n> This means the second time when its invoked, its using the cached dropout mask created in the **FIRST** trace.\r\n\r\nSo, in high level, tf.function will convert a python function into a tf graph, which will be used to execution. The conversion process is called \"trace\". During tracing, tf.function will check if there is any variable gets created. If that is the case, tf.function will trace the second time and make sure no other variable is created, as a verification. When the verification failed, it will raise an error.\r\n\r\nIn your case, the function body will create following items in the first trace:\r\n1. cached masks\r\n2. variables for bad_cell when it is called with inputs. The build() function is first invoked when cell is not build.\r\n\r\nSince tf.function detect the variable creation, it will try to trace the second time. During the second trace, the code will try to use the cached mask in a different graph (created in the first trace). Since it can't access those tensors across the graph, it raises an error to you.\r\n\r\nAs a verification, you can try build your cell first outside the tf.function for your existing code, and it should make it work too.", "```python\r\nimport tensorflow as tf\r\n\r\n\r\ngood_cell = tf.keras.layers.LSTMCell(units=2, dropout=0.0, implementation=1)\r\nbad_cell = tf.keras.layers.LSTMCell(units=2, dropout=0.1, implementation=1)\r\ninputs = tf.ones((1, 2))\r\ngood_cell.build(inputs.shape)\r\nbad_cell.build(inputs.shape)\r\n\r\nstate = good_cell.get_initial_state(inputs)\r\n\r\n@tf.function\r\ndef no_dropout():\r\n  output = good_cell(inputs, state)\r\n  return output\r\n\r\n@tf.function\r\ndef dropout():\r\n  output = bad_cell(inputs, state)\r\n  return output\r\n\r\nprint('='*50)\r\nprint(no_dropout())\r\nprint('='*50)\r\nprint(dropout())\r\n```\r\n", "So, are there any fix plans?"]}, {"number": 33990, "title": "Update README.md for community ppc64le 2.x links", "body": "Copying what ROCm did, for ppc64le we are providing stable\r\nbuild links for TensorFlow 1.15 and 2.X releases.", "comments": ["The updated README.md file can be viewed here: https://github.com/wdirons/tensorflow/blob/update_readme_for_ppc64le_2_0_builds/README.md#community-supported-builds\r\n\r\nThank you.", "@chsigg, please review"]}, {"number": 33989, "title": "module 'tensorflow' has no attribute 'AdamOptimizer'", "body": "## Docs URL\r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor\r\n\r\n## Tensorflow Version (that I'm using);\r\n2.0.0\r\n\r\n## Issue\r\nI'm trying to create an \"estimator using an optimizer with a learning rate decay\". The documentation says to use `tf.AdamOptimizer`, `tf.exponential_decay`, and `tf.get_global_step`. Tensorflow has none of those attributes. I've tried playing around with suggestions from stack overflow (ie -- tf.train.AdamOptimizer), but I can't figure out where any of those attributes are actually located.\r\n\r\nexample code\r\n```\r\nmodel= tf.estimator.DNNRegressor(\r\n    feature_columns=feature_columns,\r\n    hidden_units = [100, 100, 100],\r\n    optimizer=lambda: tf.AdamOptimizer(\r\n        learning_rate=tf.exponential_decay(\r\n            learning_rate=0.1,\r\n            global_step=tf.get_global_step(),\r\n            decay_steps=10000,\r\n            decay_rate=0.96\r\n        )\r\n    )\r\n)\r\n```\r\n", "comments": ["Instead of using `tf.AdamOptimizer` or `tf.train.AdamOptimizer` use `tf.optimizers.Adam()`\r\n\r\nI hope this helps and clears the issue\r\nPing me if you don't get the issue cleared", "Thank you! that worked.\r\n\r\nFor anyone else who had a similar problem:\r\n### Adam \r\n* [tf.optimizers.Adam()](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\r\n\r\n### exponential_decay\r\n* [tf.optimizers.schedules.ExponentialDecay()](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay)\r\n\r\n### get_global_step\r\n* I don't think `get_global_step()` is in version 2 [(docs)](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/get_global_step)", "Closing this issue since its resolved. Thanks!"]}, {"number": 33988, "title": "Keras .fit() yields incorrect results when using a custom loss function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow version (use command below): 2.0.0\r\n\r\n**Describe the current behavior**\r\nWhen using a custom loss function, Keras' `.fit()` produces incorrect results.\r\n\r\n**Code to reproduce the issue**\r\nSee this [Colab Notebook](https://colab.research.google.com/drive/1Q5hVfxVKdeqtIPWrYuvsPQ-IE0dUOeWO).", "comments": ["Issue replicating for TF-2.0. ", "@netw0rkf10w Can you please provide simple standalone code to reproduce the issue. It is really faster  to resolve with simple standalone codes. Thanks!", "@jvishnuvardhan To show that the results of `.fit()` are incorrect, I had to add GradientTape for comparison. Here's [a notebook without custom training loop](https://colab.research.google.com/drive/1eNAFiOY8fDWMF4ydx7-RIoc8Fi7gRia_), but then you'll have to figure out why the results are not correct.\r\nThanks.", "FYI when using a different custom loss function, derived directly from `tf.keras.losses.Loss`, then Keras `.fit()`'s results are consistent with GradientTape's:\r\n\r\n```\r\nclass SparseCategoricalCrossentropyIgnoreLabel(tf.keras.losses.Loss):\r\n    \"\"\"Computes the crossentropy loss between the labels and predictions,\r\n        with ignored label.\r\n        Derived directly from tf.keras.losses.Loss\r\n    \"\"\"\r\n    def __init__(self, ignore_label=None, from_logits=False,\r\n                 reduction=tf.keras.losses.Reduction.AUTO,\r\n                 name='sparse_categorical_crossentropy_ignore_label'):\r\n        super().__init__(reduction=reduction, name=name)\r\n        self.ignore_label = ignore_label\r\n        self.from_logits = from_logits\r\n\r\n    def call(self, y_true, y_pred):\r\n        \"\"\"\r\n        Keep only batch dimension for the final loss\r\n        \"\"\"\r\n        y_true = tf.reshape(y_true, (tf.shape(y_true)[0], -1, tf.shape(y_true)[-1]))\r\n        y_pred = tf.reshape(y_pred, (tf.shape(y_pred)[0], -1, tf.shape(y_pred)[-1]))\r\n        loss_tensor = tf.keras.backend.sparse_categorical_crossentropy(\r\n                y_true, y_pred,\r\n                from_logits=self.from_logits)\r\n        mask = tf.ones_like(loss_tensor)\r\n        if self.ignore_label is not None:\r\n            mask = (y_true != self.ignore_label)\r\n            mask = tf.squeeze(mask, [-1])\r\n            loss_tensor = tf.where(mask, loss_tensor, 0)\r\n        \r\n        num_elements = tf.reduce_sum(tf.cast(mask, tf.float32), axis=1)\r\n        loss_tensor = tf.reduce_sum(loss_tensor, axis=1)/num_elements\r\n        loss_tensor = tf.where(num_elements > 0, loss_tensor, 0)\r\n        return loss_tensor\r\n```\r\n\r\nThe loss function defined in the above Colab notebook (i.e. the one that caused the issue) is derived from `tf.keras.losses.SparseCategoricalCrossentropy`:\r\n\r\n```\r\nclass SparseCategoricalCrossentropyIgnoreLabel(tf.keras.losses.SparseCategoricalCrossentropy):\r\n    \"\"\"Computes the crossentropy loss between the labels and predictions,\r\n        with ignored label.\r\n        Derived from tf.keras.losses.SparseCategoricalCrossentropy\r\n    \"\"\"\r\n    def __init__(self, ignore_label=None, from_logits=False,\r\n                 reduction=tf.keras.losses.Reduction.AUTO,\r\n                 name='sparse_categorical_crossentropy_ignore_label'):\r\n        super().__init__(from_logits=from_logits, reduction=reduction, name=name)\r\n        self.ignore_label = ignore_label\r\n        \r\n    def __call__(self, y_true, y_pred, sample_weight=None):\r\n        if self.ignore_label is not None:\r\n            if sample_weight is not None:\r\n                sample_weight = tf.where(y_true == self.ignore_label, 0, sample_weight)\r\n            else:\r\n                sample_weight = y_true != self.ignore_label\r\n        return super().__call__(y_true, y_pred, sample_weight)\r\n```", "@netw0rkf10w With `tf-nightly` I am seeing loss values are similar. Please take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/0fc2c5a7aeaff8e0ee0bf78e1934584c/tf_custom_loss.ipynb). Please let us know what you think.\r\n\r\nHere is the loss from `model.fit`\r\n\r\n```\r\nEpoch 1/5\r\n19/19 [==============================] - 16s 854ms/step - loss: 0.9181 - sparse_categorical_accuracy: 0.5864 - val_loss: 0.8268 - val_sparse_categorical_accuracy: 0.6771\r\nEpoch 2/5\r\n19/19 [==============================] - 15s 794ms/step - loss: 0.5648 - sparse_categorical_accuracy: 0.7903 - val_loss: 0.6254 - val_sparse_categorical_accuracy: 0.7927\r\nEpoch 3/5\r\n19/19 [==============================] - 15s 778ms/step - loss: 0.4492 - sparse_categorical_accuracy: 0.8387 - val_loss: 0.5172 - val_sparse_categorical_accuracy: 0.8288\r\nEpoch 4/5\r\n19/19 [==============================] - 15s 774ms/step - loss: 0.3835 - sparse_categorical_accuracy: 0.8566 - val_loss: 0.4660 - val_sparse_categorical_accuracy: 0.8445\r\nEpoch 5/5\r\n19/19 [==============================] - 15s 775ms/step - loss: 0.3476 - sparse_categorical_accuracy: 0.8679 - val_loss: 0.4404 - val_sparse_categorical_accuracy: 0.8488\r\n```\r\n\r\nHere is the loss from custom training\r\n\r\n```\r\nEpoch 1/5\r\n19/19 [==============================] - 18s 970ms/step - loss: 0.4031 - sparse_categorical_accuracy: 0.3776 - val_loss: 0.2922 - val_sparse_categorical_accuracy: 0.3049\r\nEpoch 2/5\r\n19/19 [==============================] - 15s 784ms/step - loss: 0.2247 - sparse_categorical_accuracy: 0.3220 - val_loss: 0.2261 - val_sparse_categorical_accuracy: 0.3394\r\nEpoch 3/5\r\n19/19 [==============================] - 15s 777ms/step - loss: 0.1888 - sparse_categorical_accuracy: 0.3448 - val_loss: 0.1981 - val_sparse_categorical_accuracy: 0.3546\r\nEpoch 4/5\r\n19/19 [==============================] - 15s 778ms/step - loss: 0.1750 - sparse_categorical_accuracy: 0.3530 - val_loss: 0.1959 - val_sparse_categorical_accuracy: 0.3566\r\nEpoch 5/5\r\n19/19 [==============================] - 15s 778ms/step - loss: 0.1657 - sparse_categorical_accuracy: 0.3522 - val_loss: 0.1928 - val_sparse_categorical_accuracy: 0.3584\r\n```", "Sorry but the loss values you showed are **clearly** not similar. Just compare the two sections **Using built-in loss function** and **Using custom loss function** in the [gist](https://colab.research.google.com/drive/1Q5hVfxVKdeqtIPWrYuvsPQ-IE0dUOeWO) to see the difference.", "Was able to reproduce the issue in TF v2.5 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/9c0e17d896996cdc76614dfb72465fd0/untitled26.ipynb)..Thanks !", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33988\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33988\">No</a>\n"]}, {"number": 33987, "title": "Update `nogpu` tag references to be `no_gpu` instead?", "body": "Currently the scripts used to run TF unit tests on the ROCm platform filter out tests that are tagged with the `no_gpu` tag\r\n\r\n* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/rocm/run_cc_core.sh\r\n* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/rocm/run_csb_tests.sh\r\n* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/rocm/run_py3_core.sh\r\n* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/xla/linux/rocm/run_py3.sh\r\n\r\nThe same applies for scripts for the CUDA platform as well.\r\n\r\nThere seem to be some tests that are tagged with the `nogpu` tag instead of the `no_gpu` tag.\r\nOne example is \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tpu/tpu.bzl#L44\r\n\r\nSuch tests do not get filtered out by the test scripts above, and as a consequence they get reported as regression failures when those test scripts are run.\r\n\r\nSo the question here is \r\n* should we update all references to the `nogpu` tag to instead be `no_gpu`\r\nOR\r\n* should we update all the scripts instead to also filter out the `nogpu` tagged tests?\r\n\r\n-----------------------\r\n\r\n@chsigg \r\n", "comments": ["/cc @parallelo ", "a related question for the `master` branch (which is now TF2.0).\r\n\r\nShould the test scripts also filter out `v1only` tagged tests?\r\n\r\n", "gentle ping....any update here?", "Sorry for the delay. I think we should change `nogpu` tags to `no_gpu`. I can take care of this.", "@chsigg thanks.\r\n\r\nhow about the `v1only` tagged tests....should they also be filtered out by default on the `master` + all `2.x` branch(es) ? ", "Ping @chsigg has this been done? Removing myself from assignees. Thanks!", "@deven-amd,\r\n\r\nAs per this [commit](https://github.com/tensorflow/tensorflow/commit/3a91a0251bdbda59e8079958ff9261fcb752f52f), `nogpu` tag references have been modified to `no-gpu` by @chsigg and so I am closing this issue. Feel free to open a new issue if you face any further problems. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33987\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33987\">No</a>\n"]}, {"number": 33986, "title": "Errors when instantiating multiple tf.keras models on different threads concurrently", "body": "**System information**\r\nI am running custom code on a MacBook Pro running Mac OS Mojave 10.14.6:\r\n```\r\n Model Name:\tMacBook Pro\r\n  Model Identifier:\tMacBookPro15,1\r\n  Processor Name:\tIntel Core i9\r\n  Processor Speed:\t2.3 GHz\r\n  Number of Processors:\t1\r\n  Total Number of Cores:\t8\r\n  L2 Cache (per Core):\t256 KB\r\n  L3 Cache:\t16 MB\r\n  Hyper-Threading Technology:\tEnabled\r\n  Memory:\t16 GB\r\n```\r\n\r\nI am running Python 3.7.3 installed via homebrew and Tensorflow 2.0.0 (v2.0.0-rc2-26-g64c3d382ca 2.0.0) installed via pip.\r\n\r\n**Describe the current behavior**\r\n\r\nWhen instantiating multiple `tf.keras.Sequential` models concurrently on different threads, I sometimes get errors in certain calls within `site-packages/tensorflow_core/python/framework/ops.py`. Tracebacks are included below.\r\n\r\n**Describe the expected behavior**\r\n\r\nExpected behavior is that multiple tf.keras models can be instantiated on multiple threads without conflict.\r\n\r\n**Code to reproduce the issue + Other info / logs**\r\n\r\nThe error that I am getting in my development code is this:\r\n\r\n```\r\nTraceback (most recent call last):\r\n...\r\n  File \"/Users/josephcappadona/Documents/demo/demo/models/models.py\", line 24, in __init__\r\n    super().__init__([logit_layer, activation_layer], name)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 114, in __init__\r\n    self.add(layer)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 174, in add\r\n    batch_shape=batch_shape, dtype=dtype, name=layer.name + '_input')\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_layer.py\", line 263, in Input\r\n    input_tensor=tensor)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_layer.py\", line 125, in __init__\r\n    ragged=ragged)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\", line 1057, in placeholder\r\n    x = array_ops.placeholder(dtype, shape=shape, name=name)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 2630, in placeholder\r\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 6671, in placeholder\r\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 793, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 548, in create_op\r\n    compute_device)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3430, in _create_op_internal\r\n    self._create_op_helper(ret, compute_device=compute_device)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3469, in _create_op_helper\r\n    for key, value in self._attr_scope_map.items():\r\nRuntimeError: dictionary changed size during iteration\r\n```\r\nhowever I cannot reproduce this error reliably. It seems to happen a very small percentage of the time, making debugging it very difficult. Essentially, this happens when running a Flask app in which multiple models are instantiated concurrently across different threads at the same time.\r\n\r\nIn trying to write some code that reproduces something similar so that I could include something in this issue, I was able to use this code\r\n```python\r\nimport tensorflow as tf\r\nfrom threading import Thread\r\n\r\ndef build_model():\r\n    layer_1 = tf.keras.layers.Dense(1, input_shape=(20,))\r\n    layer_2 = tf.keras.layers.Activation(tf.sigmoid)\r\n    tf.keras.Sequential([layer_1, layer_2])\r\n\r\nfor i in range(100):\r\n    t = Thread(target=build_model)\r\n    t.start()\r\n```\r\n\r\nin a jupyter notebook to generate this error\r\n\r\n```\r\nException in thread Thread-369:\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\r\n    self.run()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\", line 865, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"<ipython-input-8-c7ffb35bc6af>\", line 4, in build_model\r\n    tf.keras.Sequential([layer_1, layer_2])\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 114, in __init__\r\n    self.add(layer)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 196, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 845, in __call__\r\n    outputs = base_layer_utils.mark_as_return(outputs, acd)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/framework/auto_control_deps.py\", line 300, in __exit__\r\n    if op.type == \"Switch\" and op.inputs[0].dtype == dtypes_module.resource:\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 2259, in type\r\n    return c_api.TF_OperationOpType(self._c_op)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xf2 in position 0: invalid continuation byte\r\n```\r\nwhich, while obviously is a different error, also happens in `tensorflow_core/python/framework/ops.py` and also involves the use of an instance variable (in this case `self._c_op`).\r\n\r\nPlease let me know if there's any other information I need to provide to help debug. I assume this has something to do with shared global state across a tensorflow session, but I'm not sure if there is something that I am supposed to do to prevent these types of errors.", "comments": ["Same error here. Launching child processes seems to work, though.", "> Launching child processes seems to work, though.\r\n\r\nThanks. I've written the following decorator to run a function as a child process. It doesn't work on functions that return `tf.keras` models, since they are not pickleable which is a requirement for the `multiprocessing.Queue` object, but it works as a temporary fix that I can use to decorate the function that instantiates and uses the model. It fixes the example code that I provided above, however I'm not positive that it fixed the original error that I got since I can't reproduce it reliably.\r\n\r\n```python\r\nimport multiprocessing as mp\r\n\r\ndef run_as_child_process(f):\r\n    \"\"\"Decorator which makes the input function run as a child process to the\r\n    thread on which it was called. Requires that the function's return value is\r\n    pickleable (this is a requirement in order to put an object in a\r\n    `multiprocessing.Queue` object) or else it will fail silently.\r\n\r\n    Args:\r\n        f: A function whose return value is pickleable.\r\n\r\n    Returns:\r\n        A function.\r\n    \"\"\"\r\n\r\n    def subprocess_f(q, *args, **kwargs):\r\n        \"\"\"Takes in a multiprocessing.Queue object and the positional and\r\n        keyword arguments for `f`. Runs `f` and places the return value in the\r\n        queue for retrieval by the parent process.\r\n\r\n        Args:\r\n            q: A multiprocessing.Queue object.\r\n        \"\"\"\r\n        ret = f(*args, **kwargs)\r\n        q.put(ret)\r\n\r\n    def new_f(*args, **kwargs):\r\n        \"\"\"Spawns a new process in which to run `f`. `q.get()` blocks until it\r\n        receives data from the child process and then returns that data.\"\"\"\r\n        q = mp.Queue()\r\n        new_args = tuple([q] + list(args))\r\n        p = mp.Process(target=subprocess_f, args=new_args, kwargs=kwargs)\r\n        p.start()\r\n        ret = q.get()\r\n        p.join()\r\n        return ret\r\n\r\n    return new_f\r\n```\r\n\r\n\r\nEdit: Sometimes the example code throws this error\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\r\n    self.run()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\", line 865, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"<ipython-input-3-6d3398ac7046>\", line 5, in build_model\r\n    model = tf.keras.Sequential([layer_1, layer_2], str(i) + '__' + str(uuid4()))\r\n...\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 548, in create_op\r\n    compute_device)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3430, in _create_op_internal\r\n    self._create_op_helper(ret, compute_device=compute_device)\r\n  File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3469, in _create_op_helper\r\n    for key, value in self._attr_scope_map.items():\r\nRuntimeError: dictionary changed size during iteration\r\n```\r\nwhich is the precise error I get in my development code, however using the decorator makes this error disappear, so I am more confident now that the decorator has fixed the issue. It would still be nice to have an official fix though.", "If someone could chime in with a more official fix, that would be appreciated.", "Can you please try using [distribution strategy](https://www.tensorflow.org/api_docs/python/tf/distribute?hl=en\r\n) and see how it progresses. Also it will be great help if you share minimal stand alone code , it helps us in localizing the issue faster.Thanks!", "The only solution I got to work was using the TF 1.0 api for creating each model in a separate graph. However, I think this is a little bit dirty, but I don't get it to work with the TF 2.0 API. @josephcappadona, in your first example, would be something like:\r\n\r\n```python3\r\ndef build_model():\r\n            with tf.compat.v1.Session() as sess:\r\n                gra = tf.Graph()\r\n                with gra.as_default():\r\n                        layer_1 = tf.keras.layers.Dense(1, input_shape=(20,))\r\n                        layer_2 = tf.keras.layers.Activation(tf.sigmoid)\r\n                        tf.keras.Sequential([layer_1, layer_2])\r\n```\r\n\r\nHow would it be the best approximation for instantiating several models, like agents for a game (only using them for inference)?.", "> Can you please try using [distribution strategy](https://www.tensorflow.org/api_docs/python/tf/distribute?hl=en) and see how it progresses. Also it will be great help if you share minimal stand alone code , it helps us in localizing the issue faster.Thanks!\r\n\r\nI was not able to get it working with the `tf.distribute.Strategy` API. It is not the training that is the issue, it is simply the instantiation of the models that throws the errors.\r\n\r\nThe example that I included above is the best example I can get, it throws an error fairly consistently. The bug is stochastic in nature, so I'm having trouble reproducing it consistently. The best I was able to do is the following:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom threading import Thread\r\nimport multiprocessing as mp\r\nfrom uuid import uuid4\r\nimport time\r\nimport numpy as np\r\n\r\ndef get_sleep_time():\r\n    return abs(np.random.normal(loc=0.1, scale=0.5))\r\n\r\ndef build_model(i, z):\r\n    time.sleep(get_sleep_time())\r\n    layer_1 = tf.keras.layers.Dense(1, input_shape=(20,))\r\n    layer_2 = tf.keras.layers.Activation(tf.sigmoid)\r\n    model = tf.keras.Sequential([layer_1, layer_2], str(i) + str(uuid4()))\r\n    z.put(model.name)\r\n    return model.name\r\n\r\nz = mp.Queue()\r\nfor i in range(100):\r\n    t = Thread(target=build_model, args=(i, z))\r\n    t.start()\r\n```\r\n\r\nThe goal of the sleeping is to make it more likely that two model instantiations will conflict. This code throws a `UnicodeDecodeError` for me about half of the time.", "> How would it be the best approximation for instantiating several models, like agents for a game (only using them for inference)?.\r\n\r\nEssentially, a model is being trained by the user on a single thread in a web app, and then it needs to be loaded and used concurrently and asynchronously across different threads of the web server. So yes, the loaded models are only being used for inference.", "> I don't get it to work with the TF 2.0 API\r\n\r\nYes, unfortunately I am unable to get your example working with TF 2.0. I appreciate your help.\r\n\r\n> How would it be the best approximation for instantiating several models, like agents for a game (only using them for inference)?.\r\n\r\nEssentially, a model needs to be loaded and used concurrently and asynchronously across different threads of a web server. So yes, the loaded models are only being used for inference.", "Hope someone can shed some light on this... I solved my issue with the approximation I posted, but I would like to know if there is a cleaner way.", "> Hope someone can shed some light on this... I solved my issue with the approximation I posted, but I would like to know if there is a cleaner way.\r\n\r\nAfter more testing, it seems this has solved my issue, but yes, a canonical way to solve this with TF 2.0 would be ideal.", "> > Launching child processes seems to work, though.\r\n> \r\n> Thanks. I've written the following decorator to run a function as a child process. It doesn't work on functions that return `tf.keras` models, since they are not pickleable which is a requirement for the `multiprocessing.Queue` object, but it works as a temporary fix that I can use to decorate the function that instantiates and uses the model. It fixes the example code that I provided above, however I'm not positive that it fixed the original error that I got since I can't reproduce it reliably.\r\n> \r\n> ```python\r\n> import multiprocessing as mp\r\n> \r\n> def run_as_child_process(f):\r\n>     \"\"\"Decorator which makes the input function run as a child process to the\r\n>     thread on which it was called. Requires that the function's return value is\r\n>     pickleable (this is a requirement in order to put an object in a\r\n>     `multiprocessing.Queue` object) or else it will fail silently.\r\n> \r\n>     Args:\r\n>         f: A function whose return value is pickleable.\r\n> \r\n>     Returns:\r\n>         A function.\r\n>     \"\"\"\r\n> \r\n>     def subprocess_f(q, *args, **kwargs):\r\n>         \"\"\"Takes in a multiprocessing.Queue object and the positional and\r\n>         keyword arguments for `f`. Runs `f` and places the return value in the\r\n>         queue for retrieval by the parent process.\r\n> \r\n>         Args:\r\n>             q: A multiprocessing.Queue object.\r\n>         \"\"\"\r\n>         ret = f(*args, **kwargs)\r\n>         q.put(ret)\r\n> \r\n>     def new_f(*args, **kwargs):\r\n>         \"\"\"Spawns a new process in which to run `f`. `q.get()` blocks until it\r\n>         receives data from the child process and then returns that data.\"\"\"\r\n>         q = mp.Queue()\r\n>         new_args = tuple([q] + list(args))\r\n>         p = mp.Process(target=subprocess_f, args=new_args, kwargs=kwargs)\r\n>         p.start()\r\n>         ret = q.get()\r\n>         p.join()\r\n>         return ret\r\n> \r\n>     return new_f\r\n> ```\r\n> \r\n> Edit: Sometimes the example code throws this error\r\n> \r\n> ```\r\n> Traceback (most recent call last):\r\n>   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\r\n>     self.run()\r\n>   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\", line 865, in run\r\n>     self._target(*self._args, **self._kwargs)\r\n>   File \"<ipython-input-3-6d3398ac7046>\", line 5, in build_model\r\n>     model = tf.keras.Sequential([layer_1, layer_2], str(i) + '__' + str(uuid4()))\r\n> ...\r\n>   File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 548, in create_op\r\n>     compute_device)\r\n>   File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3430, in _create_op_internal\r\n>     self._create_op_helper(ret, compute_device=compute_device)\r\n>   File \"/Users/josephcappadona/.local/share/virtualenvs/demo-qfKwvgY4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3469, in _create_op_helper\r\n>     for key, value in self._attr_scope_map.items():\r\n> RuntimeError: dictionary changed size during iteration\r\n> ```\r\n> \r\n> which is the precise error I get in my development code, however using the decorator makes this error disappear, so I am more confident now that the decorator has fixed the issue. It would still be nice to have an official fix though.\r\n\r\nNot sure if this applies, but you could maybe do `w = model.get_weights()`, send the list of numpy arrays to the Queue, and then `model.set_weights(w)` on the other side. It's a bit ugly, but should allow you to effectively send a model in a Queue.", "The problem for us is in the model/graph declaration part. It seems that all the 100 threads create 100 models on the same graph, so some node names conflict. My idea was that each thread had it's separate session so this don't occur. However, I still have the question for someone on the TF community: What is the best way (with TF 2.0) to make a model on each thread (not having the name collisions) using only them for inference?\r\n\r\n\r\n**EDIT:** Solved. I think what you, @josephcappadona, are looking for is this: https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/Graph#as_graph_def. Basically it'ts the above solution but without the tf.Session part. This way, you are creating a separate graph for each thread using only the TF 2.0 API. \r\n\r\n```python3\r\ndef build_model():\r\n                gra = tf.Graph()  # if this is in a class, you should save it with self.gra\r\n                with gra.as_default():\r\n                        layer_1 = tf.keras.layers.Dense(1, input_shape=(20,))\r\n                        layer_2 = tf.keras.layers.Activation(tf.sigmoid)\r\n                        tf.keras.Sequential([layer_1, layer_2])\r\n```", "@josephcappadona,\r\nCan you please let us know if [this comment](https://github.com/tensorflow/tensorflow/issues/33986#issuecomment-550186975) has resolved your issue? \r\n\r\nAlso, your code could be run without any error with **`Tensorflow Version 2.5`** (executed more than 10 times, even in **`Jupyter Notebook`**). Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/7184cde9009d10668ed5e0d3e932c11e/gh_33986.ipynb) of the Working Code.\r\n\r\nThanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 33985, "title": "Unable to get_layer by name on custom layer and lambda layer", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos mohave\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0\r\n- TensorFlow version (use command below): pip install tensorflow==2.0.0\r\n- Python version: python3.7\r\n- Bazel version (if compiling from source):  -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n\r\nget_layer raises exception\r\n\r\n**Describe the expected behavior**\r\n\r\nShould pass and not raise exception.\r\n\r\n    ValueError: No such layer: embeddings\r\n\r\n**Code to reproduce the issue**\r\n\r\nimport tensorflow as tf\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass L2NormalizeLayer(tf.keras.layers.Layer):\r\n    def __init__(self, name=\"normalize\", **kwargs):\r\n        super(L2NormalizeLayer, self).__init__(name=name, **kwargs)\r\n\r\n    def call(self, input):\r\n        return tf.keras.backend.l2_normalize(input, axis=1)\r\n\r\n    def get_config(self):\r\n        config = super(L2NormalizeLayer, self).get_config()\r\n        return config\r\n\r\nshape = (224, 224, 3)\r\n\r\n# functional model\r\nbase_model2 = tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=shape)\r\ninputs = tf.keras.Input(shape=shape, name=\"input\")\r\nx = base_model2(inputs)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\r\nx = tf.keras.layers.Dense(256, activation=\"relu\")(x)\r\ny = L2NormalizeLayer(name=\"embeddings\")(x)\r\n#y = tf.keras.layers.Lambda(lambda k: tf.keras.backend.l2_normalize(k, axis=1), name=\"embeddings\")(x)\r\noutputs = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"probs\")(x)\r\nmodel2 = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n# after training model i would like to load it and extract probs with embeddings\r\ntf.keras.models.save_model(model2, \"model.h5\")\r\nmodel_l2 = tf.keras.models.load_model(\"model.h5\")\r\n\r\nmodel_loaded = tf.keras.Model(\r\n    inputs=model_l2.input, outputs=[model_l2.get_layer(layer_name).output for layer_name in [\"probs\", \"embeddings\"]]\r\n)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\n19:29:07.817639: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-04 19:29:07.832947: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa8ada42530 executing computations on platform Host. Devices:\r\n2019-11-04 19:29:07.832973: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\nTraceback (most recent call last):\r\n  File \"save2.py\", line 36, in <module>\r\n    inputs=model_l2.input, outputs=[model_l2.get_layer(layer_name).output for layer_name in [\"probs\", \"embeddings\"]]\r\n  File \"save2.py\", line 36, in <listcomp>\r\n    inputs=model_l2.input, outputs=[model_l2.get_layer(layer_name).output for layer_name in [\"probs\", \"embeddings\"]]\r\n  File \"/Users/michallukac/env/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 539, in get_layer\r\n    raise ValueError('No such layer: ' + name)\r\nValueError: No such layer: embeddings\r\n```", "comments": ["I have tried on colab with TF version 2.0, 2.1.0-dev20191103 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/604379b8a672a26e23f2cf0f05343ef5/untitled329.ipynb). Thanks!", "Ok I think the problem is that y = L2NormalizeLayer(...) is not specified in the outputs of tf.keras.Model. Once I changed code to:\r\n\r\nmodel2 = tf.keras.Model(inputs=inputs, outputs=[outputs, y])\r\n\r\neverything pass. I don't know if this is intentional behavior, if so then you can close it now.", "@Cospel This is intended as the argument `x` is passed to both `Dense` and `L2NormalizeLayer`. So there are two outputs in this functional model. \r\n\r\nI am closing this issue as it is resolved. Feel free to open a new issue if you encounter any other bugs/performance issues. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33985\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33985\">No</a>\n"]}, {"number": 33984, "title": "Failed to convert object of type <class 'tensorflow.python.ops.init_ops_v2.RandomNormal'> to Tensor", "body": "This problem relates to adapting existing code from previous TensorFlow versions to use Bayesian probability estimation in Dense layers. \r\n\r\n[Original code is here](http://krasserm.github.io/2019/03/14/bayesian-neural-networks/)\r\n\r\nThe function for tensorflow 1.12 + keras 2.2.2 is as follows:\r\n\r\n```\r\nfrom keras import backend as K\r\nfrom keras import activations, initializers\r\nfrom keras.layers import Layer\r\nimport tensorflow as tf\r\n\r\ndef mixture_prior_params(sigma_1, sigma_2, pi, return_sigma=False):\r\n    params = K.variable([sigma_1, sigma_2, pi], name='mixture_prior_params')\r\n    sigma = np.sqrt(pi * sigma_1 ** 2 + (1 - pi) * sigma_2 ** 2)\r\n    return params, sigma\r\n\r\ndef log_mixture_prior_prob(w):\r\n    comp_1_dist = tf.distributions.Normal(0.0, prior_params[0])\r\n    comp_2_dist = tf.distributions.Normal(0.0, prior_params[1])\r\n    comp_1_weight = prior_params[2]    \r\n    return K.log(comp_1_weight * comp_1_dist.prob(w) + (1 - comp_1_weight) * comp_2_dist.prob(w))    \r\n\r\n# Mixture prior parameters shared across DenseVariational layer instances\r\nprior_params, prior_sigma = mixture_prior_params(sigma_1=1.0, sigma_2=0.1, pi=0.2)\r\n\r\nclass DenseVariational(Layer):\r\n    def __init__(self, output_dim, kl_loss_weight, activation=None, **kwargs):\r\n        self.output_dim = output_dim\r\n        self.kl_loss_weight = kl_loss_weight\r\n        self.activation = activations.get(activation)\r\n        super().__init__(**kwargs)\r\n\r\n    def build(self, input_shape):  \r\n        self._trainable_weights.append(prior_params) \r\n\r\n        self.kernel_mu = self.add_weight(name='kernel_mu', \r\n                                         shape=(input_shape[1], self.output_dim),\r\n                                         initializer=initializers.normal(stddev=prior_sigma),\r\n                                         trainable=True)\r\n        self.bias_mu = self.add_weight(name='bias_mu', \r\n                                       shape=(self.output_dim,),\r\n                                       initializer=initializers.normal(stddev=prior_sigma),\r\n                                       trainable=True)\r\n        self.kernel_rho = self.add_weight(name='kernel_rho', \r\n                                          shape=(input_shape[1], self.output_dim),\r\n                                          initializer=initializers.constant(0.0),\r\n                                          trainable=True)\r\n        self.bias_rho = self.add_weight(name='bias_rho', \r\n                                        shape=(self.output_dim,),\r\n                                        initializer=initializers.constant(0.0),\r\n                                        trainable=True)\r\n        super().build(input_shape)\r\n\r\n    def call(self, x):\r\n        kernel_sigma = tf.math.softplus(self.kernel_rho)\r\n        kernel = self.kernel_mu + kernel_sigma * tf.random.normal(self.kernel_mu.shape)\r\n\r\n        bias_sigma = tf.math.softplus(self.bias_rho)\r\n        bias = self.bias_mu + bias_sigma * tf.random.normal(self.bias_mu.shape)\r\n                \r\n        self.add_loss(self.kl_loss(kernel, self.kernel_mu, kernel_sigma) + \r\n                      self.kl_loss(bias, self.bias_mu, bias_sigma))\r\n        \r\n        return self.activation(K.dot(x, kernel) + bias)\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return (input_shape[0], self.output_dim)\r\n    \r\n    def kl_loss(self, w, mu, sigma):\r\n        variational_dist = tf.distributions.Normal(mu, sigma)\r\n        return self.kl_loss_weight * K.sum(variational_dist.log_prob(w) - log_mixture_prior_prob(w))\r\n```\r\n\r\nTo get this working for Tensorflow 2.0, I've got the following:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import backend as K\r\nfrom tensorflow.keras import activations, initializers\r\nfrom tensorflow.keras.layers import Layer\r\nimport tensorflow_probability as tfp\r\n\r\ndef mixture_prior_params(sigma_1, sigma_2, pi, return_sigma=False):\r\n    params = K.variable([sigma_1, sigma_2, pi], name='mixture_prior_params')\r\n    sigma = np.sqrt(pi * sigma_1 ** 2 + (1 - pi) * sigma_2 ** 2)\r\n    return params, sigma\r\n\r\ndef log_mixture_prior_prob(w):\r\n    comp_1_dist = tfp.distributions.Normal(0.0, prior_params[0])\r\n    comp_2_dist = tfp.distributions.Normal(0.0, prior_params[1])\r\n    comp_1_weight = prior_params[2]\r\n    return K.log(comp_1_weight * comp_1_dist.prob(w) + (1 - comp_1_weight) * comp_2_dist.prob(w))\r\n\r\n# Mixture prior parameters shared across DenseVariational layer instances\r\nprior_params, prior_sigma = mixture_prior_params(sigma_1=1.0, sigma_2=0.1, pi=0.2)\r\n\r\nclass DenseVariational(Layer):\r\n    def __init__(self, output_dim, kl_loss_weight, activation=None, **kwargs):\r\n        self.output_dim = output_dim\r\n        self.kl_loss_weight = kl_loss_weight\r\n        self.activation = activations.get(activation)\r\n        super().__init__(**kwargs)\r\n        \r\n    def build(self, input_shape):\r\n        self._trainable_weights.append(prior_params)\r\n        self.kernel_mu = self.add_weight(name='kernel_mu', shape=(input_shape[1], self.output_dim), initializer=tf.random_normal_initializer(stddev=prior_sigma), trainable=True)\r\n        self.bias_mu = self.add_weight(name='bias_mu', shape=(self.output_dim,), initializer=tf.random_normal_initializer(stddev=prior_sigma), trainable=True)\r\n        self.kernel_rho = self.add_weight(name='kernel_rho', shape=(input_shape[1], self.output_dim), initializer=tf.constant_initializer(0.0), trainable=True)\r\n        self.bias_rho = self.add_weight(name='bias_rho', shape=(self.output_dim,), initializer=tf.constant_initializer(0.0), trainable=True)\r\n        super().build(input_shape)\r\n        \r\n    def call(self, x):\r\n        kernel_sigma = tf.math.softplus(self.kernel_rho)\r\n        kernel = self.kernel_mu + kernel_sigma * tf.random_normal_initializer(self.kernel_mu.shape)\r\n        bias_sigma = tf.math.softplus(self.bias_rho)\r\n        bias = self.bias_mu + bias_sigma * tf.random_normal_initializer(self.bias_mu.shape)\r\n        self.add_loss(self.kl_loss(kernel, self.kernel_mu, kernel_sigma) + self.kl_loss(bias, self.bias_mu, bias_sigma))\r\n        return self.activation(K.dot(x, kernel) + bias)\r\n        \r\n    def compute_output_shape(self, input_shape):\r\n        return (input_shape[0], self.output_dim)\r\n        \r\n    def kl_loss(self, w, mu, sigma):\r\n        variational_dist = tfp.distributions.Normal(mu, sigma)\r\n        return self.kl_loss_weight * K.sum(variational_dist.log_prob(w) - log_mixture_prior_prob(w))\r\n       \r\n```\r\n\r\nHowever, when I try to add this layer with the following code in a sequential model:\r\n\r\n`model.add(DenseVariational(128, kl_loss_weight=kl_loss_weight, activation='relu'))\r\n`\r\nI get the following error:\r\n\r\n> WARNING:tensorflow:Entity <bound method DenseVariational.call of <__main__.DenseVariational object at 0x7f0cf1837b10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DenseVariational.call of <__main__.DenseVariational object at 0x7f0cf1837b10>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\n> WARNING: Entity <bound method DenseVariational.call of <__main__.DenseVariational object at 0x7f0cf1837b10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DenseVariational.call of <__main__.DenseVariational object at 0x7f0cf1837b10>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/home/bly/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n>     result = method(self, *args, **kwargs)\r\n>   File \"/home/bly/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 196, in add\r\n>     output_tensor = layer(self.outputs[0])\r\n>   File \"/home/bly/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 842, in __call__\r\n>     outputs = call_fn(cast_inputs, *args, **kwargs)\r\n>   File \"/home/bly/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 237, in wrapper\r\n>     raise e.ag_error_metadata.to_exception(e)\r\n> TypeError: in converted code:\r\n> \r\n>     <stdin>:18 call\r\n>         \r\n>     /home/bly/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py:903 binary_op_wrapper\r\n>         y, dtype_hint=x.dtype.base_dtype, name=\"y\")\r\n>     /home/bly/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1242 convert_to_tensor_v2\r\n>         as_ref=False)\r\n>     /home/bly/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1296 internal_convert_to_tensor\r\n>         ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n>     /home/bly/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py:286 _constant_tensor_conversion_function\r\n>         return constant(v, dtype=dtype, name=name)\r\n>     /home/bly/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py:227 constant\r\n>         allow_broadcast=True)\r\n>     /home/bly/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py:265 _constant_impl\r\n>         allow_broadcast=allow_broadcast))\r\n>     /home/bly/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:545 make_tensor_proto\r\n>         \"supported type.\" % (type(values), values))\r\n> \r\n>     TypeError: Failed to convert object of type <class 'tensorflow.python.ops.init_ops_v2.RandomNormal'> to Tensor. Contents: <tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f0f8c54cd90>. Consider casting elements to a supported type.\r\n\r\nThe problem lies in the use of tf.random_normal_initializer in place of the earlier initializers.normal. I am not sure how to resolve this - I can't see a direct replacement for this in the Tensorflow 2.0 documentation. ", "comments": ["@leedrake5 ,\r\nI tried running the given code snippets in colab for TF-1.15 and didnot face any error. Can you pelase upgrade to latest tf version-1.15 ? find the [Gist](https://colab.sandbox.google.com/gist/oanush/103ebebfa3713a636273edb102abd236/untitled21.ipynb) of colab.Thanks", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@leedrake5 ,\r\nAny update on the issue?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Apologies for the delay - haven\u2019t been able to get back to this, but will report when I do. Thanks. "]}, {"number": 33983, "title": "Build error in Macbook (tensorflow lite)", "body": "- OS Platform and Distribution Macbook\r\n- TensorFlow installed from (source or binary): github latest bf282dece59bdf88f7a58bcf1064723cb3eea51e \r\n- TensorFlow version:trunk\r\n- Bazel version (if compiling from source):  1.0.0-homebrew\r\n- GCC/Compiler version (if compiling from source): Apple clang version 11.0.0 (clang-1100.0.33.8)\r\n\r\nRun ./tensorflow/lite/tools/make/build_lib.sh or ./tensorflow/lite/tools/make/build_ios_universal_lib.sh\r\nSame build problem:\r\n\r\ng++ -O3 -DNDEBUG -fPIC  --std=c++11 -I. -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/ -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/experimental/ruy/block_map.cc -o /Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/gen/osx_x86_64/obj/tensorflow/lite/experimental/ruy/block_map.o\r\nIn file included from tensorflow/lite/core/api/op_resolver.cc:16:\r\nIn file included from ./tensorflow/lite/core/api/op_resolver.h:20:\r\n**./tensorflow/lite/schema/schema_generated.h:2660:8: error: ISO C++ forbids forward references to 'enum' types**\r\nIn file included from tensorflow/lite/core/api/flatbuffer_conversions.cc:  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {\r\n16:\r\n       ^In file included from \r\n./tensorflow/lite/core/api/flatbuffer_conversions.h:24:\r\nIn file included from ./tensorflow/lite/core/api/op_resolver.h:20:\r\n**./tensorflow/lite/schema/schema_generated.h:2660:8: error: ISO C++ forbids forward references to 'enum' types**\r\n**./tensorflow/lite/schema/schema_generated.h:2660:32: error: field has incomplete type 'enum FlatBuffersVTableOffset'**\r\n  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {\r\n       ^\r\n  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {\r\n", "comments": ["Run ./tensorflow/lite/tools/make/download_dependencies.sh\r\n   before build_lib.sh fixes the issue above, but still encounter error:\r\n\r\ng++ -O3 -DNDEBUG -fPIC  --std=c++11 -I. -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/ -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/tools/make/downloads/absl/absl//types/optional.cc -o /Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/gen/osx_x86_64/obj/tensorflow/lite/tools/make/downloads/absl/absl//types/optional.o\r\ntensorflow/lite/tools/make/downloads/absl/absl//types/optional.cc:20:1: error: no type named 'init_t' in 'absl::nullopt_t'; did you mean\r\n      'optional_internal::init_t'?\r\nnullopt_t::init_t nullopt_t::init;\r\n^~~~~~~~~~~~~~~~~\r\noptional_internal::init_t\r\n/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/absl/absl/types/internal/optional.h:65:8: note: 'optional_internal::init_t' declared\r\n      here\r\nstruct init_t {\r\n       ^\r\ntensorflow/lite/tools/make/downloads/absl/absl//types/optional.cc:20:30: error: no member named 'init' in 'absl::nullopt_t'\r\nnullopt_t::init_t nullopt_t::init;\r\n                  ~~~~~~~~~~~^\r\ntensorflow/lite/tools/make/downloads/absl/absl//types/optional.cc:21:24: error: redeclaration of 'nullopt' with a different type: 'const absl::nullopt_t' vs\r\n      'const ::absl::internal::identity_t<nullopt_t> &' (aka 'const absl::nullopt_t &')\r\nextern const nullopt_t nullopt{nullopt_t::init};\r\n                       ^\r\n/Users/kmok/workspaces/tensorflow/tensorflow/lite/tools/make/downloads/absl/absl/types/optional.h:82:43: note: previous definition is here\r\nABSL_INTERNAL_INLINE_CONSTEXPR(nullopt_t, nullopt,\r\n                                          ^\r\n3 errors generated.\r\n", "I have the same question at 'flatbuffer' and 'enum'. Please make sure that U **use the latest FLATTERBUFFER** so that _flatbuffers/flatbuffers.h has type 'FlatBuffersVTableOffset'._ I solve the problem with it. U can try it. Good Luck.", "It had nothing   related to flatbuffer from the log , also the flatbuffer is downloaded by download_dependencies.sh, not  using system one.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33983\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33983\">No</a>\n"]}, {"number": 33982, "title": "Cherrypick release builds.", "body": "", "comments": ["Note to me: this needs different CI VM for Ubuntu Sanity and Windows GPU. Will run the tests again when ready", "@mihaimaruseac Hi Mihai, I saw multiple PRs being reviewed for r1.15. Any plans for another patch release, e.g. 1.15.1? If so, we could arrange some testing effort in downstream projects.", "Hi.\r\n\r\nWe're trying to get the 1.15 and 2.0 branches to be able to run presubmits on them from the state of the branch instead of from head of `master`. Once this and #33981 are merged we can arrange some downstream testing to ensure things are ok."]}, {"number": 33981, "title": "Cherrypick the relase builds.", "body": "", "comments": ["Note to me: this needs different CI VM for Ubuntu Sanity and Windows GPU (same as #33982). Will run the tests again when ready.\r\n\r\nMoreover, this fails sanity, ubuntu cpu and the macoses because contrib is missing in build definition. Will get a fix.", "MacOS builds fixed, only the sanity and windows bazel left (due to VM change)"]}, {"number": 33980, "title": "Shared library libtensorflowlite.so cannot be found after building from source", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 x86 64 bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): docker image devel-gpu-py3\r\n- TensorFlow version (use command below):\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: \r\n\r\n**Describe the current behavior**\r\n\r\nGoal is to build TF Lite with the extended runtime to support non-tf-ops. For this I followed the steps from the official guide (https://www.tensorflow.org/lite/guide/ops_select#c). I added the missing flex delegate dependencies to `/tensorflow_src/tensorflow/lite/BUILD`:\r\n\r\n```\r\ntflite_cc_shared_object(\r\n    name = \"libtensorflowlite.so\",\r\n    linkopts = select({\r\n        \"//tensorflow:macos\": [\r\n            \"-Wl,-exported_symbols_list,$(location //tensorflow/lite:tflite_exported_symbols.lds)\",\r\n            \"-Wl,-install_name,@rpath/libtensorflowlite.so\",\r\n        ],\r\n        \"//tensorflow:windows\": [],\r\n        \"//conditions:default\": [\r\n            \"-z defs\",\r\n            \"-Wl,--version-script,$(location //tensorflow/lite:tflite_version_script.lds)\",\r\n        ],\r\n    }),\r\n    deps = [\r\n        \":framework\",\r\n        \":tflite_exported_symbols.lds\",\r\n        \":tflite_version_script.lds\",\r\n        \"//tensorflow/lite/kernels:builtin_ops\",\r\n        \"//tensorflow/lite/delegates/flex:delegate\",\r\n    ],\r\n)\r\n```\r\nI ran the configure script with defaults only and the bazel build command with the mentioned options:\r\n\r\n```\r\nbazel build --config=monolithic --define=with_select_tf_ops=true -c opt //tensorflow/lite:libtensorflowlite.so\r\n```\r\nThis builds without errors. When building my C++ app, it also compiles without errors. But when executing the binary, I get an error:\r\n\r\n```\r\n./myapp: error while loading shared libraries: libtensorflowlite.so: cannot open shared object file: No such file or directory\r\n```\r\nThis is not a path problem because it lies in the same directory as my other .so files. The app works when using the static library generated by the shell scripts in the same directory.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe app should run just fine with the paths specified.\r\n\r\n**Other info / logs**\r\n\r\n**.tf_configure.bazel_rc**\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/local/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python3.6/dist-packages\"\r\nbuild --python_path=\"/usr/local/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --config=xla\r\nbuild --config=tensorrt\r\nbuild --action_env TF_CUDA_VERSION=\"10.0\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda-10.0\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"3.5,5.2,6.0,6.1,7.0\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-no_gpu\r\ntest --build_tag_filters=-no_gpu\r\ntest --test_env=LD_LIBRARY_PATH\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n", "comments": ["Building a shared library with `--config=monolithic`, which means to build things static mostly, looks weird. Please remove it and try again.", "@freedomtan \r\nWithout the flag, the library cannot be built anymore. Seems this is mandatory if compiling the extended lite runtime.", "Right, monolithic is expected here, this looks like https://github.com/tensorflow/tensorflow/issues/33959.  To confirm, would you mind commenting out `build --incompatible_remove_legacy_whole_archive` in `tensorflow/.bazelrc` and see if that makes a difference?\r\n\r\n", "@jdduke: thanks for the hint. After some testing, I got varying results depending on the docker image used:\r\n\r\nUsing the `devel-gpu-py3` image (even on a non-GPU system) works if you add the current path of the `.so` to `LD_LIBRARY_PATH` via `export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/custom/path/to/so`. This causes a lot of warnings to appear during the execution of the app (missing cuda and libcudnn), but no errors.\r\n\r\nUsing `devel` and `devel-py3`, the linker does not seem to be able to resolve the symbols within the `.so` file. This can be worked around by using the hack you suggested. For the images I downloaded, however, the file is located in `/tensorflow_src/.bazelrc`. "]}, {"number": 33979, "title": "Crash with MultiWorkerMirroredStrategy Keras Example from docs", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10/7.6.4.38\r\n- GPU model and memory: Tesla P100 16GB\r\n\r\n**Describe the current behavior**\r\nI can't make run the tutorial described here:\r\nhttps://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\r\nThe example is crashing with the following TF_CONFIG\r\n```python\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"server1:23531\", \"server2:41660\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 0}\r\n})\r\n```\r\nIn the other machine\r\n```python\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"server1:23531\", \"server2:41660\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 1}\r\n})\r\n```\r\nWhen the script start processing the first epoch it crashes, I tested on a single machine and it worked,\r\n\r\n**Describe the expected behavior**\r\nDon't crash...\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\n#!/usr/bin/env python\r\n# coding: utf-8\r\n\r\n# # Multiple Worker Training\r\nimport tensorflow as tf\r\n# Use tensorflow datasets\r\nimport tensorflow_datasets as tfds\r\ntfds.disable_progress_bar()\r\nimport os\r\nimport json\r\n\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 64\r\n\r\nNUM_WORKERS = 2\r\n# Here the batch size scales up by number of workers since \r\n# `tf.data.Dataset.batch` expects the global batch size. Previously we used 64, \r\n# and now this becomes 128.\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS\r\n\r\n# Define TF_CONVIG environemnt variable\r\n# This step create a environment variable that gives the location and ports available on the server to perform training.\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"server1:23531\", \"server2:41660\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 1}\r\n})\r\n\r\n# This need to be called at the program startup\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\n\r\ndef make_datasets_unbatched():\r\n    # Scaling MNIST data from (0, 255] to (0., 1.]\r\n    def scale(image, label):\r\n        image = tf.cast(image, tf.float32)\r\n        image /= 255\r\n        return image, label\r\n\r\n    datasets, info = tfds.load(name='mnist',with_info=True,as_supervised=True)\r\n\r\n    #return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)\r\n    return datasets['train'].map(scale).shuffle(BUFFER_SIZE)\r\n\r\n\r\ndef build_and_compile_cnn_model():\r\n    model = tf.keras.Sequential([\r\n          tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n          tf.keras.layers.MaxPooling2D(),\r\n          tf.keras.layers.Flatten(),\r\n          tf.keras.layers.Dense(64, activation='relu'),\r\n          tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    model.compile(\r\n          loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n          optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n          metrics=['accuracy'])\r\n    return model\r\n\r\n# #### Define a Strategy and distribute training\r\nwith strategy.scope():\r\n    # Creation of dataset, and model building/compiling need to be within \r\n    # `strategy.scope()`.\r\n    train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\r\n    multi_worker_model = build_and_compile_cnn_model()\r\n    \r\n\r\nmulti_worker_model.fit(x=train_datasets, epochs=3)\r\n\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```bash\r\n2019-11-04 16:59:37.654216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-04 16:59:38.488142: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-04 16:59:38.488853: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4\r\nAdditional GRPC error information:\r\n{\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n         [[{{node allreduce_1/CollectiveReduce_1}}]]\r\n         [[replica_2/strided_slice/_7]]\r\n2019-11-04 16:59:38.488860: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4\r\nAdditional GRPC error information:\r\n{\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n         [[{{node allreduce_1/CollectiveReduce_1}}]]\r\n         [[GroupCrossDeviceControlEdges_0/metrics/accuracy/div_no_nan/_85]]\r\n2019-11-04 16:59:38.489032: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4\r\nAdditional GRPC error information:\r\n{\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n         [[{{node allreduce_1/CollectiveReduce_1}}]]\r\n         [[SGD/SGD/group_deps/_165]]\r\n2019-11-04 16:59:38.489335: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4\r\nAdditional GRPC error information:\r\n{\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n         [[{{node allreduce_1/CollectiveReduce_1}}]]\r\n2019-11-04 16:59:39.358800: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4\r\nAdditional GRPC error information:\r\n{\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n         [[{{node allreduce_1/CollectiveReduce_1}}]]\r\n         [[GroupCrossDeviceControlEdges_2/SGD/SGD/update_0/Const/_117]]\r\n2019-11-04 16:59:40.201240: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-11-04 16:59:40.210692: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-11-04 16:59:40.276529: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-11-04 16:59:40.294888: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-11-04 16:59:40.356048: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-11-04 16:59:40.360051: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n      1/Unknown - 7s 7s/stepTraceback (most recent call last):\r\n  File \"multi_worker_train.py\", line 83, in <module>\r\n    multi_worker_model.fit(x=train_datasets, epochs=3)\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 789, in fit\r\n    *args, **kwargs)\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 776, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 771, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: 3 root error(s) found.\r\n  (0) Internal:  Collective Op  has group_size 8 and group_key2 but that group has size 4\r\nAdditional GRPC error information:\r\n{\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n         [[node allreduce_1/CollectiveReduce_1 (defined at /mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n         [[GroupCrossDeviceControlEdges_0/metrics/accuracy/div_no_nan/_85]]\r\n  (1) Internal:  Collective Op  has group_size 8 and group_key2 but that group has size 4\r\nAdditional GRPC error information:\r\n{\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n         [[node allreduce_1/CollectiveReduce_1 (defined at /mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n  (2) Internal:  Collective Op  has group_size 8 and group_key2 but that group has size 4\r\nAdditional GRPC error information:\r\n{\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n         [[node allreduce_1/CollectiveReduce_1 (defined at /mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n         [[replica_2/strided_slice/_7]]\r\n0 successful operations.\r\n2 derived errors ignored. [Op:__inference_distributed_function_2500]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function -> distributed_function\r\n\r\n2019-11-04 16:59:40.527776: W tensorflow/core/kernels/data/generator_dataset_op.cc:102] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n2019-11-04 16:59:40.884570: W tensorflow/core/common_runtime/eager/context.cc:290] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n```\r\n", "comments": ["> _Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template_\r\n> \r\n> **System information**\r\n> \r\n>     * Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n> \r\n>     * OS Platform and Distribution: Ubuntu 18.04\r\n> \r\n>     * TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n> \r\n>     * TensorFlow version (use command below): 2.0\r\n> \r\n>     * Python version: 3.7.3\r\n> \r\n>     * CUDA/cuDNN version: 10/7.6.4.38\r\n> \r\n>     * GPU model and memory: Tesla P100 16GB\r\n> \r\n> \r\n> **Describe the current behavior**\r\n> I can't make run the tutorial described here:\r\n> https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\r\n> The example is crashing with the following TF_CONFIG\r\n> \r\n> ```python\r\n> os.environ['TF_CONFIG'] = json.dumps({\r\n>     'cluster': {\r\n>         'worker': [\"server1:23531\", \"server2:41660\"]\r\n>     },\r\n>     'task': {'type': 'worker', 'index': 0}\r\n> })\r\n> ```\r\n> \r\n> In the other machine\r\n> \r\n> ```python\r\n> os.environ['TF_CONFIG'] = json.dumps({\r\n>     'cluster': {\r\n>         'worker': [\"server1:23531\", \"server2:41660\"]\r\n>     },\r\n>     'task': {'type': 'worker', 'index': 1}\r\n> })\r\n> ```\r\n> \r\n> When the script start processing the first epoch it crashes, I tested on a single machine and it worked,\r\n> \r\n> **Describe the expected behavior**\r\n> Don't crash...\r\n> \r\n> **Code to reproduce the issue**\r\n> Provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n> \r\n> ```python\r\n> #!/usr/bin/env python\r\n> # coding: utf-8\r\n> \r\n> # # Multiple Worker Training\r\n> import tensorflow as tf\r\n> # Use tensorflow datasets\r\n> import tensorflow_datasets as tfds\r\n> tfds.disable_progress_bar()\r\n> import os\r\n> import json\r\n> \r\n> BUFFER_SIZE = 10000\r\n> BATCH_SIZE = 64\r\n> \r\n> NUM_WORKERS = 2\r\n> # Here the batch size scales up by number of workers since \r\n> # `tf.data.Dataset.batch` expects the global batch size. Previously we used 64, \r\n> # and now this becomes 128.\r\n> GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS\r\n> \r\n> # Define TF_CONVIG environemnt variable\r\n> # This step create a environment variable that gives the location and ports available on the server to perform training.\r\n> os.environ['TF_CONFIG'] = json.dumps({\r\n>     'cluster': {\r\n>         'worker': [\"server1:23531\", \"server2:41660\"]\r\n>     },\r\n>     'task': {'type': 'worker', 'index': 1}\r\n> })\r\n> \r\n> # This need to be called at the program startup\r\n> strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n> \r\n> \r\n> def make_datasets_unbatched():\r\n>     # Scaling MNIST data from (0, 255] to (0., 1.]\r\n>     def scale(image, label):\r\n>         image = tf.cast(image, tf.float32)\r\n>         image /= 255\r\n>         return image, label\r\n> \r\n>     datasets, info = tfds.load(name='mnist',with_info=True,as_supervised=True)\r\n> \r\n>     #return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)\r\n>     return datasets['train'].map(scale).shuffle(BUFFER_SIZE)\r\n> \r\n> \r\n> def build_and_compile_cnn_model():\r\n>     model = tf.keras.Sequential([\r\n>           tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n>           tf.keras.layers.MaxPooling2D(),\r\n>           tf.keras.layers.Flatten(),\r\n>           tf.keras.layers.Dense(64, activation='relu'),\r\n>           tf.keras.layers.Dense(10, activation='softmax')\r\n>     ])\r\n>     model.compile(\r\n>           loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n>           optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n>           metrics=['accuracy'])\r\n>     return model\r\n> \r\n> # #### Define a Strategy and distribute training\r\n> with strategy.scope():\r\n>     # Creation of dataset, and model building/compiling need to be within \r\n>     # `strategy.scope()`.\r\n>     train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\r\n>     multi_worker_model = build_and_compile_cnn_model()\r\n>     \r\n> \r\n> multi_worker_model.fit(x=train_datasets, epochs=3)\r\n> ```\r\n> \r\n> **Other info / logs**\r\n> Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n> \r\n> ```shell\r\n> 2019-11-04 16:59:37.654216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n> 2019-11-04 16:59:38.488142: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 2019-11-04 16:59:38.488853: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4\r\n> Additional GRPC error information:\r\n> {\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n>          [[{{node allreduce_1/CollectiveReduce_1}}]]\r\n>          [[replica_2/strided_slice/_7]]\r\n> 2019-11-04 16:59:38.488860: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4\r\n> Additional GRPC error information:\r\n> {\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n>          [[{{node allreduce_1/CollectiveReduce_1}}]]\r\n>          [[GroupCrossDeviceControlEdges_0/metrics/accuracy/div_no_nan/_85]]\r\n> 2019-11-04 16:59:38.489032: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4\r\n> Additional GRPC error information:\r\n> {\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n>          [[{{node allreduce_1/CollectiveReduce_1}}]]\r\n>          [[SGD/SGD/group_deps/_165]]\r\n> 2019-11-04 16:59:38.489335: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4\r\n> Additional GRPC error information:\r\n> {\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n>          [[{{node allreduce_1/CollectiveReduce_1}}]]\r\n> 2019-11-04 16:59:39.358800: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Collective Op  has group_size 8 and group_key2 but that group has size 4\r\n> Additional GRPC error information:\r\n> {\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n>          [[{{node allreduce_1/CollectiveReduce_1}}]]\r\n>          [[GroupCrossDeviceControlEdges_2/SGD/SGD/update_0/Const/_117]]\r\n> 2019-11-04 16:59:40.201240: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n> 2019-11-04 16:59:40.210692: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n> 2019-11-04 16:59:40.276529: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n> 2019-11-04 16:59:40.294888: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n> 2019-11-04 16:59:40.356048: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n> 2019-11-04 16:59:40.360051: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n>       1/Unknown - 7s 7s/stepTraceback (most recent call last):\r\n>   File \"multi_worker_train.py\", line 83, in <module>\r\n>     multi_worker_model.fit(x=train_datasets, epochs=3)\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n>     use_multiprocessing=use_multiprocessing)\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 789, in fit\r\n>     *args, **kwargs)\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 776, in wrapper\r\n>     mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n>     task_id, session_config, rpc_layer)\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n>     return worker_fn(strategy)\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 771, in _worker_fn\r\n>     return method(model, **kwargs)\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\r\n>     total_epochs=epochs)\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n>     batch_outs = execution_function(iterator)\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n>     distributed_function(input_fn))\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n>     result = self._call(*args, **kwds)\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\r\n>     return self._stateless_fn(*args, **kwds)\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n>     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n>     self.captured_inputs)\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n>     ctx, args, cancellation_manager=cancellation_manager)\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n>     ctx=ctx)\r\n>   File \"/mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n>     six.raise_from(core._status_to_exception(e.code, message), None)\r\n>   File \"<string>\", line 3, in raise_from\r\n> tensorflow.python.framework.errors_impl.InternalError: 3 root error(s) found.\r\n>   (0) Internal:  Collective Op  has group_size 8 and group_key2 but that group has size 4\r\n> Additional GRPC error information:\r\n> {\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n>          [[node allreduce_1/CollectiveReduce_1 (defined at /mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n>          [[GroupCrossDeviceControlEdges_0/metrics/accuracy/div_no_nan/_85]]\r\n>   (1) Internal:  Collective Op  has group_size 8 and group_key2 but that group has size 4\r\n> Additional GRPC error information:\r\n> {\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n>          [[node allreduce_1/CollectiveReduce_1 (defined at /mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n>   (2) Internal:  Collective Op  has group_size 8 and group_key2 but that group has size 4\r\n> Additional GRPC error information:\r\n> {\"created\":\"@1572886778.488763529\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Collective Op  has group_size 8 and group_key2 but that group has size 4\",\"grpc_status\":13}\r\n>          [[node allreduce_1/CollectiveReduce_1 (defined at /mnt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n>          [[replica_2/strided_slice/_7]]\r\n> 0 successful operations.\r\n> 2 derived errors ignored. [Op:__inference_distributed_function_2500]\r\n> \r\n> Function call stack:\r\n> distributed_function -> distributed_function -> distributed_function\r\n> \r\n> 2019-11-04 16:59:40.527776: W tensorflow/core/kernels/data/generator_dataset_op.cc:102] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n> 2019-11-04 16:59:40.884570: W tensorflow/core/common_runtime/eager/context.cc:290] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n> ```\r\n\r\n I get the same issue,  \r\n\r\nmulti_worker_model.fit(x=train_datasets, epochs=3)\r\nchange to \r\n\r\ntrain_datasets = train_datasets.repeat()\r\n\r\nmodel.fit(train_datasets, steps_per_epoch=total_train_num/ // batch_size, epochs=3)\r\nit works.\r\n\r\n", "I realized as well, that quite often tensorflow 2.0 becomes more stable when we define specifically the tf.data dataset size (I've also found similar issue on multiple GPUs). Anyway this highlight a bug or at least something that need to be changed on the tutorial right?\r\n\r\nAnyway thanks @zwqjoy", "> I realized as well, that quite often tensorflow 2.0 becomes more stable when we define specifically the tf.data dataset size (I've also found similar issue on multiple GPUs). Anyway this highlight a bug or at least something that need to be changed on the tutorial right?\r\n> \r\n> Anyway thanks @zwqjoy\r\n\r\nexport NCCL_DEBUG=INFO\r\nexport NCCL_SOCKET_IFNAME=eth0\r\n\r\n\r\nworker 0:\r\n<img width=\"1231\" alt=\"\u56fe\u7247\" src=\"https://user-images.githubusercontent.com/12653212/69774767-da81d400-11d1-11ea-8563-48366b80c27f.png\">\r\n\r\n\r\n\r\nworker 1:\r\n\r\n<img width=\"1236\" alt=\"\u56fe\u7247\" src=\"https://user-images.githubusercontent.com/12653212/69774776-e077b500-11d1-11ea-97bd-22215aac2279.png\">\r\n\r\n\r\n", "Thanks for the report - we'll update the tutorial to mention that `steps_per_epoch` needs to be provided to ensure successful training.", "@leonardoaraujosantos I think this issue is resolved by @rcho who updated the [source](https://github.com/tensorflow/docs/commit/ce303cb9045562e3af14be7f642b08b9f0c7a9e2#diff-d4562f95b1b572ef2da36dbba776cf3c). You could also check the updated tutorial [here](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/multi_worker_with_keras.ipynb).\r\n\r\nI am closing this issue as it was resolved. feel free to open the issue if it persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33979\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33979\">No</a>\n"]}, {"number": 33978, "title": "Keras load weights fails to load model from directory containing [[", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary, using conda\r\n- TensorFlow version (use command below): unknown 1.14.0, with intel MKL\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: no gpu\r\n- GPU model and memory: no gpu\r\n\r\n**Describe the current behavior**\r\nWhen model checkpoint is stored in a directory containing `[[` in its name, the model fails to load, looks like the model files are missing, throwing NotFoundError\r\n\r\n`model.load_weights(osp.join(model_dir, 'model'))` throws exception when for example, the model_dir='/home/projects/my_ml/data/sims/bs=2048_dataset=[[2019_9_13],[2019_9_12,2019_9_11,2019_9_10,2019_9_9]]'\r\n\r\n**Describe the expected behavior**\r\n\r\n`model.load_weights(osp.join(model_dir, 'model'))` should load the data without problems\r\n\r\n**Code to reproduce the issue**\r\nHere is [google colab code](https://colab.research.google.com/drive/1N1oPDXcmijzXs49C3DDLjm2pVHsUQvYP) with mnist example.\r\n\r\n**Other info / logs**\r\nThe error is NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for bs=2048_dataset=[[2019_9_13],[2019_9_12,2019_9_11,2019_9_10,2019_9_9]]/model", "comments": ["@racinmat ,\r\nThanks for reporting the issue,  can you please provide access to link` google colab code` provided? ", "Fixed, I put here new link, it should be accessible now.", "Issue replicating for the given code in both TF-1.14 and 1.15. Thanks!", "Is there some additional response needed? Or why the tag?", "@racinmat Model directory name looks very interesting. What kind of use case you have that requires this interesting model directory? Thanks!\r\n ", "I just cast list of parameters of training the model to string, and some parameters are 2D arrays, and I am using this string as a directory with all files related to that run. I made my quick fix by replacing [ with ( and ] with ), but I consider it a workaround, not a proper solution.", "Hi @racinmat, this does seem like a bug in our checkpoint loading code. We're not able to fix this right away, so I'll mark this as open for contributions if anyone wants to help resolve this.", "I have the same issue\r\nwhen there is any char between [], it will fail to load\r\ne.g. [a] doesn't work, but [] works", "I can replicate this issue in TensorFlow 2.3.0. Hope I can take a try at fixing the issue.", "> I can replicate this issue in TensorFlow 2.3.0. Hope I can take a try at fixing the issue.\r\n\r\nHi @AnimeshSinha1309 , are you still working on this issue?", "Sorry, I kinda got stuck, anyone who wants to work is welcome to. I am unable to setup debugging on the C++ portions of the Tensorflow code, I would greatly appriciate it if someone could explain how to step through TF code, Python and C++ together.", "Comments say the issue is happening also on Tensorflow 2.3, so we still need help I think.", "@ymodak  Hi, since this issue is still there in Tensorflow 2.3, can you prevent this from automatic closing until this is fixed? Thanks.\r\n", "Was able to reproduce your issue in TF Nightly 2.6, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/99d6bc061e5f29c90db6886ea8e4b2fd/untitled43.ipynb).", "You need to use in your colab:\r\n```python\r\nmodel = model.load_weights(osp.join(re.escape(model_dir), 'model'))\r\n```\r\nI don't know if we could escape internally", "@racinmat ,\r\n\r\nCan you please respond to the above comment? ", "So I can save the model without escaping, but I need to escape it for loading?\r\nIn the documentation I can not find any mention of regular expressions, so it's unclear why it does help.\r\nThis should be mentioned in the docs if this is the proper way to escape model names, right?\r\nI understand this is a workaround, not a fix of the issue.", "Can you reproduce this with TF 2.5.0?\n\nEDIT:\nI see It was tested on nightly", "Was able to replicate the issue with TF v2.5 ,please find the [gist ](https://colab.research.google.com/gist/mohantym/aed6490aedca803357a48b577946d1cc/33798.ipynb)here. ", "So I think the issue here is that loading the model needs to identify all files in a subdirectory and to do that it uses `GetMatchingPaths`. But `[` is a special character for `GetMatchingPaths`.", "Yes I think is the same as https://github.com/tensorflow/tensorflow/issues/35489", "Confirmed. Closing this as duplicate of #35489\r\n\r\nIt will take some time until this gets fixed, but hopefully we can get this fixed in the near future.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33978\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33978\">No</a>\n"]}, {"number": 33977, "title": "AttributeError: 'Tensor' object has no attribute '_keras_shape'", "body": "\r\nthis si my code \r\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\r\n                                               include_top=False,\r\n                                               weights='imagenet')\r\n\r\n\r\nthis is my error\r\nTraceback (most recent call last):\r\n  File \"/Users/***/Sites/python/nlp/images/Transfer learning with a pretrained ConvNet.py\", line 154, in <module>\r\n    weights='imagenet')\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/__init__.py\", line 70, in wrapper\r\n    return base_fun(*args, **kwargs)\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/mobilenet_v2.py\", line 32, in MobileNetV2\r\n    return mobilenet_v2.MobileNetV2(*args, **kwargs)\r\n  File \"/anaconda3/lib/python3.6/site-packages/keras_applications/mobilenet_v2.py\", line 355, in MobileNetV2\r\n    expansion=1, block_id=0)\r\n  File \"/anaconda3/lib/python3.6/site-packages/keras_applications/mobilenet_v2.py\", line 461, in _inverted_res_block\r\n    in_channels = inputs._keras_shape[-1]\r\nAttributeError: 'Tensor' object has no attribute '_keras_shape'\r\n\r\nI learn tensorflow2.0  Tutorials Transfer learning with a pretrained ConvNet\r\nplease help me .\r\n\r\n", "comments": ["what you can do is:\r\ninstead of this code: `from keras.models import Model` change to `from tensorflow.keras.models import Model`  or `from tensorflow.python.keras.layers import Input` to `from keras.layers import Input`\r\n\r\nDepending on the version of tensorflow/keras the attribute names can differ.\r\n\r\nI hope it helps.\r\nPing me if this doesn't solve the problem.", "The tutorial https://www.tensorflow.org/tutorials/images/transfer_learning works as expected in google colab.\r\nIf have custom code, try replacing imports from ```keras``` to ```tf.keras```\r\nAlso what version of TF and Keras are you using? ", "my code is the tutorial code ,i use the pychrom  \r\n\r\ntf.version\r\n2.0.0-beta0\r\ntf.keras.version\r\n2.2.4-tf\r\n\r\n@dubesar,\r\nyour solvtion  ,it not work.\r\n@ymodak \r\nmy code and  the tutorial code is \" keras= tf.keras\"   \r\n it not work.please help me . i try many times ,it not work\r\n\r\n", "google colab. is  ok\uff0cwhy pychrom is not work\u3002@ymodak", "it is work ,just update TF and keras @ymodak @dubesar "]}, {"number": 33976, "title": "  java.lang.IllegalArgumentException: Invalid output Tensor index: 1", "body": "I've trained my own model for object detection with TensorFlow and I got it working with Tensorflow mobile for android. Now since Tensorflow Lite is released and is going to replace mobile in the future I wanted to start working with it. The Tensorflow team provided a demo for TFLite for object detection. So I tried to get it working with my model but I got the error in the title. Here's the logcat :\r\n\r\n    java.lang.IllegalArgumentException: Invalid output Tensor index: 1\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.getOutputTensor(NativeInterpreterWrapper.java:308)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:164)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:296)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:194)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:181)", "comments": ["Are there any updates on this issue?", "No\nOn Thu, Nov 21, 2019, 17:41 Auburgo <notifications@github.com> wrote:\n\n> Are there any updates on this issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33976?email_source=notifications&email_token=ANVTWPMWL7OTRUXNJUISNHLQUZ3FJA5CNFSM4JIULELKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE2ALBQ#issuecomment-557057414>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ANVTWPLRCUT4EADVCXO57G3QUZ3FJANCNFSM4JIULELA>\n> .\n>\n", "@NidamanuriChenchaiah Some values in the android example are hard coded for that particular use case, so this might not work as is for a new tflite model. I suggest you try the following: \r\n\r\n1. Refer to a similar issue ([31788](https://github.com/tensorflow/tensorflow/issues/31788)) and try resolving it using their suggestions:\r\n  a) Match input and output shape array, modifying NUM_DETECTIONS, etc.\r\n  b) Use tools like our [.tflite visualizer](https://www.tensorflow.org/lite/guide/faq#how_do_i_inspect_a_tflite_file) to better understand your graph inputs/outputs, or the excellent [Netron](https://electronjs.org/apps/netron) model visualizer (that handles .tflite models as well as other model formats) to ensure input/output tensors have the same shape.\r\n\r\n2. If nothing works: upload the model, share a github link to your project or a google colab link where we can reproduce the issue. ", "Closing the issue due to inactivity. If you can provide more information, feel free to re-open the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33976\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33976\">No</a>\n", "how did you solve this exception please\r\n java.lang.IllegalArgumentException: Invalid output Tensor index: 1 i converted a yolov3-tiny model\r\ni changed the NUM_DETECTION into 2535 (NUM_DETECTION=2535) because the input shape is (1,416,416,6) and the output shape is (1,2535,6)"]}, {"number": 33975, "title": "Bazel build does not pick up correct compiler include paths", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.5\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7.5\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): GCC 8.2.0\r\n- CUDA/cuDNN version: 10.1\r\n\r\n\r\n**Describe the problem**\r\n\r\nBazel does not pick up the correct include paths of GCC and returns errors such as:\r\n\r\n```\r\nERROR: /tmp/easybuild-tmp/eb-5QGVSJ/tmpMZolLj-bazel-build/external/fft2d/BUILD.bazel:27:1: undeclared inclusion(s) in rule '@fft2d//:fft2d':\r\nthis rule is missing dependency declarations for the following files included by 'external/fft2d/fft2d/fftsg2d.c':\r\n  '/sw/installed/GCCcore/8.2.0/lib/gcc/x86_64-pc-linux-gnu/8.2.0/include/stddef.h'\r\n  '/sw/installed/GCCcore/8.2.0/lib/gcc/x86_64-pc-linux-gnu/8.2.0/include/stdarg.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n- `TF_NEED_CUDA=1 ./configure`\r\n- `export TF_MKL_DOWNLOAD=1 &&   bazel --output_base=/tmp/easybuild-tmp/eb-5QGVSJ/tmpMZolLj-bazel-build --install_base=/tmp/easybuild-tmp/eb-5QGVSJ/tmpMZolLj-bazel-build/inst_base --output_user_root=/tmp/easybuild-tmp/eb-5QGVSJ/tmp514aJM-user_root build --compilation_mode=opt --config=opt --subcommands --verbose_failures --jobs=24 --action_env=PYTHONPATH --action_env=EBPYTHONPREFIXES --distinct_host_configuration=false  --config=mkl //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nNote that the installation is triggered via EasyBuild\r\n\r\n**Any other info / logs**\r\n[command.log](https://github.com/tensorflow/tensorflow/files/3804323/command.log)\r\n\r\n", "comments": ["This seems to happen only when CUDA is requested (TF_NEED_CUDA=1` or `--config=cuda`)\r\n\r\nSo I traced this down to some mysterious `--crosstool_top=@local_config_cuda//crosstool:toolchain` which led me to a file in the `*bazel-build` folder: `/external/local_config_cuda/crosstool/BUILD`\r\n\r\nThere I see some `cc_toolchain_config` with `builtin_include_directories` which is set to a list of folders. In that I found `/software/haswell/GCCcore/8.2.0/include/c++/8.2.0/x86_64-pc-linux-gnu` which is symlinked to from `/sw/installed/GCCcore/8.2.0/lib/gcc/x86_64-pc-linux-gnu/8.2.0/include/`.\r\n\r\nI assume the paths in the `BUILD` file come from something like `gcc -v -E - < /dev/null` which includes resolved paths, but the dependency checker does use the unresolved paths. Can this be fixed or is this already fixed somewhere?", "I don't have any fix for this but I have also been struggling with getting it to compile from source with similar errors such as:\r\n```\r\nERROR: /home/user/.tmp/external/zlib_archive/BUILD.bazel:5:1: undeclared inclusion(s) in rule '@zlib_archive//:zlib':\r\nthis rule is missing dependency declarations for the following files included by 'external/zlib_archive/uncompr.c':\r\n  '/usr/ebuild/software/GCCcore/9.2.0/lib/gcc/x86_64-pc-linux-gnu/9.2.0/include/stddef.h'\r\n  '/usr/ebuild/software/GCCcore/9.2.0/lib/gcc/x86_64-pc-linux-gnu/9.2.0/include-fixed/limits.h'\r\n  '/usr/ebuild/software/GCCcore/9.2.0/lib/gcc/x86_64-pc-linux-gnu/9.2.0/include-fixed/syslimits.h'\r\n  '/usr/ebuild/software/GCCcore/9.2.0/lib/gcc/x86_64-pc-linux-gnu/9.2.0/include/stdarg.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 12.929s, Critical Path: 4.14s\r\n```\r\nor even\r\n```\r\nERROR: /home/user/tests/tensorflowTest/fromsource/tensorflow/tensorflow/lite/c/BUILD:6:1: undeclared inclusion(s) in rule '//tensorflow/lite/c:c_api_internal':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/lite/c/c_api_internal.c':\r\n  '/usr/ebuild/software/GCCcore/9.2.0/lib/gcc/x86_64-pc-linux-gnu/9.2.0/include/stdbool.h'\r\n  '/usr/ebuild/software/GCCcore/9.2.0/lib/gcc/x86_64-pc-linux-gnu/9.2.0/include/stddef.h'\r\n  '/usr/ebuild/software/GCCcore/9.2.0/lib/gcc/x86_64-pc-linux-gnu/9.2.0/include/stdint.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build                                                                                                                                                                      INFO: Elapsed time: 13.962s, Critical Path: 6.44s\r\nINFO: 3 processes: 3 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nI haven't been able to get anything to work despite editing a myriad of files, rerunning configure, clearing out the bazel cache, and recloning fresh.\r\n\r\nI'm trying to compile it with CUDA support using a miniconda distribution with the bare minimums required for compilation on CentOS7.", "I was able to dance around this issue using the following patch: https://github.com/easybuilders/easybuild-easyconfigs/blob/develop/easybuild/easyconfigs/t/TensorFlow/TensorFlow-1.14.0_fix-cuda-build.patch .\r\n\r\nIf you're using EasyBuild: note that a sufficiently recent `CUDA` version is required by TensorFlow 2.0.0 (the one included in `fosscuda/2019a` is too old)", "This patch does indeed work for us. It is up to the maintainers to decide whether this is a Bazel or a Tensorflow bug, I opened an issue there too. It seems the core issue is that `g++ -E -v -` returns resolved paths but the dependency checker does use plain/unresolved paths and Bazel does not try to resolve them there.", "I would like to confirm that this patch works and thank @boegel for the link to the patch. I wrote a small python script to make changes to the highlighted file in my automatic installer for tensorflow 2.0 . \r\n\r\nHere's hoping to a more official fix from the maintainers of either bazel or tensorflow.", "The proposed patch does not work in some circumstances such as when using compiler wrappes like ccache as that throws of the relation between compiler and include paths. See https://github.com/bazelbuild/bazel/issues/10167\r\n\r\nSo I guess this must be fixed in Bazel.\r\n\r\n**However**: Strangely this issue does not occur when not using the CUDA build, so for \"normal\" GCC it must be able to handle the include paths on symlinks. Most likely the creation of a derived toolchain is wrong/outdated? I've seen some discussion about handling \"system\" include paths internally in Bazel making this handling not required. But I don't understand enough about the Bazel internals to propose a change.", "I'm a little confused. so the issue is with Bazel not being able to resolve the paths in general or particularly when using mkl flag?", "This has nothing to do with MKL, it is Bazel generally not able to match resolved vs unresolved paths, see https://github.com/tensorflow/tensorflow/issues/33975#issuecomment-549397196 and https://github.com/bazelbuild/bazel/issues/10167: It compares unresolved symlinks to resolved paths reported by gcc. @ymodak You might want to fix the tags and assignment.\r\n\r\nA potential patch to TensorFlow is https://github.com/easybuilders/easybuild-easyconfigs/blob/develop/easybuild/easyconfigs/t/TensorFlow/TensorFlow-2.1.0_fix-cuda-build.patch which works so far for us (using symlinks AND compiler wrapper ccache).", "@Flamefire,\r\nIs this still an issue?\r\n\r\nCould you please update TensorFlow to the latest stable version v.2.4.1 and let us know if you are facing the same error. Thanks!", "Yes, even for 2.4.1 we still need to patch using https://github.com/easybuilders/easybuild-easyconfigs/blob/develop/easybuild/easyconfigs/t/TensorFlow/TensorFlow-2.1.0_fix-cuda-build.patch", "We pass -no-canonical-prefixes and -no-canonical-system-headers so gcc does not resolve symlinks. If gcc still resolves symlinks, I'd be curious what happens.\r\nCan somebody create a Dockerfile that I could use to debug that ends with this error?", "> We pass -no-canonical-prefixes and -no-canonical-system-headers so gcc does not resolve symlinks.\r\n\r\nWhere? Not in https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda_configure.bzl#L268 as far as I can tell\r\n\r\nNote that recreating this issue is easy: Simply create a symlink to the GCC installation folder and pass the gcc path via the symlink, not the actual path", "@Flamefire \r\nIs this still an issue in latest tf version, please confirm and move to close status if it is not an issue.", "Yes this is still an issue", "Hi @Flamefire ! Can you try [latest document instructions ](https://www.tensorflow.org/install/source)with GCC 7.3.2 , Bazel 3.7.2 and TF 2.7 and let us know?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33975\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33975\">No</a>\n"]}, {"number": 33974, "title": "assert_shapes broken code in documentation", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/debugging/assert_shapes\r\n\r\n## Description of issue (what needs changing):\r\nThe source code in the example is incorrect.\r\n\r\nIs:\r\ntf.assert_shapes([\r\n  (x: ('N', 'Q')),\r\n  (y: ('N', 'D')),\r\n  (param: ('Q',)),\r\n  (scalar: ()),\r\n])\r\n\r\nShould be:\r\ntf.assert_shapes([\r\n  (x, ('N', 'Q')),\r\n  (y, ('N', 'D')),\r\n  (param, ('Q',)),\r\n  (scalar, ()),\r\n]).\r\n\r\nNote that \":\" is not allowed in Python to form 2-tuples. ", "comments": ["Thanks for the report! Can you make a PR? That file is here: https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/check_ops.py#L1697-L1747 (link at top of page on tf.org)", "@lamberta  I have opened a pull request making the relevant changes to the documentation folder.", "Thanks @dubesar, then its sorted.", "@MarkDaoust  this issue can be closed now as it's solved"]}, {"number": 33973, "title": "Calling set_session before fit_generator causes training to freeze when using multiprocessing", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- Mobile device if the issue happens on mobile device: N/A\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: 3.5\r\n- Bazel version: N/A\r\n- GCC/Compiler version: N/A\r\n- CUDA/cuDNN version: 10.0 / 7\r\n- GPU model and memory: GTX 1080Ti\r\n\r\n**Describe the current behavior**\r\nI am trying to modify TF session parameters in combination with Keras (e.g. the allowed GPU memory) and use `set_session()` to store these parameters. However, if `set_session()` is called  before `fit_generator()`, it causes the training to freeze when using multiprocessing. To reproduce the error, set `if True` in the main function.\r\n\r\n**Describe the expected behavior**\r\nNo freeze should occur.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.keras.layers import Input, Conv2D\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.utils import Sequence\r\n\r\n\r\ndef get_model(input_shape, output_shape, compile_batch_size):\r\n    assert K.image_data_format() == 'channels_last'\r\n    m_input = Input(shape=input_shape, batch_size=compile_batch_size)\r\n    m_output = Conv2D(output_shape[-1], 1, activation=None)(m_input)\r\n    model = Model(inputs=m_input, outputs=m_output)\r\n    return model\r\n\r\n\r\nclass DataGenerator(Sequence):\r\n    def __init__(self, batch_size, image_size, num_batches):\r\n        assert K.image_data_format() == 'channels_last'\r\n\r\n        self.batch_size = batch_size\r\n        self.image_size = image_size\r\n        self.num_batches = num_batches\r\n        self.on_epoch_end()\r\n\r\n    def __len__(self):\r\n        assert self.num_batches > 0\r\n        return self.num_batches\r\n\r\n    def __getitem__(self, index):\r\n        return np.zeros((self.batch_size,) + self.image_size).astype('float32'), np.zeros(\r\n            (self.batch_size,) + self.image_size).astype('float32')\r\n\r\n    def on_epoch_end(self):\r\n        pass\r\n\r\n\r\nif __name__ == '__main__':\r\n    print(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\n    # set to True to trigger infinite wait\r\n    if True:\r\n        # limit GPU memory\r\n        config = tf.ConfigProto()\r\n        config.gpu_options.allow_growth = True\r\n        config.gpu_options.per_process_gpu_memory_fraction = 0.5\r\n        tf.keras.backend.set_session(tf.Session(config=config))\r\n\r\n    inp_shape = (128, 64, 2)\r\n    outp_shape = (128, 64, 2)\r\n    batch_size = 16\r\n\r\n    gen = DataGenerator(batch_size, inp_shape, 10)\r\n    model = get_model(inp_shape, outp_shape, batch_size)\r\n    model.summary()\r\n    model.compile(loss=[\"mse\"], optimizer=\"adam\", metrics=[\"accuracy\"])\r\n    model.fit_generator(gen, steps_per_epoch=10, epochs=5, workers=2, use_multiprocessing=True)\r\n\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"keras_mwe.py\", line 57, in <module>\r\n    model.fit_generator(gen, steps_per_epoch=10, epochs=5, workers=2, use_multiprocessing=True)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\", line 1433, in fit_generator\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_generator.py\", line 220, in model_iteration\r\n    batch_data = _get_next_batch(generator, mode)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_generator.py\", line 362, in _get_next_batch\r\n    generator_output = next(generator)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/data_utils.py\", line 779, in get\r\n    inputs = self.queue.get(block=True).get()\r\n  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 602, in get\r\n    self.wait(timeout)\r\n  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 599, in wait\r\n    self._event.wait(timeout)\r\n  File \"/usr/lib/python3.5/threading.py\", line 549, in wait\r\n    signaled = self._cond.wait(timeout)\r\n  File \"/usr/lib/python3.5/threading.py\", line 293, in wait\r\n    waiter.acquire()\r\nKeyboardInterrupt\r\n```\r\n", "comments": ["I have tried on colab with TF version 1.14 , 1.15 and i am not seeing any error message. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/68747e5a584d105ac7130e411721d251/untitled330.ipynb). Thanks!", "Thanks @ravikyram for creating the colab gist. Accidentally, the bug was disabled in the code snippet I posted (`if False`), but I updated it accordingly. The colab [gist](https://colab.research.google.com/gist/moberweger/1fd076e48c974019fc45d21172913b0f/untitled330.ipynb) can now reproduce the error, with slightly different message, though.", "@moberweger I tried reproducing your issue using Tensorflow 1.15 but was not able to reproduce it. Please find my gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/c7d13ee8fb33c915980d9a1845d0d3d5/untitled217.ipynb).", "Thanks @gowthamkpr Interestingly, the gist worked also for me when executed for the first time, but still I could reproduce the error it in your colab after running a few times with TF 1.14 and 1.15. I also checked TF 2.0 through the `compat` interface, and could trigger the same error, see my gist [here](https://colab.research.google.com/gist/moberweger/2553560a5deeb2eaa5e3cfc23516ef34/untitled217.ipynb) The colab execution of the code in question is not responding, so I stopped executing after 10 min, which ultimately gives me the error.", "@moberweger I can run it with TF1.15-gpu without any issues but with TF 2.0 I can see the issue is persisting. I am closing this issue but Please create a new issue specifying Tensorflow 2.0 and we can deal with it separately. Thanks!"]}, {"number": 33972, "title": "Fix the issue #32041: Fix riscv32_mcu link failure in build period.", "body": "Fix the issue https://github.com/tensorflow/tensorflow/issues/32041/\r\n\r\n1) tflite:riscv:Fix riscv32_mcu build failed with undefined references \r\n    Fix the ld error: undefined references to __wrap_puts for build commands like `make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=riscv32_mcu hello_world_bin` \r\n    The Makefile variables XXX_TEST_SRCS/XXX_SRCS in targets/mcu_riscv_makefile.inc are overridden by the the examples's respective makefile.inc (eg. hello_world/Makefile.inc), which leads to the architecture specified __wrap__funs are not included correctly.\r\n2) tflite:riscv:Fix hidden symbol `__dso_handle' isn't defined.    \r\n    For arduino sketch in of riscv_mcu examples, this patch fix this bug by declare the following global variable    `void* __dso_handle;`\r\n3) how to test:\r\n`make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=riscv32_mcu hello_world_test_bin`\r\n\r\n`make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=riscv32_mcu hello_world_bin`\r\n\r\n", "comments": ["@nkreeger\r\nPatch updated as you suggest, pls review again.", "@zhoupeng Can you please resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@zhoupeng gentle ping to resolve conflicts. Thanks!", "It has been 29 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 33971, "title": "[Lite] Support Int8 Unpack Operator", "body": "Added support for Unpack Operator\r\nAdded relevant tests.\r\n\r\nSolves issue [#31902](https://github.com/tensorflow/tensorflow/issues/31902)", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33971) for more info**.\n\n<!-- need_author_cla -->", "@MohamedNourArm please sign CLA.", "@rthadur I have the CLA agreement signed (my first commit to the PR had the cla label set to yes). I don't know why after resolving conflicts I get this error that it was \"unable to find agreements for all the commit author(s) or Co-authors,\" when I didn't add anything except resolving the conflicts. Any idea what could be causing this?", "@MohamedNourArm do you have any other contributors for this PR ? if yes they also need to sign the CLA.", "Was the change from `bazel-genfiles` to `bazel-bin` intentional? Seems like that should be part of a separate CL (if it's needed)?", "@rthadur I haven't authored any of the other commits in this PR. They were all a product of me resolving a merge conflict and merging the latest commits from master, the authors of which presumably all have the CLA. @jdduke regarding the change from `bazel-genfiles` to `bazel-bin,` this commit was also a result of me updateing the branch and was not authored by me, but says it's authored by \"TensorFlower Gardener.\"", "Hmm, my suggestion would be to rebase your commit on master and update the PR. Otherwise your commit looks fine.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33971) for more info**.\n\n<!-- ok -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33971) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33971) for more info**.\n\n<!-- ok -->", "@alanchiao can you please review new changes ?", "@alanchiao, @rthadur, @MohamedNourArm has made the proposed changes - can you please review them?", "@alanchiao , @rthadur, added the missing break statement as per Alan's comment. Can you please review again?", "@suharshs, can this be merged please?", "Working with rthadur@ to get this merged. ", "any idea which  TensorFlow version its working "]}, {"number": 33970, "title": "Force psutil version to 5.6.3 for python2", "body": "The latest (5.6.4) is broken on python2:\r\n```\r\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\r\nCollecting psutil\r\n  Using cached https://files.pythonhosted.org/packages/47/ea/d3b6d6fd0b4a6c12984df652525f394e68c8678d2b05075219144eb3a1cf/psutil-5.6.4.tar.gz\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n    Preparing wheel metadata ... error\r\n    ERROR: Command errored out with exit status 1:\r\n     command: /localdata/anthonyb/workspace/external/tf_python_python2/bin/python2 /localdata/anthonyb/workspace/external/tf_python_python2/local/lib/python2.7/site-packages/pip/_vendor/pep517/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpbqQ5kk\r\n         cwd: /tmp/pip-install-Rk4IAs/psutil\r\n    Complete output (2 lines):\r\n    running dist_info\r\n    error: 'egg_base' must be a directory name (got )\r\n```", "comments": ["Looks like the Android Demo App failure is unrelated to that change:\r\n```\r\nERROR: /tmpfs/tmp/bazel/external/com_google_protobuf/BUILD:295:1: C++ compilation of rule '@com_google_protobuf//:protoc_lib' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 40 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\ngcc: error: unrecognized command line option '-std=c++14'\r\nTarget //tensorflow/core/common_runtime/eager:execute failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/core/grappler/optimizers/BUILD:254:1 C++ compilation of rule '@com_google_protobuf//:protoc_lib' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 40 argument(s) skipped)\r\n```", "Dropping pull request: already fixed by `67d196ccac934b173e7d773269184d141321bec6`"]}, {"number": 33969, "title": "2 issues: tf.train.Checkpoint does not seem to save optimizer state, impossible to load_weights using skip_mismatch=True", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, with minor modifications from the official documentation\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): `!pip install tf-nightly-gpu`\r\n- TensorFlow version (use command below): \r\n- Python version: `v1.12.1-17291-ga2a3b22 2.1.0-dev20191103`\r\n\r\n**Describe the current behavior**\r\n\r\n1. When restoring from a tf.train.Checkpoint, training does not seem to resume from exactly where it was left off.\r\n\r\n2. It is not possible to load pretrained weights to the same model with different output dimension, even with `skip_mismatch=True`.\r\n\r\n**Code to reproduce the issue**\r\nThe two issues can be reproduced using [this Colab Notebook](https://colab.research.google.com/drive/1M0PDxTsXfL0lvFcsEhNTvNG-titdARYo).\r\n", "comments": ["Issue replicating for the given code. Thanks!", "Hi, are there any updates on this? Seeing this issue on `tf-nightly==2.2.0.dev20200119` also using model checkpoints saved via `checkpoint.save`. Checking `optimizer.iterations` after reinstantiation, it is always reset to be 0. ", "@netw0rkf10w @mathemakitten Is this still an issue? Can you please check with latest version `TF2.2` and `TF-nightly` and let us know how it perform. I tested your code with `tf-nightly`. Regarding you two issues\r\n\r\nIssue 1 : This was resolved as the loss after restoring checkpoint is smaller.\r\nIssue 2 (with tf format): loading of weights is based on topology when the model is a subclass model. This was the error \"NotImplementedError: Weights may only be loaded based on topology into Models when loading TensorFlow-formatted weights (got by_name=True to load_weights).\"\r\n\r\nIssue 2 (with h5 format): Approach should be to call model on some data to initiate the weights and then assign the weights. Currently it throws an error \"ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.\"\r\n\r\nPlease check [this](https://www.tensorflow.org/guide/checkpoint) resource for checkpointing and [this](https://www.tensorflow.org/guide/keras/save_and_serialize) is for save_load subclass models.\r\n\r\nCheck the example from the above link\r\n```\r\n# Create a simple functional model\r\ninputs = keras.Input(shape=(784,), name='digits')\r\nx = keras.layers.Dense(64, activation='relu', name='dense_1')(inputs)\r\nx = keras.layers.Dense(64, activation='relu', name='dense_2')(x)\r\noutputs = keras.layers.Dense(10, name='predictions')(x)\r\nfunctional_model = keras.Model(inputs=inputs, outputs=outputs, name='3_layer_mlp')\r\n\r\n# Define a subclassed model with the same architecture\r\nclass SubclassedModel(keras.Model):\r\n  def __init__(self, output_dim, name=None):\r\n    super(SubclassedModel, self).__init__(name=name)\r\n    self.output_dim = output_dim\r\n    self.dense_1 = keras.layers.Dense(64, activation='relu', name='dense_1')\r\n    self.dense_2 = keras.layers.Dense(64, activation='relu', name='dense_2')\r\n    self.dense_3 = keras.layers.Dense(output_dim, name='predictions')\r\n  def call(self, inputs):\r\n    x = self.dense_1(inputs)\r\n    x = self.dense_2(x)\r\n    x = self.dense_3(x)\r\n    return x\r\n  def get_config(self):\r\n    return {'output_dim': self.output_dim, 'name': self.name}\r\n\r\nsubclassed_model = SubclassedModel(10)\r\n# Call the subclassed model once to create the weights.\r\nsubclassed_model(tf.ones((1, 784)))\r\n\r\n# Copy weights from functional_model to subclassed_model.\r\nsubclassed_model.set_weights(functional_model.get_weights())\r\n```\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Mine has been resolved, running on TF2.2. Thanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33969\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33969\">No</a>\n"]}, {"number": 33968, "title": "Can not compile the tensorflow lite example", "body": "When I use the command \"make -f tensorflow/lite/experimental/micro/tools/make/Makefile micro_speech\", it shows the error message below:\r\n**./tensorflow/lite/experimental/micro/kernels/activation_utils.h:43:23: error: \u2018signbit\u2019 was not declared in this scope \r\n    return signbit(a);**\r\nI am sure I had build this example successfully few days ago(I built it on Ubuntu 16.04 x64). Is there anything changed for this example this week? Thanks.\r\n", "comments": ["@kop57890, Just to verify, did you follow this [link](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#micro-speech-example). Thanks!", "Yes,I did. Or can't this example able to run on the Ubuntu 16.04? Thanks!", "Hi, \r\nI comment \"case kTfLiteActSignBit: return signbit(a);\" and then execute the following command \"make -f tensorflow/lite/experimental/micro/tools/make/Makefile micro_speech\"\r\nIt successfully compiled\r\nWhen I run micro_speech on Ubuntu 16.04 , It does not work(As shown below)\r\n![1](https://user-images.githubusercontent.com/41892320/68205749-fa284100-0005-11ea-8267-7c7defefc4c9.jpg)\r\nSo I use gdb to debug(As shown below)\r\n![2](https://user-images.githubusercontent.com/41892320/68205809-26dc5880-0006-11ea-9ff8-6c33fe0f19ac.jpg)\r\nDoes micro_speech need to run on a specific version? Thanks\r\n", "I got the same error building tensorflow lite micro:\r\n./tensorflow/lite/experimental/micro/kernels/activation_utils.h: In function \u2018float tflite::ops::micro::ActivationValFloat(TfLiteFusedActivation, float)\u2019:\r\n./tensorflow/lite/experimental/micro/kernels/activation_utils.h:43:23: error: \u2018signbit\u2019 was not declared in this scope\r\n       return signbit(a);\r\n                       ^\r\n./tensorflow/lite/experimental/micro/kernels/activation_utils.h:43:23: note: suggested alternative:\r\nIn file included from /usr/include/c++/5/random:38:0,\r\n                 from /usr/include/c++/5/bits/stl_algo.h:66,\r\n                 from /usr/include/c++/5/algorithm:62,\r\n                 from ./tensorflow/lite/experimental/micro/kernels/activation_utils.h:19,\r\n                 from tensorflow/lite/experimental/micro/kernels/svdf.cc:20:\r\n/usr/include/c++/5/cmath:682:5: note:   \u2018std::signbit\u2019\r\n     signbit(_Tp __x)\r\n     ^\r\ntensorflow/lite/experimental/micro/tools/make/Makefile:250: recipe for target 'tensorflow/lite/experimental/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/experimental/micro/kernels/svdf.o' failed\r\nmake: *** [tensorflow/lite/experimental/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/experimental/micro/kernels/svdf.o] Error 1\r\n", "For the benefit of someone else running into this error, I fixed the problem by adding a line \"using namespace std;\" in the file tensorflow/lite/experimental/micro/kernels/svdf.cc.\r\n\r\n#include <math.h>\r\n**using namespace std;**\r\n\r\n#include \"tensorflow/lite/c/builtin_op_data.h\"\r\n#include \"tensorflow/lite/c/c_api_internal.h\"\r\n#include \"tensorflow/lite/experimental/micro/kernels/activation_utils.h\"\r\n#include \"tensorflow/lite/experimental/micro/micro_utils.h\"\r\n\r\n", "It looks like this has been fixed by changing signbit to std::signbit.  I verified building with the same command locally. Closing for now - please re-open if this remains an issue for you."]}, {"number": 33967, "title": "Cannot use dictionary embeddings metadata for keras callbacks", "body": "Using dictionary as embeddings_metadata does not seem to be handled correctly in the Keras callbacks v2.\r\nIndeed, while trying to do so I get the following error message:\r\n```ValueError: Unrecognized `Embedding` layer names passed to `keras.callbacks.TensorBoard` `embeddings_metadata` argument: dict_keys(['embedding_1', 'embedding_2'])```\r\nAfter looking at the source code (see [here](https://github.com/tensorflow/tensorflow/blob/32d76ec3e633e0ef5c980b5a91a510bb71d668ed/tensorflow/python/keras/callbacks.py#L1575)), I figured that, when the metadata is not a string, it checked if the layer names was in `embedding.metadata_path`. However, this variable is never set in this case and it is an empty string by default.\r\n\r\nLooking at how it is handled in the [v1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks_v1.py), I think the check should be done on `self.embeddings_metadata` (assuming if it is not a string it is a dictionary). So\r\n```if layer.name in embedding.metadata_path:```\r\nshould be something like\r\n```if layer.name in self.embeddings_metadata:```\r\nMaybe it should be a copy of this dictionary or there should be a test to check that this is indeed a dictionary.", "comments": ["@ouakif, Will it be possible to share the sample standalone code reproduce the reported issue. Thanks!", "I tried to run this [example](https://keras.io/examples/tensorboard_embeddings_mnist/) from the Keras website.\r\nYou can run it on a [Colab](https://colab.research.google.com/) adding ` %tensorflow_version 2.x` on the first line.\r\n``` python\r\nfrom os import makedirs\r\nfrom os.path import exists, join\r\n\r\nfrom tensorflow.keras.callbacks import TensorBoard\r\nfrom tensorflow.keras.datasets import mnist\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\r\nfrom tensorflow.keras import backend as K\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nbatch_size = 128\r\nnum_classes = 10\r\nepochs = 12\r\nlog_dir = './logs'\r\n\r\nif not exists(log_dir):\r\n    makedirs(log_dir)\r\n\r\n# input image dimensions\r\nimg_rows, img_cols = 28, 28\r\n\r\n# the data, split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\nif K.image_data_format() == 'channels_first':\r\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n    input_shape = (1, img_rows, img_cols)\r\nelse:\r\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n    input_shape = (img_rows, img_cols, 1)\r\n\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\n\r\n# save class labels to disk to color data points in TensorBoard accordingly\r\nwith open(join(log_dir, 'metadata.tsv'), 'w') as f:\r\n    np.savetxt(f, y_test)\r\n\r\n# convert class vectors to binary class matrices\r\ny_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\ny_test = tf.keras.utils.to_categorical(y_test, num_classes)\r\n\r\ntensorboard = TensorBoard(batch_size=batch_size,\r\n                          embeddings_freq=1,\r\n                          # This does not work\r\n                          embeddings_metadata={'features': 'metadata.tsv'}\r\n                          # I also tested this and does not work either\r\n                          # embeddings_metadata='metadata.tsv'\r\n                          )\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, kernel_size=(3, 3),\r\n                 activation='relu',\r\n                 input_shape=input_shape))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128, activation='relu', name='features'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes, activation='softmax'))\r\n\r\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\r\n              optimizer=tf.keras.optimizers.Adadelta(),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          callbacks=[tensorboard],\r\n          epochs=epochs,\r\n          verbose=1,\r\n          validation_data=(x_test, y_test))\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\nprint('Test loss:', score[0])\r\nprint('Test accuracy:', score[1])\r\n```", "I could reproduce the issue on colab with Tf 2.0. \r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/ee2bfba3b871578892def0b1742308ac/untitled240.ipynb). Thanks!", "@ouakif \r\n`embeddings_layer_names` is not supported in TensorFlow 2.0. Instead, all Embedding layers will be visualized.\r\n\r\nFor more info you can take a look at the source code [here](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/callbacks.py#L1475)\r\n\r\nThanks!", "Thank you for your answer.\r\nHowever, here I am trying to set `embeddings_metadata` which should be supported (see [here](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/callbacks.py#L1406))\r\nIn fact, one of the issues is that `_configure_embeddings` is trying to check if the layer.name is in `embedding.metadata_path` while this variable is never filled (see [here](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/callbacks.py#L1531)).", "Here are the changes that I would suggest. Not yet tested with the nightly version https://github.com/tensorflow/tensorflow/compare/master...ouakif:correct_embeddings_metadata?expand=1", "> Using dictionary as embeddings_metadata does not seem to be handled correctly in the Keras callbacks v2.\r\n> Indeed, while trying to do so I get the following error message:\r\n> `` ValueError: Unrecognized `Embedding` layer names passed to `keras.callbacks.TensorBoard` `embeddings_metadata` argument: dict_keys(['embedding_1', 'embedding_2']) ``\r\n> After looking at the source code (see [here](https://github.com/tensorflow/tensorflow/blob/32d76ec3e633e0ef5c980b5a91a510bb71d668ed/tensorflow/python/keras/callbacks.py#L1575)), I figured that, when the metadata is not a string, it checked if the layer names was in `embedding.metadata_path`. However, this variable is never set in this case and it is an empty string by default.\r\n> \r\n> Looking at how it is handled in the [v1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks_v1.py), I think the check should be done on `self.embeddings_metadata` (assuming if it is not a string it is a dictionary). So\r\n> `if layer.name in embedding.metadata_path:`\r\n> should be something like\r\n> `if layer.name in self.embeddings_metadata:`\r\n> Maybe it should be a copy of this dictionary or there should be a test to check that this is indeed a dictionary.\r\n\r\nHi\r\ncould you tell me the metadata format?  Particularly for word of bag, is it the word files?\r\nlike this \r\n```\r\nwith open('./logs/text_classify/word.tsv','w') as f:\r\n    for i in range(len(word2id)):\r\n        f.write('{}'.format(id2word[i]) + '\\n')\r\n```", "Has this issue been solved?\r\n\r\nI tried running the Keras example mentioned by @ouakif on Colab using TensorFlow version 2.2.0-rc3. It did not work. The following error is given:\r\n\r\n    ValueError: Unrecognized `Embedding` layer names passed to `keras.callbacks.TensorBoard` `embeddings_metadata` argument: dict_keys(['features'])", "+1 on this issue", "I ran into this error while trying to visualize the word embeddings in Tensorboard Projector (as opposed to the default word IDs). Code to reproduce below. Using method 1 in my example, I can load the metadata.tsv file manually in Projector to see the words, but I'd like to load these automatically using the `embeddings_metadata` argument.\r\n\r\nThe [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) says you can provide a string with the file location/name that would apply to the Embedding layer, but from method 2 in my example, I get `AttributeError: 'str' object has no attribute 'keys'`. When I pass a dictionary as seen in method 3, I get `ValueError: Unrecognized Embedding layer names passed to keras.callbacks.TensorBoard embeddings_metadata argument: dict_keys(['embedding'])`\r\n\r\nKeras 2.3.0-tf\r\nTensorflow 2.2.0\r\nPython 3.7.7\r\nmacOS 10.15.4\r\n```python\r\nimport csv\r\nimport os\r\nfrom tensorflow import keras\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport time\r\n\r\ndatasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\r\n\r\nwords = ['zero', 'one', 'two', 'three', 'four']\r\nword_ids = list(range(5))\r\nvocab_init = tf.lookup.KeyValueTensorInitializer(words, np.array(word_ids, dtype=np.int64))\r\ntable = tf.lookup.StaticVocabularyTable(vocab_init, 1)\r\n\r\ndef preprocess(X_batch, y_batch):\r\n    X_batch = tf.strings.substr(X_batch, 0, 300)\r\n    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \")\r\n    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\r\n    X_batch = tf.strings.split(X_batch)\r\n    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch\r\n\r\ndef encode_words(X_batch, y_batch):\r\n    return table.lookup(X_batch), y_batch\r\n\r\ntrain_set = datasets['train'].batch(32).map(preprocess)\r\ntrain_set = train_set.map(encode_words).prefetch(1)\r\n\r\n# save words as metadata to include in tensorboard callback\r\nwith open('metadata.tsv', 'w') as f:\r\n    writer = csv.writer(f, delimiter='\\t')\r\n    writer.writerows(zip(['id'], ['word']))\r\n    writer.writerows(zip(word_ids, words))\r\n    writer.writerows(zip([5], ['oov']))\r\n\r\nroot_logdir = os.path.join(os.curdir, 'my_logs')\r\n\r\ndef get_run_logdir():\r\n    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\r\n    return os.path.join(root_logdir, run_id)\r\n\r\nrun_logdir = get_run_logdir()\r\n\r\nembed_size = 6\r\nmodel = keras.models.Sequential([\r\n    keras.layers.Embedding(6, embed_size, input_shape=[None], name='embedding'),\r\n    keras.layers.GRU(6, return_sequences=True),\r\n    keras.layers.GRU(6),\r\n    keras.layers.Dense(1, activation='sigmoid')\r\n])\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\n# three methods tried:\r\n# 1. shows word_ids only in tensorboard projector\r\n#tensorboard_cb = keras.callbacks.TensorBoard(run_logdir, embeddings_freq=1)\r\n# 2. str obj has not attribute keys\r\n#tensorboard_cb = keras.callbacks.TensorBoard(run_logdir, embeddings_freq=1, embeddings_metadata='metadata.tsv')\r\n# 3. unrecognized embedding layer name\r\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir, embeddings_freq=1, embeddings_metadata={'embedding': 'metadata.tsv'})\r\n\r\nhistory = model.fit(train_set, epochs=1, callbacks=[tensorboard_cb])\r\n```", "@rchao would love to get your feedback on this issue", "Was able to reproduce the issue in TF v2.5 ,please check the gist [here](https://colab.research.google.com/gist/sushreebarsa/2f6cb875530a158f65ecd8f9549b6a4a/untitled25.ipynb)..Thanks !", "Was able to replicate the issue with TF 2.6.0-dev20210606,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/28bad14c285ba2748060f095e42d3f3d/untitled242.ipynb) ..Thanks", "@ouakif \r\nPlease post this issue on keras-team/keras repo.\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "@ouakif ,\r\nAs commented above please feel free to close this issue here and post in Keras repo.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33967\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33967\">No</a>\n"]}, {"number": 33966, "title": "Any way to generate offline documents for TF2.0 in HTML/pdf/markdown?", "body": "Since we have network issues visiting official tensorflow document website within our area, are there any clues for generating offline api-docs with the document repository?", "comments": ["Hello.\r\n\r\nUsually we generate the API reference Markdown in the tensorflow/docs repo. It should all be readable by browsing GitHub, just select the branch: https://github.com/tensorflow/docs/tree/r1.14/site/en/api_docs\r\nBut we're a bit behind for this last release. @MarkDaoust, can you generate the latest-greatest, please? (b/142128205)\r\n\r\nThe rest of the site docs are in the tensorflow/docs [site/en directory](https://github.com/tensorflow/docs/tree/master/site/en). Many are Jupyter notebooks and the HTML is only available through tensorflow.org.", "Sure, it helps. Thanks.", "> But we're a bit behind for this last release. @MarkDaoust, can you generate the latest-greatest, please? (b/142128205)\r\n\r\nYes.\r\n\r\n> Many are Jupyter notebooks and the HTML is only available through tensorflow.org.\r\n\r\nYou can convert them with `jupyter nbconvert` but if you have jupyter installed you can just use the notebook interface."]}, {"number": 33965, "title": "Fixed cpu frequency retrieval and clock cycle for IBM Power systems", "body": "", "comments": ["Thank you @penpornk for the comments and sorry for the delay as I was occupied in other tasks. I've addressed all your suggestions.", "@penpornk Thank you for reviewing and merging this."]}, {"number": 33964, "title": "[tflite] expose more NNAPI Delegate to Java", "body": "1. use StatefulNnApiDelegate instead of the deprecated `tflite::NnApiDelegate()`, see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/nnapi/nnapi_delegate.h#L173-L181)\r\n2. make it possible to specify NNAPI [execution preference](https://developer.android.com/ndk/reference/group/neural-networks#preferencecode) in Java.", "comments": ["This is failing some internal tests, stay tuned.", "@freedomtan Could you please check reviewer comments and keep us posted. Thanks!", "@gbaned I did that already."]}, {"number": 33963, "title": "init_from_checkpoint support loading different variables from multiple checkpoints", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**For example**\r\nsay I created a model with encoder + decoder, could `init_from_checkpoint` support loading encoder from A-checkpoint, and loading decoder from B-checkpoint?\r\n```python\r\ninit_from_checkpoint(init_checkpoint_0, assignment_map_0)\r\ninit_from_checkpoint(init_checkpoint_1, assignment_map_0)\r\n```\r\n\r\n**System information**\r\n- TensorFlow version (you are using): version 1.12\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?** probably not\r\n\r\n**Who will benefit with this feature?** developers who need more flexible graph loading\uff0c who need more flexible transfer leaning tricks\r\n\r\n**Any Other info.**\r\npip install \r\non both windows and linux\r\n", "comments": ["\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n", "@congchan \r\nAny updates on this issue please. Thanks!", "> @congchan\r\n> Any updates on this issue please. Thanks!\r\n\r\n<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**For example**\r\nsay I created a model with encoder + decoder, could `init_from_checkpoint` support loading encoder from A-checkpoint, and loading decoder from B-checkpoint?\r\n```python\r\ninit_from_checkpoint(init_checkpoint_0, assignment_map_0)\r\ninit_from_checkpoint(init_checkpoint_1, assignment_map_0)\r\n```\r\n\r\n**System information**\r\n- TensorFlow version (you are using): version >1.12\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?** probably not\r\n\r\n**Who will benefit with this feature?** developers who need more flexible graph loading\uff0c who need more flexible transfer leaning tricks\r\n\r\n**Any Other info.**\r\npip install \r\non both windows and linux\r\n", "@congchan \r\n\r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "@ravikyram  what it means for PR of use cases? is it a model ? a training scripts? or an implementation?", "@congchan \r\nIs it possible to elaborate the reported feature request with a sample code ? or provide more information about feature request.Thanks!", "OK, take the code snippet from https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/run_classifier.py#L661\r\nfor examples:\r\nis it possible to load a bert from the pre-trained bert ckp, and follow a bi-lstm layer loaded from another pre-trained ckp?\r\n\r\n", "You should already be able to use multiple `init_from_checkpoint` calls to different checkpoints.\r\n\r\nIn the [API documentation](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/init_from_checkpoint), there are several examples of how to use init_from_checkpoint. You can either initialize all variables, or specific variables."]}]