[{"number": 45666, "title": "micro: fix build warning due to example person_detection", "body": "`make` was warning because the recipe for target person_model_int8\r\nwas being defined twice, once in the main Makefile at the\r\nlocation given in the warning:\r\n\r\n    tensorflow/lite/micro/tools/make/Makefile:583:\r\n      warning: overriding recipe for target\r\n      'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\n    tensorflow/lite/micro/tools/make/Makefile:583:\r\n      warning: ignoring old recipe for target\r\n      'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\n\r\nand once in the person_detection example's include Makefile, which is\r\nincluded by the main Makefile.\r\n\r\nThese warnings were causing annoying, false-positives while\r\nbrowsing complier errors via my editor following a build.\r\n\r\nTo fix, remove the re-definition in the example's include Makefile.\r\n\r\nThis fixes #45665.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45665, "title": "micro: make build warnings cause false-positives while browsing compiler errors via editor", "body": "@tensorflow/micro\r\n\r\nmake warnings regarding the recipe for person_model_int8 during the micro build are cause annoying, false-positives while browsing complier errors via my editor following a build. E.g.:\r\n\r\n    % make -f tensorflow/lite/micro/tools/make/Makefile test\r\n    tensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.\r\n    tensorflow/lite/micro/tools/make/Makefile:581: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\n    tensorflow/lite/micro/tools/make/Makefile:581: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\n    g++ -std=c++11 [....]\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: Linux Debian bullseye\r\n- GNU Make 4.3\r\n- TensorFlow installed from: source\r\n- Tensorflow version: master at fbcb024d\r\n- Target platform: `make -f tensorflow/lite/micro/tools/make/Makefile test`", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45665\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45665\">No</a>\n"]}, {"number": 45662, "title": "Wrong device returned for GPUCompatibleFIFOQueueTests.testEnqueueDequeue test", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0rc4\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): GCC 8.3.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: V100\r\n\r\n**Describe the current behavior**\r\nIn the //tensorflow/python/kernel_tests:fifo_queue_test target the GPUCompatibleFIFOQueueTests.testEnqueueDequeue returns the wrong device: ` /job:localhost/replica:0/task:0/device:CPU:0 vs /job:localhost/replica:0/task:0/device:GPU:0`\r\n\r\nSee below log\r\n\r\n**Standalone code to reproduce the issue**\r\nRun bazel test\r\n\r\n**Other info / logs** \r\n\r\n```\r\nFAIL: testEnqueueDequeue (__main__.GPUCompatibleFIFOQueueTests)\r\nGPUCompatibleFIFOQueueTests.testEnqueueDequeue\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/fifo_queue_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1221, in decorated\r\n    run_eagerly(self, **kwargs)\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/fifo_queue_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1205, in run_eagerly\r\n    f(self, *args, **kwargs)\r\n  File \"/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/fifo_queue_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/fifo_queue_test.py\", line 425, in testEnqueueDequeue\r\n    self.assertEqual(elems[0].device, dequeued_tensor.device)\r\nAssertionError: \r\n- /job:localhost/replica:0/task:0/device:CPU:0\r\n?                                        ^\r\n+ /job:localhost/replica:0/task:0/device:GPU:0\r\n?                                        ^\r\n\r\n```", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45662\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45662\">No</a>\n"]}, {"number": 45658, "title": " Error during training: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): installed from conda\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: Python 3.7.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda64_101, cudnn64_7\r\n- GPU model and memory: GeForce RTX 2080 Super, 8 GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nDuring training neural network on 17th epoch I faced error:\r\n\r\n```F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1```\r\n\r\nI tried rerun many times and every time failed epoch number of training was different.\r\n\r\n**Describe the expected behavior**\r\n\r\nI think training should be stable.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nI deployed this repo: https://github.com/arthurflor23/handwritten-text-recognition\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThis code:\r\n```python\r\nimport tensorflow as tf\r\n\r\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\r\n```\r\n\r\ngives me:\r\n\r\n```\r\n2020-12-14 19:14:24.943891: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-14 19:14:26.611932: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-12-14 19:14:26.614457: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-12-14 19:14:26.644016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Super computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 48 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-14 19:14:26.644168: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-14 19:14:26.647233: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-12-14 19:14:26.649912: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-14 19:14:26.651042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-12-14 19:14:26.654689: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-14 19:14:26.656359: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-12-14 19:14:26.662690: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-14 19:14:26.662820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-12-14 19:14:27.098083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-14 19:14:27.098175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-12-14 19:14:27.098252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-12-14 19:14:27.098438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6265 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Super, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-12-14 19:14:27.101774: I tensorflow/core/common_runtime/direct_session.cc:358] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2080 Super, pci bus id: 0000:01:00.0, compute capability: 7.5\r\n```\r\n\r\n```\r\n(tf_gpu) D:\\repositories\\handwritten-text-recognition\\src>python main.py --source=bentham --train\r\nWeights are from ..\\output\\bentham\\flor\\checkpoint_weights.hdf5\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput (InputLayer)           [(None, 1024, 128, 1)]    0\r\n_________________________________________________________________\r\nconv2d (Conv2D)              (None, 1024, 64, 16)      160\r\n_________________________________________________________________\r\np_re_lu (PReLU)              (None, 1024, 64, 16)      16\r\n_________________________________________________________________\r\nbatch_normalization (BatchNo (None, 1024, 64, 16)      112\r\n_________________________________________________________________\r\nfull_gated_conv2d (FullGated (None, 1024, 64, 16)      4640\r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 1024, 64, 32)      4640\r\n_________________________________________________________________\r\np_re_lu_1 (PReLU)            (None, 1024, 64, 32)      32\r\n_________________________________________________________________\r\nbatch_normalization_1 (Batch (None, 1024, 64, 32)      224\r\n_________________________________________________________________\r\nfull_gated_conv2d_1 (FullGat (None, 1024, 64, 32)      18496\r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 512, 16, 40)       10280\r\n_________________________________________________________________\r\np_re_lu_2 (PReLU)            (None, 512, 16, 40)       40\r\n_________________________________________________________________\r\nbatch_normalization_2 (Batch (None, 512, 16, 40)       280\r\n_________________________________________________________________\r\nfull_gated_conv2d_2 (FullGat (None, 512, 16, 40)       28880\r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 512, 16, 40)       0\r\n_________________________________________________________________\r\nconv2d_3 (Conv2D)            (None, 512, 16, 48)       17328\r\n_________________________________________________________________\r\np_re_lu_3 (PReLU)            (None, 512, 16, 48)       48\r\n_________________________________________________________________\r\nbatch_normalization_3 (Batch (None, 512, 16, 48)       336\r\n_________________________________________________________________\r\nfull_gated_conv2d_3 (FullGat (None, 512, 16, 48)       41568\r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 512, 16, 48)       0\r\n_________________________________________________________________\r\nconv2d_4 (Conv2D)            (None, 256, 4, 56)        21560\r\n_________________________________________________________________\r\np_re_lu_4 (PReLU)            (None, 256, 4, 56)        56\r\n_________________________________________________________________\r\nbatch_normalization_4 (Batch (None, 256, 4, 56)        392\r\n_________________________________________________________________\r\nfull_gated_conv2d_4 (FullGat (None, 256, 4, 56)        56560\r\n_________________________________________________________________\r\ndropout_2 (Dropout)          (None, 256, 4, 56)        0\r\n_________________________________________________________________\r\nconv2d_5 (Conv2D)            (None, 256, 4, 64)        32320\r\n_________________________________________________________________\r\np_re_lu_5 (PReLU)            (None, 256, 4, 64)        64\r\n_________________________________________________________________\r\nbatch_normalization_5 (Batch (None, 256, 4, 64)        448\r\n_________________________________________________________________\r\nreshape (Reshape)            (None, 256, 256)          0\r\n_________________________________________________________________\r\nbidirectional (Bidirectional (None, 256, 256)          296448\r\n_________________________________________________________________\r\ndense (Dense)                (None, 256, 256)          65792\r\n_________________________________________________________________\r\nbidirectional_1 (Bidirection (None, 256, 256)          296448\r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 256, 98)           25186\r\n=================================================================\r\nTotal params: 922,354\r\nTrainable params: 921,074\r\nNon-trainable params: 1,280\r\n_________________________________________________________________\r\nTrain for 1101 steps, validate for 172 steps\r\nEpoch 1/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 19.5034\r\nEpoch 00001: val_loss improved from inf to 18.13556, saving model to ..\\output\\bentham\\flor\\checkpoint_weights.hdf5\r\n1101/1101 [==============================] - 150s 136ms/step - loss: 19.5006 - val_loss: 18.1356\r\nEpoch 2/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 18.7811\r\nEpoch 00002: val_loss did not improve from 18.13556\r\n1101/1101 [==============================] - 140s 127ms/step - loss: 18.7732 - val_loss: 18.9815\r\nEpoch 3/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 17.4834\r\nEpoch 00003: val_loss did not improve from 18.13556\r\n1101/1101 [==============================] - 140s 127ms/step - loss: 17.4750 - val_loss: 18.3697\r\nEpoch 4/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 16.9503\r\nEpoch 00004: val_loss improved from 18.13556 to 17.28087, saving model to ..\\output\\bentham\\flor\\checkpoint_weights.hdf5\r\n1101/1101 [==============================] - 140s 127ms/step - loss: 16.9409 - val_loss: 17.2809\r\nEpoch 5/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 16.1360\r\nEpoch 00005: val_loss improved from 17.28087 to 16.63544, saving model to ..\\output\\bentham\\flor\\checkpoint_weights.hdf5\r\n1101/1101 [==============================] - 139s 126ms/step - loss: 16.1276 - val_loss: 16.6354\r\nEpoch 6/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 15.7264\r\nEpoch 00006: val_loss improved from 16.63544 to 16.15779, saving model to ..\\output\\bentham\\flor\\checkpoint_weights.hdf5\r\n1101/1101 [==============================] - 140s 128ms/step - loss: 15.7176 - val_loss: 16.1578\r\nEpoch 7/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 15.0694\r\nEpoch 00007: val_loss improved from 16.15779 to 15.39602, saving model to ..\\output\\bentham\\flor\\checkpoint_weights.hdf5\r\n1101/1101 [==============================] - 139s 127ms/step - loss: 15.0607 - val_loss: 15.3960\r\nEpoch 8/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 14.6364\r\nEpoch 00008: val_loss improved from 15.39602 to 15.06812, saving model to ..\\output\\bentham\\flor\\checkpoint_weights.hdf5\r\n1101/1101 [==============================] - 139s 126ms/step - loss: 14.6277 - val_loss: 15.0681\r\nEpoch 9/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 14.4449\r\nEpoch 00009: val_loss improved from 15.06812 to 15.01459, saving model to ..\\output\\bentham\\flor\\checkpoint_weights.hdf5\r\n1101/1101 [==============================] - 139s 127ms/step - loss: 14.4367 - val_loss: 15.0146\r\nEpoch 10/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 14.1694\r\nEpoch 00010: val_loss improved from 15.01459 to 14.35110, saving model to ..\\output\\bentham\\flor\\checkpoint_weights.hdf5\r\n1101/1101 [==============================] - 139s 127ms/step - loss: 14.1645 - val_loss: 14.3511\r\nEpoch 11/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 13.7056\r\nEpoch 00011: val_loss improved from 14.35110 to 13.85971, saving model to ..\\output\\bentham\\flor\\checkpoint_weights.hdf5\r\n1101/1101 [==============================] - 139s 126ms/step - loss: 13.6979 - val_loss: 13.8597\r\nEpoch 12/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 13.3614\r\nEpoch 00012: val_loss did not improve from 13.85971\r\n1101/1101 [==============================] - 140s 127ms/step - loss: 13.3553 - val_loss: 13.9131\r\nEpoch 13/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 13.0623\r\nEpoch 00013: val_loss improved from 13.85971 to 13.21627, saving model to ..\\output\\bentham\\flor\\checkpoint_weights.hdf5\r\n1101/1101 [==============================] - 140s 127ms/step - loss: 13.0562 - val_loss: 13.2163\r\nEpoch 14/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 12.9299\r\nEpoch 00014: val_loss did not improve from 13.21627\r\n1101/1101 [==============================] - 141s 128ms/step - loss: 12.9227 - val_loss: 13.3021\r\nEpoch 15/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 12.6765\r\nEpoch 00015: val_loss improved from 13.21627 to 13.18161, saving model to ..\\output\\bentham\\flor\\checkpoint_weights.hdf5\r\n1101/1101 [==============================] - 141s 128ms/step - loss: 12.6724 - val_loss: 13.1816\r\nEpoch 16/1000\r\n1100/1101 [============================>.] - ETA: 0s - loss: 12.4314\r\nEpoch 00016: val_loss improved from 13.18161 to 13.12220, saving model to ..\\output\\bentham\\flor\\checkpoint_weights.hdf5\r\n1101/1101 [==============================] - 142s 129ms/step - loss: 12.4244 - val_loss: 13.1222\r\nEpoch 17/1000\r\n 130/1101 [==>...........................] - ETA: 1:56 - loss: 10.35982020-12-14 17:54:37.809588: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n```\r\n", "comments": ["@sergorl,\r\nLooking at similar issues [#33536](https://github.com/tensorflow/tensorflow/issues/33536#issuecomment-544509220) and [#43914](https://github.com/tensorflow/tensorflow/issues/43914#issuecomment-706627108), seems like the error is caused due to the NVIDIA drivers. \r\n\r\nCould you please update TensorFlow, CUDA, cuDNN and the NVIDIA drivers to the latest version as per the [installation guide](https://www.tensorflow.org/install/gpu#windows_setup) and check if you are facing the same issue. Thanks!", "@amahendrakar, I did like you said: I updated cuda, cudnn, gpu driver. But now I see all gets stuck: there is no cuda activity, but memory consuming happens, training freezes on first iteration.\r\n\r\n\r\n![error](https://user-images.githubusercontent.com/15359221/102215765-386f7f80-3eeb-11eb-82cc-865ca26bb213.png)\r\n", "> I did like you said: I updated cuda, cudnn, gpu driver\r\n\r\n@sergorl,\r\nCould you please specify the TensorFlow, CUDA and cuDNN version you are using now? \r\n\r\nAlso, please run the below code snippet and share the output with us. \r\n```\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n\r\n```\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45658\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45658\">No</a>\n", "Hi @sergorl,\r\nI've been facing a similar kind of issue which happens randomly, I was wondering whether you were able to fix the problem?\r\n\r\nThanks,\r\nAnidh Singh"]}, {"number": 45657, "title": "micro: port op FLOOR_DIV from lite", "body": "\r\n@tensorflow/micro\r\n\r\nThis issue tracks my work porting operator FLOOR_DIV from lite to micro.\r\n\r\nThe port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:\r\n\r\nPR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver\r\nPR 2: Extract the reference implementation out of tensorflow/lite/kernels/floor_div.cc into its own header which can be included without dragging in reference_ops.h's dependences\r\nPR 3: Copy operator from lite to micro without making any changes or including in the build\r\nPR 4: Delete extra code from the micro copy of the operator\r\nPR 5: Port micro copy of operator as necessary and add a corresponding test", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45657\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45657\">No</a>\n"]}, {"number": 45656, "title": "sum/avg/mean ops of tf.keras.backend do not work correctly over batch axis", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1 (docker)\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1.243/7.6.4.38-1\r\n- GPU model and memory: 1080-TI\r\n\r\n**Describe the current behavior**\r\nWhen adding an aggregation function on the batch axis (axis=0), everything is fine, as long as the batchsize is <=32, with batchsize >32 the output shape changes.\r\n\r\nCode and test tp reproduce on:\r\nhttps://stackoverflow.com/questions/64893958/sum-avg-mean-ops-of-tf-keras-backend-do-not-work-correctly-over-batch-axis\r\n\r\n**Describe the expected behavior**\r\noutput shape is the expected on from model summary and not depending on the batchsize.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nsee https://stackoverflow.com/questions/64893958/sum-avg-mean-ops-of-tf-keras-backend-do-not-work-correctly-over-batch-axis\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue with TF v2.2, TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/9cc299da89b22d5aeeea92a97b5cd7b3/45656.ipynb). Thanks!", "@joba01,\r\nUsing **`Relu Activation Layer`** instead of **`keras.backend.max`** gives a reliable `Output` and IMO is recommended, because even the Documentation for [tf.keras.backend.max](https://www.tensorflow.org/api_docs/python/tf/keras/backend/max?hl=FA) is not available in the Tensorflow site, as it is disabled in the [Source Code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2272).\r\n\r\nPlease find [the Gist](https://colab.research.google.com/gist/rmothukuru/bb2e9b3747622c445e1ec8588fd1a6dd/45656.ipynb#scrollTo=u1w8q9D1Bzrq) of working code. \r\n\r\nAlso, if you want to use an Activation Function with Customized Functionality, please refer this [Stack Overflow Answer](https://stackoverflow.com/a/43915483/11530462). Thanks!", "thanks, but as in the title this is the same for sum/avg/mean. Also Relu is not the same as max. The function is also not deprecated in your linked source.", "Thanks for reporting this issue! \r\n\r\nThis error is caused by model.predict(). Basically inside this method your data is split into several epochs by the first axis (the batch axis), and the processed results of each epoch are concatenated together as the final output. This is based on the assumption that all operations in keras.model or keras.layer should not touch the batch axis. For example, suppose we have input data of size (63, 2), when calling predict(), the data is handled as two frames of size (32, 2) and (31, 2), then their outputs are concatenated, which makes the output value of shape (4, ).\r\n\r\nOne workaround is to use \\_\\_call\\_\\_() method instead of predict(). In your example, instead of calling\r\n```\r\nx = tf.random.normal((orders ,parameters))\r\nresult = model.predict(x)\r\nprint(result.shape)\r\n```\r\nuse this\r\n```\r\nx = tf.random.normal((orders ,parameters))\r\nresult = model(x)\r\nprint(result.shape)\r\n```", "@chenmoneygithub, thanks a lot for that clarification. I have read that input and output of a model must have the same batchsize/axis-0, but still couldn't explain this behaviour.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45656\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45656\">No</a>\n"]}, {"number": 45655, "title": "Use a raw string for docstring with TeX", "body": "like [this](https://github.com/tensorflow/tensorflow/commit/868a90b9eb8c640350ebf921df0103851ea25d06).\r\nCurrently the formula in this document is broken.\r\n\r\n![image](https://user-images.githubusercontent.com/8027142/102102199-e12bc980-3e6e-11eb-80cb-8ca0850e67ee.png)\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45655) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 45654, "title": "fix asan check in c++ gradients", "body": "@saxenasaurabh \r\n\r\nLet's see if this PR fixes the problems with asan. There are some more leaks indeed, but I couldn't find a fix for them and they seem related to other components of the code. Here is the list:\r\n \r\n- `tensorflow::BuildImmediateExecutionContext`\r\n- `tensorflow::gradients::Tape::ComputeGradient`\r\n- `tensorflow::EagerContext::CreateOperation`\r\n- `tensorflow::gradients::TapeOperation::Execute`", "comments": ["Thanks for finding and fixing this. Would you mind describing / documenting how you are debugging mem leaks. It may be useful for future contributors.\r\n\r\n> Could you please elaborate what does this args mean ? Maybe it is related to the leaks above and I could do something. Thank you !\r\n\r\nThis basically says do not perform heap leak checking for the entire program but only for annotated code blocks (using an internal API). Since there are no annotated code-blocks in this test this basically skips checking for leaks. We can remove this once the leaks are gone.", "> Thanks for finding and fixing this. Would you mind describing / documenting how you are debugging mem leaks. It may be useful for future contributors.\r\n\r\nI am not sure where the documenting should go. I think it is best if we could add this [option](https://stackoverflow.com/a/57733619) to our `.bazelrc` file.", "> > Thanks for finding and fixing this. Would you mind describing / documenting how you are debugging mem leaks. It may be useful for future contributors.\r\n> \r\n> I am not sure where the documenting should go. I think it is best if we could add this [option](https://stackoverflow.com/a/57733619) to our `.bazelrc` file.\r\n\r\ncc: @mihaimaruseac ", "> This basically says do not perform heap leak checking for the entire program but only for annotated code blocks (using an internal API). Since there are no annotated code-blocks in this test this basically skips checking for leaks. We can remove this once the leaks are gone.\r\n\r\nI found this site https://gperftools.github.io/gperftools/heap_checker.html . It seems related to that `args`. Could you check it out ?", "> > This basically says do not perform heap leak checking for the entire program but only for annotated code blocks (using an internal API). Since there are no annotated code-blocks in this test this basically skips checking for leaks. We can remove this once the leaks are gone.\r\n> \r\n> I found this site https://gperftools.github.io/gperftools/heap_checker.html . It seems related to that `args`. Could you check it out ?\r\n\r\nAha, yep looks like the correct link.", "FYI,\r\n\r\nWhen building asan without debugging, e.g only `--copt -fsanitize=address --linkopt -fsanitize=address`, `custom_gradient_test` pass. However, when adding `--copt -O1 --copt -g -c dbg`, there is a leak in `tensorflow::BuildImmediateExecutionContext` ( it is probably why we need `heap-check=local` ).\r\n\r\nMy guess is we could drop `no_cuda_asan` for `gradient_checker_test`.\r\n\r\n`nn_grad_test` has some leaks even without debugging symbol, so we will have to leave it as-is. \r\n\r\n"]}, {"number": 45653, "title": "RTX3070 CUDA 11.1 CuDNN 8.0.5 not GPU available with tensorflow 2.5 (nightly version)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home\r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version: tf-nightly-gpu==2.5.0.dev20201213\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: installed via anaconda (conda 4.9.2) using pip install tf-nightly-gpu\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 11.1 / cuDNN 8.0.5\r\n- GPU model and memory: Geforce RTX 3070 8 Go (Driver 460.79)\r\n\r\n\r\n**Describe the problem**\r\nI have encountered a problem when I tried to install tensorflow-gpu in my anaconda environment.\r\n\r\nWhen I tested the availability of GPU after import of tensorflow, it seems that cusolver64_10.dll is missing and I cannot use my GPU with Tensorflow. From what I know, Tensorflow is not available for Cuda toolkit 11.1. So, should I try to compile Tensorflow or show I try older version of Cuda toolkit for example in virtual machine ?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n(deeplearning-tf) C:\\Users\\Utilisateur>python\r\nPython 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflos as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflos'\r\n>>> import tensorflow as tf\r\n2020-12-14 12:30:59.774387: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n>>> print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n2020-12-14 12:31:28.599495: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-12-14 12:31:28.631910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:\r\npciBusID: 0000:2b:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.815GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-14 12:31:28.631996: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-14 12:31:29.003118: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-14 12:31:29.003522: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-14 12:31:29.266489: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-14 12:31:29.287591: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-14 12:31:29.288565: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2020-12-14 12:31:29.462695: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-14 12:31:29.475294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-14 12:31:29.475391: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1764] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\nNum GPUs Available:  0\r\n\r\n\r\nThank you in advance for your help.\r\nBest\r\nEnrico", "comments": ["Yes please check https://github.com/tensorflow/tensorflow/issues/44697 https://github.com/tensorflow/tensorflow/issues/44291", "@eperspicace,\r\nCould you please install CUDA 11 instead of CUDA 11.1 and check if you're facing the same issue?\r\n\r\nAlso as @bhack mentioned, please go through these similar issues [#44567](https://github.com/tensorflow/tensorflow/issues/44567#issuecomment-729504167), [#45258](https://github.com/tensorflow/tensorflow/issues/45258#issuecomment-739871133) for more information. Thanks!", "@bhack, @amahendrakar \r\nthank you for your answers. I will uninstall CUDA11.1 and install CUDA11.0 instead. It should solve the problem of dll librairy. \r\nBest,\r\nEnrico", "Dear @bhack and @amahendrakar ,\r\nI installed CUDA11.0 beside CUDA11.1 without uninstalling it (as mentioned in the link you provided). I also downloaded and unpacked cuDNN 8.0.4. Tensorflow was installed inside anaconda environment from pip tf-nightly-gpu numpy==1.19.3 command line. I was able to import tensorflow and test for GPU availability. It seems to work as I didn't get any error. Thank again for your help!\r\nRegards,\r\nEnrico", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45653\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45653\">No</a>\n"]}, {"number": 45652, "title": "Can't load Model with SparseTensor inputs if it calls a Sequential model internally", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using model-sublcassing to create a `Model` that: \r\n\r\n1. Has a SparseTensor input\r\n2. Uses a Sequential model as internal layer\r\n\r\nthe model cannot be loaded using `tf.keras.models.load_model`.\r\n\r\nThe issue is twofold, and I believe it is related to how the signature of `call` is computed upon saving/loading. \r\n\r\nThe first problem is that `SparseTensorSpec` does not have a `name` attribute and thus the call to\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/dd3499e26c7dceaad5333ea0762903de242150b5/tensorflow/python/keras/saving/saved_model/load.py#L812\r\n\r\nfails when it has to deal with sparse tensors. \r\n\r\nThe second problem is that the signature of the model cannot be properly restored if it uses a Sequential model internally. This, I can't explain. \r\nWhatever the cause, removing the Sequential model causes the `load_model` method to take this path instead: \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/dd3499e26c7dceaad5333ea0762903de242150b5/tensorflow/python/keras/saving/saved_model/load.py#L814\r\n\r\nthus avoiding the crash. \r\n\r\nThe title of the issue is a bit weird, I didn't know how to describe the problem concisely :D\r\n\r\n**Describe the expected behavior**\r\n\r\nA Model's signature should be independent of the sub-modules used in its call function and the saving/loading API should support SparseTensors. \r\n\r\nI am happy to contribute code to solve this issue, but I would need some guidance on how to best proceed. \r\n\r\n**Standalone code to reproduce the issue**\r\n```py\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Model, Sequential\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.models import save_model, load_model\r\n\r\n\r\nclass Works(Model):\r\n    def build(self, input_shape):\r\n        self.fc = Dense(32)\r\n\r\n    def call(self, inputs):\r\n        a, b = inputs\r\n        b = self.fc(b)\r\n        return tf.sparse.sparse_dense_matmul(a, b)\r\n\r\n\r\nclass Crashes(Model):\r\n    def build(self, input_shape):\r\n        self.fc = Sequential([Dense(32)])  # <<<< THIS IS THE ONLY DIFFERENCE\r\n\r\n    def call(self, inputs):\r\n        a, b = inputs\r\n        b = self.fc(b)\r\n        return tf.sparse.sparse_dense_matmul(a, b)\r\n\r\n# Inputs\r\na = tf.sparse.from_dense(tf.ones((10, 10)))\r\nb = tf.ones((10, 10))\r\n\r\n# This works OK, no Sequential model\r\nworks = Works()\r\nworks([a, b])\r\nsave_model(works, 'works')\r\nload_model('works')\r\n\r\n# This crashes, it uses a Sequential model\r\ncrashes = Crashes()\r\ncrashes([a, b])\r\nsave_model(crashes, 'crashes')\r\nload_model('crashes')  # <<<< FAILS\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```py\r\nTraceback (most recent call last):\r\n  File \"~/dev/test.py\", line 38, in <module>\r\n    load_model('crashes')\r\n  File \"~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\", line 187, in load_model\r\n    return saved_model_load.load(filepath, compile, options)\r\n  File \"~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 121, in load\r\n    path, options=options, loader_cls=KerasObjectLoader)\r\n  File \"~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 633, in load_internal\r\n    ckpt_options)\r\n  File \"~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 194, in __init__\r\n    super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n  File \"~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 130, in __init__\r\n    self._load_all()\r\n  File \"~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 221, in _load_all\r\n    self._finalize_objects()\r\n  File \"~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 526, in _finalize_objects\r\n    _finalize_saved_model_layers(layers_revived_from_saved_model)\r\n  File \"~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 706, in _finalize_saved_model_layers\r\n    inputs = infer_inputs_from_restored_call_function(call_fn)\r\n  File \"~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 985, in infer_inputs_from_restored_call_function\r\n    spec = nest.map_structure(common_spec, spec, spec2)\r\n  File ~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/util/nest.py\", line 635, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/util/nest.py\", line 635, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 981, in common_spec\r\n    x.dtype, x.name)\r\nAttributeError: 'SparseTensorSpec' object has no attribute 'name'\r\n\r\n```\r\n", "comments": ["It was resolved in TF 2.4 please install  `pip install tensorflow==2.4.0-rc3` and run your example again.", "@daniel-s-ingram \r\nI ran the code on nightly and tf 2.4 and do not face any issues, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/147fc3c9db3a06bcf3b06b4daa42a347/untitled482.ipynb)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45652\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45652\">No</a>\n"]}, {"number": 45650, "title": "  W/native: op_kernel.cc:1401 OP_REQUIRES failed at save_restore_tenProplem restoring the checkpoint in my android app: sor.cc:175 : Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for [B@66ce18", "body": "I have created a simple tensorflow model and trained it on device using tensorflow java api. I send the weights to a server that I created and there I average them with other phones weights and created a upgrated model so I can do Federated Learning. \r\n\r\nWhen I download the checkpoint_name.ckpt.meta file so I can restore the checkpoint in the app I got one error that I dont understand. I run this code \r\n\r\n\r\nGraph graph = new Graph();\r\ngraph.importGraphDef(graphdef);\r\n...\r\ncheckpoinPrefix = org.tensorflow.Tensors.create(\"checkpoint_name.ckpt.meta\"); \r\n...\r\n...\r\nsess.runner().feed(\"save/Const\", checkpointPrefix).addTarget(\"save/restore_all\").run();\r\n\r\n**error:**\r\nW/native: op_kernel.cc:1401 OP_REQUIRES failed at save_restore_tensor.cc:175 : Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for [B@66ce18", "comments": ["@alejandroaguileraalcalde-ing,\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nalso also the the exact sequence of commands / steps that you executed before running into the error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45649, "title": "Support `class_weight` for TimeSeries models in Keras (`model.fit`)", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No): As much as I can\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently it is only possible to specify `class_weight` in `tf.keras.engine.training.Model.fit` for y (labels) with at most 2 dimensions `(batch, class)`. Would it be possible to support weights for TimeSeries classification models with three-dimensional labels `(batch, time_step, class)`?\r\n\r\nNote that it is currently possible to specify timestep-specific weights for individual input samples using `sample_weights`.\r\n\r\n**Will this change the current api? How?**\r\nMaybe. Currently, `class_weight` is accepted in the form of `Dict[int, float]`. It may be possible to change the format to `np.array` with 1 or 2 dimensions. It is also possible to maintain the current argument format and apply the same class weights to each time-step uniformly \r\n\r\n**Who will benefit with this feature?**\r\nPeople who are training time-series classification models with imbalanced classes.\r\n\r\n**Any Other info.**\r\nn/a", "comments": ["Take a look at https://github.com/keras-team/keras/issues/3653", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45648, "title": " 2.4.0 sess.run freezing in Windows 10 with RTX 3090", "body": "my DeepFaceLab software uses tf 2.3.1 with compat.v1, it works fine.\r\nWhen I upgrade to 2.4.0rc4, tf sess.run is freezed without any errors.\r\nSeems like it depends on model complexity.\r\nModels with big graph freeze instantly.\r\nModel with simpler graph may be freezed in 30 minutes.\r\nI don't know how to write reproduction code.\r\n\r\nOS: Windows 10\r\n\r\nWindows 10 setting\r\nHardware-accelerated GPU scheduling to \u201c default graphics setting \u201d, solves the problem.\r\n\r\n![fwy9iW8 1](https://user-images.githubusercontent.com/8076202/102058534-11408180-3e09-11eb-9bea-839dce2e9870.jpg)\r\n", "comments": ["TF 2.X packages come with gpu support however if the gpu environment setup is not compatible it silently falls back on cpu for computations.\r\nTensorFlow 2.4 pip packages are built with CUDA11 and cuDNN 8.0.2.\r\nSee [release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.4.0).", "I am using CUDA11 and cuDNN 8.0.2", "@iperov You saved my life. I have exactly same issue on my large scale CNN network, and it solved by enabling Hardware accelerated GPU scheduling in Windows 10. (Python 3.8 + TensorFlow 2.4.0 + RTX 2080 Ti)", "@KichangKim\r\n\r\n> I have exactly same issue on my large scale CNN network\r\n\r\ndo you also use compat.v1 ?\r\n", "No, I used keras API. TF 2.4.0 makes application freezed (0% cpu/gpu usage) without errors.", "@KichangKim\r\n> I used keras API. \r\n\r\nso it is mean the problem not only with v1 graph system, but also with v2 eager execution?", "2.4.0 release does not fix the issue.", "The same with me. I have tried 2.4.0-rc0 rc1 rc2 rc3 rc4 and 2.4.0 but unexpected thread freezing usually but not always  appear.", "@ymodak still no solution?\r\nwhy users should do the fix in windows settings?", "I don't know what the \"Hardware accelerated GPU scheduling\" feature does (perhaps it uses the new CUDA-graph HW support in Ampere?), so I don't understand why it is interacting poorly with TensorFlow.  Someone familiar with this new setting in Windows will have to chime in.", "I am using RTX 2080, it is not ampere.", "also in Windows Server 2019 no solution at all. \r\nStable freeze with tf 2.4.0 and rtx 3090", "`https://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.15.4%2Bnv20.12/py38/CPU%2BGPU/cuda111cudnn8avx2`\r\nThis repo build tf1.15 whl with cuda11 support\uff0cmaybe you can try it", "@Fannhhyy \r\npy3.8 - no\r\navx2 - no", "> @Fannhhyy\r\n> py3.8-\u6ca1\u6709\r\n> avx2-\u6ca1\u6709\r\n\r\n`https://github.com/fo40225/tensorflow-windows-wheel/issues/167#issuecomment-767277221`\r\nThe author fixed the nvidia-tensorflow build on windows. Maybe you can try to build it.\r\n\r\n`https://drive.google.com/file/d/1kPFVQGiYkUM7Q_jQzypxehfMxgOgDOKb/view?usp=sharing`\r\nI tried to build it , but I don't have time to test the whl. This whl need cuda11.2 .", "Hi, could you please check if the issue still persists with latest Tensorflow version or tf-nightly version. \r\nAlso, make sure you update the drivers and the tested build configurations from [here](https://www.tensorflow.org/install/source_windows#gpu). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45648\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45648\">No</a>\n"]}, {"number": 45647, "title": "micro: port operator FILL from lite", "body": "For micro, implement the FILL kernel and test, and add both to the build.\r\n\r\nThis PR contains several commits for readable history and ease of review.\r\n\r\nThis is PR5 in the sequence of PRs, outlined in issue #45306, to port operator FILL from lite to micro.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "This PR is stacked atop PR, #45646. It should be rebased and moved from draft to ready status as soon as that PR is merged.\r\n\r\nThe appropriate sequence of PRs and mapping of commits to PRs in this port of operator FILL was discussed in #45612.", "@rkuester This PR is in draft, can you please take a look on it. Thanks!", "@rkuester  This PR is in draft, can you please take a look on it. Thanks!", "FWIW, this PR is stacked atop PR #45646 and is awaiting approval thereof before I rebase it and take it out of draft.", "@rkuester Can you please resolve conflicts? Thanks!"]}, {"number": 45646, "title": "micro: remove lite-specific code from copy of FILL", "body": "Remove the bulk of lite-specific code from the micro\r\nimplementation of operator FILL.\r\n- Flatten the namespace\r\n- Don't resize output tensors\r\n- Remove input and output types other than int8 and float32\r\n- Don't use gtest\r\n\r\nThis is PR4 in the sequence of PRs, outlined in issue #45306, to port operator FILL from lite to micro.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "This PR is stacked atop PR #45457. It should be rebased and moved from draft to ready status as soon as that PR is merged.\r\n\r\nThe appropriate sequence of PRs and mapping of commits to PRs in this port of operator FILL was discussed in #45612.", "There's a formatting error flagged in the TFLite Micro test:\r\n\r\n```\r\nFormatting errors found\r\n--- /workspace/tensorflow/lite/micro/kernels/fill.cc  (original)\r\n+++ /workspace/tensorflow/lite/micro/kernels/fill.cc  (reformatted)\r\n@@ -85,8 +85,7 @@\r\n       break;\r\n     default:\r\n       context->ReportError(\r\n-          context,\r\n-          \"Fill only currently supports float32 for input 1, got %d.\",\r\n+          context, \"Fill only currently supports float32 for input 1, got %d.\",\r\n           value->type);\r\n       return kTfLiteError;\r\n   }\r\n```\r\n\r\nIt's also using context->ReportError, so we'll need to switch that to the macro instead too.", "@rkuester This PR is in draft, can you please take a look on it. Thanks!"]}, {"number": 45645, "title": "Can't Install TF on Apple M1", "body": "**Issue Creation**\r\nOn trying to install TF 2.3 using `pip install tensorflow==2.3` from the terminal, I got a success message.\r\nI could see tensorflow 2.3 in the list of installed packages by using either `conda list` or `pip list`\r\n\r\nWhen I run python from the terminal and trying to import tensorflow, it returns this error message:\r\n\r\n```\r\n(tfenv) mgd@MGD-m1 ~ % conda list tensorflow\r\n# packages in environment at /Users/mgd/opt/anaconda3/envs/tfenv:\r\n#\r\n# Name                    Version                   Build  Channel\r\ntensorflow                2.3.0                    pypi_0    pypi\r\ntensorflow-datasets       4.1.0                    pypi_0    pypi\r\ntensorflow-estimator      2.3.0                    pypi_0    pypi\r\ntensorflow-metadata       0.26.0                   pypi_0    pypi\r\n(tfenv) mgd@MGD-m1 ~ % python\r\nPython 3.8.5 (default, Sep  4 2020, 02:22:02) \r\n[Clang 10.0.0 ] :: Anaconda, Inc. on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nzsh: illegal hardware instruction  python\r\n(tfenv) mgd@MGD-m1 ~ % \r\n```\r\n\r\non uninstalling it and trying to installing again using:\r\n`pip install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.3.0-cp38-cp38-macosx_10_14_x86_64.whl`\r\nsame issue\r\n\r\nSame scenario on trying latest version. i.e. by using just `pip install tensorflow` to get 2.3.1\r\n\r\nOn trying to import it from Jupyter Lab, no error message is shown and also no interaction is happening. It's as if it's empty cell.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.0.1\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: 2.3.1\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n\r\n", "comments": ["Same issue reported before but no fix\r\nhttps://github.com/tensorflow/tensorflow/issues/44999", "@mabidou,\r\nCould you please try installing TensorFlow as per [this guide](https://github.com/apple/tensorflow_macos#installation) from Apple and let us know if it helps.\r\n\r\nAlso, please go through issue [#44751](https://github.com/tensorflow/tensorflow/issues/44751) for more information. Thanks!", "I have tried mentioned [link](https://github.com/apple/tensorflow_macos#installation) that has created a new env and also installed mac version that is not possible to import.\r\n\r\nHere are more details. After installation, I got\r\n```\r\nTensorFlow and TensorFlow Addons with ML Compute for macOS 11.0 successfully installed.\r\nTo begin, activate the virtual environment:\r\n   . \"/Users/mgd/tensorflow_macos_venv/bin/activate\" \r\n```\r\nWhen activating the env using `source /Users/mgd/tensorflow_macos_venv/bin/activate`\r\n\r\nI switched to it & I found the following packages, noting that I uninstalled previous version before starting this trial. Anyhow, here's what I found.\r\n\r\n```\r\ntensorflow-addons      0.11.2\r\ntensorflow-estimator   2.3.0\r\ntensorflow-macos       0.1a1 \r\n```\r\n\r\nAfter running Python inside the terminal and trying to import it, I failed & here's the full log\r\n\r\n```\r\n(base) mgd@MGD-m1 ~ % source /Users/mgd/tensorflow_macos_venv/bin/activate\r\n(tensorflow_macos_venv) (base) mgd@MGD-m1 ~ % python --version\r\nPython 3.8.5\r\n(tensorflow_macos_venv) (base) mgd@MGD-m1 ~ % python\r\nPython 3.8.5 (default, Sep  4 2020, 02:22:02) \r\n[Clang 10.0.0 ] :: Anaconda, Inc. on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow-macos as tf\r\n  File \"<stdin>\", line 1\r\n    import tensorflow-macos as tf\r\n                     ^\r\nSyntaxError: invalid syntax\r\n>>> import tensorflow_macos as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow_macos'\r\n>>> \r\n```\r\n", "Seems no solution so far!\r\nhttps://github.com/tensorflow/tensorflow/pull/45404", "Also TensorFlow Data Sets is not working and I reported it \r\nhttps://github.com/tensorflow/datasets/issues/2872 ", "As mentioned in #45631, this should be reported on the Apple fork."]}, {"number": 45644, "title": "what are the .pb and .h5  model file?", "body": "```\r\nmodel = create_model()\r\nmodel.fit(train_images, train_labels, epochs=5)\r\n\r\n!mkdir -p saved_model\r\n\r\nmodel.save('saved_model/my_model') \r\n\r\nmodel.save('my_model.h5') \r\n```\r\nwhat's the .pb  model?\r\nWhat are the differences and similarities between it and TensorRT's engine file?", "comments": ["> There are two formats you can use to save an entire model to disk: the TensorFlow SavedModel format, and the older Keras H5 format. \r\n\r\nhttps://www.tensorflow.org/guide/keras/save_and_serialize", "@pogevip \r\nThese are to formats used when you want to deploy your model, example if you want to deploy on android the acceptable format is .pb(info such as graph definition,checkpoints,weights are combined or freezed into a single file .pb--protocol buffer whereas .h5 is keras model), which later needs to be converted to \".tflite\". \r\nWe use .pb and not .h5 for deployment of large scale use of model as a product.\r\nFor your question about TensorRT a .pb model can be converted to TensorRT model.\r\n\r\nAs this is not a bug or feature request, please move this to closed status if the replies are satisfactory else create a issue on [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow), As there is also a larger community that reads questions there.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45644\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45644\">No</a>\n"]}, {"number": 45643, "title": "Add classifier_activation param to applications", "body": "I have noticed that the DenseNet and NasNet models did not have the possibility to specify the `classifier_activation` parameter, when the main classes have it implemented and the other models in `tf.keras.applications` also have them available, hence I have added those parameters. I have tested the code locally.", "comments": ["I have added the unit tests and squashed the commit with the previous one. Could you please elaborate on the pbtxt request? What do you need me to change and where?", "> Could you please elaborate on the pbtxt request? What do you need me to change and where?\r\n\r\nI've checked that we should be able to handle that on our side.", "@ivallesp can you please check sanity build failures ?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 45642, "title": "Tensorflow 1.x binary not found with pip install (Python=3.7)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.15\r\n- Python version: 3.7.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n**Describe the problem**\r\nTensorflow 1.x (specifically 1.15) binary cannot be found when installing via `pip` \r\n\r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.15 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1, 2.4.0rc0, 2.4.0rc1, 2.4.0rc2, 2.4.0rc3, 2.4.0rc4)\r\nERROR: No matching distribution found for tensorflow-gpu==1.15\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nconda create -n tf1 python=3.7\r\nconda activate tf1\r\npip install tensorflow-gpu==1.15\r\n```\r\n\r\n", "comments": ["@ad12 \r\n\r\nPlease, refer similar issue #39380 , #34302 and see if it helps you.Also, could you please check if you are using the 64 bit version of Python. Thanks!", "@ravikyram thanks for these suggestions\r\n\r\n#39380 mentions that Tensorflow 1.X is not compatible with Python 3.8. I am running with 3.7.9 so this may not be the issue. I also tried with python 3.6.12 and also getting the same `pip install` error\r\n\r\n#34302 mentions upgrading `pip>=19.0` as the fix. I am running with `pip=20.3.1`\r\n\r\nI am also using the 64-bit Python version. ", "Please attach the output of `pip debug verbose` and `pip install -vv tensorflow-gpu==1.15.4` (preferably in a new clean virtualenv)", "It appears to have resolved itself, thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45642\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45642\">No</a>\n"]}, {"number": 45641, "title": "TFLM: Adding TANH op optimized for CEVA-BX1", "body": "See github issue:  https://github.com/tensorflow/tensorflow/issues/45607\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@yair-ehrenwald Can you please resolve conflicts? Thanks!", "@yair-ehrenwald Any update on this PR? Please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I will open a new PR for this later on."]}, {"number": 45640, "title": "activation, kernel, bias are all attributes in Dense", "body": "Current documentation only says Dense implements such math operation. It's unclear to user how to access `kernel` and `bias`.\r\n\r\nIt turns out, through reading the code, these are all attributes added in this layer.\r\n\r\n`kernel` and `bias` are added in `build()`, while `activation` is added in the constructor.\r\n\r\n", "comments": ["@fchollet fixed"]}, {"number": 45639, "title": "__call__ will call build", "body": "Make the requirement clear. We don't have to `build` before `__call__`.", "comments": []}, {"number": 45638, "title": "Add keras.metrics.HarmonicMean", "body": "Previous discussion at https://github.com/tensorflow/addons/pull/2288", "comments": ["> Ok after the ecosystem review It seems that we could implement this here instead of TF core.\r\n\r\nOk, looks like this will be going into Addons. SGMT. Thank you!"]}, {"number": 45637, "title": "Is input_shape on Dense for public use?", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\r\n\r\nhttps://www.tensorflow.org/tutorials/customization/custom_layers\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\n[The API documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) doesn't mention we can pass `input_shape` and `input_dim` to tf.keras.layers.Dense,  but various (official) resources use it, eg. https://www.tensorflow.org/tutorials/customization/custom_layers.\r\n\r\nSince the official authoritive API documentation doesn't claim the existance of `input_shape`, readers get very confused why other resources make extensive use of `input_shape`.\r\n\r\nI checked the source code, which indeed accesses input_shape.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/1987dba1a166d0336ccea8176dd2a16a65ddf19a/tensorflow/python/keras/engine/base_layer.py#L318-L339\r\n\r\nIs `input_shape` a private parameter but setting it through user code is an acceptable exploit?\r\n\r\nOr is it public and we just forget to add it on the documentation?\r\n\r\n\r\n\r\n\r\n", "comments": ["I think It Is implicit in the documentation: `Inherits From: Layer, Module`.\r\nAs you see the documented `kwargs` in the docs are passed to the parent init:\r\nhttps://github.com/tensorflow/tensorflow/blob/3727302f03a495b665efda24a845c4b30a7cffe9/tensorflow/python/keras/layers/core.py#L1156:L1157", "@bhack the inheritance exists, but the constructors of the parent classes don't mention `input_shape` neither, though `Layer.build` accepts input_shape, not sure if they function the same.\r\n\r\nI'm taking about documentation, to be clear.", "You can see `input_dim` and `input_shape` for the parent at https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer", "Also other then single line examples note that for real use cases some `kwargs` make sense for input layers:\n\nhttps://github.com/tensorflow/tensorflow/blob/edc060801f9e049e67933a8fbf5059d4bced7f7a/tensorflow/python/keras/engine/base_layer.py#L326:L329", "> You can see input_dim and input_shape for the parent at https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer\r\n\r\nThat's exactly what I found \"though Layer.build accepts input_shape, not sure if they function the same.\"\r\n\r\nPlus, input_dim at https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer is an arbitrarily user-defined argument. \r\n\r\nAre you saying the identifier names seem arbitrarily but actually carefully crafted that match `allowed_kwargs` ?\r\n", "In that `input_dim` example it is not passed to the `__init__` parent.\n\nHere you can find the `input_dim/input_shape` handling in the parent https://github.com/tensorflow/tensorflow/blob/9697081dac25ef5c3e95b98b6e1113261611a4b9/tensorflow/python/keras/engine/base_layer.py#L418:L433", "Thanks. Are we going to add information provided in https://github.com/tensorflow/tensorflow/blob/9697081dac25ef5c3e95b98b6e1113261611a4b9/tensorflow/python/keras/engine/base_layer.py#L418:L433 to **documentation**?", "@gqqnbig Probably if you can open a PR It will be accepted but I am not sure about:\r\n\r\n> in this case we will later create an input layer to insert before the current layer\r\n\r\nAre you interested to submit a Documentation PR?", "@bhack Sorry, I'm too busy with other things.", "The docs are now updated. Thanks!\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?version=nightly"]}, {"number": 45636, "title": "tf.enable_eager_execution() not working", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: cuDNN 7.6.5\r\n- GPU model and memory: NVIDIA GTX 1050\r\n\r\n**Describe the current behavior**\r\nI am trying to use the function tensor.numpy(), which requires eager_excecution to be enabled. I checked online, and it said that Tensorflow 2.0 has eager_execution enabled by default. However, when I run `print(tf.executing_eagerly())` the output is `False`\r\n\r\nI have tried the `tf.compat.v1.enable_eager_execution()` function, but it does not seem to change anything\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.enable_eager_execution()\r\nprint(tf.executing_eagerly())\r\n\r\n```\r\n", "comments": ["MeteoRex11 \r\nI ran the code shared and do not face issue reported, please refer  to the [gist here](https://colab.research.google.com/gist/Saduf2019/c323090d207a00edd540c4d7a4ff5b57/untitled485.ipynb). Could you try on colab and let us know.", "> MeteoRex11\r\n> I ran the code shared and do not face issue reported, please refer to the [gist here](https://colab.research.google.com/gist/Saduf2019/c323090d207a00edd540c4d7a4ff5b57/untitled485.ipynb). Could you try on colab and let us know.\r\n\r\nIt is working on colab, but not on my system. Is it because of some installation issue?", "@MeteoRex11 Did you pass  [`--config=v2`](https://github.com/tensorflow/tensorflow/issues/29573) flag while building from source?\r\nSee https://www.tensorflow.org/install/source#expandable-1\r\n", "> @MeteoRex11 Did you pass [`--config=v2`](https://github.com/tensorflow/tensorflow/issues/29573) flag while building from source?\r\n> See https://www.tensorflow.org/install/source#expandable-1\r\n\r\nNo actually I had installed tensorflow via pip, in an anaconda environment. ", "@MeteoRex11 \r\nIs this still an issue.", "Yes it still doesn't work. I tried reinstalling Tensorflow but the issue persists.\r\n\r\nAny update on this @Saduf2019 ?\r\n", "@MeteoRex11 I just tried on my `windows-10` desktop and don't see any issue there. I have exactly same TF version as you. You don't need `tf.compat.v1.enable_eager_execution()`. It is difficult to test why it is working wrongly for you. I guess it is more related to windows-10 and not related to TF.\r\n\r\nCan you please try \r\n\r\n```\r\nimport tensorflow as tf\r\n#tf.compat.v1.enable_eager_execution()\r\nprint(tf.executing_eagerly())\r\ntf.__version__()\r\n```\r\n\r\n", "> @MeteoRex11 I just tried on my `windows-10` desktop and don't see any issue there. I have exactly same TF version as you. You don't need `tf.compat.v1.enable_eager_execution()`. It is difficult to test why it is working wrongly for you. I guess it is more related to windows-10 and not related to TF.\r\n> \r\n> Can you please try\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> #tf.compat.v1.enable_eager_execution()\r\n> print(tf.executing_eagerly())\r\n> tf.__version__()\r\n> ```\r\n\r\nThe issue was being replicated on Linux as well, but reinstalling Tensorflow, CuDNN & CudaToolKit's older versions fixed it. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45636\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45636\">No</a>\n"]}, {"number": 45635, "title": "RTX 3060 TI, creating GPU device take about 5 Minutes", "body": "Windows 10 Prof.\r\nRTX 3060 TI\r\nTensorFlow 2.1.0\r\nInstaller: Anaconda\r\n\r\nNote: Same with Ubuntu 20.04 LTS and PIP Installing\r\n\r\nCode:\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.test.is_gpu_available()\r\n```\r\nOutput:\r\n\r\nWARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\n2020-12-13 10:23:03.545074: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-12-13 10:23:03.555731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-12-13 10:23:03.609505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6\r\ncoreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-13 10:23:03.621055: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-13 10:23:03.636314: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-12-13 10:23:03.649406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-13 10:23:03.657126: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-12-13 10:23:03.672274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-13 10:23:03.682682: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-12-13 10:23:03.704980: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-13 10:23:03.709446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-12-13 10:28:51.751301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-13 10:28:51.758864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-12-13 10:28:51.763278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2020-12-13 10:28:51.768347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device \r\n\r\n**<---- about 5 minutes nothing ---->**\r\n\r\n(/device:GPU:0 with 6281 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\nTrue\r\n", "comments": ["another Test based on this paper:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport time\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nstart = time.time()\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n\r\nend = time.time()\r\nprint(\"Time delta: \", end - start)\r\n```\r\n\r\nOutput: Time delta:  218.35683226585388\r\nCommandline: 2020-12-13 10:47:49.454830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n\r\n```\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nmodel.compile(optimizer='adam',\r\n              loss=loss_fn,\r\n              metrics=['accuracy'])\r\n\r\nstart = time.time()\r\nmodel.fit(x_train, y_train, epochs=5)\r\nend = time.time()\r\nprint(\"Time delta: \", end - start)\r\n```\r\n\r\nOutput: \r\n\r\nTrain on 60000 samples\r\nEpoch 1/5\r\n<---- long time nothing ---->\r\n60000/60000 [==============================] - 76s 1ms/sample - loss: 0.2948 - accuracy: 0.9145\r\nEpoch 2/5\r\n60000/60000 [==============================] - 3s 56us/sample - loss: 0.1434 - accuracy: 0.9589\r\nEpoch 3/5\r\n60000/60000 [==============================] - 3s 57us/sample - loss: 0.1090 - accuracy: 0.9676\r\nEpoch 4/5\r\n60000/60000 [==============================] - 3s 56us/sample - loss: 0.0893 - accuracy: 0.9727\r\nEpoch 5/5\r\n60000/60000 [==============================] - 3s 57us/sample - loss: 0.0730 - accuracy: 0.9774\r\nTime delta:  89.69082736968994\r\n\r\nWenn I run other models that use much more training data, the training is not starting. My idea is, that the memory transfer is very slow.\r\n", "@Doev \r\n\r\nJust to verify are you facing the same issue with latest stable TF version 2.3 and nightly versions as well? \r\nThanks!", "I found out, that it is not the amount of training data, but the number of trainable parameters that causes the delay.\r\n\r\nWith tensorflow-gpu 2.3 (Anaconda), I have the problem, that is only running at the cpu. I will try to setup and virtual environement with pip, that is the same as another px with an GTS 1050 Ti.\r\n\r\nSo far I have no idea how to use the nightly build.", "The Problem with Tensorflow-gpu 2.3 (Anaconda) is, that it does not install cudnn correctly and for that, it goes with the cpu.\r\n\r\nI copyed the the virtual environemnt (tf 2.3) from the pc with the good running GTS 1050 Ti and have the same problem (Long delays while starting tf or before first training).\r\n", "Commandline output until it delays the first time:\r\n\r\n[W 09:24:27.040 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20201214092417 (::1) 10.03ms referer=http://localhost:8888/notebooks/Untitled.ipynb\r\n[I 09:24:27.262 NotebookApp] Kernel started: 4c268691-1393-4bc4-860d-f2264de97874, name: python3\r\n2020-12-14 09:24:30.113136: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n[I 09:26:27.238 NotebookApp] Saving file at /Untitled.ipynb\r\n2020-12-14 09:27:37.473538: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-12-14 09:27:37.505228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6\r\ncoreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-14 09:27:37.505433: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-14 09:27:37.525331: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-12-14 09:27:37.531209: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-14 09:27:37.534229: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-12-14 09:27:37.541114: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-14 09:27:37.544944: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-12-14 09:27:37.557576: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-14 09:27:37.557832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-12-14 09:27:37.559870: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-14 09:27:37.570839: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x201435ac8f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-12-14 09:27:37.570934: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-12-14 09:27:37.571772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6\r\ncoreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-14 09:27:37.572394: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-14 09:27:37.573126: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-12-14 09:27:37.573677: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-14 09:27:37.576330: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-12-14 09:27:37.578139: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-14 09:27:37.580568: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-12-14 09:27:37.581116: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-14 09:27:37.582098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0", "I will now test the nightly: tf_nightly-2.5.0.dev20201213-cp38-cp38-win_amd64.whl\r\n\r\n`import tensorflow as tf`\r\n\r\n ** On entry to DGEBAL parameter number  3 had an illegal value\r\n ** On entry to DGEHRD  parameter number  2 had an illegal value\r\n ** On entry to DORGHR DORGQR parameter number  2 had an illegal value\r\n ** On entry to DHSEQR parameter number  4 had an illegal value\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\doev\\venv\\tf_nightly\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\doev\\venv\\tf_nightly\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\doev\\venv\\tf_nightly\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 29, in <module>\r\n    import numpy as np\r\n  File \"C:\\Users\\doev\\venv\\tf_nightly\\lib\\site-packages\\numpy\\__init__.py\", line 305, in <module>\r\n    _win_os_check()\r\n  File \"C:\\Users\\doev\\venv\\tf_nightly\\lib\\site-packages\\numpy\\__init__.py\", line 302, in _win_os_check\r\n    raise RuntimeError(msg.format(__file__)) from None\r\nRuntimeError: The current Numpy installation ('C:\\\\Users\\\\doev\\\\venv\\\\tf_nightly\\\\lib\\\\site-packages\\\\numpy\\\\__init__.py') fails to pass a sanity check due to a bug in the windows runtime. See this issue for more information: https://tinyurl.com/y3dm3h86\r\n", "TF 2.X packages come with gpu support however if the gpu environment setup is not compatible it silently falls back on cpu for computations.\r\nTensorFlow 2.4 (and nightly 2.5.xxx) pip packages are built with CUDA11 and cuDNN 8.0.2.\r\nSee TF gpu, cuda cudnn [compatibility matrix](https://github.com/tensorflow/tensorflow/releases/tag/v2.4.0) ", "Actually I get an new error:\r\n\r\nWindows 10\r\nvirtual environment / pip: Tensorflow 2.4.0\r\nGPU-Driver: 460.79\r\ncuda_11.0.2_451.48_win10\r\ncudnn-11.0-windows-x64-v8.0.2.39\r\n\r\nEverything works now without delays until I call modell.fit:\r\n\r\n### python error:\r\n\r\nUnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node model/conv2d/Relu (defined at <ipython-input-1-3ca5da3d7565>:35) ]] [Op:__inference_train_function_3041]\r\n\r\n### commandline:\r\n...\r\n2020-12-15 15:14:23.681512: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-15 15:14:23.683266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6\r\ncoreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-15 15:14:23.683427: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-15 15:14:23.683584: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-15 15:14:23.683754: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-15 15:14:23.683895: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-15 15:14:23.684031: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-15 15:14:23.684189: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-15 15:14:23.684326: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-15 15:14:23.684453: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-15 15:14:23.684649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-12-15 15:14:25.689226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-15 15:14:25.689412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2020-12-15 15:14:25.689792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2020-12-15 15:14:25.692672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6617 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2020-12-15 15:14:25.697317: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-15 15:14:42.325717: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2020-12-15 15:14:44.210420: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-15 15:14:46.178140: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-15 15:14:46.219817: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-15 15:14:48.158021: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2020-12-15 15:14:48.158231: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2020-12-15 15:14:48.162046: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2020-12-15 15:14:48.162241: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2020-12-15 15:14:48.162715: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops_fused_impl.h:697 : Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\r\n", "A simple mnist-test-network with flatten, dense and dropout layers works fine on the gpu.", "Can you try [limiting gpu memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)? Shut down all your python processes and in new session/colab try your example.\r\n```python\r\n#On top of your script\r\nimport tensorflow as tf\r\ngpus = tf.config.list_physical_devices('GPU')\r\ntf.config.set_visible_devices(gpus[0], 'GPU')\r\n# Rest of your code\r\n ...\r\n```", "I used the same code to compare two environemnts:\r\n\r\n1) Windows 10, GTX 1050 Ti, TF 2.3.1\r\n2) Windows 10, RTX 3060 Ti, TF 2.4.0\r\n\r\nThe code runs as expected on environment 1 with no error-messages on the console. It fails on environment 2 and the error messages are already pasted above.\r\n\r\nThe code above to limiting gpu memory shows no effect on both environments. GPU memory is always used for nearly 100%. But the code runs fine with the 4GB of the G TS450 Ti.\r\n\r\nThe command where the exception happens is:\r\n\r\n```\r\n...\r\n     61   except core._NotOkStatusException as e:\r\n\r\nUnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node model/conv2d/Relu (defined at <ipython-input-17-7f1bbb39a367>:1) ]] [Op:__inference_train_function_1658]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n\r\nConsole Error:\r\n```\r\n2020-12-16 21:33:18.290073: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2020-12-16 21:33:18.290200: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2020-12-16 21:33:18.290830: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops_fused_impl.h:697 : Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n```\r\n\r\nMaybe I can test TF 2.4.0 with linux soon.", "I apologize I gave incomplete code prior. Can you please try:\r\n```python\r\n#On top of your script\r\nimport tensorflow as tf\r\ngpu = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpu[0], True) #limits gpu memory\r\n# Rest of your code\r\n```\r\nhttps://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth", "When I do use memory limiting the ram usage stays low, until I call model.fit(). After that it grows again until short before 100%. There are many new lines at the console log:\r\n\r\n2020-12-16 23:36:03.051777: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n\r\nBut training now works on the RTX 3060 Ti. Training is about 2,5 times faster, and GPU goes up to 8% (GTS 1050 Ti 85%). I can use a higher batch. That are good news. (Edit: as I see, The Windows Taskmanager shows wrong values. GPU is running 80-90%)\r\n\r\nThank you!\r\n", "Glad it worked. I will close this issue now. Thanks!"]}, {"number": 45634, "title": "Mapping LSTM over arbitrary tensor shapes", "body": "**System information**\r\n- TensorFlow version 2.3.1:\r\n\r\nDense layers map over tensors of up to rank 5, by just being applied to the last dimension:\r\n\r\n```\r\nimport tensorflow as tf\r\nm = tf.keras.Sequential([tf.keras.layers.Dense(1)])\r\nprint(m(tf.ones((10, 9, 8, 6, 5))).shape)\r\n```\r\nSo they consume 1 dimension and allow for a freedom of 4 dimensions of additional structure (where the first one usually represents a batch)\r\n\r\nWhy is this different for LSTM layers, which only support inputs of rank 3:\r\n\r\nThis works:\r\n ```\r\nimport tensorflow as tf\r\nm = tf.keras.Sequential([tf.keras.layers.LSTM(1)])\r\nprint(m(tf.ones((8, 6, 5))).shape)\r\n```\r\n\r\nThis doesn't work:\r\n ```\r\nimport tensorflow as tf\r\nm = tf.keras.Sequential([tf.keras.layers.LSTM(1)])\r\nprint(m(tf.ones((9, 8, 6, 5))).shape)\r\n```\r\nleading to following error: \r\n\r\n```\r\nValueError: Input 0 of layer sequential_7 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [9, 8, 6, 5]\r\n```\r\n\r\nAlso LSTM layers consume 2 dimensions, so there is only 1 additional dimension, which is usually required to be the batch.\r\n\r\nI am wondering why there are these limitations and if the LSTM layers could at least made equal to the dense layers in that regard. I am aware that through reshaping one can emulate that behaviour but this raises the question if it introduces an unnecessary computational overhead. In addition to that it is convenient to work with `None` dimensions which makes the reshaping problematic in graph mode. \r\n", "comments": ["Was able to reproduce the issue with TF v2.2, TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/a09e5d391ccd3dc178a4dbcd035af475/45634.ipynb). Thanks!", "@jonas-eschmann,\r\nYes, it is different and it is expected. It is because, **`Dense`** Layer is just a **`Dot Product`** between `Weights` and `Inputs`, added by `Biases`. \r\n\r\n[Documentation of Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) states:\r\n\r\n> Note: If the input to the layer has a rank greater than 2, then Dense computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 1 of the kernel (using tf.tensordot). For example, if input has dimensions (batch_size, d0, d1), then we create a kernel with shape (d1, units), and the kernel operates along axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there are batch_size * d0 such sub-tensors). The output in this case will have shape (batch_size, d0, units).\r\n\r\n\r\nHowever, [LSTM and GRU](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45) has different Architecture  and different Operations taking place inside them, which mandates the `Input Tensor` to be **`3-Dimensional`**.\r\n\r\nHence, the documentation of [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU#call_arguments_2) and [GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU#call_arguments_2) states:\r\n\r\n> inputs: A 3D tensor, with shape [batch, timesteps, feature].", "@jonas-eschmann,\r\nCan you please respond to the above comment. Thanks! ", "Hi, thanks for the response and sorry for the late reply. \r\nI don't see why recurrent layers should behave differently. The just consume the last two dimensions of the input instead of only the last one. So it operates along axes 1 and 2 of the input (on every sub-tensor of shape (1, d0, d1)). There is no reason why it couldn't operate on subtensors of shape (1, 1, d0, d1), as this would be the same as reshaping the first two dimensions into one batch dimension, applying the recurrent layer (with its current restrictions) and then reshaping again to recover the first two dimensions. ", "@jonas-eschmann,\r\nIn case of a Dense Layer, the computation that happen is just, **`Activation(W.X + b)`** but in case of **`LSTM`**, \r\nbecause of its `Multi Gate Architecture`, the computations that take place are\r\n\r\n```python\r\nit = \u03c3(Wixt + Riht-1 + bWi + bRi)\r\nft = \u03c3(Wfxt + Rfht-1 + bWf + bRf)\r\not = \u03c3(Woxt + Roht-1 + bWo + bRo)\r\nc't = tanh(Wcxt + Rcht-1 + bWc + bRc)\r\nct = ft \u25e6 ct-1 + it \u25e6 c't\r\nht = ot \u25e6 tanh(ct)\r\n```\r\n\r\nPlease find [this link](https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNMode_t) for more details.\r\n\r\nSince many calculations are involved in **`LSTM Layer`**, it mandates the Input Shape to be a **`3D Tensor`**. Similarly, a **`Conv2D Layer`** mandates its Input to be a `4D Tensor`, `(N,H,W,C) `.", "@jonas-eschmann,\r\nCan you please respond to the above comment? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45634\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45634\">No</a>\n"]}, {"number": 45633, "title": "ERROR: An error occurred during the fetch of repository 'com_google_protobuf'", "body": "- Windows 10 - fresh installation to reproduce this problem (after experiencing this issue on another Windows 10 machine).\r\n- Following the instructions in https://www.tensorflow.org/install/source_windows (without installing MSYS2).\r\n- TensorFlow version: r2.3 and latest (tried both)\r\n- Python version: 3.8.6\r\n- Bazel version: 3.1.0 and 3.7.1 (both were tried and give the same results)\r\n- Compiler version: according to the instructions as mentioned above (Install Visual C++ Build Tools 2019)\r\n\r\n**Describe the problem**\r\nFor r2.3, the compilation starts as usual and fail with:\r\n```\r\n...\r\nRepository rule tf_http_archive defined at:\r\n  C:/users/ranip/documents/tensorflow/third_party/repo.bzl:134:19: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'com_google_protobuf':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/ranip/documents/tensorflow/third_party/repo.bzl\", line 110\r\n                _apply_patch(ctx, <1 more arguments>)\r\n        File \"C:/users/ranip/documents/tensorflow/third_party/repo.bzl\", line 67, in _apply_patch\r\n                _wrap_bash_cmd(ctx, <1 more arguments>)\r\n        File \"C:/users/ranip/documents/tensorflow/third_party/repo.bzl\", line 28, in _wrap_bash_cmd\r\n                fail(<1 more arguments>)\r\nBAZEL_SH environment variable is not set\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@com_google_protobuf//': Traceback (most recent call last):\r\n        File \"C:/users/ranip/documents/tensorflow/third_party/repo.bzl\", line 110\r\n                _apply_patch(ctx, <1 more arguments>)\r\n        File \"C:/users/ranip/documents/tensorflow/third_party/repo.bzl\", line 67, in _apply_patch\r\n                _wrap_bash_cmd(ctx, <1 more arguments>)\r\n        File \"C:/users/ranip/documents/tensorflow/third_party/repo.bzl\", line 28, in _wrap_bash_cmd\r\n                fail(<1 more arguments>)\r\nBAZEL_SH environment variable is not set\r\nINFO: Elapsed time: 180.653s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (8 packages loaded, 12 targets configured)\r\n    currently loading: tensorflow\r\n```\r\n\r\nFor latest master, it fails with:\r\n```\r\n...\r\nRepository rule third_party_http_archive defined at:\r\n  C:/users/ranip/documents/tensorflow/third_party/repo.bzl:216:28: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'flatbuffers':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/ranip/documents/tensorflow/third_party/repo.bzl\", line 193\r\n                _apply_delete(ctx, <1 more arguments>)\r\n        File \"C:/users/ranip/documents/tensorflow/third_party/repo.bzl\", line 73, in _apply_delete\r\n                _wrap_bash_cmd(ctx, <1 more arguments>)\r\n        File \"C:/users/ranip/documents/tensorflow/third_party/repo.bzl\", line 28, in _wrap_bash_cmd\r\n                fail(<1 more arguments>)\r\nBAZEL_SH environment variable is not set\r\nERROR: C:/users/ranip/documents/tensorflow/tensorflow/tools/pip_package/BUILD:281:1: //tensorflow/tools/pip_package:build_pip_package depends on //tensorflow/lite/python:tflite_convert in repository @ which failed to fetch. no such package '@flatbuffers//': Traceback (most recent call last):\r\n        File \"C:/users/ranip/documents/tensorflow/third_party/repo.bzl\", line 193\r\n                _apply_delete(ctx, <1 more arguments>)\r\n        File \"C:/users/ranip/documents/tensorflow/third_party/repo.bzl\", line 73, in _apply_delete\r\n                _wrap_bash_cmd(ctx, <1 more arguments>)\r\n        File \"C:/users/ranip/documents/tensorflow/third_party/repo.bzl\", line 28, in _wrap_bash_cmd\r\n                fail(<1 more arguments>)\r\nBAZEL_SH environment variable is not set\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@flatbuffers//': Traceback (most recent call last):\r\n        File \"C:/users/ranip/documents/tensorflow/third_party/repo.bzl\", line 193\r\n                _apply_delete(ctx, <1 more arguments>)\r\n        File \"C:/users/ranip/documents/tensorflow/third_party/repo.bzl\", line 73, in _apply_delete\r\n                _wrap_bash_cmd(ctx, <1 more arguments>)\r\n        File \"C:/users/ranip/documents/tensorflow/third_party/repo.bzl\", line 28, in _wrap_bash_cmd\r\n                fail(<1 more arguments>)\r\nBAZEL_SH environment variable is not set\r\nINFO: Elapsed time: 174.239s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (50 packages loaded, 14 targets configured)\r\n    currently loading: tensorflow/lite/python\r\n```\r\n\r\nI am aware of issues https://github.com/tensorflow/tensorflow/issues/37897 and https://github.com/tensorflow/tensorflow/issues/35414 - no \"Windows Subsystem for Linux\" was installed on this Windows - as said - it is a fresh Windows 10 install, just to reproduce this problem. Did not use gitbash, or any Unix like shell.  Only Powershell was used. \r\n", "comments": ["@rani-pinchuk \r\nPlease refer to this resolved [issue](https://github.com/tensorflow/tensorflow/issues/42587#issuecomment-678813034) with same error and let us know. [ also verify if your pip version is latest]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45633\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45633\">No</a>\n", "> @rani-pinchuk\r\n> Please refer to this resolved [issue](https://github.com/tensorflow/tensorflow/issues/42587#issuecomment-678813034) with same error and let us know. [ also verify if your pip version is latest]\r\n\r\nI have tried it again, from a totally clean windows installation, following the instructions in https://www.tensorflow.org/install/source_windows to the letter, but WITHOUT installing MSYS2. Using r2.4. \r\n\r\nI still get \"An error occurred during the fetch of repository 'flatbuffers'\". Pip version is the latest. BAZEL_VC has no impact, and I don't see how the issue pointed to is resolved.\r\n\r\nAs I am trying doing this again and again from a fresh Windows 10 installation, the only conclusion I have is that something is missing in the documentation in https://www.tensorflow.org/install/source_windows\r\n\r\nAny help will be appreciated.\r\n\r\n\r\n", "`BAZEL_SH environment variable is not set`\r\n\r\nSeems like a Bazel bug to me.\r\n\r\nCan you compile other projects that use Bazel?", "I am not very much familiar with Bazel, and have no other projects to compile with it. \r\n\r\nI wonder if you could, though, explain (or document) the Window setup that is used to create the nightly-builds (or the release builds - see  #46538). I thought that this setup is based on the documentation in https://www.tensorflow.org/install/source_windows and therefore, this documentation must be correct... \r\n\r\nIn any case, if it helps, I will be happy to follow your instructions and try to build other project on the setup I have. ", "https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/tools/ci_build/rel/windows/ contains the scripts we are using for the release (nightly or final release). What is not shown there is just the VM setup and setting up credentials and build optimizations for internal hosts.\r\n\r\nMost Google projects have a bazel build. You can also make a custom one.", "Did you manage to test on another project?", "Closing as no response has been received and it doesn't look like a TF issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45633\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45633\">No</a>\n"]}, {"number": 45632, "title": "Conda install Tensorflow 2.1 with mkl (avx/avx2 support) not working", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): tensorflow 2.1 with mkl binary from Anaconda\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.9 (default, Aug 31 2020, 17:10:11)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: na\r\n- GPU model and memory: na\r\n\r\n**Describe the current behavior**\r\ntensorflow complains that \"Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\".\r\n\r\n**Describe the expected behavior**\r\nWith Intel's mkl version, we shouldn't get this error message. \r\n\r\n**Standalone code to reproduce the issue**\r\nI have tensorflow2.0 with mkl. \r\n\r\nHere's the code:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ninput_A = keras.layers.Input(shape=[5], name=\"wide_input\")\r\ninput_B = keras.layers.Input(shape=[6], name=\"deep_input\")\r\nhidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\r\nhidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\r\nconcat = keras.layers.concatenate([input_A, hidden2])\r\noutput = keras.layers.Dense(1, name=\"output\")(concat)\r\nmodel = keras.models.Model(inputs=[input_A, input_B], outputs=[output])\r\n\r\n```\r\n\r\nHere's the output:\r\n```\r\n2020-12-12 17:06:46.037846: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-12-12 17:06:46.040928: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n```\r\n\r\n\r\nHere's the mkl check from https://software.intel.com/content/www/us/en/develop/articles/intel-optimization-for-tensorflow-installation-guide.html\r\nCode:\r\n```\r\nmajor_version = int(tf.__version__.split(\".\")[0])\r\nif major_version >= 2:\r\n   from tensorflow.python import _pywrap_util_port\r\n   print(\"MKL enabled:\", _pywrap_util_port.IsMklEnabled())\r\nelse:\r\n   print(\"MKL enabled:\", tf.pywrap_tensorflow.IsMklEnabled()) \r\n```\r\nOutput:\r\n\r\n`MKL enabled: True\r\n`\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nPlease let me know how I can assist in debugging this issue.", "comments": ["I am new to github. Can someone please add label \"comp:mkl\", as referenced at https://software.intel.com/content/www/us/en/develop/articles/intel-optimization-for-tensorflow-installation-guide.html#inpage-nav-5", "@gmatalongthewatchtower \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements).Thanks!", "It does have it. i7-10610U CPU @1.8GHz; 4 Cores 8 Logical Processors", "@gmatalongthewatchtower \r\n\r\nInstallation issues within the Anaconda environment are tracked in the Anaconda repo.\r\n\r\nCould you please submit a new issue using [this link](https://github.com/ContinuumIO/anaconda-issues/issues) and fill in the template, so that the issue can be tracked there. Thanks!", "Isn't this an issue with Tensorflow? I am not sure what Anaconda environment will do here.", "@gmatalongthewatchtower \r\n\r\nThe warning is not complaining that AVX is not enabled. It is just an informative message saying only for some ops, Intel's oneDNN would be used and AVX or AVX2 would be turned on.\r\n\r\nPlease refer to #45853", "Closing this issue since it is being addressed on the thread tagged above. Thanks!"]}, {"number": 45631, "title": "Is there any support anywhere for Tensorflow on Apple Silicon?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No): Not sure what this means?\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current API? How?**\r\nNot sure. I'm unaware of what changes need to be made.\r\n\r\n**Who will benefit with this feature?**\r\nPeople who own machines running Apple Silicon. \r\n\r\n**Any Other info.**\r\nCurrently trying to import TF and Keras and I keep getting an error stating that it's an illegal hardware instruction.\r\n", "comments": ["It Is a duplicate of https://github.com/tensorflow/tensorflow/issues/44751", "@TheMartian32,\r\nCould you please try installing TensorFlow as per [this guide](https://github.com/apple/tensorflow_macos#installation) from Apple and check if it works.\r\n\r\nAlso, as mentioned by @bhack please go through issue [#44751](https://github.com/tensorflow/tensorflow/issues/44751) for more information. Thanks!", "It seems no support yet. @amahendrakar is posting same response to anyone reporting the issue as happened with me here https://github.com/tensorflow/tensorflow/issues/45645  #45645 ", "@mabidou Actually there seems to be a solution. This is what got it working for me ( although so far I've only imported Tensorflow and Keras ): https://github.com/apple/tensorflow_macos/issues/3\r\n\r\nBut it seems to be working great! \ud83d\udc4d ", "> @mabidou Actually there seems to be a solution. This is what got it working for me ( although so far I've only imported Tensorflow and Keras ): [apple/tensorflow_macos#3](https://github.com/apple/tensorflow_macos/issues/3)\r\n> \r\n> But it seems to be working great! \ud83d\udc4d\r\n\r\n@TheMartian32 \r\nWhich version you ended in? Regular `tensorflow   2.3.1 ` or  `tensorflow-macos   0.1a1`\r\nMy target is to have the regular `tensorflow   2.3.1 `   ", "Closing as duplicate of #44751\r\n\r\n`tensorflow-macos` issues should go to that repository, as that is a fork of this one with code we don't maintain.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45631\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45631\">No</a>\n", "If #44751  is already closed, where is the solution then?", "TF on Apple Silicon is supported only via the fork at the moment.", "@mabidou:\r\n> In the near future, we\u2019ll be making updates like this even easier for users to get these performance numbers by integrating the forked version into the TensorFlow master branch. \r\n\r\nhttps://blog.tensorflow.org/2020/11/accelerating-tensorflow-performance-on-mac.html", "@mabidou \r\n\r\nSo I ended up with TF 2.4 or whichever version is specified in that specific fork. Anyway, the solution was in the comments somewhere. It used mini conda, the fork of TF, and a virtual environment.", "We should expect a solution from TensorFlow team not to delegate it to Apple. Hope that TensorFlow team is working on that @mihaimaruseac ", "Apple Silicon support is provided via a fork from Apple, At the moment there is no way TF team at Google can maintain that build.", "Hi @mihaimaruseac \r\nTF is your product and it's expected to be supported from you in upcoming versions but I think that it's inconvenient to leave Apple Silicon users to use that fork.", "We only provide official support for [these wheels](https://pypi.org/project/tensorflow/#files). Anything else is community driven. Time is limited for everyone, we cannot support everything. Due to diminishing returns, headcount and priority issues, we only target the most common platforms."]}]