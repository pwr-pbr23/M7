[{"number": 38898, "title": "unique_with_counts is quite slow", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: docker image nvcr.io/nvidia/tensorflow:20.03-tf2-py3 (Linux x86_64 Ubuntu 18.04.4 LTS)\r\n- TensorFlow installed from: docker image\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.2 / 7.6\r\n- GPU model and memory: Tesla V100-SXM2 32GB\r\n\r\n**Describe the current behavior**\r\nunique_with_counts is quite slow. In comparison pytorch is 50 - 100 times faster:\r\n\r\n| framework   |   mean (ms) | dtype   |\r\n| ----------- | ----------: | ------- |\r\n| tensorflow  |   234.770   | int32   |\r\n| tensorflow  |   231.738   | int16   |\r\n| tensorflow  |   400.438   | float32 |\r\n| tensorflow  |   307.234   | float16 |\r\n| pytorch     |     5.372   | int32   |\r\n| pytorch     |     4.086   | int16   |\r\n| pytorch     |     5.080   | float32 |\r\n| pytorch     |     3.511   | float16 |\r\n\r\nThe script to produce these results can be found here: https://github.com/liob/bugreport_tf_unique_with_counts\r\n\r\n**Describe the expected behavior**\r\nIt is apparent that tf implements unique_with_counts as a single-threaded CPU routine. A GPU implementation, such as in pytorch, will most certainly speed up the operation dramatically.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1ye_wN-DkfIgz-AtQncbiM0h0zsVe7sKx\r\n", "comments": ["@liob \r\ni ran the code shared by you and face a diffrent error on both 2.1 and nightly, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/2d707444f149ba2e6e60d76ff7512bfd/untitled154.ipynb)", "@Saduf2019 \r\nthe newly created notebook did not include the sample data. Here is a fixed version:\r\nhttps://colab.research.google.com/gist/liob/778c1bc083600b1384c216b5169fd4ad/untitled154.ipynb\r\n\r\nThank you for taking a look at this issue.", "i am able to replicate this issue, please find the [gist here for nightly](https://colab.sandbox.google.com/gist/Saduf2019/e988e80bfb1b5e1dc3de10dc1dbd307d/38898.ipynb) . [and 2.1](https://colab.sandbox.google.com/gist/Saduf2019/194b4d8d4dd185f1a0104abc78ebaa60/untitled155.ipynb)Thanks!", "Thanks for your issue. The performance stats have significantly improved with TF 2.6\r\n\r\ndtype | framework | mean (ms)\r\n-- | -- | --\r\nint32 | tensorflow | 39.084363\r\nint16 | tensorflow | 25.341967\r\nfloat32 | tensorflow | 34.118117\r\nfloat16 | tensorflow | 22.233753\r\nint32 | pytorch | 32.295503\r\nfloat32 | pytorch | 24.424627\r\n\r\n"]}, {"number": 38896, "title": "go: Add input mapping option when importing Graph", "body": "I can't actually run the test because it's missing some imports, but I don't understand why the imports are missing.", "comments": ["HI @ctessum \r\nThanks for the PR, but please file a PR against the tensorflow:master branch. We only accept PR's that have been merged into master in a release branch. \r\nThanks!", "Apologies, I opened a new one at #38960"]}, {"number": 38895, "title": "Change code examples in tf.strings.format to use TF 2.x", "body": "The code examples in tf.strings.format still uses 1.x (`tf.compat.v1`)\r\nwhich is not appropriate now.\r\n\r\nThis PR changes code examples in tf.strings.format to use TF 2.x.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Can you please address Ubuntu Sanity errors? Thanks!", "Thanks @mihaimaruseac @gbaned for the help. I have updated the PR. The previous failure was caused by the fact that doctest does not use \"```\" to locate the code block anymore (use \">>>\" and \"...\") instead. I have removed the redundant \"```\" that was causing the test failure in the PR.\r\n\r\nPlease take a look and let me know if there are any other issues.", "Thanks @mihaimaruseac, the PR has been updated with a shorter example and the pylint disable has been removed as well."]}, {"number": 38894, "title": "Fix docstring error of MlirPassthroughOp", "body": "In api_def_MlirPassthroughOp.pbtxt, the docstring of MlirPassthroughOp has\r\na typo (extra `=`) which needs to be removed:\r\n\r\n```diff\r\n @tf.function\r\n def foo(x, y):\r\n-  return = mlir_passthrough_op([x, y], mlir_module, Toutputs=[tf.float32])\r\n+  return mlir_passthrough_op([x, y], mlir_module, Toutputs=[tf.float32])\r\n\r\n```\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 38893, "title": "Cannot save tensorflow_probability.distributions.PixelCNN with tf.train.Checkpoint", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX 1070\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nTrying to save a model which includes a tfd.PixelCNN gives the traceback:\r\n```Traceback (most recent call last):\r\n  File \"test.py\", line 16, in <module>\r\n    checkpoint.save(file_prefix='fails_before_here')\r\n  File \"/home/equint/GitHub/pyroclast/env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py\", line 1902, in save\r\n    file_path = self.write(\"%s-%d\" % (file_prefix, checkpoint_number))\r\n  File \"/home/equint/GitHub/pyroclast/env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py\", line 1832, in write\r\n    output = self._saver.save(file_prefix=file_prefix)\r\n  File \"/home/equint/GitHub/pyroclast/env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py\", line 1168, in save\r\n    file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)\r\n  File \"/home/equint/GitHub/pyroclast/env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py\", line 1108, in _save_cached_when_graph_building\r\n    object_graph_tensor=object_graph_tensor)\r\n  File \"/home/equint/GitHub/pyroclast/env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py\", line 1076, in _gather_saveables\r\n    feed_additions) = self._graph_view.serialize_object_graph()\r\n  File \"/home/equint/GitHub/pyroclast/env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py\", line 379, in serialize_object_graph\r\n    trackable_objects, path_to_root = self._breadth_first_traversal()\r\n  File \"/home/equint/GitHub/pyroclast/env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py\", line 199, in _breadth_first_traversal\r\n    for name, dependency in self.list_dependencies(current_trackable):\r\n  File \"/home/equint/GitHub/pyroclast/env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py\", line 159, in list_dependencies\r\n    return obj._checkpoint_dependencies\r\n  File \"/home/equint/GitHub/pyroclast/env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/data_structures.py\", line 509, in _checkpoint_dependencies\r\n    \"automatically un-wrapped and subsequently ignored.\" % (self,)))\r\nValueError: Unable to save the object ListWrapper([0, 1, 2]) (a list wrapper constructed to track trackable TensorFlow objects). A list element was replaced (__setitem__, __setslice__), deleted (__delitem__, __delslice__), or moved (sort). In order to support restoration on object creation, tracking is exclusively for append-only data structures\r\n```\r\n\r\n**Describe the expected behavior**\r\nShouldn't have a problem saving a distribution using `tf.train.Checkpoint.save`\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\ntfd = tfp.distributions\r\n\r\nmodel = tfd.PixelCNN(\r\n    image_shape=(28, 28, 1),\r\n    conditional_shape=(28, 28, 1),\r\n    num_resnet=1,\r\n    num_hierarchies=2,\r\n    num_filters=32,\r\n    num_logistic_mix=4,\r\n    dropout_p=.3,\r\n)\r\ncheckpoint = tf.train.Checkpoint(model=model)\r\ncheckpoint.save(file_prefix='fails_before_here')\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Am experiencing same issue.\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.15.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: Using CPU", "Was able to reproduce the issue with [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/73f066b5ffd0a1f73901e634e4200b81/38893-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/cbdab58e14ffb942cb05cf9cfb967b6b/38893-tf-nightly.ipynb). Please find the attached gist. Thanks!", "This is now fixed in TensorFlow Probability (tensorflow/probability#902)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38893\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38893\">No</a>\n", "Thanks for the update. I've relaxed the checking a bit anyway (not sure exactly what it was complaining about, but it should complain less overall now)."]}, {"number": 38892, "title": "Internal tensorflow-keras error loading hd5 model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nno\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\npip\r\n- TensorFlow version (use command below):\r\n2.0.0b1\r\n- Python version:\r\n3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\nnot using cuda\r\n- GPU model and memory:\r\nN/A\r\n\r\nI'm trying to load a trained model in an aws EC2 machine and I get the following:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/kraken/krk_modeling/src/services/logger.py\", line 51, in wrapper\r\n    return function(*args, **kwargs)\r\n  File \"src/wrapper.py\", line 81, in execute_pipeline\r\n    conv.build(wkobflow, wkobwthr, wkfcwthr, LOG)\r\n  File \"/home/ubuntu/kraken/krk_modeling/src/models/neuralnet.py\", line 41, in build\r\n    self.models[s] = load_model(self.PATH + self.FORMAT(s))\r\n  File \"/home/ubuntu/kraken/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\", line 137, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/home/ubuntu/kraken/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 162, in load_model_from_hdf5\r\n    custom_objects=custom_objects)\r\n  File \"/home/ubuntu/kraken/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/home/ubuntu/kraken/venv/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py\", line 90, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/ubuntu/kraken/venv/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 192, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/home/ubuntu/kraken/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1123, in from_config\r\n    process_layer(layer_data)\r\n  File \"/home/ubuntu/kraken/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1107, in process_layer\r\n    layer = deserialize_layer(layer_data, custom_objects=custom_objects)\r\n  File \"/home/ubuntu/kraken/venv/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py\", line 90, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/ubuntu/kraken/venv/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 194, in deserialize_keras_object\r\n    return cls.from_config(cls_config)\r\n  File \"/home/ubuntu/kraken/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 451, in from_config\r\n    return cls(**config)\r\n  File \"/home/ubuntu/kraken/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2417, in __init__\r\n    self.node_def = node_def_pb2.NodeDef.FromString(node_def)\r\nTypeError: a bytes-like object is required, not 'dict'\r\n```\r\n\r\nI am running the same code in my machine normally, the models load and run. on EC2 I'm using tensorflow 2.0.0b1, same version used in training. Also, the model is in production in another machine and loading normally too. I tried to re-install hdf5 and re-install tensorflow after that, and still getting the error. Please help =]\r\n\r\nthe problem is generated only by loading the model. no code is required, since the problem is originated from tensorflow.keras.models.load_model.\r\n", "comments": ["@elvis1020 \r\nplease share simple stand alone code for us to replicate the issue faced.", "`from tensorflow.keras.models import load_model`\r\n`model = load_model('<file_path>')`", "I've uploaded one of the models. maybe it would be easier.\r\nBut it must be loaded on an aws EC2 instance, because that's the issue. On my computer, the models load normally, and the tensorflow version is the same.\r\nhttps://drive.google.com/open?id=1kzyzAuNHTlCHc4Nx-JUhczCB7aTDMxp6", "@elvis1020 Yes I was able to reproduce the bug in tensorflow 2.0.0b1. Please try using tensorflow 2.0 or 2.1 or 2.2 versions as this bug has been fixed. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this issue as it has been inactive for more than 3 weeks. Please add additional comments for us to open this issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38892\">No</a>\n"]}, {"number": 38891, "title": "Adding regularization losses with the `model.add_loss` method doesn't seem to do the intended thing", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0-rc3\r\n- Python version: 3.7\r\n\r\n\r\n**Describe the current behavior**\r\nI was trying to add regularization losses to models that are already build, for example `keras_applications` models. I did this using the `model.add_loss` method. After adding losses from all the layers, calling model.losses seems to return a list containing the same loss value for each of the layers, which seems weird.\r\n```\r\ninputs = tf.keras.Input(shape=[100])\r\nx = tf.keras.layers.Dense(10)(inputs)\r\nx = tf.keras.layers.Dense(15)(x)\r\nx = tf.keras.layers.Dense(5)(x)\r\noutputs = tf.keras.layers.Dense(5)(x)\r\n\r\nmodel = tf.keras.Model(inputs, outputs)\r\n\r\nassert model.losses == []\r\n\r\nl2_regularizer = tf.keras.regularizers.l2(1e-4)\r\nfor i in range(len(model.layers)):\r\n    layer = model.layers[i]\r\n    if isinstance(layer, tf.keras.layers.Dense):\r\n        model.add_loss(lambda: l2_regularizer(layer.kernel))\r\n        \r\nprint(model.losses)\r\n```\r\nouptut:\r\n```\r\n[<tf.Tensor: shape=(), dtype=float32, numpy=0.0004718543>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.0004718543>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.0004718543>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.0004718543>]\r\n```\r\n\r\nInteresting thing to note is, if i add the losses without a loop, everything seems to work as intended\r\n```\r\ninputs = tf.keras.Input(shape=[100])\r\nx = tf.keras.layers.Dense(10)(inputs)\r\nx = tf.keras.layers.Dense(15)(x)\r\nx = tf.keras.layers.Dense(5)(x)\r\noutputs = tf.keras.layers.Dense(5)(x)\r\n\r\nmodel = tf.keras.Model(inputs, outputs)\r\n\r\nassert model.losses == []\r\n\r\nl2_regularizer = tf.keras.regularizers.l2(1e-4)\r\nmodel.add_loss(lambda: l2_regularizer(model.layers[1].kernel))\r\nmodel.add_loss(lambda: l2_regularizer(model.layers[2].kernel))\r\nmodel.add_loss(lambda: l2_regularizer(model.layers[3].kernel))\r\nmodel.add_loss(lambda: l2_regularizer(model.layers[4].kernel))\r\n        \r\nprint(model.losses)\r\n```\r\noutput:\r\n```\r\n[<tf.Tensor: shape=(), dtype=float32, numpy=0.0018836524>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.0012256705>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.0007460108>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.0004328331>]\r\n```\r\n\r\nI am aware that i can always set the `kernel_regularizer` in layer's arg while building the model, but in this case i wanted to add a regularization loss to the model that was already built.\r\n ", "comments": ["Update:\r\nThis seems to work for now, but is it the right way to do it?\r\n```\r\ninputs = tf.keras.Input(shape=[100])\r\nx = tf.keras.layers.Dense(10)(inputs)\r\nx = tf.keras.layers.Dense(15)(x)\r\nx = tf.keras.layers.Dense(5)(x)\r\noutputs = tf.keras.layers.Dense(5)(x)\r\n\r\nmodel = tf.keras.Model(inputs, outputs)\r\n\r\nassert model.losses == []\r\n\r\nl2_regularizer = tf.keras.regularizers.l2(1e-4)\r\n\r\ndef add_l2_regularization(layer):\r\n    def _add_l2_regularization():\r\n        l2 = tf.keras.regularizers.l2(1e-4)\r\n        return l2(layer.kernel)\r\n    return _add_l2_regularization\r\n\r\nfor i in range(len(model.layers)):\r\n    layer = model.layers[i]\r\n    if isinstance(layer, tf.keras.layers.Dense):\r\n        model.add_loss(add_l2_regularization(layer))\r\n\r\nmodel.losses\r\n```\r\noutput:\r\n```\r\n[<tf.Tensor: shape=(), dtype=float32, numpy=0.001901342>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.0011366734>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.00079565955>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.0005400869>]\r\n```", "@srihari-humbarwadi \r\n\r\nI am trying to reproduce the issue and i am seeing the below error`(NameError: name 'add_l2_loss' is not defined`) in the updated code.Can you please help me with the reproducible code. Thanks!", "@ravikyram I have updated the code snippet, please take a look", "I have tried on colab with TF version 2.2.0-rc3 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/3b99b38929a3bc5c06eb1a2a8f8703d2/untitled831.ipynb).Thanks!", "@pavithrasv any updates?", "i meet the same problem,why will bug happens in the code of the first version", "Had the same issue. The reason is quite tricky !\r\nIt is related to the way lambda handles variables defined in outer scope. It is explained here : \r\n\r\nhttps://docs.python.org/3/faq/programming.html#why-do-lambdas-defined-in-a-loop-with-different-values-all-return-the-same-result\r\n\r\nIt means that in our case, the value of variable \"layer\" used by any lambda when model.losses is called is always the last value taken by the variable \"layer\" in the for loop.\r\n\r\nSo if we do that :         \r\n`model.add_loss(lambda x=layer: l2_regularizer(x.kernel))`\r\ninstead of \r\n` model.add_loss(lambda: l2_regularizer(layer.kernel))`\r\n\r\nit works ! :exploding_head: :joy:  \r\n \r\n\r\nHowever, @srihari-humbarwadi solution is better because using a parameter in the lambda function seems to cause serialization issues when calling tf.saved_model.save (https://stackoverflow.com/questions/64538733/cannot-save-a-model-when-a-regularizer-is-added-to-a-model-from-tf-keras-applica).", "Still an issue in TF 2.5 as well. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/9b0604c74ed8f45f8609445130873245/untitled.ipynb).", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38891\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38891\">No</a>\n"]}, {"number": 38890, "title": "tensorflow lite build for imx6 with poky toolchain", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n\r\n**step 1:** \r\n**Clone: Clone branch r1.12**\r\ngit clone https://github.com/tensorflow/tensorflow.git -b r1.12\r\n\r\nalexvatti@SYR-DEEPLEARN2:~/AlexBuildLocation/jadak/April_24/tensorflowlite/tensorflow$ .**/tensorflow/contrib/lite/tools/make/download_dependencies.sh** \r\n\r\n\r\n**Cross Compilation / Build for i.MX6 with Poky Tool chain**\r\n\r\n### System information\r\n- **OS Platform and Distribution : ubuntu 18.04 [Linux SYR 4.15.0-96-generic #97-Ubuntu SMP Wed Apr 1 03:25:46 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux]\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.12\r\n- **Python version**: python 3.7\r\n- **Bazel version (if compiling from source)**: bazel release 0.26.1\r\n- **GCC/Compiler version (if compiling from source)**: COLLECT_GCC=arm-poky-linux-gnueabi-gcc , gcc version 9.2.0 (GCC) \r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n**arm-poky-linux-gnueabi-gcc**\r\nCOLLECT_GCC=arm-poky-linux-gnueabi-gcc\r\nCOLLECT_LTO_WRAPPER=/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr/libexec/arm-poky-linux-gnueabi/gcc/arm-poky-linux-gnueabi/9.2.0/lto-wrapper\r\nTarget: arm-poky-linux-gnueabi\r\nConfigured with: ../../../../../../work-shared/gcc-9.2.0-r0/gcc-9.2.0/configure --build=x86_64-linux --host=x86_64-pokysdk-linux --target=arm-poky-linux-gnueabi --prefix=/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr --exec_prefix=/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr --bindir=/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr/bin/arm-poky-linux-gnueabi --sbindir=/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr/bin/arm-poky-linux-gnueabi --libexecdir=/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr/libexec/arm-poky-linux-gnueabi --datadir=/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr/share --sysconfdir=/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/etc --sharedstatedir=/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/com --localstatedir=/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/var --libdir=/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr/lib/arm-poky-linux-gnueabi --includedir=/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr/include --oldincludedir=/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr/include --infodir=/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr/share/info --mandir=/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr/share/man --disable-silent-rules --disable-dependency-tracking --with-libtool-sysroot=/media/samsungQVO2TB/alexvatti/jadak/yocto/build/tmp/work/x86_64-nativesdk-pokysdk-linux/gcc-cross-canadian-arm/9.2.0-r0/recipe-sysroot --with-gnu-ld --enable-shared --enable-languages=c,c++ --enable-threads=posix --enable-multilib --enable-default-pie --enable-c99 --enable-long-long --enable-symvers=gnu --enable-libstdcxx-pch --program-prefix=arm-poky-linux-gnueabi- --without-local-prefix --enable-lto --disable-libssp --enable-libitm --disable-bootstrap --disable-libmudflap --with-system-zlib --with-linker-hash-style=gnu --enable-linker-build-id --with-ppl=no --with-cloog=no --enable-checking=release --enable-cheaders=c_global --without-isl --with-gxx-include-dir=/not/exist/usr/include/c++/9.2.0 --with-build-time-tools=/media/samsungQVO2TB/alexvatti/jadak/yocto/build/tmp/work/x86_64-nativesdk-pokysdk-linux/gcc-cross-canadian-arm/9.2.0-r0/recipe-sysroot-native/usr/arm-poky-linux-gnueabi/bin --with-sysroot=/not/exist --with-build-sysroot=/media/samsungQVO2TB/alexvatti/jadak/yocto/build/tmp/work/x86_64-nativesdk-pokysdk-linux/gcc-cross-canadian-arm/9.2.0-r0/recipe-sysroot --enable-poison-system-directories --disable-static --enable-nls --with-glibc-version=2.28 --enable-initfini-array\r\ngcc version 9.2.0 (GCC)\r\n\r\n\r\n**Describe the problem**\r\n\r\nalexvatti@SYR-DEEPLEARN2:~/AlexBuildLocation/jadak/April_24/tensorflowlite/tensorflow$ ./tensorflow/contrib/lite/tools/make/build_rpi_lib.sh\r\n+ set -e\r\n+++ dirname ./tensorflow/contrib/lite/tools/make/build_rpi_lib.sh\r\n++ cd ./tensorflow/contrib/lite/tools/make\r\n++ pwd\r\n+ SCRIPT_DIR=/home/alexvatti/AlexBuildLocation/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make\r\n+ cd /home/alexvatti/AlexBuildLocation/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/../../../../..\r\n+ CC_PREFIX=arm-poky-linux-gnueabi-\r\n+ make -j 3 -f tensorflow/contrib/lite/tools/make/Makefile TARGET=rpi TARGET_ARCH=armv7l\r\n/bin/sh: 1: [[: not found\r\narm-poky-linux-gnueabi-g++ -O3 -DNDEBUG --std=c++11 -march=armv7-a -mfpu=neon -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/../../../../../ -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/../../../../../../ -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/ -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/eigen -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/absl -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/gemmlowp -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/neon_2_sse -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/farmhash/src -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/contrib/lite/allocation.cc -o /media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/allocation.o\r\narm-poky-linux-gnueabi-g++ -O3 -DNDEBUG --std=c++11 -march=armv7-a -mfpu=neon -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/../../../../../ -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/../../../../../../ -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/ -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/eigen -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/absl -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/gemmlowp -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/neon_2_sse -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/farmhash/src -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/contrib/lite/arena_planner.cc -o /media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/arena_planner.o\r\narm-poky-linux-gnueabi-gcc -O3 -DNDEBUG -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/../../../../../ -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/../../../../../../ -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/ -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/eigen -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/absl -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/gemmlowp -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/neon_2_sse -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/farmhash/src -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/contrib/lite/c/c_api_internal.c -o /media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/c/c_api_internal.o\r\nIn file included from ./tensorflow/contrib/lite/c/c_api_internal.h:34,\r\n                 from tensorflow/contrib/lite/c/c_api_internal.c:16:\r\n/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr/lib/arm-poky-linux-gnueabi/gcc/arm-poky-linux-gnueabi/9.2.0/include/stdint.h:9:16: fatal error: stdint.h: No such file or directory\r\n    9 | # include_next <stdint.h>\r\n      |                ^~~~~~~~~~\r\ncompilation terminated.\r\ntensorflow/contrib/lite/tools/make/Makefile:173: recipe for target '/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/c/c_api_internal.o' failed\r\nmake: *** [/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/c/c_api_internal.o] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\nIn file included from tensorflow/contrib/lite/allocation.cc:16:\r\n./tensorflow/contrib/lite/allocation.h:20:10: fatal error: cstdio: No such file or directory\r\n   20 | #include <cstdio>\r\n      |          ^~~~~~~~\r\nIn file included from tensorflow/contrib/lite/arena_planner.cc:15:\r\n./tensorflow/contrib/lite/arena_planner.h:18:10: fatal error: memory: No such file or directory\r\n   18 | #include <memory>\r\n      |          ^~~~~~~~\r\ncompilation terminated.\r\ncompilation terminated.\r\ntensorflow/contrib/lite/tools/make/Makefile:169: recipe for target '/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/arena_planner.o' failed\r\nmake: *** [/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/arena_planner.o] Error 1\r\ntensorflow/contrib/lite/tools/make/Makefile:169: recipe for target '/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/allocation.o' failed\r\nmake: *** [/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflowlite/tensorflow/tensorflow/contrib/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/contrib/lite/allocation.o] Error 1\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n**cat ./tensorflow/contrib/lite/tools/make/build_rpi_lib.sh**\r\n#!/bin/bash -x\r\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\r\nset -e\r\n\r\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\r\ncd \"$SCRIPT_DIR/../../../../..\"\r\n\r\nCC_PREFIX=arm-poky-linux-gnueabi- make -j 3 -f tensorflow/contrib/lite/tools/make/Makefile TARGET=rpi TARGET_ARCH=armv7l\r\n\r\n**alexvatti@SYR-DEEPLEARN2:~/AlexBuildLocation/jadak/April_24/tensorflowlite/tensorflow$ cat tensorflow/contrib/lite/tools/make/targets/rpi_makefile.inc**\r\n# Settings for Raspberry Pi.\r\nifeq ($(TARGET),rpi)\r\n  # Default to the architecture used on the Pi Two/Three (ArmV7), but override this\r\n  # with TARGET_ARCH=armv6 to build for the Pi Zero or One.\r\n  TARGET_ARCH := armv7l\r\n  **TARGET_TOOLCHAIN_PREFIX := arm-poky-linux-gnueabi-**\r\n\r\n  ifeq ($(TARGET_ARCH), armv7l)\r\n    CXXFLAGS += \\\r\n      -march=armv7-a \\\r\n      -mfpu=neon \\\r\n      -funsafe-math-optimizations \\\r\n      -ftree-vectorize \\\r\n      -fPIC\r\n\r\n\r\n**alexvatti@SYR-DEEPLEARN2:~/AlexBuildLocation/jadak/April_24/tensorflowlite/tensorflow$ ./tensorflow/contrib/lite/tools/make/build_rpi_lib.sh**\r\n\r\n\r\n**Any other info / logs**\r\n\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["[tensorflow build issue information.txt](https://github.com/tensorflow/tensorflow/files/4533125/tensorflow.build.issue.information.txt)\r\n", "Can't you just use crossbuild-essential-armhf ? Why do you need the poky toolchain?", "We want to integrate this with our poky build system", "Can't you simply use the following command?\r\n\r\n```sh\r\nmake -f tensorflow/lite/tools/make/Makefile TARGET_TOOLCHAIN_PREFIX=arm-poky-linux-gnueabi-\r\n```", "\r\n**Step 1:**\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\n\r\n**Step 2:**\r\ncd  tensorflow\r\n make -f tensorflow/lite/tools/make/Makefile TARGET_TOOLCHAIN_PREFIX=arm-poky-linux-gnueabi-\r\nfind: \u2018tensorflow/lite/tools/make/downloads/absl/absl/\u2019: No such file or directory\r\narm-poky-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11  -DTFLITE_WITHOUT_XNNPACK -fPIC -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -pthread -I. -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/ -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/ruy -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/allocation.cc -o /media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/allocation.o\r\nIn file included from tensorflow/lite/allocation.cc:16:\r\n./tensorflow/lite/allocation.h:20:10: fatal error: cstdio: No such file or directory\r\n   20 | #include <cstdio>\r\n      |          ^~~~~~~~\r\ncompilation terminated.\r\ntensorflow/lite/tools/make/Makefile:300: recipe for target '/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/allocation.o' failed\r\nmake: *** [/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/allocation.o] Error 1\r\n\r\n", "You forgot the following step.\r\n\r\n```\r\n./tensorflow/lite/tools/make/download_dependencies.sh\r\n```\r\n\r\nhttps://www.tensorflow.org/lite/guide/build_arm64", "**alexvatti@SYR-DEEPLEARN2:~/AlexBuildLocation/jadak/April_24/tensorflow_april_30/tensorflow$ ./tensorflow/lite/tools/make/download_dependencies.sh**\r\ndownloading https://gitlab.com/libeigen/eigen/-/archive/1e41406c362788057b3adcd9a25b73f43e6e6492/eigen-1e41406c362788057b3adcd9a25b73f43e6e6492.tar.gz\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100 2548k  100 2548k    0     0  5377k      0 --:--:-- --:--:-- --:--:-- 5366k\r\nchecking sha256 of tensorflow/lite/tools/make/downloads/eigen\r\n/tmp/tmp.lpXZhmjtrB/eigen-1e41406c362788057b3adcd9a25b73f43e6e6492.tar.gz: OK\r\ndownloading https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/gemmlowp/archive/12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3.zip\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100  913k  100  913k    0     0  1753k      0 --:--:-- --:--:-- --:--:-- 1753k\r\nchecking sha256 of tensorflow/lite/tools/make/downloads/gemmlowp\r\n/tmp/tmp.mLt7mg52UI/12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3.zip: OK\r\nArchive:  /tmp/tmp.mLt7mg52UI/12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3.zip\r\n12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3\r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/.gitignore  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/.travis.yml  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/AUTHORS  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/BUILD  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/CONTRIBUTING  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/CONTRIBUTORS  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/LICENSE  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/Makefile.travis  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/README.md  \r\n extracting: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/WORKSPACE  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/contrib/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/contrib/CMakeLists.txt  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/doc/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/doc/design.md  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/doc/kernel.md  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/doc/less-than-8-bit.md  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/doc/low-precision.md  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/doc/output.md  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/doc/packing.md  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/doc/public.md  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/doc/quantization.md  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/doc/quantization_example.cc  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/eight_bit_int_gemm/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/eight_bit_int_gemm/eight_bit_int_gemm.cc  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/eight_bit_int_gemm/eight_bit_int_gemm.h  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/fixedpoint/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/fixedpoint/fixedpoint.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/fixedpoint/fixedpoint_avx.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/fixedpoint/fixedpoint_msa.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/fixedpoint/fixedpoint_neon.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/fixedpoint/fixedpoint_sse.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/flags.bzl  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/allocator.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/block_params.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/common.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/compute.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/detect_platform.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/dispatch_gemm_shape.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/kernel.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/kernel_avx.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/kernel_default.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/kernel_msa.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/kernel_neon.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/kernel_reference.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/kernel_sse.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/multi_thread_gemm.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/output.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/output_avx.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/output_msa.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/output_neon.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/output_sse.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/pack.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/pack_avx.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/pack_msa.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/pack_neon.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/pack_sse.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/platform.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/simd_wrappers.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/simd_wrappers_common_neon_sse.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/simd_wrappers_msa.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/simd_wrappers_neon.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/simd_wrappers_sse.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/single_thread_gemm.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/internal/unpack.h  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/jni/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/jni/Android.mk  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/jni/Application.mk  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/README  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/base.h  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/generators/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/generators/cc_emitter.py  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/generators/common.py  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/generators/metagemm_generate_headers.sh  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/generators/neon_emitter.py  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/generators/neon_emitter_64.py  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/generators/quantized_mul_kernels_arm_32.py  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/generators/quantized_mul_kernels_arm_64.py  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/generators/quantized_mul_kernels_common.py  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/generators/streams_arm_32.py  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/generators/streams_arm_64.py  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/generators/streams_common.py  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/generators/transform_kernels_arm_32.py  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/generators/transform_kernels_arm_64.py  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/generators/transform_kernels_common.py  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/legacy_multi_thread_common.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/legacy_multi_thread_gemm.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/legacy_multi_thread_gemv.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/legacy_operations_common.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/legacy_single_thread_gemm.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/multi_thread_common.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/multi_thread_gemm.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/multi_thread_transform.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/quantized_mul_kernels.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/quantized_mul_kernels_arm_32.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/quantized_mul_kernels_arm_64.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/single_thread_gemm.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/single_thread_transform.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/streams.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/streams_arm_32.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/streams_arm_64.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/test_gemm_correctness.cc  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/test_streams_correctness.cc  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/test_transform_benchmark.cc  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/test_transform_correctness.cc  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/transform_kernels.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/transform_kernels_arm_32.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/meta/transform_kernels_arm_64.h  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/profiling/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/profiling/instrumentation.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/profiling/profiler.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/profiling/pthread_everywhere.h  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/public/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/public/bit_depth.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/public/gemmlowp.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/public/map.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/public/output_stages.h  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/scripts/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/scripts/ci-before.sh  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/scripts/ci-test.sh  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/scripts/test-android.sh  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/standalone/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/standalone/cache_counters.cc  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/standalone/encode.py  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/standalone/neon-gemm-kernel-benchmark.cc  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/benchmark.cc  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/benchmark_all_sizes.cc  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/benchmark_meta_gemm.cc  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/correctness_meta_gemm.cc  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/\r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test.xcodeproj/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test.xcodeproj/project.pbxproj  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/AppDelegate.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/AppDelegate.mm  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/Base.lproj/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/Base.lproj/LaunchScreen.xib  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/Base.lproj/Main.storyboard  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/Images.xcassets/\r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/Images.xcassets/AppIcon.appiconset/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/Images.xcassets/AppIcon.appiconset/Contents.json  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/Info.plist  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/ViewController.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/ViewController.m  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/ios/gemmlowp_test/main.m  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test.cc  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test_allocator.cc  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test_blocking_counter.cc  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test_data.cc  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test_data.h  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test_fixedpoint.cc  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/test/test_math_helpers.cc  \r\n   creating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/\r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/armv8-64bit-kernel-for-less-than-8-bit.txt  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/error-diffusion-experiments.txt  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/fast-gemv.txt  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/less-than-8-bit-without-requantization.txt  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/multi-threading-experiments.txt  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/neon-depth-major-sources-packing.txt  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/remove-default-template-param-values.txt  \r\n  inflating: /tmp/tmp.LEpjWhHxa7/gemmlowp-12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3/todo/x86-kernels.txt  \r\ndownloading https://github.com/google/ruy/archive/4bdb31ab484e624deef9620ecde2156ca17f6567.zip\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100   149  100   149    0     0    615      0 --:--:-- --:--:-- --:--:--   615\r\n100  298k    0  298k    0     0   462k      0 --:--:-- --:--:-- --:--:--  462k\r\nchecking sha256 of tensorflow/lite/tools/make/downloads/ruy\r\n/tmp/tmp.AsllFhf7CX/4bdb31ab484e624deef9620ecde2156ca17f6567.zip: OK\r\nArchive:  /tmp/tmp.AsllFhf7CX/4bdb31ab484e624deef9620ecde2156ca17f6567.zip\r\n4bdb31ab484e624deef9620ecde2156ca17f6567\r\n   creating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/\r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/BUILD  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/CONTRIBUTING.md  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/LICENSE  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/README.md  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/WORKSPACE  \r\n   creating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/\r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/BUILD  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/allocator.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/allocator.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/allocator_test.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/benchmark.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/block_map.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/block_map.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/block_map_test.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/blocking_counter.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/blocking_counter.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/build_defs.oss.bzl  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/check_macros.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/check_macros_test.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/common.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/context.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/context.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/context_get_ctx.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/context_get_ctx.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/context_test.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/cpu_cache_size.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/ctx.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/ctx.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/ctx_impl.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/ctx_test.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/detect_arm.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/detect_arm.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/detect_x86.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/detect_x86.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/dispatch.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/example.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/example_advanced.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/gtest_wrapper.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/have_built_path_for.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/have_built_path_for_avx2.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/have_built_path_for_avx512.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/have_built_path_for_avxvnni.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/have_built_path_for_sse42.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/kernel.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/kernel_arm.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/kernel_arm32.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/kernel_arm64.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/kernel_avx2.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/kernel_avx512.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/kernel_avxvnni.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/kernel_common.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/kernel_sse42.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/kernel_x86.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/mat.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/matrix.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/matrix_test.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/mul_params.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/mul_params_test.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/opt_set.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/pack.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/pack_arm.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/pack_arm.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/pack_avx2.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/pack_avx512.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/pack_avxvnni.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/pack_common.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/pack_sse42.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/pack_x86.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/path.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/platform.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/pmu.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/pmu.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/prepack.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/prepacked_cache.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/prepacked_cache.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/prepacked_cache_test.cc  \r\n   creating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/profiler/\r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/profiler/BUILD  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/profiler/README.md  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/profiler/instrumentation.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/profiler/instrumentation.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/profiler/profiler.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/profiler/profiler.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/profiler/test.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/profiler/test_instrumented_library.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/profiler/test_instrumented_library.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/profiler/treeview.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/profiler/treeview.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/ruy.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/ruy_advanced.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/ruy_test.bzl  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/ruy_test_ext.oss.bzl  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/side_pair.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/size_util.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/size_util_test.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/test.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/test_fast.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/test_slow.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/test_special_mul_params.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/thread_pool.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/thread_pool.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/time.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/trace.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/trace.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/trmul.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/trmul.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/trmul_params.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/tune.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/tune.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/tune_test.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/tune_tool.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/wait.cc  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/wait.h  \r\n  inflating: /tmp/tmp.x8k44HEiWY/ruy-4bdb31ab484e624deef9620ecde2156ca17f6567/ruy/wait_test.cc  \r\ndownloading https://github.com/google/googletest/archive/release-1.8.0.tar.gz\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100   132  100   132    0     0    758      0 --:--:-- --:--:-- --:--:--   758\r\n100 1251k  100 1251k    0     0  3136k      0 --:--:-- --:--:-- --:--:-- 8752k\r\nchecking sha256 of tensorflow/lite/tools/make/downloads/googletest\r\n/tmp/tmp.EdKaDRYCVF/release-1.8.0.tar.gz: OK\r\ndownloading https://github.com/abseil/abseil-cpp/archive/df3ea785d8c30a9503321a3d35ee7d35808f190d.tar.gz\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100   159  100   159    0     0    913      0 --:--:-- --:--:-- --:--:--   913\r\n100 1640k    0 1640k    0     0  2519k      0 --:--:-- --:--:-- --:--:-- 2519k\r\nchecking sha256 of tensorflow/lite/tools/make/downloads/absl\r\n/tmp/tmp.l2pEb36bVd/df3ea785d8c30a9503321a3d35ee7d35808f190d.tar.gz: OK\r\ndownloading https://github.com/intel/ARM_NEON_2_x86_SSE/archive/master.zip\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100   129  100   129    0     0    697      0 --:--:-- --:--:-- --:--:--   693\r\n100  103k    0  103k    0     0   231k      0 --:--:-- --:--:-- --:--:--  231k\r\nArchive:  /tmp/tmp.hGUfqefJlk/master.zip\r\n0f4e857421c964826def1820bbfe0707f73ebda4\r\n   creating: /tmp/tmp.icT1z8uY4F/ARM_NEON_2_x86_SSE-master/\r\n  inflating: /tmp/tmp.icT1z8uY4F/ARM_NEON_2_x86_SSE-master/.gitignore  \r\n  inflating: /tmp/tmp.icT1z8uY4F/ARM_NEON_2_x86_SSE-master/CMakeLists.txt  \r\n  inflating: /tmp/tmp.icT1z8uY4F/ARM_NEON_2_x86_SSE-master/LICENSE  \r\n  inflating: /tmp/tmp.icT1z8uY4F/ARM_NEON_2_x86_SSE-master/NEON_2_SSE.h  \r\n  inflating: /tmp/tmp.icT1z8uY4F/ARM_NEON_2_x86_SSE-master/ReadMe.md  \r\n   creating: /tmp/tmp.icT1z8uY4F/ARM_NEON_2_x86_SSE-master/cmake/\r\n extracting: /tmp/tmp.icT1z8uY4F/ARM_NEON_2_x86_SSE-master/cmake/Config.cmake.in  \r\ndownloading https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/farmhash/archive/816a4ae622e964763ca0862d9dbd19324a1eaf45.tar.gz\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100  456k  100  456k    0     0  2388k      0 --:--:-- --:--:-- --:--:-- 2388k\r\nchecking sha256 of tensorflow/lite/tools/make/downloads/farmhash\r\n/tmp/tmp.41LAsAa4bl/816a4ae622e964763ca0862d9dbd19324a1eaf45.tar.gz: OK\r\ndownloading https://github.com/google/flatbuffers/archive/v1.12.0.tar.gz\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100   127  100   127    0     0    729      0 --:--:-- --:--:-- --:--:--   729\r\n100 1118k    0 1118k    0     0  2206k      0 --:--:-- --:--:-- --:--:-- 8502k\r\nchecking sha256 of tensorflow/lite/tools/make/downloads/flatbuffers\r\n/tmp/tmp.sZt6J4iz8r/v1.12.0.tar.gz: OK\r\ndownloading https://storage.googleapis.com/mirror.tensorflow.org/www.kurims.kyoto-u.ac.jp/~ooura/fft2d.tgz\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100 54434  100 54434    0     0   442k      0 --:--:-- --:--:-- --:--:--  442k\r\nchecking sha256 of tensorflow/lite/tools/make/downloads/fft2d\r\n/tmp/tmp.QdppuML9LS/fft2d.tgz: OK\r\ndownloading https://github.com/Maratyszcza/FP16/archive/febbb1c163726b5db24bed55cc9dc42529068997.zip\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100   155  100   155    0     0    890      0 --:--:-- --:--:-- --:--:--   895\r\n100 91243    0 91243    0     0   217k      0 --:--:-- --:--:-- --:--:--  217k\r\nArchive:  /tmp/tmp.wFxucOCVWn/febbb1c163726b5db24bed55cc9dc42529068997.zip\r\nfebbb1c163726b5db24bed55cc9dc42529068997\r\n   creating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/\r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/.gitignore  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/CMakeLists.txt  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/LICENSE  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/README.md  \r\n   creating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/bench/\r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/bench/alt-element.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/bench/from-alt-array.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/bench/from-ieee-array.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/bench/ieee-element.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/bench/to-alt-array.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/bench/to-ieee-array.cc  \r\n   creating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/cmake/\r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/cmake/DownloadGoogleBenchmark.cmake  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/cmake/DownloadGoogleTest.cmake  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/cmake/DownloadPSimd.cmake  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/configure.py  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/confu.yaml  \r\n   creating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/include/\r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/include/fp16.h  \r\n   creating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/include/fp16/\r\n extracting: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/include/fp16/__init__.py  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/include/fp16/avx.py  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/include/fp16/avx2.py  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/include/fp16/bitcasts.h  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/include/fp16/fp16.h  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/include/fp16/psimd.h  \r\n   creating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/jni/\r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/jni/Android.mk  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/jni/Application.mk  \r\n   creating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/\r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/alt-from-fp32-value.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/alt-to-fp32-bits.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/alt-to-fp32-psimd.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/alt-to-fp32-value.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/alt-to-fp32x2-psimd.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/bitcasts.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/ieee-from-fp32-value.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/ieee-to-fp32-bits.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/ieee-to-fp32-psimd.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/ieee-to-fp32-value.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/ieee-to-fp32x2-psimd.cc  \r\n   creating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/peachpy/\r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/peachpy/alt-xmm-to-fp32-xmm-avx.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/peachpy/alt-xmm-to-fp32-ymm-avx2.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/peachpy/stubs.py  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/tables.cc  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/test/tables.h  \r\n   creating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/third-party/\r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/third-party/THHalf.h  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/third-party/eigen-half.h  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/third-party/float16-compressor.h  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/third-party/half.hpp  \r\n  inflating: /tmp/tmp.tgSZQiXJaC/FP16-febbb1c163726b5db24bed55cc9dc42529068997/third-party/npy-halffloat.h  \r\ndownload_dependencies.sh completed successfully.\r\nalexvatti@SYR-DEEPLEARN2:~/AlexBuildLocation/jadak/April_24/tensorflow_april_30/tensorflow$ make -f tensorflow/lite/tools/make/Makefile TARGET_TOOLCHAIN_PREFIX=arm-poky-linux-gnueabi-\r\narm-poky-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11  -DTFLITE_WITHOUT_XNNPACK -fPIC -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -pthread -I. -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/ -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/ruy -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/allocation.cc -o /media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/allocation.o\r\nIn file included from tensorflow/lite/allocation.cc:16:\r\n./tensorflow/lite/allocation.h:20:10: fatal error: cstdio: No such file or directory\r\n   20 | #include <cstdio>\r\n      |          ^~~~~~~~\r\ncompilation terminated.\r\ntensorflow/lite/tools/make/Makefile:300: recipe for target '/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/allocation.o' failed\r\nmake: *** [/media/samsungQVO2TB/alexvatti/jadak/April_24/tensorflow_april_30/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/allocation.o] Error 1\r\n", "cstdio is a standard C++ header file.\r\nhttp://www.cplusplus.com/reference/cstdio/\r\nYou'd better check if your toolchain supports C++ compilation correctly.", "diff --git a/tensorflow/lite/tools/make/Makefile b/tensorflow/lite/tools/make/Makefile\r\nindex ad3832f996..6b1bedb0dd 100644\r\n--- a/tensorflow/lite/tools/make/Makefile\r\n+++ b/tensorflow/lite/tools/make/Makefile\r\n@@ -44,7 +44,7 @@ INCLUDES := \\\r\n -I$(OBJDIR)\r\n # This is at the end so any globally-installed frameworks like protobuf don't\r\n # override local versions in the source tree.\r\n-INCLUDES += -I/usr/local/include\r\n+INCLUDES += -I/usr/local/include -I/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/include/c++/9.2.0 -I/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/include/c++/9.2.0/arm-poky-linux-gnueabi -I/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/include\r\n", "while linking , facing the error ( libs did not find) ", "Step 1:\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\n\r\nStep 2:\r\n./tensorflow/lite/tools/make/download_dependencies.sh\r\n\r\nStep 3:\r\n\r\nalexvatti@SYR-DEEPLEARN2:~/AlexBuildLocation/jadak/April_30/tensorflow/imx6/tensorflow$ git diff\r\ndiff --git a/tensorflow/lite/tools/make/Makefile b/tensorflow/lite/tools/make/Makefile\r\nindex ad3832f996..d669a44be2 100644\r\n--- a/tensorflow/lite/tools/make/Makefile\r\n+++ b/tensorflow/lite/tools/make/Makefile\r\n@@ -44,11 +44,13 @@ INCLUDES := \\\r\n -I$(OBJDIR)\r\n # This is at the end so any globally-installed frameworks like protobuf don't\r\n # override local versions in the source tree.\r\n-INCLUDES += -I/usr/local/include\r\n-\r\n+INCLUDES += -I/usr/local/include -I/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/include/c++/9.2.0 -I/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/include/c++/9.2.0/arm-poky-linux-gnueabi -I/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/include\r\n # These are the default libraries needed, but they can be added to or\r\n # overridden by the platform-specific settings in target makefiles.\r\n LIBS := \\\r\n+-L/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/lib \\\r\n+-L/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/lib \\\r\n+-L/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/lib/arm-poky-linux-gnueabi/9.2.0 \\\r\n -lstdc++ \\\r\n -lpthread \\\r\n -lm \\\r\n\r\n\r\nStep 4:\r\nmake -f tensorflow/lite/tools/make/Makefile TARGET=imx6 TARGET_TOOLCHAIN_PREFIX=arm-poky-linux-gnueabi-\r\n\r\nStep 5:\r\n\r\narm-poky-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11  -DTFLITE_WITHOUT_XNNPACK -I. -I/media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/downloads/ -I/media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/downloads/ruy -I/media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -I/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/include/c++/9.2.0 -I/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/include/c++/9.2.0/arm-poky-linux-gnueabi -I/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/include \\\r\n-o /media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/gen/imx6_x86_64/bin/minimal /media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/gen/imx6_x86_64/obj/tensorflow/lite/examples/minimal/minimal.o \\\r\n /media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/gen/imx6_x86_64/lib/libtensorflow-lite.a -Wl,-O1 -Wl,--hash-style=gnu -Wl,--as-needed -L/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/lib -L/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/lib -L/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/lib/arm-poky-linux-gnueabi/9.2.0 -lstdc++ -lpthread -lm -lz -ldl\r\n/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr/libexec/arm-poky-linux-gnueabi/gcc/arm-poky-linux-gnueabi/9.2.0/real-ld: cannot find Scrt1.o: No such file or directory\r\n/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr/libexec/arm-poky-linux-gnueabi/gcc/arm-poky-linux-gnueabi/9.2.0/real-ld: cannot find crti.o: No such file or directory\r\n/opt/poky/3.0.2/sysroots/x86_64-pokysdk-linux/usr/libexec/arm-poky-linux-gnueabi/gcc/arm-poky-linux-gnueabi/9.2.0/real-ld: cannot find crtbeginS.o: No such file or directory\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/tools/make/Makefile:331: recipe for target '/media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/gen/imx6_x86_64/bin/minimal' failed\r\nmake: *** [/media/samsungQVO2TB/alexvatti/jadak/April_30/tensorflow/imx6/tensorflow/tensorflow/lite/tools/make/gen/imx6_x86_64/bin/minimal] Error 1\r\n", "\r\nEven though: added this\r\n**-L/opt/poky/3.0.2/sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/lib** \r\n\r\nalexvatti@SYR-DEEPLEARN2:/opt/poky/3.0.2$ find ./ -iname  \"Scrt1.o\"\r\n./sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/lib/Scrt1.o\r\n\r\nalexvatti@SYR-DEEPLEARN2:/opt/poky/3.0.2$ find ./ -iname  \"crti.o\"\r\n./sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/lib/crti.o\r\n\r\nalexvatti@SYR-DEEPLEARN2:/opt/poky/3.0.2$ find ./ -iname  \"crtbeginS.o\"\r\n./sysroots/cortexa7t2hf-neon-poky-linux-gnueabi/usr/lib/arm-poky-linux-gnueabi/9.2.0/crtbeginS.o\r\n\r\nalexvatti@SYR-DEEPLEARN2:/opt/poky/3.0.2$ \r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Using CMake should be easier than Makefile to use custom toolchains.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake_arm", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38890\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38890\">No</a>\n"]}, {"number": 38889, "title": "why does tensorflow2 use multiple Gpu but only one is used, the other always can not use", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nWhen I run my code to train, I found only one gpu is used while the others can not fully used. Exactly, for example, when the first gpu almost out of memory,  the code can not use the memory of other gpus. I don't know if it is a bug?", "comments": ["@tensorflow help!", "By default, tensorflow uses the `GPU:0` as the default GPU. I would suggest first check the available GPUs using `tf.config.list_physical_devices` to see the number of GPUs available. \r\n\r\nIf you have more than one GPU available then you need to do distributed GPU training. \r\n\r\nThis can be easily done by defining a `strategy.scope()` These scopes are given in [this](https://www.tensorflow.org/guide/distributed_training) documentation link.  \r\n\r\nPlease use a suitable strategy as per needs. In the simple case of having one machine with multiple GPUs, I guess the `tf.distribute.MirroredStrategy` will be useful. Please check the above docs for more info.", "@KurtSunxx \r\nCould you please update as per above comment.", "Sorry.I forget release my setting.\r\n\r\nCUDA:CUDA10.0\r\n\r\nCUDNN:cudnn 7.6.5\r\n\r\nTensorflow:2.0\r\n\r\ngcc:5.4\r\n\r\nmy code like the function: train_step() in https://tensorflow.google.cn/tutorials/generative/pix2pix\r\n\r\nbut in my train_step, I call a model(named classifier) in function A, while there still call the same model in function B of function A.So in short, the flow chart of my code is like the following:\r\n\r\n\r\n`def func2()`\r\n    `.....`\r\n    `model()`\r\n    `.....`\r\n`def func1()`\r\n    `.....`\r\n    `func2`\r\n    `model()`\r\n    `.....`\r\n`def train_step():`\r\n    `.....`\r\n    `func1`\r\nThe OOM happens when use func1 in train_step.The method to build a model like https://tensorflow.google.cn/tutorials/generative/pix2pix in which, I use a tf.keras.Model class.``", "> @KurtSunxx\r\n> Could you please update as per above comment.\r\n\r\nSorry Sir. I dont know update what. Cause my english is realy poor. I just added some setting of my device and software environment. Is this information is helpfull?", "> By default, tensorflow uses the `GPU:0` as the default GPU. I would suggest first check the available GPUs using `tf.config.list_physical_devices` to see the number of GPUs available.\r\n> \r\n> If you have more than one GPU available then you need to do distributed GPU training.\r\n> \r\n> This can be easily done by defining a `strategy.scope()` These scopes are given in [this](https://www.tensorflow.org/guide/distributed_training) documentation link.\r\n> \r\n> Please use a suitable strategy as per needs. In the simple case of having one machine with multiple GPUs, I guess the `tf.distribute.MirroredStrategy` will be useful. Please check the above docs for more info.\r\n\r\nThanks Sir. I have tried this method. But it didn't work. By the way, Do you know if all the version of tensorflow(including tf1.x) also use one Gpu by default even if we set multiple GPUs to use? Cause I find that when I use tensorflow1.15, This also happens.\r\n", "> \r\n> \r\n> > By default, tensorflow uses the `GPU:0` as the default GPU. I would suggest first check the available GPUs using `tf.config.list_physical_devices` to see the number of GPUs available.\r\n> > If you have more than one GPU available then you need to do distributed GPU training.\r\n> > This can be easily done by defining a `strategy.scope()` These scopes are given in [this](https://www.tensorflow.org/guide/distributed_training) documentation link.\r\n> > Please use a suitable strategy as per needs. In the simple case of having one machine with multiple GPUs, I guess the `tf.distribute.MirroredStrategy` will be useful. Please check the above docs for more info.\r\n> \r\n> Thanks Sir. I have tried this method. But it didn't work. By the way, Do you know if all the version of tensorflow(including tf1.x) also use one Gpu by default even if we set multiple GPUs to use? Cause I find that when I use tensorflow1.15, This also happens.\r\n\r\nTensorflow does not do distributed computing by default. Even in v1 or in v2. By setting multiple GPU cards it won't mean that all will be used. I see that in your example you are writing a custom training loop instead of model.fit().\r\n\r\nAs per your system settings there is no issue with that part. Have a look at [this example ](https://tensorflow.google.cn/tutorials/distribute/custom_training) to use distributed training on custom loops.", "> > > By default, tensorflow uses the `GPU:0` as the default GPU. I would suggest first check the available GPUs using `tf.config.list_physical_devices` to see the number of GPUs available.\r\n> > > If you have more than one GPU available then you need to do distributed GPU training.\r\n> > > This can be easily done by defining a `strategy.scope()` These scopes are given in [this](https://www.tensorflow.org/guide/distributed_training) documentation link.\r\n> > > Please use a suitable strategy as per needs. In the simple case of having one machine with multiple GPUs, I guess the `tf.distribute.MirroredStrategy` will be useful. Please check the above docs for more info.\r\n> > \r\n> > \r\n> > Thanks Sir. I have tried this method. But it didn't work. By the way, Do you know if all the version of tensorflow(including tf1.x) also use one Gpu by default even if we set multiple GPUs to use? Cause I find that when I use tensorflow1.15, This also happens.\r\n> \r\n> Tensorflow does not do distributed computing by default. Even in v1 or in v2. By setting multiple GPU cards it won't mean that all will be used. I see that in your example you are writing a custom training loop instead of model.fit().\r\n> \r\n> As per your system settings there is no issue with that part. Have a look at [this example ](https://tensorflow.google.cn/tutorials/distribute/custom_training) to use distributed training on custom loops.\r\n\r\nThanks Sir! I will try it soon! Best wishes to you.", "> > > By default, tensorflow uses the `GPU:0` as the default GPU. I would suggest first check the available GPUs using `tf.config.list_physical_devices` to see the number of GPUs available.\r\n> > > If you have more than one GPU available then you need to do distributed GPU training.\r\n> > > This can be easily done by defining a `strategy.scope()` These scopes are given in [this](https://www.tensorflow.org/guide/distributed_training) documentation link.\r\n> > > Please use a suitable strategy as per needs. In the simple case of having one machine with multiple GPUs, I guess the `tf.distribute.MirroredStrategy` will be useful. Please check the above docs for more info.\r\n> > \r\n> > \r\n> > Thanks Sir. I have tried this method. But it didn't work. By the way, Do you know if all the version of tensorflow(including tf1.x) also use one Gpu by default even if we set multiple GPUs to use? Cause I find that when I use tensorflow1.15, This also happens.\r\n> \r\n> Tensorflow does not do distributed computing by default. Even in v1 or in v2. By setting multiple GPU cards it won't mean that all will be used. I see that in your example you are writing a custom training loop instead of model.fit().\r\n> \r\n> As per your system settings there is no issue with that part. Have a look at [this example ](https://tensorflow.google.cn/tutorials/distribute/custom_training) to use distributed training on custom loops.\r\n\r\nSorry Sir. maybe I can not make my idea clearly. I mean I had set 3 GPU to train, But when tensorflow need to allocate more memory, why it can not allocate the extra memory to other GPU cards I sepecified before? Is that can be solved by tf.distribute? For example, I have 3 GPUs whose memory is 11G, in fact, I found when I running my code, the first one uses almost 11G while the others usese 155M. And then tensorflow need allocate more memory, but the first one has on more memory to do. I think in this case, it should allocate memory to other GPUs, but in fact it just breakdown and return a OOM error. As far as I know, distribute training is like a parallel training while select a part of training procedure on one card and the other training procedure on the other cards. But it seems can not solve what I just asked above. I feel so puzzled.", "Okay. So I get your problem. One of your GPUs is being used too much and you want your data to be shifted to other GPUs to avoid the OOM error. \r\n\r\nQ. But when tensorflow need to allocate more memory, why it can not allocate the extra memory to other GPU cards I sepecified before?\r\n\r\nA. Tensorflow does not allocate memory to other GPUs properly or synchronously if it is not in tf.distribute. \r\n\r\nQ. Is that can be solved by tf.distribute? \r\nA. Okay, tf.distribute provides several ways of dividing your data among various GPUs. It also supports synchronous distributed trining on GPUs. \r\n`tf.distribute` will be able to handle synchronous distributed trining on GPUs which is required in your case. This will not be activated explicitly without specifying.\r\n\r\nGetting OOM error is because you are unable to fit more data / model into your GPU currently. \r\n\r\nQ. Will tf.distribute solve OOM ?\r\nA. Maybe, it depends how huge your model is also how many operations can be parellized. \r\n\r\nFrom the docs this is what MirroredStrategy will do\r\n```\r\nMirroredStrategy\r\n\r\ntf.distribute.MirroredStrategy supports synchronous distributed training on multiple GPUs on one machine.\r\n\r\nIt creates one replica per GPU device. Each variable in the model is mirrored across all the replicas.\r\n\r\nTogether, these variables form a single conceptual variable called MirroredVariable. \r\n\r\nThese variables are kept in sync with each other by applying identical updates.\r\n```\r\nplease see the [distributed training docs](https://tensorflow.google.cn/guide/distributed_training#types_of_strategies). It might solve your OOM problem but again it depends on your model size and training.\r\n\r\nBut I am sure this training strategy will be more efficient also significantly faster.\r\n", "> Okay. So I get your problem. One of your GPUs is being used too much and you want your data to be shifted to other GPUs to avoid the OOM error.\r\n> \r\n> Q. But when tensorflow need to allocate more memory, why it can not allocate the extra memory to other GPU cards I sepecified before?\r\n> \r\n> A. Tensorflow does not allocate memory to other GPUs properly or synchronously if it is not in tf.distribute.\r\n> \r\n> Q. Is that can be solved by tf.distribute?\r\n> A. Okay, tf.distribute provides several ways of dividing your data among various GPUs. It also supports synchronous distributed trining on GPUs.\r\n> `tf.distribute` will be able to handle synchronous distributed trining on GPUs which is required in your case. This will not be activated explicitly without specifying.\r\n> \r\n> Getting OOM error is because you are unable to fit more data / model into your GPU currently.\r\n> \r\n> Q. Will tf.distribute solve OOM ?\r\n> A. Maybe, it depends how huge your model is also how many operations can be parellized.\r\n> \r\n> From the docs this is what MirroredStrategy will do\r\n> \r\n> ```\r\n> MirroredStrategy\r\n> \r\n> tf.distribute.MirroredStrategy supports synchronous distributed training on multiple GPUs on one machine.\r\n> \r\n> It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas.\r\n> \r\n> Together, these variables form a single conceptual variable called MirroredVariable. \r\n> \r\n> These variables are kept in sync with each other by applying identical updates.\r\n> ```\r\n> \r\n> please see the [distributed training docs](https://tensorflow.google.cn/guide/distributed_training#types_of_strategies). It might solve your OOM problem but again it depends on your model size and training.\r\n> \r\n> But I am sure this training strategy will be more efficient also significantly faster.\r\n\r\nSo nice you are! Thanks for your detailed answers! I will check and try it by myself!", "@KurtSunxx\r\nPlease confirm if we may move this issue to closed status", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38889\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38889\">No</a>\n"]}, {"number": 38888, "title": "9", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38888) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 38887, "title": "Can we auto-convert a tensor into a numpy value when necessary in computing graph executing mode(&tf.function mode)?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (2.1): 2.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n```\r\n  def testTensorAndNumpy():\r\n    npArray=np.random.uniform(0, 10000, size=200)\r\n    #npArray=tf.random.uniform((200,), 0, 10000)\r\n    @tf.function\r\n    def myFunc(tensorValue):\r\n      return(npArray[tensorValue])\r\n\r\n    #tensorValue=3\r\n    tensorValue=tf.convert_to_tensor(3)\r\n    print(myFunc(tensorValue))\r\n    print(npArray[tensorValue])\r\n\r\n```\r\nThe above code works fine if we comment the @tf.function, but when we use @tf.function, it'll reporting the following errors:\r\n```\r\n\r\n....\r\n    NotImplementedError: Cannot convert a symbolic Tensor (tensorValue:0) to a numpy array.\r\n....\r\n```\r\nThe root cause should be that the tensorflow's computing graph executing mode couldn't auto-convert the tensor to numpy value, but when in eager mode, this conversion could happen correctly and automatically. It seems not only my test case could trigger this bug, many other bugs report also relate to this root cause.\r\n\r\n**Will this change the current api? How?**\r\nNo, it seems no need change the API spec, but we need change the implementation for tensorflow's tf.function.\r\n\r\n**Who will benefit with this feature?**\r\nMany engineers, it seems not only my test codes could trigger this error, many other tensorflow users also found similar issue. I searched \"NotImplementedError: Cannot convert a symbolic Tensor \", about 10 issues exists, I guess the root cause for most them should be same as this test failure's root cause. \r\nhttps://github.com/tensorflow/tensorflow/issues?q=NotImplementedError%3A+Cannot+convert+a+symbolic+Tensor\r\n\r\n**Any Other info.**\r\nWe could change the npArray to a tensor, and then we could use tensor as the index for another tensor, this works fine(by disabling the first line and enable the second line). But because in my usage scenario, I need use numpy data array as the data source, and use tensor variable to access different part of this numpy data array. Because this combination could help me save lots of GPU memory, the GPU memory is much more expensive than CPU memory, and all tensors malloc need consume GPU memory. So I consider this issue as a new feature request instead of a bug report. \r\nAfter implmenting this feature, engineers could feel more nature when switching between tensorflow's eager mode and graph mode, and most of those old connected bugs should also be fixed automatically.", "comments": ["Do you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "@ravikyram yes, I've use case to require this feature. The test codes I've provided could show that our current version tensorflow couldn't support this feature:\r\n\r\n```\r\n  def testTensorAndNumpy():\r\n    npArray=np.random.uniform(0, 10000, size=200)\r\n    #npArray=tf.random.uniform((200,), 0, 10000)\r\n    @tf.function\r\n    def myFunc(tensorValue):\r\n      return(npArray[tensorValue])\r\n\r\n    #tensorValue=3\r\n    tensorValue=tf.convert_to_tensor(3)\r\n    print(myFunc(tensorValue))\r\n    print(npArray[tensorValue])\r\n```\r\n\r\nBut I've no solution to implement this feature yet, so I couldn't submit a PR(pull request, right?).\r\nMy usage scenario is using a huge numpy data array as my tensorflow model's database, my tf model need freequently access data randomly from it, and the numpy data array is very huge, it couldn't be loaded into GPU because GPU's memory space is always much smaller than CPU's memroy space.\r\n\r\nAnd I've found a workaround to this limitation for myself already. I could use tf.convert_to_tensor() to generate a tensor which will be always stored in CPU's memory, so the above test code could be changed to:\r\n```\r\n....\r\nnpArray=np.random.uniform(0, 10000, size=200)\r\ntfArray=tf.convert_to_tensor(npArray)\r\nprint(tfArray.device)\r\n# the following codes will handle tfArray\r\n...... \r\n\r\n```\r\n\r\nYou could freely close this issue or use it to trace this new feature request: using tensor data array as slice index to access numpy data array in computing graph mode and tf.function mode (eager mode already suport this feature).\r\n\r\nThanks!", "Fundamentally, one cannot convert a graph tensor to numpy array because the graph does not execute in Python - so there is no NumPy at graph execution. The only way to achieve this mix is by using tf.py_function, which is ok assuming you don't want a portable model, but which will negatively affect the performance of your graph. py_function has a number of other shortcomings as well, which is the reason why it's not inserted by default into the graph.\r\n\r\nThat said, I believe the correct conversion here would be for the NumPy array to be auto-converted to Tensor. That actually happens most of the time:\r\n\r\n * `tesor_array[numpy_index]` converts `numpy_index` to tensor\r\n * `tensor_x + numpy_y` converts `numpy_y` to tensor\r\n\r\nTherefore, the following should also be true:\r\n\r\n * `numpy_array[tensor_index]` converts `numpy_array` to tensor\r\n\r\nSo from that perspective, we have a bug.\r\n\r\nSo this FR could generalize as: \"any operation involving mixes of numpy and tensors should consistently upcast all non-tensor values to tensor\". Unfortunately, that behavior is impossible to obtain in eager mode, which is what will probably make it infeasible.", "@mdanatg Thanks for your clear explanation.\r\n\"one cannot convert a graph tensor to numpy array because the graph does not execute in Python - so there is no NumPy at graph execution. \"\r\n\r\nYes, agree. And your explanation remind me to change the test codes to the following:\r\n```\r\n  def testTensorAndNumpy():\r\n    npArray=np.random.uniform(0, 10000, size=200)\r\n    @tf.function\r\n    def myFunc(tensorValue, inputNPArray):\r\n      return(inputNPArray[tensorValue])\r\n\r\n    #tensorValue=3\r\n    tensorValue=tf.convert_to_tensor(3)\r\n    print(myFunc(tensorValue, npArray))\r\n    print(npArray[tensorValue])\r\n\r\n```\r\n\r\nBecause I've put the numpy array into the tf.function's parameter, so the tensorflow could auto-convert the numpy array into tensor. then the test could pass. But in my original test, the npArray is a bigger scope variale, the tf.function current mechanism couldn't catch the opportunity to do the auto-coversion from numpy to tensor. If we put it into function's parameter, the numpy-tensor conversion could happen by current tf.function mechanism.", "\"So this FR could generalize as: \"any operation involving mixes of numpy and tensors should consistently upcast all non-tensor values to tensor\". Unfortunately, that behavior is impossible to obtain in eager mode, which is what will probably make it infeasible.\"\r\nYes, it's infeasible. but I've confirmed that in the eager mode:\r\n`numpy_array[tensor_index]` \r\nwill auto-convert the tensor_index into numpy value, and it works fine so far. So how to define the data type auto-conversion princple in tensorflow is decided by your team, I just report what I've found.", "@clockzhong Thank you, I agree that this is an important observation that we should document.\r\n\r\nThe difference between function arguments and values captured through the function's closure is an important one too, and is related to another known limitation in which the function is not retraced if any of the variable it closes over change.\r\n\r\nI think both of these aspects should be mentioned in the docstring of tf.function.", "@clockzhong  Closing this issue as it is the Intended Behavior.Please feel free to re-open the issue if you have any concerns.Thanks!", "@saikumarchalla  Ok, got it, thanks!"]}, {"number": 38886, "title": "ImportError: DLL load failed: The specified procedure could not be found.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home Single Language\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: 2.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: None\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI am trying to build an application and this error seems to stay even after I downgraded my tensor flow version from 2.1 to 2.0.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nC:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python36\\python.exe C:/Users/Dell/PycharmProjects/MajorProject017/ChatBot.py\r\nTraceback (most recent call last):\r\n  File \"C:/Users/Dell/PycharmProjects/MajorProject017/ChatBot.py\", line 2, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\core\\framework\\graph_pb2.py\", line 7, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\nProcess finished with exit code 1\r\n\r\n", "comments": ["@priyankkumar218,\r\nCould you please check [this comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) on a similar issue and let us know if it helps. Thanks!\r\n", "> Could you please check [this comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) on a similar issue and let us know if it helps. Thanks!\r\n\r\nAny updates regarding this issue? Thanks!", "If you donwgraded, you probably also need to downgrade the MSVC redistributable", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @priyankkumar218,\r\n> Could you please check [this comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) on a similar issue and let us know if it helps. Thanks!\r\n\r\nThis didn't help either.", "@priyankkumar218,\r\nCould you please share the make and model of your CPU and also check if you are using 64-bit Python. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38886\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38886\">No</a>\n"]}, {"number": 38885, "title": "tf.compat.v1.resource_loader.get_path_to_datafile", "body": "I had implemented this code:\r\n\r\nimport tensorflow as tf\r\n\r\n_flow_warp_ops = tf.load_op_library(\r\n    tf.compat.v1.resource_loader.get_path_to_datafile(\"./lib/flow_warp.so\"))\r\n\r\nBut it is showing error:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-12-d0892143bf54>\", line 2, in <module>\r\n    tf.compat.v1.resource_loader.get_path_to_datafile(\"./lib/flow_warp.so\"))\r\n\r\n  File \"/home/akshat_suwalka/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/load_library.py\", line 57, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\n\r\nNotFoundError: libcudart.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nAs you can see that file already i have in \"lib/flow_warp.so\"\r\n![Screenshot from 2020-04-25 14-05-38](https://user-images.githubusercontent.com/35618437/80275290-e9779e80-86fd-11ea-94d4-00dfd3bd30a1.png)\r\n\r\n\r\n\r\n", "comments": ["@akshat-suwalka \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "I am using ubuntu 16.04 LTS\r\nTensorflow is the latest version as I had installed day before yesterday. ", "In anaconda i had installed all the latest packages on that day including cudatoolkit, cudnn.\r\nlibcudart.so.8.0 is showing bcz i dont have cuda 8.0 version.\r\nIf that was the reason then i search here: https://repo.anaconda.com/pkgs/main/linux-64/\r\nwhere all the packages are available for linux but here i didn't find cuda 8.0 version.\r\n\r\nIs there any possibility to run that code in latest version of cuda??\r\nAny suggestion what should i do to resolve this problem?", "@akshat-suwalka What version of CUDA are you using?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38884, "title": "How use model(saved by tf1.14.0) in the tf1.5.0?", "body": "Hi,  tensorflow team:\r\n    I have a model saved by tensorflow-1.14.0. In my case, I must use this model in a tensorflow-1.5.0 environment. Is there any ways to convert the high-tensorflow-version model (pb file or ckpt) to a low-tensorflow-version model? \r\n                 Looking  forward to your repply\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38883, "title": "PR #34911 not being included in releases", "body": "**System information**\r\n- Have I written custom code (as opposed to using example directory):  Nope\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10 x64\r\n- TensorFlow backend (yes / no):  Yes\r\n- TensorFlow version:  2.1.0\r\n- Keras version:  2.2.4-tf (using tf.keras)\r\n- Python version:  3.7.5\r\n- CUDA/cuDNN version:  11\r\n- GPU model and memory:  1070 with 8GB\r\n\r\n**Describe the current behavior**\r\nIn `site-packages/tensorflow_core/python/keras/utils/generic_utils.py`, the PR I made [here](https://github.com/tensorflow/tensorflow/pull/34911) on December 6, 2019 isn't being applied to PyPi releases of TensorFLow, specifically TF 2.1.0 in my case. The code in that file when cleanly installing TensorFlow is as follows:\r\n```python\r\nself._dynamic_display = ((hasattr(sys.stdout, 'isatty') and\r\n                          sys.stdout.isatty()) or\r\n                         'ipykernel' in sys.modules or\r\n                         'posix' in sys.modules)\r\n```\r\n\r\n**Describe the expected behavior**\r\nWhen it should be:\r\n```python\r\nself._dynamic_display = ((hasattr(sys.stdout, 'isatty') and\r\n                          sys.stdout.isatty()) or\r\n                         'ipykernel' in sys.modules or\r\n                         'posix' in sys.modules or\r\n                         'PYCHARM_HOSTED' in os.environ)\r\n```\r\nas changed in the PR. Is there some other place I need to create a PR for this change to take effect in the releases?\r\n\r\n**Code to reproduce the issue**  \r\nInstall TensorFlow 2.1.0 on Windows 10, Python 3.7.5\r\n", "comments": ["Hi.\r\n\r\nYour PR (#34911) was merged into `master` on December 13th. However, the release branch for the 2.1 release (`r2.1`) has been cut on November 11th. Since there was no cherrypick of the commit, the change was not included in the 2.1 release. [Here](https://github.com/tensorflow/tensorflow/compare/r2.1) is a list of all commits included in the 2.1 release.\r\n\r\nHowever, it will be surely included in the 2.2 release as it is already included in the release candidates. In fact, once it landed on master it will be included in all releases for all branches cut __after__ that date (unless it is being rolled back).\r\n\r\nFor the other releases, we can do a cherry-pick to `r1.15`, `r2.0` and `r2.1` (or a subset of them). I will merge the existing cherry-picks when we do a patch release to any of these versions, as long are they are still in the support window.\r\n\r\nDo you want to make the cherry-picks, please?", "Thanks for the response. Ah, I didn't know the 2.1 branch was created before the merge, but that makes sense now. Let me look into cherry picking with Git and I'll get back to you, never tried that before.\r\n\r\nEdit: I think this should be good. The three PRs are below this", "Closing this now since TF 2.2 official has been released. Thanks!"]}, {"number": 38882, "title": "Tensorflow.Contrib Not Found", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Pip installed\r\n- TensorFlow version:2.2.0\r\n- Python version:3.8.2 64 Bit\r\n- Installed using virtualenv? pip? conda?: Pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:No graphic Card\r\n- GPU model and memory: ---\r\n\r\n\r\n\r\nI am using Tflearn library which uses Tensorflow.contrib as a module. But It is showing error that it has no library as such\r\nThe stack code is here\r\n2020-04-25 09:45:43.517997: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-04-25 09:45:43.525381: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on\r\nyour machine.\r\nTraceback (most recent call last):\r\n  File \"f:/MACHINE LEARNING/chatbot/main.py\", line 10, in <module>\r\n    import tflearn\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tflearn\\__init__.py\", line 4, in <module>\r\n    from . import config\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tflearn\\config.py\", line 5, in <module>\r\n    from .variables import variable\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tflearn\\variables.py\", line 7, in <module>\r\n    from tensorflow.contrib.framework.python.ops import add_arg_scope as contrib_add_arg_scope\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\n\r\nI just imported Tflearn and Tensorflow and no code was written.\r\n", "comments": ["> **System information**\r\n> \r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n> * TensorFlow installed from (source or binary): Pip installed\r\n> * TensorFlow version:2.2.0\r\n> * Python version:3.8.2 64 Bit\r\n> * Installed using virtualenv? pip? conda?: Pip\r\n> * Bazel version (if compiling from source):\r\n> * GCC/Compiler version (if compiling from source):\r\n> * CUDA/cuDNN version:No graphic Card\r\n> * GPU model and memory: ---\r\n> \r\n> I am using Tflearn library which uses Tensorflow.contrib as a module. But It is showing error that it has no library as such\r\n> The stack code is here\r\n> 2020-04-25 09:45:43.517997: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n> 2020-04-25 09:45:43.525381: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on\r\n> your machine.\r\n> Traceback (most recent call last):\r\n> File \"f:/MACHINE LEARNING/chatbot/main.py\", line 10, in\r\n> import tflearn\r\n> File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tflearn__init__.py\", line 4, in\r\n> from . import config\r\n> File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tflearn\\config.py\", line 5, in\r\n> from .variables import variable\r\n> File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tflearn\\variables.py\", line 7, in\r\n> from tensorflow.contrib.framework.python.ops import add_arg_scope as contrib_add_arg_scope\r\n> ModuleNotFoundError: No module named 'tensorflow.contrib'\r\n> \r\n> I just imported Tflearn and Tensorflow and no code was written.\r\n\r\nTensorflow.contrib is not present in Tensorflow v2.\r\nTry downgrading your version to 1.4 or below.", "@dhruv-colosus \r\nAs you are using the version of tensorflow which does not have contrib please downgrade.\r\n\r\nplease refer to these links for more clarity: [link1](https://github.com/tensorflow/tensorflow/issues/36193#issuecomment-609923865) [link2](https://github.com/tensorflow/tensorflow/issues/31350#issuecomment-542827486) [link3](https://github.com/tensorflow/tensorflow/issues/32229)", "For more info see this [https://www.tensorflow.org/guide/migrate#a_note_on_slim_contriblayers](https://www.tensorflow.org/guide/migrate#a_note_on_slim_contriblayers)\r\nThanks!", "> For more info see this https://www.tensorflow.org/guide/migrate#a_note_on_slim_contriblayers\r\n> Thanks!\r\n\r\nAre there any disadvantage  of older version over this one. And when I try to downgrade this error occur\r\nC:\\Users\\HP>pip install tensorflow==1.4\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.4 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3)\r\nERROR: No matching distribution found for tensorflow==1.4\r\n", "@dhruv-colosus \r\nPlease note you are referring to incorrect tensorflow version, the tensorflow version should be 1.14 and not 1.4", "> @dhruv-colosus\r\n> Please note you are referring to incorrect tensorflow version, the tensorflow version should be 1.14 and not 1.4\r\n\r\n\r\nC:\\Users\\HP>pip install tensorflow==1.14\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.14 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3)\r\nERROR: No matching distribution found for tensorflow==1.14\r\n\r\nStill getting error\r\n", "@dhruv-colosus \r\nPlease  downgrade to python version 3.7 as per [this comment](https://github.com/tensorflow/tensorflow/issues/33838#issuecomment-566342226) there is no pip package for 3.8.\r\nCan you please confirm if you used \"python3 -m pip install tensorflow==1.14.0\" else please do so and let us know.\r\nPlease follow [this issue](https://github.com/tensorflow/tensorflow/issues/33838), it will answer all queries related to this issue.", "@dhruv-colosus 1.4 is too old, please don't use it. 1.14 is also no longer supported (more than a year since release).\r\n\r\nSo, you can only use 1.15, 2.0, 2.1 and (soon) 2.2 if you want to use something that is still being supported (1.15 and 2.1 are supported for 3 years).\r\n\r\nIf `pip install` does not work, try upgrading pip and setuptools.", "> @dhruv-colosus 1.4 is too old, please don't use it. 1.14 is also no longer supported (more than a year since release).\r\n> \r\n> So, you can only use 1.15, 2.0, 2.1 and (soon) 2.2 if you want to use something that is still being supported (1.15 and 2.1 are supported for 3 years).\r\n> \r\n> If `pip install` does not work, try upgrading pip and setuptools.\r\n\r\nI have fully updated version of pip and setuptools but still no version lower than 2.0 is allowed to be downloaded.", "> @dhruv-colosus\r\n> Please downgrade to python version 3.7 as per [this comment](https://github.com/tensorflow/tensorflow/issues/33838#issuecomment-566342226) there is no pip package for 3.8.\r\n> Can you please confirm if you used \"python3 -m pip install tensorflow==1.14.0\" else please do so and let us know.\r\n> Please follow [this issue](https://github.com/tensorflow/tensorflow/issues/33838), it will answer all queries related to this issue.\r\n\r\nI used pip with Python 3.6 also but still its not happening", "Can you post (attach) the full output of the following?\r\n\r\n```\r\npython -m pip install --upgrade pip setuptools\r\npython -m pip debug --verbose\r\npython -m pip install -vvv tensorflow=99.99.99   # for debugging, I know it does not exist\r\n```", "```\r\nMicrosoft Windows [Version 10.0.17134.1425]\r\n(c) 2018 Microsoft Corporation. All rights reserved.\r\n\r\nC:\\Users\\HP>python -m pip install --upgrade pip setuptools\r\nCollecting pip\r\n  Downloading pip-20.1-py2.py3-none-any.whl (1.5 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.5 MB 409 kB/s\r\nCollecting setuptools\r\n  Downloading setuptools-46.1.3-py3-none-any.whl (582 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 582 kB 125 kB/s\r\nInstalling collected packages: pip, setuptools\r\n  Attempting uninstall: pip\r\n    Found existing installation: pip 20.0.2\r\n    Uninstalling pip-20.0.2:\r\n      Successfully uninstalled pip-20.0.2\r\n  Attempting uninstall: setuptools\r\n    Found existing installation: setuptools 41.2.0\r\n    Uninstalling setuptools-41.2.0:\r\n      Successfully uninstalled setuptools-41.2.0\r\nSuccessfully installed pip-20.1 setuptools-46.1.3\r\n\r\nC:\\Users\\HP>python -m pip debug --verbose\r\nWARNING: This command is only meant for debugging. Do not use this with automation for parsing and getting these details, since the output and options of this command may change without notice.\r\npip version: pip 20.1 from C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pip (python 3.8)\r\nsys.version: 3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 23:03:10) [MSC v.1916 64 bit (AMD64)]\r\nsys.executable: C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\python.exe\r\nsys.getdefaultencoding: utf-8\r\nsys.getfilesystemencoding: utf-8\r\nlocale.getpreferredencoding: cp1252\r\nsys.platform: win32\r\nsys.implementation:\r\n  name: cpython\r\n'cert' config value: Not specified\r\nREQUESTS_CA_BUNDLE: None\r\nCURL_CA_BUNDLE: None\r\npip._vendor.certifi.where(): C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pip\\_vendor\\certifi\\cacert.pem\r\npip._vendor.DEBUNDLED: False\r\nvendored library versions:\r\n  appdirs==1.4.3\r\n  CacheControl==0.12.6\r\n  colorama==0.4.3\r\n  contextlib2==0.6.0.post1 (Unable to locate actual module version, using vendor.txt specified version)\r\n  distlib==0.3.0\r\n  distro==1.5.0 (Unable to locate actual module version, using vendor.txt specified version)\r\n  html5lib==1.0.1\r\n  ipaddress==1.0.23\r\n  msgpack==1.0.0 (Unable to locate actual module version, using vendor.txt specified version)\r\n  packaging==20.3\r\n  pep517==0.8.2\r\n  progress==1.5\r\n  pyparsing==2.4.7\r\n  requests==2.23.0\r\n  certifi==2020.04.05.1\r\n  chardet==3.0.4\r\n  idna==2.9\r\n  urllib3==1.25.8\r\n  resolvelib==0.3.0\r\n  retrying==1.3.3 (Unable to locate actual module version, using vendor.txt specified version)\r\n  setuptools==44.0.0 (Unable to locate actual module version, using vendor.txt specified version)\r\n  six==1.14.0\r\n  toml==0.10.0\r\n  webencodings==0.5.1 (Unable to locate actual module version, using vendor.txt specified version)\r\nCompatible tags: 30\r\n  cp38-cp38-win_amd64\r\n  cp38-abi3-win_amd64\r\n  cp38-none-win_amd64\r\n  cp37-abi3-win_amd64\r\n  cp36-abi3-win_amd64\r\n  cp35-abi3-win_amd64\r\n  cp34-abi3-win_amd64\r\n  cp33-abi3-win_amd64\r\n  cp32-abi3-win_amd64\r\n  py38-none-win_amd64\r\n  py3-none-win_amd64\r\n  py37-none-win_amd64\r\n  py36-none-win_amd64\r\n  py35-none-win_amd64\r\n  py34-none-win_amd64\r\n  py33-none-win_amd64\r\n  py32-none-win_amd64\r\n  py31-none-win_amd64\r\n  py30-none-win_amd64\r\n  cp38-none-any\r\n  py38-none-any\r\n  py3-none-any\r\n  py37-none-any\r\n  py36-none-any\r\n  py35-none-any\r\n  py34-none-any\r\n  py33-none-any\r\n  py32-none-any\r\n  py31-none-any\r\n  py30-none-any\r\n\r\nC:\\Users\\HP>python -m pip install tensorflow==1.5\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.5 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0)\r\nERROR: No matching distribution found for tensorflow==1.5\r\n\r\nC:\\Users\\HP>python -m pip install tensorflow==1.15\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0)\r\nERROR: No matching distribution found for tensorflow==1.15\r\n\r\nC:\\Users\\HP>python -m pip install -vvv tensorflow=99.99.99   # for debugging, I know it does not exist\r\nNon-user install because site-packages writeable\r\nCreated temporary directory: C:\\Users\\HP\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-utpivz2z\r\nCreated temporary directory: C:\\Users\\HP\\AppData\\Local\\Temp\\pip-req-tracker-oevbpce7\r\nInitialized build tracking at C:\\Users\\HP\\AppData\\Local\\Temp\\pip-req-tracker-oevbpce7\r\nCreated build tracker: C:\\Users\\HP\\AppData\\Local\\Temp\\pip-req-tracker-oevbpce7\r\nEntered build tracker: C:\\Users\\HP\\AppData\\Local\\Temp\\pip-req-tracker-oevbpce7\r\nCreated temporary directory: C:\\Users\\HP\\AppData\\Local\\Temp\\pip-install-b_3tbgyk\r\nERROR: Invalid requirement: 'tensorflow=99.99.99'\r\nHint: = is not a valid operator. Did you mean == ?\r\nException information:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pip\\_vendor\\packaging\\requirements.py\", line 98, in __init__\r\n    req = REQUIREMENT.parseString(requirement_string)\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 1955, in parseString\r\n    raise exc\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 3814, in parseImpl\r\n    raise ParseException(instring, loc, self.errmsg, self)\r\npip._vendor.pyparsing.ParseException: Expected stringEnd, found '='  (at char 10), (line:1, col:11)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pip\\_internal\\req\\constructors.py\", line 358, in parse_req_from_line\r\n    req = Requirement(req_as_string)\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pip\\_vendor\\packaging\\requirements.py\", line 100, in __init__\r\n    raise InvalidRequirement(\r\npip._vendor.packaging.requirements.InvalidRequirement: Parse error at \"'=99.99.9'\": Expected stringEnd\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 188, in _main\r\n    status = self.run(options, args)\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 185, in wrapper\r\n    return func(self, options, args)\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 300, in run\r\n    reqs = self.get_requirements(\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 321, in get_requirements\r\n    req_to_add = install_req_from_line(\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pip\\_internal\\req\\constructors.py\", line 396, in install_req_from_line\r\n    parts = parse_req_from_line(name, line_source)\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pip\\_internal\\req\\constructors.py\", line 373, in parse_req_from_line\r\n    raise InstallationError(msg)\r\npip._internal.exceptions.InstallationError: Invalid requirement: 'tensorflow=99.99.99'\r\nHint: = is not a valid operator. Did you mean == ?\r\nRemoved build tracker: 'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Temp\\\\pip-req-tracker-oevbpce7'\r\n```\r\n\r\n", "The relevant part of the output is likely this (note usage of ` ``` ` to make it more readable)\r\n\r\n```\r\nCompatible tags: 30\r\n  cp38-cp38-win_amd64\r\n  cp38-abi3-win_amd64\r\n  cp38-none-win_amd64\r\n  cp37-abi3-win_amd64\r\n  cp36-abi3-win_amd64\r\n  cp35-abi3-win_amd64\r\n  cp34-abi3-win_amd64\r\n  cp33-abi3-win_amd64\r\n  cp32-abi3-win_amd64\r\n  py38-none-win_amd64\r\n  py3-none-win_amd64\r\n  py37-none-win_amd64\r\n  py36-none-win_amd64\r\n  py35-none-win_amd64\r\n  py34-none-win_amd64\r\n  py33-none-win_amd64\r\n  py32-none-win_amd64\r\n  py31-none-win_amd64\r\n  py30-none-win_amd64\r\n  cp38-none-any\r\n  py38-none-any\r\n  py3-none-any\r\n  py37-none-any\r\n  py36-none-any\r\n  py35-none-any\r\n  py34-none-any\r\n  py33-none-any\r\n  py32-none-any\r\n  py31-none-any\r\n  py30-none-any\r\n```\r\n\r\nMatching that list with the available pips it looks like you can install [2.2 with python3.8](https://pypi.org/project/tensorflow/2.2.0/#files) but no others.\r\n\r\nFor example, for 1.15, https://pypi.org/project/tensorflow/1.15.0/#files says that you should have one of the following tags to install via pip: `cp37-cp37m-win_amd64`, `cp36-cp36m-win_amd64` and `cp35-cp35m-win_amd64`.\r\n\r\nHow did you get Python on your machine? Maybe that is at fault?", "@dhruv-colosus\r\nPlease update as per above comment", "I downloaded it through the python.org site only. So is there no solution available??", "You will have to download Python3.7 from the python site to use 1.15, 2.0 or 2.1.\r\n\r\nTF 2.2 is the only one working in Python3.8.", "> You will have to download Python3.7 from the python site to use 1.15, 2.0 or 2.1.\r\n> \r\n> TF 2.2 is the only one working in Python3.8.\r\n\r\nI already tried with 3.6 also, 3.7 is the only option so??", "The tags above signal an incompatibility between Pythons. https://github.com/pypa/pip/issues/6526"]}, {"number": 38881, "title": "[INTEL MKL] remapper test fix.", "body": "", "comments": []}, {"number": 38880, "title": "Update snappy library to 1.1.8", "body": "The previous snappy version (1.1.7) is really old (2017). For\r\nthat reason it makes sense to update snappy to 1.1.8 which was\r\nreleased in 2020.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 38879, "title": "If tf.data.Dataset.list_files shuffles files, a subsequent take is ignored", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **binary (pip install tensorflow)**\r\n- TensorFlow version (use command below): **2.1.0**\r\n- Python version: **2.7**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: **CUDA 10.1, cuDNN 7.6.5** (the bug happens even `with tf.device('/cpu')`\r\n- GPU model and memory: **Quadro M1200, 4096MB**\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nThere are 100 TFRecord examples stored on disk, sharded over multiple files.  I first do `tf.data.Dataset.list_files`, then an `interleave`d map, mapping the dataset to a `TFRecordDataset`.  Then I do a `map`, with a mapper that parses each serialized example.  Then a `take` to keep 17 elements, then and a `repeat` (forever).\r\n\r\nI iterate over the (infinite) dataset for 400 iterations, keeping track of the number of **unique** elements seen.  If I set `shuffle=False` in the call to `list_files`, I see only 17 unique elements.  If I set `shuffle=True`, I see all 100 unique elements, which is wrong. \r\n\r\n**Describe the expected behavior**\r\nI expect to only see 17 unique elements, regardless of whether or not I pass `shuffle=True` or `shuffle=False`.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nPlease download the attached file (called `reproduce_bug.txt` because GitHub doesn't like it if I attach .py files).  Rename it to `reproduce_bug.py` and run with `python reproduce_bug.py`.  The file has many comments that further illustrate the bug.\r\n[reproduce_bug.txt](https://github.com/tensorflow/tensorflow/files/4531280/reproduce_bug.txt)\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried on colab with TF version 2.1.0, 2.2.0-rc3 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/a8aa5a26289e4340e3769f62210b2ac5/untitled823.ipynb).Thanks!", "@ravikyram @gowthamkpr thank you for looking into this issue.  Glad you were able to reproduce the bug.  Is there an ETA for when I can expect a fix?  Cheers", "This is working as expected. `repeat` will re-execute the upstream computation for every repetition. In the case `shuffle=True` different order of shuffling will be used (since the `reshuffle_each_iteration` argument of `shuffle` defaults to `True`). \r\n\r\nSince the `reshuffle_each_iteration` argument of `shuffle` is not exposed via `list_files`, you can either: a) replace the use of `list_files(pattern, shuffle=True)` with `from_tensor_slices(globbed_pattern).shuffle(num_files, reshuffle_each_iteration=False)` or b) add `cache()` after your `take` transformation.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38879\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38879\">No</a>\n"]}, {"number": 38878, "title": "BatchNormalization layer has different output when loading a checkpoint saved from TF 2.0b1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Official Tensorflow Docker Container (2.0.0, 2.0.0-beta1, latest) with py3 and gpu support\r\n- TensorFlow installed from (source or binary): Docker\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0 / 10.2\r\n- GPU model and memory: Tested on RTX 2080Ti, Titan V, GTX 1080\r\n\r\n\r\n**Describe the current behavior**\r\nIn my project I trained several (adversarial) segmentation models using both Tensorflow 2.0.0-a0 and 2.0.0-b1. \r\nUpon upgrading Tensorflow to version 2.1 I noticed a decrease in the accuracy of my models between 5~15% (Dice Score) with respect to the version 2.0.0-b1. \r\nI ran several tests loading the same checkpoints in multiple tensorflow versions by comparing the layer outputs and I noticed that starting from the first BatchNormalization layer the outputs of networks loaded with TF 2.0.0 (stable) are different from those loaded using TF <= 2.0.0-b1(even though the loaded weights are the same). BatchNormalization behavior is consistent starting from 2.0 stable onward. \r\n\r\n**Describe the expected behavior**\r\nThe output of BatchNormalization layer (and thus of the network) should be the same when using TF 2.0.0b1 and 2.0.0, when the checkpoint has been trained and saved using beta1.\r\n\r\n**Standalone code to reproduce the issue**\r\nI tried reproducing the issue using the examples or pretrained networks, but the differences are not noticeable as they are in my models. I wrote a notebook that can be run on both 2.0.0b1 and 2.1 to highlight the differences. \r\nhttps://drive.google.com/open?id=1vw3vaPU32lqI2jAOvp0toXIgD84R_Tx4\r\nBesides the notebook there are also the input data I used for testing and the output of the last layer for both TF2.0-b1 and TF2.1 (other versions are not inclued as they seem to match with either b1 or 2.1). I'm not including individual layers outputs due to size constraints.\r\nIn the pics fodler can find also some visual examples of the network outputs.\r\n\r\n\r\n**Other info / logs** \r\nI'm attaching just an example of just one mismatching output for the same input. In the drive folder you can find the test for an entire batch.\r\n![hist-61-2 0 0-beta1](https://user-images.githubusercontent.com/9888197/80245680-69085d80-866b-11ea-8e26-ee7fd8797f0f.jpg)\r\n![hist-61-2 1 0](https://user-images.githubusercontent.com/9888197/80245684-69a0f400-866b-11ea-982e-1722c6733861.jpg)\r\n\r\nI couldn't understand what this could be related to, I'd like to have some insights of what could be the cause.\r\nThank you!", "comments": ["@edoardogiacomello,\r\nI tried to reproduce the issue but did not observe much difference in the output with [TF v2.0](https://colab.research.google.com/gist/amahendrakar/65b2c0ba8212dc311fba6dfe8c4c0619/38878-2-0.ipynb), [TF v2.1](https://colab.research.google.com/gist/amahendrakar/160fce6364808f672b111819b2ad8271/38878-2-1.ipynb) and [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/bdee9985cb66c522c2ad0e30e76f7bac/38878-2-2.ipynb). Please find the attached gist. Thanks!", "> @edoardogiacomello,\r\n> I tried to reproduce the issue but did not observe much difference in the output with [TF v2.0](https://colab.research.google.com/gist/amahendrakar/65b2c0ba8212dc311fba6dfe8c4c0619/38878-2-0.ipynb), [TF v2.1](https://colab.research.google.com/gist/amahendrakar/160fce6364808f672b111819b2ad8271/38878-2-1.ipynb) and [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/bdee9985cb66c522c2ad0e30e76f7bac/38878-2-2.ipynb). Please find the attached gist. Thanks!\r\n\r\n@amahendrakar  I'm sorry, I may have made a mistake while managing the different containers. I checked again and I've been able to reproduce the error in version **2.0.0-beta1** (both on Docker and Colab), so the change may have happened between **2.0.0b1** and **2.0**. Starting from 2.0 (stable) the results are consistent with future versions, as you showed.\r\n\r\nI'd like to find out what's causing this difference in BatchNormalization outputs and if there's a way to adapt my checkpoints to the stable version without a complete re-training.\r\n", "@edoardogiacomello Please feel free to close the issue as this was resolved already. You could use stable version of `TF2.1` or `TF2.0`. Thanks!", "@jvishnuvardhan BatchNormalization layer may have been fixed (it changed for sure), but this issue is referred to loading a model that is trained with a previous version and it doesn't behave the way it should. I don't see how using a stable version could fix the issue, which is indeed related to loading a beta checkpoint in a stable version. \r\n\r\nCould you give a reference on what the issue was or how it has been resolved? I'd like to avoid re-launching about 8 months of training (or getting stuck into beta for the whole project) if there could be a workaround. Thank you :)", "There were a number of changes between the beta and 2.1, and we would recommend retraining or adjusting weights as necessary. For long-lived models, it is best to use stable versions of TF, as nightlies and RCs sometimes contain issues that require non-backwards compatible changes versus the subsequent release."]}, {"number": 38877, "title": "K.in_train_phase() broken in eager mode when used outside of Lambda layer", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow version (use command below): 2.2.0-rc3 (v2.2.0-rc3-0-gaad398b5e9)\r\n- Python version: 3.0\r\n\r\n**Describe the current behavior**\r\nK.in_train_phase(). It always returns the alternative option (both during model.fit() and model.predict() in keras).  But it does work when you disable eager mode or when you wrap it in a Lambda layer. This is unexpected because in TF1 and when eager is disabled the function works as expected.\r\n\r\n**Describe the expected behavior**\r\nI would expect K.in_train_phase() to work as described in the documentation.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1yaw-UhOgj7UyLC4NEtQPYiZRYvnxhz_G\r\n", "comments": ["i am able to replicate the issue please find the[ gist here](https://colab.sandbox.google.com/gist/Saduf2019/e42e9f9409902a15edef04ef4eb9b3b2/38870.ipynb)", "Was able to reproduce the issue in TF 2.5 on colab. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/0014965a7a05fffd2b118708f0788233/untitled.ipynb).", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38877\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38877\">No</a>\n"]}, {"number": 38876, "title": "Creating multiple stacked variable within tf.map_fn  ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.10\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, I am trying to create multiple variables within tf.map_fn but it just creates a single variable copied multiple times.\r\n\r\n**Will this change the current api? How?**\r\nMaybe\r\n\r\n**Who will benefit with this feature?**\r\nProjects related to multi-object tracking\r\n\r\n**Any Other info.**\r\n", "comments": ["Do you have any use case that requires the feature you are interested in? .Please, share the related code.Thanks!", "Hi RaviKyram, \r\n\r\nThank you for your response, I am trying to use RT-MDNet (Real-Time_Multi- Domain Network) to achieve multi-object tracking. for that, I am trying to create a higher rank weight matrix stacked over one another depending on the number of objects detected. Currently, I was able to create multiple variables outside the graph but now I am trying to stack them and pass it to the last fully connected layer:\r\n\r\n```\r\nfor n_obj in range(pos_data.shape[0]):\r\n        W6 = tf.get_variable(\"fc6/kernel-\" + str(n_obj), shape=(1, 512, 1),\r\n                             initializer=tf.contrib.layers.xavier_initializer())\r\n\r\nprob, train_op, accuracy, loss, pred, initialize_vars, y, fc6 = build_branches(fc5, W6)\r\n\r\ndef build_branches(fc5, W6):\r\n    y = tf.placeholder(tf.int64, [None, None])\r\n\r\n\r\n    with tf.variable_scope(\"fc6\", reuse=tf.AUTO_REUSE):\r\n        b6 = tf.get_variable('bias', shape=(1, 1, 1), initializer=tf.zeros_initializer())\r\n\r\n    fc6 = tf.add(tf.matmul(fc5, W6), b6)\r\n\r\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\r\n                                                                         logits=fc6))\r\n\r\n    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"fc6\")\r\n\r\n    with tf.variable_scope(\"\",  reuse=tf.AUTO_REUSE):\r\n\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001, name='adam')\r\n        train_op = optimizer.minimize(loss, var_list=train_vars)\r\n\r\n\r\n        initialize_vars = train_vars\r\n        initialize_vars += [optimizer.get_slot(var, name)\r\n                            for name in optimizer.get_slot_names()\r\n                            for var in train_vars]\r\n        if isinstance(optimizer, tf.train.AdamOptimizer):\r\n            initialize_vars += optimizer._get_beta_accumulators()\r\n\r\n    prob = tf.nn.softmax(fc6)\r\n    pred = tf.argmax(prob, 2)\r\n    correct_pred = tf.equal(pred, y)\r\n\r\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n\r\n    return prob, train_op, accuracy, loss, pred, initialize_vars, y, fc6\r\n\r\n```\r\nI was able to create multiple branches but what I want is a single branch incorporating all the created variables stacked before the matmul operation on fc6 tensor. I tried several things but nothing worked.\r\n\r\n![multi-task_kernel](https://user-images.githubusercontent.com/28593585/80916940-1d853c00-8d5c-11ea-9187-de659ad36f83.png)\r\n\r\n", "here is an update to the issue, I was able to generate the multiple kernels outside the graph in a for loop and then giving it to the graph:\r\n\r\n```\r\n    w6 = []\r\n    for n_obj in range(pos_data.shape[0]):\r\n        w6.append(tf.get_variable(\"fc6/kernel-\" + str(n_obj), shape=(512, 2),\r\n                             initializer=tf.contrib.layers.xavier_initializer()))\r\n\r\n    print(\"modeling fc6 branches...\")\r\n    prob, train_op, accuracy, loss, pred, initialize_vars, y, fc6 = build_branches(fc5, w6)\r\n\r\ndef build_branches(fc5, w6):\r\n    y = tf.placeholder(tf.int64, [None, None])\r\n\r\n    b6 = tf.get_variable('fc6/bias', shape=2, initializer=tf.zeros_initializer())\r\n\r\n    fc6 = tf.add(tf.matmul(fc5, w6), b6)\r\n\r\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\r\n                                                                         logits=fc6))\r\n\r\n    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"fc6\")\r\n\r\n    with tf.variable_scope(\"\",  reuse=tf.AUTO_REUSE):\r\n\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001, name='adam')\r\n        train_op = optimizer.minimize(loss, var_list=train_vars)\r\n\r\n        initialize_vars = train_vars\r\n        initialize_vars += [optimizer.get_slot(var, name)\r\n                            for name in optimizer.get_slot_names()\r\n                            for var in train_vars]\r\n        if isinstance(optimizer, tf.train.AdamOptimizer):\r\n            initialize_vars += optimizer._get_beta_accumulators()\r\n\r\n    prob = tf.nn.softmax(fc6)\r\n    pred = tf.argmax(prob, 2)\r\n    correct_pred = tf.equal(pred, y)\r\n\r\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n\r\n    return prob, train_op, accuracy, loss, pred, initialize_vars, y, fc6\r\n```\r\n\r\n![concatenated_kernel_multi-task](https://user-images.githubusercontent.com/28593585/80975139-001aa580-8e22-11ea-80e7-9b2fe19da13c.png)\r\n\r\nNow I want to figure out a way to delete the kernels for the object that is lost during the tracking. I guess that is not possible after the graph generation.", "Please ask this type of question on StackOverflow as it's not a TF bug."]}, {"number": 38875, "title": "Passing tf.keras.Model as tf.function argument does not create concrete function", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.5\r\n\r\n**Describe the current behavior**\r\n\r\nPassing a `tf.keras.Model` or `tf.keras.Optimizer` as argument into `tf.function` does not create a concrete function. I expect that it would, since function tracing works as it should if the model/optimizer is a global variable.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def call(self, inputs):\r\n        return 2 * inputs\r\n\r\n@tf.function\r\ndef step_model(model, inputs):\r\n    return model(inputs)\r\n\r\n@tf.function\r\ndef step(inputs):\r\n    return model(inputs)\r\n\r\ninputs = tf.convert_to_tensor(1, dtype=tf.float32)\r\nmodel = MyModel()\r\n# This works as expected\r\nprint(f\"step() = {step(inputs)}\") # 2.0\r\nprint(f\"step() concrete functions: {step._list_all_concrete_functions_for_serialization()}\") # [<tensorflow.python.eager.function.ConcreteFunction object at 0x13a2c0510>]\r\n# This does not, no concrete function is saved\r\nprint(f\"step_model() = {step_model(model, inputs)}\") # 2.0\r\nprint(f\"step_model() concrete functions: {step_model._list_all_concrete_functions_for_serialization()}\") # []\r\n```\r\n\r\nOutput:\r\n```bash\r\nstep() = 2.0\r\nstep() concrete functions: [<tensorflow.python.eager.function.ConcreteFunction object at 0x133788410>]\r\nstep_model() = 2.0\r\nstep_model() concrete functions: []\r\n```\r\n\r\n\r\n\r\nIt appears that passing a `tf.keras.Model` as an argument into `tf.function` is not supported, as tracing fails. In a different use case, this error appears:\r\n```bash\r\nINFO:tensorflow:Unsupported signature for serialization: ((<tensorflow.python.framework.func_graph.UnknownArgument object at 0x13a2c0510>))\r\n```\r\n\r\nMy use case requires limiting usage of global variables, since there are several models running simultaneously and they need to be garbage collected efficiently. How can I pass a model as a function argument into a `tf.function`?\r\n", "comments": ["working well on my system.\r\n\r\nTry calling class name in super, this might work.", "Was able to reproduce the issue with TF v2.1, [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/522cba9b7938d89254fa81cf07272114/38875.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/67ad4261f2903c039227577c93f55081/38875-tf-nightly.ipynb). Please find the attached gist. Thanks!", "This is a known issue. The problem is that we cannot introspect inside a model and generate a single TF graph which is agnostic to the details of the model (which is what it would mean to serialize a concrete function which can take the model as an argument), mostly because we cannot represent function pointers in tf graph now.\r\n\r\nInstead I recommend you do something like\r\n\r\n```\r\ndef get_model_fn(model):\r\n  @tf.function\r\n  def fn(data):\r\n    return model(data)\r\n  return fn\r\n```\r\n\r\nand then call get_model_fn on all the models you want, to ensure the model gets properly captured.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38875\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38875\">No</a>\n", "cc @k-w-w ", "Another option is to make step_model a method of MyModel (methods are special cased, so the \"self\" argument doesn't have to be translated to encodable argument):\r\n\r\n```\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def call(self, inputs):\r\n        return 2 * inputs\r\n\r\n    @tf.function\r\n    def step_model(self, inputs):\r\n        return self(inputs)\r\n\r\ninputs = tf.convert_to_tensor(1, dtype=tf.float32)\r\nmodel = MyModel()\r\n\r\nprint(f\"step_model() = {model.step_model(inputs)}\") # 2.0\r\nprint(f\"step_model() concrete functions: {model.step_model._list_all_concrete_functions_for_serialization()}\") # [<tensorflow.python.eager.function.ConcreteFunction object at 0x7fd9cc231b00>]\r\n```\r\n", "Thanks for the suggestions. The reason I'm passing in a model is I'm working with BERT-based architectures and will often compare two models when running the same training script. How does BERT-base compare against ALBERT-base, for example? This precludes creating the step_model() model argument, though that is useful information. @alextp That is an interesting idea, I'll give it a spin.\r\n\r\nHope you are able to find a fix for the underlying issue! Would love an update if that happens.", "The solution works only if you call the model itself, but not if you want to access the weights for example. In Reinforcement Learning DQN Algorithm, you need to regularly update the \"target\" model with the weights of the \"inline\" model... In that case, the solution concretely doesn't work! Any idea, how to pass the model to a function for such a purpose? (You can pass the model via the function closure, either by setting it into a variable in the global scope or by nesting the tf.function into an outer function, or by packing everything into a class... but in all three cases, you loose the \"functional\" design)"]}, {"number": 38874, "title": "Migrate mobilenet_v3 from keras-team/keras-applications to tensorflow/tensorflow/python/keras/applications/", "body": "Just like mobilenet_v2.py and all the others migrated to https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras/applications on Apr 3rd, https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet_v3.py is ready to be migrated since Apr 2nd\r\n\r\n\r\n**Are you willing to contribute it (Yes/No):** No\r\n\r\n**Will this change the current api? How?** No\r\n\r\n**Who will benefit with this feature?** All users", "comments": ["MobileNetV3 application is now added in TF 2.4.1.\r\nSee api docs:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV3Large\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV3Small\r\nThanks!"]}, {"number": 38873, "title": "Add symmetric int16 support to tflu softmax reference kernel", "body": "\r\nSigned-off-by: SiCongLi <sicong.li@arm.com>", "comments": ["@sicong-li-arm Can you please resolve conflicts? Thanks!", "> @sicong-li-arm Can you please resolve conflicts? Thanks!\r\n\r\nDone. Thanks for the reminder!", "Used placement new instead of cast to properly construct SoftmaxOpData. Removed the refactoring code to simplify the commit.", "@hajuho @advaitjain Could you please have a look into this patch? Thanks!", "@advaitjain I'm not used with tfl micro code. Could you review this change?", "Hi @advaitjain could you please have a look into this PR? Much appreciated!  ", "> Thanks for this! Could you also link to the design doc or any RFC we have for the int16 integration, since before now it hasn't been used on the Micro side and I'm not as familiar with how we're approaching it.\r\n\r\n@petewarden  Thanks so much for your review! I don't have any generic design doc for int16 integration but I can ask around. As for this particular integration, I was simply integrating the new LUT-based tflite int16 softmax routine, and reusing its logic for generating the LUTs. The original tflite patch is [here](https://github.com/tensorflow/tensorflow/pull/35996)   ", "Hi @petewarden! I've addressed your comments. Could you please have a look? Thanks very much! :) ", "@sicong-li-arm please check below errors \r\n\r\n`tensorflow/lite/micro/testing/test_utils.cc:16:\r\n.//tensorflow/lite/micro/testing/test_utils.h:97:19: error: unused variable 'input_dims' [-Werror,-Wunused-variable]\r\n TfLiteIntArray* input_dims = IntArrayFromInitializer(input_dims_data);\r\n                 ^\r\n1 error generated.\r\nBroken by missing target ///tensorflow/lite/micro/kernels:micro_ops\r\ncontent_copy\r\n/tensorflow/lite/micro/kernels/softmax.cc:55:7: error: implicit conversion increases floating-point precision: 'float' to 'double' \r\n     TF_LITE_ENSURE_NEAR(context, output->params.scale, 1.f / 32768,\r\n     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n.//tensorflow/lite/c/common.h:231:9: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n   if (delta > epsilon) {                                             \\\r\n       ^~~~~ ~\r\n/tensorflow/lite/micro/kernels/softmax.cc:56:36: error: implicit conversion increases floating-point precision: 'float' to 'double' \r\n                         (0.001 * 1.f / 32768));\r\n                                ~ ^~~\r\n.//tensorflow/lite/c/common.h:231:17: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n   if (delta > epsilon) {                                             \\\r\n               ^~~~~~~\r\n/tensorflow/lite/micro/kernels/softmax.cc:55:62: error: implicit conversion increases floating-point precision: 'float' to 'double' \r\n     TF_LITE_ENSURE_NEAR(context, output->params.scale, 1.f / 32768,\r\n     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~\r\n.//tensorflow/lite/c/common.h:233:60: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n                        __FILE__, __LINE__, #a, #b, (a), (b));        \\\r\n                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~\r\n.//tensorflow/lite/c/common.h:161:39: note: expanded from macro 'TF_LITE_KERNEL_LOG'\r\n   (context)->ReportError((context), __VA_ARGS__); \\\r\n   ~                                 ^~~~~~~~~~~\r\n/tensorflow/lite/micro/kernels/softmax.cc:55:51: error: implicit conversion increases floating-point precision: 'float' to 'double' \r\n     TF_LITE_ENSURE_NEAR(context, output->params.scale, 1.f / 32768,\r\n     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~\r\n.//tensorflow/lite/c/common.h:233:55: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n                        __FILE__, __LINE__, #a, #b, (a), (b));        \\\r\n                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~\r\n.//tensorflow/lite/c/common.h:161:39: note: expanded from macro 'TF_LITE_KERNEL_LOG'\r\n   (context)->ReportError((context), __VA_ARGS__); \\\r\n   ~                                 ^~~~~~~~~~~\r\n/tensorflow/lite/micro/kernels/softmax.cc:61:9: error: implicit conversion increases floating-point precision: 'float' to 'double' \r\n       TF_LITE_ENSURE_NEAR(context, output->params.scale, 1.f / 65536,\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n.//tensorflow/lite/c/common.h:231:9: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n   if (delta > epsilon) {                                             \\\r\n       ^~~~~ ~\r\n/tensorflow/lite/micro/kernels/softmax.cc:62:38: error: implicit conversion increases floating-point precision: 'float' to 'double' \r\n                           (0.001 * 1.f / 65536));\r\n                                  ~ ^~~\r\n.//tensorflow/lite/c/common.h:231:17: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n   if (delta > epsilon) {                                             \\\r\n               ^~~~~~~\r\n/tensorflow/lite/micro/kernels/softmax.cc:61:64: error: implicit conversion increases floating-point precision: 'float' to 'double' \r\n       TF_LITE_ENSURE_NEAR(context, output->params.scale, 1.f / 65536,\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~\r\n.//tensorflow/lite/c/common.h:233:60: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n                        __FILE__, __LINE__, #a, #b, (a), (b));        \\\r\n                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~\r\n.//tensorflow/lite/c/common.h:161:39: note: expanded from macro 'TF_LITE_KERNEL_LOG'\r\n   (context)->ReportError((context), __VA_ARGS__); \\\r\n   ~                                 ^~~~~~~~~~~\r\n/tensorflow/lite/micro/kernels/softmax.cc:61:53: error: implicit conversion increases floating-point precision: 'float' to 'double' \r\n       TF_LITE_ENSURE_NEAR(context, output->params.scale, 1.f / 65536,\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~\r\n.//tensorflow/lite/c/common.h:233:55: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n                        __FILE__, __LINE__, #a, #b, (a), (b));        \\\r\n                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~\r\n.//tensorflow/lite/c/common.h:161:39: note: expanded from macro 'TF_LITE_KERNEL_LOG'\r\n   (context)->ReportError((context), __VA_ARGS__); \\\r\n   ~                                 ^~~~~~~~~~~\r\n/tensorflow/lite/micro/kernels/softmax.cc:76:31: error: implicit conversion increases floating-point precision: 'float' to 'double' \r\n         input->params.scale * params->beta /\r\n         ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~ ~\r\n/tensorflow/lite/micro/kernels/softmax.cc:152:51: error: implicit conversion increases floating-point precision: 'float' to 'double' \r\n   gen_lut([](float value) { return 1.0 / (1.0 + value); }, 0.0, 1.0,\r\n                                               ~ ^~~~~\r\n/tensorflow/lite/micro/kernels/softmax.cc:130:9: error: unused variable 'params' [-Werror,-Wunused-variable]\r\n auto* params = reinterpret_cast<TfLiteSoftmaxParams*>(node->builtin_data);\r\n\r\n`", "> @sicong-li-arm please check below errors\r\n> \r\n> `tensorflow/lite/micro/testing/test_utils.cc:16:\r\n> .//tensorflow/lite/micro/testing/test_utils.h:97:19: error: unused variable 'input_dims' [-Werror,-Wunused-variable]\r\n> TfLiteIntArray* input_dims = IntArrayFromInitializer(input_dims_data);\r\n\r\nHi @rthadur. Thanks for posting this. I forgot to turn on -Wall when building. I'll fix these soon.   ", "@sicong-li-arm can you please resolve conflicts.", "> @sicong-li-arm can you please resolve conflicts.\r\n\r\n@rthadur Done. Thanks for the reminder!", "Hi @rthadur. Could you please have a look once you're available? Thanks! :) ", "@sicong-li-arm can you please check below errors ?\r\n\r\n`tensorflow/lite/micro/kernels/softmax.cc:56:31: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n                         1.f / 32768, (0.001 * 1.f / 32768));\r\n                         ~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/lite/c/common.h:230:26: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n   auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));              \\\r\n                     ~  ^\r\ntensorflow/lite/micro/kernels/softmax.cc:56:31: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n                         1.f / 32768, (0.001 * 1.f / 32768));\r\n                         ~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/lite/c/common.h:230:40: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n   auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));              \\\r\n                                   ~  ^\r\ntensorflow/lite/micro/kernels/softmax.cc:56:31: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n                         1.f / 32768, (0.001 * 1.f / 32768));\r\n                         ~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/lite/c/common.h:230:48: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n   auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));              \\\r\n                                              ^  ~\r\ntensorflow/lite/micro/kernels/softmax.cc:56:49: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n                         1.f / 32768, (0.001 * 1.f / 32768));\r\n                                             ~ ^~~\r\n./tensorflow/lite/c/common.h:231:17: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n   if (delta > epsilon) {                                             \\\r\n               ^~~~~~~\r\ntensorflow/lite/micro/kernels/softmax.cc:56:31: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n                         1.f / 32768, (0.001 * 1.f / 32768));\r\n                         ~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/lite/c/common.h:233:60: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n                        __FILE__, __LINE__, #a, #b, (a), (b));        \\\r\n                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~\r\n./tensorflow/lite/c/common.h:161:39: note: expanded from macro 'TF_LITE_KERNEL_LOG'\r\n   (context)->ReportError((context), __VA_ARGS__); \\\r\n   ~                                 ^~~~~~~~~~~\r\ntensorflow/lite/micro/kernels/softmax.cc:62:33: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n                           1.f / 65536, (0.001 * 1.f / 65536));\r\n                           ~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/lite/c/common.h:230:26: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n   auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));              \\\r\n                     ~  ^\r\ntensorflow/lite/micro/kernels/softmax.cc:62:33: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n                           1.f / 65536, (0.001 * 1.f / 65536));\r\n                           ~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/lite/c/common.h:230:40: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n   auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));              \\\r\n                                   ~  ^\r\ntensorflow/lite/micro/kernels/softmax.cc:62:33: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n                           1.f / 65536, (0.001 * 1.f / 65536));\r\n                           ~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/lite/c/common.h:230:48: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n   auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));              \\\r\n                                              ^  ~\r\ntensorflow/lite/micro/kernels/softmax.cc:62:51: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n                           1.f / 65536, (0.001 * 1.f / 65536));\r\n                                               ~ ^~~\r\n./tensorflow/lite/c/common.h:231:17: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n   if (delta > epsilon) {                                             \\\r\n               ^~~~~~~\r\ntensorflow/lite/micro/kernels/softmax.cc:62:33: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n                           1.f / 65536, (0.001 * 1.f / 65536));\r\n                           ~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/lite/c/common.h:233:60: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n                        __FILE__, __LINE__, #a, #b, (a), (b));        \\\r\n                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~\r\n./tensorflow/lite/c/common.h:161:39: note: expanded from macro 'TF_LITE_KERNEL_LOG'\r\n   (context)->ReportError((context), __VA_ARGS__); \\\r\n   ~                                 ^~~~~~~~~~~\r\ntensorflow/lite/micro/kernels/softmax.cc:152:51: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n   gen_lut([](float value) { return 1.0 / (1.0 + value); }, 0.0, 1.0,\r\n\r\n`", "> @sicong-li-arm can you please check below errors ?\r\n> \r\n> `tensorflow/lite/micro/kernels/softmax.cc:56:31: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n> 1.f / 32768, (0.001 * 1.f / 32768));\r\n> ~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> ./tensorflow/lite/c/common.h:230:26: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n> auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));\r\n> ~ ^\r\n> tensorflow/lite/micro/kernels/softmax.cc:56:31: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n> 1.f / 32768, (0.001 * 1.f / 32768));\r\n> ~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> ./tensorflow/lite/c/common.h:230:40: note: expanded from macro 'TF_LITE_ENSURE_NEAR'\r\n> auto delta = ((a) > (b)) ? ((a) - (b)) : ((b) - (a));\r\n> ~ ^\r\n> tensorflow/lite/micro/kernels/softmax.cc:56:31: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n> 1.f / 32768, (0.001 * 1.f / 32768));\r\n\r\n\r\nHi @rthadur thanks for posting this. I wonder if the -Wdouble-promotion is strictly required? The implicit promotion to double seems to be required by the printf style format from the error reporter. I could explicitly cast them to double of course, but the implicit promotion seems to be everywhere within the codebase?  ", "> This change is failing internal tests due to additional cflags that are applied internally but not on the OSS builds:\r\n> -Wdouble-promotion -Werror\r\n> \r\n> WIth these tow flags added to CXXFLAGS in micro/tools/make/Makefile, I can reproduce the errors that we are seeing internally.\r\n> \r\n> That the flags are different for the internal and external builds is a bug and we have a change that is being reviewed that should fix that issue. In the meantime, if you could rebase, resolve conflicts and then locally apply the additional cflags, you can likely get this change to a state that it passes internal and external presubmit checks.\r\n\r\nHi @advaitjain thanks for the review! I have pushed fixes for the double promotion. ", "@sicong-li-arm Can you please resolve conflicts? Thanks!", "Hi @gbaned! Resolved the conflicts and (force) rebased. Thanks! :)   ", "Hi @gbaned @advaitjain ! All checks have passed! Could you please have a look to see if this PR can be merged? Thanks! :)", "@sicong-li-arm can you please check below errors \r\n\r\n`third_party/tensorflow/lite/micro/kernels/softmax.cc:153:66: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n    gen_lut([](float value) { return std::exp(value); }, -10.0f, 0.0f,\r\n\r\nthird_party/tensorflow/lite/micro/kernels/softmax.cc:153:58: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n    gen_lut([](float value) { return std::exp(value); }, -10.0f, 0.0f,\r\n\r\nthird_party/tensorflow/lite/micro/kernels/softmax.cc:156:70: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n    gen_lut([](float value) { return 1.0f / (1.0f + value); }, 0.0f, 1.0f,\r\n\r\nthird_party/tensorflow/lite/micro/kernels/softmax.cc:156:64: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n    gen_lut([](float value) { return 1.0f / (1.0f + value); }, 0.0f, 1.0f,\r\n    ~~~~~~~                                                    ^~~~\r\n4 errors generated.`", "> @sicong-li-arm can you please check below errors\r\n> \r\n> `third_party/tensorflow/lite/micro/kernels/softmax.cc:153:66: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n> gen_lut([](float value) { return std::exp(value); }, -10.0f, 0.0f,\r\n> \r\n> third_party/tensorflow/lite/micro/kernels/softmax.cc:153:58: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n> gen_lut([](float value) { return std::exp(value); }, -10.0f, 0.0f,\r\n> \r\n> third_party/tensorflow/lite/micro/kernels/softmax.cc:156:70: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n> gen_lut([](float value) { return 1.0f / (1.0f + value); }, 0.0f, 1.0f,\r\n> \r\n> third_party/tensorflow/lite/micro/kernels/softmax.cc:156:64: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n> gen_lut([](float value) { return 1.0f / (1.0f + value); }, 0.0f, 1.0f,\r\n> ~~~~~~~ ^~~~\r\n> 4 errors generated.`\r\n\r\nHi @rthadur ! gen_lut expected double values instead of floats, hence the error.  @petewarden This means this will need to be reverted back to double. Would this be acceptable? An alternative would be to copy the gen_lut function over but with float computation.\r\n\r\nI'll resolve the conflicts and push for a fix as soon as @petewarden approves. In the meantime @rthadur could you please share what tests you ran to produce this error? Somehow my local `make -f tensorflow/lite/micro/tools/make/Makefile ...` , even with double-promotion warning on failed to catch this error (maybe because it's invoking a function from lite instead of micro?).", "Could you add a float variant of the gen_lut function? I'd imagine that will be a lot faster on the Cortex M cores, since they only support single precision. Thanks!", "Hi @petewarden @rthadur. I resolved the conflicts and pushed a fix. Could you please have a look when you are available? Thanks!  ", "@sicong-li-arm  Still, conflicts are appearing. Can you please resolve those? Thanks!", "Hi @gbaned @rthadur, the conflict is resolved. Thanks! ", "@sicong-li-arm Still, conflicts are appearing. Can you please resolve those? Thanks!", "@petewarden  Can you please assist on above comments from @sicong-li-arm. Thanks!", "Hi @petewarden @gbaned @njeffrie  ! Sorry for leaving this hanging as I was occupied with something else. I'd like to finish this asap, so I'll try my best to respond to this PR every day.\r\n\r\nSince the commit history becomes stale with each forced push, I'll provide a summary of the changes and remaining issues from the last push:\r\n\r\n1. Rebased on top of @njeffrie 's latest changes with the new test approach (the new approach looks much nicer btw) and added 4 new tests for int16 in accordance with the new approach.\r\n\r\n2. Removed redundant test helper methods (Dequantize, F2Q16S, CreateQuantizedTensor, etc.), as a result of the previous bullet.\r\n\r\n3. (With question) Increased the tolerance for 3D and 4D int16 tests from 1 to 7. I wonder if such a tolerance is acceptable? I reckon this inaccuracy mainly stems from the conversion of the floating golden outputs to int16. Alternatively we can have another golden outputs in int16 directly, and test with bit-exactness.\r\n\r\n4. (With question) Following [this discussion](https://github.com/tensorflow/tensorflow/pull/38873#discussion_r460820208,) above, and some suggestions from others, I tried allocating the user_data in prepare instead of init, in order to conditionally allocate based on the tensor type. It passed the kernel tests but since user_data is supposed to be allocated by init, I wonder if this approach is the right way forward? \r\n\r\nThanks in advance!", "Hi @petewarden @gbaned @njeffrie could you please have a look at the new changes and the remaining questions? Thanks ! :) ", "@njeffrie could you take a look if you get a chance? Thanks!", "Hi @njeffrie could you please have a look at the new changes and the questions? Thanks! ", "Hi @njeffrie @advaitjain could you please have a look at my latest commit addressing the issues we discussed? Thanks! :)", "Thank you all! :)"]}, {"number": 38872, "title": "[TF2.2] Mixed Ragged and tensors inputs breaks function tensors spec names", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): tf2.2 rc3\r\n- TensorFlow version (use command below):2.2 rc3 collab\r\n- Python version: google collab\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI'm trying to save a model with two inputs:\r\n- one ragged\r\n- one tensor\r\n\r\nboth are embedded into a 3 dim vector and then summed up together. If we save that model it breaks because the tensors names are not well defined. \r\n\r\n```\r\ninp_tensor = Input(shape=[], ragged=False, dtype=tf.int32)\r\ninp_ragged = Input(shape=[None, ], ragged=True, dtype=tf.int32, name='test')\r\n\r\nout = Embedding(10, 3)(inp_tensor)\r\nout2 = Embedding(10, 3)(inp_ragged)\r\nout2 = Lambda(tf.reduce_sum, arguments=dict(axis=1))(out2)\r\nsummed = Add()([out, out2])\r\nm = Model([inp_tensor, inp_ragged], summed)\r\nm.save('/tmp/test')\r\n```\r\n> ValueError: If specifying TensorSpec names for nested structures, either zero or all names have to be specified.\r\n\r\nIf we debug we find out that the non ragged tensor has a name while the two others have name=None. I tried to name the ragged_input to fix that but the issue stays the same. \r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py(1196)_get_defun_inputs()\r\n   1194     specified_names = [arg.name for arg in tensor_specs if arg.name]\r\n   1195     if specified_names and len(specified_names) < len(tensor_specs):\r\n-> 1196       raise ValueError(\"If specifying TensorSpec names for nested structures, \"\r\n   1197                        \"either zero or all names have to be specified.\")\r\n   1198 \r\n\r\nipdb> specified_names\r\n['inputs/0']\r\nipdb> tensor_specs\r\n[TensorSpec(shape=(None,), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None,), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None)]\r\n```\r\n**Describe the expected behavior**\r\nWe should be able to save that model, the same model with two ragged inputs can be saved !\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/gist/tanguycdls/200096266d1cb0c5a6c83cbb416c1279/untitled4.ipynb\r\n", "comments": ["I tried in colab with TF 2.1 and i am not seeing any issue. However i am able to reproduce the issue in colab with TF 2.2.0-rc3. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/8e6c30bbab16d2929cae9652a30c87e7/untitled821.ipynb).Thanks!", "Facing same issue with 2.2.0", "@shkarupa-alex seems to be fixed in the latest tf-nightly :) !\r\nhttps://colab.research.google.com/gist/tanguycdls/200096266d1cb0c5a6c83cbb416c1279/untitled4.ipynb#scrollTo=qWJanvalyxiJ \r\n\r\nI'm going to do some thorough tests to be sure. ", "@shkarupa-alex Agree with @tanguycdls . Looks like this was resolved in `tf-nightly`. Please take a look at the [gist](https://colab.research.google.com/gist/jvishnuvardhan/fa7dabf593f61fb8bd76a5a46b7b789a/untitled4.ipynb). Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "Yes, it's working now", "I am closing this issue as this was resolved. Thanks for the confirmation. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38872\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38872\">No</a>\n", "fyi closed by that commit: https://github.com/tensorflow/tensorflow/commit/d1a6976039c769423814f690661fd2c1b471b08c"]}, {"number": 38871, "title": "Quantize Model for Deploying on the Cloud using TF Serving", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2.0rc3\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nActually, Tensorflow Lite is assumed as the default for model optimization, but it is focused on Edge Devices, what are the alternatives to Cloud Deployments with Tensorflow Serving? The docs do not seem to have some guidance about this topic. How to Quantize a model that will be deployed on the Cloud using TensorFlow Serving?\r\n\r\nThis StackOverflow post from 1 month ago with no responses have the same problem: https://stackoverflow.com/questions/60491542/tensorflow-model-quantization-best-strategy (I'm not the poster of that question)\r\n\r\nPD: This type of question never receives responses in StackOverflow, so, there's no more option than asks for support here, Thanks in advance!\r\n", "comments": ["Any advances or hints on this topic? at least some guidance about how to quantize or optimize models fo being deployed on Tensorflow Serving?", "Hello there, i found a solution, i don't test it yet, but seems legit:\r\n\r\nTensorflow Serving has a parameter for load tflite flat buffers instead .pb SavedModels: https://github.com/tensorflow/serving/blob/master/tensorflow_serving/model_servers/main.cc#L191\r\n\r\nCheck the parameter there, it is experimental, but you can try it. Just modify your Dockerfile to include that flag."]}, {"number": 38870, "title": "Gradients for tf.py_function with mixed arguments", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 19.3\r\n- TensorFlow installed from (source or binary): conda binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\ngradient calculation throws an error if a py_function is used that has both integer and floating point inputs/outputs.\r\n\r\n**Describe the expected behavior**\r\nGradients with respect to all integers should be zero/None, and others should be correctly calculated.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ndef pf(x, y):\r\n    return x ** 2, y ** 2\r\n\r\ndef pyf(x, y):\r\n    return tf.py_function(pf, [x, y], [tf.int32, tf.float32])\r\n\r\nx = tf.constant(5)\r\nv = tf.Variable(0.5)\r\nwith tf.GradientTape() as tape:\r\n    y, m = pyf(x, v)\r\n    z = tf.cast(y, tf.float32) * m\r\n\r\nprint(tape.gradient(z, v))\r\n```\r\nWhen calling `pf`, gradient computation works, but for `pyf` we get first a warning and then an error. \r\n\r\n\r\nThe problematic code seems to be in `script_ops.py`:\r\n```\r\n@ops.RegisterGradient(\"EagerPyFunc\")\r\ndef _EagerPyFuncGrad(op, *dy):\r\n  \"\"\"Computes the gradient of an EagerPyFunc.\"\"\"\r\n\r\n  token = op.get_attr(\"token\")\r\n\r\n  def eagerly_executed_grad(*dy):\r\n    tape, eager_inputs, eager_outputs = tape_cache.pop(compat.as_bytes(token))\r\n    return tape.gradient(eager_outputs, eager_inputs, output_gradients=dy)\r\n\r\n  with ops.control_dependencies(op.outputs):\r\n    return _internal_py_func(\r\n        func=eagerly_executed_grad,\r\n        inp=dy,\r\n        Tout=[tensor.dtype for tensor in op.inputs],\r\n        eager=True,\r\n        is_grad_func=True)\r\n```", "comments": ["@ngc92 \r\ni ran your code in nightly and do not face any error, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/9ef6582d4190888740409bc944dae316/38870.ipynb)\r\nthe error faced in 2.1 is as per [this gist](https://colab.sandbox.google.com/gist/Saduf2019/cbed748edb5e596abacd846dfcc28b32/untitled155.ipynb), can you please confirm.", "yes, this is the expected error. This is strange, because as far as I can see the code for `def _EagerPyFuncGrad` has not been changed. Do you know if there is already a test case for this behaviour?\r\n\r\nEdit: I also tried with 2.2rc3, there the error still seems to exist.", "if it doesn't reproduce in nightly let's close the issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38870\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38870\">No</a>\n", "does a test case for this behaviour already exist?", "I see a similar issue, also using a pyfunc and get the error message \"Tensor is unhashable. Instead, use tensor.ref() as the key\". It seems to be raised also from within \"script_ops.py:363 _EagerPyFuncGrad\". I don't have code to reproduce the issue. I'm using tf 2.2.0 on Ubuntu Linux."]}, {"number": 38869, "title": "[TFLite 16x8] Versioning of 16x8 reference kernels", "body": "In this PR \r\nI increase version for all merged reference kernels with activations - int16 and weights - int8.\r\n", "comments": ["Hi @gbaned Sorry to bother, but could you please check whether this PR has stuck ? It is marked as 'ready-to-pull' for 6 days. We have another PRs that we want to upstream and they will be in conflict with this one.\r\nThanks a lot!", "Hi @rthadur ! Sorry to bother, but could you please check whether this PR has stuck with the internal failures ? Thanks!"]}, {"number": 38868, "title": "Distribution algorithm has limited performance on CPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): (Red Hat 4.8.5-16)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source build\r\n- TensorFlow version (use command below): 2.1 \r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): 7.4.0 \r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nRandom generator uses specific philox_4x32_10 algorithm. It will generate 4 elements per [call](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/random/random_distributions.h#L114) in Distribution and do additional computation for these 4 elements. Is there any special consideration for this design? As you know, 4 elements are not very friendly for CPU AVX feature, especial for those low precision type(Half, Bfloat16) w/o native instructs support, because **conversion**\u00a0overhead will become obvious if we can't fulfill vectorization register, that will make bad overall performance for the [result](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/random/random_distributions.h#L764).\r\n\r\n**Describe the expected behavior**\r\nIs it possible to handle the **conversion** and **computation** out of the Distribution? I mean handle them in the place where is needed. For example, `RandomUniform` op calls Distribution [here ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/random_op_cpu.h#L95) many times and we may handle data out of the Distribution and loop. The fake code is somehow like this\r\n```C\r\n// https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/random/random_distributions.h\r\n// Helper function to convert an 16-bit integer to a half between [1..2).\r\nPHILOX_DEVICE_INLINE Eigen::half NewUint16ToHalf(uint16 x) {\r\n  // IEEE754 halfs are formatted as follows (MSB first):\r\n  //    sign(1) exponent(5) mantissa(10)\r\n  // Conceptually construct the following:\r\n  //    sign == 0\r\n  //    exponent == 15  -- an excess 15 representation of a zero exponent\r\n  //    mantissa == 10 random bits\r\n  const uint16 man = x & 0x3ffu;  // 10 bit mantissa\r\n  const uint16 exp = static_cast<uint16>(15);\r\n  const uint16 val = (exp << 10) | man;\r\n\r\n  Eigen::half result;\r\n  result.x = val;\r\n  /***********************************************\r\n      do not do the conversion and minus here.\r\n      return result - Eigen::half(1.0);\r\n  ************************************************/\r\n  return result;\r\n}\r\n\r\n...\r\n    // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/random_op_cpu.h#L95\r\n    gen.Skip(start_group);\r\n    int64 offset = start_group * kGroupSize;\r\n\r\n    // First fill all the full-size groups\r\n    int64 limit_group_full = std::min(limit_group, size / kGroupSize);\r\n    for (int64 index = start_group; index < limit_group_full; ++index) {\r\n      /********************************************************\r\n        Use new Distribution without conversion and minus\r\n        auto samples = dist(&gen);\r\n      *********************************************************/\r\n      auto samples = new_dist(&gen);\r\n      std::copy(&samples[0], &samples[0] + kGroupSize, data + offset);\r\n      offset += kGroupSize;\r\n    }\r\n    /****************************************************************************\r\n      Do the vector conversion and minus out of loop to get CPU AVX feature\r\n    *****************************************************************************/\r\n    new_dist.vectorize(start, length);\r\n```\r\n\r\nIs this kind optimization reasonable?\r\n\r\n**Standalone code to reproduce the issue**\r\nI made some simple test, low precision Distribution are usually slower 10~30%  with current implementation.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@Zantares \r\n\r\nWill it be possible to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "> @Zantares\r\n> \r\n> Will it be possible to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n\r\nOfc, I would like to share a performance UT in TF once I finished it.\r\nBTW, there's better solution to solve this performance issue like to optimize Generator algorithm, but I think it's more complex.\r\n", "Please check UT in \r\n[ut_diff.txt](https://github.com/tensorflow/tensorflow/files/4549767/ut_diff.txt), I ran it with cmd on x86 18 cores server:\r\nnumactl -N 0 -l /home/tenglu/miniconda3/lib/bazel/bin/bin/bazel run --config=mkl --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mavx512f --copt=-mavx512cd --copt=-mavx512dq --copt=-DENABLE_INTEL_MKL_BFLOAT16 --verbose_failures --test_timeout=7200 -c opt -- //tensorflow/core/kernels:random_op_test -- --benchmarks=..\r\n\r\nand got :\r\n```\r\nExecuting tests from //tensorflow/core/kernels:random_op_test\r\n-----------------------------------------------------------------------------\r\nRunning main() from test_main.cc\r\nBenchmark                                  Time(ns) Iterations\r\n--------------------------------------------------------------\r\nBM_cpu_RandomUniform_DT_FLOAT/1048576       1276092        687\t 821.7M items/s\r\nBM_cpu_RandomUniform_DT_FLOAT/2097152       2364219        320\t 887.0M items/s\r\nBM_cpu_RandomUniform_DT_FLOAT/8388608       9888000        100\t 848.4M items/s\r\nBM_cpu_RandomUniform_DT_HALF/1048576        1455563        565\t 720.4M items/s\r\nBM_cpu_RandomUniform_DT_HALF/2097152        2887788        222\t 726.2M items/s\r\nBM_cpu_RandomUniform_DT_HALF/8388608       11319790        100\t 741.1M items/s\r\nBM_cpu_RandomUniform_DT_BFLOAT16/1048576    1839554        480\t 570.0M items/s\r\nBM_cpu_RandomUniform_DT_BFLOAT16/2097152    3642229        223\t 575.8M items/s\r\nBM_cpu_RandomUniform_DT_BFLOAT16/8388608   13337270        100\t 629.0M items/s\r\n```\r\n\r\nOne of the bottleneck is that I mentioned, low precision type will cost more on conversion in `Helper Function` of Distribution.\r\n\r\n", "We have figured out a generic optimization for this issue and submitted it here: https://github.com/tensorflow/tensorflow/pull/39747.", "@Zantares,\r\nSorry for the delayed response. Can you please confirm if we can close this issue as the [associated PR](https://github.com/tensorflow/tensorflow/pull/39747) has been merged? Thanks!", "> @Zantares,\r\n> Sorry for the delayed response. Can you please confirm if we can close this issue as the [associated PR](https://github.com/tensorflow/tensorflow/pull/39747) has been merged? Thanks!\r\n\r\nThanks, this issue can be closed now."]}]