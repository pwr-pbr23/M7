[{"number": 21982, "title": "`tf.gfile.Rename` failed to operate on S3", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04, Mac OS High Sierra 10.13.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Binary for Ubuntu, source for Mac OS\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.15.2-homebrew (Mac OS)\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.1.0 (clang-902.0.39.2) (Mac OS)\r\n- **CUDA/cuDNN version**: cuda 9.0, cudnn 7.1 (Ubuntu)\r\n- **GPU model and memory**: TITAN V\r\n- **Exact command to reproduce**:\r\n\r\n1. Upload a file to s3, say \"s3://test/file.txt\"\r\n2. Rename the file through `tf.gfile.Rename` to \"s3://test/file_renamed.txt\" using the following code\r\n```python\r\nimport tensorflow as tf\r\nsrc = \"s3://test/file.txt\"\r\ndst = \"s3://test/file_renamed.txt\"\r\ntf.gfile.Rename(src, dst)\r\n```\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI believe it is a bug in TensorFlow.\r\n\r\nA problem is that this function doesn't seem to work for s3 storage, another problem is that the content of `InternalError` is empty (with only a single colon, see below).\r\n\r\nThis problem is first noticed when I was trying to start a distributed training session using Estimator. This problem comes out when the underlying `CheckpointSaverHook` tries to save the graph serialization. In the call `graph_io.write_graph`, the `file_io.atomic_write_string_to_file` function is called, which involves `file_io.wrtie_string_to_file` followed by a `file_io.rename` call. The first call succeeded, but the second one failed. I came up with the above reproduction code after going through the error stack, until I hit the empty `InternalError` that I got no information to work with.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nNote that `src` and `dst` are s3 paths, the following error is extracted from an interactive python session.\r\n```\r\n>>> tf.gfile.Rename(src, dst)\r\n2018-08-30 10:38:20.947227: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /root//.aws/config and using profilePrefix = 1\r\n2018-08-30 10:38:20.947301: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /root//.aws/credentials and using profilePrefix = 0\r\n2018-08-30 10:38:20.947333: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /root//.aws/credentials for credentials file and /root//.aws/config for the config file , for use with profile default\r\n2018-08-30 10:38:20.947379: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating HttpClient with max connections2 and scheme http\r\n2018-08-30 10:38:20.947415: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2\r\n2018-08-30 10:38:20.947443: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 900000\r\n2018-08-30 10:38:20.947483: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2018-08-30 10:38:20.947582: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25\r\n2018-08-30 10:38:20.947677: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2018-08-30 10:38:20.948021: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2\r\n2018-08-30 10:38:20.948051: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2018-08-30 10:38:21.031694: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\r\n2018-08-30 10:38:21.031791: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\r\n2018-08-30 10:38:21.031929: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2018-08-30 10:38:21.032102: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2018-08-30 10:38:21.051376: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2018-08-30 10:38:21.051520: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2018-08-30 10:38:21.073577: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2018-08-30 10:38:21.073852: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2018-08-30 10:38:21.152881: W tensorflow/core/platform/s3/aws_logging.cc:57] Unable to generate a proper httpResponse from the response stream.   Response code: 404\r\n2018-08-30 10:38:21.153006: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/miniconda3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 415, in rename\r\n    compat.as_bytes(oldname), compat.as_bytes(newname), overwrite, status)\r\n  File \"/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 519, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InternalError: : \r\n>>> \r\n```", "comments": ["Closing this issue as it worked on the AWS S3 instances. This problem probably arises from my company's internal S3 implementation "]}, {"number": 21981, "title": "MKL DNN: Softmax dim fix: Supporting 1D to 5D tensors, address the regression of CI", "body": "Softmax test failed because recently some tests uses 3D tensors as input. Before, MKL softmax only supported 2D tensors. Now, in this PR, we are supporting 1D, 2D, 3D, 4D and 5D tensors as input to Softmax. \r\nThus, this PR fixes the CI regression.", "comments": [" @tatianashp "]}, {"number": 21980, "title": "Variable in a while loop - Segmentation fault (core dumped) ", "body": "I use TF 1.10.0, build from master last week.\r\n\r\nI am trying to make a function to erase patches in images and wanted to apply it with map to a Dataset. I use scatter_nd_update, so I had to define a variable that would be updated outside of the loop and assign tensor to it within the loop, but then I get segmentation fault.\r\n\r\n\r\nLog and the code\r\n\r\n```\r\nUsing TensorFlow backend.\r\n2018-08-30 21:36:51.556778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-08-30 21:36:51.557216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.03GiB\r\n2018-08-30 21:36:51.557236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-30 21:36:51.755752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-30 21:36:51.755792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \r\n2018-08-30 21:36:51.755799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \r\n2018-08-30 21:36:51.755996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9692 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-08-30 21:36:51.840570: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nUsing device /gpu:0, and data format channels_last.\r\nStarting\r\nTensor(\"learning_rate_decay:0\", shape=(), dtype=float32)\r\nWARNING:tensorflow:case: An unordered dictionary of predicate/fn pairs was provided, but exclusive=False. The order of conditional tests is deterministic but not guaranteed.\r\n(300, 28, 28, 1)\r\n2018-08-30 21:36:52.518523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-30 21:36:52.518567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-30 21:36:52.518576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \r\n2018-08-30 21:36:52.518584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \r\n2018-08-30 21:36:52.518686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9692 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-08-30 21:36:52.518876: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nSegmentation fault (core dumped)\r\n```\r\n\r\n```\r\ndef random_erase(img_z, img, label, width=28, height=28):\r\n        #with g.as_default():\r\n        a = tf.random_uniform(minval=0,   maxval=width-1, shape=[], dtype=tf.int32)\r\n        b = tf.random_uniform(minval=a+1, maxval=width,   shape=[], dtype=tf.int32)\r\n\r\n        c = tf.random_uniform(minval=0,   maxval=height-1, shape=[], dtype=tf.int32)\r\n        d = tf.random_uniform(minval=c+1, maxval=height,   shape=[], dtype=tf.int32)\r\n\r\n        h = tf.range(start=a, limit=b, dtype=tf.int32)\r\n        v = tf.range(start=c, limit=d, dtype=tf.int32)\r\n\r\n        h, v = h[ None, :, None ], v[ :, None, None ]\r\n        cartesian_product = tf.concat( [ h + tf.zeros_like( v ),\r\n                                        tf.zeros_like( h ) + v ], axis = 2, name=\"caretsian_product\")\r\n        \r\n        random_patch = tf.random_uniform(minval=0, maxval=255, shape=[d-c,b-a], dtype=tf.float32, name=\"random_patch\")\r\n        return tf.scatter_nd_update(tf.assign(img_z, img), cartesian_product, random_patch, name=\"image_with_patch\"), label\r\n\r\n```\r\n\r\nand I use it in\r\n\r\n```\r\n    img_z = tf.Variable(tf.zeros(shape=[28*28], dtype=tf.float32),\r\n                            trainable=False, use_resource=True)\r\n    ds = mnist_train('.')\r\n    ds = ds.map(lambda img, label:random_erase(img_z, img, label))\r\n```", "comments": ["I will post a workaround or what was the issue when I find it, as for now I will close the ticket as I think the problem is with my code not TF itself."]}, {"number": 21979, "title": "[Feature Request] Keeping Keras Callback functionality in Estimators created from `keras.model_to_estimator`", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: n/a\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: n/a\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\n\r\nCurrently if I want to define a model in Keras and train it in a distributed fashion, the easiest/most-apparent method is to convert it into an Estimator using [model_to_estimator](https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator). Unfortunately this has a few issues, and one of them is that any [Keras Callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback) go unpreserved.\r\n\r\nTheoretically these callbacks could be converted into some form of [`SessionRunHook`](https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook) although the SessionRunHook doesn't know things that the Callback does (e.g. epoch information). However, this does not happen at all -- the `EstimatorSpec` that's returned out of the `model_fn` from `_create_keras_model_fn` [does not specify any sort of `training_hooks`](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/estimator/keras.py#L400) regardless of whether or not the Keras model has Callbacks defined.\r\n\r\nI'm sure this is a non-trivial problem to solve, however it is frustrating that `model_to_estimator` causes a loss in functionality like this.\r\n\r\nAre there any plans to remedy this going forward?\r\n\r\nThanks!", "comments": ["Any progress on it? \ud83d\ude80 ", "Wait yeah this is kinda big... I wish I knew that before spending a couple hours trying to decipher tf.estimator docs and outdated examples getting the rest of it set up.", "Hi, any news about this or another way to do the same thing ?", "Hi, any news about this or another way to do the same thing ?\r\n\r\n", "This is an important issue. With a Keras model, you can reduce the learning rate on plateau, then stop training if the loss still does not improve. Powerful, and avoids a lot of experiment with LR schedules.", "@zmjjmz,\r\nPlease refer [this tutorial](https://www.tensorflow.org/tutorials/distribute/keras) which shows how to use **`Keras Callbacks`**, `TensorBoard`, `ModelCheckpoint`, `LearningRateScheduler` etc.. and to `Train` the `Keras Model` in `Distributed Fashion`.\r\n\r\nPlease refer [this Guide](https://www.tensorflow.org/guide/distributed_training) for more information on `Training` a `Tensorflow Keras Model` in `Distributed Fashion`.\r\n\r\nHope this helps. Thanks!", "Hey there,\r\n\r\nGlad to see that 2.5 years later, this is now possible :) I'll close this issue."]}, {"number": 21978, "title": "Problem with TF runtime when running Ape-X code", "body": "I ran the following command. \r\n`(env) hkaushal-mac01:ape-x hkaushal$ python apex.py --env video_pinball --num-timesteps 1000000000 --logdir=/tmp/agent\r\n`\r\n\r\nI am trying to run through the [ape-x code ](https://github.com/uber-research/ape-x), and the above errors are generated.\r\n\r\nPython 2.7.15 :: Anaconda, Inc. ( Python version) \r\n The other issues opened regarding this didn't help my case, and there are no stack-overflow posts regarding this as well since Ape-X was released only a day ago.\r\nI'm trying to operate under gym_tensorflow of the Ape-X code for some context. \r\nThanks! \r\n\r\n\r\n\r\n\r\n```\r\nraceback (most recent call last):\r\n  File \"/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib\r\n  Referenced from: /Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Reason: image not found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"apex.py\", line 20, in <module>\r\n    import tensorflow as tf\r\n  File \"/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib\r\n  Referenced from: /Users/hkaushal/Documents/env/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Reason: image not found\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n```\r\n\r\n`hkaushal-mac01:tools hkaushal$ bash ./tf_env_collect.sh \r\n`\r\n also gives me a similar error. ( this the command to provide the system information)\r\n\r\n\r\n```\r\ng++ -std=c++11 -shared -fPIC -I -I/external/nsync/public -L -D_GLIBCXX_USE_CXX11_ABI=0 -O2 -DGOOGLE_CUDA=1 -I/Users/hkaushal/Documents/ape-x/gym_tensorflow/atari-py/atari_py/ale_interface/src -I/Users/hkaushal/Documents/ape-x/gym_tensorflow/atari-py/atari_py/ale_interface/src/controllers -I/Users/hkaushal/Documents/ape-x/gym_tensorflow/atari-py/atari_py/ale_interface/src/os_dependent -I/Users/hkaushal/Documents/ape-x/gym_tensorflow/atari-py/atari_py/ale_interface/src/environment -I/Users/hkaushal/Documents/ape-x/gym_tensorflow/atari-py/atari_py/ale_interface/src/external -L/Users/hkaushal/Documents/ape-x/gym_tensorflow/atari-py/atari_py/ale_interface/build -framework Cocoa .//*.cpp .//atari/*.cpp -ltensorflow_framework -lale -o gym_tensorflow.so\r\n.//tf_env.cpp:19:10: fatal error: 'tensorflow/core/framework/op_kernel.h' file\r\n      not found\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n1 error generated.\r\n.//atari/tf_atari.cpp:18:10: fatal error: 'ale_interface.hpp' file not found\r\n#include <ale_interface.hpp>\r\n         ^~~~~~~~~~~~~~~~~~~\r\n1 error generated.\r\nmake: *** [gym_tensorflow.so] Error 1\r\n\r\n```\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It was an issue, but running the command which provided the system's details also generated an error, thus I could not provide more info. This has still not been resolved, no. ", "Nagging Assignee @jart: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is this still an issue for you? Did you try posting this issue on [uber-research/ape-x](https://github.com/uber-research/ape-x/issues)?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21977, "title": "(ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.10\r\n- **Python version**:\r\n3.6.2\r\n\r\nTensorflow CPU\r\n\r\n\r\n### Describe the problem\r\nwhenever trying to import tensorflow i get this error.(ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.)\r\n\r\n\r\n```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n```\r\nI know this problem has to do with the windows installation, some problem with the visual c++ compiler 2015 update 3.\r\nThe thing is i have them already and it doesnt work. I have tried using an Anaconda enviorment and it didnt work neither whe i used pip3 install --upgrade tensorflow there. But, when i install with the conda installer (conda install tensorflow) it worked perfectly. \r\n\r\nSo i assume the problem has to do with some path or something like that in my native enviorment. But i dont have a clue...", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@yifeif do you have any idea what would cause this?", "@tensorflowbutler Hi, im using the CPU version of it, i dont know if it has something to do because im really new to this. I test it in others computers and worked fine.\r\nWhat do you mean by exact command to reproduce? \r\nIm sorry for not having the knowledge to anser to your questions, im new to all this and im trying to learn...", "Potential duplicate of https://github.com/tensorflow/tensorflow/issues/19584\r\nWhat is your CPU make/model?", "Suspected duplicate of #19584\r\nPlease reopen if you have data to suggest otherwise.", "Same issue in PyCharm, Windows 10 64 bits,\r\nErrors:\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\nFailed to load the native TensorFlow runtime.\r\n\r\nCode in Python:\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nimport tensorflow as TF_NAMESPACE\r\n# Create tensor.\r\nTF_EXERCISE_msg_tensor = TF_NAMESPACE.string_join([\"Hellow \",\"TensorFlow!\"])\r\n# Launch session.\r\nwith TF_NAMESPACE.session as TF_EXERCISE_sess:\r\n    print(TF_EXERCISE_sess.run(TF_EXERCISE_msg_tensor))\r\n", "> ### System information\r\n> * **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n> * **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n>   Windows 10\r\n> * **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n> * **TensorFlow installed from (source or binary)**:\r\n>   Source\r\n> * **TensorFlow version (use command below)**:\r\n>   1.10\r\n> * **Python version**:\r\n>   3.6.2\r\n> \r\n> Tensorflow CPU\r\n> \r\n> ### Describe the problem\r\n> whenever trying to import tensorflow i get this error.(ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.)\r\n> \r\n> ```\r\n> >>> import tensorflow as tf\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n>     return importlib.import_module(mname)\r\n>   File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n>   File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n>   File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n>     return importlib.import_module('_pywrap_tensorflow_internal')\r\n>   File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n> ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n>   File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n>     return importlib.import_module(mname)\r\n>   File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n>   File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n>   File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n>     return importlib.import_module('_pywrap_tensorflow_internal')\r\n>   File \"C:\\Users\\Cesar\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n> ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n> ```\r\n> \r\n> I know this problem has to do with the windows installation, some problem with the visual c++ compiler 2015 update 3.\r\n> The thing is i have them already and it doesnt work. I have tried using an Anaconda enviorment and it didnt work neither whe i used pip3 install --upgrade tensorflow there. But, when i install with the conda installer (conda install tensorflow) it worked perfectly.\r\n> \r\n> So i assume the problem has to do with some path or something like that in my native enviorment. But i dont have a clue...\r\n\r\nThis worked for me;\r\n\r\nPython 3.7.5 (default, Oct 31 2019, 15:18:51) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> exit()\r\n\r\n(MachineLEarning) C:\\Users\\rosha\\Downloads\\Documents\\Python\\DMProject>pip install tensorflow==2.0.0a0\r\nCollecting tensorflow==2.0.0a0\r\n  Downloading https://files.pythonhosted.org/packages/c7/90/cf5e7fbf4a3c14b314e1ed6571eae1f9ad3b5e32a4e24b2b817466435a21/tensorflow-2.0.0a0-cp37-cp37m-win_amd64.whl (49.4MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49.5MB 139kB/s\r\nRequirement already satisfied: termcolor>=1.1.0 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (1.1.0)\r\nRequirement already satisfied: numpy<2.0,>=1.14.5 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (1.18.2)\r\nRequirement already satisfied: google-pasta>=0.1.2 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (0.2.0)\r\nRequirement already satisfied: gast>=0.2.0 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (0.2.2)\r\nCollecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301\r\n  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.0MB 156kB/s\r\nRequirement already satisfied: wheel>=0.26 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (0.34.2)\r\nRequirement already satisfied: protobuf>=3.6.1 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (3.11.3)\r\nRequirement already satisfied: absl-py>=0.7.0 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (0.9.0)\r\nRequirement already satisfied: six>=1.10.0 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (1.14.0)\r\nRequirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (1.1.0)\r\nRequirement already satisfied: astor>=0.6.0 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (0.8.1)\r\nRequirement already satisfied: keras-applications>=1.0.6 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (1.0.8)\r\nRequirement already satisfied: grpcio>=1.8.6 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (1.28.1)\r\nCollecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115\r\n  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 419kB 182kB/s\r\nRequirement already satisfied: werkzeug>=0.11.15 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0a0) (1.0.1)\r\nRequirement already satisfied: markdown>=2.6.8 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0a0) (3.2.1)\r\nRequirement already satisfied: setuptools in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from protobuf>=3.6.1->tensorflow==2.0.0a0) (46.1.3)\r\nRequirement already satisfied: h5py in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow==2.0.0a0) (2.10.0)\r\nInstalling collected packages: tb-nightly, tf-estimator-nightly, tensorflow\r\n  Found existing installation: tensorflow 2.1.0\r\n    Uninstalling tensorflow-2.1.0:\r\n      Successfully uninstalled tensorflow-2.1.0\r\nSuccessfully installed tb-nightly-1.14.0a20190301 tensorflow-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\r\n\r\n(MachineLEarning) C:\\Users\\rosha\\Downloads\\Documents\\Python\\DMProject>python\r\nPython 3.7.5 (default, Oct 31 2019, 15:18:51) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n>>> import tensorflow as tf\r\n>>> import tensorflow as tf\r\n>>> import tensorflow\r\n>>> tensorflow --version\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nNameError: name 'version' is not defined\r\n>>> tf.VERSION\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'VERSION'\r\n>>> x1 = tf.constant([1,2,3,4])\r\n2020-04-17 07:27:33.083319: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n>>> x1 = tf.constant([1])\r\n>>> x2 = tf.constant([5])\r\n>>> result = tf.multiply(x1,x2)\r\n>>> print(result)\r\ntf.Tensor([5], shape=(1,), dtype=int32)\r\n>>> a = tf.constant([3,4])\r\n>>> b = tf.constant([45,8])\r\n>>> print(tf.mu;tiply(a,b))\r\n  File \"<stdin>\", line 1\r\n    print(tf.mu;tiply(a,b))\r\n               ^\r\nSyntaxError: invalid syntax\r\n>>> print(tf.multiply(a,b))\r\ntf.Tensor([135  32], shape=(2,), dtype=int32)\r\n>>> x1 = tf.constant([1,2,3,4])\r\n>>> x2 = tf.constant([5,6,7,8])\r\n>>> result = tf.multiply(x1, x2)\r\n>>> print(result)\r\ntf.Tensor([ 5 12 21 32], shape=(4,), dtype=int32)"]}, {"number": 21976, "title": "Build can't find standard libraries", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: master\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: bazel release 0.16.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.5.0-1ubuntu2) 5.4.1 20171010\r\n- **CUDA/cuDNN version**:9.0/7.0\r\n- **GPU model and memory**: Nvidia GTX 970\r\n- **Exact command to reproduce**:  bazel build --verbose_failures -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n### Describe the problem\r\nI tried building TF from source, however my build always fails. During the build, some standard library is not found. So far I have seen cassert, fstream, cstdint and stddef.h. When I compile a simple test program which just loads these and returns 0 with gcc everything goes well. I have tried adding paths to CROSSTOOL, but that didn't help (though I am not sure if the place in the file was correct).\r\n\r\n\r\n### Bazel output\r\n```\r\nERROR: /home/kocur/.cache/bazel/_bazel_kocur/fe42bf71aeb545557cf2194b2de024c0/external/protobuf_archive/BUILD:260:1: C++ compilation of rule '@protobuf_archive//:js_embed' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n  (cd /home/kocur/.cache/bazel/_bazel_kocur/fe42bf71aeb545557cf2194b2de024c0/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=:/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/usr/local/cuda-9.0/targets/x86_64-linux/lib/:/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/usr/local/cuda-9.0/targets/x86_64-linux/lib/ \\\r\n    PATH=/home/kocur/.local/share/virtualenvs/code-B3GmqseA/bin:/home/kocur/bin:/home/kocur/.local/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda/bin:/usr/local/Matlab/R2012a/bin:/usr/lib/jvm/java-10-oracle/bin:/usr/lib/jvm/java-10-oracle/db/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/external/protobuf_archive/_objs/js_embed/embed.d '-frandom-seed=bazel-out/host/bin/external/protobuf_archive/_objs/js_embed/embed.o' -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 -c external/protobuf_archive/src/google/protobuf/compiler/js/embed.cc -o bazel-out/host/bin/external/protobuf_archive/_objs/js_embed/embed.o)\r\nexternal/protobuf_archive/src/google/protobuf/compiler/js/embed.cc:31:19: fatal error: cassert: No such file or directory\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 4.996s, Critical Path: 0.19s\r\nINFO: 10 processes: 10 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n### Output of echo | gcc -E -xc++ - -v\r\n\r\n```\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nTarget: x86_64-linux-gnu\r\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 5.5.0-1ubuntu2' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-5-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-5-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-5-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\r\nThread model: posix\r\ngcc version 5.4.1 20171010 (Ubuntu 5.5.0-1ubuntu2)\r\nCOLLECT_GCC_OPTIONS='-E' '-v' '-mtune=generic' '-march=x86-64'\r\n /usr/lib/gcc/x86_64-linux-gnu/5/cc1plus -E -quiet -v -imultiarch x86_64-linux-gnu -D_GNU_SOURCE - -mtune=generic -march=x86-64 -fstack-protector-strong -Wformat -Wformat-security\r\nignoring duplicate directory \"/usr/include/x86_64-linux-gnu/c++/5\"\r\nignoring nonexistent directory \"/usr/local/include/x86_64-linux-gnu\"\r\nignoring nonexistent directory \"/usr/lib/gcc/x86_64-linux-gnu/5/../../../../x86_64-linux-gnu/include\"\r\n#include \"...\" search starts here:\r\n#include <...> search starts here:\r\n /usr/include/c++/5\r\n /usr/include/x86_64-linux-gnu/c++/5\r\n /usr/include/c++/5/backward\r\n /usr/lib/gcc/x86_64-linux-gnu/5/include\r\n /usr/local/include\r\n /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed\r\n /usr/include/x86_64-linux-gnu\r\n /usr/include\r\nEnd of search list.\r\n# 1 \"<stdin>\"\r\n# 1 \"<built-in>\"\r\n# 1 \"<command-line>\"\r\n# 1 \"/usr/include/stdc-predef.h\" 1 3 4\r\n# 1 \"<command-line>\" 2\r\n# 1 \"<stdin>\"\r\nCOMPILER_PATH=/usr/lib/gcc/x86_64-linux-gnu/5/:/usr/lib/gcc/x86_64-linux-gnu/5/:/usr/lib/gcc/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/5/:/usr/lib/gcc/x86_64-linux-gnu/\r\nLIBRARY_PATH=/usr/lib/gcc/x86_64-linux-gnu/5/:/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/:/lib/x86_64-linux-gnu/:/lib/../lib/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/x86_64-linux-gnu/5/../../../:/lib/:/usr/lib/\r\nCOLLECT_GCC_OPTIONS='-E' '-v' '-mtune=generic' '-march=x86-64'\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "/CC @gunan, any ideas?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I cannot reproduce this problem on a docker container.\r\nMaybe you need to run:\r\n```\r\nsudo apt-get install build-essential\r\n```", "I ended up using downloaded clang for compilation (ability to download during build is a great feature). I am closing this, but I wasn't able to find out what is wrong with my gcc and/or tf build."]}, {"number": 21975, "title": "Recalculate a convolution value with the eager execution", "body": "### System information\r\n- **Windows 10**:\r\n- **TensorFlow installed from conda**:\r\n- **TensorFlow version 1.9**:\r\n- **Python version 3.6**:\r\n\r\n### Describe the problem\r\nI'm trying to use the eager execution.\r\n\r\nI create a training set, a weight and a convolution layer.\r\n\r\nI declare the convolution and change the weights.\r\n\r\nHow can I get the convolution calculated again without having to declare the layer again?\r\n\r\nI expected it to be something like that:\r\n\r\n\timport tensorflow as tf\r\n\r\n\ttf.enable_eager_execution()    \r\n\ttfe = tf.contrib.eager\r\n\r\n\r\n\tTrainingDataExample = tf.constant(0.5, shape=[8, 5, 6, 1], name=\"Inputs\") \r\n\tWeightExample = tfe.Variable(tf.truncated_normal([1, 3, 1, 4], seed=1), name=\"Weights\")\r\n\tConvExample = tf.nn.conv2d(TrainingDataExample, WeightExample, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"Conv\")\r\n\r\n\tNewWeightExample = tf.constant(2.0, shape=[1, 3, 1, 4], name=\"NewWeights\")\r\n\tWeightExample = tf.assign(WeightExample, NewWeightExample )\r\n\tresult = ConvExample \r\n\r\n\tprint (result)\r\nBut it does not work, the value of the convolution is not updated.\r\n\r\nI know I can do it like this:\r\n\t\r\n\tNewWeightExample = tf.constant(2.0, shape=[1, 3, 1, 4], name=\"NewWeights\")\r\n\tWeightExample = tf.assign(WeightExample, NewWeightExample )\r\n\tConvExample = tf.nn.conv2d(TrainingDataExample, WeightExample,\r\n\t                           strides=[1, 1, 1, 1], padding=\"VALID\",\r\n\t                           name=\"Conv\")\r\n\r\nBut I understand that the main class contains a __call__ method just like the model class.\r\n\r\nMaybe this is wrong, but I would like to know if I can do this without having to recompute the convolution again.\r\n", "comments": ["I don't think that is possible without re-running the model. Since operation evaluate immediately, in this example `ConvExample` already has a reference to the result of the convolution; changing the value of the filter does not trigger a new computation.\r\n\r\nFor example in [Eager execution - Build a model](https://www.tensorflow.org/guide/eager#build_a_model) (look at `MNISTModel`) you can see how the function `call` actually re-runs all the necessary computations.\r\n\r\nTL;DR `tf.nn.conv2d(...)` returns the result immediately. It is not triggered by changing some of its parameters. \r\n "]}, {"number": 21974, "title": "Seeding tf.random_uniform with tf.Tensors", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.6\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.6.5\r\n- **Exact command to reproduce**: (feature request, see code below)\r\n\r\n### Describe the problem\r\n\r\nFor my application I need to sample a random tensor on one device and later transfer it to a different device. To save on networking I would like to instead only transfer a seed (that gets expanded on both devices before further processing). I need `tf.random_uniform` in particular.\r\n\r\nProblem is that currently it seems the seed for `tf.random_uniform` must be a Python `int`, yet for this application the seed cannot be part of the script/graph but must be sampled by the first device.\r\n\r\n```\r\nTypeError: Expected int for argument 'seed2' not <tf.Tensor 'random_uniform_1/mod:0' shape=() dtype=int64>.\r\n```\r\n\r\nAt the same time it's not clear whether this represents an inherent limitation of TensorFlow since `Dataset.shuffle` does seem to support `tf.Tensors` as seeds (inspired by #17284).\r\n\r\n### Source code / logs\r\n\r\nThe following code is what I'd ideally like to do but produces the error message above:\r\n```python\r\nimport tensorflow as tf\r\n\r\nseed = tf.random_uniform(shape=[], dtype=tf.int64, maxval=1000000)\r\nx = tf.random_uniform(shape=(1,), dtype=tf.int32, maxval=100, seed=seed)\r\n\r\nsess = tf.Session()\r\nprint(sess.run(x))\r\n```\r\n\r\nThis code works but seems sub-ideal:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nseed = tf.random_uniform(shape=[], dtype=tf.int64, maxval=1000000)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(np.arange(100))\r\ndataset = dataset.shuffle(buffer_size=100, seed=seed)\r\niterator = dataset.make_initializable_iterator()\r\nx = iterator.get_next()\r\n\r\nsess = tf.Session()\r\nsess.run(iterator.initializer)\r\nprint(sess.run(x))\r\n```", "comments": ["Could you replace the first tf.random_uniform by np.random.* ?", "@facaiy thanks for replying!\r\n\r\nI believe I'd have to wrap it in a `tf.py_func` to get it executed on the right device, but `py_func` [doesn't support that](https://www.tensorflow.org/api_docs/python/tf/py_func) afaik.", "There are a couple of other possible approaches, though without understanding your application it's hard to say what would work better:\r\n\r\n1. Use one of the stateless RNG ops in [`tf.contrib.stateless`](https://www.tensorflow.org/api_docs/python/tf/contrib/stateless).\r\n2. Use [`tf.data.experimental.RandomDataset`](https://www.tensorflow.org/api_docs/python/tf/contrib/data/RandomDataset). It generates random `int64` values, and you could use it together with `tf.contrib.stateless` to generate numbers in a distribution of your choice. See the [implementation](https://github.com/tensorflow/tensorflow/blob/030f7828c9cfcef0c875d60198db57d0cd94f214/tensorflow/python/data/experimental/ops/interleave_ops.py#L187-L195) of `tf.data.experimental.sample_from_datasets()` for an example of how to use it.", "thank you for the suggestions @mrry, I will have a closer look!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "In the end we solved our issue via a custom op."]}, {"number": 21973, "title": "C++ compilation of rule '//tensorflow/python:py_func_lib' failed", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source (r1.10)\r\n- **TensorFlow version (use command below)**: r1.10\r\n- **Python version**: 3.7.0\r\n- **Bazel version (if compiling from source)**: 0.16.0\r\n- **GCC/Compiler version (if compiling from source)**: 8.2.0\r\n- **CUDA/cuDNN version**: 9.2/7.2\r\n- **GPU model and memory**: GTX 1060 6GB\r\n- **Exact command to reproduce**: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Describe the problem\r\nUnable to build TensorFlow from source.\r\n\r\n### Source code / logs\r\n\r\nOutput of `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures`\r\n\r\n```\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nLoading: \r\nLoading: 0 packages loaded\r\nDEBUG: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: \r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nWARNING: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:357:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/kfac/python/ops/BUILD:80:1: in py_library rule //tensorflow/contrib/kfac/python/ops:loss_functions: target '//tensorflow/contrib/kfac/python/ops:loss_functions' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded).\r\nINFO: Found 1 target...\r\n[13 / 20] [-----] BazelWorkspaceStatusAction stable-status.txt\r\n[98 / 112] Compiling tensorflow/core/kernels/slice_op_gpu.cu.cc; 1s local ... (4 actions running)\r\nERROR: /home/mukundan/machine_learning/tensorflow/tensorflow/python/BUILD:330:1: C++ compilation of rule '//tensorflow/python:py_func_lib' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/mukundan/.pyenv/plugins/pyenv-virtualenv/shims:/home/mukundan/.pyenv/shims:/home/mukundan/.pyenv/bin:/home/mukundan/bin:/home/mukundan/.pyenv/plugins/pyenv-virtualenv/shims:/home/mukundan/.pyenv/shims:/home/mukundan/.pyenv/bin:/home/mukundan/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/opt/cuda/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/python/_objs/py_func_lib/py_func.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/python/_objs/py_func_lib/py_func.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_JEMALLOC -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -DSQLITE_OMIT_DEPRECATED '-DGRPC_ARES=0' '-DPB_FIELD_16BIT=1' -iquote . -iquote bazel-out/host/genfiles -iquote bazel-out/host/bin -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/jemalloc -iquote bazel-out/host/genfiles/external/jemalloc -iquote bazel-out/host/bin/external/jemalloc -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_python -iquote bazel-out/host/genfiles/external/local_config_python -iquote bazel-out/host/bin/external/local_config_python -iquote external/double_conversion -iquote bazel-out/host/genfiles/external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/curl -iquote bazel-out/host/genfiles/external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/genfiles/external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/genfiles/external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/genfiles/external/aws -iquote bazel-out/host/bin/external/aws -iquote external/cub_archive -iquote bazel-out/host/genfiles/external/cub_archive -iquote bazel-out/host/bin/external/cub_archive -iquote external/org_sqlite -iquote bazel-out/host/genfiles/external/org_sqlite -iquote bazel-out/host/bin/external/org_sqlite -iquote external/snappy -iquote bazel-out/host/genfiles/external/snappy -iquote bazel-out/host/bin/external/snappy -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote bazel-out/host/bin/external/png_archive -iquote external/lmdb -iquote bazel-out/host/genfiles/external/lmdb -iquote bazel-out/host/bin/external/lmdb -iquote external/gemmlowp -iquote bazel-out/host/genfiles/external/gemmlowp -iquote bazel-out/host/bin/external/gemmlowp -iquote external/grpc -iquote bazel-out/host/genfiles/external/grpc -iquote bazel-out/host/bin/external/grpc -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/host/genfiles/external/jemalloc/include -isystem bazel-out/host/bin/external/jemalloc/include -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/local_config_python/numpy_include -isystem bazel-out/host/genfiles/external/local_config_python/numpy_include -isystem bazel-out/host/bin/external/local_config_python/numpy_include -isystem external/local_config_python/python_include -isystem bazel-out/host/genfiles/external/local_config_python/python_include -isystem bazel-out/host/bin/external/local_config_python/python_include -isystem external/double_conversion -isystem bazel-out/host/genfiles/external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/genfiles/external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/genfiles/external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/genfiles/external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/png_archive -isystem bazel-out/host/genfiles/external/png_archive -isystem bazel-out/host/bin/external/png_archive -isystem external/local_config_cuda/cuda/cuda/extras/CUPTI/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/extras/CUPTI/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/extras/CUPTI/include -isystem external/grpc/include -isystem bazel-out/host/genfiles/external/grpc/include -isystem bazel-out/host/bin/external/grpc/include -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/host/genfiles/external/grpc/third_party/address_sorting/include -isystem bazel-out/host/bin/external/grpc/third_party/address_sorting/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -c tensorflow/python/lib/core/py_func.cc -o bazel-out/host/bin/tensorflow/python/_objs/py_func_lib/py_func.pic.o)\r\nIn file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/common_runtime/device.h:43,\r\n                 from ./tensorflow/core/common_runtime/graph_runner.h:23,\r\n                 from ./tensorflow/core/common_runtime/shape_refiner.h:20,\r\n                 from ./tensorflow/c/c_api_internal.h:30,\r\n                 from ./tensorflow/c/eager/c_api_internal.h:30,\r\n                 from tensorflow/python/lib/core/py_func.cc:24:\r\n./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':\r\n./tensorflow/core/util/tensor_format.h:500:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {\r\n                                         ~~~~^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/python/lib/core/py_func.cc: In function 'tensorflow::Status tensorflow::ConvertNdarrayToTensor(PyObject*, tensorflow::Tensor*)':\r\ntensorflow/python/lib/core/py_func.cc:355:39: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]\r\n           el = PyUnicode_AsUTF8AndSize(input_data[i], &el_size);\r\n                ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/refcount.h:20,\r\n                 from ./tensorflow/core/platform/tensor_coding.h:21,\r\n                 from ./tensorflow/core/framework/resource_handle.h:19,\r\n                 from ./tensorflow/core/framework/allocator.h:24,\r\n                 from ./tensorflow/core/framework/tensor.h:20,\r\n                 from ./tensorflow/python/lib/core/py_func.h:22,\r\n                 from tensorflow/python/lib/core/py_func.cc:16:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int]':\r\n./tensorflow/core/util/tensor_format.h:452:47:   required from here\r\n./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n   ^\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int]':\r\n./tensorflow/core/util/tensor_format.h:461:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = long unsigned int; T2 = long long int; std::__cxx11::string = std::__cxx11::basic_string<char>]':\r\ntensorflow/python/lib/core/py_func.cc:323:5:   required from here\r\n./tensorflow/core/platform/default/logging.h:230:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n                         ==)  // Compilation error with CHECK_EQ(NULL, x)?\r\n./tensorflow/core/platform/macros.h:88:49: note: in definition of macro 'TF_PREDICT_TRUE'\r\n #define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))\r\n                                                 ^\r\n./tensorflow/core/platform/default/logging.h:229:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\r\n TF_DEFINE_CHECK_OP_IMPL(Check_EQ,\r\n ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 6.180s, Critical Path: 5.87s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["Nagging Assignee @bignamehyp: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21972, "title": "Is it possible to use the autograph-compiled code on older Tensorflow versions?", "body": "### Describe the problem\r\nI have tried autograph on the nightly channel. It works and is very useful to me. I could not hope to write such code myself. However, it is not a very good idea to stay on the nightly channel for the whole time. Since autograph can generate the code, it seems natural to me to use such code and migrate back to a more stable channel. \r\n\r\nHowever, the generated code doesn't seem to be readily usable. It contains unrecognized tokens like `__ag`. I cannot get past that. Is it even possible? Can you disclose some of the plans on that?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@aquariusjay @alexbw can you comment on whether working at nightly is the intended use-method for AG, or reassign to someone who can, please?", "Hey @phizaz great to hear that AutoGraph is useful to you. If you are able, we'd love if you could share the code that you used it in!\r\nWe're still actively working on AutoGraph, so to get the most out of it (before TF 2.0 is released), staying on nightly gets you the most bug-fixes and functionality. \r\nHowever, if that's not possible, try using the 1.10 release or later.", "@alexbw r1.10 doesn't seem to support list appending and `tf.stack`. However, I found it work on r1.11 (compiled from source).\r\n\r\nI think I could share the code here, I simplify the code a bit:\r\n\r\n```\r\n@autograph.convert()\r\ndef chunkify(tensor, max_chunk_size):\r\n    Agrs:\r\n        tensor: the tensor to be chunked\r\n        max_chunk_size: maximum size of a chunk\r\n    \"\"\"\r\n    s = tf.shape(tensor)\r\n    i = 0\r\n    out = []\r\n    autograph.set_element_type(out, tensor.dtype)\r\n\r\n    while i < s[0]:\r\n        # cut a new tensor\r\n        t = tensor[i:i+max_chunk_size]\r\n\r\n        ...\r\n\r\n        out.append(t)\r\n        i += max_chunk_size\r\n\r\n    out = tf.stack(out)\r\n\r\n    return out\r\n```\r\n", "@mdanatg ", "Now that the stable release reached 1.13, I believe you will find the basic autograph functionality in the stable branch."]}, {"number": 21971, "title": "InvalidArgumentError: Input to reshape is a tensor with 1 values, but the requested shape has 0", "body": "### System information\r\n- Custom code:\r\n- OS Platform and Distribution: Windows 7 x64\r\n- TensorFlow installed from: Anaconda\r\n- Bazel version: N/A\r\n- Tensorflow version 1.8.0:\r\n- python 3.6.0 :\r\n- Mobile device: N/A\r\n- Exact command to reproduce: N/A\r\n- GPU model and memory: NVIDIA QUADRO K1100M\r\n- CUDA/cuDNN: 9.0.176/7.0\r\n\r\n### The Problem\r\n\r\nI strongly believe there's a problem with the gradient computation. \r\n\r\n\r\n``````python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nl2_u = 0.\r\nl2_i = 0.\r\ninit_std = 0.1\r\nnp_users = np.int64(100)\r\nnp_items = np.int64(100)\r\nn_factors = np.int64(10)\r\nlr = 0.1\r\n\r\ntrain_x = np.stack([np.random.randint(np_users, size=1000), np.random.randint(np_items, size=1000)], axis=1)\r\ntrain_y = np.random.randint(2, size=1000)\r\n\r\ndef define_graph():\r\n    tf.reset_default_graph()\r\n    n_users = tf.placeholder(dtype=tf.int32, shape=[], name='n_users')\r\n    n_items = tf.placeholder(dtype=tf.int32, shape=[], name='n_items')\r\n    x = tf.placeholder(shape=[None, 2], dtype=tf.int32, name=\"x\")\r\n    y = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\r\n    w = tf.placeholder(tf.float32, shape=[None], name='sample_weights')\r\n    reg_u = tf.constant(l2_u, dtype=tf.float32, name='lambda_u')\r\n    reg_i = tf.constant(l2_i, dtype=tf.float32, name='lambda_i')\r\n\r\n    u_shape = tf.stack([n_users, n_factors])\r\n    u_rnd = tf.random_normal(shape=u_shape, dtype=tf.float32, mean=0, stddev=init_std)\r\n    U = tf.verify_tensor_all_finite(\r\n                tf.Variable(u_rnd,\r\n                            trainable=True,\r\n                            name=\"users\",\r\n                            validate_shape=False), msg='NaN or Inf in parameters')\r\n\r\n    i_shape = tf.stack([n_items, n_factors])\r\n    i_rnd = tf.random_normal(shape=i_shape,\r\n                                dtype=tf.float32,\r\n                                mean=0,\r\n                                stddev=init_std)\r\n    I = tf.verify_tensor_all_finite(\r\n                tf.Variable(i_rnd,\r\n                            trainable=True,\r\n                            name=\"items\",\r\n                            validate_shape=False), msg='NaN or Inf in parameters')\r\n\r\n    ub_shape = tf.expand_dims(n_users, axis=0)\r\n    ub_rnd = tf.random_normal(dtype=tf.float32,\r\n                                 shape=ub_shape,\r\n                                 mean=0,\r\n                                 stddev=init_std)\r\n    u_bias = tf.verify_tensor_all_finite(\r\n                    tf.Variable(ub_rnd,\r\n                                trainable=True,\r\n                                name='u_bias',\r\n                                validate_shape=False), msg='NaN or Inf in parameters')\r\n\r\n    ib_shape = tf.expand_dims(n_items, axis=0)\r\n    ib_rnd = tf.random_normal(dtype=tf.float32,\r\n                                 shape=ib_shape,\r\n                                 mean=0,\r\n                                 stddev=init_std)\r\n    i_bias = tf.verify_tensor_all_finite(\r\n                    tf.Variable(ib_rnd,\r\n                                trainable=True,\r\n                                name='i_bias',\r\n                                validate_shape=False), msg='NaN or Inf in parameters')\r\n    g_bias = tf.verify_tensor_all_finite(\r\n                    tf.Variable(0., \r\n                                dtype=tf.float32,\r\n                                trainable=True, \r\n                                name='g_bias'), msg='NaN or Inf in parameters')\r\n    users = tf.nn.embedding_lookup(U, x[:, 0])\r\n    items = tf.nn.embedding_lookup(I, x[:, 1])\r\n    \r\n    ub = tf.expand_dims(tf.nn.embedding_lookup(u_bias, x[:, 0]), 1)\r\n    ib = tf.expand_dims(tf.nn.embedding_lookup(i_bias, x[:, 1]), 1)\r\n    y_hat = tf.reduce_sum(users * items, axis=1, keepdims=True) + ub + ib + g_bias\r\n    \r\n    loss = tf.square(tf.expand_dims(y, 1) - tf.sigmoid(y_hat))\r\n    reduced_loss = tf.reduce_mean(loss, keepdims=True)\r\n    reg_u = l2_u * tf.nn.l2_loss(U)\r\n    reg_i = l2_i * tf.nn.l2_loss(I)\r\n    reg = reg_i + reg_u\r\n    target = reg + reduced_loss\r\n    checked_target = tf.verify_tensor_all_finite(target, \r\n                                                 msg='NaN or Inf in target value',\r\n                                                 name='target')\r\n    trainer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(checked_target)\r\n    return trainer, n_users, n_items, x, y\r\n\r\nwith tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):\r\n    op, n_users, n_items, x, y = define_graph()\r\n\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init, feed_dict={n_users: np_users, n_items: np_items})\r\n\r\nsess.run(op, feed_dict={x: train_x, y: train_y})\r\n``````\r\nError logs:\r\n\r\n``````bash\r\nInvalidArgumentError: Input to reshape is a tensor with 1 values, but the requested shape has 0\r\n\t [[Node: gradients/Mean_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/add_4_grad/Reshape_1, gradients/Mean_grad/DynamicStitch/_47, ^gradients/add_4_grad/tuple/group_deps)]]\r\n\r\nCaused by op 'gradients/Mean_grad/Reshape', defined at:\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\r\n    app.launch_new_instance()\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\r\n    self.io_loop.start()\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 422, in run_forever\r\n    self._run_once()\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\r\n    handle._run()\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 145, in _run\r\n    self._callback(*self._args)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\r\n    handler_func(fileobj, events)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\r\n    self._handle_recv()\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-210-ae7fb3e220df>\", line 2, in <module>\r\n    op, n_users, n_items, x, y = define_graph()\r\n  File \"<ipython-input-209-8e2c9c7479df>\", line 72, in define_graph\r\n    trainer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(checked_target)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 414, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 526, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 494, in gradients\r\n    gate_gradients, aggregation_method, stop_gradients)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 636, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 385, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 636, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 119, in _MeanGrad\r\n    sum_grad = _SumGrad(op, grad)[0]\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 83, in _SumGrad\r\n    grad = array_ops.reshape(grad, output_shape_kept_dims)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6112, in reshape\r\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'Mean', defined at:\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n[elided 23 identical lines from previous traceback]\r\n  File \"<ipython-input-210-ae7fb3e220df>\", line 2, in <module>\r\n    op, n_users, n_items, x, y = define_graph()\r\n  File \"<ipython-input-209-8e2c9c7479df>\", line 64, in define_graph\r\n    reduced_loss = tf.reduce_mean(loss, keepdims=True)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1562, in reduce_mean\r\n    name=name))\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 4495, in mean\r\n    name=name)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\Daniel\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 1 values, but the requested shape has 0\r\n\t [[Node: gradients/Mean_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/add_4_grad/Reshape_1, gradients/Mean_grad/DynamicStitch/_47, ^gradients/add_4_grad/tuple/group_deps)]]\r\n``````", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@drpngx You can close the issue, as it works with TF 1.10", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21970, "title": "tf.errors.OpError and its subclasses are pickleable", "body": "The builtin `BaseException` class defines a generic `__reduce__` method which assumes that the `args` attribute corresponds to constructor arguments. See `BaseException_reduce` in [Objects/exceptions.c](https://github.com/python/cpython/blob/master/Objects/exceptions.c#L129). `tf.errors.OpError` keeps args empty and therefore cannot be pickled via `BaseException.__reduce__`. This commit fixes  the issue by defining `OpError.__reduce__`.\r\n\r\nSide question: is there a reason `tf.errors.OpError` does not pass its arguments to the `Exception` constructor to make them available via `args` and in the repr?", "comments": []}, {"number": 21969, "title": "Python wheel doesn't work on some Intel Xeon CPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux simi 4.4.0-131-generic #157-Ubuntu SMP Thu Jul 12 15:51:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nn/a\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary (pip install)\r\n- **TensorFlow version (use command below)**:\r\nI cannot run the suggested command (more bellow) but the version installed by pip is tensorflow-1.10.1.\r\n- **Python version**:\r\n3.5.2\r\n- **Bazel version (if compiling from source)**:\r\nn/a\r\n- **GCC/Compiler version (if compiling from source)**:\r\nn/a\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n- **GPU model and memory**:\r\nn/a\r\n- **Exact command to reproduce**:\r\n```\r\npython -c 'import tensorflow'\r\n```\r\n\r\n### Describe the problem\r\n\r\nI cannot even import the tensorflow module:\r\n```\r\n$ ./tfvenv/bin/python -c 'import tensorflow'\r\nIllegal instruction   \r\n```\r\nDigging through the system log, I found this line that seems to give more details on the problem:\r\n```\r\naug 30 11:07:27 simi kernel: traps: python[6186] trap invalid opcode ip:7f989e396250 sp:7fff8959df80 error:0 in libtensorflow_framework.so[7f989df07000+c88000]\r\n```\r\n\r\nAfter some googling, my understanding is that the tf build uses instruction that are not available on my CPU.  Since it is a regular x86_64 architecture, I assume it is a bug but maybe not? Are there particular CPU requirements for tensorflow that I didn't manage to find?\r\n\r\n### Source code / logs\r\nI'm pasting the output of `lshw -class cpu` from the machine where the issue occurs:\r\n```\r\n  *-cpu:0\r\n       description: CPU\r\n       product: Intel(R) Xeon(R) CPU           E5430  @ 2.66GHz\r\n       vendor: Intel Corp.\r\n       physical id: 400\r\n       bus info: cpu@0\r\n       version: Intel(R) Xeon(R) CPU           E5430  @ 2.66GHz\r\n       slot: CPU1\r\n       size: 2667MHz\r\n       capacity: 3600MHz\r\n       width: 64 bits\r\n       clock: 1333MHz\r\n       capabilities: x86-64 fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx constant_tsc arch_perfmon pebs bts rep_good nopl aperfmperf eagerfpu pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 xsave lahf_lm kaiser tpr_shadow vnmi flexpriority dtherm\r\n       configuration: cores=4 enabledcores=4 threads=4\r\n  *-cpu:1\r\n       description: CPU\r\n       product: Intel(R) Xeon(R) CPU           E5430  @ 2.66GHz\r\n       vendor: Intel Corp.\r\n       physical id: 401\r\n       bus info: cpu@1\r\n       version: Intel(R) Xeon(R) CPU           E5430  @ 2.66GHz\r\n       slot: CPU2\r\n       size: 2667MHz\r\n       capacity: 3600MHz\r\n       width: 64 bits\r\n       clock: 1333MHz\r\n       capabilities: x86-64 fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx constant_tsc arch_perfmon pebs bts rep_good nopl aperfmperf eagerfpu pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 xsave lahf_lm kaiser tpr_shadow vnmi flexpriority dtherm\r\n       configuration: cores=4 enabledcores=4 threads=4\r\n```", "comments": ["Oh, I just found the release notes for 1.6 that say \"Prebuilt binaries will use AVX instructions. This may break TF on older CPUs.\" I assume that is my problem here. Sorry for the noise.", "I have the same problem under Debian 10 Buster, Python v2.7, tensorflow v1.14.0:\r\n```\r\nlsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID: Debian\r\nDescription:    Debian GNU/Linux 10 (buster)\r\nRelease:        10\r\nCodename:       buster\r\n\r\n```\r\n\r\n```\r\npython -c 'import tensorflow'\r\nIllegal instruction\r\n\r\nError log:\r\nNov  4 15:59:00 moodle37 kernel: [9768885.859206] traps: python[2419] trap invalid opcode ip:7f2fc9fbba59 sp:7ffd402810b0 error:0 in libtensorflow_framework.so.1[7f2fc9896000+18f8000]\r\n\r\n```\r\n\r\n\r\n```\r\nsudo lshw -class cpu\r\n  *-cpu\r\n       description: CPU\r\n       product: Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz\r\n       vendor: Intel Corp.\r\n       physical id: 4\r\n       bus info: cpu@0\r\n       version: Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz\r\n       slot: CPU #000\r\n       size: 2100MHz\r\n       capacity: 4230MHz\r\n       width: 64 bits\r\n       capabilities: lm fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp x86-64 constant_tsc arch_perfmon pebs bts nopl xtopology tsc_reliable nonstop_tsc cpuid pni ssse3 cx16 sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer hypervisor lahf_lm pti ssbd ibrs ibpb stibp tsc_adjust arat flush_l1d arch_capabilities\r\n       configuration: cores=1 enabledcores=1\r\n\r\n```\r\n\r\n```\r\npython -V\r\nPython 2.7.16\r\n\r\n```\r\n\r\n\r\n```\r\npip list\r\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Ple                                                                                                                                                             ase upgrade your Python as Python 2.7 won't be maintained after that date. A fut                                                                                                                                                             ure version of pip will drop support for Python 2.7. More details about Python 2                                                                                                                                                              support in pip, can be found at https://pip.pypa.io/en/latest/development/relea                                                                                                                                                             se-process/#python-2-support\r\nPackage              Version\r\n-------------------- ---------\r\nabsl-py              0.8.1\r\nasn1crypto           0.24.0\r\nastor                0.8.0\r\nbackports.weakref    1.0.post1\r\nbzr-etckeeper        0.0.0\r\nconfigparser         3.5.0b2\r\ncryptography         2.6.1\r\ncycler               0.10.0\r\nentrypoints          0.3\r\nenum34               1.1.6\r\nfuncsigs             1.0.2\r\nfuture               0.17.1\r\nfutures              3.3.0\r\ngast                 0.3.2\r\ngoogle-pasta         0.1.7\r\ngrpcio               1.24.3\r\nh5py                 2.10.0\r\nipaddress            1.0.17\r\nKeras-Applications   1.0.6\r\nKeras-Preprocessing  1.1.0\r\nkeyring              17.1.1\r\nkeyrings.alt         3.1.1\r\nMarkdown             3.1.1\r\nmatplotlib           1.5.3\r\nmock                 3.0.5\r\nmoodlemlbackend      1.0.1\r\nnumpy                1.16.5\r\npip                  19.3.1\r\nprotobuf             3.9.1\r\npycrypto             2.6.1\r\nPyGObject            3.30.4\r\npyparsing            2.4.2\r\npython-dateutil      2.8.0\r\npytz                 2019.2\r\npyxdg                0.25\r\nscikit-learn         0.20.4\r\nscipy                0.17.1\r\nSecretStorage        2.3.1\r\nsetuptools           41.2.0\r\nsix                  1.12.0\r\ntensorboard          1.14.0\r\ntensorflow           1.14.0\r\ntensorflow-estimator 1.14.0\r\ntermcolor            1.1.0\r\nWerkzeug             0.16.0\r\nwheel                0.33.6\r\nwrapt                1.11.2\r\n\r\n```"]}, {"number": 21968, "title": "AssertionError: tensorflow mulitiple GPU train with MirroredStrategy", "body": "I want to train with multiple gpu with tensorflow-1.10. I use the estimator and MirroredStrategy. The model is resnet50 which is official model of tensorflow . The following is my code and error log. Am I use MirroredStrategy in the wrong way?\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.contrib.slim.nets import resnet_v2\r\nimport tensorflow.contrib.slim as slim\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\r\nx_train = np.expand_dims(x_train, 3).astype(np.float32)\r\ny_train = y_train.astype(np.int32)\r\nx_test = np.expand_dims(x_test, 3).astype(np.float32)\r\ny_test = y_test.astype(np.int32)\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\ncls_num = 10\r\n\r\n\r\ndef model_fn(features, labels, mode):\r\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n    with slim.arg_scope(resnet_v2.resnet_arg_scope()):\r\n        logits, endpoints = resnet_v2.resnet_v2_50(features, \r\n                num_classes=cls_num,\r\n                is_training=is_training)\r\n\r\n    logits = tf.squeeze(logits, [1, 2])\r\n    preds = tf.argmax(logits, 1)\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n    optimizer = tf.train.AdagradOptimizer(learning_rate=0.01)\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    with tf.control_dependencies(update_ops):\r\n        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n\r\n    accuracy = tf.metrics.accuracy(labels=labels, predictions=preds)\r\n    metrics = {'accuracy': accuracy}\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\r\n\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op) \r\n\r\n\r\ndef train_input_fn():\r\n    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n    dataset = dataset.repeat(1).batch(128)\r\n    return dataset\r\n\r\ndef eval_input_fn():\r\n    dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\r\n    dataset = dataset.repeat(1).batch(128)\r\n    return dataset\r\n\r\n\r\ndistribution = tf.contrib.distribute.MirroredStrategy()\r\nconfig = tf.estimator.RunConfig(train_distribute=distribution)\r\nestimator = tf.estimator.Estimator(model_fn=model_fn, model_dir='logs', config=config)\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)\r\neval_specs = tf.estimator.EvalSpec(input_fn=eval_input_fn)\r\nfor _ in xrange(100):\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs)\r\n```\r\n\r\nThis is the error log:\r\n```\r\n2018-08-30 16:58:47.446417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1 \r\n2018-08-30 16:58:47.446428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N Y \r\n2018-08-30 16:58:47.446437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   Y N \r\n2018-08-30 16:58:47.446877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:0 with 21544 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\n2018-08-30 16:58:47.447098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:1 with 21544 MB memory) -> physical GPU (device: 1, name: Tesla P40, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\r\nINFO:tensorflow:Configured nccl all-reduce.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Error reported to Coordinator: \r\nTraceback (most recent call last):\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 519, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1133, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"test_mnist.py\", line 22, in model_fn\r\n    is_training=is_training)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_v2.py\", line 287, in resnet_v2_50\r\n    scope=scope)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_v2.py\", line 216, in resnet_v2\r\n    net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_utils.py\", line 215, in stack_blocks_dense\r\n    net = block.unit_fn(net, rate=1, **unit)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_v2.py\", line 101, in bottleneck\r\n    inputs, activation_fn=nn_ops.relu, scope='preact')\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 650, in batch_norm\r\n    outputs = layer.apply(inputs, training=is_training)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 805, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 362, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 736, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/layers/normalization.py\", line 158, in call\r\n    return super(BatchNormalization, self).call(inputs, training=training)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/layers/normalization.py\", line 514, in call\r\n    outputs = self._fused_batch_norm(inputs, training=training)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/layers/normalization.py\", line 420, in _fused_batch_norm\r\n    momentum)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/layers/normalization.py\", line 369, in _assign_moving_average\r\n    with ops.colocate_with(variable):\r\n  File \"/home/soft/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3939, in _colocate_with_for_gradient\r\n    with self.colocate_with(op, ignore_existing):\r\n  File \"/home/soft/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3992, in colocate_with\r\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1255, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1094, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 414, in _tensor_conversion_mirrored\r\n    assert not as_ref\r\nAssertionError\r\nTraceback (most recent call last):\r\n  File \"test_mnist.py\", line 58, in <module>\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 451, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 590, in run\r\n    return self.run_local()\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 691, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 376, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1143, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1255, in _train_model_distributed\r\n    self.config)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/training/distribute.py\", line 777, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 308, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 519, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1133, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"test_mnist.py\", line 22, in model_fn\r\n    is_training=is_training)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_v2.py\", line 287, in resnet_v2_50\r\n    scope=scope)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_v2.py\", line 216, in resnet_v2\r\n    net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_utils.py\", line 215, in stack_blocks_dense\r\n    net = block.unit_fn(net, rate=1, **unit)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/nets/resnet_v2.py\", line 101, in bottleneck\r\n    inputs, activation_fn=nn_ops.relu, scope='preact')\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 650, in batch_norm\r\n    outputs = layer.apply(inputs, training=is_training)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 805, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 362, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 736, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/layers/normalization.py\", line 158, in call\r\n    return super(BatchNormalization, self).call(inputs, training=training)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/layers/normalization.py\", line 514, in call\r\n    outputs = self._fused_batch_norm(inputs, training=training)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/layers/normalization.py\", line 420, in _fused_batch_norm\r\n    momentum)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/keras/layers/normalization.py\", line 369, in _assign_moving_average\r\n    with ops.colocate_with(variable):\r\n  File \"/home/soft/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3939, in _colocate_with_for_gradient\r\n    with self.colocate_with(op, ignore_existing):\r\n  File \"/home/soft/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3992, in colocate_with\r\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1255, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1094, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/soft/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 414, in _tensor_conversion_mirrored\r\n    assert not as_ref\r\nAssertionError\r\n```", "comments": ["if I change the model net from resnet to my custom net, it works fine. Is there something special on `resnet_v2_50` net?\r\n```\r\n#with slim.arg_scope(resnet_v2.resnet_arg_scope()):\r\n#    logits, endpoints = resnet_v2.resnet_v2_50(features, \r\n#            num_classes=cls_num,\r\n#            is_training=training)\r\n#logits = tf.squeeze(logits, [1, 2])\r\n#preds = tf.argmax(logits, 1)\r\n\r\nnet = tf.layers.conv2d(inputs=features, filters=8, kernel_size=3,\r\n                strides=1, activation=tf.nn.relu, padding='same')\r\nnet = tf.layers.batch_normalization(net, training=training)\r\nnet = tf.layers.max_pooling2d(inputs=net, pool_size=2,\r\n                strides=2, name=\"maxpool1\")\r\n\r\nnet = tf.layers.flatten(net)\r\nlogits = tf.layers.dense(net, cls_num)\r\npreds = tf.argmax(logits, 1)\r\n```", "Same error but mine is on the object detection API . I tried to modify the estimator with mirror strategy but occurring the assert error. ", "Thank you for reporting the error. josh11b@ is working on this issue and will update this bug as soon as possible.", "Actually, this issue is likely fixed in release 1.11, could you try with that and see if that fixes the issue? ", ">>> tf.__version__\r\n'1.11.0-rc0'\r\nbut the issue still happen", "1.git/models/research# git log\r\ncommit 17fa52864bfc7a7444a8b921d8a8eb1669e14ebd\r\ntensorflow# git log\r\ncommit 1e438195399650604fb3aa3a53c67339f1167882\r\nAuthor: Amit Patankar <amitpatankar@google.com>\r\ngit checkout -b v1.11.0-rc0 v1.11.0-rc0\r\n2.\r\n# git diff object_detection/model_main.py\r\n-  config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir)\r\n+\r\n+  distribution = tf.contrib.distribute.MirroredStrategy()\r\n+  config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir, train_distribute=distribution, eval_distribute=distribution)\r\n3\r\nmodels/research# python object_detection/model_main.py --pipeline_config_path=object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config --logtostderr --model_dir=/data/checkpoint/del2 --num_gpus=4", "@yuefengz I thought your fix to contrib layers a while back would've fixed this. Is this some other issue then? ", "Nagging Assignee @yuefengz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Duplicate with #22399. Closing it for now. "]}, {"number": 21967, "title": "ImportError: DLL load failed: The specified procedure could not be found.", "body": "Hello, I have tried many times to install tensorflow 1.5 on windows 10, but I encounter the same error each time : \r\n`python\r\nPython 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\jules\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\jules\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\jules\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\jules\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.`\r\n\r\nany help would be greatly appreciated", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I have the same exact problem", "Maybe this?\r\nhttps://github.com/protocolbuffers/protobuf/issues/5046", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Is this still an issue ? If so, please respond to the above.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I resolved this by upgrading Python 3.6.0 to Python 3.6.1, as suggested by @dongwenjian [here.](https://github.com/protocolbuffers/protobuf/issues/5046#issuecomment-439736098)", "> \r\n> \r\n> I resolved this by upgrading Python 3.6.0 to Python 3.6.1, as suggested by @dongwenjian [here.](https://github.com/protocolbuffers/protobuf/issues/5046#issuecomment-439736098)\r\n\r\nJust tried this and it worked! Thanks :)", "I have the latest version of python and still I am facing the same issue. Any help will be appreciated. I tried upgrading and downgrading the python version as well.  @ry  ", "ImportError: DLL load failed: The specified procedure could not be found.\r\n"]}, {"number": 21966, "title": "fix the comparison error when building a CPP API application", "body": "### Summary\r\n\r\nWhen building a CPP API application with \"-Wall -Werror\" option, `error: comparison between signed and unsigned integer expressions' occurs since return type of num_elements() is 'int64' instead of 'size_t' in ops.h to express -1. This patch fixes this bug by explicit type casting.\r\n\r\n### related issue\r\n* https://github.com/tensorflow/tensorflow/issues/20428\r\n\r\nSigned-off-by: Sangjung Woo <sangjung.woo@samsung.com>", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "### submit v2\r\n* Use `int64` intstead of naive `int`\r\n\r\n\r\n\r\n@reedwm, Could you go over this PR again? Thanks in advance.", "@reedwm @aaroey, \r\nPing!  Any comments?", "@aaroey,\r\nIf you are satisfied with this PR, could you merge it?\r\nOr let me know something to do. "]}, {"number": 21965, "title": "cannot find  \"flatbuffers/flatbuffers.h\"  head  file", "body": "in tensorflow/contrib/lite/schema/schema_generated.h file ::\r\n```\r\n#ifndef FLATBUFFERS_GENERATED_SCHEMA_TFLITE_H_\r\n#define FLATBUFFERS_GENERATED_SCHEMA_TFLITE_H_\r\n\r\n#include \"flatbuffers/flatbuffers.h\"\r\n#include <string>\r\nnamespace tflite {}\r\n```\r\n\r\nI want to ask where is the \"  flatbuffers/flatbuffers.h \"  file .\r\nI cannot find the  head   file in  tensorflow directory . \r\n Thanks", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It is pulled in by https://github.com/tensorflow/tensorflow/blob/master/third_party/flatbuffers/workspace.bzl", "thank you", "What do I need to specify or do to get the flatbuffers build? they are not installed in the configure routine and also not in the bazel build process. Do I need to set any flags for the flatbuffers to build?", "[https://www.tensorflow.org/lite/guide/build_rpi](https://www.tensorflow.org/lite/guide/build_rpi) @julian59189  download_dependencies.sh", "@julian59189 did you figured it out? I am facing the same issue.\r\n@dtsmith2001 Can you please elaborate your answer a bit. It's still not clear to me.", "@Pradeepyrl there is a script, `download_dependencies.sh` See instructions at https://www.tensorflow.org/lite/guide/build_rpi (even though link is for Raspberry Pi, most of the steps there apply on Linux too -- I just used them a few days ago)\r\n\r\nClosing this issue as it has been already resolved and is old. If issue occurs again, please open a new one, filling in issue template", "hi @mihaimaruseac It seems there is no download_dependencies.sh anymore. Is there a script to pull the flatbuffer now?", "The build changed to now use CMake. Please follow instructions at https://www.tensorflow.org/lite/guide/build_cmake and open a new issue if that does not work."]}, {"number": 21964, "title": "tf.sparse_concat does not infer result shape from inputs with fully defined shape", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nNA\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.10\r\n- **Python version**:\r\n3.6.5\r\n- **Bazel version (if compiling from source)**:\r\nNA\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNA\r\n- **CUDA/cuDNN version**:\r\n7.0\r\n- **GPU model and memory**:\r\nYes\r\n- **Exact command to reproduce**:\r\n```python\r\nIn [9]: import tensorflow as tf\r\n\r\nIn [10]: x = tf.SparseTensor(indices=[[0,0],[1,1]], values=[1,2], dense_shape=[2,2])\r\n\r\nIn [11]: y = tf.SparseTensor(indices=[[0,0],[1,1]], values=[1,2], dense_shape=[2,2])\r\n\r\nIn [12]: x.get_shape()\r\nOut[12]: TensorShape([Dimension(2), Dimension(2)])\r\n\r\nIn [13]: y.get_shape()\r\nOut[13]: TensorShape([Dimension(2), Dimension(2)])\r\n\r\nIn [14]: z = tf.sparse_concat(-1, [x,y])\r\n\r\nIn [15]: z.get_shape()\r\nOut[15]: TensorShape([Dimension(None), Dimension(None)])\r\n```\r\n\r\nThe shape of z should be infered if shape of x and y are fully defined.", "comments": ["I think the feature request makes sense. Unfortunately, because SparseTensor.dense_shape is a Tensor, I'm afraid that we have to use session to calculate the value of shape, eg `sess.run(z.dense_shape)`", "Thanks,\r\n\r\nCurrently, I did the following wrapping for a workaround\r\n\r\n```python\r\ndef sparse_concat(values, axis, *args, **kwargs):\r\n    shapes = [x.get_shape() for x in values]\r\n    res = tf.sparse_concat(axis, values, *args, **kwargs)\r\n    if all(x.is_fully_defined() for x in shapes):\r\n        dense_shape = shapes[0].as_list()\r\n        D = sum(x.as_list()[axis] for x in shapes)\r\n        dense_shape[axis] = D\r\n        res = tf.SparseTensor(indices=res.indices,\r\n                              values=res.values,\r\n                              dense_shape=dense_shape)\r\n\r\n    return res\r\n\r\n```", "Sounds like a good idea for me. Would you like to create a Pull Request to fix the issue after:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/63be4a65a1ba37848d3bfbe72ea2b60184d9979e/tensorflow/python/ops/sparse_ops.py#L314-L315\r\n\r\ncc @ebrevdo @aselle @drpngx How do you think?\r\n\r\n", "I'm sorry I'm not familar with the tensorflow codebase, so I don't know which tests need to run and how to run them after the modification. If anyone thinks this issue makes sense, please create a pull request. Thanks!", "@drpngx Hi, could you help here? Thanks.", "/CC @skye for shape inference. Unfortunately I'm out next week.", "Added a PR #24018 for the fix."]}, {"number": 21963, "title": "Building Windows Tensorflow C++ Debug version failed", "body": "**System information:**\r\n\r\n- Windows 10 Home\r\n- TensorFlow version: r1.4\r\n- Visual studio version: vs2015 Community x64\r\n- CUDA version: 8.0\r\n- CUDNN version: 6.0\r\n- GPU: NVIDIA GeForce 1080ti\r\n- Cmake version: 3.9.0\r\n- Swig version: 3.0.10\r\n\r\n**Problem Description:**\r\n\r\n- I followed some online guide to build tensorflow, firstly I used ALL_BUILD, but had compiler is out of heap space problem;\r\n- I set compiler to x64 and build tf_core_kernels -> tensroflow_static->tensorflow project only, it said cannot open file \"Debug\\tf_core_gpu_kernels.lib\";\r\n- I built tf_core_gpu_kernels only, the error message is: Severity\tCode Description Project File Line Suppression State Error calling a __host__ function(\"std::_Debug_message\") from a __device__ function(\"std::_Debug_lt<const int &, const int &> \") is not allowed\ttf_core_gpu_kernels\tD:\\Programs\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\xutility\t911\r\n\r\nMay I know how should I deal with this problem? Or anybody has successfully built tensorflow c++ windows debug version? If yes, may I know your configurations?\r\n\r\nThank you very much!\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It looks like you are using a fairly out of date version of TensorFlow. Can you try upgrading your version of TensorFlow, and resubmit if the issue continues? Thanks."]}, {"number": 21962, "title": "how to do speech test?", "body": "Hi Tech Support:\r\n\r\nThere is speech_test.cc in tensorflow/tensorflow/contrib/lite/models. After I modified BUILD file(attached here), I can have bazel-bin/tensorflow/contrib/lite/models/speech_test to run and also I have downloaded the needed model file  Models:Speech hotword model (Svdf rank=1), Speech hotword model (Svdf rank=2)\r\nSpeaker-id model, TTS model, ASR AM modelaccording to  \r\n the instrucitons from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/models/testdata/g3doc/README.md\r\nand also I have created  speech_hotword_model_in.csv and another needed .csv. \r\n\r\nWhen I run \"bazel-bin/tensorflow/contrib/lite/models/speech_test\", I have errors like:\r\n\r\n[ RUN      ] LongTests/SpeechTest.HotwordOkGoogleRank1Test/0\r\n[       OK ] LongTests/SpeechTest.HotwordOkGoogleRank1Test/0 (0 ms)\r\n[ RUN      ] LongTests/SpeechTest.HotwordOkGoogleRank2Test/0\r\n[       OK ] LongTests/SpeechTest.HotwordOkGoogleRank2Test/0 (1 ms)\r\n[ RUN      ] LongTests/SpeechTest.SpeakerIdOkGoogleTest/0\r\ntensorflow/contrib/lite/kernels/lstm.cc:262 node->outputs->size != 3 (4 != 3)\r\nFailed to allocate tensors\r\ntensorflow/contrib/lite/models/speech_test.cc:138: Failure\r\nValue of: testing::ParseAndRunTests(&os, &test_driver, GetMaxInvocations())\r\n  Actual: false\r\nExpected: true\r\nFailed to allocate tensors\r\n[  FAILED  ] LongTests/SpeechTest.SpeakerIdOkGoogleTest/0, where GetParam() = -1 (0 ms)\r\n[ RUN      ] LongTests/SpeechTest.AsrAmTest/0\r\ntensorflow/contrib/lite/kernels/lstm.cc:262 node->outputs->size != 3 (4 != 3)\r\nFailed to allocate tensors\r\ntensorflow/contrib/lite/models/speech_test.cc:151: Failure\r\nValue of: testing::ParseAndRunTests(&os, &test_driver, GetMaxInvocations())\r\n  Actual: false\r\nExpected: true\r\nFailed to allocate tensors\r\n[  FAILED  ] LongTests/SpeechTest.AsrAmTest/0, where GetParam() = -1 (1 ms)\r\n[ RUN      ] LongTests/SpeechTest.AsrLmTest/0\r\ntensorflow/contrib/lite/models/speech_test.cc:162: Failure\r\nValue of: Init(\"speech_asr_lm_model.test_spec\", &test_driver, &in_file)\r\n  Actual: false\r\nExpected: true\r\n\r\nI don't know where I can get correct speech_hotword_model_in.csv so I just created one including speech_asr_am_model_in.csv.\r\n\r\nI am not sure if it is a bug or I don't have the correct model_in.csv file. I cannot find the solutions from website so I have to ask the help from here.\r\n\r\nOr could you tell me the correct way to do speech test under ubuntu 16.04 please? Thank you very much in advance.\r\n\r\ncsv files needed are \r\nspeech_speakerid_model_in.csv, speech_asr_am_model_in.csv\r\nspeech_endpointer_model_in.csv, speech_tts_model_in.csv\r\n, speech_hotword_model_in.csv\r\n\r\n\r\nThanks and best regards\r\nFrank\r\n\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: modified some code and please see attached speech_test_changes.zip\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:No, try to run in desktop ubuntu machine.\r\n- **TensorFlow installed from (source or binary)**:build from source\r\n- **TensorFlow version (use command below)**:1.8\r\n- **Python version**:3.5\r\n- **Bazel version (if compiling from source)**:0.13.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:9.1 see atthced tf.env file\r\n- **GPU model and memory**:GTX 1080Ti\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n[speech_test_changes.zip](https://github.com/tensorflow/tensorflow/files/2334177/speech_test_changes.zip)\r\n\r\n\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\n\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2334116/tf_env.txt)\r\n", "comments": ["Hi @fzhu129, thanks for the report, we'll take a look and get back to you.", "Hi @jdduke, Thank you very much ", "@jdduke ,Could you please update the status on this issue and as well please provide steps to compile speech_test.cc on Ubuntu and run on it.\r\n\r\nThanks,\r\nNiranjan", "Over to @alanchiao who is more familiar with that test.", "@alanchiao added speech_test.cc to ./tensorflow/contrib/lite/tools/make/Makefile.txt file and rename to Makefile and refer the attached [Makefile.txt](https://github.com/tensorflow/tensorflow/files/2443717/Makefile.txt)\r\nhowever it is giving compilation error(/tensorflow/contrib/lite/tools/make/downloads/googletest/googletest/include/gtest/internal/gtest-port.h:408:11: fatal error: 'strings.h' file not found)\r\n while compiling on Ubuntu to run on Linux using below command\r\nmake -f ./tensorflow/contrib/lite/tools/make/Makefile\r\nand also I modified [speech_test.cc.txt](https://github.com/tensorflow/tensorflow/files/2443724/speech_test.cc.txt)\r\n\r\n\r\n\r\n\r\n\r\n", "@alanchiao please provide steps to compile speech_test.cc on Ubuntu Linux using ./tensorflow/contrib/lite/tools/make/Makefile and steps to run on Ubuntu Linux  including models files  \r\nspeech_hotword_model_rank1_2017_11_14.tflite  \r\nspeech_speakerid_model_2017_11_14.tflite      \r\nspeech_tts_model_2017_11_14.tflite            \r\nspeech_hotword_model_rank2_2017_11_14.tflite  \r\nspeech_terse_am_model_2017_11_14.tflite", "@fzhu129 : as you may have noticed, we've disabled the set of the tests in speech_test.cc (e.g. DISABLED_EndpointerTest). The reason is that we made a change in our kernels (e.g. LSTM) that would have required us to update the model files to have these tests continue to run (and pass with the provided in.csv and out.csv files). **These model files haven't been updated and the tests will not pass as is.**\r\n\r\n@niruyadla : from Frank's attached files, it looks like he commented out the inclusion of googletest.h in speech_test.cc. Could you try that out and look at the other changes he's made to make things build?\r\n\r\n@fzhu129 : @niruyadla : may I ask how these tests help you? I'm happy to recreate them if they provide value - I thought that they didn't help anyone in the community (since the unit tests themselves provide coverage for any changes people want to make and are easier to debug, and the models themselves (modified from originals) are unlikely to be usable in a demo).\r\n\r\nAny reasoning will also provide us with rationale to maintain these tests moving forward. Thanks!", "@alanchiao , As of now we don't have any speech models which are in LSTM Quant and available in TFLite, I assume some of the below tflite models may be quant lstm model and we like to run this model as part of TFLite Speech apk and run its full network to understand further. \r\nspeech_hotword_model_rank1_2017_11_14.tflite\r\nspeech_speakerid_model_2017_11_14.tflite\r\nspeech_tts_model_2017_11_14.tflite\r\nspeech_hotword_model_rank2_2017_11_14.tflite\r\nspeech_terse_am_model_2017_11_14.tflite\r\n\r\nMy goal is to run speech Quant model(lstm quant) on TFLite running on ubuntu and as well on Android.\r\n\r\nif possible could you please generate these models in order to run with the latest TensorFlowlite code and update it.\r\n\r\n", "@niruyadla : I'm assuming float models wouldn't serve your need. The models you see now are all floating point models. \r\n\r\nAre you trying to understand the effects of post-training quantization on LSTMs (e.g. how much the outputs differ in various models, latency) or the logic in our EvalHybrid kernels (ex: by running things through via gdb)?\r\n\r\nIf it's the above, this would be a feature request to maintain sample speech models (float and quantized) that run on TFLite for the sake of directly seeing the effects of post-training quantization (beyond what's publicly listed on blog posts, etc.). \r\n- This wouldn't be for creating demos (since we've modified the models so that they wouldn't work as well).\r\n- This wouldn't be for easily creating TFLite models from Tensorflow (since easy LSTM conversion support through TOCO is still lacking).\r\n\r\nLet me know what you think. Thanks!  ", "@alanchiao , do you have any speech quant models(not necessary lstm,even if you use conv or some other nodes it is fine ) which can run using TFlite on Ubunutu Linux so that I can debug (ex: by running things through via gdb)", "@alanchiao  but it is still good idea to run these models with speech_test so that community can try to understand how these models are working on lstm with single input(like OKGOOGLE hotword audio sample run thru network and generate output and matched with golden output.\r\nSo please modify speech_test.cc so that we can compile on Linux and run using gdb to step in and understand more about these lstm and svdf float models to detect \"okgoogle\" hotword.\r\n", "@alanchiao To give you an insight into what we are doing , we are porting TensorFlow Lite to our audio platforms. In order to confirm that what we ported is correct your speech_test.cc is very important for us.\r\nMaintaining speech_test.cc along with the tflite models is therefore very important to us.\r\nIdeally if you can provide both float and quant tflite models ,it willbe very useful to us.\r\nAtleast float tflite models are mandatory for us ,Can you regenerate working tflite models to work with speech_test.cc ?", "@alanchiao , do you have any update on this?\r\n", "@niruyadla : I'll try to add one of the speech models (hopefully submitted tomorrow) with both float and quant models to help you with your porting process.\r\n\r\nI'd be curious to how you plan to create your models to run via TFLite on your audio platforms. There isn't currently an easy path from Tensorflow for RNNs/LSTMs besides hand-creating the models.\r\n\r\nGiven that the Tensorflow to TFLite conversion process is still very difficult for LSTMs, the original thought was to remove these tests since we didn't think the community would\r\nuse these model tests (or the LSTM kernels) until the conversion process was ready.  You both are the first to bring this up in the last several months since they were first public.", "@alanchiao thanks for your update and will wait for your models.\r\nRight now we are using CNN based TFL speech model  to run on our audio platforms.", "@alanchiao, do you have any update on this?", "@alanchiao Any update from you?", "@alanchiao : Have you added one of the speech models with both float and quant models? Where I can find it? And also would like to know how to do quant models(some hits) for lstm or transformer? Thank you very much in advance.", "Added an acoustic model (float and quantized) in this [commit](https://github.com/tensorflow/tensorflow/commit/eb17145a7e8f8d50418d0238e8dbd445ea2bd1d7).\r\n\r\nAdditional support will be provided as TOCO is able to better automatically convert RNNs/LSTMs.", "@fzhu129 ,could you please share your BUILD changes here to build speech_test binary?", "Made below change now I could build speech_test \r\ndiff --git a/tensorflow/lite/models/BUILD b/tensorflow/lite/models/BUILD\r\nindex 8730160..d5bc8ca 100644\r\n--- a/tensorflow/lite/models/BUILD\r\n+++ b/tensorflow/lite/models/BUILD\r\n@@ -6,9 +6,30 @@ package(\r\n licenses([\"notice\"])  # Apache 2.0\r\n \r\n exports_files([\"LICENSE\"])\r\n-\r\n-load(\"//tensorflow/lite:build_def.bzl\", \"tflite_copts\")\r\n+load(\"//tensorflow:tensorflow.bzl\", \"tf_cc_test\")\r\n+load(\"//tensorflow/lite:build_def.bzl\",\"tflite_copts\")\r\n \r\n exports_files(glob([\r\n     \"testdata/*\",\r\n ]))\r\n+\r\n+\r\n+tf_cc_test(\r\n+    name = \"speech_test\",\r\n+    srcs = [\"speech_test.cc\"],\r\n+       data = [\r\n+               \"//tensorflow/lite/models:testdata/speech_asr_am_model_in.csv\",\r\n+\t       \"//tensorflow/lite/models:testdata/speech_asr_am_model.tflite\",\r\n+\t       \"//tensorflow/lite/models:testdata/speech_asr_am_model_out.csv\",\r\n+\t\t           ],\r\n+    tags = [\"no_oss\"],\r\n+    deps = [    \r\n+\t\"//tensorflow/lite/testing:parse_testdata_lib\",\r\n+        \"//tensorflow/lite/testing:tflite_driver\",\r\n+        \"//tensorflow/lite/testing:util\",\r\n+        \"@com_google_googletest//:gtest\",\r\n+    ],\r\n+)\r\n+\r\n+\r\n+"]}, {"number": 21961, "title": "ppc64le: //tensorflow/python/kernel_tests:depthwise_conv_op_test core dumps", "body": "Please assign this issue to me\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master branch from 8/29 (065f9b833ffbb3b2f03d63febb186275674ba133)\r\n- **Python version**: 3,6\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0, 7\r\n- **GPU model and memory**: 4 V100 GPUs with 16 GB of memory each\r\n- **Exact command to reproduce**:\r\nbazel test --config=cuda --test_tag_filters=-no_oss,-oss_serial,-no_gpu,-benchmark-test --test_timeout 300,450,1200,3600 --local_test_jobs=4 --test_output=errors --build_tests_only //tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/python/kernel_tests:depthwise_conv_op_test\r\n\r\n### Describe the problem\r\nRunning unit test of gpu code with python3, depthwise_conv_op_test core dumps \r\n\r\n### Source code / logs\r\n```\r\n==================== Test output for //tensorflow/python/kernel_tests:depthwise_conv_op_test:\r\nRunning test /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/depthwise_conv_op_test  on GPU 0\r\n2018-08-29 21:27:33.969227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties:\r\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\r\npciBusID: 0004:04:00.0\r\ntotalMemory: 15.75GiB freeMemory: 15.34GiB\r\n2018-08-29 21:27:33.969304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-29 21:31:13.982520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-29 21:31:13.982546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0\r\n2018-08-29 21:31:13.982556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N\r\n2018-08-29 21:31:13.983032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n.2018-08-29 21:31:14.042699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-29 21:31:14.042728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-29 21:31:14.042738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0\r\n2018-08-29 21:31:14.042746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N\r\n2018-08-29 21:31:14.043084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n2018-08-29 21:31:14.087484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-29 21:31:14.087516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-29 21:31:14.087524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0\r\n2018-08-29 21:31:14.087531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N\r\n2018-08-29 21:31:14.087867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n2018-08-29 21:31:21.854127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-29 21:31:21.854182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-29 21:31:21.854189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0\r\n2018-08-29 21:31:21.854197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N\r\n2018-08-29 21:31:21.854553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n2018-08-29 21:31:21.898468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-29 21:31:21.898491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-29 21:31:21.898498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0\r\n2018-08-29 21:31:21.898511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N\r\n2018-08-29 21:31:21.898847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n2018-08-29 21:31:21.955406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-29 21:31:21.955430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-29 21:31:21.955437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0\r\n2018-08-29 21:31:21.955450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N\r\n2018-08-29 21:31:21.955770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n2018-08-29 21:31:21.999863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-29 21:31:21.999884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-29 21:31:21.999892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0\r\n2018-08-29 21:31:21.999900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N\r\n2018-08-29 21:31:22.000215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n2018-08-29 21:31:22.057828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-29 21:31:22.057853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-29 21:31:22.057861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0\r\n2018-08-29 21:31:22.057869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N\r\n2018-08-29 21:31:22.058188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n2018-08-29 21:31:22.101895: F ./tensorflow/core/util/cuda_launch_config.h:186] Check failed: err == cudaSuccess (8 vs. 0)\r\n*** Received signal 6 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(+0x76ec9c)[0x7fff8aa4ec9c]\r\n[0x7fff9e0d04d8]\r\n/lib/powerpc64le-linux-gnu/libc.so.6(gsignal+0x40)[0x7fff9deeedb0]\r\n/lib/powerpc64le-linux-gnu/libc.so.6(abort+0x2b4)[0x7fff9def1314]\r\n/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow8internal15LogMessageFatalD1Ev+0x3c)[0x7fff90a6b1dc]\r\n/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow29LaunchDepthwiseConv2dGPUSmallIN5Eigen4halfELNS_24DepthwiseConv2dDirectionE0ELin1ELin1ELi8ELb1ES2_EENS_6StatusEPNS_15OpKernelContextERKNS_13DepthwiseArgsEPKT_SC_PSA_NS_12TensorFormatE+0x458)[0x7fff8eb22478]\r\n/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow29LaunchDepthwiseConv2dGPUSmallIN5Eigen4halfELNS_24DepthwiseConv2dDirectionE0ELin1ELin1EEENS_6StatusEPNS_15OpKernelContextERKNS_13DepthwiseArgsEPKT_SC_PSA_NS_12TensorFormatE+0x1e4)[0x7fff8eb2bf14]\r\n/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow21LaunchDepthwiseConvOpIN5Eigen9GpuDeviceENS1_4halfEEclEPNS_15OpKernelContextERKNS_13DepthwiseArgsEPKS3_SB_PS3_NS_12TensorFormatE+0x1e8)[0x7fff8eb30df8]\r\n/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow23DepthwiseConv2dNativeOpIN5Eigen9GpuDeviceENS1_4halfEE7ComputeEPNS_15OpKernelContextE+0x6f8)[0x7fff8eae47d8]\r\n/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice13ComputeHelperEPNS_8OpKernelEPNS_15OpKernelContextE+0x3ec)[0x7fff8a9546cc]\r\n/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice7ComputeEPNS_8OpKernelEPNS_15OpKernelContextE+0xd8)[0x7fff8a954d98]\r\n/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(+0x6ca9f8)[0x7fff8a9aa9f8]\r\n/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(_ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x350)[0x7fff8aa1a810]\r\n/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(_ZNSt17_Function_handlerIFvvEZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEEC4EibS5_EUlvE_E9_M_invokeERKSt9_Any_data+0x28)[0x7fff8aa1b168]\r\n/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x74)[0x7fff8aa174d4]\r\n/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(_ZNSt6thread5_ImplISt12_Bind_simpleIFSt8functionIFvvEEvEEE6_M_runEv+0x40)[0x7fff8aa50170]\r\n/usr/lib/powerpc64le-linux-gnu/libstdc++.so.6(+0xe49a4)[0x7fff975549a4]\r\n/lib/powerpc64le-linux-gnu/libpthread.so.0(+0x8070)[0x7fff9e088070]\r\n/lib/powerpc64le-linux-gnu/libc.so.6(clone+0x98)[0x7fff9dfd3a70]\r\n*** END MANGLED STACK TRACE ***\r\n\r\n*** Begin stack trace ***\r\n        tensorflow::CurrentStackTrace[abi:cxx11]()\r\n\r\n        __kernel_sigtramp_rt64\r\n        gsignal\r\n        abort\r\n        tensorflow::internal::LogMessageFatal::~LogMessageFatal()\r\n        tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPUSmall<Eigen::half, (tensorflow::DepthwiseConv2dDirection)0, -1, -1, 8, true, Eigen::half>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, Eigen::half const*, Eigen::half const*, Eigen::half*, tensorflow::TensorFormat)\r\n        tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPUSmall<Eigen::half, (tensorflow::DepthwiseConv2dDirection)0, -1, -1>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, Eigen::half const*, Eigen::half const*, Eigen::half*, tensorflow::TensorFormat)\r\n        tensorflow::LaunchDepthwiseConvOp<Eigen::GpuDevice, Eigen::half>::operator()(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, Eigen::half const*, Eigen::half const*, Eigen::half*, tensorflow::TensorFormat)\r\n        tensorflow::DepthwiseConv2dNativeOp<Eigen::GpuDevice, Eigen::half>::Compute(tensorflow::OpKernelContext*)\r\n        tensorflow::BaseGPUDevice::ComputeHelper(tensorflow::OpKernel*, tensorflow::OpKernelContext*)\r\n        tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*)\r\n\r\n        Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)\r\n        std::_Function_handler<void (), Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, bool, tensorflow::thread::EigenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\r\n        std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\r\n        std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> >::_M_run()\r\n\r\n\r\n        clone\r\n*** End stack trace ***\r\n/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/tools/ci_build/gpu_build/parallel_gpu_execute: line 64: 57550 Aborted                 (core dumped) \"$TEST_BINARY\" $@\r\n================================================================================\r\n```\r\n\r\ngdb where of the core dump:\r\n\r\n```\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/powerpc64le-linux-gnu/libthread_db.so.1\".\r\nCore was generated by `/usr/bin/python3 /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/'.\r\nProgram terminated with signal SIGABRT, Aborted.\r\n#0  0x00007fff9deeedb0 in __GI_raise (sig=<optimized out>) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n54      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\r\n[Current thread is 1 (Thread 0x7ffd517ff1a0 (LWP 58088))]\r\n(gdb) where\r\n#0  0x00007fff9deeedb0 in __GI_raise (sig=<optimized out>) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n#1  0x00007fff9def1270 in __GI_abort () at abort.c:89\r\n#2  0x00007fff8aa4ed18 in tensorflow::testing::StacktraceHandler(int, siginfo_t*, void*) ()\r\n   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so\r\n#3  <signal handler called>\r\n#4  0x00007fff9deeedb0 in __GI_raise (sig=<optimized out>) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n#5  0x00007fff9def1270 in __GI_abort () at abort.c:89\r\n#6  0x00007fff90a6b1dc in tensorflow::internal::LogMessageFatal::~LogMessageFatal() ()\r\n   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fff8eb22478 in tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPUSmall<Eigen::half, (tensorflow::DepthwiseConv2dDirection)0, -1, -1, 8, true, Eigen::half>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, Eigen::half const*, Eigen::half const*, Eigen::half*, tensorflow::TensorFormat) ()\r\n   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007fff8eb2bf14 in tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPUSmall<Eigen::half, (tensorflow::DepthwiseConv2dDirection)0, -1, -1>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, Eigen::half const*, Eigen::half const*, Eigen::half*, tensorflow::TensorFormat) ()\r\n   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007fff8eb30df8 in tensorflow::LaunchDepthwiseConvOp<Eigen::GpuDevice, Eigen::half>::operator()(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, Eigen::half const*, Eigen::half const*, Eigen::half*, tensorflow::TensorFormat) ()\r\n   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007fff8eae47d8 in tensorflow::DepthwiseConv2dNativeOp<Eigen::GpuDevice, Eigen::half>::Compute(tensorflow::OpKernelContext*) ()\r\n   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007fff8a9546cc in tensorflow::BaseGPUDevice::ComputeHelper(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so\r\n#12 0x00007fff8a954d98 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so\r\n#13 0x00007fff8a9aa9f8 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()\r\n   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so\r\n#14 0x00007fff8aa1a810 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so\r\n#15 0x00007fff8aa1b168 in std::_Function_handler<void (), Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, bool, tensorflow::thread::EigenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so\r\n#16 0x00007fff8aa174d4 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so\r\n#17 0x00007fff8aa50170 in std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> >::_M_run() ()\r\n   from /home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/depthwise_conv_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so\r\n#18 0x00007fff975549a4 in ?? () from /usr/lib/powerpc64le-linux-gnu/libstdc++.so.6\r\n#19 0x00007fff9e088070 in start_thread (arg=0x7ffd517ff1a0) at pthread_create.c:335\r\n#20 0x00007fff9dfd3a70 in clone () at ../sysdeps/unix/sysv/linux/powerpc/powerpc64/clone.S:96\r\n(gdb)\r\n\r\n```\r\n", "comments": ["Nagging Assignee @wdirons: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @wdirons I cannot reproduce this problem.\r\nIs it possible the core dump was caused by parallel_gpu_execute having GPU count hardcoded to 8?", "I noticed above I was using python 3.6, I'll try to recreate this today. If not I will close it.\r\n\r\nIn the last overnight OSU test (which is python 2.7) it passed:\r\n//tensorflow/python/kernel_tests:depthwise_conv_op_test                  PASSED in 70.8s\r\n\r\nhttps://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Build_and_Test/98/consoleText\r\n", "This problem does indeed appear to be fixed. Thank you gunan!  (I had accidentally posted this message in the wrong defect, 21960 and then deleted it) "]}, {"number": 21960, "title": "ppc64le: //tensorflow/python/kernel_tests:matrix_exponential_op_test fails", "body": "Please assign this issue to me\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:  Master branch from 8/29 (065f9b833ffbb3b2f03d63febb186275674ba133)\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0 / 7\r\n- **GPU model and memory**: 4 V100 GPUs with 16 GB of memory each\r\n- **Exact command to reproduce**: \r\nbazel test --config=cuda --test_tag_filters=-no_oss,-oss_serial,-no_gpu,-benchmark-test --test_timeout 300,450,1200,3600 --local_test_jobs=4 --test_output=errors --build_tests_only //tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/python/kernel_tests:matrix_exponential_op_test\r\n\r\n### Describe the problem\r\ntestL1Norms_complex128_3_2500 in matrix_exponential_op_test fails with a slight difference:\r\nnot close lhs =  [14.19404]\r\nnot close rhs =  [14.191119]\r\n\r\n\r\n### Source code / logs\r\n```\r\n======================================================================\r\nFAIL: testL1Norms_complex128_3_2500 (__main__.ExponentialOpTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python                      /kernel_tests/matrix_exponential_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/matrix_exponential_op_test.py\", line 234, in Test\r\n    self._verifyExponentialReal(scale * matrix)\r\n  File \"/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python                      /kernel_tests/matrix_exponential_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/matrix_exponential_op_test.py\", line 68, in _verifyExponentialReal\r\n    self._verifyExponential(x, np_type)\r\n  File \"/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python                      /kernel_tests/matrix_exponential_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/matrix_exponential_op_test.py\", line 64, in _verifyExponential\r\n    self.assertAllClose(np_ans, out, rtol=1e-4, atol=1e-3)\r\n  File \"/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python                      /kernel_tests/matrix_exponential_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1443, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python                      /kernel_tests/matrix_exponential_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1413, in _assertAllCloseRecursive\r\n    (path_str, path_str, msg)))\r\n  File \"/home/wdirons/upstream/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_wdirons/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python                      /kernel_tests/matrix_exponential_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1348, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=msg, equal_nan=True)\r\n  File \"/usr/local/lib/python3.5/dist-packages/numpy/testing/nose_tools/utils.py\", line 1396, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/usr/local/lib/python3.5/dist-packages/numpy/testing/nose_tools/utils.py\", line 779, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.0001, atol=0.001\r\nMismatched value: a is different from b.\r\n(mismatch 5.555555555555557%)\r\n x: array([[[  4.719045,   3.237183,  -1.798661],\r\n        [  5.548645,   3.793037,  -2.115632],\r\n        [ -5.977193,  -4.089234,   2.273146]],...\r\n y: array([[[  4.719063,   3.237082,  -1.798567],\r\n        [  5.548592,   3.793213,  -2.115799],\r\n        [ -5.977084,  -4.089204,   2.273036]],...\r\n\r\n----------------------------------------------------------------------\r\nRan 5 tests in 5.153s\r\n\r\nFAILED (failures=1)\r\n<class 'numpy.complex128'> (2, 3, 3) 25.0 [[[-0.25091976+0.j  0.90142861+0.j  0.46398788+0.j]\r\n  [ 0.19731697+0.j -0.68796272+0.j -0.68801096+0.j]\r\n  [-0.88383278+0.j  0.73235229+0.j  0.20223002+0.j]]\r\n\r\n [[ 0.41614516+0.j -0.95883101+0.j  0.9398197 +0.j]\r\n  [ 0.66488528+0.j -0.57532178+0.j -0.63635007+0.j]\r\n  [-0.63319098+0.j -0.39151551+0.j  0.04951286+0.j]]]\r\nnot close where =  (array([1]), array([2]), array([2]))\r\nnot close lhs =  [14.19404]\r\nnot close rhs =  [14.191119]\r\nnot close dif =  [0.0029211]\r\nnot close tol =  [0.00241911]\r\ndtype = float32, shape = (2, 3, 3)\r\n<class 'numpy.complex64'> (2, 3, 3) 5.0 [[[-0.25091976+0.j  0.90142864+0.j  0.4639879 +0.j]\r\n  [ 0.19731697+0.j -0.6879627 +0.j -0.68801093+0.j]\r\n  [-0.88383275+0.j  0.7323523 +0.j  0.20223002+0.j]]\r\n\r\n [[ 0.41614515+0.j -0.958831  +0.j  0.9398197 +0.j]\r\n  [ 0.6648853 +0.j -0.5753218 +0.j -0.6363501 +0.j]\r\n  [-0.633191  +0.j -0.39151552+0.j  0.04951286+0.j]]]\r\n<class 'numpy.float64'> (2, 3, 3) 6.0 [[[-0.25091976  0.90142861  0.46398788]\r\n  [ 0.19731697 -0.68796272 -0.68801096]\r\n  [-0.88383278  0.73235229  0.20223002]]\r\n\r\n [[ 0.41614516 -0.95883101  0.9398197 ]\r\n  [ 0.66488528 -0.57532178 -0.63635007]\r\n  [-0.63319098 -0.39151551  0.04951286]]]\r\n================================================================================\r\n\r\nFAILED: //tensorflow/python/kernel_tests:matrix_exponential_op_test (Summary)\r\n```", "comments": ["Nagging Assignee @wdirons: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@wdirons I just tried this now, with cuda 9.1, but I cannot reproduce the problem.\r\nRunning this command got an all passing result for me:\r\nbazel test --config=cuda tensorflow/python/kernel_tests:matrix_exponential_op_test --local_test_jobs=1 --runs_per_test=100\r\n\r\nI will close this issue now, please reopen if you are still seeing it.\r\n", "This is failing in the OSU nightly test with cuda 9.2, I will try the command above today and see what I get.", "This is still failing on ppc64le. I'll try more variations to see if I can isolate the problem."]}, {"number": 21959, "title": "tf.contrib.training.batch_sequences_with_states treats batching as duplicating", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nWhen `train.batch_sequences_with_states` extracts batches from a sequence of samples, what it actually does is duplicating the same segment for `batch_size` times.\r\n\r\n### Source code / logs\r\nThe code is revised based on [batch_sequences_with_states_test.py](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/contrib/training/python/training/batch_sequences_with_states_test.py)\r\n\r\n```python\r\nfrom pprint import pprint\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.training.python.training import sequence_queueing_state_saver as sqss\r\n\r\nbatch_size = 3\r\nnum_unroll = 2\r\nlstm_size = 4\r\nvalue_length = 8\r\ninput_key = tf.as_string(tf.cast(10000 * tf.random_uniform(()), tf.int32))\r\ninput_sequences = {'input': np.random.rand(value_length, 3)}\r\ninput_context = {'context_key': [1]}\r\ninitial_states = {\"lstm_state\": np.random.rand(10, lstm_size)}\r\n\r\nwith tf.Session() as sess:\r\n    batch = sqss.batch_sequences_with_states(\r\n        input_key=input_key,\r\n        input_sequences=input_sequences,\r\n        input_context=input_context,\r\n        input_length=value_length,\r\n        initial_states=initial_states,\r\n        num_unroll=num_unroll,\r\n        batch_size=batch_size)\r\n    state = batch.state('lstm_state')\r\n    update_state = batch.save_state('lstm_state', state + 1)\r\n    coord = tf.train.Coordinator()\r\n    tf.train.start_queue_runners(sess=sess, coord=coord)\r\n    input_batch_val = sess.run([\r\n        batch.key, batch.next_key, batch.sequences['input'],\r\n        batch.context['context_key'], state, batch.length, update_state][2])\r\n    pprint(input_batch_val)\r\n```\r\nOutput:\r\n```\r\narray([[[0.33537843, 0.76504494, 0.368679  ],\r\n        [0.47943187, 0.58871135, 0.06263617]],\r\n\r\n       [[0.33537843, 0.76504494, 0.368679  ],\r\n        [0.47943187, 0.58871135, 0.06263617]],\r\n\r\n       [[0.33537843, 0.76504494, 0.368679  ],\r\n        [0.47943187, 0.58871135, 0.06263617]]])\r\n```\r\nWhat further justifies my suspect is the expected values of sequences in unit tests [batch_sequences_with_states_test.py](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/contrib/training/python/training/batch_sequences_with_states_test.py), which is duplicating the first segment of `\"seq1\"`.\r\n```python\r\ndef _testBasicPadding(self, pad, key=None, make_keys_unique=False):\r\n    num_unroll = 2  # Divisor of value_length - so no padding necessary.\r\n    expected_seq1_batch1 = np.tile(\r\n        self.sequences[\"seq1\"][np.newaxis, 0:num_unroll, :],\r\n(self.batch_size, 1, 1))\r\n```", "comments": ["Is this still useful? It looks like this is using queues. Is there functionality in `tf.data` that you can use instead?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Sorry that I didn't notice your comment posted in January.\r\n\r\nAfter extensive exploration of the source code of tensorflow and the [code](https://github.com/tensorflow/models/tree/master/research/adversarial_text) written by Andrew Dai and Neal Wu, I found out that the argument `input_sequences` requires thread-and-queue-based implementation of the input sequences. Nonetheless, this crucial point is absent from the documentation. Later, I made a [notebook](https://github.com/htcai/Companion-to-TensorFlow-Documentation/blob/master/tf_contrib_training_batch_sequences_with_states.ipynb) illustrating the usage of `batch_sequences_with_states`.\r\n\r\nThanks for your suggestion, I have started to learn tf.data.", "@htcai Thanks for the nice writeup and the resource. Thanks!"]}, {"number": 21958, "title": "Update GPU occupancy checking to utilize CUDA's occupancy calculator ", "body": "This change will make supporting future architectures easier and removes dependence on hard-coded GPU data\r\n\r\n-Replace references to the UnqueryableDeviceParams struct with calls to CUDA's built-in occupancy calculation functions\r\n-Update calls to the occupancy checking functions with the new changes\r\n-Changes should provide more long-term reliability and will remove the need to manually update hard-coded data values for new GPU architectures\r\n\r\n@tfboyd  @zheng-xq ", "comments": ["Thanks for the PR, this looks really good.\r\n\r\nMy only concern is: Do you know when the new CUDA driver function you're calling was added?", "Looks like the functions were implemented in CUDA 6.5 (mentioned here: https://devblogs.nvidia.com/10-ways-cuda-6-5-improves-performance-productivity/)", "Thanks for the feedback, this commit moves the occupancy functions over to cuda_gpu_executor, which was the only file they appeared to be called from.  Device_description should be fine without cuda now", "lgtm, but let's see what the tests say.", "Looks like the tests are clean enough other than the clang-format business.  If you can make that change we should be able to merge this.\r\n\r\nThank you again for the patch, this is a good change.", "Thanks for your help, hopefully this fixes the issue", "Thanks @MattConley for the clang fix, but it seems there are still some left, would you mind to fix them (again)?\r\nThanks for the patient.", "Apologies, I had overlooked the placement of some references and dereferences; this should be all of the required formats within the changed code", "Nagging Assignee @aaroey: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@aaroey @jlebar Thanks again for your help; is this ready to be merged in?", "FYI I'm trying to merge this, running into some build problems internally, still not sure exactly what is going on.", "Thanks for the update; I'm not getting the issues on my end, please let me know if I can do anything to help.", "I'm not sure why I didn't see this when I reviewed the patch earlier, but this is actually a functional change for XLA.\r\n\r\nPreviously we'd set the block size to `device_desc.threads_per_core_limit() / device_desc.blocks_per_core_limit()`.  This is the smallest block size such that we can fill a core with these blocks.\r\n\r\nNow we set the block size to the *max* block size: `device_desc.threads_per_block_limit();`.  That's the opposite of the old code.\r\n\r\nWas this change intentional on your part?  I don't think it was intentional on my part...\r\n\r\nIs there a CUDA API that gives us information that lets us recover the old behavior?  (I'm not seeing it.) \r\n Or do we have to go back to hardcoding this info?", "@MattConley friendly ping.  I don't want to revert this if I don't have to, but I'm not actually sure how to fix this.", "Apologies for the delayed response; this change in behavior was my mistake.  It looks like the desired functionality is available through the  `cuOccupancyMaxActiveBlocksPerMultiprocessor` driver function, which is accessible from CUDADriver's `GetMaxOccupiedBlocksPerCore` wrapper.\r\n\r\nI'm looking at adding back the `blocks_per_core_limit` functionality to the device description, and then letting the cuda_gpu_executor fill in the field using the above call.  I'm running into a slight issue of passing in a valid CUfunction, but aside from that it looks like this is the easiest way to achieve desired behavior while avoiding hard-coding the values.\r\n\r\nI should have a solution fairly quickly, and will submit a merge request as soon as it's ready.", "Hey, friendly ping on this.  (I can also pick it up if you're busy; like, there's no obligation, it's not like this is your job.  :)", "Just submitted PR #24944 with a potential fix.  (Again, sorry for the delay :)  The solution seems somewhat clunky, but does integrate the occupancy calculator without the need for hardcoded values; definitely open to suggestions."]}, {"number": 21957, "title": "tf.scatter_nd_update - 'Tensor' object has no attribute '_lazy_read'", "body": "-----------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.5)\r\nprotobuf (3.6.1)\r\ntensorflow (1.10.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.10.0\r\ntf.GIT_VERSION = b'unknown'\r\ntf.COMPILER_VERSION = b'unknown'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed Aug 29 19:57:14 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a\r\n\r\n```\r\n1080Ti\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.random_uniform(minval=0, maxval=9, shape=[], dtype=tf.int32)\r\nimg = tf.zeros([10], dtype=tf.int32)\r\nimg_z = tf.scatter_nd_update(img, a, tf.zeros_like(a))\r\n                                           \r\n```\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI am not able to update a variable using tf.scatter_nd_update with indices/updates coming from Tensors `tf.random_uniform`. In another part of my code I use indices and updates coming from `tf.nn.top_k` and that works fine. \r\n\r\n### Source code / logs\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-9-ed4c565a2efd> in <module>()\r\n      3 a = tf.random_uniform(minval=0, maxval=10, shape=[], dtype=tf.int32)\r\n      4 img = tf.zeros([9], dtype=tf.int32)\r\n----> 5 img_z = tf.scatter_nd_update(img, a, tf.zeros_like(a))\r\n      6 \r\n      7 with tf.Session() as sess:\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py in scatter_nd_update(ref, indices, updates, use_locking, name)\r\n    357     return gen_state_ops.scatter_nd_update(\r\n    358         ref, indices, updates, use_locking, name)\r\n--> 359   return ref._lazy_read(gen_state_ops.resource_scatter_nd_update(  # pylint: disable=protected-access\r\n    360       ref.handle, indices, ops.convert_to_tensor(updates, ref.dtype),\r\n    361       name=name))\r\n\r\nAttributeError: 'Tensor' object has no attribute '_lazy_read'\r\n```\r\n", "comments": ["We can close it. Wrapping `img` in `tf.Variable()` solved the issue. Although I am not sure if that is the way to do when in my application `img` comes from the Dataset and the whole function that ends with `scatter_nd_update` is a part of the pipeline (called with ds.map()).\r\n\r\nI think we can close the ticket.", "I meet same error when I use Variable on Colabolatory (tf ver:1.14).\r\nJupyter note shows:\r\nhttps://github.com/IAMAl/QNet/blob/master/QNet.ipynb", "If I wrap `img` in `tf.Variable` then I get a new error.\r\n\r\n     `ValueError: Shape must be at least rank 1 but is rank 0 for 'ScatterNdUpdate' (op: 'ScatterNdUpdate') with input shapes: [10], [], [].`\r\n\r\nI am trying to use `tf.scatter_nd_update` by setting up a graph and use it in a loop while new values for attributes of `tf.scatter_nd_update` are supplied using a `feed_dict`. Any idea how to do this?"]}, {"number": 21956, "title": "[Intel MKL] Fix for stringpiece build failure", "body": "", "comments": ["@aaroey can you merge this PR? Thanks"]}, {"number": 21955, "title": "tf.stack on windows gives wrong result", "body": "### System information\r\n- Custom code:\r\n- Windows 7 x64:\r\n- Anaconda distributed\r\n- Tensorflow version 1.8.0:\r\n- python 3.6.0 :\r\n- CUDA  V9.0.176 /cuDNN 7.0:\r\n- NVIDIA QUADRO K1100M :\r\n\r\n### The problem\r\n``````python\r\nimport tensorflow as tf\r\nn_users = tf.placeholder(dtype=tf.int64, shape=[], name='n_users')\r\nn_items = tf.placeholder(dtype=tf.int64, shape=[], name='n_items')\r\nshape = tf.stack([n_users, n_items])\r\n\r\nsess = tf.Session()\r\nsess.run(shape, feed_dict={n_users: 100, n_items: 100})\r\n# array([-4631544634021885188,  4529727110648879041], dtype=int64)\r\n``````\r\nIt seems that it overflows. \r\n\r\nI also tried by using np.int64 instead of python native integers and it gives: \r\narray([ 10, 4357074203494797000], dtype=int64). \r\nIt works if I define int32 placeholders. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Can you try upgrading to the most recent version of TF (1.10)?", "@skye I tried with tensorflow 1.10 and it works", "Great, thanks for reporting back! I'll go ahead and close this since it works now."]}, {"number": 21954, "title": "Add libtensorflow_cc.so to pip_package BUILD", "body": "This allows to run inference from C++ code if we link against the `libtensorflow_cc.so` library that will get installed under `/usr/local/lib/python2.7/dist-packages/tensorflow` by default. Otherwise, this library doesn't get installed there and there's no other way to run inference from C++ using Tensorflow AFAIK.\r\n\r\nI've seen some issues related with this topic, but it's not clear to me what are the drawbacks of doing this, other than increasing the compile time for the additional target.\r\n\r\nSorry if I missed something and there's actually another way.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "I dont think this works just yet. the size of libtensorflow_cc.so would push the pip package even bigger (_cc.so is huge) and the pip package is already really big. But more importantly, last I heard the C++ API/ABI was not stable yet so I don't think shipping it is necesarily a good idea.\r\n\r\nThat said, I do ship _cc.so in the Gentoo TF package but I do it in /usr/lib64/ not under the python package.\r\n\r\nAlso, this is related: https://github.com/tensorflow/tensorflow/issues/18360", "Thanks for your comment @perfinion .\r\n\r\nYou're right, `libtensorflow_cc.so` is quite large.\r\n\r\n> the C++ API/ABI was not stable yet\r\nDo you know if that's also true for just running inference?\r\n\r\nAlthough it'd require more work, with the appropriate guidance I might be able to look into a cleaner solution. Maybe an additional artifact (e.g. deb packge in Debian/Ubuntu) that gets generated for the C++ interface only, and also installing it in the standard path.\r\nDo you know how to roughly do that, and where? (some additional bazel rules, ...)", "@efernandez yeah, making a .deb would probably be the best solution. I have packaged TensorFlow on Gentoo, you can refer to the ebuild for guidance. I'm not that familiar with debian but i'll definitely help however I can.\r\nHere is the gentoo package, its quite involved but you can hopefully make sense of it:\r\nhttps://github.com/gentoo/gentoo/blob/master/sci-libs/tensorflow/tensorflow-1.10.0_rc1.ebuild", "This PR as is would make the pip package much larger so this is not something we would like to do.\r\nMost of the API endpoints should already be available through libtensorflow_framework.so\r\n\r\nFor the rest of the discussion, I think this is a nice discussion to have in build@tensorflow.org"]}, {"number": 21953, "title": "Feature request: partial_run_reset()", "body": "Experimental partial_run feature leaks memory. The problem is demonstrated below (with explanatory comments). A simple solution might look like `session.partial_run_reset()` at the end of the while loop. \r\n\r\nThanks if this can be added!\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# Define graph\r\nsome_placeholder_value = tf.placeholder(tf.float32, shape=[])\r\nlong_list_of_ones = tf.ones(10000)\r\nsome_operation = long_list_of_ones + 1\r\na_second_operation = some_operation + some_placeholder_value\r\n\r\n# Initialize\r\ninitialize = tf.global_variables_initializer()\r\nsession = tf.Session()\r\nsession.run(initialize)\r\n\r\n# Using partial run in a loop causes a memory leak if not all fetches are fetched. \r\n# For increased dynamism please allow resetting the partial run graph so that one \r\n# does not have to know for sure which fetches will be used ahead of time.\r\n\r\nwhile True:\r\n    # Watch as application memory steadily increases... \r\n    handle = session.partial_run_setup([some_operation, a_second_operation], \r\n                                       [some_placeholder_value])\r\n    run = session.partial_run(handle, some_operation)\r\n```\r\n\r\nMemory usage grows really fast! Since a great advantage of this feature is that it affords greater dynamism and modularity, the option to reset at the end of each iteration would help since it would allow one to use this feature without knowing the full list of fetches a priori.\r\n\r\nAlternatively, allowing the same graph components to be re-fetched would also do the trick (by putting handle outside the while loop) and would afford even more dynamism and modularity... but is probably harder to implement.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "/CC @vrv, can you take a look?", "The two authors / maintainers of this experimental feature no longer work on TensorFlow, as far as I know, and partial_run functionality is probably better solved by using Eager Execution.\r\n\r\nI'd love to leave this open for community contributions to fix this, but my guess is that no one will ever get to this, and everyone is better off moving to Eager, which is supposed to be the default mode in TF 2.0 anyway and will be supported much better."]}]