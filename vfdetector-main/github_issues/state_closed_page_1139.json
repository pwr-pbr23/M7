[{"number": 19044, "title": "Upgrade pip in install_python3.5_pip_packages.sh", "body": "Copying over change for r1.8 branch to r1.7 branch.\r\nCorresponding change:\r\nhttps://github.com/tensorflow/tensorflow/pull/18709/files", "comments": []}, {"number": 19043, "title": "Code documetnantion. Probably misprint in function parameter description.", "body": "https://github.com/tensorflow/tensorflow/blob/a44996a84b24c43cca40c685a009fd59275755ab/tensorflow/contrib/slim/python/slim/learning.py#L573\r\n\r\nI believe there was a typo in the description in the penultimate word (\"and\"), which is probably worth replacing with \"are\".\r\nExample:  \r\nlog_every_n_steps: The frequency, in terms of global steps, that the loss and global step **are** logged.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "That seems reasonable to me; seems like a good PR if you're interested in contributing.", "Created a pull request Update Learning.py to resolve this issue.", "Fixed in #19064 . Thanks!"]}, {"number": 19042, "title": "Anyone gotten Facebook's AlphaGo to compile yet", "body": "How much stronger is it than leela zero on only one playout?\r\n\r\n\r\nhttps://github.com/gcp/leela-zero/issues/1311\r\n\r\nhttps://github.com/pytorch/ELF/issues/1", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19041, "title": "Saving Estimators, loading from checkpoints and accessing placeholders.", "body": "I posted this question here on stackoverflow (https://stackoverflow.com/questions/50011969/tensorflow-estimator-api-input-tensor-names) and someone else posted a similar question earlier (https://stackoverflow.com/questions/48335699/tensorflow-what-are-the-input-nodes-for-tf-estimator-models), yet no satisfying answers. \r\n\r\nWhen using an estimator in tensorflow and passing the inputs using `tf.estimator.inputs.numpy_input_function()`, I would like to find a way to access the input placeholders (one for features and one for labels).\r\nI train the model using `tf.estimator.estimator.train()` and save the checkpoint files in `model_dir`. \r\nLater, I would like to load the model and variables using the following snippet:\r\n`sess = tf.Session()`\r\n`saver = tf.train.import_meta_graph(model_dir+'/model.ckpt-1.meta')`\r\n`saver.restore(sess, tf.train.latest_checkpoint(model_dir))`\r\n`predictions = graph.get_tensor_by_name('softmax_tensor:0')`\r\n\r\nThen do `sess.run(predictions,....)`. But there's no way to pull out the inputs' placeholder for the `feed_dict` parameter of `sess.run()`.\r\n\r\nAnother related issue I have: I would also like to add placeholders to an estimator other than the features and labels placeholders. For example if I have dropout, I would like the dropout_rate or the `tf.layers.dropout()` function's `training` param to be placeholders. This way after loading the model from chkpt as above, I can turn off dropout. \r\n\r\n\r\nHave I written custom code Y\r\nOS Platform and Distribution Ubuntu 18.04\r\nTensorFlow installed from pip\r\nTensorFlow version r1.8.0\r\nBazel version N/A\r\nCUDA/cuDNN version 9.0/7.0.1\r\nGPU model and memory GTX 1080TI\r\nExact command to reproduce N/A\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thank you, I just updated it above. Sorry, this is my first time opening an issue here. ", "Can you please elaborate how you use placeholders in the code here? Some code snippet on how you're setting those up and passing them into the numpy_input_fn?", "I am not using place holders in setting up and training the estimator, this is done internally by the estimator object. Here is a code snippet on how I set up the estimator and train it.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom keras.datasets import cifar10\r\n\r\ndef model_fn(features, labels, mode, params):\r\n\r\n    x = tf.reshape(features['x'], [-1, 32, 32, 3])\r\n    x = tf.to_float(x)\r\n    flatten = tf.layers.flatten(x)\r\n    logits = tf.layers.dense(inputs=flatten,\r\n                             units=10)\r\n\r\n\r\n    predictions = {\r\n        'classes': tf.argmax(input=logits, axis=1),\r\n        'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\r\n    }\r\n\r\n    # Calculate Loss (for both TRAIN and EVAL modes)\r\n    l2_loss = tf.losses.get_regularization_losses()\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n    loss += tf.reduce_sum(l2_loss)\r\n    # Calculate Accuracy (for both TRAIN and EVAL modes)\r\n    accuracy = tf.metrics.accuracy(labels=labels,\r\n                                   predictions=predictions['classes'],\r\n                                   name='acc_op')\r\n    metrics = {'accuracy': accuracy}\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = tf.train.AdamOptimizer()\r\n        learning_rate = optimizer._lr\r\n        train_op = optimizer.minimize(\r\n            loss=loss,\r\n            global_step=tf.train.get_global_step())\r\n\r\n\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          train_op=train_op)\r\n\r\n\r\n    # Add evaluation metrics (for EVAL mode)\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          eval_metric_ops=metrics)\r\n\r\n\r\nif __name__== '__main__':\r\n    # Load CIFAR10 data\r\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n\r\n    # Cast y arrays into int32\r\n    y_train = np.array(y_train, dtype=np.int32)\r\n\r\n    # Create the Estimator\r\n    model_dir = '/home/zeyad/delete_me/'\r\n    classifier = tf.estimator.Estimator(\r\n        model_fn=model_fn,\r\n        params=None,\r\n        model_dir=model_dir)\r\n\r\n    input_fn = tf.estimator.inputs.numpy_input_fn(x={'x':x_train},\r\n                                                  y=y_train,\r\n                                                  batch_size=1, shuffle=True)\r\n\r\n    classifier.train(\r\n        input_fn=input_fn,\r\n        max_steps=5)\r\n\r\n```\r\n\r\nNow after the above saves the checkpoint files, I would like to load those checkpoint files using the following snippet, but I am not sure how to retrieve the input placeholder by name:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel_dir = '/home/zeyad/delete_me/'\r\ndef attack():\r\n\r\n    sess = tf.Session()\r\n    saver = tf.train.import_meta_graph(model_dir+'/model.ckpt-1.meta')\r\n    saver.restore(sess, tf.train.latest_checkpoint(model_dir))\r\n    graph = tf.get_default_graph()\r\n\r\n    inputs = graph.get_tensor_by_name()     # what do i put here?\r\n```\r\n\r\nIf I were able to do that then i can implement things such as implementing an adversarial attack on a previously saved network, disabling dropout, etc... \r\n", "I have run into the exact same problem, after training the network with the estimator api I would like to add placeholder input notes to graph in order to deploy it on edge and I just don't seem to be able to find a way to do this.", "If you view your graph in tensorboard, you can pretty easily discern the name of the tensors you'll need at inference time.\r\nThen, something like: \r\n\r\n```\r\nsaver.restore(sess, tf.train.latest_checkpoint(model_dir))\r\ngd = sess.graph_def\r\ninp, prediction = tf.import_graph_def(\r\n  gd, return_elements = ['input_node:0', 'output_node/BiasAdd:0'])\r\nresult = sess.run(prediction, feed_dict={inp: my_prediction_inputs})\r\n```\r\n\r\nyou can specify more than one input and output also. However, if your restored graph has batch_norm or dropout you may run into issues with this method. There is also probably a more elegant solution, but I just gave up and stopped using estimators in favor of having direct access to the session and layers and both training and inference time. ", "Thank you. I tried using tensorboard to do so for my [code snippet above](https://github.com/tensorflow/tensorflow/issues/19041#issuecomment-389611457), however, the names of the tensors are not easy to discern.  In that code snippet, the first computation that I define in the graph is a `reshape` op. Tensorboard shows the input to that op as another op called `random_shuffle_queue_DequeueUpTo` which is not a placeholder. And once again the inputs to that op is `enqueue_input/random_shuffle_queue` which is also an op and not a placeholder - this latter op has no inputs. I was not able to trace back to the original placeholder. I am not sure if I am missing something obvious. \r\n\r\nAs for using the lower level api, I do so as well, however, when generating adversarial examples for instance, it is easier to download a trained network from the web and work with it. Having access to the input placeholder would make this feasible (i ended up re-purposing the PREDICT mode in the estimator to produce adversarial examples and it looks really whacky.) ", "@zeyademam In the example I gave you should be able to choose arbitrary 'opname:output_index' pairings as your \"placeholder\". This may be technically incorrect, but you may want to think of all node:output_index pairings in a loaded graph as placeholders for the purpose of picking inputs and outputs. Think of it like you're going to the store, starting from your house. The entry point isn't your house, its the first street you take when you walk out the door. Again, no promises on how accurate this metaphor is from a technical standpoint .\r\n\r\nAdditionally, to figure out what the number output_index is, just find which index from random_shuffle_queue_DequeueueUpTo leads to reshape.", "I have a general set of questions:\r\nWhy is saving and reloading models in Keras a few lines and every tutorial about how to save and reload tensorflow estimators is at least dozens of lines. Is there no way to treat a session, graph and a set of tensorflow variables/tensors as a single entity? Doesn't Keras already exist to do that? Many things only support tensorflow and not Keras so converting a Keras model to tensorflow estimators is useful at times (thank you for that feature!). But saving them seems to necessarily involve manually handling the session, graph and set of variables separately. Is there a way around this that I am not aware of?", "I think loading estimators to keep training, evaluating, or perform predictions is fairly simple. \r\nMy issue is with loading estimators in a lower level api to do more than what those three estimator ModeKeys allow. This is pretty difficult to do in my opinion. I gave up and used a workaround. ", "> bly a more elegant solution, but I just gave up and stopped using estimators in favor of having direct access to the session and layers and both training and inference time.\r\n\r\nWhat do you use instead of estimators? Aren't those good for training? Also what is the change in tf 2.0 ?", "@zeyademam\r\nCould you please try on latest stable version of tf and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19041\">No</a>\n"]}, {"number": 19040, "title": "Merge r1.8 back to master after 1.8.0 release.", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to determine that you authored the commits in this PR.  Maybe you used a different email address in the git commits than was used to sign the CLA?  If someone else authored these commits, then please add them to this pull request and have them confirm that they're okay with them being contributed to Google.  If there are co-authors, make sure they're formatted properly.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- unknown_author -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->"]}, {"number": 19039, "title": "[WIP] Enabling support for S3 and Google Storage for the MKL Docker image.", "body": "@claynerobison Couldn't get this to build locally, any hints?", "comments": ["I'm working on a new Dockerfile that just pulls from tensorflow/tensorflow:lastest-devel. This file will be obsolete."]}, {"number": 19038, "title": "TensorFlow 1.8 upgrade causes Keras CI build to stall", "body": "Here's a detailed description of the issue: https://github.com/keras-team/keras/issues/10100\r\n\r\n**Summary:**\r\n\r\n- Keras runs its CI tests on Travis\r\n- Keras has two builds for its TensorFlow backend tests (Py 2.7 and Py 3.6)\r\n- TF dependency is installed via `pip install tensorflow` (CPU version)\r\n- A few days ago this started installing TF 1.8\r\n- Both TF backend builds immediately started timing out after 25-50 min (high variance) with the error \"No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.\". Normally the TF tests take 17-18 min.\r\n- No test started failing, this is entirely a build stalling issue.\r\n\r\n**Reverting to TF 1.7 fixes the issue.** The exact cause of the stalling is completely unknown.\r\n\r\nIn addition, we know that:\r\n\r\n- The failure is stochastic; sometimes one of the two TF builds (for Python 2.7 and 3.6) fails while the other passes. Most common case is that both fail, indicating that P(failure) is high.\r\n- There are cases where TF Py 2.7 fails while 3.6 passes and cases where 3.6 fails while 2.7 passes, so the issue is not specific to a Python version.\r\n- The failure is **not** due to an increase in the time taken by the tests. Disabling all large tests did not resolve the issue.\r\n\r\n**Such stalling problems on Travis typically arise from either:**\r\n\r\n- hanging process\r\n- excessive RAM consumption leading to freezing execution\r\n\r\nIt is likely that TF 1.8 suffers from one of these two issues.\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "For the record, we still don't know what is causing the issue.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "This issue seems to me still very actual, see unsuccessful attempt to update tensorflow in keras tests in https://github.com/keras-team/keras/pull/10674 and https://github.com/keras-team/keras/pull/10929, they kind of \"monkeyhacked\" this by skipping tests that are having this problem in Keras.\r\n\r\nI'm also stuck with `tensorflow==1.7.0` for my tests leveraging multiprocessing (running several tests leveraging keras in a row). I'm having exactly the same problem also with `tf.keras` since `tensorflow==1.8.0`.\r\n\r\nI'd be for re-opening this issue!"]}, {"number": 19037, "title": "corrected command for installation of cuda command-line tools", "body": "docs correction for [16214](https://github.com/tensorflow/tensorflow/issues/16214)", "comments": ["Nagging Assignee @protoget: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 75 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 90 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This file has conflicts. Can you update the PR or close it if it is obsolete. Thanks!", "new install_linux.md takes care of it"]}, {"number": 19036, "title": "Updating versions for 1.7.1", "body": "", "comments": ["I think windows bazel build was setup after 1.7. We can ignore it.", "I changed versions in install_sources.md tables back to 1.7.0."]}, {"number": 19035, "title": "Grammar fixes on architecture.md", "body": "", "comments": []}, {"number": 19034, "title": "corrected docs for installing cuda-command-line-tools", "body": "correcting [16214](https://github.com/tensorflow/tensorflow/issues/16214)", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}, {"number": 19033, "title": "corrected docs for installing cuda-command-line-tools", "body": "correcting [16214](https://github.com/tensorflow/tensorflow/issues/16214)", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}, {"number": 19032, "title": "Cleanup usage of the protobuf workspace.", "body": "- Always refer to protobuf as @com_google_protobuf rather than @protobuf_archive.\r\n- Remove com_google_protobuf_cc, which is not required by modern Bazel versions.", "comments": ["Looks like @benoitsteiner added the `@com_google_protobuf_cc` alias in 4e18625c55afdbe50e922a70a12df05320e387e0 to support an external repository. I'll let him review.", "Can I expect a review soon? This change gets stale quickly.", "@mrry, @benoitsteiner: Is there any chance someone else could review this? Thank you.", "I've triggered the tests. I don't see an issue with this unless we find some failure.", "I think the test failures we're seeing now are flakes or existing breakage? I checked a few of the failing ones, and the same failures are appearing on master currently.", "I agree. I'll attempt a pull.", "@benjaminp could you pull rebase? It doesn't appear to build.", "@drpngx, I've now pushed a merge from master. (Does tensorflow have a preference between merges and rebases?)\r\n\r\nI hope this update makes it submitable. As I noted in May, this mechanical change rots fast. Let me know if I can help further.", "I prefer rebase because it's easier to see. Let me run the tests.", "This should be handled by our sync engineer -- whomever that is, feel free to ask me for help with the necessary changes.", "I'm pushing a rebase on the latest master in case that's required.", "How do we find out who the sync engineer is?", "Nobody is assigned. That is not good. @wt-huang can you pull this in?", "@benjaminp sorry about this, this PR fell through the cracks.", "Ah, in fact, @drpngx has been working on this -- because it's changing the build a fair amount, it's tricky to get through our sync scripts.", "Yeah, I started looking into this back then. It requires a lot of changes inside, because the protobuf dependencies are added manually, then a script goes and edits the build files, by renaming the ones that we already have and also adding manually in some other rules. We also have rules internally mirroring, but different from the public ones. So the ones that we have added manually must be removed, since the bazel rule that you added already adds them. There are different places where they are added. The way I had been working through this was to make an edit and send it to the internal CI but that's infeasible.\r\n\r\nThere are about 80 mentions of protobuf in our transformation script and 200 mentions in the build files.", "@drpngx Thanks for looking into this, sounds like a lot of decluttering. I know it might be tricky, but you may consider using a mapping tool or an internal script to somehow semi-automate this.", "@benjaminp can you please resolve conflicts", "Previous comments suggest merging this needs from Google-internal changes. Is anyone interested in making those?", "Nagging Reviewer @agarwal-ashish, @fredbertsch, @martinwicke, @yongtang, @yupbank: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "I'm happy to rebase it if a Googler is willing to do the work to merge it.", "Please try to work with gunan or angerson. This is something we should do\nbut complex to integrate on the Google side.\n\nOn Sat, May 25, 2019, 5:53 AM Benjamin Peterson <notifications@github.com>\nwrote:\n\n> I'm happy to rebase it if a Googler is willing to do the work to merge it.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/19032?email_source=notifications&email_token=AE75E3LG3DHPUOWI2SWVDBDPXBPW3A5CNFSM4E6AQRH2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGUFOI#issuecomment-495796921>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AE75E3PGNC4ZOAFJV4A4VNDPXBPW3ANCNFSM4E6AQRHQ>\n> .\n>\n", "Maybe it would be easier to semantically apply this PR rather than literally importing it? It's big but I basically just did a global rename of `protobuf_archive` to `com_google_protobuf` in all Starlark and `BUILD` files.", "mentioning @gunan @angersson.", "Manually integrated the changes, commit incoming", "Woohoo!\n\nOn Tue, May 28, 2019, 10:17 AM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> Manually integrated the changes, commit incoming\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/19032?email_source=notifications&email_token=AE75E3M45WHHFZYLMPHCCMLPXVSJLA5CNFSM4E6AQRH2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWMZSRQ#issuecomment-496605510>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AE75E3OBLSO3P52ZT2PUEQTPXVSJLANCNFSM4E6AQRHQ>\n> .\n>\n", "This is getting delayed due to a build failure on gpu that I have to investigate, but will get there", "This should be fixed now in 338c2f269b1dfc13fe8f94c6cf9c64b4ae82b928", "Thank you @mihaimaruseac ", "Yes, thank you very much for taking care of that.", "Awesome thank you so much!\n\nOn Wed, Jun 19, 2019, 4:41 PM Benjamin Peterson <notifications@github.com>\nwrote:\n\n> Yes, thank you very much for taking care of that.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/19032?email_source=notifications&email_token=AE75E3M3Z3BKZUTTZMDWOO3P3K74JA5CNFSM4E6AQRH2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYDSHYI#issuecomment-503784417>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AE75E3NSPECZJP7XQJ7FUTDP3K74JANCNFSM4E6AQRHQ>\n> .\n>\n"]}, {"number": 19031, "title": "command Typo", "body": "", "comments": []}, {"number": 19030, "title": "Copy module list before iterating over it", "body": "Copying sys.modules.values before iterating over it. Seems like we are seeing errors that dictionary is getting modified during iteration on some plaforms.\r\nAlso, I am adding \"from tensorflow import python\" explicitly since we are iterating over modules under tensorflow.python.", "comments": []}, {"number": 19029, "title": "Invalid License Error While Running bazel build for tensorflow", "body": "I am trying to compile tensorflow for Macbook, but every time I am getting same error.\r\n\r\nWhen I Run this command\r\n\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nI get this error\r\n\r\n\r\n`/private/var/tmp/_bazel_cagrigider/2c548436011ab6b7308cd0cc70a91ed3/external/bazel_tools/tools/cpp/BUILD:3:1: invalid license type: 'notice'`\r\n\r\n[Full Output Of Terminal](https://i.stack.imgur.com/S9MtY.png)\r\n\r\nMac OS 10.13.3\r\n\r\nXcode version = 9.2\r\n\r\nTensorflow version = 1.8\r\n\r\nBazel version = 0.13.0\r\n\r\nThanks.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I am trying to use \r\nTensorflow 1.8 on my 2015 Macbook Pro (OS X 10.13.3 High Sierra) CPU only.\r\n\r\nI installed Tensorflow from [https://www.tensorflow.org/install/install_mac](tensorflow)\r\n\r\nTensorflow Version = 1.8\r\nBazel Version = 0.13.0\r\n\r\nI am getting error with that command \r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n", "This looks like it could potentially be a bug with our build system or a problem with Bazel. Same failure instance: https://github.com/bazelbuild/bazel/issues/5157\r\n\r\n@gunan Any clues on this? We could defer to the Bazel-side bug for now.", "This is an exact duplicate of bazelbuild/bazel#5157\r\nThe failure is also emitted from bazel_tools external repository.\r\nI will close this as a duplicate."]}, {"number": 19028, "title": "Is it possible to parepare two set of weights for feedforward and backprop separately?", "body": "Currently I'm working on the network quantization and I try to implement the ideas of the paper [\"Mixed precision training\"](https://arxiv.org/abs/1710.03740) using tensorflow. In this paper, the author prepare two set of weights (with different data type) for feedforward and backprop separately. I want to know if current tensorflow can support this kind of feature. Thanks.\r\n\r\n![screenshot from 2018-05-02 10-14-31](https://user-images.githubusercontent.com/13360081/39538427-a48b88fa-4df1-11e8-99ee-e2ee284b94a9.png)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "A GitHub issue is probably not the best forum for this, but @zheng-xq @protoget @bignamehyp may want to comment as they are looking into making this easier going forward. But yes, it is possible right now (by keeping variables as floats, but casting to half precision before computation, and then scaling the loss/gradients before applying them), and they may want to chime in with details.  ", "It isn't really two sets of weights. The weights are casted and used in following computation. The gradients are casted back to variable dtype before applied on weights.\r\n\r\nWe're working on supporting it at tensorflow layer level for mixed precision training. Its goal is to support training with ops done in one dtype and variables in a separate dtype, which seems to solve most of our usecases so far.\r\n\r\n"]}, {"number": 19027, "title": "Speed regression on tensorflow version > 1.4.1", "body": "We have observed some massive slowdowns in some Keras/tensorflow models with tensorflow versions newer than 1.4.1.\r\nNot sure if the issue is with tensorflow or with the way keras creates the tensorflow models, so I am cross-posting these issue to both repos (apologies for that!).\r\n\r\nHere is a script reproducing the issue:\r\n\r\n## Setup\r\n```python\r\nfrom pandas import get_dummies\r\nfrom sklearn.datasets import make_classification\r\n\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout\r\nfrom keras.callbacks import EarlyStopping\r\n\r\nX, y = make_classification(n_samples=10000, n_features=10, n_informative=8,\r\n                           n_classes=5, n_clusters_per_class=1, random_state=0)\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(10, input_dim=10, activation='relu'))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(5, activation='sigmoid'))\r\nmodel.add(Dropout(0.1))\r\nmodel.add(Dense(5, activation='softmax'))\r\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\r\n\r\nyk = get_dummies(y)\r\n```\r\n\r\n## Fitting\r\n\r\n```python\r\nmodel.fit(X, yk, epochs=10, batch_size=50, \r\n               callbacks=[EarlyStopping(monitor='loss', patience=2)])   \r\n```\r\n\r\nHere are some timings for the fit method:\r\n* Tensorflow 1.4.1:  **2.91 s \u00b1 452 ms** per loop  (obtained using ipython's `%%timeit` magic, 7 loops)\r\n* Tensorflow 1.5.0:  CPU times: user **2min 19s**, sys: 5min 22s, total: 7min 41s Wall time: 1min 2s\r\n* Tensorflow 1.6.0: CPU times: user **5min 5s**, sys: 12min 31s, total: 17min 36s Wall time: 2min 37s\r\n* Tensorflow 1.7.0: CPU times: user **5min 5s**, sys: 12min 39s, total: 17min 45s Wall time: 2min 39s\r\n\r\nSo, it seems there was a massive slowdown in version 1,5, and then a further one in 1.6 (which similar speed in 1.7). All the tests are run on a conda environment with python 3.6.5 and keras 2.1.5, with the corresponding tensorflow versions all coming from the anaconda `defaults` channel.\r\n\r\nThe GPU accelerated version of keras/tensorflow (`keras-gpu` conda package) does not present the issue.\r\n\r\nThanks in advance!", "comments": ["Could this be related to #17383? The magnitude of the slowdown is similar to the one reported there. Note that I see the slowdown in 1.7 as well (haven't been able to test 1.8 as there is no conda package yet)", "I saw about a 10% decrease in performance when going from 1.7 to 1.8.\r\nI can't share code (NDA) but I use the same operations as you in a larger graph, and with the tf.data.Dataset API. No Keras.", "Do you also see the massive performance decrease between 1.4 and 1.5?", "@tfboyd @tatianashp any comment here?", "Nagging Assignee @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The latest anaconda package (tensorflow 1.8) does not show the slowdown anymore, with timings back in line to what 1.4.1 was; so this issue can be closed.\r\n\r\nHowever, it would be great if (an adequate variation of) the test above could be added as a regression test to ensure that the problem doesn't come back.", "Nagging Assignee @tfboyd: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19026, "title": "Tensorflow lite interpreter->AllocateTensors() fails with  it->size != alloc.size (15840 != 0)", "body": "=### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nRaspberry Pi 3\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.7.0\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.10.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ng++ 6.3.0\r\n- **CUDA/cuDNN version**:\r\nNone\r\n- **GPU model and memory**:\r\n\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nI was able to build tensorflow-lite.a lib and test out examples without any problem. However, when i tried loading my own model (converted from frozen .pb graph), I get following error. function interpreter->AllocateTensors()  fails for some reason. Not able to understand why?\r\n\r\ntensorflow/contrib/lite/simple_memory_arena.cc:82 it->size != alloc.size (15840 != 0)\r\n\r\n", "comments": ["Loaded model foo.tflite\r\nresolved reporter\r\nconstruct interpreter\r\ntensors size: 15\r\nnodes size: 5\r\ninputs: 1\r\ninput(0) name: input\r\n0: MatMul_bias, 48, 1, 0, 0\r\n1: Reshape, 15680, 1, 0, 0\r\n2: add, 48, 1, 0, 0\r\n3: fc1/MatMul_bias, 512, 1, 0, 0\r\n4: fc1/Relu, 512, 1, 0, 0\r\n5: fc1/W/transpose, 2007040, 1, 0, 0\r\n6: fc2/MatMul_bias, 512, 1, 0, 0\r\n7: fc2/Relu, 512, 1, 0, 0\r\n8: fc2/W/transpose, 65536, 1, 0, 0\r\n9: fc3/MatMul_bias, 512, 1, 0, 0\r\n10: fc3/Relu, 512, 1, 0, 0\r\n11: fc3/W/transpose, 65536, 1, 0, 0\r\n12: final_fc/transpose, 6144, 1, 0, 0\r\n13: input, 15840, 1, 0, 0\r\n14: labels_softmax, 48, 1, 0, 0\r\ninput: 13\r\nnumber of inputs: 1\r\nnumber of outputs: 1\r\ntensorflow/contrib/lite/simple_memory_arena.cc:82 it->size != alloc.size (15840 != 0)\r\nSegmentation fault\r\n", "Breakpoint 2, tflite::SimpleMemoryArena::Deallocate (this=0x7f7398, context=0x7f0b80, alloc=...)\r\n    at tensorflow/contrib/lite/simple_memory_arena.cc:82\r\n82            TF_LITE_ENSURE_EQ(context, it->size, alloc.size);\r\n(gdb) print it\r\n$1 = {offset = 0, size = 15680}\r\n(gdb) print alloc\r\n$2 = (const tflite::ArenaAlloc &) @0x7f7440: {offset = 0, size = 0}\r\n(gdb) \r\n\r\nThis is on the first time the breakpoint is hit which confirms my earlier suspicion. \r\n", "Disabling check at simple_memory_arena.cc:82 - TF_LITE_ENSURE_EQ(context, it->size, alloc.size); \r\ndoes not help. interpreter->Invoke() throws an error \r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n0x0000000000561a6d in tflite::tensor_utils::PortableMatrixBatchVectorMultiplyAccumulate (matrix=0x7ffff68e14c4, m_rows=128, \r\n    m_cols=3920, vector=0x0, n_batch=1, result=0x7fb4c0, result_stride=1)\r\n    at tensorflow/contrib/lite/kernels/internal/reference/portable_tensor_utils.cc:65\r\n65              *result_in_batch += *matrix_ptr++ * *vector_in_batch++;", "This looks more like a model issue. I tried with different model and it works fine. ", "The simple_arena code is riddled with bugs. I was trying another models and it screws up memory after few interpreter invocations. Can we fully test the simple_arena code?", "Nagging Assignee @petewarden: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Not worth the wait. closing it.", "cc @aselle, @martinwicke ", "@cogmeta, we're sorry you had trouble. Some of the bugs you have seen have been resolved in the arena allocator. In particular, the input is now not-destroyed. Previously, subsequent invoke's() after the first invoke would receive different inputs. Now that the inputs are touched. Also, there was an issue resolved with 0 size tensors. I understand you are frustrated. It would also be helpful if you could give us a copy of your original graphdef and your tflite model with repro instructions.  Thanks!", "I think this is now resolved. Could you try a nightly or build from source. In particular, make sure whatever verison you use has this\r\nhttps://github.com/tensorflow/tensorflow/commit/85c518b8d306204cd7111f321a4b7b204fc554f4\r\nUnfortunately, this is not in r1.9, so you'll need to use master or the nightly.\r\n", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks for closing the issue. Did it actually work for you? We'd love to hear.", "@aselle , I get this issue still on v1.13.1, but with different parameters. \r\n\r\nRuntimeError: tensorflow/lite/simple_memory_arena.cc:96 it->size != alloc.size (-595591168 != 411041792)\r\n\r\nI'm trying to run on a Raspberry Pi with a Resnet-18 - should I open a new issue?"]}, {"number": 19025, "title": "Branch 195061425", "body": "", "comments": []}, {"number": 19024, "title": "Bug : creating variables from restored variables get reinitialized but not always", "body": "\r\n\r\nI've been experiencing the bug on two configurations :\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 / Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: conda tensorflow-GPU / conda tensorflow (CPU)\r\n- **TensorFlow version (use command below)**: 1.6 / 1.4\r\n- **Python version**:  Python 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA Toolkit 9.1 and cuDNN 7.1.2 / None\r\n- **GPU model and memory**: 1080ti 11GB / \r\n- **Exact command to reproduce**:\r\n\r\n```\r\nx = {\r\n    '1' : tf.Variable(tf.random_normal([3, 3]), trainable=True, name='x1')\r\n    }\r\nsaver = tf.train.Saver(var_list=x)\r\n\r\nwith tf.Session() as sess:\r\n    \r\n    \r\n    \r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    print(sess.run(x))\r\n    saver.save(sess, './savedModels/bug/x')\r\n```\r\n\r\n> \r\n> {'1': array([[-2.80644059, -0.7185123 ,  0.70223355],\r\n>        [ 1.06445408, -0.72174907,  1.29832721],\r\n>        [ 1.52049255, -1.19468224, -1.02100158]], dtype=float32)}\r\n\r\n```\r\nfor i in range(0,10):\r\n    with tf.Session() as sess:\r\n\r\n        new_saver = tf.train.import_meta_graph('./savedModels/bug/x.meta')\r\n        new_saver.restore(sess, './savedModels/bug/x')\r\n\r\n        x = {\r\n            '1' : tf.Variable(tf.get_default_graph().get_tensor_by_name(\"x1:0\"), trainable=False)\r\n            }\r\n        sess.run(tf.global_variables_initializer())\r\n        print(sess.run(x['1']))\r\n```\r\n\r\n> \r\n> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x\r\n> [[ 0.49538991 -0.11817512 -0.10268462]\r\n>  [ 0.51088262  1.28533709  0.05063328]\r\n>  [-1.30132782  0.41913262  1.3775363 ]]\r\n**> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x\r\n> [[-2.80644059 -0.7185123   0.70223355]\r\n>  [ 1.06445408 -0.72174907  1.29832721]\r\n>  [ 1.52049255 -1.19468224 -1.02100158]]**\r\n> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x\r\n> [[-2.29186296  0.36665305  0.28915456]\r\n>  [ 0.35180676  0.10379237  1.17195833]\r\n>  [-1.05604255  0.01500762  2.31890965]]\r\n> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x\r\n> [[ 0.79247308  0.43540171 -1.15054667]\r\n>  [-1.51901126  0.4132176  -1.45383906]\r\n>  [-1.19301605 -0.21523039  1.42999935]]\r\n> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x\r\n> [[-0.03097171 -1.22662902 -0.30857435]\r\n>  [-0.49384364 -0.98362756  0.12052365]\r\n>  [-0.78354359  0.58901048 -1.8879807 ]]\r\n> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x\r\n> [[-0.91630346 -1.20698965 -0.10588568]\r\n>  [-2.06214571 -0.81664461  0.58493197]\r\n>  [ 0.40064785 -1.33241594  2.23627448]]\r\n**> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x\r\n> [[-2.80644059 -0.7185123   0.70223355]\r\n>  [ 1.06445408 -0.72174907  1.29832721]\r\n>  [ 1.52049255 -1.19468224 -1.02100158]]**\r\n> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x\r\n> [[-1.16766882  0.48798651 -1.84950495]\r\n>  [-0.90750635  0.68074739 -0.86705917]\r\n>  [ 0.6242311  -0.75273407  0.95616102]]\r\n> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x\r\n> [[ 0.02900926 -0.71752155  1.40167475]\r\n>  [-1.76051104 -0.38902128 -0.42455733]\r\n>  [ 0.61904657 -0.45060599  1.10379994]]\r\n> INFO:tensorflow:Restoring parameters from ./savedModels/bug/x\r\n> [[-1.51588821 -2.11397028  0.33227748]\r\n>  [ 0.34245569 -2.17953372  0.60712498]\r\n>  [ 1.37212336  0.4941766   0.47627288]]\r\n\r\n-------------------------------------------\r\n\r\nRestoring the variable without using a sess.run() to ensure the constance, and then running the initializer should either:\r\n-give the same result because variable are stored as numbers\r\n-give different numbers because it is stored as a variable and thus, get initialized too.\r\n\r\nIn any case, we shouldn't find back the variables from time to time, it should be extremely unlikely to find back EXACTLY the same values when initialized, or, we should always find the same values.\r\n\r\nNOTA BENE: \r\nI only find the values back when I use tensorflow 1.4 ( I haven't been able to check this on 1.7 yet), has this bug been fixed in the way I described above since?\r\n", "comments": ["The problem seems to have been fixed in 1.6", "Closing since issue is fixed in latest version. Thank you for documenting that in case anyone else runs into this issue."]}, {"number": 19023, "title": "Doesn`t work with Golang version until 1.7", "body": "### System information\r\n- OS Platform and Distribution: `Linux Ubuntu 16.04` \r\n- TensorFlow version: `1.7.0` \r\n- TensorFlow installed from `binary`\r\n- Have I written custom code: `not`, thats because i can`t install the library\r\n- Bazel version: bazel is `not installed`\r\n- CUDA/cuDNN version: `i have not` CUDA/cuDNN\r\n- GPU model and memory: GPU - `AMD MULLINS (DRM 2.50.0 / 4.13.0-37-generic, LLVM 5.0.0)`, RAM - `6.8 gb`\r\n- Exact command to reproduce: `go get github.com/tensorflow/tensorflow/tensorflow/go`\r\n\r\n### Tthe problem\r\nI installed TensorFlow as shown in the [instruction](https://www.tensorflow.org/install/install_go), but with `go get github.com/tensorflow/tensorflow/tensorflow/go` failed:\r\n```\r\n# github.com/tensorflow/tensorflow/tensorflow/go\r\ncould not determine kind of name for C. CBytes\r\n```\r\nbut it was only in Golang version below 1.7, BUT to use it I can't because I need another library gocv (and other binding OpenCV for Golang) work only in version 1.6. \r\n### Question\r\nIs there any possibility to use TensorFlow with Golang 1.6?\r\nP.S. [env collect](https://www.dropbox.com/s/vawhuyh1d58a3k6/tf_env.txt?dl=0)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Unfortunately, nope, we don't have the bandwidth to support very old versions of Go.\r\nCurrently, we're trying to keep in sync with [the supported releases of Go](https://golang.org/doc/devel/release.html#policy), which at this time corresponds to 1.10 and 1.9. \r\n\r\nIf a simple code change makes things work with older versions of Go, without sacrificing compatibility and performance with the supported Go releases, then we'd be happy to accept a pull request. If not, will have to rely on community support.\r\n\r\nThanks! Sorry that I didn't give the answer you were probably hoping for!", "Its very funny but  Tensorflow libs dont work also with golang 1.10 1.11 1.12 \r\n\r\n7 months ago...\r\nOmg"]}, {"number": 19022, "title": "Documentation format mistake", "body": "### Describe the problem\r\n\r\n\r\nhere, the `exeption_mode` ought to be an arg, while Markdown recognizes it as a list member\r\n![image](https://user-images.githubusercontent.com/11363529/39514973-4726e0bc-4e2b-11e8-9cb3-dc70f6273e5e.png)\r\n\r\n![image](https://user-images.githubusercontent.com/11363529/39514893-0d7b9ee8-4e2b-11e8-98d6-40acb1171315.png)\r\n\r\napi doc address: \r\nhttps://www.tensorflow.org/api_docs/python/tf/enable_eager_execution\r\n\r\nthe problem source code:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L5426\r\n\r\n### System information\r\nv1.8.0-0-g93bc2e2072 1.8.0\r\n\r\nIt's not a problem of my environment, it's a problem of the official api doc\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Added a PR #19116 as an attempt to fix the doc format issue."]}, {"number": 19021, "title": "unsupported operation SquaredDifference & Pow", "body": "Hi,\r\n\r\nThere are some unsupported operation\r\n\r\n2018-05-02 16:44:35.396116: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference\r\n2018-05-02 16:44:35.396179: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow\r\n2018-05-02 16:44:35.396269: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference\r\n2018-05-02 16:44:35.396311: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow\r\n2018-05-02 16:44:35.396454: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference\r\n2018-05-02 16:44:35.396494: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow\r\n2018-05-02 16:44:35.396635: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference\r\n2018-05-02 16:44:35.396675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow\r\n2018-05-02 16:44:35.396792: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference\r\n2018-05-02 16:44:35.396818: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow\r\n2018-05-02 16:44:35.396909: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference\r\n2018-05-02 16:44:35.396936: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow\r\n2018-05-02 16:44:35.397025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference\r\n2018-05-02 16:44:35.397051: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow\r\n2018-05-02 16:44:35.397145: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: SquaredDifference\r\n2018-05-02 16:44:35.397172: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1270] Converting unsupported operation: Pow\r\n", "comments": ["Endorse.", "I see pow in r1.10 and I have a way to get rid of the SquaredDifference in moments so yeah!", "@gbildson Thx, you inspired me!\r\nIn my situation , I changed the tensorflow code and fix it!\r\nchange the code in tensorflow/python/ops/nn_impl.py\r\n`math_ops.squared_difference(y, array_ops.stop_gradient(mean)),`\r\nto\r\n`    \r\ndifference = math_ops.subtract(y, array_ops.stop_gradient(mean))\r\n`\r\n`\r\n    math_ops.mul(difference,difference),\r\n`\r\nwhich is in mements def.", "Nagging Assignee @andrehentz: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is SquaredDifference being used in your training graph or eval graph. Right now you should only try to convert an eval graph which should probably not have the squared difference operation (commonly used in the loss). So create a separate eval graph and freeze the training checkpoint into that. \r\n\r\nIf SquaredDifference is needed in your eval graph, ignore my above comment. And I have tracked your ask of squared difference in the aggregation issue above.", " I'm using InstanceNormalization in a trained Image Transformation Network.\u00a0 This uses tf.moments to do it's thing so I do need it.\u00a0 My realization and workaround was that I can replace the squared_difference in tf.moments with a tf.subtraction and tf.multiply to do the same.\u00a0 This appears to work although may not be as performant.\u00a0\nThanks-greg\n    On Saturday, August 18, 2018, 8:39:36 PM EDT, Suharsh Sivakumar <notifications@github.com> wrote:  \n \n \nIs SquaredDifference being used in your training graph or eval graph. Right now you should only try to convert an eval graph which should probably not have the squared difference operation (commonly used in the loss). So create a separate eval graph and freeze the training checkpoint into that.\n\nIf SquaredDifference is needed in your eval graph, ignore my above comment. And I have tracked your ask of squared difference in the aggregation issue above.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n   ", "dear gbildson,\r\nCan you please explain how did you bypassed the usage of tf.moments (in order to calculate the var and then var^0.5 for the instance normalization )?\r\n\r\nThanks", " I went into the tf.moments source and changed it to use supported methods:\n\u00a0 \u00a0 diff = math_ops.subtract(y, array_ops.stop_gradient(mean))\u00a0 \u00a0 variance = math_ops.reduce_mean(\u00a0 \u00a0 \u00a0 \u00a0 #math_ops.squared_difference(y, array_ops.stop_gradient(mean)),\u00a0 \u00a0 \u00a0 \u00a0 math_ops.multiply(diff,diff),\u00a0 \u00a0 \u00a0 \u00a0 axes,\nSo I broke squared_difference down into a subtract and a multiply.\n\n    On Tuesday, October 23, 2018, 8:35:05 AM EDT, Vadim <notifications@github.com> wrote:  \n \n \ndear gbildson,\nCan you please explain how did you bypassed the usage of tf.moments (in order to calculate the var and then var^0.5 for the instance normalization )?\n\nThanks\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n   ", "Let me try that more cleanly ...\r\nI went into the tf.moments source and changed it to use supported methods:\r\n~~~~\r\n   diff = math_ops.subtract(y, array_ops.stop_gradient(mean))\r\n    variance = math_ops.reduce_mean(\r\n        #math_ops.squared_difference(y, array_ops.stop_gradient(mean)),\r\n        math_ops.multiply(diff,diff),\r\n        axes,\r\n        keepdims=True,\r\n        name=\"variance\")\r\n~~~~\r\n\r\nand in recent tflite versions, pow is supported so this works in instance normalization:\r\n`            normalized = (inputs - mu) / (sigma_sq + EPSILON) ** .5\r\n`", "Now I'm just wondering about the performance and memory overhead of that manual squared_difference.", "Thanks @gbildson ,\r\nI can try to benchmark the model with those changes, and I will try to update. My last solution did use a lot of memory to perform Square operations:\r\n\r\n```\r\n2018-10-24 15:35:10.255607: I tensorflow/core/util/stat_summarizer.cc:85] ============================== Summary by node type ==============================\r\n2018-10-24 15:35:10.255611: I tensorflow/core/util/stat_summarizer.cc:85]                    [Node type]          [count]         [avg ms]         [avg %]          [cdf %]       [mem KB]      [times called]\r\n2018-10-24 15:35:10.255616: I tensorflow/core/util/stat_summarizer.cc:85]                         Conv2D               14         5227.149         76.688%          76.688%     1152921.625            14\r\n2018-10-24 15:35:10.255620: I tensorflow/core/util/stat_summarizer.cc:85]            Conv2DBackpropInput                2          614.725          9.019%          85.707%     1293926.375             2\r\n2018-10-24 15:35:10.255625: I tensorflow/core/util/stat_summarizer.cc:85]                           Mean               64          283.185          4.155%          89.861%       7431.704             64\r\n2018-10-24 15:35:10.255629: I tensorflow/core/util/stat_summarizer.cc:85]                            Mul               33          150.679          2.211%          92.072%          0.000             33\r\n2018-10-24 15:35:10.255633: I tensorflow/core/util/stat_summarizer.cc:85]                            Add               32          142.681          2.093%          94.165%          0.000             32\r\n2018-10-24 15:35:10.255642: I tensorflow/core/util/stat_summarizer.cc:85]                            Sub               16          133.475          1.958%          96.123%          0.000             16\r\n2018-10-24 15:35:10.255649: I tensorflow/core/util/stat_summarizer.cc:85]                         Square               16          126.811          1.860%          97.984%     1551052.750            16\r\n2018-10-24 15:35:10.255657: I tensorflow/core/util/stat_summarizer.cc:85]                           Relu               10           82.338          1.208%          99.192%          0.000             10\r\n2018-10-24 15:35:10.255664: I tensorflow/core/util/stat_summarizer.cc:85]                           AddN                5           52.525          0.771%          99.962%          0.000              5\r\n2018-10-24 15:35:10.255675: I tensorflow/core/util/stat_summarizer.cc:85]                        Sigmoid                1            2.446          0.036%          99.998%          0.000              1\r\n2018-10-24 15:35:10.255682: I tensorflow/core/util/stat_summarizer.cc:85]                          Const               54            0.069          0.001%          99.999%          0.000             54\r\n2018-10-24 15:35:10.255690: I tensorflow/core/util/stat_summarizer.cc:85]                          Rsqrt               16            0.043          0.001%         100.000%          0.000             16\r\n2018-10-24 15:35:10.255697: I tensorflow/core/util/stat_summarizer.cc:85]                        _Retval                1            0.005          0.000%         100.000%          0.000              1\r\n2018-10-24 15:35:10.255705: I tensorflow/core/util/stat_summarizer.cc:85]                           NoOp                1            0.004          0.000%         100.000%          0.000              1\r\n2018-10-24 15:35:10.255714: I tensorflow/core/util/stat_summarizer.cc:85]                           _Arg                1            0.001          0.000%         100.000%          0.000              1\r\n```\r\n"]}, {"number": 19020, "title": "tf.data leaves hash table not initialized", "body": "tensorflow version: 1.6\r\n\r\n### bug description: \r\n  when using hash_table in \"tensorflow.python.ops.gen_lookup_ops\"  in tf.data.Dataset.map function\r\nbecause  tf.data.Dataset.map do not use the default graph, the hash_table can not be initialized.\r\n\r\n### Exception: \r\n  FailedPreconditionError (see above for traceback): Table not initialized.\r\n\r\n### code:\r\nfrom __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\ntry:\r\n    from tensorflow.python.ops.gen_lookup_ops import hash_table as _hash_table\r\n    from tensorflow.python.ops.gen_lookup_ops import initialize_table as _initialize_table\r\n    from tensorflow.python.ops.gen_lookup_ops import initialize_table_from_text_file as _initialize_table_from_text_file\r\n    from tensorflow.python.ops.gen_lookup_ops import lookup_table_find as _lookup_table_find\r\n    from tensorflow.python.ops.gen_lookup_ops import lookup_table_size as _lookup_table_size\r\nexcept:\r\n    from tensorflow.python.ops.gen_lookup_ops import _hash_table\r\n    from tensorflow.python.ops.gen_lookup_ops import _initialize_table\r\n    from tensorflow.python.ops.gen_lookup_ops import _initialize_table_from_text_file\r\n    from tensorflow.python.ops.gen_lookup_ops import _lookup_table_find\r\n    from tensorflow.python.ops.gen_lookup_ops import _lookup_table_size    \r\n\r\ndef look_up(input_tensor, look_up_table_ref, default_value=tf.constant(0, dtype=tf.int64), name=None):\r\n    i_shape = input_tensor.get_shape()\r\n    r = _lookup_table_find(look_up_table_ref, keys=input_tensor, default_value=default_value)\r\n    r.set_shape(i_shape)\r\n    return r\r\n\r\ndef string2int64_via_map(input_tensor, keys, values, default_value=0, table_ref=None): \r\n    from tensorflow.python.framework import ops\r\n    with tf.name_scope('string2int64_via_map'):\r\n        key_type = tf.string\r\n        value_type = tf.int64\r\n        keys = tf.convert_to_tensor(keys, dtype=key_type)\r\n        values = tf.convert_to_tensor(values, dtype=value_type)\r\n        default_value = tf.convert_to_tensor(default_value, dtype=value_type)\r\n        if(table_ref is None):\r\n            table_ref = _hash_table(key_dtype=key_type, value_dtype=value_type)\r\n        init_op = _initialize_table(table_ref, keys, values)\r\n        ops.add_to_collection(ops.GraphKeys.TABLE_INITIALIZERS, init_op)\r\n        indices = _lookup_table_find(table_ref, keys=input_tensor, default_value=default_value)\r\n        \r\n        print(\"graph in string2int64_via_map: \\t%s\" % tf.get_default_graph())\r\n        return indices, table_ref\r\n\r\n\r\ndef __test_string2int64_via_map():\r\n    ''' this function works well\r\n    '''\r\n    print('__test_string2int64_via_map()')\r\n    keys = ['s1', 's2', 's3']\r\n    values = [10, 20, 30]\r\n    input_tensor = ['s2', 's4', 's6', 's8', 's3']\r\n    default_value = 0\r\n    \r\n    indices, _ = string2int64_via_map(input_tensor, keys, values, default_value)\r\n    with tf.Session() as sess:\r\n        sess.run(tf.tables_initializer())\r\n        rr = sess.run(indices)\r\n        print(rr)\r\n        import sys\r\n        sys.stdout.flush()\r\n\r\ndef test_dataset_using_hashmap():\r\n    features = ['s1', 's2', 's3']\r\n    labels = [0, 1, 2]\r\n    sess = tf.Session()\r\n    \r\n    def mapfun_works(txt, label):\r\n        return (tf.string_join([\"prefix:\", txt], separator=''), label)\r\n    \r\n    def mapfun_using_hash_table(txt, label):\r\n        keys = ['s1', 's2', 's3']\r\n        values = [10, 20, 30]\r\n        default_value = 0\r\n        indices, _ = string2int64_via_map(txt, keys, values, default_value)\r\n        return (indices, label)\r\n        \r\n    mapfunc = mapfun_using_hash_table  # change to mapfun1 works\r\n    \r\n    def train_input_fn(features, labels, batch_size):\r\n        dataset = tf.data.Dataset.from_tensor_slices((features, labels))\r\n        dataset = dataset.shuffle(10).repeat().batch(batch_size)\r\n        dataset = dataset.map(mapfunc, num_parallel_calls=1)\r\n        return dataset\r\n    \r\n    dataset = train_input_fn(features, labels, batch_size=4)\r\n    it = dataset.make_initializable_iterator()\r\n    sess.run(it.initializer)\r\n    sess.run(tf.tables_initializer())  # not work because the hash table is in another graph\r\n    \r\n    print(\"graph in main: \\t\\t\\t%s\" % tf.get_default_graph())\r\n    print(sess.run(it.get_next()))\r\n\r\n\r\nif __name__ == '__main__':\r\n    __test_string2int64_via_map()\r\n    print('\\n------------------------------\\n')\r\n    test_dataset_using_hashmap()\r\n\r\n", "comments": ["The solution is to create the table once outside the `mapfunc`, and call the find operation inside the `mapfunc`."]}, {"number": 19019, "title": "Fix Makefile to not use benchmark anymore (switch to minimal)", "body": "Minimal uses nothing and does almost nothing, but it does nothing\r\nrequiring protos or rest of tensorflow runtime.\r\n\r\nBenchmark_model originally was more like this, but it became\r\nuseful for actually benchmarking, making it less useful as a minimal\r\nexample.", "comments": []}, {"number": 19018, "title": "java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.nmanoharlal.emoji2/com.example.nmanoharlal.emoji2.MainActivity}: java.lang.IllegalStateException: Attempting to use uninitialized value conv1d_1/W", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nWhen using model.pb with the TensorFlowInferenceInterface, we are getting above exception.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n"]}, {"number": 19017, "title": "Strang behavior in memory copy with control dependencies.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I have costom code.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: `redhat-7.2`\r\n- **TensorFlow installed from (source or binary)**: `binary`\r\n- **TensorFlow version (use command below)**: `1.4.0`\r\n- **Python version**: `2.7`\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: `CUDA-8.0`, `cuDNN-6.0`\r\n- **GPU model and memory**: `Nvidia - K80`\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nThere are two different ways to apply memory copy from CPU to GPU:0 in the following code:\r\n```\r\n  with tf.control_dependencies([update_op]):\r\n    # Method-1 :\r\n    # with tf.device('/GPU:0'):\r\n    #   a_cpu_to_gpu = a_cpu.read_value()\r\n    # train_ops.append(vars[i].assign(a_cpu_to_gpu).op)\r\n\r\n    # Method-2 :\r\n    train_ops.append(vars[i].assign(a_cpu).op)\r\n```\r\n`a_cpu` is a variable in CPU device. `vars[i]` is a variable in GPU:0 device. The first one (Method-1) is to read the value in GPU:0 and then assign it to `a_cpu_to_gpu`. the second one (Method-2) is directly assign the variable in CPU to the variable in GPU:0.\r\n\r\nWhen I enable the `trace_level`, I find that the timeline of those two codes are different:\r\n\r\nMethod-1\r\n![image](https://user-images.githubusercontent.com/7370869/39508070-8e8204a4-4e13-11e8-97e3-f166403f0bef.png)\r\n\r\nMethod-2\r\n![image](https://user-images.githubusercontent.com/7370869/39507750-05f705d6-4e12-11e8-8aec-ff88db046e77.png)\r\n\r\nIn `Method-1`, I think the read_value op `a_cpu.read_value()` only depends on the i-th `update_op`. So the MEMCPYHtoD (green bar) should be right after the the i-th update_op, which means right after the MEMCPYDtoH (purple bar). I don't understand why they appear in the end of the timeline.\r\n\r\nIn `Method-2`, MEMCPYHtoD (green bar) appears at the beginning of the timeline. `vars[i].assign(a_cpu).op` depends on `update_op`, how can the memcpy be executed before the `update_op` ?\r\n\r\nI have no clue why it behaves like this, can anyone tell me why the memcpy behaves like that in `Method-1` and `Method-2` and what's the difference between them.\r\n\r\nThanks.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport os\r\n\r\nfrom tensorflow.python.client import timeline\r\nslim = tf.contrib.slim\r\n\r\ntrain_ops = []\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\r\n\r\nnet = tf.random_normal(shape=(32, 16, 16, 128))\r\n\r\nwith tf.device('/GPU:0'):\r\n\r\n  for i in range(10):\r\n    net = slim.conv2d(net, 128, [11, 11], padding='SAME', scope='conv_%s' % i)\r\n\r\n  loss = tf.reduce_mean(net, name='loss_func')\r\n  grad = tf.gradients(loss, tf.global_variables(), gate_gradients=True, name='my_gradients')\r\n\r\nvars = tf.global_variables()\r\nnum_vars = len(vars)\r\n\r\nfor i in range(num_vars - 1, -1, -1):\r\n\r\n  with tf.device('/CPU:0'):\r\n    a_cpu = tf.get_variable('a_cpu_%s' % i, initializer=vars[i].initial_value)\r\n    update_op = optimizer.apply_gradients([(grad[i], a_cpu)], name='apply_%s' % i)\r\n\r\n  with tf.control_dependencies([update_op]):\r\n    # Method-1 :\r\n    # with tf.device('/GPU:0'):\r\n    #   a_cpu_to_gpu = a_cpu.read_value()\r\n    # train_ops.append(vars[i].assign(a_cpu_to_gpu).op)\r\n\r\n    # Method-2 :\r\n    train_ops.append(vars[i].assign(a_cpu).op)\r\n\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  for i in range(10):\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    sess.run(train_ops, options=run_options, run_metadata=run_metadata)\r\n    trace_filename = os.path.join('/tmp/delete_me/trace', 'trace-global-step-%d.json' % i)\r\n    if not tf.gfile.Exists(os.path.dirname(trace_filename)):\r\n      os.makedirs(os.path.dirname(trace_filename))\r\n    trace = timeline.Timeline(step_stats=run_metadata.step_stats)\r\n    with tf.gfile.Open(trace_filename, 'w') as trace_file:\r\n      trace_file.write(trace.generate_chrome_trace_format(show_memory=False))\r\n```\r\n", "comments": ["I also tried another code which copy `a_cpu` from CPU to GPU:1 (not GPU:0)\r\ncode:\r\n```\r\nimport tensorflow as tf\r\nimport os\r\n\r\nfrom tensorflow.python.client import timeline\r\nslim = tf.contrib.slim\r\n\r\ntrain_ops = []\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\r\n\r\nnet = tf.random_normal(shape=(32, 16, 16, 128))\r\n\r\nwith tf.device('/GPU:0'):\r\n\r\n  for i in range(10):\r\n    net = slim.conv2d(net, 128, [11, 11], padding='SAME', scope='conv_%s' % i)\r\n\r\n  loss = tf.reduce_mean(net, name='loss_func')\r\n  grad = tf.gradients(loss, tf.global_variables(), gate_gradients=True, name='my_gradients')\r\n\r\nvars = tf.trainable_variables()\r\nnum_vars = len(vars)\r\n\r\nfor i in range(num_vars - 1, -1, -1):\r\n\r\n  with tf.device('/CPU:0'):\r\n    a_cpu = tf.get_variable('a_cpu_%s' % i, initializer=vars[i].initial_value)\r\n  update_op = optimizer.apply_gradients([(grad[i], a_cpu)], name='apply_%s' % i)\r\n\r\n  with tf.control_dependencies([update_op]):\r\n    # Method-1 :\r\n    # with tf.device('/GPU:0'):\r\n    #   a_cpu_to_gpu = a_cpu.read_value()\r\n    # train_op = vars[i].assign(a_cpu_to_gpu).op\r\n    # train_ops.append(train_op)\r\n\r\n    # Method-2 :\r\n    # train_ops.append(vars[i].assign(a_cpu).op)\r\n\r\n    # Method-3 :\r\n    with tf.device('/GPU:1'):\r\n      a_cpu_to_gpu = a_cpu.read_value()\r\n    train_ops.append(a_cpu_to_gpu.op)\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  for i in range(10):\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    sess.run(train_ops, options=run_options, run_metadata=run_metadata)\r\n    trace_filename = os.path.join('/tmp/delete_me/trace', 'trace-global-step-%d.json' % i)\r\n    if not tf.gfile.Exists(os.path.dirname(trace_filename)):\r\n      os.makedirs(os.path.dirname(trace_filename))\r\n    trace = timeline.Timeline(step_stats=run_metadata.step_stats)\r\n    with tf.gfile.Open(trace_filename, 'w') as trace_file:\r\n      trace_file.write(trace.generate_chrome_trace_format(show_memory=False))\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/7370869/39515871-ee9d8664-4e2d-11e8-9484-d3e6d6836760.png)\r\n\r\n\r\nIn this way, MEMCPYHtoD (green bar) overlaps with compute_gradients.\r\nBut in `Method-2`, MEMCPYHtoD (green bar) can not overlap with compute_gradients.\r\n\r\nIt seems that MEMCPYHtoD (green bar) cannot overlap with the backward computation. But it can somehow overlap with forward computation (blue bars) in `Method-1`.\r\n\r\nIf I change the `with tf.device('/GPU:1'):` to `with tf.device('/GPU:0'):`, the timeline is:\r\n\r\n![image](https://user-images.githubusercontent.com/7370869/39515848-d8956864-4e2d-11e8-9d5d-f39fc6148a74.png)\r\n", "Control dependencies only affect operations created within the control dependencies block. Because `a_cpu` was created outside of that block, it can evaluate before `update_op` has executed. `a_cpu_to_gpu` is created within the block, so it's guaranteed to run after update_op\r\n", "@yaroslavvb Thanks for reply. \r\nSo in Method-2, `a_cpu` evaluates before update_op has executed. Does that means old value of `a_cpu` (before updating the gradients to `a_cpu`) is assigned to `vars[i]` ? (That is not what I expected)\r\n\r\nAnd in Method-1, I can understand `a_cpu_to_gpu` should run after `update_op`, but what I can't understand is why all of the `a_cpu_to_gpu` run after the last `update_op`. What I expect is each `a_cpu_to_gpu` executes right after the corresponding `update_op` of `a_cpu` which means `MEMCPYDtoH` and `MEMCPYHtoD` shall execute alternately and sequentially.", "I have simplified the code to :\r\n```\r\nimport tensorflow as tf\r\nimport os\r\n\r\nfrom tensorflow.python.client import timeline\r\n\r\ntrain_ops = []\r\nendpoints = []\r\n\r\nwith tf.device('/GPU:0'):\r\n  a = tf.random_normal(shape=(2048, 2048), name='a')\r\n  b = tf.random_normal(shape=(2048, 2048), name='b')\r\n  for i in range(10):\r\n    a = tf.matmul(a, b, name='matmul_a_b_%s' % i)\r\n    endpoints.append(a)\r\n\r\nfor i in range(len(endpoints)):\r\n  with tf.device('/CPU:0'):\r\n    memcpy_d2h = tf.identity(endpoints[i])\r\n  with tf.device('/GPU:0'):\r\n    memcpy_h2d = tf.identity(memcpy_d2h)\r\n  train_ops.append(memcpy_h2d.op)\r\n\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  for i in range(10):\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    sess.run(train_ops, options=run_options, run_metadata=run_metadata)\r\n    trace_filename = os.path.join('/tmp/delete_me/trace', 'trace-%d.json' % i)\r\n    if not tf.gfile.Exists(os.path.dirname(trace_filename)):\r\n      os.makedirs(os.path.dirname(trace_filename))\r\n    trace = timeline.Timeline(step_stats=run_metadata.step_stats)\r\n    with tf.gfile.Open(trace_filename, 'w') as trace_file:\r\n      trace_file.write(trace.generate_chrome_trace_format(show_memory=False))\r\n```\r\n\r\nThis code outputs the same timeline as Method-1 (all MEMCPYHtoD ops execute after the last MEMCPYDtoH)\r\nand also when I change the device of from GPU:0 to GPU:1\r\n```\r\n  with tf.device('/GPU:1'):\r\n    memcpy_h2d = tf.identity(memcpy_d2h)\r\n```\r\nIt outputs the same timeline as Method-3 (MEMCPYDtoH and MEMCPYHtoD execute alternately and sequentially.)\r\n\r\nIn addition, I also reverse the device GPU and CPU like:\r\n```\r\nwith tf.device('/GPU:0'):\r\n  a = tf.random_normal(shape=(2048, 2048), name='a')\r\n  b = tf.random_normal(shape=(2048, 2048), name='b')\r\n  for i in range(10):\r\n    a = tf.matmul(a, b, name='matmul_a_b_%s' % i)\r\n    endpoints.append(a)\r\n\r\nfor i in range(len(endpoints)):\r\n  with tf.device('/CPU:0'):\r\n    memcpy_d2h = tf.identity(endpoints[i])\r\n  with tf.device('/GPU:0'):\r\n    memcpy_h2d = tf.identity(memcpy_d2h)\r\n  train_ops.append(memcpy_h2d.op)\r\n```\r\nI have timeline:\r\n![image](https://user-images.githubusercontent.com/7370869/39563617-8fe18ff6-4ee3-11e8-9380-8c7f9049cdfc.png)\r\n\r\n\r\nIn this case, MEMCPYDtoH and MEMCPYHtoD execute alternately and sequentially.\r\nWhat is reason of these differences?", "Well, I made some more experiments. If I change the size of `a` and `b`, I find that `Matmul` can somehow overlap with `MEMCPYH2D`\r\nWhen `a` and `b` are 512 x 512\r\n```\r\nimport tensorflow as tf\r\nimport os\r\n\r\nfrom tensorflow.python.client import timeline\r\n\r\ntrain_ops = []\r\nendpoints = []\r\n\r\nwith tf.device('/GPU:0'):\r\n  a = tf.random_normal(shape=(512, 512), name='a')\r\n  b = tf.random_normal(shape=(512, 512), name='b')\r\n  for i in range(10):\r\n    a = tf.matmul(a, b, name='matmul_a_b_%s' % i)\r\n    endpoints.append(a)\r\n\r\nfor i in range(len(endpoints)):\r\n  with tf.device('/CPU:0'):\r\n    memcpy_d2h = tf.identity(endpoints[i])\r\n  with tf.device('/GPU:0'):\r\n    memcpy_h2d = tf.identity(memcpy_d2h)\r\n  train_ops.append(memcpy_h2d.op)\r\n\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  for i in range(10):\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    sess.run(train_ops, options=run_options, run_metadata=run_metadata)\r\n    trace_filename = os.path.join('/tmp/delete_me/trace', 'trace-%d.json' % i)\r\n    if not tf.gfile.Exists(os.path.dirname(trace_filename)):\r\n      os.makedirs(os.path.dirname(trace_filename))\r\n    trace = timeline.Timeline(step_stats=run_metadata.step_stats)\r\n    with tf.gfile.Open(trace_filename, 'w') as trace_file:\r\n      trace_file.write(trace.generate_chrome_trace_format(show_memory=False))\r\n```\r\n\r\nTimeline is:\r\n![image](https://user-images.githubusercontent.com/7370869/39565097-54d615ca-4ee9-11e8-9066-b45e4cc7ffcf.png)\r\n\r\nWhen When `a` and `b` are 256 x 256:\r\n```\r\n  a = tf.random_normal(shape=(256, 256), name='a')\r\n  b = tf.random_normal(shape=(256, 256), name='b')\r\n```\r\nTimeline is:\r\n![image](https://user-images.githubusercontent.com/7370869/39565157-8817f1a6-4ee9-11e8-9cf2-021d5edcdf65.png)\r\n\r\n", "I notice that there are new environment variables `TF_GPU_THREAD_MODE` and `TF_GPU_THREAD_COUNT` in latest TensorFlow\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/core/common_runtime/gpu/gpu_device.cc#L332\r\nIs there anything to do with that ? (I am using TensorFlow-1.4)", "Anyone can help me :) @angersson @yaroslavvb ", "It's not clear what you are seeing is a bug, can you isolate the problem further?", "@yaroslavvb Sorry for not being clear. My question is in Method-1, why MEMCPYH2D can not overlap with the backward computation while MEMCPYD2H can.\r\n![image](https://user-images.githubusercontent.com/7370869/39508070-8e8204a4-4e13-11e8-97e3-f166403f0bef.png)", "(I marked @yaroslavvb as assignee since he's been looking at this already. Thanks!)", "Still waiting for reply :-)", "Still waiting for reply :-)", "@yaroslavvb @angersson anyone help? :)", "Sorry, I don't have context and I see a lot of discussion in this bug. Can you restate what is the actual issue that is still unresolved?\r\n\r\nAlso, I see variables and control dependencies; does enabling resource variables (tf.enable_resource_variables() on nightly) make the issue go away?", "@sleepfin can you update the bug on what is the current issue? Did you check whether resource variables make this issue go away?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to low activity"]}, {"number": 19016, "title": "No OpKernel was registered to support Op 'CudnnRNNParamsToCanonical' with these attrs.  Registered devices: [CPU], Registered kernels:   <no registered kernels>", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.1.0/7.0\r\n- **GPU model and memory**:GTX 1080, 11GB\r\n- **Exact command to reproduce**:\r\nCode used to export the model on CPU/GPU:\r\n```\r\nwith tf.Graph().as_default() as graph:\r\n        inputs, outputs = create_graph()\r\n\r\n\r\n        # Create a saver using variables from the above newly created graph\r\n        saver = tf.train.Saver(tf.global_variables())\r\n\r\n        with tf.Session() as sess:\r\n            # Restore the model from last checkpoints\r\n            ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\r\n            saver.restore(sess, ckpt.model_checkpoint_path)\r\n\r\n            # (re-)create export directory\r\n            export_path = os.path.join(\r\n                tf.compat.as_bytes(FLAGS.export_dir),\r\n                tf.compat.as_bytes(str(FLAGS.export_version)))\r\n            if os.path.exists(export_path):\r\n                shutil.rmtree(export_path)\r\n\r\n            # create model builder\r\n            builder = tf.saved_model.builder.SavedModelBuilder(export_path)\r\n\r\n            input_node = graph.get_tensor_by_name('input_node:0')\r\n            input_lengths = graph.get_tensor_by_name('input_lengths:0')\r\n            outputs = graph.get_tensor_by_name('output_node:0')\r\n\r\n            # create tensors info\r\n            predict_tensor_inputs_info = tf.saved_model.utils.build_tensor_info(input_node)\r\n            predict_tensor_inputs_length_info = tf.saved_model.utils.build_tensor_info(input_lengths)\r\n            predict_tensor_scores_info = tf.saved_model.utils.build_tensor_info(outputs)\r\n\r\n            # build prediction signature\r\n            prediction_signature = (\r\n                tf.saved_model.signature_def_utils.build_signature_def(\r\n                    inputs={'input': predict_tensor_inputs_info,'input_len':predict_tensor_inputs_length_info},\r\n                    outputs={'output': predict_tensor_scores_info},\r\n                    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\r\n                )\r\n            )\r\n\r\n            # save the model\r\n            builder.add_meta_graph_and_variables(\r\n                sess, [tf.saved_model.tag_constants.SERVING],\r\n                signature_def_map={\r\n                    'infer': prediction_signature\r\n                })\r\n\r\n            builder.save()\r\n```\r\n\r\n### Describe the problem\r\nTrained one model on GPU using CudnnLSTM (tf.contrib.cudnn_rnn.CudnnLSTM). When trying to export using saved_model API on CPU getting the below error. Export works on GPU and it works for predictions also only on GPU. We want to use the model for prediction on CPU also. So, how can we export the trained model on GPU so that it can be used on CPU ?\r\n\r\n### Error log:\r\n\r\n```\r\nFile \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 501, in _create_saveable\r\n    name=\"%s_saveable\" % self.trainable_variables[0].name.split(\":\")[0])\r\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 262, in __init__\r\n    weights, biases = self._OpaqueParamsToCanonical()\r\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 315, in _OpaqueParamsToCanonical\r\n    direction=self._direction)\r\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/ops/gen_cudnn_rnn_ops.py\", line 769, in cudnn_rnn_params_to_canonical\r\n    name=name)\r\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\r\n    op_def=op_def)\r\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'CudnnRNNParamsToCanonical' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: CudnnRNNParamsToCanonical = CudnnRNNParamsToCanonical[T=DT_FLOAT, direction=\"bidirectional\", dropout=0, input_mode=\"linear_input\", num_params=16, rnn_mode=\"lstm\", seed=0, seed2=0, _device=\"/device:GPU:0\"](CudnnRNNParamsToCanonical/num_layers, CudnnRNNParamsToCanonical/num_units, CudnnRNNParamsToCanonical/input_size, cudnn_lstm/opaque_kernel/read)]]\r\n```", "comments": ["Note that `CudnnLSTM` is a GPU-only operation (it depends on CuDNN) - so the error makes sense since the graph was constructed in a non-portable way.\r\n\r\nThat said, I agree that we could do better with the user experience here so that your model is more portable.\r\n\r\n@drpngx @protoget I believe you folks are looking into this, so could you comment on the best way forward for now?", "Hi praveeny1986@ the best way for now is to use ```CudnnCompatibleLSTM``` for inference. See example here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py#L104", "@protoget , @asimshankar ,\r\nWe trained the model using CudnnLSTM and then for inferencing on CPU we tried the CudnnCompatibleLSTMCell (didnt find CudnnCompatibleLSTM as you mentioned in the API but instead found CudnnCompatibleLSTMCell and hence used it) as mentioned in example.\r\n\r\n**Training code on GPU:**\r\n```\r\nlstm = tf.contrib.cudnn_rnn.CudnnLSTM(num_layers=1,\r\n                                         num_units=n_cell_dim,\r\n                                         direction='bidirectional',\r\n                                         seed=FLAGS.random_seed,\r\n                                         dtype=tf.float32)\r\nlstm.build(inputs.get_shape())\r\noutputs, output_states = lstm(inputs, training=is_training)\r\n```\r\n**Inferencing code on CPU:**\r\n```\r\n   single_cell = lambda: tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(n_cell_dim, \r\n   reuse=tf.get_variable_scope().reuse)\r\n\r\n   lstm_fw_cell = [single_cell() for _ in range(1)]\r\n   lstm_bw_cell = [single_cell() for _ in range(1)]\r\n   (outputs, output_state_fw,\r\n    output_state_bw) = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(lstm_fw_cell,\r\n                                                            lstm_bw_cell,\r\n                                                            inputs,\r\n                                                            dtype=tf.float32,\r\n                                                            time_major=True,\r\n                                                            sequence_length=seq_length)\r\n\r\n```\r\n**But we are getting the following error:**\r\n```\r\nKey stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/cudnn_compatible_lstm_cell/bias not found in checkpoint\r\n[[Node: save_1/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_1/tensor_names, save_1/RestoreV2_1/shape_and_slices)]]\r\n```\r\nHow can we make it work?", "@praveeny1986 you might be running into some scoping issues when you build your model.\r\n\r\nThis is a working example for TF 1.6.0.\r\nhttps://gist.github.com/protoget/9b45881f23c96e201a90581c8f4b692d\r\n\r\nIt's no different than your example above, only slightly modified to make it a self-contained runnable script.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@protoget Your code does not include an optimizer so you can restore. But if I use any optimizer at the training stage, I still fail in restoring.", "Hi @praveeny1986 @seanliu96 , I have met the same problem when tried to  use the CudnnLSTM trained on GPU and then for inferencing on CPU, have you solved it?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "replying on behalf of @praveeny1986 , this is no more an issue for us. We were able to make it work by using CudnnLSTM", "Hi @drpngx @praveeny1986 @seanliu96 @cocoakeith @seanliu96, we have met the exact same problem. It persists from tensorflow 1.6 to 1.11.0rc1. We can train on GPU through the @praveeny1986 @protoget workaround [https://gist.github.com/protoget/9b45881f23c96e201a90581c8f4b692d](https://gist.github.com/protoget/9b45881f23c96e201a90581c8f4b692d). However, the inference code for CPU does not work for us:\r\n\r\n` No OpKernel was registered to support Op 'LSTMBlockCell' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\t [[Node: OCR/Rnn/bdrnn/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/cudnn_compatible_lstm_cell/LSTMBlockCell = LSTMBlockCell[T=DT_FLOAT, cell_clip=-1, forget_bias=0, use_peephole=false](OCR/Rnn/bdrnn/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3, OCR/Rnn/bdrnn/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Identity_3, OCR/Rnn/bdrnn/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Identity_4, OCR/Rnn/bdrnn/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/cudnn_compatible_lstm_cell/LSTMBlockCell/Enter, OCR/Rnn/bdrnn/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/cudnn_compatible_lstm_cell/zeros, OCR/Rnn/bdrnn/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/cudnn_compatible_lstm_cell/zeros, OCR/Rnn/bdrnn/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/cudnn_compatible_lstm_cell/zeros, OCR/Rnn/bdrnn/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/cudnn_compatible_lstm_cell/LSTMBlockCell/Enter_1)]]`\r\n\r\n------\r\nUPDATED:\r\n\r\nIt was actually another situation, a known issue. Solved by importing the contrib.rnn modules:\r\n\r\n`from tensorflow.contrib.rnn import *\r\n`", "Hi,\r\nI have got the same problem with my model fit on GPU and predictions on CPU. \r\nTo make it work I had to:\r\n- fit my model with LSTM instead of CudNNLSTM and GRU instead of CudNNGRU\r\n- remove the imports of CudNNLSTM or CudNNGRU from Keras layers in my prediction kernel\r\n- restart my kernel \r\n\r\nIt worked after these steps. ", "> Hi,\r\n> I have got the same problem with my model fit on GPU and predictions on CPU.\r\n> To make it work I had to:\r\n> \r\n> * fit my model with LSTM instead of CudNNLSTM and GRU instead of CudNNGRU\r\n> * remove the imports of CudNNLSTM or CudNNGRU from Keras layers in my prediction kernel\r\n> * restart my kernel\r\n> \r\n> It worked after these steps.\r\n\r\nswitching to normal LSTM/GRU is not really a solution...", "> Hi,\r\n> I have got the same problem with my model fit on GPU and predictions on CPU.\r\n> To make it work I had to:\r\n> \r\n> * fit my model with LSTM instead of CudNNLSTM and GRU instead of CudNNGRU\r\n> * remove the imports of CudNNLSTM or CudNNGRU from Keras layers in my prediction kernel\r\n> * restart my kernel\r\n> \r\n> It worked after these steps.\r\n\r\nUsing CudNNLSTM is way faster. Like, 15x faster tbh. ", "I don't think that there is any other way. I did the tuning with the CUD version and then switched to normal LSTM to export my model. \r\n"]}, {"number": 19015, "title": "Building from source with Python3.6 raises `java.lang.RuntimeException`", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**:  master branch, commit d0f5bc1\r\n- **Python version**: Python 3.6\r\n- **Bazel version (if compiling from source)**:\r\n\r\nBuild label: 0.13.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Mon Oct 18 21:33:40 +50297 (1525078013620)\r\nBuild timestamp: 1525078013620\r\nBuild timestamp as int: 1525078013620\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\nCUDA 9.0 / cuDNN 7.0 \r\n\r\n- **GPU model and memory**:\r\n\r\nGeForce GTX TITAN X 12 Gb\r\n\r\n- **Exact command to reproduce**:\r\n\r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nIn `./configure` I use the default settings for everything except I use `/usr/bin/python3.6` and CUDA.\r\n\r\n```\r\n~/tensorflow master* \u21e3 ethanbro@rldl2\r\n\u276f bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nThis raises the following error:\r\n```\r\nStarting local Bazel server and connecting to it...\r\n..............\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: /home/ethanbro/.cache/bazel/_bazel_ethanbro/5e4124942ab73951559485c7924c6c9e/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/ethanbro/.cache/bazel/_bazel_ethanbro/5e4124942ab73951559485c7924c6c9e/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nUnhandled exception thrown during build; message: Unrecoverable error while evaluating node 'PACKAGE:tensorflow/core/kernels' (requested by nodes '//tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (673307310)', '//tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@6511864 false (1063568825)', '//tensorflow/core/grappler/clusters:single_machine com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (1846558957)', '//tensorflow/core/grappler/costs:measuring_cost_estimator com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (1957631410)')\r\nINFO: Elapsed time: 3.346s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (130 packages loaded)\r\n    currently loading: tensorflow/core ... (2 packages)\r\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'PACKAGE:tensorflow/core/kernels' (requested by nodes '//tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (673307310)', '//tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@6511864 false (1063568825)', '//tensorflow/core/grappler/clusters:single_machine com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (1846558957)', '//tensorflow/core/grappler/costs:measuring_cost_estimator com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (1957631410)')\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:424)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:355)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: Invalid EvalException:\r\njava.lang.InterruptedException\r\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:485)\r\n\tat com.google.common.util.concurrent.AbstractFuture$TrustedFuture.get(AbstractFuture.java:85)\r\n\tat com.google.common.util.concurrent.ForwardingFuture.get(ForwardingFuture.java:62)\r\n\tat com.google.devtools.build.lib.vfs.UnixGlob$GlobFuture.get(UnixGlob.java:444)\r\n\tat com.google.devtools.build.lib.vfs.UnixGlob$GlobFuture.get(UnixGlob.java:432)\r\n\tat com.google.devtools.build.lib.packages.GlobCache.fromFuture(GlobCache.java:219)\r\n\tat com.google.devtools.build.lib.packages.GlobCache.getGlobUnsorted(GlobCache.java:161)\r\n\tat com.google.devtools.build.lib.packages.GlobCache.globUnsorted(GlobCache.java:248)\r\n\tat com.google.devtools.build.lib.packages.PackageFactory$LegacyGlobber.fetch(PackageFactory.java:305)\r\n\tat com.google.devtools.build.lib.skyframe.PackageFunction$SkyframeHybridGlobber$HybridToken.resolve(PackageFunction.java:1080)\r\n\tat com.google.devtools.build.lib.skyframe.PackageFunction$SkyframeHybridGlobber$HybridToken.access$600(PackageFunction.java:1046)\r\n\tat com.google.devtools.build.lib.skyframe.PackageFunction$SkyframeHybridGlobber.fetch(PackageFunction.java:1003)\r\n\tat com.google.devtools.build.lib.packages.PackageFactory.callGlob(PackageFactory.java:569)\r\n\tat com.google.devtools.build.lib.packages.SkylarkNativeModule.glob(SkylarkNativeModule.java:92)\r\n\tat sun.reflect.GeneratedMethodAccessor99.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:360)\r\n\tat com.google.devtools.build.lib.syntax.BuiltinCallable.call(BuiltinCallable.java:130)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:719)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:839)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:823)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)\r\n\tat com.google.devtools.build.lib.syntax.BinaryOperatorExpression.doEval(BinaryOperatorExpression.java:251)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execAssignment(Eval.java:50)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:171)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:210)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execIfBranch(Eval.java:62)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:177)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execIf(Eval.java:119)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:193)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:854)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:826)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:180)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.BuildFileAST.execTopLevelStatement(BuildFileAST.java:225)\r\n\tat com.google.devtools.build.lib.syntax.BuildFileAST.exec(BuildFileAST.java:198)\r\n\tat com.google.devtools.build.lib.packages.PackageFactory.evaluateBuildFile(PackageFactory.java:1659)\r\n\tat com.google.devtools.build.lib.packages.PackageFactory.createPackageFromAst(PackageFactory.java:1298)\r\n\tat com.google.devtools.build.lib.skyframe.PackageFunction.loadPackage(PackageFunction.java:1234)\r\n\tat com.google.devtools.build.lib.skyframe.PackageFunction.compute(PackageFunction.java:476)\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:347)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:355)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n\tat com.google.devtools.build.lib.syntax.EvalException.<init>(EvalException.java:112)\r\n\tat com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:209)\r\n\tat com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:217)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:393)\r\n\tat com.google.devtools.build.lib.syntax.BuiltinCallable.call(BuiltinCallable.java:130)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:719)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:839)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:823)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)\r\n\tat com.google.devtools.build.lib.syntax.BinaryOperatorExpression.doEval(BinaryOperatorExpression.java:251)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execAssignment(Eval.java:50)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:171)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:210)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execIfBranch(Eval.java:62)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:177)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execIf(Eval.java:119)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:193)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:854)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:826)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:180)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.BuildFileAST.execTopLevelStatement(BuildFileAST.java:225)\r\n\tat com.google.devtools.build.lib.syntax.BuildFileAST.exec(BuildFileAST.java:198)\r\n\tat com.google.devtools.build.lib.packages.PackageFactory.evaluateBuildFile(PackageFactory.java:1659)\r\n\tat com.google.devtools.build.lib.packages.PackageFactory.createPackageFromAst(PackageFactory.java:1298)\r\n\tat com.google.devtools.build.lib.skyframe.PackageFunction.loadPackage(PackageFunction.java:1234)\r\n\tat com.google.devtools.build.lib.skyframe.PackageFunction.compute(PackageFunction.java:476)\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:347)\r\n\t... 4 more\r\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'PACKAGE:tensorflow/core/kernels' (requested by nodes '//tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (673307310)', '//tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@6511864 false (1063568825)', '//tensorflow/core/grappler/clusters:single_machine com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (1846558957)', '//tensorflow/core/grappler/costs:measuring_cost_estimator com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@216e49db true (1957631410)')\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:424)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:355)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: Invalid EvalException:\r\njava.lang.InterruptedException\r\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:485)\r\n\tat com.google.common.util.concurrent.AbstractFuture$TrustedFuture.get(AbstractFuture.java:85)\r\n\tat com.google.common.util.concurrent.ForwardingFuture.get(ForwardingFuture.java:62)\r\n\tat com.google.devtools.build.lib.vfs.UnixGlob$GlobFuture.get(UnixGlob.java:444)\r\n\tat com.google.devtools.build.lib.vfs.UnixGlob$GlobFuture.get(UnixGlob.java:432)\r\n\tat com.google.devtools.build.lib.packages.GlobCache.fromFuture(GlobCache.java:219)\r\n\tat com.google.devtools.build.lib.packages.GlobCache.getGlobUnsorted(GlobCache.java:161)\r\n\tat com.google.devtools.build.lib.packages.GlobCache.globUnsorted(GlobCache.java:248)\r\n\tat com.google.devtools.build.lib.packages.PackageFactory$LegacyGlobber.fetch(PackageFactory.java:305)\r\n\tat com.google.devtools.build.lib.skyframe.PackageFunction$SkyframeHybridGlobber$HybridToken.resolve(PackageFunction.java:1080)\r\n\tat com.google.devtools.build.lib.skyframe.PackageFunction$SkyframeHybridGlobber$HybridToken.access$600(PackageFunction.java:1046)\r\n\tat com.google.devtools.build.lib.skyframe.PackageFunction$SkyframeHybridGlobber.fetch(PackageFunction.java:1003)\r\n\tat com.google.devtools.build.lib.packages.PackageFactory.callGlob(PackageFactory.java:569)\r\n\tat com.google.devtools.build.lib.packages.SkylarkNativeModule.glob(SkylarkNativeModule.java:92)\r\n\tat sun.reflect.GeneratedMethodAccessor99.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:360)\r\n\tat com.google.devtools.build.lib.syntax.BuiltinCallable.call(BuiltinCallable.java:130)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:719)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:839)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:823)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)\r\n\tat com.google.devtools.build.lib.syntax.BinaryOperatorExpression.doEval(BinaryOperatorExpression.java:251)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execAssignment(Eval.java:50)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:171)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:210)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execIfBranch(Eval.java:62)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:177)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execIf(Eval.java:119)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:193)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:854)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:826)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:180)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.BuildFileAST.execTopLevelStatement(BuildFileAST.java:225)\r\n\tat com.google.devtools.build.lib.syntax.BuildFileAST.exec(BuildFileAST.java:198)\r\n\tat com.google.devtools.build.lib.packages.PackageFactory.evaluateBuildFile(PackageFactory.java:1659)\r\n\tat com.google.devtools.build.lib.packages.PackageFactory.createPackageFromAst(PackageFactory.java:1298)\r\n\tat com.google.devtools.build.lib.skyframe.PackageFunction.loadPackage(PackageFunction.java:1234)\r\n\tat com.google.devtools.build.lib.skyframe.PackageFunction.compute(PackageFunction.java:476)\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:347)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:355)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n\tat com.google.devtools.build.lib.syntax.EvalException.<init>(EvalException.java:112)\r\n\tat com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:209)\r\n\tat com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:217)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:393)\r\n\tat com.google.devtools.build.lib.syntax.BuiltinCallable.call(BuiltinCallable.java:130)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:719)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:839)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:823)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)\r\n\tat com.google.devtools.build.lib.syntax.BinaryOperatorExpression.doEval(BinaryOperatorExpression.java:251)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execAssignment(Eval.java:50)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:171)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:210)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execIfBranch(Eval.java:62)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:177)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execIf(Eval.java:119)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:193)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:462)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:440)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:854)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:826)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:180)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:162)\r\n\tat com.google.devtools.build.lib.syntax.BuildFileAST.execTopLevelStatement(BuildFileAST.java:225)\r\n\tat com.google.devtools.build.lib.syntax.BuildFileAST.exec(BuildFileAST.java:198)\r\n\tat com.google.devtools.build.lib.packages.PackageFactory.evaluateBuildFile(PackageFactory.java:1659)\r\n\tat com.google.devtools.build.lib.packages.PackageFactory.createPackageFromAst(PackageFactory.java:1298)\r\n\tat com.google.devtools.build.lib.skyframe.PackageFunction.loadPackage(PackageFunction.java:1234)\r\n\tat com.google.devtools.build.lib.skyframe.PackageFunction.compute(PackageFunction.java:476)\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:347)\r\n\t... 4 more\r\n```\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nHere is the .tf_configure.bazelrc:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3.6\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python3.6/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python3.6\"\r\nbuild --define with_jemalloc=true\r\nbuild:gcp --define with_gcp_support=true\r\nbuild:hdfs --define with_hdfs_support=true\r\nbuild --define with_s3_support=true\r\nbuild --define with_kafka_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild:gdr --define with_gdr_support=true\r\nbuild:verbs --define with_verbs_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\nbuild --action_env TF_CUDA_VERSION=\"9.0\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr/lib/x86_64-linux-gnu\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_NCCL_VERSION=\"1\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/home/ethanbro/.mujoco/mjpro150/bin:/usr/local/cuda-9.0/lib64:\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild --define grpc_no_ares=true\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\nbuild --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\n```\r\n\r\nHere is the content of tf_env.txt:\r\n```\r\n\r\n== cat /etc/issue ===============================================\r\nLinux rldl2 4.4.0-122-generic #146-Ubuntu SMP Mon Apr 23 15:34:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux rldl2 4.4.0-122-generic #146-Ubuntu SMP Mon Apr 23 15:34:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                              1.13.3                \r\nprotobuf                           3.4.0                 \r\ntensorflow-tensorboard             0.1.8                 \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\nImportError: No module named platform\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /home/ethanbro/.mujoco/mjpro150/bin:/usr/local/cuda-9.0/lib64:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue May  1 23:26:41 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX TIT...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n| 22%   22C    P8    14W / 250W |     12MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX TIT...  Off  | 00000000:03:00.0 Off |                  N/A |\r\n| 22%   23C    P8    15W / 250W |     11MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX TIT...  Off  | 00000000:82:00.0 Off |                  N/A |\r\n| 22%   22C    P8    15W / 250W |     11MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX TIT...  Off  | 00000000:83:00.0 Off |                  N/A |\r\n| 22%   22C    P8    15W / 250W |     11MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1585      G   /usr/lib/xorg/Xorg                            11MiB |\r\n|    1      1585      G   /usr/lib/xorg/Xorg                            10MiB |\r\n|    2      1585      G   /usr/lib/xorg/Xorg                            10MiB |\r\n|    3      1585      G   /usr/lib/xorg/Xorg                            10MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176\r\n/usr/local/MATLAB/R2015a/bin/glnxa64/libcudart.so.6.5.14\r\n```", "comments": ["Ah I just got the exact same error. There seems to be a problem with bazel 0.13. Switching to bazel 0.12 solved it for me.", "Yes this solved it. Thanks for the advice. ", "It would be nice to open an issue in bazel repo.", "Bazel 0.12 worked for me. Do not install 0.13!"]}]