[{"number": 49741, "title": "Handle one more division by 0 in TFLite.", "body": "PiperOrigin-RevId: 370800140\nChange-Id: I9ab42e5aaccf02f226d1282611490a54cf7d273e", "comments": []}, {"number": 49739, "title": "Add docs about forcing zero output for mask with Bidirectional", "body": "#49738 \r\nAdd description that the output of mask timestep is zero when `returns_sequences` of layer is true with Bidirectional.", "comments": []}, {"number": 49738, "title": "No description about forcing zero output for mask in Bidirectional", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nlstm = tf.keras.layers.LSTM(4, return_sequences=True)\r\nbi = tf.keras.layers.Bidirectional(lstm)\r\n\r\nx = tf.random.normal([1,4,16])\r\nmask = tf.constant([[True, True, True, False]])\r\n\r\nprint(lstm(x, mask=mask))\r\nprint(bi(x, mask=mask))\r\n```\r\n\r\n```\r\ntf.Tensor(\r\n[[[ 0.01245601  0.38689056  0.01844893 -0.0718843 ]\r\n  [ 0.14610071  0.23905458  0.39626616 -0.17714866]\r\n  [-0.00543382 -0.06880241  0.04203304 -0.08996341]\r\n  [-0.00543382 -0.06880241  0.04203304 -0.08996341]]], shape=(1, 4, 4), dtype=float32)\r\ntf.Tensor(\r\n[[[-0.24271122  0.05120631 -0.06832076 -0.5101022   0.3812662\r\n    0.44380718 -0.07919203  0.07195219]\r\n  [-0.16884208  0.18173794 -0.0029141  -0.13847476  0.08567893\r\n    0.2971174   0.15979137  0.01258926]\r\n  [-0.11614764  0.17977336 -0.20486659 -0.0677676   0.40112934\r\n    0.27802816 -0.20526664  0.1625048 ]\r\n  [ 0.          0.          0.          0.          0.\r\n    0.          0.          0.        ]]], shape=(1, 4, 8), dtype=float32)\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/python/keras/layers/wrappers.py#L492-L498\r\nWhen we use `tf.keras.layers.Bidirectional` with `return_sequences=True`, The output for masked sequences become zero because **force_zero_output_for_mask** internally. However, there is no description that the masked timestep should be zero with `return_sequences=True` in Bidirectional layer documentation.\r\nWithout `Bidirectional`, `zero_output_for_mask` is false by default. Many of people hardly expect the output of masked timestep is zero. So I think this is very confused and easy to misunderstand.\r\n\r\nI think the fact that it forces zero oputput for mask internally should be added to the Bidirectional document.\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n\r\nYes I may be able to add document about this.", "comments": ["Is it ok to close issue because the PR merged?", "@cosmoquester,\r\nYes please. Thank you for your contribution. "]}, {"number": 49736, "title": "[MLIR][DISC] add ConvertConvDynamic and ConvertGatherV2OpDynamic", "body": "We are porting our MLIR-based dynamic shape compiler to tf community (From OP def, Patttern, to Optimization pass, etc).\r\nThis is the second PR about tf2mhlo pattern conversion, which including ConvertConvDynamic and ConvertGatherV2OpDynamic.\r\nThe rest pattern conversions we will add:\r\n- ConvertRangeOpxxx\r\n- ConvertSigmoidOpxxx\r\n- ConvertSliceOpxxx\r\n- ConvertSplitOpxxx\r\n- ConvertSqueezeOpxxx\r\n- ConvertStridedSliceOpxxx\r\n- ConvertTileOpxxx\r\n- ConvertTopKV2Opxxx\r\n- ConvertUnpackOpxxx\r\n- ConvertPrintOp\r\n- ConvertSigmoidGradOpxxx\r\n- ConvertSignOpxxx\r\n", "comments": ["Seems like there isn't much left in this PR?", "> Seems like there isn't much left in this PR?\r\n\r\nNevermind, I mis-read what GitHub was showing me...", "Can you rebase this? Seems like there is a conflict now.", "> Can you rebase this? Seems like there is a conflict now.\r\n\r\ndone"]}, {"number": 49735, "title": "[TFLite] Add int8 and int16x8 support for the REDUCE_PROD operator", "body": "Hello,\r\n\r\nThis PR adds int8 and int16x8 support for the REDUCE_PROD operator.\r\n\r\nIdeally the kernel should do the product of all the quantized elements in the reduced axes and then do a final rescale:\r\n![equation2](https://user-images.githubusercontent.com/21028116/119684315-51864000-be3c-11eb-9914-fcc27d410e6c.png)\r\n\r\nThis would give the best accuracy but unfortunately it would quickly overflow the accumulator. To overcome the issue the product is rescaled at each step in a way similar to\r\n![equation](https://user-images.githubusercontent.com/21028116/119684201-39aebc00-be3c-11eb-8f1c-e7c15fab3143.png)\r\nwith a slight modification that the rescaling is done after each `in_i*in_i+1` multiplication plus a final rescaling to avoid losing too much precision by rescaling each `in_i` individually.\r\n\r\nNote that it can degenerate to a precision of 0 if an intermediate results is too close to 0 and becomes a quantized 0. Example 1.5 * 0.5 * 0.1 * 0.01 * 5.3 * 8.3 * 2.3 * 9.6 * 10.2 = 7.43043888 but the intermediate 1.5 * 0.5 * 0.1 * 0.01 = 0.00075 could become a quantized 0 and the whole quantized result will be 0 instead of something close to 7.4. It's unfortunately difficult to avoid it without a dynamic rescaling (and thus dynamic rescaling factor) that would rescale the accumulator once it is too close to its minimum/maximum value. \r\n\r\nIt could also happen if the 0.01 input in the example end-up to be quantized as a 0 (which would happen with the [0;10.2] range of the example using 8-bit as the quantized step would be 10.2 / 255 = 0.04). \r\n\r\nThese cases should hopefully be infrequent.\r\n\r\n\r\nThibaut", "comments": ["Removing myself as reviewer since this is a PR changing tflite and I'm focused of TFLM.", "@abattery  Can you please review this PR ? Thanks!", "Thank you for the review, I addressed the comments and updated the PR.", "@jianlijianli I updated the PR to adapt it to some changes from the master branch. The PR will need a re-approval, thanks!", "Closing as it seems it has been merged."]}, {"number": 49734, "title": "can't import plot_model from keras.utils", "body": "When I run this line of code on google colab (with or without the GPU),\r\n\r\n```from keras.utils import plot_model```\r\n\r\nI get the following error:\r\n\r\n\"cannot import name 'plot_model' from 'keras.utils' (/usr/local/lib/python3.7/dist-packages/keras/utils/__init__.py)\"\r\n\r\nWhen I went to bed last night the code worked. This morning it doesn't. What happened and what can I do? Thanks in advance.\r\n\r\n", "comments": ["Try with\r\n\r\n`from tensorflow.keras.utils import plot_model`\r\n\r\n\r\nbut there is a bug in keras that should be fixed.\r\n", "Also see the guides for usage examples https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model\r\n```python\r\nimport tensorflow as tf\r\ntf.keras.utils.plot_model(model, \"my_first_model.png\")\r\n```\r\nAdditionally, The colab environment variables are lost if the notebook is idle for sometime (not sure exact amount of time). So you my want to rerun whole notebook from top most cell and try again.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49734\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49734\">No</a>\n"]}, {"number": 49732, "title": "Can't convert saved model to TFLite format", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- TensorFlow installation (pip package or built from source): pip \r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0\r\n\r\n### 2. Code\r\n\r\nI am trying to convert my saved model to TFlite format\r\n```\r\n\r\nloaded_model = tf.keras.models.load_model(model_path)\r\n# loaded_model.summary()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\r\nlitemodel = converter.convert()\r\nwith open('model.tflite', 'wb') as f:\r\n    f.write(litemodel)\r\n```\r\n\r\nOutput error:\r\n```\r\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\r\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/core.py:1061: UserWarning: mrcnn.fpn is not loaded, but a Lambda layer uses it. It may cause errors.\r\n  , UserWarning)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/core.py:1061: UserWarning: mrcnn.inference is not loaded, but a Lambda layer uses it. It may cause errors.\r\n  , UserWarning)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/core.py:1061: UserWarning: mrcnn.rpn is not loaded, but a Lambda layer uses it. It may cause errors.\r\n  , UserWarning)\r\nWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\r\nWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\r\n  category=CustomMaskWarning)\r\nINFO:tensorflow:Assets written to: /tmp/tmpw8wqnnco/assets\r\nINFO:tensorflow:Assets written to: /tmp/tmpw8wqnnco/assets\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    293                                                  debug_info_str,\r\n--> 294                                                  enable_mlir_converter)\r\n    295       return model_str\r\n\r\n5 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n     37       debug_info_str,\r\n---> 38       enable_mlir_converter)\r\n     39 \r\n\r\nException: /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/saved_model/load.py:152:0: error: 'tf.TensorListReserve' op requires element_shape to be 1D tensor during TF Lite transformation pass\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/save.py:206:0: note: called from\r\n<ipython-input-12-cead14d55f93>:2:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py:537:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py:208:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py:399:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py:233:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/saved_model/load.py:152:0: error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/save.py:206:0: note: called from\r\n<ipython-input-12-cead14d55f93>:2:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py:537:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py:208:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py:399:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py:233:0: note: called from\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-12-cead14d55f93> in <module>()\r\n      4 \r\n      5 converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\r\n----> 6 litemodel = converter.convert()\r\n      7 with open(r'/content/drive/MyDrive/Model (Segmentation TFlite)/segmentation_model.tflite', 'wb') as f:\r\n      8     f.write(litemodel)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n   1056 \r\n   1057     result = super(TFLiteKerasModelConverterV2,\r\n-> 1058                    self).convert(graph_def, input_tensors, output_tensors)\r\n   1059     self._increase_conversion_success_metric(result)\r\n   1060     return result\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py in convert(self, graph_def, input_tensors, output_tensors)\r\n    784         input_tensors=input_tensors,\r\n    785         output_tensors=output_tensors,\r\n--> 786         **converter_kwargs)\r\n    787 \r\n    788     if self.experimental_new_quantizer:\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    701       input_data.SerializeToString(),\r\n    702       debug_info_str=debug_info_str,\r\n--> 703       enable_mlir_converter=enable_mlir_converter)\r\n    704   return data\r\n    705 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    295       return model_str\r\n    296     except Exception as e:\r\n--> 297       raise ConverterError(str(e))\r\n    298 \r\n    299   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/saved_model/load.py:152:0: error: 'tf.TensorListReserve' op requires element_shape to be 1D tensor during TF Lite transformation pass\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/save.py:206:0: note: called from\r\n<ipython-input-12-cead14d55f93>:2:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py:537:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py:208:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py:399:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py:233:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/saved_model/load.py:152:0: error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/save.py:206:0: note: called from\r\n<ipython-input-12-cead14d55f93>:2:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py:537:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py:208:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py:399:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py:233:0: note: called from\r\n```\r\n\r\nI have also tried directly using the `tf.lite.TFLiteConverter.from_saved_model` to convert, but getting a different error with that:\r\n```\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    293                                                  debug_info_str,\r\n--> 294                                                  enable_mlir_converter)\r\n    295       return model_str\r\n\r\n4 frames\r\nException: <unknown>:0: error: loc(callsite(callsite(\"mask_rcnn/mrcnn_detection/map/TensorArrayV2_1@__inference__wrapped_model_8467\" at \"StatefulPartitionedCall@__inference_signature_wrapper_26893\") at \"StatefulPartitionedCall\")): 'tf.TensorListReserve' op requires element_shape to be 1D tensor during TF Lite transformation pass\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"mask_rcnn/mrcnn_detection/map/TensorArrayV2_1@__inference__wrapped_model_8467\" at \"StatefulPartitionedCall@__inference_signature_wrapper_26893\") at \"StatefulPartitionedCall\")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    295       return model_str\r\n    296     except Exception as e:\r\n--> 297       raise ConverterError(str(e))\r\n    298 \r\n    299   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: <unknown>:0: error: loc(callsite(callsite(\"mask_rcnn/mrcnn_detection/map/TensorArrayV2_1@__inference__wrapped_model_8467\" at \"StatefulPartitionedCall@__inference_signature_wrapper_26893\") at \"StatefulPartitionedCall\")): 'tf.TensorListReserve' op requires element_shape to be 1D tensor during TF Lite transformation pass\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"mask_rcnn/mrcnn_detection/map/TensorArrayV2_1@__inference__wrapped_model_8467\" at \"StatefulPartitionedCall@__inference_signature_wrapper_26893\") at \"StatefulPartitionedCall\")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n```\r\n\r\nThe saved model is a simple MaskRCNN model. \r\nI have tried this on a WSL Ubuntu 18.04 Shell as well as Google Colab, but not able to work out the error.\r\nSorry, if I am missing something trivial in this the conversion. \r\n", "comments": ["Could you try saved model conversion with the Select TF option?", "@abattery So, I added the following piece \r\n```\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\n```\r\n\r\nAnd this did solve the error I was getting. Thanks!", "Closing the error as it has been resolved. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49732\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49732\">No</a>\n"]}, {"number": 49731, "title": "[MLIR][DISC] Add memref_disc dialect as a temporary solution to optim\u2026", "body": "MemRefDisc Dialect is an expansion for MemRefDialect for DISC. This is a temporary workaround for optimizing the index calculations during codegen. Refer to https://llvm.discourse.group/t/add-an-expanded-load-store-op-in-memref-dialect/3503/26 for background informations.", "comments": ["Seems like we converged in the thread on a different solution for this right? \r\nHow do you see the roadmap on all this here?\r\nI'm not convinced we want to take a dependency on this temporary solution right now, instead of implementing something along the line of what was described on Discourse.", "> Seems like we converged in the thread on a different solution for this right?\r\n> How do you see the roadmap on all this here?\r\n> I'm not convinced we want to take a dependency on this temporary solution right now, instead of implementing something along the line of what was described on Discourse.\r\n\r\nI agree to migrate to an explicit linearize/delinearize together with an mermef.offset_load when they are ready. However they are currently not there and in my understanding still need more efforts for the details, and also much effort in revising the two codegen passes. I would prefer a private memref_disc dialect for the moment to get this merged and left as a TODO. This will not cause side-effects for other users, and will not bring dependency for memref_disc except for the two codegen passes.", "With offline discussion, i'll cancel this PR"]}, {"number": 49730, "title": "Model.fit stuck if uses MirroredStrategy with 10 GPUs", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution:: Linux Ubuntu 16.04\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.8.7\r\n- CUDA/cuDNN version: 11.0.228 / 8.0.4\r\n- GPU model and memory: GTX 1080Ti 11GB\r\n\r\n**Describe the current behavior**\r\nI've a machine with 10x GPU and I'm trying to use all the power using MirroredStrategy. When I run model.fit the program stucks or is really slow. If I use only first gpu (GPU:0) the learning process is fast as expected. I use a generator that read samples from a HDF5 file format. The process with 1 GPU takes 80s for first epoch and than 20s for the others. When using MirroredStrategy the process stucks or takes 2/3 mins to run.\r\n\r\n**Describe the expected behavior**\r\nAt least the same time as if running with only 1 GPU\r\n\r\n**Standalone code to reproduce the issue**\r\nYou can use variable `use_strategy ` to enable or not MirroredStrategy.\r\n```\r\nimport random\r\n\r\nfrom tensorflow import keras\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nuse_strategy = True\r\nclass Generator(tf.keras.utils.Sequence):\r\n    def __init__(self):\r\n        print(\"GENERATED\")\r\n        self.samples = np.random.rand(100000, 5, 20)\r\n        self.labels = np.random.randint(2, size=(100000, 1))\r\n        self.indices = list(range(0, 100000))\r\n        random.Random().shuffle(self.indices)\r\n\r\n    def __len__(self):\r\n        return 100000\r\n\r\n    def __getitem__(self, idx):\r\n        return self.samples[self.indices[idx]], self.labels[self.indices[idx]]\r\n\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\nfor device in physical_devices:\r\n    tf.config.experimental.set_memory_growth(device, True)\r\n\r\nif use_strategy:\r\n    strategy = tf.distribute.MirroredStrategy()\r\n    num_gpu = strategy.num_replicas_in_sync\r\nelse:\r\n    num_gpu = 1\r\n    \r\nd = tf.data.Dataset.from_generator(Generator,\r\n                                   output_signature=(tf.TensorSpec(shape=(5, 20)), tf.TensorSpec(shape=(1,))))\r\nd = d.batch(32*num_gpu).prefetch(tf.data.AUTOTUNE).cache()\r\n\r\nif use_strategy:\r\n    options = tf.data.Options()\r\n    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\n    d = d.with_options(options)\r\n\r\n    with strategy.scope():\r\n        model = keras.models.Sequential()\r\n        model.add(keras.layers.Conv1D(32, kernel_size=1, strides=1, activation=\"relu\", padding=\"same\", input_shape=(5, 20)))\r\n        model.add(keras.layers.MaxPooling1D(pool_size=3, padding=\"same\"))\r\n        model.add(keras.layers.Conv1D(64, kernel_size=1, strides=1, activation=\"relu\", padding=\"same\", input_shape=(5, 20)))\r\n        model.add(keras.layers.MaxPooling1D(pool_size=3, padding=\"same\"))\r\n        model.add(keras.layers.Flatten())\r\n        model.add(keras.layers.Dense(1024, activation=\"relu\"))\r\n        model.add(keras.layers.Dense(1, activation=\"softmax\"))\r\n        model.compile(\r\n            optimizer=keras.optimizers.Adam(0.003),\r\n            loss=\"categorical_crossentropy\",\r\n            metrics=[\"accuracy\"],\r\n        )\r\nelse:\r\n    model = keras.models.Sequential()\r\n    model.add(keras.layers.Conv1D(32, kernel_size=1, strides=1, activation=\"relu\", padding=\"same\", input_shape=(5, 20)))\r\n    model.add(keras.layers.MaxPooling1D(pool_size=3, padding=\"same\"))\r\n    model.add(keras.layers.Conv1D(64, kernel_size=1, strides=1, activation=\"relu\", padding=\"same\", input_shape=(5, 20)))\r\n    model.add(keras.layers.MaxPooling1D(pool_size=3, padding=\"same\"))\r\n    model.add(keras.layers.Flatten())\r\n    model.add(keras.layers.Dense(1024, activation=\"relu\"))\r\n    model.add(keras.layers.Dense(1, activation=\"softmax\"))\r\n    model.compile(\r\n        optimizer=keras.optimizers.Adam(0.003),\r\n        loss=\"categorical_crossentropy\",\r\n        metrics=[\"accuracy\"],\r\n    )\r\n\r\nmodel.fit(d, epochs=5)\r\n```\r\n\r\n**Other info / logs** \r\nThis is an output with `use_strategy = False`\r\n```\r\n2021-05-26 11:50:33.912438: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-05-26 11:50:35.657549: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-05-26 11:50:35.658587: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-05-26 11:50:41.023529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:04:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:41.024948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:\r\npciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:41.026395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties:\r\npciBusID: 0000:06:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:41.027791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties:\r\npciBusID: 0000:07:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:41.029170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 4 with properties:\r\npciBusID: 0000:08:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:41.030585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 5 with properties:\r\npciBusID: 0000:0b:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:41.031954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 6 with properties:\r\npciBusID: 0000:0c:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:41.033335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 7 with properties:\r\npciBusID: 0000:0d:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:41.034735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 8 with properties:\r\npciBusID: 0000:0e:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:41.036114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 9 with properties:\r\npciBusID: 0000:0f:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:41.036139: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-05-26 11:50:41.038966: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-05-26 11:50:41.039009: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-05-26 11:50:41.040262: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-05-26 11:50:41.040558: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-05-26 11:50:41.044146: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-05-26 11:50:41.044346: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-05-26 11:50:41.076938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\r\n2021-05-26 11:50:41.082129: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-05-26 11:50:41.086016: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-05-26 11:50:43.498811: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 0 and 9, status: Internal: failed to enable peer access from 0x618ae70 to 0x9163340: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.518689: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 1 and 9, status: Internal: failed to enable peer access from 0x6750470 to 0x9163340: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.536020: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 2 and 9, status: Internal: failed to enable peer access from 0x6c72680 to 0x9163340: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.547585: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 3 and 9, status: Internal: failed to enable peer access from 0x719d4c0 to 0x9163340: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.560961: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 4 and 9, status: Internal: failed to enable peer access from 0x76d1cf0 to 0x9163340: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.567989: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 5 and 9, status: Internal: failed to enable peer access from 0x7c0ebb0 to 0x9163340: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.572534: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 6 and 9, status: Internal: failed to enable peer access from 0x8154b50 to 0x9163340: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.576183: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 7 and 9, status: Internal: failed to enable peer access from 0x86a3c50 to 0x9163340: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.576448: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 8 and 9, status: Internal: failed to enable peer access from 0x8bfd120 to 0x9163340: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.576651: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 0, status: Internal: failed to enable peer access from 0x9163340 to 0x618ae70: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.576844: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 1, status: Internal: failed to enable peer access from 0x9163340 to 0x6750470: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.577034: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 2, status: Internal: failed to enable peer access from 0x9163340 to 0x6c72680: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.577243: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 3, status: Internal: failed to enable peer access from 0x9163340 to 0x719d4c0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.577432: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 4, status: Internal: failed to enable peer access from 0x9163340 to 0x76d1cf0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.577612: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 5, status: Internal: failed to enable peer access from 0x9163340 to 0x7c0ebb0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.577789: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 6, status: Internal: failed to enable peer access from 0x9163340 to 0x8154b50: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.577966: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 7, status: Internal: failed to enable peer access from 0x9163340 to 0x86a3c50: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.578144: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 8, status: Internal: failed to enable peer access from 0x9163340 to 0x8bfd120: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 11:50:43.581287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:04:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:43.583663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:\r\npciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:43.585870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties:\r\npciBusID: 0000:06:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:43.590771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties:\r\npciBusID: 0000:07:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:43.592978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 4 with properties:\r\npciBusID: 0000:08:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:43.597939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 5 with properties:\r\npciBusID: 0000:0b:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:43.600146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 6 with properties:\r\npciBusID: 0000:0c:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:43.602322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 7 with properties:\r\npciBusID: 0000:0d:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:43.604462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 8 with properties:\r\npciBusID: 0000:0e:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:43.607100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 9 with properties:\r\npciBusID: 0000:0f:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 11:50:43.607193: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-05-26 11:50:43.607356: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-05-26 11:50:43.607421: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-05-26 11:50:43.607482: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-05-26 11:50:43.607540: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-05-26 11:50:43.607596: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-05-26 11:50:43.607656: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-05-26 11:50:43.607718: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-05-26 11:50:43.664208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\r\n2021-05-26 11:50:43.664293: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-05-26 11:50:48.133740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-05-26 11:50:48.133793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 2 3 4 5 6 7 8 9\r\n2021-05-26 11:50:48.133804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y Y Y Y Y Y Y Y Y\r\n2021-05-26 11:50:48.133810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N Y Y Y Y Y Y Y Y\r\n2021-05-26 11:50:48.133815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 2:   Y Y N Y Y Y Y Y Y Y\r\n2021-05-26 11:50:48.133823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 3:   Y Y Y N Y Y Y Y Y Y\r\n2021-05-26 11:50:48.133845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 4:   Y Y Y Y N Y Y Y Y Y\r\n2021-05-26 11:50:48.133868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 5:   Y Y Y Y Y N Y Y Y Y\r\n2021-05-26 11:50:48.133878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 6:   Y Y Y Y Y Y N Y Y Y\r\n2021-05-26 11:50:48.133885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 7:   Y Y Y Y Y Y Y N Y Y\r\n2021-05-26 11:50:48.133893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 8:   Y Y Y Y Y Y Y Y N Y\r\n2021-05-26 11:50:48.133900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 9:   Y Y Y Y Y Y Y Y Y N\r\n2021-05-26 11:50:48.152300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10271 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\n2021-05-26 11:50:48.161298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10271 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1)\r\n2021-05-26 11:50:48.169676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10271 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n2021-05-26 11:50:48.175658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10271 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:07:00.0, compute capability: 6.1)\r\n2021-05-26 11:50:48.183729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 10271 MB memory) -> physical GPU (device: 4, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)\r\n2021-05-26 11:50:48.192600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 10271 MB memory) -> physical GPU (device: 5, name: GeForce GTX 1080 Ti, pci bus id: 0000:0b:00.0, compute capability: 6.1)\r\n2021-05-26 11:50:48.201611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 10271 MB memory) -> physical GPU (device: 6, name: GeForce GTX 1080 Ti, pci bus id: 0000:0c:00.0, compute capability: 6.1)\r\n2021-05-26 11:50:48.211137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 10271 MB memory) -> physical GPU (device: 7, name: GeForce GTX 1080 Ti, pci bus id: 0000:0d:00.0, compute capability: 6.1)\r\n2021-05-26 11:50:48.217026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:8 with 10271 MB memory) -> physical GPU (device: 8, name: GeForce GTX 1080 Ti, pci bus id: 0000:0e:00.0, compute capability: 6.1)\r\n2021-05-26 11:50:48.229431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:9 with 10271 MB memory) -> physical GPU (device: 9, name: GeForce GTX 1080 Ti, pci bus id: 0000:0f:00.0, compute capability: 6.1)\r\n2021-05-26 11:50:48.446515: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-05-26 11:50:48.447341: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200000000 Hz\r\nEpoch 1/5\r\n2021-05-26 11:50:49.157305: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\nGENERATED\r\n2021-05-26 11:50:49.610821: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-05-26 11:50:49.614300: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n3125/3125 [==============================] - 35s 10ms/step - loss: 0.0000e+00 - accuracy: 0.4967\r\nEpoch 2/5\r\n3125/3125 [==============================] - 13s 4ms/step - loss: 0.0000e+00 - accuracy: 0.4967\r\nEpoch 3/5\r\n3125/3125 [==============================] - 13s 4ms/step - loss: 0.0000e+00 - accuracy: 0.4967\r\nEpoch 4/5\r\n3125/3125 [==============================] - 13s 4ms/step - loss: 0.0000e+00 - accuracy: 0.4967\r\nEpoch 5/5\r\n3125/3125 [==============================] - 13s 4ms/step - loss: 0.0000e+00 - accuracy: 0.4967\r\n```\r\n\r\nThis is an output with `use_strategy = True`\r\n```\r\n2021-05-26 12:00:10.570133: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-05-26 12:00:12.355013: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-05-26 12:00:12.356136: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-05-26 12:00:17.816965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:04:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:17.818449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:\r\npciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:17.819861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties:\r\npciBusID: 0000:06:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:17.821265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties:\r\npciBusID: 0000:07:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:17.822701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 4 with properties:\r\npciBusID: 0000:08:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:17.824115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 5 with properties:\r\npciBusID: 0000:0b:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:17.825524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 6 with properties:\r\npciBusID: 0000:0c:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:17.826935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 7 with properties:\r\npciBusID: 0000:0d:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:17.828344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 8 with properties:\r\npciBusID: 0000:0e:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:17.829788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 9 with properties:\r\npciBusID: 0000:0f:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:17.829811: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-05-26 12:00:17.832797: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-05-26 12:00:17.832845: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-05-26 12:00:17.834196: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-05-26 12:00:17.837631: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-05-26 12:00:17.838391: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-05-26 12:00:17.838609: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-05-26 12:00:17.864974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\r\n2021-05-26 12:00:17.867972: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-05-26 12:00:17.872954: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-05-26 12:00:19.976176: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 0 and 9, status: Internal: failed to enable peer access from 0x68a9e70 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:19.993878: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 1 and 9, status: Internal: failed to enable peer access from 0x6e6f490 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.009534: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 2 and 9, status: Internal: failed to enable peer access from 0x7391670 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.022940: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 3 and 9, status: Internal: failed to enable peer access from 0x78bc490 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.035994: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 4 and 9, status: Internal: failed to enable peer access from 0x7df0c90 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.042246: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 5 and 9, status: Internal: failed to enable peer access from 0x832db50 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.049801: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 6 and 9, status: Internal: failed to enable peer access from 0x8873ab0 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.052185: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 7 and 9, status: Internal: failed to enable peer access from 0x8dc2bc0 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.052495: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 8 and 9, status: Internal: failed to enable peer access from 0x931c060 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.055092: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 0, status: Internal: failed to enable peer access from 0x9882170 to 0x68a9e70: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.055349: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 1, status: Internal: failed to enable peer access from 0x9882170 to 0x6e6f490: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.055538: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 2, status: Internal: failed to enable peer access from 0x9882170 to 0x7391670: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.055722: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 3, status: Internal: failed to enable peer access from 0x9882170 to 0x78bc490: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.055902: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 4, status: Internal: failed to enable peer access from 0x9882170 to 0x7df0c90: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.056074: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 5, status: Internal: failed to enable peer access from 0x9882170 to 0x832db50: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.056246: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 6, status: Internal: failed to enable peer access from 0x9882170 to 0x8873ab0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.056420: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 7, status: Internal: failed to enable peer access from 0x9882170 to 0x8dc2bc0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.056593: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 8, status: Internal: failed to enable peer access from 0x9882170 to 0x931c060: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.059009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:04:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:20.061213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:\r\npciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:20.063382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties:\r\npciBusID: 0000:06:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:20.065544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties:\r\npciBusID: 0000:07:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:20.070266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 4 with properties:\r\npciBusID: 0000:08:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:20.074928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 5 with properties:\r\npciBusID: 0000:0b:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:20.077085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 6 with properties:\r\npciBusID: 0000:0c:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:20.079152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 7 with properties:\r\npciBusID: 0000:0d:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:20.081209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 8 with properties:\r\npciBusID: 0000:0e:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:20.083240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 9 with properties:\r\npciBusID: 0000:0f:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-05-26 12:00:20.083289: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-05-26 12:00:20.083342: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-05-26 12:00:20.083378: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-05-26 12:00:20.083402: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-05-26 12:00:20.083426: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-05-26 12:00:20.083451: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-05-26 12:00:20.083475: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-05-26 12:00:20.083500: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-05-26 12:00:20.129612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\r\n2021-05-26 12:00:20.129683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-05-26 12:00:24.578650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-05-26 12:00:24.578701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 2 3 4 5 6 7 8 9\r\n2021-05-26 12:00:24.578718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y Y Y Y Y Y Y Y Y\r\n2021-05-26 12:00:24.578725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N Y Y Y Y Y Y Y Y\r\n2021-05-26 12:00:24.578737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 2:   Y Y N Y Y Y Y Y Y Y\r\n2021-05-26 12:00:24.578779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 3:   Y Y Y N Y Y Y Y Y Y\r\n2021-05-26 12:00:24.578791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 4:   Y Y Y Y N Y Y Y Y Y\r\n2021-05-26 12:00:24.578802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 5:   Y Y Y Y Y N Y Y Y Y\r\n2021-05-26 12:00:24.578812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 6:   Y Y Y Y Y Y N Y Y Y\r\n2021-05-26 12:00:24.578822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 7:   Y Y Y Y Y Y Y N Y Y\r\n2021-05-26 12:00:24.578832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 8:   Y Y Y Y Y Y Y Y N Y\r\n2021-05-26 12:00:24.578839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 9:   Y Y Y Y Y Y Y Y Y N\r\n2021-05-26 12:00:24.594625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10271 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\n2021-05-26 12:00:24.600899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10271 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1)\r\n2021-05-26 12:00:24.607133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10271 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n2021-05-26 12:00:24.613263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10271 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:07:00.0, compute capability: 6.1)\r\n2021-05-26 12:00:24.619044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 10271 MB memory) -> physical GPU (device: 4, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)\r\n2021-05-26 12:00:24.625345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 10271 MB memory) -> physical GPU (device: 5, name: GeForce GTX 1080 Ti, pci bus id: 0000:0b:00.0, compute capability: 6.1)\r\n2021-05-26 12:00:24.631179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 10271 MB memory) -> physical GPU (device: 6, name: GeForce GTX 1080 Ti, pci bus id: 0000:0c:00.0, compute capability: 6.1)\r\n2021-05-26 12:00:24.636911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 10271 MB memory) -> physical GPU (device: 7, name: GeForce GTX 1080 Ti, pci bus id: 0000:0d:00.0, compute capability: 6.1)\r\n2021-05-26 12:00:24.643134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:8 with 10271 MB memory) -> physical GPU (device: 8, name: GeForce GTX 1080 Ti, pci bus id: 0000:0e:00.0, compute capability: 6.1)\r\n2021-05-26 12:00:24.649300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:9 with 10271 MB memory) -> physical GPU (device: 9, name: GeForce GTX 1080 Ti, pci bus id: 0000:0f:00.0, compute capability: 6.1)\r\n2021-05-26 12:00:25.017011: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-05-26 12:00:25.018083: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200000000 Hz\r\nEpoch 1/5\r\n2021-05-26 12:00:38.830859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-05-26 12:00:39.152138: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\nGENERATED\r\n2021-05-26 12:00:41.550966: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n\r\n<-- STUCKED HERE \r\n```", "comments": ["Hi @TheEnigmist, there is always some amount of overhead associated with distributed training. If you're using `MirroredStrategy` then there is overhead at the end of each step to synchronize the gradients across replicas. The key is to make sure that the host successfully keeps the worker devices occupied by offloading enough work. For a small model like you have where one epoch only takes 20 seconds before distribution,  the overhead of synchronizing across 10 devices will probably just outweigh the benefits.\r\n\r\nOne way to verify this is to use the TensorFlow Profiler and see what part of each step is taking the most time. The Optimize [GPU Performance Guide ](https://www.tensorflow.org/guide/gpu_performance_analysis)provides information on how to diagnose performance issues. This[ section of the guide](https://www.tensorflow.org/guide/gpu_performance_analysis#optimize_performance_on_multi-gpu_single_host) in particular shows how to diagnose performance for `MirroredStrategy`.", "I understand what you mean and I expect that the program runs really slow, not that is permanetly stucked. It doesn't start even the first epoch.\r\nThe problem is that I can't use TensorBoard since Model.fit is stucked, nothing happen, even updates for TensorBoard. \r\nAfter 30 min I need to force exit of my program sinse is all stucked. I don't know if is a bug or a performance problem.", "Is it always stuck? Or does it run sometimes? Also does it only hang when using `MirroredStrategy`, or will it get stuck without distribution as well?\r\n\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Was able to replicate the issue with TF 2.6.0-dev20210606,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/6a5a0f29ce2cae7918315972470a0455/untitled235.ipynb) ..Thanks", "> Was able to replicate the issue with TF 2.6.0-dev20210606,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/6a5a0f29ce2cae7918315972470a0455/untitled235.ipynb) ..Thanks\r\n\r\nI see colab here used only 1 CPU, if you want to reproduce properly this issue you need to use more than 1 GPU.\r\n\r\n>Is it always stuck? Or does it run sometimes? Also does it only hang when using MirroredStrategy, or will it get stuck without distribution as well?\r\n\r\nIt stucks only when `MirroredStrategy` without parameters is used\r\n\r\nBtw maybe I found what is wrong with my hardware. If use selective GPU in `MirroredStrategy` it starts after 40/50s:\r\n```\r\nMirroredStrategy([\"GPU:0\",\"GPU:1,\"GPU:2\",\"GPU:3,\"GPU:4\",\"GPU:5\"GPU:6\",\"GPU:7\",\"GPU:8\"])\r\n```\r\nAs soon as I add GPU:9 it stucks. \r\n\r\nMaybe this is due to an error I see in Tensorflow logs, seems like GPU9 is not properly connected with other GPU and this send MirroredStrategy in deadlock trying to synchronize what can't be synchronized:\r\n```\r\n2021-05-26 12:00:19.976176: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 0 and 9, status: Internal: failed to enable peer access from 0x68a9e70 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:19.993878: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 1 and 9, status: Internal: failed to enable peer access from 0x6e6f490 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.009534: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 2 and 9, status: Internal: failed to enable peer access from 0x7391670 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.022940: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 3 and 9, status: Internal: failed to enable peer access from 0x78bc490 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.035994: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 4 and 9, status: Internal: failed to enable peer access from 0x7df0c90 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.042246: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 5 and 9, status: Internal: failed to enable peer access from 0x832db50 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.049801: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 6 and 9, status: Internal: failed to enable peer access from 0x8873ab0 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.052185: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 7 and 9, status: Internal: failed to enable peer access from 0x8dc2bc0 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.052495: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 8 and 9, status: Internal: failed to enable peer access from 0x931c060 to 0x9882170: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.055092: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 0, status: Internal: failed to enable peer access from 0x9882170 to 0x68a9e70: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.055349: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 1, status: Internal: failed to enable peer access from 0x9882170 to 0x6e6f490: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.055538: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 2, status: Internal: failed to enable peer access from 0x9882170 to 0x7391670: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.055722: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 3, status: Internal: failed to enable peer access from 0x9882170 to 0x78bc490: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.055902: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 4, status: Internal: failed to enable peer access from 0x9882170 to 0x7df0c90: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.056074: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 5, status: Internal: failed to enable peer access from 0x9882170 to 0x832db50: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.056246: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 6, status: Internal: failed to enable peer access from 0x9882170 to 0x8873ab0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.056420: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 7, status: Internal: failed to enable peer access from 0x9882170 to 0x8dc2bc0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2021-05-26 12:00:20.056593: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1677] Unable to enable peer access between device ordinals 9 and 8, status: Internal: failed to enable peer access from 0x9882170 to 0x931c060: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49729, "title": "Error run tf-mlir-translate", "body": "when i run bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate  --graphdef-to-mlir hand_detect_global.pb -o /tmp/mlir_hand_detect.mlir                    \r\n\r\nit says \r\n2021-05-26 17:00:11.917161: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"Fact\" device_type: \"CPU\" label: \"sergey\"') for unknown op: Fact\r\n2021-05-26 17:00:11.917564: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"Fact\" device_type: \"CPU\" label: \"Sergey\"') for unknown op: Fact\r\n2021-05-26 17:00:11.917573: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"Fact\" device_type: \"GPU\" host_memory_arg: \"fact\"') for unknown op: Fact\r\n\r\nbut , i print the op in the pb, can't find Fact op, so i dont know why give me this error", "comments": ["@mittkongg \r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]Thanks\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49729\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49729\">No</a>\n"]}, {"number": 49728, "title": "Error on Google Colab but not on local machine", "body": "9 hours ago I ran some code on Google Colab which ran totally fine, this morning I get the following error trying to run the exact same code:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-4472585283ee> in <module>()\r\n     11 #Model on top\r\n     12 x = base_model.output\r\n---> 13 x = Flatten()(x)\r\n     14 x = Dense(128, activation='relu')(x)\r\n     15 x = Dropout(0.5)(x)\r\n\r\n5 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n     97   ctx.ensure_initialized()\r\n---> 98   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n     99 \r\n    100 \r\n\r\nValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\r\n```\r\n\r\n\r\nThe exact same code runs totally fine on local runtime", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49728\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49728\">No</a>\n", "The colab environment variables are lost if the notebook is idle for sometime (not sure exact amount of time). So you my want to rerun whole notebook from top most cell and try again."]}, {"number": 49725, "title": "Dtype error when using Mixed Precision and building EfficientNetB0 Model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: python 3.7\r\n- GPU model and memory: Tesla T4\r\n\r\n**Error**\r\n`TypeError: Input 'y' of 'Sub' Op has type float16 that does not match type float32 of argument 'x'`\r\n\r\n**Current behaviour**\r\n\r\nWhile using Mixed Precision and building a Keras Functional API model (EfficientNet B0), it shows the below error\r\n\r\n<img width=\"935\" alt=\"image\" src=\"https://user-images.githubusercontent.com/57211163/119546314-6669c180-bdb1-11eb-918f-3cb13d225759.png\">\r\n<img width=\"1248\" alt=\"image\" src=\"https://user-images.githubusercontent.com/57211163/119546338-6c5fa280-bdb1-11eb-844d-91175160e6e5.png\">\r\n\r\n**Describe the expected behaviour**\r\nThe Global Policy I set in the previous cell was `mixed_float16`. The problem works fine when running on `tensorflow 2.4.1` so the bug is with `tensorflow 2.5.0`\r\n\r\n\r\nYou can reproduce the same error using the below notebook :\r\nhttps://colab.research.google.com/drive/1TfNZSIJ_I7IZI35RsGFnTdj-6beMHV2_?usp=sharing\r\n", "comments": ["Following this, I'm having the same issue.\r\n\r\nWorks on 2.4.1, breaks on 2.5.0", "It seems like a bug in `TF 2.5` (also in `tf-nightly`). Here is the reproducible code \r\n\r\n```\r\nimport tensorflow as tf \r\nprint(tf.__version__) # 2.5\r\n\r\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\r\ntf.keras.applications.EfficientNetB0(\r\n    include_top=True, weights='imagenet', input_tensor=None,\r\n    input_shape=None, pooling=None, classes=1000,\r\n    classifier_activation='softmax'\r\n)\r\n```", "I was able to reproduce the issue in tf [v2.5](https://colab.research.google.com/gist/tilakrayal/deac7077472f47ce5624e2015ddf3056/49725-2-5.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/afe5a652eb182b7caf44cba1f54b34d6/food_visionnightly.ipynb). In tf v2.4, code executed without errors.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/a4753ef385f6d94f6034aac4983110d0/49725-2-4food_vision.ipynb).", "It seems like a bug with `EfficientNetB0`itself. I was able to run `mixed_precision` without any errors on the Resnet101 model. \r\n![Screenshot 2021-05-27 at 5 14 47 AM](https://user-images.githubusercontent.com/45290954/119745070-048d8280-beab-11eb-871e-d1215ab3b577.png)\r\n", "The above error occurs only one \r\n\r\n- **TensorFlow 2.5** and **tf-nightly** (currently tf 2.6.\r\n- Only on the **EfficientNet Bx** family but not other architecture.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49725\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49725\">No</a>\n"]}, {"number": 49724, "title": "Wrong warning message for tf.data.experimental.enable.debug_mode()", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 pro 64 bit 21H1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip3\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.3 update 1, 8.2.0\r\n- GPU model and memory: RTX 2060 super 8GB\r\n\r\n**Describe the current behavior**\r\nWhen trying to access tensor.numpy inside a tf.data function, Tensorflow reports tensor object has no numpy attribute. A warning message is given: \r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3703: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\r\nHowever, this warning message is wrong. The \"tf.data.experimental.enable.debug_mode()\" does not exist as a function.\r\n**Describe the expected behavior**\r\nThe correct command is 'tf.data.experimental.enable_debug_mode()', thus there is a typo that replaced the underscore after enable to be a dot. I have not checked thoroughly in the entire repo for any other occurrence of the same typo.\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing): yes, fix typo.\r\nhttps://github.com/tensorflow/tensorflow/pull/49825\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nAs this is a simple typo issue, I don't think an example is needed. Here is a permalink to the line in the source code that has the typo.\r\nhttps://github.com/tensorflow/tensorflow/blob/dc487160e67bdb9b4bb317e0922e3ae3eb513c43/tensorflow/python/data/ops/dataset_ops.py#L3918", "comments": ["@aliencaocao,\r\nI have linked [your PR](https://github.com/tensorflow/tensorflow/pull/49825) to this issue. Meanwhile, can you please provide a reproducible code demonstrating the bug so that community can understand it clearly? Thanks! ", "Sure. But bfore the code itself, I would just like to remind that this is possibly just a typo and not really a program bug. The stack trace is provided in case of any use, but in this case I doubt if it will be useful.\r\n\r\nOn a side note, if anyone can suggest a way for me to convert a tensor into numpy array in this particular use case (inside a tf.data mapped function), please advise me as I desperately need to do this. I did try the suggested `tf.data.experimental.enable_debug_mode()` (I corrected the typo here), but it still gives the same 'AttributeError: 'Tensor' object has no attribute 'numpy'' error.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nds = tfds.load('oxford_iiit_pet', split='train')\r\ntf.config.run_functions_eagerly(True)\r\n\r\ndef preprocess(datapoint):\r\n    mask = datapoint['segmentation_mask']\r\n    mask.numpy()\r\n    return mask\r\n\r\nds.map(preprocess)\r\n```\r\n\r\nRunning this will give me a warning:\r\n\r\n```\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3703: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\r\n  warnings.warn(\r\n```\r\nThe suggested `tf.data.experimental.enable.debug_mode()` is the typo, and should be corrected to `tf.data.experimental.enable_debug_mode()`\r\n\r\nfull stack trace:\r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n\r\n<ipython-input-1-16c739303191> in <module>\r\n     10     return mask\r\n     11 \r\n---> 12 ds.map(preprocess)\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic)\r\n   1923         warnings.warn(\"The `deterministic` argument has no effect unless the \"\r\n   1924                       \"`num_parallel_calls` argument is specified.\")\r\n-> 1925       return MapDataset(self, map_func, preserve_cardinality=True)\r\n   1926     else:\r\n   1927       return ParallelMapDataset(\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py in __init__(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\r\n   4481     self._use_inter_op_parallelism = use_inter_op_parallelism\r\n   4482     self._preserve_cardinality = preserve_cardinality\r\n-> 4483     self._map_func = StructuredFunctionWrapper(\r\n   4484         map_func,\r\n   4485         self._transformation_name(),\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\r\n   3710     resource_tracker = tracking.ResourceTracker()\r\n   3711     with tracking.resource_tracker_scope(resource_tracker):\r\n-> 3712       self._function = fn_factory()\r\n   3713       # There is no graph to add in eager mode.\r\n   3714       add_to_graph &= not context.executing_eagerly()\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in get_concrete_function(self, *args, **kwargs)\r\n   3132          or `tf.Tensor` or `tf.TensorSpec`.\r\n   3133     \"\"\"\r\n-> 3134     graph_function = self._get_concrete_function_garbage_collected(\r\n   3135         *args, **kwargs)\r\n   3136     graph_function._garbage_collector.release()  # pylint: disable=protected-access\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)\r\n   3098       args, kwargs = None, None\r\n   3099     with self._lock:\r\n-> 3100       graph_function, _ = self._maybe_define_function(args, kwargs)\r\n   3101       seen_names = set()\r\n   3102       captured = object_identity.ObjectIdentitySet(\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\r\n   3442 \r\n   3443           self._function_cache.missed.add(call_context_key)\r\n-> 3444           graph_function = self._create_graph_function(args, kwargs)\r\n   3445           self._function_cache.primary[cache_key] = graph_function\r\n   3446 \r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3277     arg_names = base_arg_names + missing_arg_names\r\n   3278     graph_function = ConcreteFunction(\r\n-> 3279         func_graph_module.func_graph_from_py_func(\r\n   3280             self._name,\r\n   3281             self._python_function,\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    997         _, original_func = tf_decorator.unwrap(python_func)\r\n    998 \r\n--> 999       func_outputs = python_func(*func_args, **func_kwargs)\r\n   1000 \r\n   1001       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py in wrapped_fn(*args)\r\n   3685           attributes=defun_kwargs)\r\n   3686       def wrapped_fn(*args):  # pylint: disable=missing-docstring\r\n-> 3687         ret = wrapper_helper(*args)\r\n   3688         ret = structure.to_tensor_list(self._output_structure, ret)\r\n   3689         return [ops.convert_to_tensor(t) for t in ret]\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py in wrapper_helper(*args)\r\n   3615       if not _should_unpack(nested_args):\r\n   3616         nested_args = (nested_args,)\r\n-> 3617       ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)\r\n   3618       if _should_pack(ret):\r\n   3619         ret = tuple(ret)\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py in wrapper(*args, **kwargs)\r\n    693       except Exception as e:  # pylint:disable=broad-except\r\n    694         if hasattr(e, 'ag_error_metadata'):\r\n--> 695           raise e.ag_error_metadata.to_exception(e)\r\n    696         else:\r\n    697           raise\r\n\r\nAttributeError: in user code:\r\n\r\n    <ipython-input-1-16c739303191>:9 preprocess  *\r\n        mask.numpy()\r\n    C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:401 __getattr__\r\n        self.__getattribute__(name)\r\n\r\n    AttributeError: 'Tensor' object has no attribute 'numpy'\r\n```", "@aliencaocao I noticed that your PR got approved but some errors are blocking it from merging. We will look into it. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49724\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49724\">No</a>\n"]}, {"number": 49723, "title": "Variables not found in tf 2.5 when calling fit()", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab run on Windows 10\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: N/A\r\n-   **TensorFlow installed from (source or binary)**:  \\*\r\n-   **TensorFlow version (use command below)**:  2.5\r\n-   **Python version**: 3.7.10\r\n-   **Bazel version (if compiling from source)**: \\*\r\n-   **GCC/Compiler version (if compiling from source)**: \\*\r\n-   **CUDA/cuDNN version**: \\*\r\n-   **GPU model and memory**: \\*\r\n-   **Exact command to reproduce**: see below\r\n\r\n\\*: For all such information, I am using Google Colab as is, without any modification.\r\n\r\n\r\n### Description of the problem\r\nI believe this is a problem with tf 2.5, as I was running all codes with no issue for the past few months with tf 2.4.1. The problem happened all of sudden and I came to notice a sudden change in the tf version to 2.5. As the later is the only change, I beleive it must be the reason.\r\n\r\nI am building a training model using keras api, everything is straightforward, with Adam chosen as an optimizer. The error happens once I call mdl.fit()\r\n\r\n### Source code:\r\nFor reference, this is a VAE model. To reproduce, create a colab file and paste the following code as is.\r\n```\r\nseq_shape = (34,4,1)\r\nbatch_size = 512\r\nlatent_dim = 2\r\n\r\ninput_seq = keras.Input(shape=seq_shape)\r\nx = layers.Conv2D(80, (5,1),activation='relu')(input_seq)\r\nshape_before_flattening = K.int_shape(x)\r\nx = layers.Flatten()(x)\r\n\r\nx = layers.Dense(80,activation='relu')(x)\r\nx = layers.Dense(40,activation='relu')(x)\r\ndense3 = layers.Dense(latent_dim)\r\nz_mean = dense3(x)\r\nz_log_var = layers.Dense(latent_dim)(x)\r\nz = layers.Lambda(sampling)([z_mean, z_log_var])\r\n\r\ndecoder_input = layers.Input(K.int_shape(z)[1:])\r\nx = layers.Dense(40,activation='relu')(decoder_input)\r\nx = layers.Dense(80,activation='relu')(x)\r\nx = layers.Dense(np.prod(shape_before_flattening[1:]),activation='relu')(x)\r\nx = layers.Reshape(shape_before_flattening[1:])(x)\r\nx = layers.Conv2DTranspose(1, (5,1),activation='sigmoid')(x)\r\ndecoder = Model(decoder_input, x)\r\n\r\nz_decoded = decoder(z)  \r\nvae = Model(input_seq, z_decoded)\r\n\r\n\r\nvae.compile(optimizer='adam',loss=[vae_loss])\r\nx = np.ones((100,34,4,1))\r\nvae.fit(x=x,y=x,batch_size=batch_size,shuffle=True,epochs=100)\r\n\r\n```\r\nTo run the above, the following two functions are needed:\r\n```\r\ndef vae_loss(x, z_decoded):\r\n    x = K.flatten(x)\r\n    z_decoded = K.flatten(z_decoded)\r\n    xent_loss =  keras.metrics.binary_crossentropy(x, z_decoded)\r\n    kl_loss =  1e-3 * -0.5 * K.mean( \r\n    1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\r\n    return K.mean(xent_loss + kl_loss)\r\n\r\ndef sampling(args):\r\n    z_mean, z_log_var = args\r\n    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\r\n    mean=0., stddev=1.)\r\n    return z_mean + K.exp(z_log_var) * epsilon\r\n\r\n```\r\nThese are all the needed imports:\r\n```\r\nimport tensorflow.compat.v1.keras.backend as K\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\nimport keras\r\nfrom keras import layers\r\nfrom keras import backend as K\r\nfrom keras.models import Model\r\nimport numpy as np\r\n```\r\n### Logs\r\nThis is the error message I get:\r\n\r\n> FailedPreconditionError: 2 root error(s) found.\r\n>   (0) Failed precondition: Could not find variable training_4/Adam/learning_rate. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Resource localhost/training_4/Adam/learning_rate/N10tensorflow3VarE does not exist.\r\n> \t [[{{node training_4/Adam/Identity/ReadVariableOp}}]]\r\n>   (1) Failed precondition: Could not find variable training_4/Adam/learning_rate. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Resource localhost/training_4/Adam/learning_rate/N10tensorflow3VarE does not exist.\r\n> \t [[{{node training_4/Adam/Identity/ReadVariableOp}}]]\r\n> \t [[_arg_input_15_0_0/_359]]\r\n> 0 successful operations.\r\n> 0 derived errors ignored.\r\n\r\nLastly, I noticed that the reported error changes every time I call vae.fit(). The shown log reports not finding leraning_rate, in other instances it was beta_1, beta_2, or the iteration number. \r\n### Things I already tried:\r\n\r\n- Using 'rmsprop' instead of 'adam': did not work. Gives the same error, with differences in the details.\r\n- `from tensorflow.keras.optimizers import Adam`: The error persists\r\n\r\n\r\nThanks,", "comments": ["@AhmadObeid \r\n\r\nCould you please share the colab gist to reproduce the issue reported here.Thanks", "@UsharaniPagadala,\r\nhere is the link:\r\nhttps://colab.research.google.com/drive/1APeQKLFdSczi6FcXnwnOpObocHN4CIM_?usp=sharing\r\nThank you", "FWIW, here's a workaround: https://colab.research.google.com/drive/15BTx23Vvwe0cCglg8E2-otOdJuMyQnLW?usp=sharing\r\n\r\nI basically just changed the `keras` imports to use the one that is integrated into tensorflow, ie `tensorflow.keras` instead.\r\n\r\nFor some reason, there's also a standalone installation of `keras` (conveniently?) installed on Colab by default that was working with the previous version of TF. You can verify with `!pip freeze`.\r\n", "@AhmadObeid \r\nI was able to reproduce the code shared in tf 2.4 and tf 2.5 with minimal changes in the imports.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/018643b5b05860307d3e764d495e5724/vaegithub_issue.ipynb) here and let us know if it helps.Thanks", "@benyeoh  and @UsharaniPagadala,\r\nI confirm that changing some imports as suggested in both of your answers fixes the problem, even with tensorflow 2.5.\r\nThank you very much for your help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49723\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49723\">No</a>\n"]}, {"number": 49722, "title": "[PluggableDevice] Make the dimensions constant in the C API's Allocate functions", "body": "The allocate functions don't modify the dimensions, so there's no reason they can't be passed as constants. This makes it easier to call the API deep in a codebase when using arrays that are already constant and that we're not allowed to modify.", "comments": ["@saxenasaurabh  Can you please review this PR ? Thanks!", "@saxenasaurabh Can you please review this PR ? Thanks!", "@PatriceVignola  Can you please resolve conflicts? Thanks!", "@gbaned Done!"]}, {"number": 49721, "title": "Make the C API dims constant for the Allocate functions", "body": "The Allocate functions don't modify the dimensions at all, so there's no reason they can't be constant. This makes it easier to call the API deep in a codebase when using arrays that we're not allowed to modify.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49721) for more info**.\n\n<!-- need_sender_cla -->", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49721) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 49720, "title": "Multiple Eager Rewriters", "body": "Enables the configuration of multiple rewriters for each of the rewriter phases in eager mode. The order of execution of the rewriters of each phase is determined by an integer ordinal number prescribed in the registration of the rewriters.", "comments": ["@philipphack this shows as a \"draft\" on github.  Is this ready to review or are you still working on it?", "Thanks, Sanjoy. This is now ready to be reviewed.", "@saxenasaurabh can you PTAL?", "@philipphack the CI failures look legit, can you PTAL?  E.g. on Ubuntu CPU I see:\r\n\r\n```\r\ntensorflow/core/common_runtime/eager/eager_op_rewrite_registry_test.cc:42:75: error: macro \"REGISTER_REWRITE\" requires 3 arguments, but only 2 given\r\n REGISTER_REWRITE(EagerOpRewriteRegistry::PRE_EXECUTION, TestEagerOpRewrite);\r\n                                                                           ^\r\ntensorflow/core/common_runtime/eager/eager_op_rewrite_registry_test.cc:42:1: error: 'REGISTER_REWRITE' does not name a type\r\n REGISTER_REWRITE(EagerOpRewriteRegistry::PRE_EXECUTION, TestEagerOpRewrite);\r\n ^~~~~~~~~~~~~~~~\r\n```", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49720) for more info**.\n\n<!-- need_author_cla -->", "Thanks, Sanjoy. The rewrite registry test now uses the updated macro and registers two rewriter runs for the same phase."]}, {"number": 49719, "title": "Prevent one more div by 0 in TFLite", "body": "PiperOrigin-RevId: 370800114\nChange-Id: I6b956aeb8c458cc6f514408d2e89ffacfe249e57", "comments": []}, {"number": 49718, "title": "Prevent one more div by 0 in TFLite", "body": "PiperOrigin-RevId: 370800114\nChange-Id: I6b956aeb8c458cc6f514408d2e89ffacfe249e57", "comments": []}, {"number": 49717, "title": "[Kernel C API] Implementation of variable ops RFC.", "body": "The implementation for the Variable Ops RFC.\r\nhttps://github.com/tensorflow/community/blob/master/rfcs/20210504-kernel-extension-variable-ops.md\r\n\r\n@penpornk , @reedwm , @saxenasaurabh , @jzhoulon ", "comments": []}, {"number": 49716, "title": "Prevent one more div by 0 in TFLite", "body": "PiperOrigin-RevId: 370800114\nChange-Id: I6b956aeb8c458cc6f514408d2e89ffacfe249e57", "comments": []}, {"number": 49715, "title": "Prevent one more div by 0 in TFLite", "body": "PiperOrigin-RevId: 370800114\nChange-Id: I6b956aeb8c458cc6f514408d2e89ffacfe249e57", "comments": []}, {"number": 49714, "title": "Prevent another div by 0 in optimized pooling implementations TFLite", "body": "PiperOrigin-RevId: 370800091\nChange-Id: I2119352f57fb5ca4f2051e0e2d749403304a979b", "comments": []}, {"number": 49713, "title": "Prevent another div by 0 in optimized pooling implementations TFLite", "body": "PiperOrigin-RevId: 370800091\nChange-Id: I2119352f57fb5ca4f2051e0e2d749403304a979b", "comments": []}, {"number": 49712, "title": "Prevent another div by 0 in optimized pooling implementations TFLite", "body": "PiperOrigin-RevId: 370800091\nChange-Id: I2119352f57fb5ca4f2051e0e2d749403304a979b", "comments": []}, {"number": 49711, "title": "Prevent another div by 0 in optimized pooling implementations TFLite", "body": "PiperOrigin-RevId: 370800091\nChange-Id: I2119352f57fb5ca4f2051e0e2d749403304a979b", "comments": []}, {"number": 49710, "title": "Fix division by zero in TFLite padding.", "body": "PiperOrigin-RevId: 370777494\nChange-Id: Ic1331e4a1603b9e4c8aa183012a6c8237410aa0f", "comments": []}, {"number": 49709, "title": "Fix division by zero in TFLite padding.", "body": "PiperOrigin-RevId: 370777494\nChange-Id: Ic1331e4a1603b9e4c8aa183012a6c8237410aa0f", "comments": []}, {"number": 49708, "title": "Fix division by zero in TFLite padding.", "body": "PiperOrigin-RevId: 370777494\nChange-Id: Ic1331e4a1603b9e4c8aa183012a6c8237410aa0f", "comments": []}, {"number": 49707, "title": "Fix division by zero in TFLite padding.", "body": "PiperOrigin-RevId: 370777494\nChange-Id: Ic1331e4a1603b9e4c8aa183012a6c8237410aa0f", "comments": []}]