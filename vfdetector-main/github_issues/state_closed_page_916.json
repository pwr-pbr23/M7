[{"number": 25977, "title": "CUDA_ERROR_ILLEGAL_ADDRESS in SoftmaxCrossEntropyWithLogits when run in FP16", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary (dockerhub: `tensorflow/tensorflow:1.13.0rc2-gpu-py3`)\r\n- TensorFlow version (use command below): `b'v1.13.0-rc2-0-gc865ec5621' 1.13.0-rc2`\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.0 / 7.4.1\r\n- GPU model and memory: Quadro GV100, 32GB\r\n\r\n**Describe the current behavior**\r\nThe `SoftmaxCrossEntropyWithLogits` op will hit a `CUDA_ERROR_ILLEGAL_ADDRESS` error when executed in fp16 with certain size inputs. \r\n\r\n**Describe the expected behavior**\r\nNo segfault -- though it's fine if the results are numerically bad.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import gen_nn_ops\r\n\r\nmb = 4005\r\nvocab = 8193\r\n\r\nlogits = tf.placeholder(tf.float16, shape=(mb, vocab))\r\nlabels = tf.placeholder(tf.int32, shape=(mb,))                        \r\ntargets = tf.one_hot(labels, depth=vocab, dtype=tf.float16)\r\n\r\nxentropy, _ = gen_nn_ops.softmax_cross_entropy_with_logits(\r\n    logits, targets)\r\n\r\nlogits_vals = np.random.randn(*logits.shape).astype(np.float16)\r\nlabels_vals = np.random.randint(0, vocab, size=labels.shape)\r\n\r\nwith tf.Session() as sess:\r\n    ret = sess.run(xentropy, {logits: logits_vals,\r\n                              labels: labels_vals})\r\n```\r\n\r\n**Other info / logs**\r\nI've attached a log from cuda-memcheck. There is an illegal write in the relevant eigen kernel.\r\n[memcheck-xentropy.log](https://github.com/tensorflow/tensorflow/files/2890344/memcheck-xentropy.log)\r\n\r\nOther comments:\r\n- This appears to be a recent regression -- I tried the same code on an older TF build (The nvidia 18.12-py3 container) and there was no error.\r\n- This is a hard bug to hit, since `tf.nn.softmax_cross_entropy_with_logits_v2` will coerce its inputs to fp32.\r\n- Many input sizes run fine; I haven't done thorough testing to establish which ones are OK and which aren't.\r\n- As mentioned above, I don't expect numerically good answers in fp16 -- this came about as a result of perf measurements when coercing fp16 everywhere.", "comments": ["@cbcase Could you try it in recent TF1.13.1 version. I don't see any error. Thanks!", "Hi @jvishnuvardhan -- I just pulled `tensorflow/tensorflow:1.13.1-gpu-py3` (image id is `d1c5af36660b`) and got the same error running the above code as a standalone script. Anything else I can provide?", "@cbcase \r\n\r\nCa you try with TF version 1.15 and see if the issue still persists.I tried in colab with TF version 1.15 and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/93f39b53adb0234e72c377306e0bcbd6/untitled121.ipynb).Thanks!", "Closing the issue since the issue was resolved with TF 1.15 version. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25977\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25977\">No</a>\n"]}, {"number": 25976, "title": "summary v2 create_file_writer() bug \"SummaryWriterInterface does not exist\"", "body": "Keeping the description brief as already discussed with @nfelt \r\n\r\nI am getting these errors when logging to tensorboard:\r\n```\r\nNotFoundError: Resource [....] N10tensorflow22SummaryWriterInterfaceE does not exist. [Op:WriteScalarSummary] name: loss/\r\n```\r\nThis is a similar outcome to #25707, but it has a completely different cause (see below).\r\n\r\nWhen using this \"set_as_default\" style:\r\n```\r\n# ... setup code\r\ntf.contrib.summary.create_file_writer(\"/logdir/path/\", flush_millis=2500).set_as_default()\r\n# ... more setup code\r\nfor ibatch in range(iterations):\r\n   # ... training code\r\n   with tf.contrib.summary.record_summaries_every_n_global_steps(FLAGS.tb_every):\r\n      tf.contrib.summary.scalar('loss', loss)\r\n```\r\n\r\nThe problem does not take place when using this style:\r\n```\r\n# ... setup code\r\nwriter = tf.contrib.summary.create_file_writer(\"/logdir/path/\", flush_millis=2500)\r\nwriter.set_as_default()\r\n# ... more setup code\r\nfor ibatch in range(iterations):\r\n   # ... training code\r\n   with tf.contrib.summary.record_summaries_every_n_global_steps(FLAGS.tb_every):\r\n      tf.contrib.summary.scalar('loss', loss)\r\n```\r\n\r\nThis is very counter-intuitive as the writer was never used after, and therefore collapsing the two lines as I did should have not caused any problem!\r\n \r\n\r\n", "comments": ["Just for context, this happens because the underlying resource is deleted when the SummaryWriter python object (returned by `create_file_writer()`) is no longer referenced.  Chaining the `.set_as_default()` method directly means there is no local variable reference to the writer.\r\n\r\nI think the bug here is arguably that `.set_as_default()` only holds onto the underlying resource handle, not the SummaryWriter object itself, which breaks the abstraction that SummaryWriter should map 1:1 to the resource (which is also what #25707 is fixing, though a separate aspect of it)."]}, {"number": 25975, "title": "Avoid double hash lookups in multiple places by using returned status from std::unordered_set<>::insert", "body": "Avoid double hash lookups in multiple places by using returned status from std::unordered_set<>::insert", "comments": []}, {"number": 25974, "title": "Able to access nodes/namespace from a graph for downloaded models", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nFor example, I download a pre-trained model from zoo such as mobilenet. After loading the graph into the session, is there a way to access the higher-level nodes which pertain to layer only. When we access nodes in the graph right now it just gives all the nodes including the ones connected to training and everything. Is there are way to access say only the convolution input/output nodes. \r\nCurrently when you find out the conv weights node and use `node.input` it gives a `mul` node before it. However, i would be looking for the `pooling` layer node or the `activation` node. So is there a way to have a super-view level without the help of tensorboard always. Helps while working on server. Kind of a simple mapping summarizing the model\r\n**Will this change the current api? How?**\r\nyes. the current nodes in the graph_def have attribute `input`. It can be tailored to another attribute say `node.main_input` which gives the next important layer input instead of a low-level op like `mul` or `add`\r\n**Who will benefit with this feature?**\r\nworking on remote servers and not able to pull up tensorboard always. Its painful to port the browser always. \r\n**Any Other info.**\r\nSimilar to model.summary() in Keras which gives the overview of the model\r\n", "comments": ["@prateethvnayak,\r\nSorry for the delayed response. AFIAK, the functionality you are looking for is possible with [tf.keras.applications.mobilenet.MobileNet](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet/MobileNet). Can you please let us know if it helps? Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 25973, "title": "[TF 2.0 API Docs] tf.keras.losses.poisson", "body": "### Existing URLs containing the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses/poisson\r\n\r\n### Description of issue (what needs changing):\r\n* **Correct Links**\r\n  All links are correct.\r\n* **Clear Description**\r\n  Lacks a description.\r\n* **Usage Example**\r\n  No usage example is provided.\r\n* **Parameters Defined**\r\n  Parameters are not defined.\r\n* **Returns Defined**\r\n  Returns are not defined.\r\n* **Raises Listed and Defined**\r\n  Errors are not defined.\r\n* **Visuals, if Applicable**\r\n  No visuals are included, but may not be applicable anyway.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nI'd like to help, yes.\ud83d\ude01\r\n", "comments": ["@duncandean I think we have the correct link ```Defined in python/keras/losses.py.```", "@ymodak I recall Martin Wicke saying something about the link generation script that should be updated. Seems to have been updated now, thanks! I'll edit the issue description. \ud83d\ude42", "Thanks for the fix!"]}, {"number": 25972, "title": "Update cudnn_rnn.py", "body": "Fix the component missing error, it should be tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell instead of tf.contrib.cudnn_rnn.CudnnCompatibleLSTM.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25972) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 25970, "title": "tf.keras computes incorrect loss values with 3+D data", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\nYes. For a minimal example, run \r\n\r\n```\r\nfrom tensorflow import keras\r\n\r\nlayer = keras.layers.Input(shape=(1, 1, 1))\r\nmodel = keras.models.Model(inputs=layer, outputs=layer)\r\nmodel.compile(optimizer='adam', loss='poisson', metrics=['poisson'])\r\ndata = [[[[[1]]], [[[2]]], [[[3]]]]]\r\nmodel.fit(x=data, y=data, batch_size=2, verbose=1, epochs=10)\r\n```\r\nand observe that loss and poisson values are different, and loss values vary:\r\n```\r\nEpoch 1/10\r\n3/3 [==============================] - 1s 236ms/sample - loss: 0.6740 - poisson: 0.4393\r\nEpoch 2/10\r\n3/3 [==============================] - 0s 2ms/sample - loss: 0.6740 - poisson: 0.4393\r\nEpoch 3/10\r\n3/3 [==============================] - 0s 40ms/sample - loss: 0.5452 - poisson: 0.4393\r\nEpoch 4/10\r\n3/3 [==============================] - 0s 96ms/sample - loss: 0.5452 - poisson: 0.4393\r\nEpoch 5/10\r\n3/3 [==============================] - 0s 1ms/sample - loss: 0.9772 - poisson: 0.4393\r\nEpoch 6/10\r\n3/3 [==============================] - 0s 2ms/sample - loss: 0.6740 - poisson: 0.4393\r\nEpoch 7/10\r\n3/3 [==============================] - 1s 201ms/sample - loss: 0.6740 - poisson: 0.4393\r\nEpoch 8/10\r\n3/3 [==============================] - 0s 2ms/sample - loss: 0.6740 - poisson: 0.4393\r\nEpoch 9/10\r\n3/3 [==============================] - 0s 999us/sample - loss: 0.9772 - poisson: 0.4393\r\nEpoch 10/10\r\n3/3 [==============================] - 1s 327ms/sample - loss: 0.9772 - poisson: 0.4393\r\n```\r\n\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n\r\n- TensorFlow installed from (source or binary):\r\n`pip install tensorflow`\r\n- TensorFlow version (use command below):\r\nv1.13.0-rc1-19-gc865ec5621, 1.13.0-rc2\r\n- Python version:\r\n3.7.2 x64\r\n- CUDA/cuDNN version:\r\nn/a\r\n- GPU model and memory:\r\nn/a\r\n\r\n**Describe the current behavior**\r\nWhen fitting a model with `loss=\"poisson\"`, I would expect reported `loss` and `poisson` values to be identical.\r\n\r\n**Describe the expected behavior**\r\n`loss` values are incorrect. They vary from epoch to epoch.\r\n\r\n**Code to reproduce the issue**\r\nSee above.\r\n\r\n**Other info / logs**\r\n\r\nMore code examples and investigations at https://stackoverflow.com/q/54802328/880783\r\n", "comments": ["@bersbersbers are you still seeing this issue? I was not able to repro this on the latest nightly.  ", "@pavithrasv you are right, `tf_nightly-1.13.0.dev20190227` does not have this issue. I can still repro it in `1.13.0rc2` as well as in `1.13.1` which has been released in the meantime. Since the issue reproduces in a stable release, I would be very interested what the underlying cause is and, in particular, how long it has existed.", "Here's my `pip freeze` in my current installation that does repro the issue:\r\n```\r\nabsl-py==0.7.0\r\naltgraph==0.16.1\r\nastor==0.7.1\r\nastroid==2.1.0\r\nastropy==3.1.2\r\nautopep8==1.4.3\r\nawkward==0.8.4\r\nbz2file==0.98\r\ncachetools==3.1.0\r\ncertifi==2018.11.29\r\nchardet==3.0.4\r\ncolorama==0.4.1\r\ncycler==0.10.0\r\ndecorator==4.3.2\r\nfuture==0.17.1\r\ngast==0.2.2\r\ngrpcio==1.18.0\r\nh5py==2.9.0\r\nidna==2.8\r\nimageio==2.5.0\r\nimageio-ffmpeg==0.2.0\r\nisbnlib==3.9.6\r\nisbnlib-dnb==0.0.3\r\nisbntools==4.3.19\r\nisort==4.3.9\r\nKeras==2.2.4\r\nKeras-Applications==1.0.7\r\nKeras-Preprocessing==1.0.9\r\nkiwisolver==1.0.1\r\nlazy-object-proxy==1.3.1\r\nmacholib==1.11\r\nMarkdown==3.0.1\r\nmatplotlib==3.0.2\r\nmccabe==0.6.1\r\nmock==2.0.0\r\nmoviepy==1.0.0\r\nnibabel==2.3.3\r\nnumpy==1.16.1\r\npackaging==19.0\r\npandas==0.24.1\r\npbr==5.1.2\r\npefile==2018.8.8\r\nPillow==5.4.1\r\npip-review==1.0\r\npipdeptree==0.13.2\r\nproglog==0.1.9\r\nprotobuf==3.6.1\r\npsutil==5.5.1\r\npy-essentials==1.4.12\r\npycodestyle==2.5.0\r\npydicom==1.2.2\r\npyhibp==3.0.0\r\nPyInstaller==3.4\r\nPyJWT==1.7.1\r\npylint==2.2.2\r\npyparsing==2.3.1\r\npypng==0.0.19\r\npython-dateutil==2.8.0\r\npytz==2018.9\r\npywin32==224\r\npywin32-ctypes==0.2.0\r\nPyYAML==3.13\r\nrequests==2.21.0\r\nrope==0.12.0\r\nscikit-learn==0.20.2\r\nscipy==1.2.1\r\nseaborn==0.9.0\r\nsix==1.12.0\r\nsklearn==0.0\r\ntee==0.0.3\r\ntensorboard==1.13.0\r\ntensorflow==1.13.1\r\ntensorflow-estimator==1.13.0\r\ntermcolor==1.1.0\r\ntqdm==4.31.1\r\nuproot==3.4.6\r\nuproot-methods==0.4.3\r\nurllib3==1.24.1\r\nWerkzeug==0.14.1\r\nwrapt==1.11.1\r\n```\r\n", "I tried to pin down when this issue was introduced and fixed:\r\n\r\n```\r\nTrue = has bug\r\n\r\ntf-nightly==\r\n1.13.0-dev20190101    True\r\n1.13.0-dev20190124    True\r\n1.13.0-dev20190125    True\r\n1.13.0-dev20190129    False\r\n1.13.0-dev20190206    False\r\n1.13.0.dev20190227    False\r\n1.14.1-dev20190306    False\r\n\r\ntensorflow==\r\n1.10.0        False (Aug 8, 2018)\r\n1.11.0-rc0    True (Sep 13, 2018)\r\n1.11.0        True\r\n1.12.0        True\r\n1.13.0-rc1    True\r\n1.13.1        True (Feb 26, 2019)\r\n2.0.0-alpha0  False (Mar 6, 2019)\r\n```\r\n\r\nSo, introduced some time in Aug/Sep 2018 - due to missing `tf-nightly` packages on `PyPi` from that time, I cannot get any closer. Fixed some time between Jan 25 and 29. That makes about 600 commits:\r\nhttps://github.com/tensorflow/tensorflow/search?q=committer-date%3A2019-01-24..2019-01-30&unscoped_q=committer-date%3A2019-01-24..2019-01-30&type=Commits", " I am closing this issue as it has been fixed. Thank you for digging into the release details!", "This bug is still in the most current version, `1.13.1`. Is there any release scheduled to be released soon to fix this? If not, I would be glad to use a workaround, but so far, I have not found any.\r\n\r\nBy the way, this is a reduced example where the batch size does divide the number of samples:\r\n\r\n```\r\nfrom tensorflow.keras import layers, metrics, models\r\n\r\nlayer = layers.Input(shape=(1, 1, 1))\r\nmodel = models.Model(inputs=layer, outputs=layer)\r\nmodel.compile(optimizer='adam', loss=metrics.mse, metrics=[metrics.mse])\r\nmodel.fit(x=[[[[[0]]], [[[0]]]]], y=[[[[[1]]], [[[1]]]]])\r\n```\r\n\r\nIt outputs:\r\n```\r\n2/2 [==============================]\r\n - 0s 23ms/sample - loss: 2.0000 - mean_squared_error: 1.0000\r\n```\r\n\r\nNote how `loss` and `mean_squared_error` are different. How can I get them to be identical?", "Have you tried https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0-alpha0? We will also have a TF 1.14 release very soon.", "> Have you tried https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0-alpha0? \r\n\r\nYes, I have, see https://github.com/tensorflow/tensorflow/issues/25970#issuecomment-470210090. TF2 does not have that issue, but I don't want my research to rely on Alpha software currently :)\r\n\r\n> We will also have a TF 1.14 release very soon.\r\n\r\nThat is good news, thank you!", "I can confirm that this bug has been fixed in `1.14.0rc0`:\r\n\r\n```\r\nfrom tensorflow import keras\r\nlayer = keras.layers.Input(shape=(1, 1, 1))\r\nmodel = keras.models.Model(inputs=layer, outputs=layer)\r\nmodel.compile(optimizer='adam', loss='poisson')\r\ndata = [[[[[1]]], [[[2]]], [[[3]]]]]\r\nmodel.fit(x=data, y=data, batch_size=2, verbose=2, epochs=10)\r\n```\r\n\r\nOutput:\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0525 11:03:20.369000  6872 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nEpoch 1/10\r\n3/3 - 0s - loss: 0.4393\r\nEpoch 2/10\r\n3/3 - 0s - loss: 0.4393\r\nEpoch 3/10\r\n3/3 - 0s - loss: 0.4393\r\nEpoch 4/10\r\n3/3 - 0s - loss: 0.4393\r\nEpoch 5/10\r\n3/3 - 0s - loss: 0.4393\r\nEpoch 6/10\r\n3/3 - 0s - loss: 0.4393\r\nEpoch 7/10\r\n3/3 - 0s - loss: 0.4393\r\nEpoch 8/10\r\n3/3 - 0s - loss: 0.4393\r\nEpoch 9/10\r\n3/3 - 0s - loss: 0.4393\r\nEpoch 10/10\r\n3/3 - 0s - loss: 0.4393\r\n```\r\n", "Thank you!", "I am using tf 2.1.0 and experience the same problem, can you suggest anything?\r\nThank you!", "I am also getting a delta between mse loss and mse metric values, but only when applying regularization (l2 or dropout). ", "![image](https://user-images.githubusercontent.com/10877157/93761631-ae43b200-fc40-11ea-9913-c1636f45d86b.png)\r\n\r\n+1", "I have the same problem with tensorflow 2.3.0 when using l1/l2 regularization.", "I'm having the same issue with TensorFlow GPU 2.1.0 and **no regularization**. However, this happens only on the validation step.", "@oO0oO0oO0o0o00  I have the same strange problem.", "same problem"]}, {"number": 25969, "title": "Is it mandatory to use same version of Tensorflow for TFLite conversion", "body": "**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version:3.6.5\r\n- Installed using virtualenv\r\n- Bazel version (if compiling from source):0.21.0\r\n- GCC/Compiler version (if compiling from source):5.4.0\r\n\r\n\r\n**Problem description**\r\nI have trained a custom model using Tensorflow 1.9 binary. Now I want to convert graph to tflite. Is it mandatory to use same  Tensorflow 1.9 for this conversion or can I use the latest version of TF ?\r\n", "comments": ["I don't think it should be an issue. As long as it contains supported ops, the model should convert just fine. Let us know if you have any issues :-)", "Hi @srjoglekar246. Thank you for your confirmation."]}, {"number": 25968, "title": "Removed unused code from bigtable_lib.cc", "body": "Warning fix:\r\ntensorflow/contrib/bigtable/kernels/bigtable_lib.cc:24:8: warning: variable 'grpc_code' set but not used [-Wunused-but-set-variable]", "comments": ["already addressed in #25916 ", "Thank you @joyalbin for investigating this! Fortunately @siju-samuel has the fix in #25916 (as he pointed out above). As a result, I'm going to close this PR.\r\n\r\nAll the best,\r\n-Brennan\r\n"]}, {"number": 25967, "title": "Fix for future incompatible changes in Bazel", "body": "Certain API methods in Bazel are being deprecated:\r\n\r\n  * `ctx.action()` -> `ctx.actions.run()`\r\n  * `attr.label(allow_files = True, single_file = True)` -> `attr.label(allow_single_file = True)`\r\n  * `ctx.template_action()` -> `ctx.actions.expand_template()`\r\n\r\nThe PR is created by `find -name *.bzl | xargs buildifier --lint=fix --warnings=attr-non-empty,attr-single-file,ctx-actions,output-group`", "comments": []}, {"number": 25966, "title": "Compilation error when building tfcompile on Windows 10 (TF 1.13-rc2)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: checkout tag v1.13-rc2\r\n- Python version: 3.5.6\r\n- Installed using virtualenv? pip? conda?: -\r\n- Bazel version (if compiling from source): 0.21\r\n- GCC/Compiler version (if compiling from source): VS 2015 14.0.25431.01 Update 3\r\n- CUDA/cuDNN version: 10.0/7.3.1\r\n- GPU model and memory: GTX 1080, 8GBs\r\n\r\n**Describe the problem**\r\nThis is a direct clone of issue #24218 as per request of @jvishnuvardhan. Except the code and CUDA version, everything else stated there applies here as well. Below the same information provided in the referenced issue, updated to the current configuration:\r\n\r\nChecked out the release 1.13-rc2 branch of the TF repo, configured bazel (configuration is below) and ran \r\n`bazel build -c opt --config=opt //tensorflow/compiler/aot:tfcompile`\r\n\r\nCompilation fails at `.\\tensorflow/compiler/tf2xla/cpu_function_runtime.h(71): error C2338`. Full error message:\r\n\r\n```\r\nERROR: C:/tf/tensorflow/compiler/tf2xla/BUILD:98:1: C++ compilation of rule '//tensorflow/compiler/tf2xla:cpu_function_runtime' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/filippo/_bazel_filippo/rbrahckg/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Filippo/Anaconda3/envs/tf_build/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Filippo/Anaconda3/envs/tf_build/lib/site-packages\r\n    SET TEMP=C:\\Users\\Filippo\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0,6.1,7.5\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\Filippo\\AppData\\Local\\Temp\r\n  C:/Users/Filippo/Anaconda3/envs/tf_build/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX2 /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/tf2xla/_objs/cpu_function_runtime/cpu_function_runtime.o /c tensorflow/compiler/tf2xla/cpu_function_runtime.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n.\\tensorflow/compiler/tf2xla/cpu_function_runtime.h(71): error C2338:\r\nTarget //tensorflow/compiler/aot:tfcompile failed to build\r\n```\r\n\r\nIt's still the same static assert failing: \r\n```\r\nstd::pair<uint64, uint64> Encode() const {\r\n    static_assert(sizeof(*this) == 16, \"\"); //<<<<<<<<<<<<<<<<< \r\n    uint64 upper = Pack(kind(), size_);\r\n    uint64 lower = entry_param_number_;\r\n    return {upper, lower};\r\n  }\r\n```\r\n**Other info / logs**\r\n\r\nBazel config:\r\n\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"C:/Users/Filippo/Anaconda3/envs/tf_build/python.exe\"\r\nbuild --action_env PYTHON_LIB_PATH=\"C:/Users/Filippo/Anaconda3/envs/tf_build/lib/site-packages\"\r\nbuild --python_path=\"C:/Users/Filippo/Anaconda3/envs/tf_build/python.exe\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --config=xla\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\"\r\nbuild --action_env TF_CUDA_VERSION=\"10.0\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"5.0,6.1,7.5\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild:opt --copt=/arch:AVX2\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --config monolithic\r\nbuild --copt=-w --host_copt=-w\r\nbuild --verbose_failures\r\nbuild --distinct_host_configuration=false\r\nbuild --experimental_shortened_obj_file_path=true\r\nbuild --define=override_eigen_strong_inline=true\r\nbuild:v2 --define=tf_api_version=2\r\n```", "comments": ["@GPhilo I could see commit https://github.com/tensorflow/tensorflow/commit/bb479118c9fed7be0186f98a3939ec28b162f2fe might have fixed the issue. It is not part of `rc2` though.", "@yongtang thanks for the update, will the commit be included in 1.13?", "@GPhilo Don't know if the commit will be picked up by the final version of 1.13.0. The rc2 is very close to the final release as far as I know.", "@GPhilo,\r\n\r\nAs we see the issue is related to an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to latest stable version of tensorflow `2.6.0`, Can you try to build from source using the [guide](https://www.tensorflow.org/install/source_windows) and also refer the [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/tfcompile.bzl). Let us know if it works fine. Thanks!", "This is definitely a very old issue, I can't really try to build tensorflow on Windows at the moment (nor would I have a reason too), but either way I'm sure the build issues would be different from those I had with version 1.13, so I'll close the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25966\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25966\">No</a>\n"]}, {"number": 25965, "title": "build failed on v1.13.0-rc1 with Geforce RTX 2080 card on Ubuntu 18.04", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution Linux Ubuntu 18.04:\r\n- TensorFlow installed from source:\r\n- TensorFlow version: v1.13.0-rc1\r\n- Python version: python3.6\r\n- Build from source:\r\n- Bazel version (if compiling from source): 0.22.0\r\n- GCC/Compiler version (if compiling from source):7.3.0\r\n- CUDA/cuDNN version: 10.0/7.3\r\n- GPU model and memory: Geforce RTX 2080/8G\r\n\r\n\r\n\r\n**Describe the problem**\r\nAfter configure the build on NVidia Geforce Card\r\nIt failed when I run\r\n` build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nbuild failed.\r\nLast message is\r\n`ERROR: /home/windy/work/opensource/tensorflow/tensorflow/core/kernels/BUILD:762:1: C++ compilation of rule '//tensorflow/core/kernels:broadcast_to_op' failed (Exit 4): crosstool_wrapper_dri\r\nver_is_not_gcc failed: error executing command\r\n  (cd /home/windy/.cache/bazel/_bazel_windy/7f7b18d654824f59399fafbf60350ed2/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/usr/local/lib:/usr/local/cuda/extras/CUPTI/lib64: \\\r\n    PATH=/home/windy/install/bin:/home/windy/bin::/home/windy/.local/bin:/usr/local/cuda/bin:/usr/local/bin:/home/windy/install/bin:/home/windy/bin::/usr/local/cuda/bin:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/core/kernels/_objs/broadcast_to_op/broadcast_to_op.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/kernels/_objs/broadcast_to_op/broadcast_to_op.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -iquote . -iquote bazel-out/host/genfiles -iquote bazel-out/host/bin -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/host/genfiles/external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/curl -iquote bazel-out/host/genfiles/external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/genfiles/external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/genfiles/external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/genfiles/external/aws -iquote bazel-out/host/bin/external/aws -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/double_conversion -isystem bazel-out/host/genfiles/external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/genfiles/external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/genfiles/external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/genfiles/external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -msse3 -pthread '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -c tensorflow/core/kernels/broadcast_to_op.cc -o bazel-out/host/bin/tensorflow/core/kernels/_objs/broadcast_to_op/broadcast_to_op.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 203.325s, Critical Path: 132.09s\r\nINFO: 644 processes: 644 local.\r\nFAILED: Build did NOT complete successfully\r\n`", "comments": ["The minimum bazel version can be 0.19 and maximum is 0.21 for TF 1.13 release candidates.\r\nAlso TF 1.13 comes with prebuilt binaries to support cuda 10\r\nCan you please switch to above acceptable bazel versions and try again?", "@ymodak,\r\nIt's fixed. not caused by bazel versions. It's caused by running out of memory in jvm.\r\n\r\nBefore you close this bug, Could you share the link for TF1.13 binary with cuda10?\r\n\r\nDetails: \r\nHW configuration\r\nCPU: Intel(R) Core(TM) i7-7820X\r\nMem: 16G\r\nDisk: 1T SSD\r\nGPU: RTX 2080\r\n\r\nI change the bazel version check between 0.19 and 0.22. Today I tried downgrade to 0.21 still have the similar problem in build.  Even clean everything and restart the build still failed somewhere. I also found each time before build failed, the system is hung there without any response.\r\n\r\nHow to fix:\r\nIt's similar to https://github.com/tensorflow/tensorflow/issues/9651. Ubuntu 18.04 default swap memory size is 2G. looks not enough for tensorflow build. Follow instructions \r\nhttps://linuxize.com/post/how-to-add-swap-space-on-ubuntu-18-04/\r\nto enlarge the swap-memory to 16G and restart computer. Tensorflow build passed.\r\n\r\nThanks,\r\nWind\r\n\r\n\r\n\r\n", "Here is the link for [TF1.13 binary with cuda10](https://github.com/tensorflow/tensorflow/tree/v1.13.0-rc2)\r\nThanks for sharing the info. Closing since its fixed.", "thanks."]}, {"number": 25964, "title": "Warning fix split_handler_ops.cc", "body": "Warning fix:\r\ntensorflow/contrib/boosted_trees/kernels/split_handler_ops.cc:541:13: warning: variable 'best_dimension_idx' set but not used [-Wunused-but-set-variable]\r\ntensorflow/contrib/boosted_trees/kernels/split_handler_ops.cc:573:21: warning: unused variable 'dimension_id' [-Wunused-variable]", "comments": ["This already addresses it https://github.com/tensorflow/tensorflow/pull/25915"]}, {"number": 25963, "title": "Updated the value to be Const", "body": "Changed the value to be const", "comments": ["@rthadur, I don't know very much about Toco (which is owned by the TF Lite team). Can you find a more fitting reviewer, please?", "@shahzadlone , i have rebased and merged the changes, can you pls approve again.", "@rthadur , i have rebased this code, and updated again, can you pls update the label to review requested.", "Closing the PR as similar is already merged.", "Closing the PR as similar is already merged."]}, {"number": 25962, "title": "Fix warning eigen_spatial_convolutions.h", "body": "Fixed warning due to extra semicolon\r\n./tensorflow/core/kernels/eigen_spatial_convolutions.h(835): warning: extra \";\" ignored\r\n./tensorflow/core/kernels/eigen_spatial_convolutions.h(1042): warning: extra \";\" ignored\r\n./tensorflow/core/kernels/eigen_spatial_convolutions.h(1257): warning: extra \";\" ignored\r\n", "comments": []}, {"number": 25961, "title": "Warning with custom categorical column with custom weights", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: MacOS 10.12\r\n- TensorFlow installed from source\r\n- TensorFlow version: v1.12\r\n- Python version: 3.6.6\r\n\r\n**Describe the current behavior**\r\n\r\nA `_CategoricalColumn` is represented with 2 `SparseTensor`s: one for indexes and one for weights. It seems all implementation of Tensorflow feature columns use the default `None` weight `SparseTensor`. However, in my project I need to assign custom weights to a feature column of type `_CategoricalColumn` and it seems we then systematically obtain the following warning:\r\n```python\r\ntensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\n```\r\nI implemented a toy example of such behaviour:\r\n\r\n```python\r\nimport collections\r\n\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.feature_column.feature_column import _CategoricalColumn\r\n\r\n\r\nclass _DebuggingCategoricalColumn(\r\n    _CategoricalColumn,\r\n    collections.namedtuple('_DebuggingCategoricalColumn', (\r\n        'dense_col',\r\n    ))):\r\n\r\n    @property\r\n    def name(self):\r\n        return 'debugging_weighted'\r\n\r\n    @property\r\n    def _parse_example_spec(self):\r\n        config = self.dense_col._parse_example_spec  # pylint: disable=protected-access\r\n        return config\r\n\r\n    @property\r\n    def _num_buckets(self):\r\n        return 4\r\n\r\n    def _transform_feature(self, inputs):\r\n        v = inputs.get(self.dense_col)\r\n        batch_size = tf.to_int64(tf.shape(v)[0])\r\n        buckets = tf.to_int64(tf.squeeze(math_ops.bucketize(input=v, boundaries=(0, 2, 4))))\r\n        indices = tf.stack([\r\n            tf.range(batch_size, dtype=tf.int64),\r\n            tf.zeros(batch_size, dtype=tf.int64)], axis=1)\r\n        id_tensor = tf.SparseTensor(\r\n            indices=indices,\r\n            values=tf.to_int64(tf.squeeze(buckets)),\r\n            dense_shape=[batch_size, 1]\r\n        )\r\n        id_weights = tf.SparseTensor(\r\n            indices=indices,\r\n            values=tf.ones(batch_size, dtype=tf.float32),\r\n            dense_shape=[batch_size, 1]\r\n        )\r\n        return id_tensor, id_weights\r\n\r\n    def _get_sparse_tensors(self, inputs, weight_collections=None, trainable=None):\r\n        tensors = inputs.get(self)\r\n        return _CategoricalColumn.IdWeightPair(tensors[0], tensors[1])\r\n\r\n\r\ndef debugging_column(input_column):\r\n    return _DebuggingCategoricalColumn(dense_col=input_column)\r\n\r\n\r\ndf = pd.DataFrame({'x': [-1., 1, 3, 5], 'y': [1., 1, 2, 3]})\r\nfeat = tf.feature_column.numeric_column('x')\r\nfeature_columns = [debugging_column(feat)]\r\ninput_fn = tf.estimator.inputs.pandas_input_fn(df, df['y'], shuffle=True, num_epochs=None)\r\nestimator = tf.estimator.LinearRegressor(feature_columns=feature_columns)\r\nestimator.train(input_fn=input_fn, steps=100)\r\n```\r\n\r\nRunning this code leads to the following warning:\r\n```python\r\ntensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\n```\r\n\r\nImportantly, replacing line `return id_tensor, id_weights` by line `return id_tensor, None` (default value for uniform weights) removes the warning.\r\n\r\nAm I doing something wrong with the custom `_CategoricalColumn` or is there an issue in TF internals? Thanks for your help\r\n", "comments": ["@plagree This is a stale issue with TF1.x. \r\n\r\nLooks like this was resolved with recent `TF1.15.5`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/56f3a9bf5f7a519ae23ee6730a68675c/untitled.ipynb) \r\n\r\nI am closing this issue as this was resolved in the recent TF versions. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25961\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25961\">No</a>\n"]}, {"number": 25960, "title": "Cannot find cudnn.h under ~ but I really have it there !", "body": "\r\n**System information**\r\n- OS Platform and Distribution : Redhat 7.5 ppc64le\r\n- TensorFlow installed from source\r\n- TensorFlow version: v1.12.0\r\n- Python version: 3.6.5  (anaconda3 v5.2)\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source):  4.8.5\r\n- CUDA/cuDNN version: 9.2/7.4.2\r\n- GPU model and memory: V100 16GB\r\n\r\n**Describe the problem**\r\n\r\nI am trying to build tensorflow r1.12 using bazel 0.15 on Redhat 7.5 ppc64le.\r\n\r\nI am stuck with the following error.\r\n\r\n    [u0017649@sys-97184 tensorflow]$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \r\n    ...\r\n    ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package \r\n    '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \r\n    \"/home/u0017649/files/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1447\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \r\n    \"/home/u0017649/files/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1187, in \r\n    _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \r\n    \"/home/u0017649/files/tensorflow/third_party/gpus/cuda_configure.bzl\", line 911, in _get_cuda_config\r\n                _cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n        File \r\n    \"/home/u0017649/files/tensorflow/third_party/gpus/cuda_configure.bzl\", line 582, in _cudnn_version\r\n                _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\r\n       File \r\n    \"/home/u0017649/files/tensorflow/third_party/gpus/cuda_configure.bzl\", line 869, in \r\n    _find_cudnn_header_dir\r\n                auto_configure_fail((\"Cannot find cudnn.h under %s\" ...))\r\n        File \r\n    \"/home/u0017649/files/tensorflow/third_party/gpus/cuda_configure.bzl\", line 317, in auto_configure_fail\r\n                fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\n    Cuda Configuration Error: Cannot find cudnn.h under /usr/local/cuda-9.2/targets/ppc64le-linux/lib\r\n\r\n\r\nI do have a soft link for cudnn.h under /usr/local/cuda-9.2/targets/ppc64le-linux/lib as below.\r\n\r\n    [u0017649@sys-97184 tensorflow]$ ls -l /usr/local/cuda-9.2/targets/ppc64le-linux/lib/cudnn.h\r\n    lrwxrwxrwx. 1 root root 57 Feb 20 10:15 /usr/local/cuda-9.2/targets/ppc64le-linux/lib/cudnn.h -> /usr/local/cuda-9.2/targets/ppc64le-linux/include/cudnn.h\r\n\r\n\r\nAny comments, pls ?\r\n", "comments": ["After reading tensorflow/third_party/gpus/cuda_configure.bzl, I could solve this by the following.  \r\n\r\n$ sudo ln -sf /usr/local/cuda-9.2/targets/ppc64le-linux/include/cudnn.h  /usr/include/cudnn.h\r\n", "Thanks for sharing the workaround. Closing this issue since its resolved. Thanks!"]}, {"number": 25959, "title": "[TF 2.0 API Docs] tf.image.non_max_suppression", "body": "### Existing URLs containing the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/non_max_suppression#aliases\r\n\r\n### Description of issue (what needs changing):\r\n\r\n* **Correct Links**\r\n  https://github.com/tensorflow/tensorflow/blob/master/python/ops/image_ops_impl.py is incorrect. The correct link should be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py.\r\n\r\n* **Clear Description**\r\n  The description is not opinionated about when to use this symbol and unclear how arguments effect the functionality.\r\n\r\n* **Usage Example**\r\n  Usage example is mixed into the description, not formatted and not self-contained\r\n\r\n* **Parameters Defined**\r\n  Parameters are defined correctly with expected formats.\r\n\r\n* **Returns Defined**\r\n  Returns are defined correctly with expected formats.\r\n\r\n* **Raises Listed and Defined**\r\n  Errors are not defined.\r\n\r\n* **Visuals, if Applicable**\r\n  No visuals are included.", "comments": ["@RajeshThallam The link on the url seems to be working with the correct link.", "Hi @RajeshThallam @dynamicwebpaige - do you mind if I take this ?", "@scouvreur , please go ahead.", "> @scouvreur , please go ahead.\r\n\r\nMaybe I don't understand the issue properly but the link seems to be working correctly. Am I missing something?", "@Ayush517 , links were incorrect when the issue was reported. There was a commit that fixed the link. I think it may be the [doc generator](https://github.com/tensorflow/docs/blob/094df83829d17ba582c275772373bb1eee498713/tools/tensorflow_docs/api_generator/generate_lib.py)", "Got it @Ayush517 @RajeshThallam - you can close this issue then", "I just checked the links and they seem to be working properly..", "Closing the issue as docs are updated."]}, {"number": 25958, "title": "The performance of SparseSoftmaxCrossEntropyWithLogits in inference is very low", "body": "SparseSoftmaxCrossEntropyWithLogits computes softmax cross entropy cost and gradients to backpropagate. \r\nWhen SparseSoftmaxCrossEntropyWithLogits used in inference, for example, in language models, we only need the cost. \r\nSparseSoftmaxCrossEntropyWithLogits is very time-consuming in language models, it is better to add a flag for inference and training, or to split this OP to two.\r\n\r\n**Describe the current behavior**\r\nProfile data of language model:\r\nnode name | requested bytes | total execution time | accelerator execution time | cpu execution time\r\n**SparseSoftmaxCrossEntropyWithLogits     245.76KB (100.00%, 0.00%),      1.58sec (100.00%, 51.51%),             0us (0.00%, 0.00%),      1.58sec (100.00%, 51.51%)**\r\nMatMul                      8905.28MB (100.00%, 88.05%),       1.10sec (48.49%, 36.00%),             0us (0.00%, 0.00%),       1.10sec (48.49%, 36.00%)\r\nAdd                                 84B (11.95%, 0.00%),       201.16ms (12.49%, 6.57%),             0us (0.00%, 0.00%),       201.16ms (12.49%, 6.57%)\r\nSplit                          671.09MB (11.95%, 6.64%),        105.97ms (5.92%, 3.46%),             0us (0.00%, 0.00%),        105.97ms (5.92%, 3.46%)\r\n\r\n**Describe the expected behavior**\r\nProfile data after optimization:\r\nnode name | requested bytes | total execution time | accelerator execution time | cpu execution time\r\nMatMul                      8905.28MB (100.00%, 88.05%),      1.11sec (100.00%, 46.86%),             0us (0.00%, 0.00%),      1.11sec (100.00%, 46.86%)\r\n**SparseSoftmaxCrossEntropyWithLogits      245.76KB (11.95%, 0.00%),      871.49ms (53.14%, 36.93%),             0us (0.00%, 0.00%),      871.49ms (53.14%, 36.93%)**\r\nAdd                                 84B (11.95%, 0.00%),       200.13ms (16.21%, 8.48%),             0us (0.00%, 0.00%),       200.13ms (16.21%, 8.48%)\r\nSplit                          671.09MB (11.95%, 6.64%),        106.76ms (7.73%, 4.52%),             0us (0.00%, 0.00%),        106.76ms (7.73%, 4.52%)\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): el7\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):1.12\r\n- Python version:3.7.2\r\n- Bazel version (if compiling from source):0.19.2\r\n- GCC/Compiler version (if compiling from source):gcc6.3\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nhttps://github.com/mlperf/inference/tree/master/cloud/language_modeling\r\n./benchmark.py\r\n\r\n", "comments": ["Hi @greg1232, do you have some comments about this optimization? Thanks.", "@azaks2 do you know who's a good person to look at this?", "@mpjlu,\r\nSorry for the delayed response. With so many updates in the **`Tensorflow Code`** and with the release of **`Subsequent Versions`**, I'm pretty confident that this performance issue should be resolved. If possible for you, can you please check it with the **`Latest Tensorflow Version (2.5)`**?\r\nIf that is not possible, please provide the reproducible code (the link you provided is not working), so that we can check it.\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 25957, "title": "keras model.compile: allow user-specific label shapes/dtypes for loss/metrics", "body": "## System information\r\n- TensorFlow version: 1.12 (also present in 1.13, 2.0)\r\n## Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n## Describe the feature and the current behavior/state.\r\n`tf.keras.models.Model.compile` currently has no way of knowing the shape/size of targets that will be passed to it during `Model.fit` (and other main looping methods). Instead it creates placeholders on a 1-to-1 basis corresponding to the same dtype and number of dimensions as each output. This is a safe assumption for most regression tasks, but leads to redundant casting/reshaping for even simple classification problems. More importantly, it leads to very confusing behaviour, loss/metric functions which need to cater for a wider variety of inputs than necessary and an inability to do proper error checking.\r\n\r\nFor example, in 2.0 `tf.keras.backend.sparse_categorical_crossentropy`'s documentation claims the first argument is `target: An integer tensor.` However, there's absolutely nothing stopping you from providing float values. This is because the implementation needs to be able to accept arbitrarily sized float values when passed placeholders during `compile`, then simply flattens and casts the values (which has no affect on inputs already of the correct shape/type).\r\n\r\nThis is particularly confusing for users trying to implement their own loss function. Mimicing the advertised interface will result in errors.\r\n\r\nThis isn't just a documentation issue however: if inappropriate float values are passed in (i.e. values that aren't the result of casting integers to floats) this will be very difficult to pick up.\r\n\r\nIf users are asked to provide `loss` and `metrics` functions, they will generally know the shape and types of the labels. Allowing them to specify this so their loss/metric functions only need to cater for these removes unecessary reshaping and casting.\r\n\r\n## Will this change the current api? How?\r\nI don't understand why loss/metric functions are called during compile with placeholders rather than being lazily evaluated during `fit`. From my perspective that would be the thing to change, resulting in no changes to the API. However, it may result in too much of a breaking change in other niche cases.\r\n\r\nLacking that, adding extra kwargs to `Model.compile - maybe `target_shapes`, `target_sizes` (`targets` is already taken) would be a non-breaking change. Default values can be created using the current assumptions.\r\n\r\n## Who will benefit with this feature?\r\n- People writing custom losses/metrics with the expectation that `y_true` shape/dtype will match that of the values in their labels.\r\n- Anyone training with models using unecessary casts/reshapes (probably not a major performance issue).\r\n", "comments": ["@jackd,\r\nSorry for the delayed response. \r\n\r\n**`Shape`** and **`Data Types`** of **`Loss`** are clearly specified in the [Documentation of Model.Compile](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile).\r\n\r\nAlso, the documentation of [Sparse Categorical Cross Entropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy#args) no longer states that **`First Argument should be an Integer Tensor`**.\r\n\r\nThanks!"]}, {"number": 25956, "title": "Use losses.deserialize() and metrics.deserialize() instead of convert_custom_objects()", "body": "...instead of `convert_custom_objects()` (which becomes unused, and since it is a local function, I removed it).\r\n\r\nThis also required changing `deserialize_keras_object()` to support lists as well.\r\n\r\nFixes #25938", "comments": ["@ageron could you please address review comments, thank you ", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "Yes, I'm sorry about that, I had no time at all in the last few weeks. I'm traveling now, so still no time, but I might revive this later on when I come back (unless someone else wants to do it).", "This problem still not solve on TF version 2.1", "I echo HossainAminur: TF 2.1 still has the same issue. Custom losses with hyperparameters are kind of common, and it's crippling that we can't save them with our models..."]}, {"number": 25955, "title": "keras `Model.compile` with loss/metrics dict and multiple outputs from same layer", "body": "## System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- Python version: ('v1.12.0-7354-gedbd8a15b9', '1.13.0-dev20190203') - also present in 2.0 preview\r\n- CUDA/cuDNN version: 10.0 / 7\r\n- GPU model and memory: quadro k620\r\n\r\n## Current Behaviour\r\nKeras' `model.compile` with dict losses matches provided loss functions with outputs passed into the constructor via each output's _layer name_. This is not unique in the case where multiple model outputs come from the same layer.\r\n\r\n## Expected Behaviour\r\nI would expect usages of lists/dicts to be consistent between losses, metrics, labels and outputs - i.e. if any are lists, they must all be lists of the same length, and if any are dicts they must all be dicts with the same set of keys. Requiring output layers to be named consistently with the key in the corresponding labels dictionary seems completely bamboozling to me, e.g. if building a hybrid model with a classification output and 'class_index' key in the labels one would have to name the final layer 'class_index' even if it outputs logits/softmax activations.\r\n\r\nThis is likely a breaking change. At the very least I would expect an error to be raised if `output_names` are not unique and users pass in dictionaries for either `loss` or `metrics` in `Model.fit`.\r\n\r\n## Code to Reproduce\r\n```python\r\nimport tensorflow as tf\r\n# network that maps 1 input to 2 separate outputs\r\nx = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\r\nx2 = tf.keras.layers.Dense(2)(x)\r\ny, z = tf.keras.layers.Lambda(\r\n    lambda x2: tf.unstack(x2, axis=-1), name='x2_unstack')(x2)\r\n# y = tf.keras.layers.Lambda(tf.identity, name='y')(y)\r\n# z = tf.keras.layers.Lambda(tf.identity, name='z')(z)  # current work-around\r\nmodel = tf.keras.models.Model(inputs=x, outputs=dict(y=y, z=z))\r\n\r\nprint(model.outputs)\r\n# [\r\n#  <tf.Tensor 'x2_unstack/unstack:0' shape=(None,) dtype=float32>,\r\n# <tf.Tensor 'x2_unstack/unstack:1' shape=(None,) dtype=float32>\r\n# ]\r\n# somewhat unexpected as not the same as the value passed to constructor, but ok...\r\nprint(model.output_names)\r\n# ['x2_unstack', 'x2_unstack']\r\n# surely that's going to cause issues later...\r\n\r\nmodel.compile('SGD', loss={'x': 'mse', 'y': 'logcosh'})\r\n# ValueError: Unknown entry in loss dictionary: y.\r\n# Only expected following keys: ['x2_unstack', 'x2_unstack']\r\n\r\nmodel.fit(x=x_data, y=dict(y=y_data, z=z_data))  # expected way of using fit\r\n```\r\n\r\n", "comments": ["Is this still an issue with latest version TF 2.4? Thanks!", "Looks all good now."]}, {"number": 25954, "title": "Issue with training using gpu on docker (slow to start and in general as well I believe, same speed as CPU)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\nNO, stock example script: tensorflow-tutorials/basic_classification.ipynb (in docker)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): not installed (docker)\r\n- TensorFlow version (use command below): b'v1.13.0-rc1-0-g54c5616308' 1.13.0-rc1 (in docker)\r\n- Python version: N/A I believe\r\n- Bazel version (if compiling from source): N/a\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version:\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart_static.a\r\n\r\n- GPU model and memory:\r\n== nvidia-smi ===================================================\r\nThu Feb 21 00:47:44 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro M1200        Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   34C    P0    N/A /  N/A |    518MiB /  4043MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nIn the introductory script model.fit(train_images, train_labels, epochs=5) begins training 4 minutes after it is run with this log ```tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally```\r\nIt also only trained at a rate of 60 us/sample which was equal to the speed of training using my CPU (different docker image)\r\n\r\n**Describe the expected behavior**\r\n\r\nmodel.fit(train_images, train_labels, epochs=5) begins training when it is run and trains at a faster speed\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nsudo docker run --name tensor-flow -it -u $(id -u):$(id -g) --runtime=nvidia -v $(realpath ~/notebooks):/tf/notebooks -p 8888:8888 tensorflow/tensorflow:latest-py3-jupyter bash\r\n```\r\nin docker bash - 'jupyter notebook --ip 0.0.0.0'\r\nThen, run the notebook in tensorflow-tutorials/basic_classification.ipynb\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nlatest-gpu-py3-jupyter\r\nStarted running\r\n\r\n```\r\n[I 05:40:40.213 NotebookApp] Adapting to protocol v5.1 for kernel b722a15a-97d0-4d5e-8a53-553c8fb58065\r\n2019-02-20 05:41:18.670612: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-02-20 05:41:18.800083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-02-20 05:41:18.800916: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5b93e80 executing computations on platform CUDA. Devices:\r\n2019-02-20 05:41:18.800955: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Quadro M1200, Compute Capability 5.0\r\n2019-02-20 05:41:18.819111: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2019-02-20 05:41:18.819805: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5bfdc50 executing computations on platform Host. Devices:\r\n2019-02-20 05:41:18.819834: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-02-20 05:41:18.820045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: Quadro M1200 major: 5 minor: 0 memoryClockRate(GHz): 1.148\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.27GiB\r\n2019-02-20 05:41:18.820069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-02-20 05:41:18.820972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-20 05:41:18.820986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-02-20 05:41:18.820993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-02-20 05:41:18.821119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3053 MB memory) -> physical GPU (device: 0, name: Quadro M1200, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2019-02-20 05:45:22.499611: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\n```\r\n\r\nTook this long to start model.fit, but cpu was on high loads for those 4 minutes (gpu memory was being used by python but utilization was low until it started training when utilization went to 50%), kernel then freezes on shut down (need to ctrl-c twice, this happens for all kernels including cpu only)\r\n\r\nlatest-gpu-jupyter does the same thing\r\n\r\n```\r\nAdapting to protocol v5.1 for kernel 0d53ac42-e696-4671-8595-c884aa8088d3\r\n2019-02-20 05:53:49.471477: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-02-20 05:53:49.535506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-02-20 05:53:49.536440: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x64416f0 executing computations on platform CUDA. Devices:\r\n2019-02-20 05:53:49.536467: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Quadro M1200, Compute Capability 5.0\r\n2019-02-20 05:53:49.554965: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2019-02-20 05:53:49.555436: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x64ab4a0 executing computations on platform Host. Devices:\r\n2019-02-20 05:53:49.555454: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-02-20 05:53:49.555645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: Quadro M1200 major: 5 minor: 0 memoryClockRate(GHz): 1.148\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.25GiB\r\n2019-02-20 05:53:49.555663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-02-20 05:53:49.556601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-20 05:53:49.556620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-02-20 05:53:49.556629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-02-20 05:53:49.556768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3031 MB memory) -> physical GPU (device: 0, name: Quadro M1200, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2019-02-20 05:57:51.253642: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\n```\r\n\r\n\r\nlatest-py3-jupyter is just as fast as gpu versions (60 us/sample) Quadro M1200 vs intel 7700HQ, it is possible that the GPU is not working in the above examples (there is some other reason for the increase in GPU utilisation) or maybe the speed really is just equivalent\r\n\r\n```\r\n[I 05:59:23.763 NotebookApp] Adapting to protocol v5.1 for kernel 02073f3e-f863-42c8-9454-4596c4f0e382\r\n2019-02-20 06:00:03.976951: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-02-20 06:00:03.998969: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2019-02-20 06:00:03.999527: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5391b50 executing computations on platform Host. Devices:\r\n2019-02-20 06:00:03.999549: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n```\r\n", "comments": ["How can you solved this issue? I have similar problem when train keras model on google colab gpu.", "I had the same problem and, in my case, it depends on how I run docker:\r\n- if I run it indicating the user, then there is a very long delay before training starts\r\n- if I run it without indicating the user, then the delay before training happens only at the first launch, while it starts immediately in the subsequent launches\r\n\r\n**Slow**\r\nIf i run the container indicating the user:\r\n`docker run -u $(id -u):$(id -g) --rm --name tf13_container --runtime=nvidia -it -v /home/user/project:/opt/project tensorflow/tensorflow:1.13.1-gpu bash`\r\nVery long delay before training, every time:\r\n`docker exec -it -w /opt/project tf13_container python -u -m trainer/task`\r\n\r\n**Fast**\r\nIf i run the container without indicating the user:\r\n`docker run --rm --name tf13_container --runtime=nvidia -it -v /home/user/project:/opt/project tensorflow/tensorflow:1.13.1-gpu bash`\r\nVery long delay before training, just the first time:\r\n`docker exec -it -w /opt/project tf13_container python -u -m trainer/task`\r\nImmediate start of training in the case of subsequent launches (without stopping the container):\r\n`docker exec -it -w /opt/project tf13_container python -u -m trainer/task`\r\n\r\nThe trick of reusing the same container without turning it off is shown in the official documentation but it works only if the container is launched without indicating the user."]}, {"number": 25953, "title": "Fix failing pyzmq installation", "body": "While building latest TensoFlow and during installing it's dependencies on a `CentOS 7` docker, I get the following error:\r\n```\r\nCollecting pyzmq>=17 (from notebook->jupyter)\r\n  Downloading https://files.pythonhosted.org/packages/64/8d/78975da77627fd863c08e8ea3c7cebce7e51bed2936be5118de6b0050638/pyzmq-18.0.0.tar.gz (1.2MB)\r\n    Complete output from command python setup.py egg_info:\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"/tmp/pip-install-rv3k_yvp/pyzmq/setup.py\", line 1241, in <module>\r\n        long_desc = f.read()\r\n      File \"/usr/lib64/python3.4/encodings/ascii.py\", line 26, in decode\r\n        return codecs.ascii_decode(input, self.errors)[0]\r\n    UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 29: ordinal not in range(128)\r\n    \r\n    ----------------------------------------\r\n\u001b[91mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-install-rv3k_yvp/pyzmq/\r\n\u001b[0m\u001b[91mYou are using pip version 18.1, however version 19.0.3 is available.\r\nYou should consider upgrading via the 'pip install --upgrade pip' command.\r\n```", "comments": ["Actually this is only effecting CentOS containers I'm building and won't apply here.\r\nClosing the PR."]}, {"number": 25952, "title": "MKL Link error libiomp5md.lib library is corrupt", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.13.0rc1\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.22\r\n- GCC/Compiler version (if compiling from source): visual studio 17\r\n- CUDA/cuDNN version: 7.4.2\r\n- GPU model and memory: titan xp, 12GB ram\r\n\r\nWhen I run ```bazel build --config=opt --config=mkl --linkopt=\"/FORCE:MULTIPLE\" -k //tensorflow:libtensorflow.so```\r\n\r\nI get this error at link time:\r\n```\r\nC:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/HostX64/x64/link.exe /nologo /DLL /SUBSYSTEM:CONSOLE -defaultlib:advapi32.lib -DEFAULTLIB:advapi32.lib -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 /MACHINE:X64 /FORCE:MULTIPLE @bazel-out/x64_windows-opt/bin/tensorflow/libtensorflow.so-2.params\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lpthread'; ignored\r\nexternal\\mkl_windows\\lib\\libiomp5md.lib : fatal error LNK1127: library is corrupt\r\n```\r\n\r\nIs there any workaround?\r\n\r\n**Any other info / logs**\r\nI have mkl 2019-update 2 installed and set in my PATH env.", "comments": ["Hi, \r\nThe mkl library on windows is currently experiencing several issues at the time of linking and a patch with a fix will be made available either this week or next week. Please try again when we have the new library released. Also, I see that your VS version is 2017 instead of 2015. Please ensure that the necessary installs are available as per the doc available here https://www.tensorflow.org/install/source_windows", "#25213", "@eloi-loomai The issue should be resolved now, please try the resolution provided in the above issue", "@preethivenkatesh I'm also encountering this issue with more recent versions of Bazel (at least 0.25.3) and Visual Studio 2017. It looks like the trick that resolved issue #25213 doesn't work anymore. Bazel uses `/WHOLEARCHIVE` regardless.", "Actually, this was the same as issue #25213 after all. The patch wasn't getting applied properly. With it, Bazel doesn't use `/WHOLEARCHIVE` and no errors occur with Visual Studio 2017 either.", "closing as this issue is fixed", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25952\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25952\">No</a>\n"]}, {"number": 25951, "title": "TFTRT: Allow ops to specify their supported datatypes", "body": "`AllowDataTypes()` is a helper function to allow each converter to specify which datatypes are supported. Previously MatMul and BiasAdd had a similar check, but now every converter uses this new standard helper function.\r\n\r\nOther minor enhancement included in this PR:\r\n* Use CreateBroadcastableScalarConstant for ConvertSquare\r\n* Remove redundant input check in ConvertTopK\r\n* Use CheckInputsWeights for ConvertLeakyRelu", "comments": []}, {"number": 25950, "title": "TFLite GPU crashes when not all ops are supported by delegate?", "body": "**System information**\r\n- I have custom code, however GPU Delegate part I have used as described in here provided sample\r\n- Ubuntu 18.04\r\n- Samsung Galaxy J5 2015 (crashes), Galaxy S7 and LG G6 (works fine), \r\n- TF build from 'org.tensorflow:tensorflow-lite:0.0.0-gpu-experimental'\r\n\r\n**Describe the current behavior**\r\nUsing TF Lite with GPU developer preview, when I run inference on my mobilenetV2 retrained model (with two outputs) on most devices it works nice. However on Galaxy J5 (odler device), it crashes with:\r\n    \r\n        E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: bazinac.aplikacenahouby, PID: 12367\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: WARNING: op code #40 cannot be handled by this delegate.  Only the first 67 ops will run on the GPU, and the remaining 10 on the CPU.GpuDelegate Prepare: No EGL error, but eglCreatePbufferSurface failedNode number 77 (GpuDelegate) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)_\r\n\r\n**Describe the expected behavior**\r\nI would expect that when _isGpuDelegateAvailable()_ method provides only such a delegate, that is capable preparing and running all the ops. It should return false otherwise, I guess. Is there some way how to tell this in advance, or should I manually check for these kind of internal errors and try to fall back by recreating Interpret without GPU support? \r\n\r\n**Code to reproduce the issue**\r\n`    protected void runInference() {\r\n\r\n        Log.d(TAG, \"Inference starts \");\r\n        Object[] inputArray = {imgData};\r\n\r\n        Map<Integer, Object> outputMap = new HashMap<>();\r\n        outputMap.put(0, outp1);\r\n        outputMap.put(1, outp2);\r\n        tflite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n        Log.d(TAG, \"Inference ends \");\r\n    }`\r\n\r\n**Other info / logs**\r\nThank you guys for your effort to make this available to us, you are doing very good job!\r\n", "comments": ["@bazinac \r\n\r\n`isGpuDelegateAvailable` essentially only checks whether AAR is available.  It is not very safe to assume that function is foolproof.  In fact, there can be various other reasons why GPU initialization might fail.  We are trying hard to improve that method though.\r\n\r\nre: if the whole graph cannot be run, return false, is against the current philosophy of delegates API.  The idea is to provide a seamless experience to the user, so that, if GPU delegate cannot be applied, it will be covered by the CPU automatically.  It has pros and cons.  Pros is, if the last couple of ops are not covered by the GPU, it will automagically work.  The cons is, that the users miss the WARNING message, and think the graph is executed on the GPU, but it was actually run on the CPU.  In that case, you don't see any speedup whatsoever.  We definitely need a better way of notifying the user, rather than just a WARNING message in the logs.\r\n\r\nre: the PBbuffer surface thing.  The OpenGL-based backend needs to create a GL context and keep it around.  We try various things to create a GL context.  First, we try to create a configless context.  If that fails, we try to created a surfaceless context.  If that fails, the final step is to create a PBuffer context.  If even that failed, we cannot proceed.  Unfortunately, your Galaxy J5 doesn't seem to pass this context creation phase.   I'm not 100% certain, but according to:\r\n\r\nhttps://compubench.com/device.jsp?benchmark=compu20&os=Android&api=rs&D=Samsung+Galaxy+J5+%28SM-J500x%29&testgroup=info\r\n\r\nGalaxy J5 2015 only seems to have GL ES 3.0.  The requirement for our GL-based backend is GL ES 3.1.  Could you please check whether you have GL ES 3.1 available on your J5?", "Hi guys, thanks for providing extensive info on the topic. \r\n\r\nYou are right, this J5 has GL ES 3.0 only, which is why it does not run on it. Maybe it would be good idea to throw some warning about required version inavailability right at the TF init time (when GPU options are passed). Thanks for expanation!"]}, {"number": 25949, "title": "[TF_Java] adding  whileLoop method for the Java API", "body": "Co-authored-by: Irene Dea <irenedea@fb.com>\r\nCo-authored-by: Samantha Andow <samdow@fb.com>", "comments": ["This commit adds an all-in-one while loop method for the Java API, per discussion [here](https://github.com/tensorflow/tensorflow/pull/24842#discussion_r250655928)\r\n\r\ncc @skye @asimshankar @karllessard \r\n\r\n", "/CC @sjamesr ", "I have no experience with this code. Please redirect.", "@sjamesr are you able to review this?", "Yes, thank you. I just got added to the tensorflow org but forgot to assign this to me."]}, {"number": 25948, "title": "Raspberry pi - Go Tensorflow - OOM allocating tensor issue", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n1. Code in Golang\r\n2. Used GO Tensorflow\r\n3. MobileNet model \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nRaspberry Pi (1.4GHz, 1024 RAM, 4 Cores)\r\n- TensorFlow version (use command below):\r\nv1.12.0\r\n\r\n**Describe the current behavior**\r\nI have written a Face Classification code which uses MobileNet (depth 80) model. But When I ran the model after 8-10 min, tensorflow is throwing OOM allocation error.\r\n**Describe the expected behavior**\r\nBecause raspberry pi has limited memory. But as the model is very small it has to run perfectly.\r\n**Code to reproduce the issue**\r\n1. Run MobileNet model classification in Golang Env\r\n2. Input tensor shape [5,224,224,3], tried other batch sizes as well 1 and 3 but no luck.\r\n**Other info / logs**\r\n```\r\n2019-02-20 10:52:26.403200: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at fused_batch_norm_op.cc:573 : Resource exhausted: OOM when allocating tensor with shape[5,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n2019-02-20 10:52:26.478760: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at pad_op.cc:137 : Resource exhausted: OOM when allocating tensor with shape[5,113,113,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\nE0220 10:52:26.483588    3282 classifier.go:203] Failed to classify face: OOM when allocating tensor with shape[5,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[{{node mobilenet_1.00_224/conv_pw_1_bn/FusedBatchNorm}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\n\r\n```\r\n", "comments": ["@asimshankar Can you PTAL? Thanks!", "Can anybody help me here?  Could not figure out how to handle this error?\r\n", "latest log\r\n```\r\nE0321 17:42:11.720840     471 classifier.go:204] Failed to classify face: OOM when allocating tensor with shape[2,113,113,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[{{node mobilenet_1.00_224/conv_pad_2/Pad}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\nE0321 17:42:16.627385     471 classifier.go:204] Failed to classify face: OOM when allocating tensor with shape[2,113,113,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[{{node mobilenet_1.00_224/conv_pad_2/Pad}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\nE0321 17:42:27.929953     471 classifier.go:204] Failed to classify face: OOM when allocating tensor with shape[2,113,113,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[{{node mobilenet_1.00_224/conv_pad_2/Pad}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n```\r\n\r\nIs this because of floating and shape [2,113,113,64]? I am trying out the quantization of model?\r\nI will update on this.", "From the description, it looks like this is a duplicate of #25554 ? Closing for now, but please reopen with more information if I'm misunderstanding."]}, {"number": 25947, "title": "tf.python.keras.utils.Sequence hangs fit_generator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):  v1.12.0-8400-g7f0bf2ef4f 1.13.0-dev20190220\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10/ 7.4.2\r\n- GPU model and memory: 2x 980Ti\r\n\r\n**Describe the current behavior**\r\n- I've made a data generator that inherits from Sequence\r\n-  calling fit_generator on it hangs if and only if use_multiprocessing=True.  It hangs in training_generator.py in`_get_next_batch `on the line `generator_output = next(generator)`\r\n**Describe the expected behavior**\r\n- Doesn't hang when using use_multiprocessing=True.\r\n\r\nI haven't narrowed it down to the bare minimum code required to reproduce - if this issue isn't explainable with the information above I'll try to cut down the code to the minimum that reproduces so that I can submit it here.\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]