[{"number": 6754, "title": "Add comment on stripping model of unused Op", "body": "Describe how to strip DecodeJpeg Op in V3 Inception model", "comments": ["Can one of the admins verify this patch?", "Thanks!", "Jenkins, test this please."]}, {"number": 6753, "title": "Fix expand_dim docs.", "body": "@itsmeolivia @wolffg ", "comments": ["Error log:\r\n\r\n```\r\n18:16:28         Start 160: C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/training/input_test.py\r\n18:16:33 160/187 Test #160: C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/training/input_test.py ......................................***Failed    5.50 sec\r\n18:16:33 .........................C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/training/input_test.py:412: DeprecationWarning: Please use assertEqual instead.\r\n18:16:33   self.assertEquals([1, 11, 2, 22, 3, 33], l)\r\n18:16:33 ..........C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/training/input_test.py:39: ResourceWarning: unclosed file <_io.TextIOWrapper name='C:\\\\windows\\\\TEMP\\\\input_test\\\\match_filenames.0' mode='w' encoding='cp1252'>\r\n18:16:33   open(name, \"w\").write(\"Some contents\")\r\n18:16:33 C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/training/input_test.py:39: ResourceWarning: unclosed file <_io.TextIOWrapper name='C:\\\\windows\\\\TEMP\\\\input_test\\\\match_filenames.1' mode='w' encoding='cp1252'>\r\n18:16:33   open(name, \"w\").write(\"Some contents\")\r\n18:16:33 C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/training/input_test.py:39: ResourceWarning: unclosed file <_io.TextIOWrapper name='C:\\\\windows\\\\TEMP\\\\input_test\\\\match_filenames.2' mode='w' encoding='cp1252'>\r\n18:16:33   open(name, \"w\").write(\"Some contents\")\r\n18:16:33 F.............................\r\n18:16:33 ======================================================================\r\n18:16:33 FAIL: test (__main__.MatchFilenamesOnceTest)\r\n18:16:33 ----------------------------------------------------------------------\r\n18:16:33 Traceback (most recent call last):\r\n18:16:33   File \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/training/input_test.py\", line 49, in test\r\n18:16:33     self.assertItemsEqual(map(tf.compat.as_bytes, filenames), star.eval())\r\n18:16:33 AssertionError: Element counts were not equal:\r\n18:16:33 \r\n18:16:33 Diff is 5300 characters long. Set self.maxDiff to None to see it.\r\n```", "Hmm, this is an issue that crept into our builds we do not know when.\r\nin r1.0 branch it went away after we synced to head.\r\nSo I think this is probably caused by one of our dependencies.\r\n\r\nBut since we will not build a new pip package from this branch, it is OK (also this is a doc only change)", "For reference, here is the failure in r1.0 branch:\r\nhttp://ci.tensorflow.org/view/Release/job/release-win/19/DEVICE=cpu,OS=windows/console"]}, {"number": 6752, "title": "Matrix Vector multiply not parallelized.", "body": "### What is the problem?\r\n\r\nMatrix vector multiply is not parallelized. Please see my example code. No matter how I change the intra_op_parallelism_threads, the running time is always similar. I used \"top\" to confirm that only one thread was used. However, the parallel speedups for square matrix matrix multiply are quite noticeable. Again, \"top\" confirmed that multiple threads were used.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04 \r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2e22f1b20fdfa77b1332c518617391dc32359c5b\r\n2. The output of `bazel version`\r\n0.4.3\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\nn = 10000\r\n\r\n#approach 1:\r\n\r\nmatrix1 = tf.constant(np.ones(n*n), shape = [n,n])\r\nmatrix2 = tf.constant(np.ones(n*1), shape = [n,1])\r\n\r\nproduct1 = tf.matmul(matrix1, matrix2)\r\n\r\nstart = time.time()\r\nsess = tf.Session(config=tf.ConfigProto(\r\n    inter_op_parallelism_threads=1,\r\n    intra_op_parallelism_threads=12))\r\nsess.run(product1)\r\nend = time.time()\r\nprint('\\n Approach 1 took: %s%%' % (end - start))\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Here's a better benchmark that removes the cost of data transfer. It seems CPU usage is indeed quite low and it won't break 6 G ops/sec on Xeon. However, this problem is bandwidth limited so you are going to spend most of the time transferring data between RAM and CPU, so I'm not sure if this performance can be improved much. @benoitsteiner \r\n\r\n\r\n```\r\nimport os\r\nimport sys\r\n#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\r\nimport tensorflow as tf\r\nimport time\r\n\r\nn = 8*8192\r\nwith tf.device(\"/cpu:0\"):\r\n    matrix1 = tf.ones((n, n))\r\n    matrix2 = tf.ones((n, 1))\r\n    product1 = tf.matmul(matrix1, matrix2)\r\n\r\ngraph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0))\r\nsess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=int(sys.argv[1]), graph_options=graph_options))\r\n\r\n# pre-warming\r\nsess.run(product1.op)\r\n\r\nstart = time.time()\r\nsess.run(product1.op)\r\nend = time.time()\r\nops = n**2 + n*(n-1) # n*(n-1) additions, n^2 multiplications\r\nelapsed = (end - start)\r\nrate = ops/elapsed/10**9\r\nprint('\\n Approach 1 took: %.2f sec, %.2f G ops/sec' % (elapsed, rate,))\r\n\r\n```", "Did you also observe that the operation is not parallelized by Eigen? Please note that this is a different question from whether the operation has high throughput.", "I can't tell if it's parallelized or not. I think the goal should be performance, if higher performance is not possible with parallelization, then it shouldn't be parallelized.", "What kind of performance are expecting/needing for this operation?", "I agree that performance should be the goal, but I actually want to get a confirmation about whether TF parallelizes matrix vector multiply.", "Not sure who handles CPU Eigen implementations, assigned to martin to reassign", "OK, there do seem to be some missing optimization opportunities here.\r\n\r\nFirst of all, matrix*vector matmul doesn't seem to be parallelized. Doing nxn by nx1 multiply uses one CPU core and runs at about 5 G ops/sec.\r\n\r\nFor some comparative experiments, note that this operation can be equivalently written as tf.mul+tf.reduce_sum, or simply as reduce_sum(...,axis=1) since our vector is all ones\r\n\r\nSome experiments of equivalent reformulations with 16 threads and n=8*8192\r\n\r\ntf.matmul(matrix, vector) -- utilizes 1 core, 5 G ops/sec\r\ntf.reduce_sum(matrix, axis=0) -- utilizes all cores, 5 G ops/sec\r\ntf.reduce_sum(matrix, axis=1) -- utilizes all cores, 26 G ops/sec\r\ntf.reduce_sum(tf.mul(matrix, vector), axis=1) -- utilizes all cores, 0.6 G ops/sec\r\n\r\n\r\n@learyg -- is this something that XLA optimization would help speed up?", "Yeah, I believe @meheffernan had some significant matrix-vector speedups in his queue at some point, not sure we found the priority to finish landing them at the time. +@eliben ", "It is correct that the Eigen implementation deliberately falls back to a sequential implementation for matrix-vector multiplication. The reason is that there are specialized sequential matvec kernels in core Eigen that are approximately 4x faster than the matmul kernel called by the parallelized tensor contraction code used for matmul in TensorFlow.\r\n\r\nIn other words, if using the matmul kernel, you'd typically have to burn >4x CPU cycles to match the performance of the sequential kernels. Based on some real-world use cases we decided to stick with this and rely on model-level parallelism. \r\n\r\nI agree that there is room for improvement and that it would be awesome if XLA could contribute better code for this.\r\n\r\nTop-level logic in tensorflow (batch_matmul is the new matmul):\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/batch_matmul_op_impl.h#L212\r\n\r\nmatvec kernels: https://bitbucket.org/eigen/eigen/src/e6d6c87e1e27ba39a8f4484366ead7d31e6e97a4/Eigen/src/Core/products/GeneralMatrixVector.h?at=default&fileviewer=file-view-default#GeneralMatrixVector.h-31\r\n\r\nmatmul block panel kernels:\r\nhttps://bitbucket.org/eigen/eigen/src/e6d6c87e1e27ba39a8f4484366ead7d31e6e97a4/Eigen/src/Core/products/GeneralBlockPanelKernel.h?at=default&fileviewer=file-view-default\r\n", "I should also mention that @benoitsteiner  and others have been working on new low level kernels based on libxsmm. I believe the current focus is to improve matmul performance on Haswell and newer processors. But, once the code is in, we should probably re-evaluate the situation for matvec as well.", "Great. Thanks.", "@rmlarsen @benoitsteiner Did the new libxsmm kernels ever happen? Can matrix-vector multiplies be made to run in parallel, even at the expense of total throughput?\r\n\r\nI'm trying to speed up a CPU workload (GPT2 generation for https://github.com/AIDungeon/AIDungeon/issues/58), and it looks like a bunch of MatMul operations that a GPU would churn through are still running single-threaded on my multi-core CPU. Even if doing the multiplies across multiple cores wasted a lot of CPU, for this application (an interactive text adventure running on a user's desktop machine), it's worth burning CPU and reducing throughput to achieve lower latency."]}, {"number": 6751, "title": "Branch 144020788", "body": "", "comments": ["Will re-open with py3 fix."]}, {"number": 6750, "title": "Error building TensorFlow within Docker Container", "body": "I am following the instructions for using TensorFlow serving from https://tensorflow.github.io/serving/serving_inception.html\r\nand after I clone tf within the container and build it with `./configure`\r\n\r\nI get the following errors\r\n\r\n```\r\nERROR: /serving/tensorflow/tensorflow/workspace.bzl:375:3: no such package '@junit_jar//jar': Error downloading [https://github.com/junit-team/junit4/releases/download/r4.12/junit-4.12.jar] to /root/.cache/bazel/_bazel_root/5071e8dca1385fb776f72b33971bf157/external/junit_jar/junit-4.12.jar: java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty and referenced by '//external:junit'.\r\n\r\nERROR: /serving/tensorflow/tensorflow/workspace.bzl:375:3: no such package '@junit_jar//jar': Error downloading [https://github.com/junit-team/junit4/releases/download/r4.12/junit-4.12.jar] to /root/.cache/bazel/_bazel_root/5071e8dca1385fb776f72b33971bf157/external/junit_jar/junit-4.12.jar: java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty and referenced by '//external:junit'.\r\n\r\nERROR: Evaluation of query \"deps(... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\r\n```\r\n\r\nand the `...` in the last error contains about 200 lines of additional text", "comments": ["Looks like this is an issue in tensorflow serving docker image.\r\n@kirilg @jharmsen Could you take a look?", "Can you try running `update-ca-certificates -f`?", "@kirilg I tried that.  There was no change", "Hmm, not sure. This the the general workaround fix for these java.security.InvalidAlgorithmParameterException errors that worked for us before. Did that command succeed and you still see the error? Can you also try:\r\n`sudo apt-get install ca-certificates-java`\r\n`sudo update-ca-certificates -f`", "Ok working now after I re-tried a few times.  My hunch is this was due to a spotty network connection - maybe something got downloaded incorrectly"]}, {"number": 6749, "title": "sparse_placeholder no longer accepts python ints in shape argument", "body": "Hi,\r\n\r\nAfter recently updating tensorflow, sparse_placeholder stopped working correctly.  It appears that the shape argument must now be int64 in order for tensorflow to convert the shape to a tensor, so the following fails:\r\n```\r\nph = tf.sparse_placeholder(dtype=tf.float32, shape=(50, 10000))\r\n```\r\nwith error message:\r\n```\r\nValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(\"Const:0\", shape=(2,), dtype=int32)'\r\n```\r\nThis is inconsistent with the behavior of tf.placeholder, for which: \r\n```\r\nph = tf.placeholder(dtype=tf.float32, shape=(50, 10000))\r\n```\r\nsucceeds. \r\n\r\nThanks,\r\nShawn\r\n", "comments": ["Maybe @mrry can comment on what might be an issue with the `tf.sparse_placeholder` shape API.", "Looks like the problem is in `array_ops._normalize_sparse_shape()`. It converts its argument to a tensor, which for a list of Python `int` values will result in a `tf.int32` tensor. I suspect the correct answer is to add a `dtype=dtypes.int64` to the `ops.convert_to_tensor()` call, but since there doesn't appear to be any unit test for this I'll defer to @ilblackdragon, who originally added that function.", "simple workaround:\r\n\r\n`shape = [1, 3]`\r\n`shape = np.array(shape, dtype=np.int64)`\r\n`tf.sparse_placeholder(tf.int32, shape=shape)`", "Any update on this issue? @ilblackdragon \r\nIn my experience the workaround does __not__ work because then it complains about:\r\n\r\n`ValueError: Tensor Tensor(\"Input_2/shape_1:0\", shape=(2,), dtype=int64) may not be fed.`\r\n\r\nIn other words: it doesn't want to do Tensor conversion itself, but it also doesn't accept to be fed with the `int64` shape Tensor.", "I also seem to be having the same problem.  Has there been any progress on this issue?", "The workaround is all well and good\r\n\r\n```\r\n>> foo = tf.sparse_placeholder(tf.float32, shape=np.array([10, 47], dtype=np.int64))\r\n>> foo.get_shape()\r\nTensorShape([Dimension(10), Dimension(47)])\r\n```\r\n\r\n.... until you want a variable dimension\r\n\r\n```\r\n>> bar = tf.sparse_placeholder(tf.float32, shape=[None, 47])\r\n>> bar.get_shape()\r\nTensorShape(None)\r\n>> bar2 = tf.sparse_placeholder(tf.float32, shape=np.array([None, 47], dtype=np.int64))\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'\r\n```", "Marking this as contributions welcome. @ilblackdragon please let me know if you plan to fix this.", "Added a PR #11153 with test cases."]}, {"number": 6748, "title": "Got an unexpected keyword argument error when I call tf.contrib.layers.convolution2d on Tensorflow for Windows", "body": "When I invoke tf.contrib.layers.convolution2d the tensorflow execution terminates with an error about one of the parameters used\r\n\r\nthe call is\r\n\r\n```\r\nlayer_one = tf.contrib.layers.convolution2d(\r\n    float_image_batch,\r\n    num_output_channels=32,     \r\n    kernel_size=(5,5),          \r\n    activation_fn=tf.nn.relu,\r\n    weight_init=tf.random_normal,\r\n    stride=(2, 2),\r\n    trainable=True)\r\n```\r\n\r\nI have tried to fix according new documentation using tf.random_normal_init but get always the same problem\r\n\r\n\r\nOS: Windows 10 x64\r\nTensorflow 0.12.0 \r\n\r\nRelated Stackoverflow question: http://stackoverflow.com/questions/41539658/tensorflow-error-when-i-try-to-use-tf-contrib-layers-convolution2d", "comments": ["Looking at the documentation here:\r\nhttps://www.tensorflow.org/api_docs/python/contrib.layers/higher_level_ops_for_building_neural_network_layers_#convolution2d\r\n\r\n- weight_init is not a keyword, it has to be weights_initializer\r\n- num_output_channels is not a keyword, maybe you mean num_outputs\r\n\r\nPlease make sure you read the documentation carefully. Python wont accept any abbreviations or any similar keywords."]}, {"number": 6747, "title": "Update release notes for 1.0.0-alpha.", "body": "", "comments": []}, {"number": 6746, "title": "Branch 143989623", "body": "", "comments": []}, {"number": 6745, "title": "Tensor flow just stopped working all of a sudden", "body": "My new to tensorflow and I was following the tutorial on tensorflow for poets to train an image classifier on a Docker image of gcr.io/tensorflow/tensorflow:latest-devel and I got it up and running but then yesterday I told it to classify an image that it has classified before and it threw an error now when im trying to retain it I keep getting this error\r\n\r\n\r\nLooking for images in 'tensorflow'\r\nNo files found\r\nLooking for images in 'tools'\r\nNo files found\r\nLooking for images in 'git'\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 1012, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 757, in main\r\n    FLAGS.validation_percentage)\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 148, in create_image_lists\r\n    file_list.extend(gfile.Glob(file_glob))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 269, in get_matching_files\r\n    compat.as_bytes(filename), status)]\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: git", "comments": ["This seems more appropriate to stack overflow (this list is for reproducible bugs in tensorflow + feature requests)", "ya, your right"]}, {"number": 6744, "title": "Bus error (core dumped) when importing TensorFlow ", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n- [Issue 2626](https://github.com/tensorflow/tensorflow/issues/2626)\r\n- [Issue 3366](https://github.com/tensorflow/tensorflow/issues/3366)\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: 8.0, 5.1.5\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```-rw-r--r-- 1 root root   558720 Jan  8 20:20 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Jan  8 20:20 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Jan  8 20:20 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 Jan  8 20:20 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Jan  8 20:20 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 61062656 Jan  8 20:35 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 61062656 Jan  8 20:35 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 61062656 Jan  8 20:35 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n```\r\n\r\nIf installed from binary pip package, provide: *did not install from pip package*\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`): `ec7929b878926c39255254e9aea992f0bc65aa68`\r\n2. The output of `bazel version`: \r\n```\r\nBuild label: 0.4.3\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Dec 22 12:31:25 2016 (1482409885)\r\nBuild timestamp: 1482409885\r\nBuild timestamp as int: 1482409885\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\n>>> import tensorflow\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally\r\nBus error (core dumped)\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\nI have not attempted solutions because I know of none -- it seems like there is a bus error when loading the driver, and I'm not sure how to proceed\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nNot sure if the output of the core dump is going to be useful or where to find it. Help would be appreciated. \r\n", "comments": ["Can you run it under gdb and show stack trace?\r\n\r\nit\r\n\r\n```\r\ncat import tensorflow > test.py\r\n^D\r\ngdb python\r\nrun test.py\r\nbt\r\n```", "Although, `bus error` looks weird, the only time I've seen it was when trying to read from NFS and network was flaky", "Hey, here is the stack trace: \r\n\r\n```\r\n(gdb) run test.py\r\nStarting program: /usr/bin/python test.py\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x7ffff3b8a700 (LWP 24061)]\r\n[New Thread 0x7ffff1389700 (LWP 24062)]\r\n[New Thread 0x7fffeeb88700 (LWP 24063)]\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally\r\n\r\nThread 1 \"python\" received signal SIGBUS, Bus error.\r\nmemset () at ../sysdeps/x86_64/multiarch/../memset.S:78\r\n78\t../sysdeps/x86_64/multiarch/../memset.S: No such file or directory.\r\n(gdb) bt\r\n#0  memset () at ../sysdeps/x86_64/multiarch/../memset.S:78\r\n#1  0x00007ffff7dddf98 in _dl_map_segments (loader=0x126a560, has_holes=<optimized out>, maplength=<optimized out>, nloadcmds=2, loadcmds=<optimized out>, type=<optimized out>,\r\n    header=<optimized out>, fd=<optimized out>, l=0x126a560) at ./dl-map-segments.h:127\r\n#2  _dl_map_object_from_fd (name=name@entry=0x7fffffffbcc0 \"libcudnn.so.5\", origname=origname@entry=0x0, fd=<optimized out>, fbp=fbp@entry=0x7fffffffb420,\r\n    realname=<optimized out>, loader=loader@entry=0x0, l_type=2, mode=-1879048190, stack_endp=0x7fffffffb418, nsid=0) at dl-load.c:1223\r\n#3  0x00007ffff7ddfa57 in _dl_map_object (loader=0x0, loader@entry=0xd73a80, name=name@entry=0x7fffffffbcc0 \"libcudnn.so.5\", type=type@entry=2, trace_mode=trace_mode@entry=0,\r\n    mode=mode@entry=-1879048190, nsid=<optimized out>) at dl-load.c:2476\r\n#4  0x00007ffff7dec3a7 in dl_open_worker (a=a@entry=0x7fffffffb9e0) at dl-open.c:237\r\n#5  0x00007ffff7de7394 in _dl_catch_error (objname=objname@entry=0x7fffffffb9d0, errstring=errstring@entry=0x7fffffffb9d8, mallocedp=mallocedp@entry=0x7fffffffb9cf,\r\n    operate=operate@entry=0x7ffff7dec300 <dl_open_worker>, args=args@entry=0x7fffffffb9e0) at dl-error.c:187\r\n#6  0x00007ffff7debbd9 in _dl_open (file=0x7fffffffbcc0 \"libcudnn.so.5\", mode=-2147483646,\r\n    caller_dlopen=0x7fffe72dc7ba <tensorflow::internal::LoadLibrary(char const*, void**)+26>, nsid=-2, argc=<optimized out>, argv=<optimized out>, env=0x9b20b0) at dl-open.c:660\r\n#7  0x00007ffff75edf09 in dlopen_doit (a=a@entry=0x7fffffffbc10) at dlopen.c:66\r\n#8  0x00007ffff7de7394 in _dl_catch_error (objname=0xa42160, errstring=0xa42168, mallocedp=0xa42158, operate=0x7ffff75edeb0 <dlopen_doit>, args=0x7fffffffbc10) at dl-error.c:187\r\n#9  0x00007ffff75ee571 in _dlerror_run (operate=operate@entry=0x7ffff75edeb0 <dlopen_doit>, args=args@entry=0x7fffffffbc10) at dlerror.c:163\r\n#10 0x00007ffff75edfa1 in __dlopen (file=<optimized out>, mode=<optimized out>) at dlopen.c:87\r\n#11 0x00007fffe72dc7ba in tensorflow::internal::LoadLibrary(char const*, void**) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#12 0x00007fffe72db6e7 in tensorflow::(anonymous namespace)::PosixEnv::LoadLibrary(char const*, void**) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#13 0x00007fffe70f12ba in perftools::gputools::internal::DsoLoader::GetDsoHandle(tensorflow::StringPiece, void**, perftools::gputools::internal::DsoLoader::LoadKind) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#14 0x00007fffe70f1ed9 in perftools::gputools::internal::DsoLoader::GetCudnnDsoHandle(void**) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#15 0x00007fffe70efd64 in std::_Function_handler<tensorflow::Status (void**), tensorflow::Status (*)(void**)>::_M_invoke(std::_Any_data const&, void**&&) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#16 0x00007fffe70f02a3 in perftools::gputools::internal::CachedDsoLoader::FetchHandleResult(std::function<tensorflow::Status (void**)>) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#17 0x00007fffe70f080c in perftools::gputools::internal::CachedDsoLoader::GetCudnnDsoHandle() ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#18 0x00007fffe70c3f6c in perftools::gputools::initialize_cudnn() () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#19 0x00007ffff7de74ea in call_init (l=<optimized out>, argc=argc@entry=2, argv=argv@entry=0x7fffffffe4b8, env=env@entry=0x9b20b0) at dl-init.c:72\r\n#20 0x00007ffff7de75fb in call_init (env=0x9b20b0, argv=0x7fffffffe4b8, argc=2, l=<optimized out>) at dl-init.c:30\r\n#21 _dl_init (main_map=main_map@entry=0xd73a80, argc=2, argv=0x7fffffffe4b8, env=0x9b20b0) at dl-init.c:120\r\n#22 0x00007ffff7dec712 in dl_open_worker (a=a@entry=0x7fffffffc900) at dl-open.c:575\r\n#23 0x00007ffff7de7394 in _dl_catch_error (objname=objname@entry=0x7fffffffc8f0, errstring=errstring@entry=0x7fffffffc8f8, mallocedp=mallocedp@entry=0x7fffffffc8ef,\r\n    operate=operate@entry=0x7ffff7dec300 <dl_open_worker>, args=args@entry=0x7fffffffc900) at dl-error.c:187\r\n#24 0x00007ffff7debbd9 in _dl_open (file=0x7fffeabaef54 \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\", mode=-2147483390,\r\n    caller_dlopen=0x5231c3 <_PyImport_GetDynLoadFunc+243>, nsid=-2, argc=<optimized out>, argv=<optimized out>, env=0x9b20b0) at dl-open.c:660\r\n#25 0x00007ffff75edf09 in dlopen_doit (a=a@entry=0x7fffffffcb30) at dlopen.c:66\r\n#26 0x00007ffff7de7394 in _dl_catch_error (objname=0xa42160, errstring=0xa42168, mallocedp=0xa42158, operate=0x7ffff75edeb0 <dlopen_doit>, args=0x7fffffffcb30) at dl-error.c:187\r\n#27 0x00007ffff75ee571 in _dlerror_run (operate=operate@entry=0x7ffff75edeb0 <dlopen_doit>, args=args@entry=0x7fffffffcb30) at dlerror.c:163\r\n#28 0x00007ffff75edfa1 in __dlopen (file=<optimized out>, mode=<optimized out>) at dlopen.c:87\r\n#29 0x00000000005231c3 in _PyImport_GetDynLoadFunc ()\r\n#30 0x0000000000522d6f in _PyImport_LoadDynamicModule ()\r\n#31 0x00000000005c7175 in ?? ()\r\n#32 0x00000000004c468a in PyEval_EvalFrameEx ()\r\n#33 0x00000000004c9d8f in PyEval_EvalFrameEx ()\r\n```", "That seems consistent with something wrong with your file-system, perhaps try copying all the CUDA .so files to another directory", "alright, I copied it into a directory as follows:\r\n```\r\nubuntu@ip-172-31-44-171:~/cuda$ cp /usr/local/cuda/lib64/libcud* .\r\nubuntu@ip-172-31-44-171:~/cuda$ export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:$(pwd)\"\r\nubuntu@ip-172-31-44-171:~/cuda$ echo $LD_LIBRARY_PATH\r\n/usr/local/cuda/lib64:/home/ubuntu/cuda\r\n```\r\nand got the same Bus error issue. ", "Stack trace makes me think the problem is outside of TensorFlow. I see similar issue here\r\n\r\nhttps://bugs.kde.org/show_bug.cgi?id=340824", "I'm seeing lots of potential causes for dlopen causing sigbus: https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=bus+error+on+dlopen\r\n\r\nI'll close this for now, please reopen if you find that problem lies with TensorFlow rather than with system dlopen call", "turns out that `libcublas.so.8.0` was unsuccessfully unzipped. I downloaded CUDA 8.0/CuDNN again, reinstalled drivers, and TF installed successfully. "]}, {"number": 6743, "title": "Is there a function which performs the opposite of tf.extract_image_patches ? ", "body": "Hello, \r\n\r\nI was looking for a function in the latest tf version, which performs the opposite of tf.extract_image_patches. If I did not miss it, it would be great to have this feature in tensorflow. \r\nIt would help me for the task I am doing.  \r\n\r\nThanks in advance,\r\nVignesh", "comments": ["+1 ", "@VigneshSrinivasan10 I found it hard to work with `tf.extract_image_patches` as it does a lot in the background. If your patches are just non-overlapping blocks then I found it easier to use `tf.reshape` and `tf.transpose`:\r\n\r\n```python\r\ndef image_to_patches(image, image_height, image_width, patch_height, patch_width):\r\n    # resize image so that it's dimensions are dividable by patch_height and patch_width\r\n    height = math.ceil(image_height/patch_height)*patch_height\r\n    width = math.ceil(image_width/patch_width)*patch_width\r\n\r\n    # shape: (height, width)\r\n    image_resized = tf.squeeze(tf.image.resize_image_with_crop_or_pad(image, height, width))\r\n    # reshapes to: (height // patch_height, patch_height, width // patch_width, patch_height, patch_width)\r\n    image_reshaped = tf.reshape(image_resized, [height // patch_height, patch_height, -1, patch_width])\r\n    # swaps axis to shape: (height // patch_height, patch_height, width // patch_width, patch_height, patch_width)\r\n    image_transposed = tf.transpose(image_reshaped, [0, 2, 1, 3])\r\n    # reshapes to ((height // patch_height)*(width // patch_width), patch_height, patch_width, 1)\r\n    return tf.reshape(image_transposed, [-1, patch_height, patch_width, 1]\r\n```\r\nSee [stackoverflow](http://stackoverflow.com/questions/41564321/split-image-tensor-into-small-patches) for more.", "@bodokaiser, `tf.extract_image_patches` in my case can also work with overlap. I am looking to add the values in the places where they overlap. ", "@VigneshSrinivasan10 in this case I think its best to put all `patches` belonging to the same `image` in one `batch` reshaping it to a 2d-tensor and then using `tf.nn.conv2d` to handle the overlap.\r\n\r\nAs this is not trivial to implement and something which I think occurs quite often it would really be great to have this supported by tensorflow.", "@bodokaiser, I totally agree. It will be great to have this supported by tensorflow", "@gpapan two users are requesting this feature. Since you introduced the ExtractImagePatches op, I figured you might be a good person to weigh in on the merits.", "The opposite of `tf.extract_image_patches` is its gradient or transpose operation.\r\nThere is an implementation of that [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.py#L575).\r\nAlso see: #2921.", "What do I need to do to create API docs for this?", "I'm going to mark this as resolved, per @gpapan's response. If you feel this should still be actionable on the part of TensorFlow, let me know and I'll re-open this issue.", "@jart @gpapan it is still not very clear to me how you use that function. I also do not find anything in the API docs about it.", "@bodokaiser we recommend [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for community-driven support.", "I still see this as a documentation issue - as mentioned earlier I could fix that with a PR if I know where to start.\n\n> Am 14.01.2017 um 03:05 schrieb Justine Tunney <notifications@github.com>:\n> \n> @bodokaiser we recommend StackOverflow for community-driven support.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "In that case, please open up a new issue requesting more in-depth documentation for `tf.extract_image_patches`. I'll triage it right away and make sure the appropriate people see it.", "Thank you @jart! you find the corresponding request at #6847.", "I found this topic & #6847 about tf.extract_image_patches and reverse tf.extract_image_patches.\r\nBut currently, it seems there are no updates about reverse tf.extract_image_patches.\r\n\r\nDear @gpapan: I tried tf.gradients in this [solution](https://stackoverflow.com/a/51785735) but it doesn't support the way to calculate the overlap area of two pictures. I'm working with overlap case and padding = \"SAME\". I can't find a solution.\r\n\r\nI'm sorry for lack of knowledge but why we can use gradient or transpose operation as the opposite of `tf.extract_image_patches` ?\r\n\r\nDear @bodokaiser, @VigneshSrinivasan10:  Would you mind sharing your solution?\r\n", "I've came up with naive solution for grayscale images (though can be modifyed for colored)\r\nSince I wanted mean of overlapping pixels, we can add all pixels in proper location and divide by the number of addition (overlaps). \r\n\r\n*   Create 2 arrays of zeros of original image size (black image)\r\n*   One array will be image where we adding our patches, second will be the \"mask\", counting how many patches that pixel recieved.\r\n*   Divide image by mask\r\n\r\nFull code with slicing first, then reversing\r\nNote, shape of inputted image: (height, width, 1) - since grayscale\r\n\r\n```Python\r\nn_images_v = 5\r\nn_images_h = 5\r\nkernel_size = (115, 216)\r\nsteps = [(image.shape[i] - kernel_size[i]) // (n_images_v - 1) for i in range(2)]\r\n\r\n# Making slices\r\nslices = tf.image.extract_patches(tf.expand_dims(image, axis=0), # adding batch axis\r\n              sizes=[1,kernel_size[0], kernel_size[1],1], \r\n              strides=[1, steps[0], steps[1], 1], \r\n              rates=[1,1,1,1], \r\n              padding=\"VALID\")\r\n\r\n# Reshaping into images (later we would want to add color channel dimension)\r\nsliced_images = tf.reshape(slices, (-1, kernel_size[0], kernel_size[1]))\r\n\r\n# Reversing\r\n\r\ndef recover_image(sliced_images, image_size, n_images):\r\n  \"\"\"\r\n  Recove image from tf.image.extract_patches. \r\n  sliced_images - tf.constant with shape - [n_images, height, width, 1]\r\n  image_size - tuple (height,width) original image size (before slicing)\r\n  n_images - tuple (horizontal, vertical) number of patches \r\n  \"\"\"\r\n  # Drop single color channel\r\n  sliced_images = tf.squeeze(sliced_images)\r\n  kernel_size = sliced_images.shape[1:3]\r\n  steps = [(image_size[i] - kernel_size[i]) // (n_images[i] - 1) for i in range(2)]\r\n  # Initialize image\r\n  recovered = tf.zeros(image_size)\r\n  mask = tf.zeros(image_size, dtype=tf.int8)\r\n  # Kernel for counting overlaps\r\n  kernel_ones = tf.ones(kernel_size,dtype=tf.int8)\r\n  for j in range(n_images[0]):\r\n    for i in range(n_images[1]):\r\n      # Make indices from meshgrid\r\n      indexes = tf.meshgrid(tf.range(steps[0] * j, # row start\r\n                                    kernel_size[0] + steps[0] * j), # row end\r\n                            tf.range(steps[1] * i, # col_start\r\n                                    kernel_size[1] + steps[1] * i), indexing='ij') # col_end\r\n      indexes = tf.stack(indexes, axis=-1)\r\n      # Add sliced image to recovered image indice\r\n      recovered = tf.tensor_scatter_nd_add(recovered, indexes, sliced_images[i+j*n_images[0]])\r\n      # Update mask\r\n      mask = tf.tensor_scatter_nd_add(mask, indexes, kernel_ones)\r\n\r\n  recovered = recovered / tf.cast(mask, tf.float32)\r\n  return recovered\r\n\r\n\r\nimg = recover_image(tf.expand_dims(sliced_images, axis=-1), \r\n                    image_size=(229, 512), \r\n                    n_images=(5,5))\r\n\r\ncv2_imshow(img.numpy()*255)\r\n```"]}, {"number": 6742, "title": "`tensorflow.examples.tutorials.mnist` not currently downloadable", "body": "### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```python\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\r\n```\r\n\r\nraises `http.client.RemoteDisconnected: Remote end closed connection without response`, if you don't already have the data cached. I believe this is because the underlying data [SOURCE_URL](http://yann.lecun.com/exdb/mnist/) is currently [down](http://downforeveryoneorjustme.com/http://yann.lecun.com/exdb/mnist/).", "comments": ["...It's back up again now. Looks like it was just a temporary disruption.", "Using the same code:\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\r\n\r\nraises a timeout error: TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.\r\n\r\nWhat would cause this?", "> Using the same code:\r\n> \r\n> from tensorflow.examples.tutorials.mnist import input_data\r\n> mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\r\n> \r\n> raises a timeout error: TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.\r\n> \r\n> What would cause this?\r\n\r\nThe issue is that the website where MNIST is canonically hosted sometimes goes down. I think an ideal solution would be to find or set up a mirror, then fall back to it? Or just find a more stable source.", "I also have the 10060 problem and it always happened. This seems like not the host problem. Do you know what caused this?", "Reopening because others seem to have this problem.", "I would suggest that if you encounter this problem, check http://downforeveryoneorjustme.com/http://yann.lecun.com/exdb/mnist/ just to make sure.\r\n\r\nRight now it is:\r\n![image](https://cloud.githubusercontent.com/assets/1022564/23486822/4ec69fae-feb0-11e6-9bcd-d009bd7a2529.png)\r\n(Sorry Yann!)", "I cannot reach this website", "Confirmed. I'm also having MNIST issues. Is there any alternative URL we can use for this which will still be compatible with existing MNIST tutorials. I'm relying on these as baselines for experiments, I'd rather keep the data and TF format the same.", "Still down seems few days... Other related issue #8116", "Same issue here, solved by downloading file from a different unofficial source. I think we should put in place an officially recognized mirror somewhere. Does anyone know if tensorflow checks a sort of signature for the dataset before using it? Or if a signature (e.g. md5) is available for the MNIST dataset so people can exchange it via unofficial circuits.", "Also cannot access.", "Closing as duplicate of https://github.com/tensorflow/tensorflow/issues/8126\r\n", "@auserdude If you retrieve the `.gz` files from any other source (wayback machine?) and drop them in your data_dir then TF will just use them.  It does not validate a checksum AFAIK.  The bug I referenced has a suitable download URL.", "ImportError: cannot import name 'model_fn' from 'tensorflow_estimator.python.estimator' (unknown location)\r\n\r\n iget this this error when i tried mnsit alternative download..neither worked for me. i even downloaded mat file locally still get this error"]}, {"number": 6741, "title": "tf.divide handles \"name\" argument differently", "body": "If installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.12.head\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```python\r\na = tf.get_variable('a', shape=[10])\r\nb = tf.get_variable('b', shape=[10])\r\nprint tf.divide(a, b, name='x').name   # x/truediv:0\r\n```\r\n\r\nNormally one would expect (like almost every other existing function in tf) that with the name='x' option, the output op will be named 'x:0'. \r\nBut instead it was named \"x/truediv:0\".", "comments": ["So what's happening here is that `divide` tries to do version-sensitive Python2 or Python 3 style divide. It calls `a/b` which drops the name, and then gets redirected to Python2 or Python3  version of division. A work-around is to use `tf.div` or `tf.truediv` instead of `tf.divide`. ", "@aselle for comment if this is likely to get fixed .... it seems `tf.divide` loses the name which makes annoying for graph/timeline visualization", "I think I have a fix. The problem is that operator overload __div__ cannot pass name, but I do want to use a class' division dispatch to honor the Python2/Python3 modes. I can do that with a dummy class. I have a fix that I'm testing internally. WIll keep this bug updated.\r\n\r\n", "@ppwwyyxx, master now has the fix, You can cherry pick it if you need it in an older version, otherwise in the mean-time you can use the workaround. Thanks!", "Great! Thanks a lot!", "Similar problem was found in `tf.case` and `tf.train.piecewise_constant`.\r\nAre they likely to be fixed?", "I think they are likely to get fixed if they are filed as separate issues (so they get triaged properly). Good to reference this issue so people can refer back to the commit that fixes things", "\ud83d\udc4d  I was hesitated to open too many similar issues..\r\nIf that's fine I'll open one for each"]}, {"number": 6740, "title": "loading text file and accessing data using tensorflow", "body": "ubuntu 14.0.04\r\npython 3.5\r\n\r\nI have a text file of size (20480,8) with all float values. I want the data in 4th column in to one array. I am able to do it using python as\r\n\r\n`file_pathname1 = os.path.join(os.path.expanduser('~'),'TF','1st_test', '20.10.22.12.09.13')`\r\n`x= np.loadtext(file_pathname1)`\r\n`y= x[:,4]`\r\n`#print(np.shape(x))`\r\n`print(np.shape(y))`\r\n\r\nI get the size of the y as (20480,)\r\n\r\nbut I am trying to copy the same as a tensor. How to access the data\r\n\r\n`file_pathname1 = os.path.join(os.path.expanduser('~'),'TF','1st_test', '20.10.22.12.09.13')`\r\n`y = tf.read_file(file_pathname1)`\r\n`sess = tf.Session()`\r\n`sess.run(y)`\r\n`print(y.get_shape())`\r\n\r\nI cant understand why the loaded file is empty array  because the output is ()\r\nonce loaded how to copy the column in to array ? ", "comments": ["for solution \r\n\r\nx1 = tf.constant(y,name = 'x1')\r\nmodel = tf.initialize_all_variables()\r\nwith tf.Session() as session:\r\n    session.run(model)\r\n    print(np.shape(session.run(x1)))"]}, {"number": 6739, "title": "lstm_cell/weights does not exist, fail only using master code, ok for release version 0.12.1", "body": "The code like below:\r\n` \r\nfor i in range(max_steps):  \r\n\r\n  with tf.variable_scope(\"RNN\", reuse=True if i > 0 else None):    \r\n\r\n     (output, state) = self.cell(last_symbol, state)\r\n`\r\n\r\n  File \"/home/gezi/mine/tensorflow-exp/deepiu/seq2seq/rnn_decoder.py\", line 189, in generate_sequence\r\n    (output, state) = self.cell(last_symbol, state)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/lstm_ops.py\", line 381, in __call__\r\n    self._num_units * 4])\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 987, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 889, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 347, in get_variable\r\n    validate_shape=validate_shape)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 332, in _true_getter\r\n    caching_device=caching_device, validate_shape=validate_shape)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 656, in _get_single_variable\r\n    \"VarScope?\" % name)\r\nValueError: Variable seq2seq/decode/RNN/lstm_cell/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n\r\n", "comments": ["Also find another error, using adagrad with latest master code which used to work with release 0.12\r\n\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py\", line 275, in optimize_loss\r\n    gradients, global_step=global_step, name=\"train\")\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 409, in apply_gradients\r\n    self._create_slots(var_list)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/adagrad.py\", line 66, in _create_slots\r\n    self._get_or_make_slot(v, val, \"accumulator\", self._name)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 592, in _get_or_make_slot\r\n    named_slots[var] = slot_creator.create_slot(var, val, op_name)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 101, in create_slot\r\n    return _create_slot_var(primary, val, '')\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 55, in _create_slot_var\r\n    slot = variable_scope.get_variable(scope, initializer=val, trainable=False)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 987, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 889, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 347, in get_variable\r\n    validate_shape=validate_shape)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 332, in _true_getter\r\n    caching_device=caching_device, validate_shape=validate_shape)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 656, in _get_single_variable\r\n    \"VarScope?\" % name)\r\nValueError: Variable bow/OptimizeLoss/bow/model_init/emb/Adagrad/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n", "I will file another bug for this ."]}, {"number": 6738, "title": "How to train Multibox object detector included in the TF Detect Android demo", "body": "There is a way to train a custom model for the multibox object detector that is included in the [TF Detect Android demo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java)? \r\n\r\n", "comments": ["@cyanoboy We're preparing a walk through for exactly this; should be out sometime in the not-too-distant future.", "Assigning to @andrewharp assuming he wants to add `Fixed #6738` to the commit message which adds that walkthrough.", "@andrewharp can you give us an update about the walk through? Thanks for your awesome work \ud83d\udc4d ", "@lukeisontheroad We're still moving around stuff internally; it's ending up being a bit tricky to open-source. In the meantime there is the [TF YOLO detector](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java) which you should be able to find training walkthroughs for online.", "Is there a trained version available for the custom inceptionV2 used for the feature extraction in the multibox object detector ?\r\nThank you for your work on this demo", "hey Andrew, is this still forthcoming?", "This is still active. We're looking to get a model compatible with the [TF Object Detection API](https://github.com/tensorflow/models/tree/master/object_detection) out in the next 2 or 3 weeks. That should provide a much easier path to retraining.\r\n\r\n@jch1 ", "Yup, this should be done soon, stay tuned!", "sounds awesome!", "@andrewharp Thanks for your information!", "The default object detector in the Android Object Detection demo is now the 80-class [Object Detection API](https://github.com/tensorflow/models/tree/master/object_detection) SSD model thanks to @jch1's commit, so retraining is now possible following the tutorials there.\r\n\r\nThe Yolo and original Multibox detectors remain available by modifying [DetectorActivity.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java#L89) (the person-detecting multibox model `@mobile_multibox` would also need to be added to [external_assets](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/BUILD#L92) if reverting)."]}, {"number": 6737, "title": "tf.nn.sampled_softmax_loss fail for master code", "body": "Build the leates tensorflow gpu version and  face this error(the code work ok for release 0.12.1) complaining float32 and int64 multiply.\r\ntf.nn.sampled_softmax_loss(self.w_t, \r\n                                          self.v, \r\n                                          inputs, \r\n                                          labels, \r\n                                          num_sampled, \r\n                                          vocab_size,\r\n                                          sampled_values=sampled_values)\r\n                                \r\n  File \"/home/gezi/mine/tensorflow-exp/util/melt/ops/seq2seq.py\", line 63, in sequence_loss_by_example\r\n    crossents = softmax_loss_function(logits, targets)\r\n  File \"/home/gezi/mine/tensorflow-exp/deepiu/seq2seq/rnn_decoder.py\", line 300, in sampled_loss\r\n    sampled_values=sampled_values)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py\", line 1180, in sampled_softmax_loss\r\n    name=name)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py\", line 963, in _compute_sampled_logits\r\n    array_ops.reshape(true_w, new_true_w_shape))\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 329, in multiply\r\n    return gen_math_ops._mul(x, y, name)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1625, in _mul\r\n    result = _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 522, in apply_op\r\n    inferred_from[input_arg.type_attr]))\r\nTypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int64 of argument 'x'.\r\n", "comments": ["looks api changed... close this"]}, {"number": 6736, "title": "batch_normalization layer argument name and documentation wrong in core and contrib layers", "body": "The batch normalization layers both in core and contrib take a `center` argument that controls whether a variable beta should be created that is then **added after the normalization**. The name of the argument (`center`) and even more so the documentation (_center: If True, subtract `beta`. If False, `beta` is ignored._) indicate however that this argument controls whether the mean of the input is subtracted as part of the normalization.", "comments": ["@sguada "]}, {"number": 6735, "title": "slim parallel_read should pass seed to string_input_producer", "body": "Currently, the seed passed to `parallel_read` is only passed to the RandomShuffleQueue, but not to the string_input_producer. This should be fixed. Furthermore, it should also be documented that the output will never be deterministic if `num_readers` is greater than 1.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e121667dc609de978a223c56ee906368d2c4ceef/tensorflow/contrib/slim/python/slim/data/parallel_reader.py#L212", "comments": ["@sguada I'm not sure if this is a bug, but @jonasrauber seems to be requesting a relatively trivial change.", "Totally it makes sense."]}, {"number": 6734, "title": "Can not register user_ops in Android APP", "body": "Hello!\r\nI want to user some user ops, such as `roi_pooling.cc` in Android. I build my Android project by Bazel, just like the Tensorflow Android demo, but I seems Android does build user_ops, the logcat shows as following:\r\n`tensorflow_inference_jni.cc:146 Could not create TensorFlow graph: Not found: Op type not registered 'RoiPooling'`\r\nIs it a bug or how can I solve this problem?\r\nThank you in advance for your help.", "comments": ["I'm not sure what roi_pooling.cc is because that doesn't appear to be in our codebase. Could you please help us out with more details?", "@jart I add roi_pooling.cc to tensorflow/core/user_ops,  you can find the code [here](https://github.com/philokey/Faster-RCNN_TF/blob/master/lib/roi_pooling_layer/roi_pooling_op.cc).", "It's awesome that you're adding features to TensorFlow. If those features have merit, we'll happily review a pull request once those features are polished off and ready for TensorFlow core. It's just that we have a policy in place where we rely on the [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow) community to provide user support. We try to keep this issue tracker limited to bugs and feature requests."]}, {"number": 6733, "title": "Export meta graph option in image retraining", "body": "I was trying to tensorflow serve my retrained graph ( *.pb file ) but if I understand correctly it doesn't contain meta graph in it, so wouldn't it be nice to have export meta graph options in retrain.py ?", "comments": ["@sherrym how do you think this request for meta graph example code should be triaged?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 6732, "title": "array operations in tensorflow", "body": "ubunutu14.0.4\r\npython3.5 \r\n\r\nHello, \r\n\r\nIm new using tensorflow library. Im trying to access data in text file and I can run it as python code but access the array elements using this library is not known. Please help\r\n\r\nI have a text file that contains float and integer values\r\n`import tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nfile_pathname1 = os.path.join(os.path.expanduser('~'),'Desktop','TF','Practice Files','test')\r\nx = np.loadtxt(file_pathname1)\r\n#y1 = x[1, 4] # load data to another variable\r\nprint(x)\r\n\r\ny = tf.readfile(file_pathname1)\r\n\r\nsess = tf.Session()\r\nprint(sess.run(y))\r\n`\r\nhow can i copy the same as array form using tensor flow\r\n\r\nbecause the output of both is different\r\n`[[ 1.3  2.5  3.5  4.4]\r\n [ 5.2  6.5  7.6  8.8]]`\r\n\r\n`b'1.3 2.5 3.5 4.4\\n5.2 6.5 7.6 8.8'`", "comments": ["this list is for bugs in TF/feature requests, could you ask on stackoverflow?"]}, {"number": 6731, "title": "Add contrib/tfprof to CMake build", "body": "Tensorflow for windows has package missing issue\r\nPath: tensorflow/contrib/tfprof\r\nAll the files under the path are missing,resnet model replies on these files.\r\nPlease pay attention,thank you.", "comments": ["![image](https://cloud.githubusercontent.com/assets/6457892/21759010/d284a55a-d67b-11e6-9b45-2c6fac98712a.png)\r\n![image](https://cloud.githubusercontent.com/assets/6457892/21759167/2e001422-d67d-11e6-8898-af6c572862bc.png)\r\n\r\nIs it caused by this?", "Thank you for reaching out. I've assigned this feature request to one of the contrib/cmake maintainers.\r\n\r\nTo the best of my knowledge, the Bazel+Windows build currently doesn't have support for the packages in contrib. So there's probably no viable workaround right now for using contrib/tfprof on Windows.", "Does It mean that this issue won't be solved in the coming months?", "We've got people actively working to bring good Windows support to TensorFlow. We can't promise timelines but we're doing our best to take care of our Windows users.", "@panyx0718 What's the status of this issue?", "Looks like other packages in contrib/ are available in Windows except tfprof.\r\nIt seems it's because this comment\r\n\"add_python_module(\"tensorflow/contrib/tfprof\" DONTCOPY)  # SWIG wrapper not implemented.\"\r\n\r\nI'm trying to enable tfprof in Windows and see what error presubmit will give me.", "@NoNoCalm I added tfprof cmake and it's now in tf.profiler. Maybe you can give it a try?"]}, {"number": 6730, "title": "tf.map_fn throws error...sometimes.... ", "body": "### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n\tlibcudart.so.8.0 (libc6,x86-64) => /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0\r\n\tlibcudart.so.7.5 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudart.so.7.5\r\n\tlibcudart.so (libc6,x86-64) => /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so\r\n\tlibcudart.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudart.so\r\n\tlibcuda.so.1 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n\tlibcuda.so.1 (libc6) => /usr/lib/i386-linux-gnu/libcuda.so.1\r\n\tlibcuda.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcuda.so\r\n\tlibcuda.so (libc6) => /usr/lib/i386-linux-gnu/libcuda.so\r\n\r\npip install tensorflow-gpu\r\n 0.12.0-rc1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI posted to the below stackoverflow link with some code and my initial problem that I mostly fixed, but it does seem that there is a problem with the tf.map_fn()\r\nhttp://stackoverflow.com/questions/41534866/image-distortion-returns-error-the-tensor-returned-for-reshape-50-was-not-vali/41536952?noredirect=1#comment70281037_41536952 \r\n\r\nI was trying to use tf.map_fn to distort a batch of images and it throws an error sometimes... the batch starts its life as a TFRecords file of flat images. The batch is turned into 4d tensor by tf.reshape then put through tf.map_fn(image distortion function) and then reshaped back into a flat 2d tensor. Most of the time I get a reshape error but not all the time. The times I don't get the error, the code will run for all epochs...\r\n\r\n`/home/mcamp/anaconda3/bin/python \"/media/mcamp/Local SSHD/Python Projects/GarageDoor2/train_model.py\"\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 960\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.342\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.94GiB\r\nFree memory: 2.53GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)\r\n*\r\nTraceback (most recent call last):\r\n  File \"/home/mcamp/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\r\n    return fn(*args)\r\n  File \"/home/mcamp/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1003, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/mcamp/anaconda3/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/mcamp/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The tensor returned for Reshape_3:0 was not valid.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/media/mcamp/Local SSHD/Python Projects/GarageDoor2/train_model.py\", line 44, in <module>\r\n    model.train(training_data, epochs=FLAGS.n_epochs)\r\n  File \"/media/mcamp/Local SSHD/Python Projects/GarageDoor2/ConvNetClass.py\", line 190, in train\r\n    training_data_dict['y_train_batch']]) #4\r\n  File \"/home/mcamp/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/home/mcamp/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/mcamp/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/mcamp/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The tensor returned for Reshape_3:0 was not valid.`\r\n", "comments": ["Could you give instructions on how to reproduce this?", "\r\nMore or less thats the code.. sorry to throw a ton of code on here.. I'm still pretty much a novice and if I did something wrong elsewhere didn't want to send anyone on a wild goose hunt. But like I said it runs about a quarter of the time no problem.. so I don't think its entirely me.. \r\n\r\n````\r\ndef read_and_decode(filename_queue,imshape=160*160*3):\r\n    reader = tf.TFRecordReader()\r\n    _, serialized_example = reader.read(filename_queue)\r\n    features = tf.parse_single_example(\r\n        serialized_example,\r\n        features={\r\n            'image_raw': tf.FixedLenFeature([], tf.string),\r\n            'label': tf.FixedLenFeature([], tf.int64),\r\n            'height': tf.FixedLenFeature([], tf.int64),\r\n            'width': tf.FixedLenFeature([], tf.int64),\r\n            'depth': tf.FixedLenFeature([], tf.int64)\r\n        })\r\n\r\n    image = tf.decode_raw(features['image_raw'], tf.uint8)\r\n    image.set_shape([imshape])\r\n    image = tf.cast(image, tf.float32)\r\n\r\n    label = tf.cast(features['label'], tf.int32)\r\n\r\n    return image, label\r\n\r\n\r\ndef inputs(train_dir, file, batch_size, num_epochs, n_classes, one_hot_labels=False, imshape=160*160*3):\r\n\r\n    if not num_epochs: num_epochs = None\r\n    filename = os.path.join(train_dir, file)\r\n\r\n    # with tf.name_scope('input'):\r\n    filename_queue = tf.train.string_input_producer(\r\n        [filename], num_epochs=num_epochs)\r\n\r\n    image, label = read_and_decode(filename_queue, imshape)\r\n\r\n    if one_hot_labels:\r\n        label = tf.one_hot(label, n_classes, dtype=tf.int32)\r\n\r\n    example_batch, label_batch = tf.train.shuffle_batch(\r\n        [image, label], batch_size=batch_size, num_threads=2,\r\n        capacity=1000, enqueue_many=False,\r\n        # Ensures a minimum amount of shuffling of examples.\r\n        min_after_dequeue=10, name=file)\r\n\r\n    return example_batch, label_batch\r\n\r\n def random_distorer(self, image):\r\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\r\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\r\n        image = tf.image.random_hue(image, max_delta=0.2)\r\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\r\n        image = tf.image.random_flip_up_down(image)\r\n        image = tf.image.random_flip_left_right(image)\r\n        return image`\r\n\r\n`    def train(self, training_data_dict, epochs=500, distort=True):\r\n        self.training_data_dict = training_data_dict # a dictionary of image batch queues\r\n        distorted_image = tf.reshape(training_data_dict['X_train_batch'], [-1, 160, 160, 3]) #1\r\n        if distort is True:\r\n            distorted_image = tf.map_fn(lambda img: self.random_distorer(img), distorted_image) #2\r\n        distorted_image = tf.reshape(distorted_image, [-1, 76800])  # 3\r\n        for epoch in range(epochs):\r\n            x_test, y_test = self.sess.run([training_data_dict['X_test_batch'], training_data_dict['y_test_batch']])\r\n            distortedX, x_train, y_train = self.sess.run([distorted_image, training_data_dict['X_train_batch'],\r\n                                                          training_data_dict['y_train_batch']]) #4\r\n            if epoch%300 == 0:\r\n                if self.distort is True:\r\n                    print('Distorting color in training images...')\r\n                distortedX, x_train, y_train = self.sess.run([distorted_image, training_data_dict['X_train_batch'],\r\n                                                              training_data_dict['y_train_batch']])\r\n                summary, loss, train_accuracy = self.sess.run([self.merged, self.train_step, self.accuracy], feed_dict={\r\n                    self.x: distortedX, self.y_: y_train, self.keep_prob: self.dropout})\r\n                print(\"Step %d, Distorted Training accuracy %g\" % (epoch, train_accuracy))\r\n            else:\r\n                summary, loss, train_accuracy = self.sess.run([self.merged, self.train_step, self.accuracy], feed_dict={\r\n                    self.x: x_train, self.y_: y_train, self.keep_prob: self.dropout})\r\n                if epoch%100 == 0:\r\n                    print(\"Step %d, Training accuracy %g\" % (epoch, train_accuracy))\r\n                self.train_writer.add_summary(summary, epoch)\r\n            if epoch%100 == 0:\r\n                summary, test_accuracy = self.sess.run([self.merged, self.accuracy], feed_dict={\r\n                    self.x: x_test, self.y_: y_test, self.keep_prob: 1.0})\r\n                print(\"Test Accuracy %g\"% test_accuracy)\r\n                self.test_writer.add_summary(summary, epoch)\r\n            if epoch%500 == 0:\r\n                print('Saving model...')\r\n                self.saver.save(self.sess, self.model_dir+'model.ckpt', epoch)\r\n            if epoch == epochs:\r\n                print('Saving model...')\r\n                self.saver.save(self.sess, self.model_dir+'model.ckpt', epoch)\r\n\r\n\r\ntraining_data = dict()\r\ntraining_data['X_train_batch'], training_data['y_train_batch'] = inputs(FLAGS.train_dir,\r\n                                                                  FLAGS.train_file,\r\n                                                                  FLAGS.batch_size,\r\n                                                                  FLAGS.n_epochs,\r\n                                                                  FLAGS.n_classes,\r\n                                                                  one_hot_labels=True,\r\n                                                                  imshape=76800)\r\n\r\n\r\ntraining_data['X_test_batch'], training_data['y_test_batch'] = inputs(FLAGS.train_dir,\r\n                                                                FLAGS.test_file,\r\n                                                                FLAGS.batch_size,\r\n                                                                FLAGS.n_epochs,\r\n                                                                FLAGS.n_classes,\r\n                                                                one_hot_labels=True,\r\n                                                                imshape=76800)\r\n\r\nnetwork_architecture = dict(W_conv1=[5, 5, 3, 25],\r\n                            W_conv2=[5, 5, 25, 50],\r\n                            W_fc1=[40 * 40 * 50, 1024],\r\n                            W_fc2=[1024, 2])\r\n\r\nmodel = BasicConvNet(76800, 2, network_architecture, batch_size=30,\r\n                     imshape=(160,160,3), dropout=0.5, model_dir='./modeltest/', histograms=False)\r\nmodel.train(training_data, epochs=FLAGS.n_epochs, distort=True)`\r\n", "Hm, I think this needs a bit more isolation/reproducible instructions before being actionable as a bug", "Closing due to seven days of no activity. If provided new information, I will re-open."]}, {"number": 6729, "title": "Library not loaded: @rpath/libcudart.8.0.dylib when running TF GPU on MacOS", "body": "Something related to linking seems to cause the build to fail on Mac OS 10.12.2 when building with CUDA.\r\n\r\nVerbose build error is:\r\n\r\n```\r\n$ bazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Found 1 target...\r\nERROR: /Users/anton/tmp/tensorflow/tensorflow/contrib/ffmpeg/BUILD:66:1: Executing genrule //tensorflow/contrib/ffmpeg:decode_audio_op_py_pygenrule failed: bash failed: error executing command \r\n  (cd /private/var/tmp/_bazel_anton/d3361bfa15c75c42ab541f3d83e8eba4/execroot/tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/cuda/bin:/Users/anton/.pyenv/shims:/Users/anton/.rbenv/shims:/Users/anton/.scalaenv/shims:/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Users/anton/unix/bin:/usr/local/cuda/bin \\\r\n    TMPDIR=/var/folders/4r/mq7ht1z11t72w5b5b6014zwm0000gn/T/ \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/contrib/ffmpeg/gen_decode_audio_op_py_py_wrappers_cc 1 > bazel-out/local_darwin-py3-opt/genfiles/tensorflow/contrib/ffmpeg/ops/gen_decode_audio_op_py.py'): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 6.\r\ndyld: Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /private/var/tmp/_bazel_anton/d3361bfa15c75c42ab541f3d83e8eba4/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/ffmpeg/gen_decode_audio_op_py_py_wrappers_cc\r\n  Reason: image not found\r\n/bin/bash: line 1: 72493 Abort trap: 6           bazel-out/host/bin/tensorflow/contrib/ffmpeg/gen_decode_audio_op_py_py_wrappers_cc 1 > bazel-out/local_darwin-py3-opt/genfiles/tensorflow/contrib/ffmpeg/ops/gen_decode_audio_op_py.py\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2.707s, Critical Path: 0.32s\r\n```\r\n\r\n### Related posts\r\nIt seems that Caffe had similar issues due to Apple dropping the `LD_LIBRARY_PATH` environment variable: https://github.com/BVLC/caffe/issues/3227\r\n\r\n### Environment info\r\nOperating System: Mac OS 10.12.2\r\n\r\nInstalled version of CUDA and cuDNN: \r\n```\r\n$ ls -l /usr/local/cuda/lib/libcud*\r\n-rwxr-xr-x  1 root  wheel  13504 Nov  3 19:39 /usr/local/cuda/lib/libcuda.dylib*\r\nlrwxr-xr-x  1 root  wheel     45 Nov  3 19:40 /usr/local/cuda/lib/libcudadevrt.a@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a\r\nlrwxr-xr-x  1 root  wheel     50 Nov  3 19:40 /usr/local/cuda/lib/libcudart.8.0.dylib@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\nlrwxr-xr-x  1 root  wheel     46 Nov  3 19:40 /usr/local/cuda/lib/libcudart.dylib@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib\r\nlrwxr-xr-x  1 root  wheel     49 Nov  3 19:40 /usr/local/cuda/lib/libcudart_static.a@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a\r\nlrwxr-xr-x  1 root  admin     47 Jan  8 16:48 /usr/local/cuda/lib/libcudnn.5.dylib@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.5.dylib\r\nlrwxr-xr-x  1 root  admin     45 Jan  8 16:48 /usr/local/cuda/lib/libcudnn.dylib@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.dylib\r\nlrwxr-xr-x  1 root  admin     48 Jan  8 16:48 /usr/local/cuda/lib/libcudnn_static.a@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn_static.a\r\n```\r\n\r\n##### Configuration command\r\n```\r\nPlease specify the location of python. [Default is /Users/anton/.pyenv/shims/python]: \r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] N\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nError in sitecustomize; set PYTHONVERBOSE for traceback:\r\nKeyError: 'PYTHONPATH'\r\nError in sitecustomize; set PYTHONVERBOSE for traceback:\r\nKeyError: 'PYTHONPATH'\r\nError in sitecustomize; set PYTHONVERBOSE for traceback:\r\nKeyError: 'PYTHONPATH'\r\nFound possible Python library paths:\r\n  /Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages\r\nPlease input the desired Python library path to use.  Default is [/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages]\r\n\r\nUsing python library path: /Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] Y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \r\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \r\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nlibcudnn.dylib resolves to libcudnn.dylib\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 3.5  \t \r\nExtracting Bazel installation...\r\n............\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n.........\r\n____Loading package: tensorflow/tools/git\r\n____Loading package: tensorflow/contrib/opt\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 40,960 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 444,518 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 743,716 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 1,170,534 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 1,597,352 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 1,998,646 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,251,050 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,675,032 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 3,144,390 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 3,579,716 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 3,788,162 bytes\r\nINFO: All external dependencies fetched successfully.\r\nConfiguration finished\r\n```\r\n\r\n#### Git revision\r\n```\r\n$ git rev-parse HEAD\r\nec7929b878926c39255254e9aea992f0bc65aa68\r\n```\r\n\r\n#### Bazel version\r\n```\r\n$ bazel version\r\nBuild label: 0.4.3-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Dec 22 15:20:15 2016 (1482420015)\r\nBuild timestamp: 1482420015\r\nBuild timestamp as int: 1482420015\r\n```\r\n\r\n### Update\r\nI modified `third_party/gpus/crosstool/CROSSTOOL.tpl`  the following lines \r\n```\r\n  cxx_flag: \"-std=c++11\"\r\n  linker_flag: \"-Wl,-no-as-needed\"\r\n``` \r\nin the toolchain section to: \r\n```\r\n  cxx_flag: \"-std=c++11\"\r\n  linker_flag: \"-Wl,-no-as-needed,-rpath,/usr/local/cuda/lib\"\r\n```\r\n\r\nNow I get failures at:\r\n```\r\nERROR: /Users/anton/tmp/tensorflow/tensorflow/python/BUILD:793:1: Executing genrule //tensorflow/python:control_flow_ops_pygenrule failed: bash failed: error executing command \r\n  (cd /private/var/tmp/_bazel_anton/d3361bfa15c75c42ab541f3d83e8eba4/execroot/tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/cuda/bin:/Users/anton/.pyenv/shims:/Users/anton/.rbenv/shims:/Users/anton/.scalaenv/shims:/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Users/anton/unix/bin:/usr/local/cuda/bin \\\r\n    TMPDIR=/var/folders/4r/mq7ht1z11t72w5b5b6014zwm0000gn/T/ \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/python/gen_control_flow_ops_py_wrappers_cc @tensorflow/python/ops/hidden_ops.txt 1 > bazel-out/local_darwin-py3-opt/genfiles/tensorflow/python/ops/gen_control_flow_ops.py'): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 6.\r\ndyld: Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /private/var/tmp/_bazel_anton/d3361bfa15c75c42ab541f3d83e8eba4/execroot/tensorflow/bazel-out/host/bin/tensorflow/python/gen_control_flow_ops_py_wrappers_cc\r\n  Reason: image not found\r\n/bin/bash: line 1: 45635 Abort trap: 6           bazel-out/host/bin/tensorflow/python/gen_control_flow_ops_py_wrappers_cc @tensorflow/python/ops/hidden_ops.txt 1 > bazel-out/local_darwin-py3-opt/genfiles/tensorflow/python/ops/gen_control_flow_ops.py\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1683.463s, Critical Path: 1587.37s\r\n```\r\n\r\nI got a bit further but it seems that I need to modify the link options in more files.", "comments": ["Can you run tensorflow GPU from an official MacOS GPU PIP package? There are some SIP-related issues (SIP needs to be disabled) which also cause a similar failure there", "The install goes well.  Just ran the tests and those as expected but not sure whether tensorflow is using my GPU. I ran the test described on https://www.tensorflow.org/how_tos/using_gpu/ to see whether Tensorflow sees my gpus and unfortunately it doesn't. The output is:\r\n```\r\nMatMul: /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:819] MatMul: /job:localhost/replica:0/task:0/cpu:0\r\nb: /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:819] b: /job:localhost/replica:0/task:0/cpu:0\r\na: /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:819] a: /job:localhost/replica:0/task:0/cpu:0\r\n[[ 22.  28.]\r\n [ 49.  64.]]\r\n```", "You should see something like this if you installed MacOS GPU version\r\n\r\n```\r\n\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.1-py3-none-any.whl\r\npip install --upgrade $TF_BINARY_URL\r\n\r\nexport CUDA_HOME=/usr/local/cuda\r\nexport DYLD_LIBRARY_PATH=\"$CUDA_HOME/lib:$CUDA_HOME:$CUDA_HOME/extras/CUPTI/lib\"\r\nexport LD_LIBRARY_PATH=$DYLD_LIBRARY_PATH\r\n\r\n(tf12rc1) (tf12rc1)bash-3.2$ python \r\nPython 3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:52:12) \r\n[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\r\n>>> \r\n```\r\n\r\nPS, you may need to manually symlink `libcuda.dylib` to `libcuda.1.dylib` in you /usr/local/cuda directory", "Same issue:\r\n```\r\nIn [1]: import tensorflow as tf\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/__init__.py in <module>()\r\n     48     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)\r\n---> 49     from tensorflow.python import pywrap_tensorflow\r\n     50     sys.setdlopenflags(_default_dlopen_flags)\r\n\r\n/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n     25             finally:\r\n\r\n/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: dlopen(/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n  Reason: image not found\r\n```\r\n\r\nnow I tried to change the library path:\r\n```\r\nsudo install_name_tool -change @rpath/libcudart.8.0.dylib /usr/local/cuda/lib/libcudart.8.0.dylib _pywrap_tensorflow.so \r\n```\r\n\r\nwhich leads to a segfault when importing tensorflow\r\n\r\n```\r\nIn [1]: import tensorflow as tf\r\n/Users/anton/.pyenv/versions/anaconda3-4.1.1/bin/python.app: line 3: 52118 Segmentation fault: 11  /Users/anton/.pyenv/versions/anaconda3-4.1.1/python.app/Contents/MacOS/python \"$@\"\r\n```\r\n\r\nReran with `python3 -v`\r\n\r\n```\r\n....(lots of messages)...\r\nimport 'importlib.abc' # <_frozen_importlib_external.SourceFileLoader object at 0x104a51080>\r\nimport 'importlib.util' # <_frozen_importlib_external.SourceFileLoader object at 0x104a45828>\r\nimport 'imp' # <_frozen_importlib_external.SourceFileLoader object at 0x104a41f98>\r\nSegmentation fault: 11\r\n```", "So this is not an issue with building, but rather you not being able to load your cuda libraries during runtime\r\n\r\nSome troubleshooting is here:\r\nhttps://github.com/tensorflow/tensorflow/issues/5141\r\n\r\n(ln -s your libcuda file, disable SIP, make sure you set LD_LIBRARY_PATH, and DYLD_LIBRARY_PATH correctly)", "Also see https://github.com/tensorflow/tensorflow/issues/6693", "Tried the following:\r\n```\r\nexport DYLD_LIBRARY_PATH=/usr/local/cuda/lib/\r\n```\r\n\r\n```\r\n$ python3 -c \"import tensorflow;\"\r\nError in sitecustomize; set PYTHONVERBOSE for traceback:\r\nKeyError: 'PYTHONPATH'\r\nTraceback (most recent call last):\r\n  File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n  File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n  Reason: image not found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n  File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n  Reason: image not found\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n```\r\n\r\ndirectory shows:\r\n```\r\n$ ls -al /usr/local/cuda/lib/libcudart.8.0.dylib \r\nlrwxr-xr-x  1 root  wheel  50 Nov  3 19:40 /usr/local/cuda/lib/libcudart.8.0.dylib@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\n```\r\n\r\nWhat should  I try next?", "turn off SIP?\n\nOn Mon, Jan 9, 2017 at 11:23 AM, Anton Bossenbroek <notifications@github.com\n> wrote:\n\n> Tried the following:\n>\n> export DYLD_LIBRARY_PATH=/usr/local/cuda/lib/\n>\n> $ python3 -c \"import tensorflow;\"\n> Error in sitecustomize; set PYTHONVERBOSE for traceback:\n> KeyError: 'PYTHONPATH'\n> Traceback (most recent call last):\n>   File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\n>     from tensorflow.python import pywrap_tensorflow\n>   File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n>     _pywrap_tensorflow = swig_import_helper()\n>   File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n>     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n>   File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/imp.py\", line 242, in load_module\n>     return load_dynamic(name, filename, file)\n>   File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/imp.py\", line 342, in load_dynamic\n>     return _load(spec)\n> ImportError: dlopen(/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\n>   Referenced from: /Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\n>   Reason: image not found\n>\n> During handling of the above exception, another exception occurred:\n>\n> Traceback (most recent call last):\n>   File \"<string>\", line 1, in <module>\n>   File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\n>     from tensorflow.python import *\n>   File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 60, in <module>\n>     raise ImportError(msg)\n> ImportError: Traceback (most recent call last):\n>   File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\n>     from tensorflow.python import pywrap_tensorflow\n>   File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n>     _pywrap_tensorflow = swig_import_helper()\n>   File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n>     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n>   File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/imp.py\", line 242, in load_module\n>     return load_dynamic(name, filename, file)\n>   File \"/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/imp.py\", line 342, in load_dynamic\n>     return _load(spec)\n> ImportError: dlopen(/Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\n>   Referenced from: /Users/anton/.pyenv/versions/anaconda3-4.1.1/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\n>   Reason: image not found\n>\n>\n> Error importing tensorflow.  Unless you are using bazel,\n> you should not try to import tensorflow from its source directory;\n> please exit the tensorflow source tree, and relaunch your python interpreter\n> from there.\n>\n> directory shows:\n>\n> $ ls -al /usr/local/cuda/lib/libcudart.8.0.dylib\n> lrwxr-xr-x  1 root  wheel  50 Nov  3 19:40 /usr/local/cuda/lib/libcudart.8.0.dylib@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\n>\n> What should I try next?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6729#issuecomment-271381085>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHMR8lEZ4q7zPm_sQKUrzKsN1H03Lks5rQoklgaJpZM4LdwIF>\n> .\n>\n", "wanted to prevent that for security reasons.... but will try that.", "Doesn't work with SIP unfortunately. Also LD_LIBRARY_PATH as I mentioned above", "I'm terribly confused by this issue. In the original error message, [python_op_gen_main.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/python_op_gen_main.cc) is crashing when invoked by a genrule to create a Python wrapper for contrib/ffmpeg:decode_audio_op because it can't load Cuda due to [SIP](https://en.wikipedia.org/wiki/System_Integrity_Protection)? I'm not sure why that tool needs to link Cuda, which is clearly build messiness. But why would SIP prevent linking Cuda?", "@jart The way TensorFlow loads dynamic libraries is not secure (loading by relative path means someone could trick TF to load malicious code from an all-writeable location). So MacOS disables it https://github.com/BVLC/caffe/issues/3227 \r\n\r\nAs far to why `gen_decode_audio_op_py_py_wrappers_cc` needs CUDA, a possibility is that is it built in in the wrong mode. Bazel builds in [two configurations](https://bazel.build/versions/master/docs/be/general.html#cross-compilation-considerations) \"host\" configuration and \"target\" configuration. The tool is a `cc_binary` [target](https://github.com/tensorflow/tensorflow/blob/ec7929b878926c39255254e9aea992f0bc65aa68/tensorflow/tensorflow.bzl#L146) which I suspect implies target mode. The proper fix would be to force it to be built in host mode, which shouldn't need GPU. One way of forcing host mode\u00a0is to introduce dependency on the binary through `tools` property of genrule , ie like [tensorflow/compiler/aot/tfcompile.bzl](https://github.com/tensorflow/tensorflow/blob/f94e20f2c704f037a113d350fd7c431e3fd75df8/tensorflow/compiler/aot/tfcompile.bzl#L140)\r\n\r\nThere's an internal issue if you search for \"avx2\", \"ppluzhnikov\" and \"tool\" that addressed similar problem in TF build", "Where is TensorFlow loading libraries by relative path? I know with op libraries get_path_to_datafile() turns relative .so paths into absolute paths before passing them along to dlopen. I'm not sure where cuda is being loaded, but I'd like to know more. If TensorFlow is being insecure, it should be fixed.\r\n\r\nRe: Cuda dep, that program is [passed](https://github.com/tensorflow/tensorflow/blob/1f5d61d0b4a2ae08319650ae78c5609c515615db/tensorflow/tensorflow.bzl#L219) to genrule via tools so Bazel is guaranteed to build it with host configuration, which seems to be a result of the internal issue you mentioned. So my best guess is Cuda is being superfluously linked because the tool depends on core:lib_internal and core:lib_internal depends on everything. Hopefully someday we can get rid of all of these hourglasses in the build graph. They make builds so slow.", "I'm just going by the error message which say `@rpath/libcudart.8.0.dylib`, which I'm assuming means that `libcudart` is loaded by path relative to location in `DYLD_LIBRARY_PATH` which Apple thought was a security hazard. I guess I don't understand why cuda is being loaded then -- my understanding was that dynamic linking of cuda was a property of target configuration rather than a particular target to be included. IE, you if you build core:lib_internal without `--config=cuda` , there won't be any references to cuda runtime", "@gunan do we dllopen() cuda with a relative path?", "Actually, maybe it's not an issue of SIP, but `LD_LIBRARY_PATH` environment getting lost during tool invocation:\r\n\r\nSomeone else hit similar issue and fixed it by modifying Bazel's genrule-setup.sh to add `LD_LIBRARY_PATH` in https://github.com/tensorflow/tensorflow/issues/4187#issuecomment-272656233\r\n\r\nThere's been a similar issue with tools getting called by Bazel not getting the environment propagated, and it was due to misconfiguration in CROSSTOOL file: https://github.com/tensorflow/tensorflow/issues/3261#issuecomment-234147379", "For CUDA, I think we have an indirection setup by cuda_configure.bzl\r\n@davidzchen @damienmg Could you take a look?\r\nLooks like we are having quite a few issues with CUDA on macos.", "Have you tried to use `--action_env PATH --action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARYPATH` flags? So those environment get shipped in the action?", "(side-note: there should be no update to the CROSSTOOL by end users, please tell us if some more things are needed to make build works everywhere).", "BTW, I just hit this issue when building today's head on Mac and was able to get things building with @damienmg 's suggestion\r\n\r\n```\r\nexport CUDA_HOME=/usr/local/cuda\r\nexport DYLD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/extras/CUPTI/lib\r\nexport LD_LIBRARY_PATH=$DYLD_LIBRARY_PATH\r\nexport PATH=$DYLD_LIBRARY_PATH:$PATH\r\nexport flags=\"--config=cuda --config=opt\"\r\nbazel build $flags --action_env PATH --action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package &> ~/temp/bazel_output.txt\r\n```", "Should we update our documentation then?", "Maybe under common problems? https://www.tensorflow.org/get_started/os_setup#common_problems\r\n\r\nNot 100% sure what those flags do. IE, it worked for `bazel build`, but not for `bazel test`, it fails with error which suggests env problems (work-around is to do `bazel test sometes` followed by `bazel-bin/.../sometest`)", "Thanks! Looping internally. `--config=opt` sets `-march=native` to get `-mavx` etc.", "I think for macos build we can set these environment variables through our bazelrc.\r\nSo users wont need to manually set them.\r\n\r\n", "/cc @aehlig: Is it normal that action_env is not passed to test?\r\n\r\n--action_env tell which environment variable to pass to compile action\r\n--test_env tell which environment variable to set to test action\r\n\r\nDefault value for action_env is LD_LIBRARY_PATH and PATH, we do not put DYLD_LIBRARY_PATH by default. So if explicitely set from the command line you want to add PATH and LD_LIBRARY_PATH too.\r\n\r\nFrom the help of Bazel exactly:\r\n```\r\n  --action_env (a 'name=value' assignment with an optional value part; may be used multiple times)\r\n    Specifies the set of environment variables available available to actions. \r\n    Variables can be either specified by name, in which case the value will be \r\n    taken from the invocation environment, or by the name=value pair which sets \r\n    the value independent of the invocation environment. This option can be \r\n    used multiple times; for options given for the same variable, the latest \r\n    wins, options for different variables accumulate.\r\n  --test_env (a 'name=value' assignment with an optional value part; may be used multiple times)\r\n    Specifies additional environment variables to be injected into the test \r\n    runner environment. Variables can be either specified by name, in which \r\n    case its value will be read from the Bazel client environment, or by the \r\n    name=value pair. This option can be used multiple times to specify several \r\n    variables. Used only by the 'bazel test' command.\r\n```\r\n", "I have the exact same problem, just running Tensorflow (not building it):\r\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n  Reason: image not found\r\n\r\nMacOS 10.12.2\r\nUsing an eGPU\r\nDYLD_LIBRARY_PATH=:/usr/local/cuda/lib\r\nls -al /usr/local/cuda/lib/libcudart*\r\n/usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\n/usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib\r\n/usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a\r\n\r\nDisabling SIP fixes this", "To summarize for new users who find this:\r\n\r\nOn MacOS you need to do this:\r\n\r\n1. Disable SIP (Security Integrity Protection)\r\n2. Set environment variables point to your CUDA installation\r\n\r\nie, \r\n```\r\nexport CUDA_HOME=/usr/local/cuda\r\nexport DYLD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/extras/CUPTI/lib\r\nexport LD_LIBRARY_PATH=$DYLD_LIBRARY_PATH\r\nexport PATH=$DYLD_LIBRARY_PATH:$PATH\r\n\r\n```\r\nFurthermore, if you are running TF through `bazel` as in `bazel test`, you need to make sure those env vars are propagated, by using `--action_env` flags\r\n\r\n`bazel test $flags --action_env PATH --action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package`", "I have not disabled SIP on my current OSX install. I can compile and run TensorFlow from the head and release branches successfully. \r\n\r\nhttp://stackoverflow.com/questions/38710339/library-not-loaded-rpath-libcudart-7-5-dylib-tensorflow-error-on-mac/41073045#41073045\r\n\r\nhttp://stackoverflow.com/questions/39865212/dyld-library-not-loaded-rpath-libcudart-8-0-dylib-while-building-tensorflow?rq=1", "This solution plus symlinking libcuda.1 (`cd /usr/local/cuda/lib && sudo ln -s libcuda.dylib libcuda.1.dylib`) also worked for me (OSX 10.12.3, TensorFlow-GPU 1.0.0, Python 3.6.)\r\n\r\nNote that I had to remove a line from my `.bash_profile` which \"activated pyenv\" (removed `eval \"$(pyenv init -)\"`).  The fix presented here appeared to have no effect until this was done.\r\n\r\nFYI: disabling SIP does not appear to be necessary, at least not in all cases (ie. my SIP is **enabled**).\r\n\r\n\r\n```\r\nSocrates:~ bRad$ python -c \"import tensorflow as tf\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.5.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.8.0.dylib locally\r\n```\r\n\r\nCompute performance has more than doubled (138%).  Thanks, everyone. ", "In my case, I am not allowed to disable SIP by the admin. From Apple's page on [Dynamic Libraries](https://developer.apple.com/library/content/documentation/DeveloperTools/Conceptual/DynamicLibraries/100-Articles/UsingDynamicLibraries.html), they note that the following.\r\n\r\n> The standard locations for dynamic libraries are ~/lib, /usr/local/lib, and /usr/lib.\r\n\r\nI didn't want to clutter /usr/local/lib or /usr/lib, so I moved my files to ~/lib.\r\n\r\n```bash\r\ncp -r /usr/local/cuda/lib ~/lib\r\n```\r\n\r\nand set\r\n\r\n```bash\r\nexport DYLD_LIBRARY_PATH=\"${HOME}/lib:${DYLD_LIBRARY_PATH}\"\r\n```\r\n\r\nand it seems to work fine.", "This fixed the problem for me on mac: install_name_tool -add_rpath /usr/local/cuda/lib /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so", "Hi - just to mention I've had similar issues following the instructions for 1.0\r\n\r\nI can confirm the following fixes all worked:\r\n\r\n\u2714\ufe0e Install_name_tool fix posted by @jason-riddle\r\n\u2714\ufe0e Stack Overflow posts linked by @normanheckscher worked\r\n\u2714\ufe0e @bradleygibson symlink fix (seems the easiest)\r\n", "I just hit this building pip for 1.0 on 10.12.3 - anaconda python 3.5\r\n\r\nThis worked for me\r\n\u2714\ufe0e Stack Overflow posts linked by @normanheckscher worked\r\n\r\nI know I had to do that a while back but haven't had to do it for a while.\r\n\r\nThen toward the end I hit #7227 and had to revert to bazel 0.4.3.  And then it all worked.\r\n", "Clone tensorflow including submodules\r\ngit clone --recurse-submodules https://github.com/tensorflow/tensorflow\r\n\r\nCheck out release 1.1 instead of using master branch\r\ngit checkout remotes/origin/r1.1\r\n\r\nSet path variables as @yaroslavvb mentioned:\r\nexport CUDA_HOME=/usr/local/cuda\r\nexport DYLD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/extras/CUPTI/lib\r\nexport LD_LIBRARY_PATH=$DYLD_LIBRARY_PATH\r\nexport PATH=$DYLD_LIBRARY_PATH:$PATH\r\n\r\nFor OSX Sierra 10.12.4:\r\nUse XCode 7.2 \r\n\r\nI documented the whole installation here:\r\nhttps://gist.github.com/to-bee/6d19b7fa0d68ee97407591de1586da62", "Looks like there were a few workarounds posted.\r\nI am closing this issue, as we also dropped support for NVIDIA GPU support on MacOS.", "As mentioned in the other huge GH issue for build troubleshooting on Macs, I've published another slightly more up-to-date tutorial here:\r\n\r\nhttps://metakermit.com/2017/compiling-tensorflow-with-gpu-support-on-a-macbook-pro/\r\n\r\n(TensorFlow 1.3.0, CUDA 8.0 and cuDNN 6.0)"]}, {"number": 6728, "title": "Build with CUDA 8.0 fails on Mac OS X 10.12.2", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n", "comments": []}, {"number": 6727, "title": " tf.contrib.learn.monitors.ValidationMonitor hangs when passed input_fn parameter", "body": "Hello!\r\nI have been working my way through the  tf.contrib.learn [tutorials](https://www.tensorflow.org/tutorials/) and have been attempting to integrate the tf.contrib.learn.monitors.ValidationMonitor into the 'deep' classifier in wide_n_deep.py as shown below.\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?  I searched both Github and Stakeoverflow with the terms 'tensorflow,' 'input_fn,' and 'validationmonitor' but wasn't able to find anyone else who reported similar issues.\r\n\r\n### Environment info\r\nOperating System:  I ran this on Ubuntu Server 16.04 on a physical I7 with a GTX1080 gpu when i noticed the problem.  I know that i was using the GPU on the original physical box from previous tests, and because during the hang the nvidia_smi command showed considerable load on the GPU.  I was able to replicate the problem with CPU on a 16.04 VM as well.\r\n\r\nInstalled version of CUDA and cuDNN: \r\n```\r\n/home/andersonjas/libcudnn5-dev_5.1.5-1+cuda8.0_amd64.deb\r\n/home/andersonjas/libcudnn5_5.1.5-1+cuda8.0_amd64.deb\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nfrom Anaconda 2.7 64 bit package:\r\n```\r\npip install tensorflow\r\n```\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\n```\r\nandersonjas@ubuntu:~$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\n0.11.head\r\n\r\n```\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```python\r\nvalidation_monitor = tf.contrib.learn.monitors.ValidationMonitor(input_fn=lambda:input_fn(df_test), \r\n                       every_n_steps=50)\r\nm.fit(input_fn=lambda: input_fn(df_train), steps=151,monitors=[validation_monitor])\r\n```\r\n\r\nDoing this in a jupyter notebool causes the code to hang indefinitely.  To make completely sure that i don't have a bug in my own code i can make the following change:\r\n\r\n```python\r\nvalidation_monitor = tf.contrib.learn.monitors.ValidationMonitor(input_fn=lambda:input_fn(df_test), \r\n                       every_n_steps=50)\r\nm.fit(input_fn=lambda: input_fn(df_train), steps=151) #,monitors=[validation_monitor])\r\n```\r\n\r\nand then the code executes fine.\r\n\r\n### What other attempted solutions have you tried?\r\nI also built an input_fn interface to the iris and boston housing price predictor code code, each showed similar 'hangs'\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\nAs a noob, i'm learning that esoteric error messages are a luxury :-),  in this case the code just hangs indefinitely.\r\n", "comments": ["FWIW,  tf.contrib.learn.monitors.ValidationMonitor  works like a champ when passed x,y parameters instead of input_fn, as instructed by the [tutorial](https://www.tensorflow.org/tutorials/monitors/)", "suffered from the same issue.\r\nLogs\uff1a\r\nWARNING:tensorflow:From c:\\users\\jay\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\monitors.py:322: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\r\nInstructions for updating:\r\nMonitors are deprecated. Please use tf.train.SessionRunHook.\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_id': 0, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000, '_environment': 'local', '_save_checkpoints_steps': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001E539FE77F0>, '_tf_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 1\r\n}\r\n, '_save_summary_steps': 100, '_task_type': None, '_is_chief': True, '_master': '', '_num_ps_replicas': 0, '_evaluation_master': ''}\r\nDEBUG:tensorflow:Setting feature info to {'work': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(594)]), is_sparse=False), 'indexWork': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(594)]), is_sparse=False), 'index': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(594)]), is_sparse=False)}.\r\nDEBUG:tensorflow:Setting labels info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(594)]), is_sparse=False)\r\nDEBUG:tensorflow:Transforming feature_column _RealValuedColumn(column_name='index', dimension=1, default_value=None, dtype=tf.float32, normalizer=None)\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nDEBUG:tensorflow:Transforming feature_column _RealValuedColumn(column_name='indexWork', dimension=1, default_value=None, dtype=tf.float32, normalizer=None)\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nDEBUG:tensorflow:Transforming feature_column _RealValuedColumn(column_name='work', dimension=1, default_value=None, dtype=tf.float32, normalizer=None)\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From c:\\users\\jay\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Saving checkpoints for 1003 into /tmp/count_model202020K\\model.ckpt.\r\nINFO:tensorflow:loss = 10987.0, step = 1003\r\nDEBUG:tensorflow:Given features: {'work': <tf.Tensor 'Const_1:0' shape=(283,) dtype=int64>, 'indexWork': <tf.Tensor 'Const_2:0' shape=(283,) dtype=int64>, 'index': <tf.Tensor 'Const:0' shape=(283,) dtype=int64>}, required signatures: {'work': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(594)]), is_sparse=False), 'indexWork': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(594)]), is_sparse=False), 'index': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(594)]), is_sparse=False)}.\r\nDEBUG:tensorflow:Given labels: Tensor(\"Const_3:0\", shape=(283,), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(594)]), is_sparse=False).\r\nDEBUG:tensorflow:Transforming feature_column _RealValuedColumn(column_name='index', dimension=1, default_value=None, dtype=tf.float32, normalizer=None)\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nDEBUG:tensorflow:Transforming feature_column _RealValuedColumn(column_name='indexWork', dimension=1, default_value=None, dtype=tf.float32, normalizer=None)\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nDEBUG:tensorflow:Transforming feature_column _RealValuedColumn(column_name='work', dimension=1, default_value=None, dtype=tf.float32, normalizer=None)\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From c:\\users\\jay\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nINFO:tensorflow:Starting evaluation at 2017-03-02-13:00:23", "@martinwicke Who would be the right person to look at ValidationMonitor now?  (@ilblackdragon wrote this)\r\n\r\n@JaySWang why did you unassign him?", "I have the same issue. Please see details below.\r\n\r\n### Environment info\r\nOperating System:\r\n- MacOS Sierra 10.12.3 (16D32)\r\n- Python 2.7.10\r\n\r\n**Installed version of CUDA and cuDNN:** \r\nn/a\r\n\r\n**A link to the pip package you installed:**\r\nhttps://pypi.python.org/packages/db/0b/8ca3299122cb22f763df7076938b12d7f9e8a814d3ab7a80df6a41ef81f7/tensorflow-1.0.1-cp27-cp27m-macosx_10_11_x86_64.whl#md5=5f0f52fc2e5243d40d6ced52c6a56c69\r\n**The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.**\r\n1.0.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code):\r\n[linear.py.zip](https://github.com/tensorflow/tensorflow/files/837779/linear.py.zip)\r\n\r\n### What other attempted solutions have you tried?\r\nn/a\r\n\r\n### Logs or other output that would be helpful\r\n[6727_logs.txt](https://github.com/tensorflow/tensorflow/files/837781/6727_logs.txt)\r\n", "I just found a workaround to what might be the same issue in Tensorflow 0.12.1. Maybe it will be helpful here.\r\n\r\nValidationMonitor did not support the x=...y=... input, and when using input_fn, I found it would loop forever the first time it tried to run the monitor during a fit() call. The log output looked like it was calculating the validation metrics over and over in an infinite loop.\r\n\r\nAfter looking at the source for ValidationMonitor I found a parameter eval_steps that defaulted to None. In other places in the contrib.learn codebase 'None' implies 'forever', so I tried adding eval_steps=1 to the constructor, and now it works as advertised.\r\n\r\ne.g.\r\n```\r\n        validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\r\n            input_fn=lambda: input_function(test_data, test_labels),\r\n            eval_steps=1,  # Try adding this\r\n            metrics=validation_metrics,\r\n            every_n_steps=10,\r\n        )   \r\n\r\n        estimator.fit(\r\n            input_fn=lambda: input_function(training_data, training_labels),\r\n            steps=training_epochs,\r\n            monitors=[validation_monitor],\r\n        ) \r\n```\r\n\r\nHope that helps!", "Hi @gmacleod,\r\n\r\nthanks for pointing out this **eval_steps** parameter. \r\n\r\nI can confirm that the issue can't be reproduced once eval_steps set to \"1\".", "I will close this issue. The semantics might be awkward, but the workaround is simple and we have deprecated monitors in favor of Hooks.", "Awesome. Thanks for the follow up.  Eager to try it out.", "Great, it solved my problem!", "@gmacleod  Thanks for pointing out this solution! It happened in R1.1 too", "@gmacleod In tf1.1.0 , even adding eval_steps=1 to the constructor, it just output the validation log one time at the end of first n_steps, not every n_steps. Do you have the same problem?", "@gmacleod Thanks!! Happens in R1.1 as well.", "@GuohongLi  Maybe you should change the params in  `config=tf.contrib.learn.RunConfig()` to save the model weights, and then you can make the monitors. [](https://www.tensorflow.org/get_started/monitors)", "This indeed solves my problem. However, now the validationmonitor only trigger **ONE** time, any solution for this?\r\n\r\n\r\n validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\r\n    input_fn=lambda: input_fn(df_test),\r\n    every_n_steps=25,metrics=validation_metrics,\r\n    eval_steps=1,\r\n    early_stopping_metric=\"auc\",\r\n    early_stopping_metric_minimize=False,\r\n    early_stopping_rounds=100)\r\n\r\nINFO:tensorflow:Validation (step 25): loss = 404.011, accuracy = 0.859006, labels/prediction_mean = 1.88953e-07, labels/actual_label_mean = 0.140994, accuracy/baseline_label_mean = 0.140994, auc = 0.5, auc_precision_recall = 0.570497, accuracy/threshold_0.500000_mean = 0.859006, precision/positive_threshold_0.500000_mean = 0.0, recall/positive_threshold_0.500000_mean = 0.0, precision = 0.0, recall = 0.0, global_step = 1\r\nINFO:tensorflow:global_step/sec: 5.23263\r\nINFO:tensorflow:loss = 51.8876, step = 101 (19.112 sec)\r\nINFO:tensorflow:global_step/sec: 28.5071\r\nINFO:tensorflow:loss = 5.04256, step = 201 (3.508 sec)\r\nINFO:tensorflow:global_step/sec: 25.6652\r\nINFO:tensorflow:loss = 11.7246, step = 301 (3.896 sec)\r\nINFO:tensorflow:global_step/sec: 29.4536\r\nINFO:tensorflow:loss = 10.9838, step = 401 (3.395 sec)\r\nINFO:tensorflow:global_step/sec: 28.0369\r\nINFO:tensorflow:loss = 8.74781, step = 501 (3.567 sec)\r\nINFO:tensorflow:global_step/sec: 27.9563\r\nINFO:tensorflow:loss = 10.7869, step = 601 (3.577 sec)\r\nINFO:tensorflow:global_step/sec: 27.0742\r\nINFO:tensorflow:loss = 17.0162, step = 701 (3.694 sec)\r\nINFO:tensorflow:global_step/sec: 27.3682\r\nINFO:tensorflow:loss = 4.63466, step = 801 (3.654 sec)\r\nINFO:tensorflow:global_step/sec: 27.6004\r\nINFO:tensorflow:loss = 6.31007, step = 901 (3.623 sec)\r\nINFO:tensorflow:global_step/sec: 27.6511\r\nINFO:tensorflow:loss = 2.64077, step = 1001 (3.616 sec)\r\nINFO:tensorflow:global_step/sec: 27.4551\r\nINFO:tensorflow:loss = 1.91578, step = 1101 (3.642 sec)\r\nINFO:tensorflow:global_step/sec: 27.7833\r\nINFO:tensorflow:loss = 7.05945, step = 1201 (3.599 sec)\r\nINFO:tensorflow:global_step/sec: 28.0504\r\nINFO:tensorflow:loss = 2.11703, step = 1301 (3.565 sec)\r\nINFO:tensorflow:global_step/sec: 28.0631\r\nINFO:tensorflow:loss = 7.89242, step = 1401 (3.563 sec)\r\nINFO:tensorflow:global_step/sec: 27.6421\r\nINFO:tensorflow:loss = 13.6624, step = 1501 (3.618 sec)\r\nINFO:tensorflow:global_step/sec: 27.7735\r\nINFO:tensorflow:loss = 2.61309, step = 1601 (3.601 sec)\r\nINFO:tensorflow:global_step/sec: 27.5637\r\nINFO:tensorflow:loss = 7.0057, step = 1701 (3.628 sec)\r\nINFO:tensorflow:global_step/sec: 27.2692\r\nINFO:tensorflow:loss = 5.1032, step = 1801 (3.667 sec)\r\nINFO:tensorflow:global_step/sec: 27.225\r\nINFO:tensorflow:loss = 1.73602, step = 1901 (3.673 sec)\r\nINFO:tensorflow:Saving checkpoints for 2000 into /var/folders/dg/jxsw2s955c1f91h17nxqm4hm395n3_/T/tmpquiqiew6/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 1.56839.", "@gmacleod Thanks eval_steps = 1 fixed the problem!\r\n\r\n@lancerts \r\nI was able to trigger validation monitor multiple time by setting saving checkpoints.\r\n\r\nthe relevant code\r\n\r\n```\r\nclassifier = tf.contrib.learn.DNNClassifier(\r\n    ...,\r\n    config = tf.contrib.learn.RunConfig(save_checkpoints_steps = 100, save_checkpoints_secs = None)\r\n)\r\n```\r\n", "from tensorflow output,\r\n\r\nINFO:tensorflow:Saving dict for global step 39807: accuracy = 0.85421, accuracy/baseline_label_mean = 0.14821, accuracy/threshold_0.500000_mean = 0.85421, auc = 0.686321, global_step = 39807, labels/actual_label_mean = 0.14821, labels/prediction_mean = 0.146081, loss = 0.39175, precision/positive_threshold_0.500000_mean = 0.580026, recall/positive_threshold_0.500000_mean = 0.0591728, validate_confusion_matrix = [[84052 699]\r\n[14430 819]], validate_streaing_precision = 0.580026, validate_streaing_recall = 0.0591728, validate_streaming_auc = 0.525859\r\nWARNING:tensorflow:Skipping summary for validate_confusion_matrix, must be a float or np.float32.\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\nINFO:tensorflow:Validation (step 40000): loss = 0.39175, accuracy = 0.85421, labels/prediction_mean = 0.146081, labels/actual_label_mean = 0.14821, accuracy/baseline_label_mean = 0.14821, auc = 0.686321, accuracy/threshold_0.500000_mean = 0.85421, precision/positive_threshold_0.500000_mean = 0.580026, recall/positive_threshold_0.500000_mean = 0.0591728, validate_confusion_matrix = [[84052 699]\r\n[14430 819]], validate_streaing_precision = 0.580026, validate_streaing_recall = 0.0591728, validate_streaming_auc = 0.525859, global_step = 39807\r\n\r\nWhy are this two AUC differs so much? When i use model.evaluate on train, validate, test data set, all the output AUCs are very close to the first auc shows in bold above.\r\nSo what is the validation_streaming_auc is calculating? I have reset the local variables in each epoch."]}, {"number": 6726, "title": "Error importing Tensorflow", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System: ubuntu 16.04 LTS - Tensorflow-GPU installed ( GTX 970 )\r\n\r\nInstalled version of CUDA and cuDNN:  7.5 - 4.5\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): \r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nTried from scratch 4 times including formatting system. \r\n\r\nInstalled from pip 2 times, and installed with bazel 2 times. No error at during setup. \r\n\r\nTried all recommendations from other developers especially faced with this one.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Can you include the entire error message?", "Ah, It is difficult to see because of the because it is not formatted.\r\nError message is:\r\n```\r\nError importing tensorflow. Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n```\r\nThe error message actually is quite helpful here. you simply need to exit your interpreter, change your directory to something other than the directory where you cloned github tensorflow repository.\r\n\r\nThe reason for the issue is python gets confused about what to import when you run \"import tensorflow\" (the one in your directory or the system installed one). That is what the error message is trying to say.\r\n\r\n", "Any pointers for the following error would be greatly appreciated. I am new to GitHub and its working model. So, please do let me know what details you would need that would help me in resolving this issue being encountered -\r\n\r\nbash_prompt: ~ $ python\r\nPython 2.7.3 (default, Oct 26 2016, 21:01:49) \r\n[GCC 4.6.3] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: clock_gettime\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error for some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "@namrathaurs welcome! Please check if #121 helps. If not It would be interesting to open a new issue and if you do that could you also specify how you installed TensorFlow?", "@Carmezim - Thanks for your quick reply to my issue here on GitHub! Appreciate it.\r\n\r\nJust as an FYI here, since I already opened a new issue :\r\nWell, I did check #121 as you suggested before I could post my issue here. However, when I was reading about updating the GLIBC version, I was fumbled as I read that a new GLIBC would mean upgrading the current version of Ubuntu to 14.04 (which may not be feasible in my scenario). Also, it was mentioned that Ubuntu 12.04 supports only GLIBC_2.16. If at all it needs to be upgraded to the next higher version, then the entire Ubuntu OS may need to undergo an upgrade.\r\n\r\nI would be happy to hear from you about this as well on my new issue discussion that i created a couple of minutes ago. Thanks for all your help, in advance!", "just FYI, this worked for me too(for Mac OS X, though) , Philippe Remy's answer here : \r\nhttp://stackoverflow.com/questions/33622842/error-in-python-after-import-tensorflow-typeerror-init-got-an-unexpect", "First, I solve problem by running Tensorflow in Docker container but Docker has it's own difficulties so I solve problem completely by using Bazel. It works well without any problem.", "Failed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help"]}, {"number": 6725, "title": "text_classification_character_rnn.py is using GRU cell at old location(compile error)", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nMac\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.12.1\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nTry to compile/run (with latest 0.12.1 TF):\r\n\r\nhttps://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/learn/text_classification_character_rnn.py\r\n\r\nThis causes error:\r\n\r\nAttributeError: module 'tensorflow.contrib.rnn' has no attribute 'GRUCell'\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nLooks like GRUCell got moved from tensorflow.contrib.rnn to tf.nn.rnn_cell.RNNCell.  I tried making this change to the example code, but this leads to other errors.  It looks like the signature changed too.  \r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["here's the fix:\r\n\r\n  instead of \r\ncell = tf.contrib.rnn.GRUCell(EMBEDDING_SIZE)\r\n  use\r\ncell = tf.nn.rnn_cell.GRUCell(EMBEDDING_SIZE)\r\n\r\n  and instead of \r\n_, encoding = tf.contrib.rnn.static_rnn(cell, word_list, dtype=tf.float32)\r\n  use\r\n_, encoding = tf.nn.rnn(cell, word_list, dtype=tf.float32)\r\n", "I am not seeing this on the latest 1.0 version of TensorFlow.  No longer an issue, as far as I am concerned."]}]