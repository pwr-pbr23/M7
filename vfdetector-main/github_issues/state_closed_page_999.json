[{"number": 23395, "title": "deploying the Tensorflow model in Python", "body": " While I am training everything is working fine but when I move on for a realtime forecast or prediction, the output what I received flunked. I do not know why is this happening. I used the reference of teh code from here: https://www.kaggle.com/raoulma/ny-stock-price-prediction-rnn-lstm-gru/notebook And tried to implement or deploy using the same code with few changes.\r\n\r\nSee the following code:\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nimport sklearn\r\nimport sklearn.preprocessing\r\nimport datetime\r\nimport os\r\nimport tensorflow as tf\r\n\r\ndf = pd.read_csv(\"Realtime_Values.csv\", index_col = 0)\r\ndf.info()\r\ndef load_data(stock,seq_len):\r\n\r\n    data_raw = stock.as_matrix() # convert to numpy array\r\n    data = []\r\n\r\n    for index in range(len(data_raw) - seq_len): \r\n        data.append(data_raw[index: index + seq_len])\r\n    #print(len(data))\r\n    data = np.array(data);\r\n\r\n    x_forecast = data[:,:-1,:]\r\n    return x_forecast\r\n\r\ndef normalize_data(df):\r\n    cols = list(df.columns.values)\r\n    min_max_scaler = sklearn.preprocessing.MinMaxScaler()\r\n    df = pd.DataFrame(min_max_scaler.fit_transform(df.values))\r\n    df.columns = cols\r\n    return df\r\nmodel_path =\"modelsOHLC\"\r\nseq_len = 9\r\n# parameters\r\nn_steps = seq_len-1 \r\nn_inputs = 4\r\nn_neurons = 100 \r\nn_outputs = 4\r\nn_layers = 4\r\nlearning_rate = 0.01\r\nbatch_size = 10\r\nn_epochs = 1000\r\ntf.reset_default_graph()\r\n\r\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\r\ny = tf.placeholder(tf.float32, [None, n_outputs])\r\nlayers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.elu)\r\n          for layer in range(n_layers)]\r\nmulti_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\r\nrnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\r\n\r\nstacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) \r\nstacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\r\noutputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\r\noutputs = outputs[:,n_steps-1,:] # keep only last output of sequence\r\n\r\nloss = tf.reduce_mean(tf.square(outputs - y)) # loss function = mean squared error \r\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) \r\ntraining_op = optimizer.minimize(loss)\r\nsaver = tf.train.Saver()\r\nsess  =tf.Session()\r\nsess.run(tf.global_variables_initializer())    \r\nif(tf.train.checkpoint_exists(tf.train.latest_checkpoint(model_path))):\r\n        saver.restore(sess, tf.train.latest_checkpoint(model_path))\r\ndf = normalize_data(df)\r\nx_forecast = load_data(df,seq_len)\r\ny_forecast_pred = sess.run(outputs, feed_dict={X: x_forecast})\r\nprint(y_forecast_pred)\r\n```\r\nI am getting weird output values and are not expected. The training and testing worked well. But when it came to real time scenario, everything was gone and I thought that need to train again. Why my saved models are not working as expected? This is truely a tensorflow bug.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: I dont know that. I am using desktop PC.\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\r\n\r\n- Python version: Python 3.5\r\n- Bazel version (if compiling from source): ?\r\n- GCC/Compiler version (if compiling from source): ?\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: Not available right now.", "comments": ["Hi, @JafferWilson \r\n\r\nCan I have the output of the prediction or output from the prediction?\r\n\r\nThanks.\r\nAlan Lam", "Hello @HackerTon Sir,\r\nI have specified that I am not with the model right now. But please let me know what exactly you want to have. The output of testing result and the real time or what exactly I can help with?", "Hi @JafferWilson,\r\n\r\nI might missed the particular statement. 'GPU Model and memory' is to specify GPU Model used in your computer and VRAM available in the particular GPU instead of the availability of nnModel. Can I have a look on the testing results and training results? You are running in a python notebook right?\r\n\r\nThanks,\r\nAlan Lam", "Hello @HackerTon ,\r\nNo I am using a python file. I am not trying to use any notebook .. But I can as soon as I copy and paste my code I will share my results. By the way I am using GTX Titan Xp graphics card.\r\n", "> Hello @HackerTon ,\r\n> No I am using a python file. I am not trying to use any notebook .. But I can as soon as I copy and paste my code I will share my results. By the way I am using GTX Titan Xp graphics card.\r\n\r\n@JafferWilson  -  Any update ?", "In \"awaiting response\" status for more than 3 days. Hence closing this. Please post the updates here if any, we will reopen the issue. Thanks !"]}, {"number": 23394, "title": "add dynamic shape support to dense_image_warp", "body": "Hi, dear tensorflower.\r\nI made a pull request to add dynamic shape input support to\r\ntf.contrib.image.dense_image_warp.\r\nPlease review it, or maybe accept it.\r\nI have checked the code style, and tested the code.\r\nThanks!\r\nPS: This is the corresponding issue https://github.com/tensorflow/tensorflow/issues/23325", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I have signed the CLA agreement.\r\n\r\n> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n\r\nI signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Hi @fcole , I have changed the code. Thanks for your suggestions. Googlebot said there's something wrong with the CLA agreement. Would you please check your CLA agreement? Since I have applied your suggestion, you are a co-author of this PR.", "I have emailed cla list to figure out why it is asking for a CLA from me. It shouldn't need one since I am in the Google org.", "Hi @zldrobit , the CLA people seem to think the error is actually due to a missing CLA for your email. Could you check again to make sure you have a CLA for that address on file? Even though it seems like you should given the bot message above...", "@fcole\uff0c it' my mistake. I have signed a new CLA for zldrobit@126.com.", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 23393, "title": "request implementation of conv1DLSTM, conv3DLSTM in keras", "body": "\r\n**System information**\r\n- 1.12\r\n- No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nthere is no conv1DLSTM, conv3DLSTM in keras.\r\n\r\n**Will this change the current api? How?**\r\nno\r\n", "comments": ["@zh794390558,\r\nSorry for the delayed response.\r\n\r\nCan you please let us know the use cases for the new APIs that you have requested? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 23392, "title": "Build  model_pruning for bug", "body": "Hi,\r\nbazel build -c opt examples/cifar10:cifar10_{train,val}\r\nEERROR: Skipping 'examples/cifar10:cifar10_train': no such package 'tensorflow/contrib/examples/cifar10': BUILD file not found on package path\r\nERROR: no such package 'tensorflow/contrib/examples/cifar10': BUILD file not found on package path\r\nThank you for your help!", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 23391, "title": "Is the tensorflow framework wrong?", "body": "When I use the tensorflow framework to train my model through the CPU, I get an tensorflow unexpected error.Error is as follows:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1361, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1340, in _run_fn\r\n    target_list, status, run_metadata)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 5, message: could not create a eltwise forward primitive descriptor, in file tensorflow/core/kernels/mkl_relu_op.cc:457\r\n\t [[Node: DAGLayer_4/Relu = _MklRelu[T=DT_FLOAT, _kernel=\"MklOp\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](DAGLayer_4/xw_plus_b, DMT/_8)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/worker/bin/run.py\", line 375, in task_main\r\n    await module_run(api) if iscoroutinefunction(module_run) else module_run(api)\r\n  File \"/worker/package/smiles_package_DAG_qlaVw/train_model/__init__.py\", line 135, in run\r\n    callbacks=[processcallback])\r\n  File \"/worker/package/smiles_package_DAG_qlaVw/DAG_tensorgraph.py\", line 91, in fit_generator\r\n    fetched_values = self.session.run(fetches, feed_dict=train_generator)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1137, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 5, message: could not create a eltwise forward primitive descriptor, in file tensorflow/core/kernels/mkl_relu_op.cc:457\r\n\t [[Node: DAGLayer_4/Relu = _MklRelu[T=DT_FLOAT, _kernel=\"MklOp\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](DAGLayer_4/xw_plus_b, DMT/_8)]]\r\n\r\nCaused by op 'DAGLayer_4/Relu', defined at:\r\n  File \"main.py\", line 32, in <module>\r\n    main()\r\n  File \"main.py\", line 28, in main\r\n    router.task_run()\r\n  File \"/worker/bin/router.py\", line 129, in task_run\r\n    p.start()\r\n  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/context.py\", line 223, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/context.py\", line 277, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/popen_fork.py\", line 26, in __init__\r\n    self._launch(process_obj)\r\n  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/popen_fork.py\", line 80, in _launch\r\n    code = process_obj._bootstrap()\r\n  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\r\n    self.run()\r\n  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/worker/bin/router.py\", line 111, in run_worker\r\n    run([worker])\r\n  File \"/atlas/home/test/.local/lib/python3.6/site-packages/autobahn/asyncio/component.py\", line 373, in run\r\n    loop.run_forever()\r\n  File \"/usr/local/anaconda3/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\r\n    self._run_once()\r\n  File \"/usr/local/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1431, in _run_once\r\n    handle._run()\r\n  File \"/usr/local/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\r\n    self._callback(*self._args)\r\n  File \"/worker/bin/router.py\", line 38, in onJoin\r\n    api = await task_main(task_id)\r\n  File \"/worker/bin/run.py\", line 375, in task_main\r\n    await module_run(api) if iscoroutinefunction(module_run) else module_run(api)\r\n  File \"/worker/package/smiles_package_DAG_qlaVw/train_model/__init__.py\", line 115, in run\r\n    model = DAG_fit.load_from_dir(model_dir)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/deepchem/models/tensorgraph/tensor_graph.py\", line 818, in load_from_dir\r\n    tensorgraph.restore()\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/deepchem/models/tensorgraph/tensor_graph.py\", line 785, in restore\r\n    self.build()\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/deepchem/models/tensorgraph/tensor_graph.py\", line 476, in build\r\n    layer.create_tensor(training=self._training_placeholder)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/deepchem/models/tensorgraph/graph_layers.py\", line 686, in create_tensor\r\n    batch_outputs = self.DAGgraph_step(batch_inputs, self.W_list, self.b_list)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/deepchem/models/tensorgraph/graph_layers.py\", line 705, in DAGgraph_step\r\n    outputs = self.activation(outputs)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/deepchem/models/tensorgraph/activations.py\", line 96, in relu\r\n    return model_ops.relu(x, alpha=alpha, max_value=max_value)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/deepchem/models/tensorgraph/model_ops.py\", line 516, in relu\r\n    x = tf.nn.relu(x)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4331, in relu\r\n    \"Relu\", features=features, name=name)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nAbortedError (see above for traceback): Operation received an exception:Status: 5, message: could not create a eltwise forward primitive descriptor, in file tensorflow/core/kernels/mkl_relu_op.cc:457\r\n\t [[Node: DAGLayer_4/Relu = _MklRelu[T=DT_FLOAT, _kernel=\"MklOp\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](DAGLayer_4/xw_plus_b, DMT/_8)]]\r\n```\r\n\r\nI managed to train this model on my GPU by running a model built by tensorflow, but I got this unexpected error when I used CPU training.\r\n\r\nWhy can't I train on the CPU? My version of tensorflow is 1.6, please give me some guidance, thank you!", "comments": ["There is missing too much information. Did you compile TensorFlow from source ? \r\n\r\nAs I can see first, it seems like there is an error with an operator from MKL (Math Kernel Library) operator. Seems like to compiled TensorFlow using config=mkl. Is this right ?\r\n\r\nRedo your issue using a template : https://github.com/tensorflow/tensorflow/issues/new/choose", "It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. Thank you for your cooperation.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23390, "title": "FLOP calculation wrong for matmul with batch dimension", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.11.0-rc2-4-gc19e29306c 1.11.0\r\n- Python version: 3.6.5\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nFLOPS is always 0 when specifying batch dimension\r\n\r\n**Describe the expected behavior**\r\nFLOPS should be linear in batch dimension\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ng = tf.Graph()\r\nwith g.as_default():\r\n    b, m, p, q = 2, 25, 16, 9\r\n    A = tf.Variable(tf.zeros([b, m, p]))\r\n    B = tf.Variable(tf.zeros([b, p, q]))\r\n    C = tf.matmul(A,B)\r\n\r\n    flops = tf.profiler.profile(g, options = tf.profiler.ProfileOptionBuilder.float_operation())\r\n    print('FLOP should be', b * m * q * 2 * p)\r\n    print('Calculated FLOP', flops.total_float_ops)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Related: https://github.com/tensorflow/tensorflow/issues/19746", "This is a duplicate of #22071", "@WuTheFWasThat  - Hi, request you to post the same code snippet and other details in #22071.\r\nWe can close this issue to avoid duplicates. We will be looking into the issue and post comments in 22071.  Thanks !"]}, {"number": 23389, "title": "projector.tensorflow.org - \"Load bookmarks\" broken in Chrome 70", "body": "The \"Load bookmarks\" button (below) in projector.tensorflow.org doesn't seem to be working: \r\n\r\n![screen shot 2018-10-30 at 2 38 16 pm](https://user-images.githubusercontent.com/8292856/47752506-dd6a6800-dc51-11e8-9ac9-3fb93a6bf599.png)\r\n\r\n\r\n\r\n\r\nBelow is the error as reported in Chrome JS console:\r\n\r\n```\r\n(index):10152 Uncaught TypeError: Cannot read property 'select' of undefined\r\n    at HTMLElement.b._uploadFile ((index):10152)\r\n    at HTMLElement.d ((index):4668)\r\n    at HTMLElement.fire ((index):4690)\r\n    at Object.fire ((index):4678)\r\n    at Object.forward ((index):4685)\r\n    at Object.click ((index):4684)\r\n    at HTMLElement.handleNative ((index):4674)\r\n```\r\n\r\n**System information**\r\nChrome Version 70.0.3538.77 (Official Build) (64-bit)\r\nmacOS High Sierra\r\n\r\n", "comments": ["I checked with Chrome Version 69.0.3497.100 (Official Build) (64-bit), macOS High Sierra and the error is no longer applicable. Is this still an issue for you?\r\n", "Thanks for checking this out. Yes, getting the same error with Chrome Version 70.0.3538.77 (Official Build) (64-bit). Also got the same error on Win10 machines running Chrome 71.\r\nSeems that the \"load bookmark\" button is broken with all current versions of Chrome (70 and above).", "We have an [open issue on tensorboard repo](https://github.com/tensorflow/tensorboard/issues/835) tracking this. Thus closing, so that we can continue discussion on same thread. Thanks!"]}, {"number": 23388, "title": "Avoid index out of range in traceable_stack", "body": "Fix for https://github.com/rstudio/tensorflow/issues/272 which blocks use of devices in TensorFlow 1.11.\r\n\r\nAn `list index out of range` exception triggers in [traceable_stack.py#L64](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/traceable_stack.py#L64) which is initialized in [ops.py#L4253](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L4253) when running from R:\r\n\r\n```\r\nlibrary(tensorflow)\r\ndata <- tf$device(\"/cpu:0\")\r\ncontext <- data$`__enter__`()\r\n```\r\n\r\nThe frame records from `tf_stack.extract_stack()` at [ops.py#L4253](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L4253) looks like:\r\n\r\n```\r\n<stdin>\r\n/\u2026/r-tensorflow/lib/python3.6/contextlib.py\r\n/\u2026/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\r\n```\r\n\r\nHowever, while used from R and other tools that embed Python directly, looks like:\r\n\r\n```\r\n/\u2026/r-tensorflow/lib/python3.6/contextlib.py\r\n/\u2026/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\r\n```\r\n\r\nSince the Python interpreter is not available when called from R (which embeds Python as a library), `<stdin>` is missing. One fix is to protect against this be reducing the call stack offset when shorter than expected.\r\n\r\nPlease let us know if you have additional feedback or suggestions to get them implemented, thanks!\r\n\r\nChange of behavior originally introduced with https://github.com/tensorflow/tensorflow/commit/e5cc33df74ec4f761da26c87bb785edfa3fb8280.", "comments": ["@skye Do you recall when this code was added in Changelist 206120307? (It looks like you were one of the original reviewers, so I'm hoping you have more context for the bug.)", "@harshini-gadige anything you need me to do to resolve the build issues, build failures look unrelated to this change. Thanks.", "@skye  - Hi, could you please look into these build failures.", "@yifeif do you know what's up with the broken builds? They appear unrelated to this change. Is there a way to merge anyway?", "cc @gunan as well", "It's possible that they were broken at head already. The build failures do not block PR import btw. I just triggered the import. You should be able to bypass the existing Kokoro failures with the CL.", "@harshini-gadige @hgadig @skye @yifeif Could someone help me understand if this fix will make it to TF 1.13? I was hoping it would hit 1.12 which was released on Nov 6, 2018, so after this got merged to main. Does someone cherry-pick commits for each release? Is there anything else I need to do to get this into TF 1.13?\r\n\r\nThank you in advance!", "@javierluraschi , looks like branch r1.13 was cut about 20 days again, which was after this PR was merged. It should be included in 1.13. cc @aselle.", "Thank you!"]}, {"number": 23387, "title": "[Intel MKL] Fix docker ffmpeg issue", "body": "Comment in the \"apt-get install\" broke the command stream: https://github.com/NervanaSystems/private-tensorflow/blob/master/tensorflow/tools/ci_build/install/install_deb_packages.sh#L47 \r\n\r\nRestart the apt-get install ffmpeg after comment. \r\n\r\n@yifeif  FYI. ", "comments": ["Fixes Public CI failure: https://tensorflow-ci.intel.com/job/tensorflow-mkl-linux-cpu/3003/console \r\n[bdw] \u001b[91m/install/install_deb_packages.sh: line 49: ffmpeg: command not found\r\n[bdw] \u001b[0mThe command '/bin/sh -c /install/install_deb_packages.sh' returned a non-zero code: 127", "Sorry about this and thanks for sending a fix @wei-v-wang! I actually just about to submit the same fix internally. I will close this PR once it propagates to GitHub if you don't mind. Thanks again!", "Ok, great. Thanks for the update!"]}, {"number": 23386, "title": "InvalidArgumentError: Invalid name: An op that loads optimization parameters into HBM for embedding. (ConfigureTPUEmbeddingHost)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.12.0-rc0\r\n- Python version: 3.6.7rc1\r\n- Bazel version (if compiling from source): 0.19.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 10.0.130 / 7.3.1.20\r\n- GPU model and memory: Nvidia M1000M / ~4GB\r\n\r\n**Describe the current behavior**\r\nPyPI packages for tensorflow-gpu didn't work for me on Ubuntu 18.10, but I (somehow) managed to compile the latest Tensorflow. I then tried to run some code that I was working on before (and which used to run fine) and got the following error:\r\n```\r\n\r\nUsing TensorFlow backend.\r\nTensorflow: 1.12.0-rc0\r\nTraceback (most recent call last):\r\n  File \"/home/neopostmodern/.PyCharm2018.1/config/scratches/scratch_3.py\", line 10, in <module>\r\n    model.add(CuDNNLSTM(64, input_shape=(10, 39), return_sequences=True))\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/engine/sequential.py\", line 165, in add\r\n    layer(x)\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 532, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 431, in __call__\r\n    self.build(unpack_singleton(input_shapes))\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/layers/cudnn_recurrent.py\", line 425, in build\r\n    from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 40, in <module>\r\n    from tensorflow.contrib import distribute\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/__init__.py\", line 34, in <module>\r\n    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py\", line 29, in <module>\r\n    from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/tpu/__init__.py\", line 69, in <module>\r\n    from tensorflow.contrib.tpu.python.ops.tpu_ops import *\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/ops/tpu_ops.py\", line 39, in <module>\r\n    resource_loader.get_path_to_datafile(\"_tpu_ops.so\"))\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 60, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid name: \r\nAn op that loads optimization parameters into HBM for embedding. Must be\r\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\r\nembedding table configuration. For example, this op is used to install\r\nparameters that are loaded from a checkpoint before a training loop is\r\nexecuted.\r\n\r\nparameters: A tensor containing the initial embedding table parameters to use in embedding\r\nlookups using the Adagrad optimization algorithm.\r\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\r\nlookups using the Adagrad optimization algorithm.\r\ntable_name: Name of this table; must match a name in the\r\n  TPUEmbeddingConfiguration proto (overrides table_id).\r\nnum_shards: Number of shards into which the embedding tables are divided.\r\nshard_id: Identifier of shard for this operation.\r\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\r\n  (deprecated).\r\n (Did you use CamelCase?); in OpDef: name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" input_arg { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: DT_FLOAT type_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" number_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type_list_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } input_arg { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: DT_FLOAT type_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" number_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type_list_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" default_value { i: -1 } description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" has_minimum: true minimum: -1 } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" default_value { s: \"\" } description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } summary: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" is_stateful: true\r\n```\r\n\r\n**Describe the expected behavior**\r\nIt should run, as it did before.\r\n\r\n**Code to reproduce the issue**\r\nReduced to the minimum:\r\n```\r\nimport tensorflow as tf\r\nfrom keras.models import Sequential\r\nfrom keras.layers import CuDNNLSTM\r\n\r\ntf.set_random_seed(42)\r\n\r\nprint(f'Tensorflow: {tf.__version__}')\r\n\r\nmodel = Sequential()\r\nmodel.add(CuDNNLSTM(64, input_shape=(10, 39), return_sequences=True))\r\n```\r\n", "comments": ["I am seeing the same issue on my build server. Configuration is as follows:\r\n\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 x64\r\n* TensorFlow installed from (source or binary): Source\r\n* TensorFlow version: master branch on Github\r\n* Python version: 3.6.7rc1\r\n* Bazel version (if compiling from source): 0.19.0\r\n* GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n* CUDA/cuDNN version: n/a\r\n\r\nI traced the problem to the function `RegisterPerTableLoadOpsForAlgorithmBody()` in  `contrib/tpu/ops/tpu_embedding_ops.cc`. This function somehow manages to create an `OpDef` C++ proto object with incorrect string pointers. I suspect a compiler bug.\r\n", "Update: Recreating my Linux test VM from scratch made the problem go away for me.", "Unfortunately I can't recreate my machine since it's my regular one - any hints how to proceed? (Or how recreate the substantial parts...)", "The code snippet you provided works fine for me. It doesn't look like a bug on TF end. You can try posting it on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow)", "Yes, the code snippet is supposed to work. It used to work on my machine, and it does on @frreiss machine too. But, as observed on both my machine and @frreiss machine, under certain circumstances, when building TF from source, it doesn't.\r\nSo, I don't see why this would not be a TF issue, although maybe build related? [This comment](https://github.com/tensorflow/tensorflow/issues/23386#issuecomment-434752741) hints to a possible origin, but I'm not savvy enough to investigate that further.", "Update:\r\nIf I replace `CuDNNLSTM` with `LSTM` the above code runs without errors. Doesn't make it less weird, but maybe it's CUDA related - although [in the other occurence of this behavior](https://github.com/tensorflow/tensorflow/issues/23386#issuecomment-434752741) no CUDA was present.\r\n@frreiss can you share the code you were running when you observed the error maybe?", "I also hit this issue when upgrading from 1.10 to 1.12.\r\nAdding `--define=framework_shared_object=true` to the bazel build suppressed it, so this may be a bug in the static build.", "Confirm roots of issue in static build. \r\nAfter removing `--config=monolithic` I can build TF without problems.", "@neopostmodern Apologies for the delay in response. Is this still an issue for you? Do you see the same behavior against latest TF?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Just got it on 1.13.0-rc2 with monolithic build:\r\n```\r\n  export USE_DEFAULT_PYTHON_LIB_PATH=1\r\n  export TF_DOWNLOAD_CLANG=0\r\n  export TF_ENABLE_XLA=0\r\n  export TF_NEED_COMPUTECPP=0\r\n  export TF_NEED_CUDA=0\r\n  export TF_NEED_HDFS=0\r\n  export TF_NEED_IGNITE=0\r\n  export TF_NEED_MPI=0\r\n  export TF_NEED_OPENCL_SYCL=0\r\n  export TF_NEED_ROCM=0\r\n  export TF_NEED_TENSORRT=0\r\n  export TF_SET_ANDROID_WORKSPACE=0\r\n  export CC_OPT_FLAGS=\"-march=haswell\"\r\n\r\n  bazel build -j 6 \\\r\n      --config=opt \\\r\n      --config=mkl \\\r\n      --config=monolithic \\\r\n      --config=noaws \\\r\n      --config=nogcp \\\r\n      --config=nohdfs \\\r\n      --config=noignite \\\r\n      --config=nokafka \\\r\n      --config=nonccl \\\r\n      //tensorflow:libtensorflow.so \\\r\n      //tensorflow:libtensorflow_cc.so \\\r\n      //tensorflow:install_headers \\\r\n      //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nbazel 19.2, linux\r\nTried to run example script from [hand3d](https://github.com/lmb-freiburg/hand3d):\r\n```\r\n> python run.py\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 47, in <module>\r\n    keypoints_scoremap_tf, keypoint_coord3d_tf = net.inference(image_tf, hand_side_tf, evaluation)\r\n  File \"/home/versus/Programming/gesture/hand3d/nets/ColorHandPose3DNetwork.py\", line 78, in inference\r\n    hand_scoremap = self.inference_detection(image)\r\n  File \"/home/versus/Programming/gesture/hand3d/nets/ColorHandPose3DNetwork.py\", line 152, in inference_detection\r\n    x = ops.conv_relu(x, 'conv%d_%d' % (block_id, layer_id+1), kernel_size=3, stride=1, out_chan=chan_num, trainable=train)\r\n  File \"/home/versus/Programming/gesture/hand3d/utils/general.py\", line 57, in conv_relu\r\n    tensor = cls.conv(in_tensor, layer_name, kernel_size, stride, out_chan, trainable)\r\n  File \"/home/versus/Programming/gesture/hand3d/utils/general.py\", line 45, in conv\r\n    tf.contrib.layers.xavier_initializer_conv2d(), trainable=trainable, collections=['wd', 'variables', 'filters'])\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/util/lazy_loader.py\", line 61, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/util/lazy_loader.py\", line 44, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/contrib/__init__.py\", line 43, in <module>\r\n    from tensorflow.contrib import distribute\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/contrib/distribute/__init__.py\", line 33, in <module>\r\n    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py\", line 27, in <module>\r\n    from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/contrib/tpu/__init__.py\", line 69, in <module>\r\n    from tensorflow.contrib.tpu.python.ops.tpu_ops import *\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/contrib/tpu/python/ops/tpu_ops.py\", line 39, in <module>\r\n    resource_loader.get_path_to_datafile(\"_tpu_ops.so\"))\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/load_library.py\", line 62, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid name: \r\nAn op that loads optimization parameters into HBM for embedding. Must be\r\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\r\nembedding table configuration. For example, this op is used to install\r\nparameters that are loaded from a checkpoint before a training loop is\r\nexecuted.\r\n\r\nparameters: A tensor containing the initial embedding table parameters to use in embedding\r\nlookups using the Adagrad optimization algorithm.\r\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\r\nlookups using the Adagrad optimization algorithm.\r\ntable_name: Name of this table; must match a name in the\r\n  TPUEmbeddingConfiguration proto (overrides table_id).\r\nnum_shards: Number of shards into which the embedding tables are divided.\r\nshard_id: Identifier of shard for this operation.\r\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\r\n  (deprecated).\r\n (Did you use CamelCase?); in OpDef: name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded\r\n...\r\n```", "Got same issue with tensorflow 1.12.0 (compiled from source with gcc 4.8 + python 2.7) \r\nas **-march=native** (default) was getting me less performance with cifar10, i used following to build tensorflow -\r\n`bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --define=grpc_no_ares=true -k //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nwhile running - \r\n` python -c 'from tensorflow.examples.tutorials.mnist import input_data' `\r\ni got \r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid name:\r\n`\r\nerror log is attached  -[mnist_test_error.log](https://github.com/tensorflow/tensorflow/files/2905136/mnist_test_error.log)\r\n\r\nUPDATE: following worked after adding --define=framework_shared_object=true in bazel build comilation line, thank you @lesniewski \r\n\r\n```\r\n[puneet@mach MNIST]$ python -c 'from tensorflow.examples.tutorials.mnist import input_data'\r\n[puneet@mach MNIST]$ echo $?\r\n0\r\n```\r\n\r\n", "I ran into this as well and this is just to document this for any future encounters. Do not add `@com_google_protobuf//:protobuf` as a bazel dependency, but use `@com_google_protobuf//:protobuf_headers` instead. If not, you end up linking protobuf twice causing an issue that all mutable protobuf strings point to the same memory address. What happens is that the last string manipulation overwrites all other protobuf strings. The name you got seems to be overwritten from `parameter_descriptions`. It's a very obscure error message and is described in a bit more detail [here](https://github.com/protocolbuffers/protobuf/issues/6031).\r\n\r\nThe reason a monolithic build fixes this, is because it just packages everything as one basically avoiding this problem in the first place. But I don't think that's a very good solution as it's just avoiding the problem instead of actually fixing it."]}, {"number": 23385, "title": "Ignoring visible gpu device (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1) with Cuda compute capability 6.1. The minimum required Cuda capability is 7.0", "body": "I downloaded the docker container at (floydhub/tensorflow:1.5.0-gpu.cuda9cudnn7-py3_aws.22). But when I tried to run a sample gpu script I got the \"ignoring visible GPU\" error mentioned in the title. \r\n\r\n**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 16.04:\r\n- TensorFlow pre installed in docker image:\r\n- TensorFlow version 1.5.0:\r\n- Python version: 3.6.2\r\n- CUDA/cuDNN version: 9.0.176\r\n- GPU model and memory: GeForce GTX 1070 (8GB memory)\r\n- Driver version: 390.87\r\n\r\n**Code to reproduce the issue using python**\r\n       import tensorflow as tf\r\n       print(tf.contrib.eager.num_gpus())\r\n  \r\n  Output:\r\n   2018-10-30 18:48:01.232945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-10-30 18:48:01.233214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.695\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 6.59GiB\r\n**2018-10-30 18:48:01.233235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1168] Ignoring visible gpu device (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1) with Cuda compute capability 6.1. The minimum required Cuda capability is 7.0**.\r\n0\r\n\r\nTF 1.5 clearly doesn't require CUDA CC of 7.0. The 1070 has a CC of 6.1 according to NVIDIA which should be enough to support TF1.5. I also have the latest drivers. Someone suggested recompiling TF from source, but that kind of defeats the purpose of using a docker container. Is there any other way? Any pointers will be much appreciated. Thank you. ", "comments": ["@satishrClemson  -  Hi, please use CUDA 9 cuDNN 7 for TF v 1.5. Please see below for the details.\r\nBelow are the tested build configurations from Tensorflow end.\r\n![image](https://user-images.githubusercontent.com/42781361/47748837-f9b5d700-dc48-11e8-9097-d3035bc63557.png)\r\n", "I already verified that, as mentioned in the problem description.  \r\n\r\nSystem information\r\n\r\n    OS Platform and Distribution : Linux Ubuntu 16.04:\r\n    TensorFlow pre installed in docker image:\r\n    TensorFlow version 1.5.0:\r\n    Python version: 3.6.2\r\n    CUDA/cuDNN version: 9.0.176\r\n    GPU model and memory: GeForce GTX 1070 (8GB memory)\r\n    Driver version: 390.87\r\n", "The official docker images are here: https://hub.docker.com/r/tensorflow/tensorflow/tags/\r\n\r\nThe docker image referenced: floydhub/tensorflow:1.5.0-gpu.cuda9cudnn7-py3_aws.22, is not an official image, and when built they specified TF_CUDA_COMPUTE_CAPABILITIES=7.0. An issue could be open against floydhub to have them rebuild the image with expanded TF_CUDA_COMPUTE_CAPABILITIES. \r\n\r\n", "This issue appears fixed if you use the docker image floydhub/1.5.0-gpu.cuda9cudnn7-py3_aws.33, \r\n\r\nThe referenced image floydhub/tensorflow:1.5.0-gpu.cuda9cudnn7-py3_aws.22 is 9 months old, and I can see they added TF_CUDA_COMPUTE_CAPABILITIES=3.7,7.0   6 months ago:\r\nhttps://github.com/floydhub/dockerfiles/blame/master/dl/tensorflow/1.5.0/Dockerfile-py3.gpu.cuda9cudnn7_aws\r\n\r\nFuture problems with floydhub docker images should be opened against that github repo.", "@satishrClemson  - Hi, please raise the issue in the relevant repo. Feel free to close this. Thanks !", "Awesome. Thanks @wdirons. Will take it up with Floydhub. "]}, {"number": 23384, "title": "TensorFlow Lite Interpreter get_tensor() ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.5.1804 (Core)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: Nightly 1.13.0-dev20181029\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Run the _label_image_tflite.py_ script attached with _gray128.jpg_, _inception_v3-l1a-zero-bias-zero-weights.tflite_, _labels.txt_, and _layer_outputs_gray128_f32_b0w0/_ in the same directory\r\n\r\n### Describe the problem\r\nWhen trying to write out tensors to file using the TensorFlow Lite Interpreter::get_tensor() function, mostly incorrect data is being returned. For the attached input, _gray128.jpg_, an image with all pixels set to RGB(128, 128, 128), I expect the layer outputs to be fairly repetitive, but it is not.\r\n\r\nTo further narrow down the issue, I modified the bias and weight tensors for the first layer to 0.0. After convolution and activation, I would expect the output tensor to be completely 0's, but it is not. The output is consistent across runs (deterministic). The input, output, bias, and weight tensors all seem to be written out correctly, but most of the intermediate output tensors do not seem to be.\r\n\r\nI am doing this to try and verify the intermediate outputs with my own calculations. I was hoping to get inception verified with floating point, then with the uint8 quantized model, then with my own model.\r\n\r\n### Source code / logs\r\nAttached is:\r\n\r\n- _label_image_tflite.py_\r\n- _inception_v3-l1a-zero-bias-zero-weights.tflite_\r\n- _labels.txt_\r\n- _gray128.jpg_\r\n\r\nJust run the script (`python3 label_image_tflite.py`) in the same directory as the other files and a subdirectory, _layer_outputs_gray128_f32_b0w0/_ .\r\n\r\nYou can see in _layer_outputs_gray128_f32_b0w0/6_tensor.txt_ that the output is not zero and doesn't appear to be repetitive despite the gray uniform input.\r\n\r\nThe label_image script was modified from the [TensorFlow Lite example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/label_image.py). The modified portion:\r\n```python\r\n.\r\n.\r\n.\r\n  interpreter.set_tensor(input_details[0]['index'], input_data)\r\n  interpreter.invoke()\r\n\r\n  # BEGIN Custom code to write out all tensors to files in a subdirectory\r\n  ind = 0\r\n  try:\r\n    while True:\r\n      dets = interpreter._get_tensor_details(ind)\r\n      tens = interpreter.get_tensor(ind)\r\n      # Expects subdirectory to exist\r\n      f_dets = open(\"layer_outputs_gray128_f32_b0w0/{}_details_{}.txt\".format(ind, dets['name'].replace('/', '-')), \"w\")\r\n      # Write details\r\n      f_dets.write(\"tensor[{}]: {}\\n\".format(ind, dets))\r\n      # Write tensor data\r\n      # Output tensors (Relu's) are giving incorrect values\r\n      tens.tofile(\"layer_outputs_gray128_f32_b0w0/{}_tensor.txt\".format(ind), \",\")\r\n      ind += 1\r\n      f_dets.close()\r\n  except:\r\n    print(\"Finished writing tensor data\")\r\n  # END custom code\r\n\r\n  output_data = interpreter.get_tensor(output_details[0]['index'])\r\n  results = np.squeeze(output_data)\r\n.\r\n.\r\n.\r\n```\r\n\r\nhttps://drive.google.com/open?id=1OhFLGVm9SVb9RnGxqw6gdePc12i5y1Nw (84MB)\r\n\r\nIf there is an alternate way I can inspect the intermediate outputs, that would be appreciated as well.\r\n\r\nThanks!\r\n\r\nEdit: I suspect this issue is related: #22891", "comments": ["After seeing [this answer](https://stackoverflow.com/a/53105809) on SO about it, I guess intermediate tensors are not guaranteed to have useful data, only the graph output(s). That makes sense for memory reasons, so I probably shouldn't expect this to work anytime soon.\r\n\r\nI was able to (hackily) achieve what I needed by modifying the .tflite file with different output tensor indices and re-running the interpreter for each new output tensor.\r\n\r\nMaybe the documentation can be updated to note that get_tensor() and tensor() can't be used to read intermediate results?", "Thanks @raymond-li, you're right that there are no guarantees about intermediate tensors. That's good feedback on updating the documentation. ", "Hi, I would like to ask how'd you modify the tflite file? ", "@hamirshekhawat I changed the appropriate bytes in the tflite file in my python script before running the model through the python engine to generate the intermediate outputs.\r\n\r\nI looked at the .proto file and did some inspection of the tflite file in a hex viewer/editor to find the bytes corresponding to the output tensor index. I provided my script with this as a byte offset.\r\n\r\nIf you still need more details, I can update in week or so. Depending on your goal, it might be better to actually parse the tflite file, change the field, and save it.", "Thanks @raymond-li . I am trying to extract a feature vector from the conv layer \"Avg Pool / s1\" in mobilenet. I am working on an Android app so I mostly dont have the luxury to work with python. But the process of changing the appropriate bytes would still work.", "This issue is still relevant. In the documentation there is no mention of the usage of function get_tensor() begin only applicable to input and output tensors of a given model. Please add this to the documentation, deprecate the function or make it possible to initialize the engine so that it creates unique memory locations for each tensor so that it is possible to use this \"feature\" for other tensors also. Right now it is mostly useless.", "I uploaded the hack-y modified label_image.py script I used to inspect intermediate tensors here: https://github.com/raymond-li/tflite_tensor_outputter\r\n\r\nIt creates two output files for each tensor, a details file and a data file."]}, {"number": 23383, "title": "Infinite loop in graph optimization", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.11.0\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n\r\nWhen calling `sess.run` for certain graphs (e.g., see below), the session call gets stuck in an infinite loop.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe session call should complete normally.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nconst1 = tf.zeros((1,))\r\nconst2 = tf.zeros((1,))\r\nconst3 = tf.zeros((1,))\r\nconst4 = tf.zeros((1,))\r\nph = tf.placeholder(tf.float32, (1,))\r\n\r\nc1 = tf.concat((const1, const2, ph), axis=0)\r\nc2 = tf.concat((const3, const4, c1), axis=0)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(c2, feed_dict={ph: np.zeros((1,))})  # gets stuck in infinite loop here\r\n```\r\n\r\n**Other info / logs**\r\nSpecifically, the build process is getting stuck in the ``ConstantFolding`` optimization pass.  This is due to the new ``MergeConcat`` optimization introduced in https://github.com/tensorflow/tensorflow/commit/e96d65246835b3a33a55c70d1f1057517ef0aa8e.  It can create a graph structure that causes the ``PartialConcatConstFolding`` optimization to enter an infinite loop.\r\n", "comments": ["@rmlarsen can you take a look?", "Fixed in #23408"]}, {"number": 23382, "title": "Fixed some spellings in core.", "body": "Just some minor spell checks in /core. Please, double check.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@pmgdeb  Can you please run clang-format on your code.", "> @pmgdeb Can you please run clang-format on your code.\r\n\r\n@harshini-gadige Thanks for merging, I run clang-format and says: no modified files to format."]}, {"number": 23381, "title": "Fix the issue for class_weight used with tf.dataset", "body": "\r\nThis fix tries to address the issue raised in #22275 where class_weight used with tf.dataset will result in:\r\n```\r\nAxisError: axis 1 is out of bounds for array of dimension 1\r\n```\r\n\r\nThis fix fixes the issue by taking into account that y\r\ncould be tensor.\r\n\r\nThis fix fixes #22275.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang  -  Can this issue be moved to \"Ready to Pull\" status ? I see it requires reviewer approval. Can you please let me know.", "Thanks @harshini-gadige for the help. I think a reviewer approval is still needed.", "Ping @pavithrasv, any chance to take a look? ", "Nagging Reviewer @fchollet, @pavithrasv, @mrry: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Looks like the issue has been fixed as part of the following commit https://github.com/tensorflow/tensorflow/commit/c9049de2515cf8643faefa66cc4aea276d390912#diff-4f0d455edc07c640cbb22c8e39a61dd5\r\n\r\nCan we just submit the test case for this?", "Thanks @pavithrasv for the review. I have updated the PR with only the test included. The metrics has been skipped as well. Please take a look.", "@yongtang  Will it be available in tensor flow 1.13? or it is in the current  1.12 version of tensor flow?", "@was84san 1.13 branch was out before this PR I believe. So I assume this fix may only be in 2.0 unless it is cherry-picked by 1.13.", "Even with the changes of the [commit](https://github.com/tensorflow/tensorflow/commit/c9049de2515cf8643faefa66cc4aea276d390912#diff-4f0d455edc07c640cbb22c8e39a61dd5) referenced above, I've got the same problem of issue #22275. I'm using tensorflow 1.12 with keras interfaces and feeding the training with a tf.dataset. Looking into the code of commit, I made some changes in order to put my training to run, e.g. change the np.argmax function [here](https://github.com/tensorflow/tensorflow/blob/c9049de2515cf8643faefa66cc4aea276d390912/tensorflow/python/keras/engine/training_utils.py#L751) to a K.argmax. Another change was on [this](https://github.com/tensorflow/tensorflow/blob/c9049de2515cf8643faefa66cc4aea276d390912/tensorflow/python/keras/engine/training_utils.py#L757) comprehension list, I'd got an error saying that it only works with eager mode.\r\n```\r\nTypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.\r\n```\r\nI changed to a tf.contrib.lookup.HashTable but I'm sure that it isn't the correct solution since it doesn't handle with classes that aren't on the class_weight dict. The point is: it seems that the solution given on commit doesn't cover all the cases. Am I right or I did something wrong to still get these errors?", "If you use model.fit with a tf.data dataset iterator and provide the class_weight dictionary, then this error is thrown: \r\n`TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.`\r\n\r\nProbable reason: training_utils.py line 722: \r\n`weights = np.asarray([class_weight[cls] for cls in y_classes if cls in class_weight])`\r\n\r\nSince y_classes is a tf.tensor (not a numpy array) you cannot iterate using \"for\".", "@ DaneilDIQA did you tried to enable the eager execution  in your code and see if it will work."]}, {"number": 23380, "title": "MKL option not building successfully", "body": "Hello,\r\n\r\nTF isn't building for me with the MKL option (this problem started a couple days ago).\r\nBuild call and error trace are below. I'm trying to build in a Docker.\r\n\r\nFor what its worth, I actually have base MKL and MKL-DNN built into my system already, but I've never been able to make TF actually pick up those libraries (always errorred out).\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian:Stretch\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: Master branch\r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.19.0\r\n- GCC/Compiler version (if compiling from source): 8.2.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Build call**\r\n\r\n```bash \r\necho \"deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list && \\\r\n    curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - && \\\r\n    apt-get update && apt-get install -y bazel  && rm -rf /var/lib/apt/lists/* && \\\r\n    ldconfig && \\\r\n    pip uninstall -y tensorflow-tensorboard tfp-nightly tensorflow_estimator tb-nightly tf-nightly tensorflow && \\\r\n    cd /opt && \\\r\n    git clone https://github.com/tensorflow/tensorflow.git && \\\r\n    cd /opt/tensorflow && \\\r\n    /bin/bash ./configure \\\r\n    && \\\r\n    bazel build \\\r\n    --config=mkl --config=opt \\\r\n    --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" \\\r\n    --copt=-msse4.2 --copt=-msse4.1 --copt=-mavx --copt=-msse2 --copt=-msse3  \\\r\n    --copt=-O3 --copt=-mfpmath=both \\\r\n    --copt=\"-DMKL_LP64\" \\\r\n    --copt=\"-fPIC\" \\\r\n    --linkopt=\"-lmkl_gf_lp64\" \\\r\n    --linkopt=\"-lmkl_gnu_thread\" \\\r\n    --linkopt=\"-dl\" \\\r\n    --linkopt=\"-lpthread\" \\\r\n    --linkopt=\"-lmkl_core\" \\\r\n    --linkopt=\"-lm\" \\\r\n    --linkopt=\"-lmkl_rt\" \\\r\n    --linkopt=\"-lmkldnn\" \\\r\n    tensorflow/tools/pip_package:build_pip_package && \\\r\n    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \\\r\n    pip install --no-deps /tmp/pip/tensorflow-*.whl && \\\r\n    cd /opt && rm -rf /opt/tensorflow /tmp/* && \\\r\n    python -c \"import tensorflow as tf; hello = tf.constant('Hello, TensorFlow!'); sess = tf.Session(); print(sess.run(hello))\" && \\\r\n    python -c \"import tensorboard\"\r\n```\r\n\r\n\r\n** Error trace **\r\n```bash \r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=noignite    \t# Disable Apacha Ignite support.\r\n\t--config=nokafka     \t# Disable Apache Kafka support.\r\nConfiguration finished\r\n\u001b[91mWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/opt/tensorflow/tools/bazel.rc\r\n\u001b[0m\u001b[91mStarting local Bazel server and connecting to it...\r\n\u001b[0m\u001b[91mINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\n\u001b[0m\u001b[91mINFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/bin/python --action_env PYTHON_LIB_PATH=/opt/conda/lib/python3.6/site-packages --python_path=/opt/conda/bin/python --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_ROCM=0 --action_env TF_NEED_CUDA=0 --action_env TF_DOWNLOAD_CLANG=0\r\nERROR: Config value mkl is not defined in any .rc file\r\n\u001b[0mThe command '/bin/sh -c echo \"deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list &&     curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - &&     apt-get update && apt-get install -y bazel  && rm -rf /var/lib/apt/lists/* &&     ldconfig &&     pip uninstall -y tensorflow-tensorboard tfp-nightly tensorflow_estimator tb-nightly tf-nightly tensorflow &&     cd /opt &&     git clone https://github.com/tensorflow/tensorflow.git &&     cd /opt/tensorflow &&     /bin/bash ./configure     &&     bazel build     --config=mkl --config=opt     --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"     --copt=-msse4.2 --copt=-msse4.1 --copt=-mavx --copt=-msse2 --copt=-msse3      --copt=-O3 --copt=-mfpmath=both     --copt=\"-DMKL_LP64\"     --copt=\"-fPIC\"     --linkopt=\"-lmkl_gf_lp64\"     --linkopt=\"-lmkl_gnu_thread\"     --linkopt=\"-dl\"     --linkopt=\"-lpthread\"     --linkopt=\"-lmkl_core\"     --linkopt=\"-lm\"     --linkopt=\"-lmkl_rt\"     --linkopt=\"-lmkldnn\"     tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --no-deps /tmp/pip/tensorflow-*.whl &&     cd /opt && rm -rf /opt/tensorflow /tmp/* &&     python -c \"import tensorflow as tf; hello = tf.constant('Hello, TensorFlow!'); sess = tf.Session(); print(sess.run(hello))\" &&     python -c \"import tensorboard\"' returned a non-zero code: 2\r\n```\r\n\r\n ", "comments": ["Oh dear... I can confirm that rolling back to using Bazel 0.18.0 *does* work and doesn't crash the build right away.... It's a Bazel issue", "Ya, The MKL containers are staying with Basel 15 until upstream shifts infrastructure.", "That method of installing Bazel is slick, but we haven't been able to validate it yet. Want to submit a PR, @sadatnfs? \ud83d\ude04 Just tag @TensorFlow-MKL so we can run QA on it.", "Not sure what to fix in my PR since this is more of building a Docker over TF. ", "@sadatnfs Is there an issue open for Bazel where we can check in which Bazel release the underlying issue is fixed?", "@boegel Naw I haven't posted anything on the Bazel end. I just assumed that maybe TF's build files haven't been updated to cater to Bazel >0.18.0 and so I just pinned to Bazel 15 for now ", "Just saw this. Posting just in case it's still relevant. \r\nCommit https://github.com/tensorflow/tensorflow/commit/a74a3217f7ff2dbee2fb618aa658cf666861545c fixed the build issue. If you use TF older than this commit, you can fix the issue yourself by calling `mv tools/bazel.rc .bazelrc` in your tensorflow folder."]}, {"number": 23379, "title": "fixed blankspaces", "body": "fixed some spaces", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Nagging Assignee @harshini-gadige: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@brokenwiings  Please sign CLA in order to look into this PR. Closing now. Please reopen and sign CLA if you would like to proceed. Thanks !"]}, {"number": 23378, "title": "Can we load a pretrained tensorflow model which don't have metafile?", "body": "Hi Team,\r\n\r\nI have a pre-trained model which contains \"model_gs_19860_0.19.data-00000-of-00001\" and \"model_gs_19860_0.19.index\" but don't have any metafile, is it possible to load this model?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 23377, "title": "official support AMD's GPU plan?", "body": "**System information**\r\n- OS Platform and Distribution : Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):tensorflow-gpu-1.8.0\r\n- Python version:python3.6\r\n- CUDA/cuDNN version:cuda9.2 cudnn7.2\r\n- GPU model and memory: AMD Radeon Radeon RX Vega 64\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nDoes the official support AMD's [ROCm](https://rocm.github.io/index.html) plan?\r\n\r\n\r\n**Who will benefit with this feature?**\r\nThe expectations of the majority of AMD users\uff0cAfter all, cost-effective.", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\n@marzlia  - You may refer [this.](https://medium.com/tensorflow/amd-rocm-gpu-support-for-tensorflow-33c78cc6a6cf) But I request you to post your question in Stack Overflow since we strictly follow the Github process(i.e to take bugs/feature requests). Thank you !\r\n"]}, {"number": 23376, "title": "ParameterServerStrategy throws an exception when training locally with multiple GPUS", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos-release-7-4.1708.el7.centos.x86_64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): compiled form source with flags \r\n```\r\nexport TF_NEED_HDFS=1\r\nexport TF_NEED_KAFKA=1\r\nexport TF_ENABLE_XLA=1\r\nexport TF_NEED_CUDA=1\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=5.2,7.0\r\nexport TF_CUDA_VERSION=9.2\r\nexport TF_CUDNN_VERSION=7\r\nexport TF_NCCL_VERSION=1.3\r\n\r\nbazel build \\\r\n            --config=opt \\\r\n            --config=cuda \\\r\n            --copt=-msse4.2 \\\r\n            --copt=-mavx \\\r\n            --copt=-mavx2 \\\r\n            --copt=-mfma \\\r\n            --copt=-O3 \\\r\n            //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n- TensorFlow version (use command below): b'v1.11.0-0-gc19e293' 1.11.0\r\n- Python version: Python 3.6.6\r\n- Bazel version (if compiling from source): 0.18.0\r\n- GCC/Compiler version (if compiling from source): 4.8.5 20150623\r\n- CUDA/cuDNN version: 9.2, 7\r\n- GPU model and memory: Tesla V100-PCIE 16GB and Tesla V100-PCIE 16GB\r\n\r\n**Describe the current behavior**\r\nI'm trying to run an official resnet model on cifar 10 dataset from https://github.com/tensorflow/models/tree/master/official/resnet \r\nwhere I replaced MirroredStrategy by ParameterServerStrategy in https://github.com/tensorflow/models/blob/master/official/utils/misc/distribution_utils.py#L48\r\nas following\r\n```\r\n# return tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus)\r\n     return tf.contrib.distribute.ParameterServerStrategy(num_gpus)\r\n```\r\n\r\nIt throws an exception\r\n```\r\nI1030 10:21:32.401904 139688013772544 tf_logging.py:115] Done calling model_fn.\r\nTraceback (most recent call last):\r\n  File \"./models/official/resnet/cifar10_main.py\", line 285, in <module>\r\n    absl_app.run(main)\r\n  File \"/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"./models/official/resnet/cifar10_main.py\", line 278, in main\r\n    run_cifar(flags.FLAGS)\r\n  File \"./models/official/resnet/cifar10_main.py\", line 273, in run_cifar\r\n    shape=[_HEIGHT, _WIDTH, _NUM_CHANNELS])\r\n  File \"/home/a.eryshev/dev/tf-experiments/models/official/resnet/resnet_run_loop.py\", line 565, in resnet_main\r\n    hooks=train_hooks, max_steps=flags_obj.max_train_steps)\r\n  File \"/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 356, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1179, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1295, in _train_model_distributed\r\n    destinations='/device:CPU:0'))[0]\r\n  File \"/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/distribute.py\", line 748, in reduce\r\n    return self._reduce(aggregation, value, destinations)\r\n  File \"/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/parameter_server_strategy.py\", line 305, in _reduce\r\n    self._verify_destinations_not_different_worker(destinations)\r\n  File \"/home/a.eryshev/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/parameter_server_strategy.py\", line 302, in _verify_destinations_not_different_worker\r\n    (d, self._worker_device))\r\nAttributeError: 'ParameterServerStrategy' object has no attribute '_worker_device'\r\n```\r\n\r\n**Describe the expected behavior**\r\nReplacing MirroredStrategy by ParameterServerStrategy launches a training with CPU acting as PS and 2 GPU workers.\r\n\r\n**Code to reproduce the issue**\r\nRun command:\r\n```\r\npython ./models/official/resnet/cifar10_main.py --data_dir=/data --model_dir=/data --benchmark_logger_type=BaseBenchmarkLogger --resnet_version=2 --clean --log_step_count_steps=10 --save_summary_steps=10 --epochs_between_evals=1820 --train_epochs=1820 --batch_size=512 --num_gpus=2\r\n```", "comments": ["Looks like there's an open PR that fix the problem https://github.com/tensorflow/tensorflow/pull/22713", "Does https://github.com/tensorflow/tensorflow/commit/66dd1e21e7ab6e2aed8413880a7f2dd7f0a20e50 fix your issue?", "Yes, thank you."]}, {"number": 23375, "title": "Protobuf error, mismatch with system installed version", "body": "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.11.0\r\n- Python version: 3.6.6 (virtualenv)\r\n- Bazel version (if compiling from source): 0.17.2\r\n- GCC/Compiler version (if compiling from source): gcc-6 (Ubuntu 6.4.0-17ubuntu1) 6.4.0 20180424\r\n- CUDA/cuDNN/NCCL version: 9.1/7.1/2.1.15\r\n- GPU model and memory: Quadro M1200\r\n\r\nI know this a typical problem and i could compile and install from source using a previous version but this time it's a bit different.\r\n\r\nCompile log is : \r\n\r\n```\r\nERROR: /home/god_kane/Downloads/tensorflow-1.11.0/tensorflow/contrib/nccl/BUILD:24:1: error while parsing .d file: /home/god_kane/.cache/bazel/_bazel_god_kane/380ea18d2d4a7f2f6db5af2f6d4bc750/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/nccl/_objs/python/ops/_nccl_ops_gpu/nccl_manager.pic.d (No such file or directory)\r\nIn file included from ./tensorflow/core/framework/variant.h:26:0,\r\n                 from ./tensorflow/core/framework/allocator.h:26,\r\n                 from ./tensorflow/core/framework/tensor.h:20,\r\n                 from ./tensorflow/core/framework/log_memory.h:19,\r\n                 from ./tensorflow/core/common_runtime/gpu/gpu_event_mgr.h:21,\r\n                 from ./tensorflow/contrib/nccl/kernels/nccl_manager.h:31,\r\n                 from tensorflow/contrib/nccl/kernels/nccl_manager.cc:15:\r\nbazel-out/k8-opt/genfiles/tensorflow/core/framework/tensor.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is\r\n #error This file was generated by a newer version of protoc which is\r\n  ^~~~~\r\nbazel-out/k8-opt/genfiles/tensorflow/core/framework/tensor.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update\r\n #error incompatible with your Protocol Buffer headers.  Please update\r\n  ^~~~~\r\nbazel-out/k8-opt/genfiles/tensorflow/core/framework/tensor.pb.h:14:2: error: #error your headers.\r\n #error your headers.\r\n  ^~~~~\r\n```\r\n\r\nThose file are correctly generated with protobuf 3.6.0 which i believe is compiled as third_party.  \r\nWhat i don't understand is that it seems what is included later are my system protobuf headers and not the one from tensorflow folder resulting in an error.  \r\nI tried to do something dirty to check, installed protobuf from source and created symbolink link to it replacing my system installed version.  \r\nDoing that worked but it's not a valid solution. I also should not need to install my own protobuf version just for using it in python.\r\nThe fault seems to be with nccl but i don't know where to look for a more clean fix.", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n", "@depauwjimmy  - Please try with Bazel 0.15.0 and GCC 4.8 with CUDA/cuDNN  9/7. This is the tested combination and it should work. \r\n\r\nAlso request you to fill [this template](https://github.com/tensorflow/tensorflow/issues/new/choose) in order to continue assisting on this issue. If this issue no longer persists, feel free to close. Thanks !", "@depauwjimmy  - Any update on this ?", "Just renaming the /usr/include/google/protobuf folder to something else let me compile without a problem.\r\nI did not had the time to try again with the recommended setup since it's a time consuming process.\r\nI'll keep it in mind if a need to compile again in the future for another platform/version.\r\nThanks for your time"]}, {"number": 23374, "title": "build libtensorflow_cc.so with debug symbols using bazel", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.8.0\r\n- Python version: Python 2.7.12\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10)\r\n- CUDA/cuDNN version: 9.2/7.2.1\r\n- GPU model and memory: Tesla K40c 12 GB VRAM\r\n\r\n**Describe the problem**\r\nI need to debug `libtensorflow_cc.so` to trace some other issue. I learned from `bazel` that I need to pass the following flag `-c dbg` -probably this one too `--strip=never`- while building the library. I noticed a significant increase in the binary size which makes sense! yet I'm still unable to step into the library code. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`bazel build --config=opt --config=cuda --config=monolithic tensorflow:libtensorflow_cc.so`\r\n\r\n**Any other info / logs**\r\n- I passed `--config=monolithic` to resolve OpenCV's `imread` issue always returning a 0x0 mat.\r\n- Furthermore, the binary built with  `-c dbg` and `--strip=never`, failed to load my model that was loaded correctly before passing those flags. but we can discuss it in a different issue if we resolve this one first.\r\n", "comments": ["Anything on this yet?", "@eslamahmedkhair what's the param --config=monolithic meaning ? i dont understand this param, mybe its related to my failed build from source.", "@jesen8 Was digging into this as well. It essentially sets `framework_shared_object` to false, which is explained here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/core/BUILD#L56-L64", "@eslamahmedkhair please confirm if you are still facing any issues.", "@eslamahmedkhair,\r\n\r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to `2.6` which is latest stable version of TF and let us know if the issue still persists in newer versions.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23374\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23374\">No</a>\n"]}, {"number": 23373, "title": "car Detection ", "body": "Hi \r\nI only want to detect car objects. Can anybody tell me which tflite class used? So that I am able to detect only car object in Android.\r\nThanks!", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 23372, "title": "TensorFlow installation for Python 3.7", "body": "I have Python 3.7 running on Anaconda on my new Windows PC. MSVCP140.dll is present\r\n\r\nI still have no clue why is it not working. If Python 3.7 is official, can you please fix Tensorflow module to support the same?\r\n\r\nScreenshots : \r\n\r\n![image](https://user-images.githubusercontent.com/36528857/47712661-846ce680-dbf5-11e8-987e-fa1554230a86.png)\r\n", "comments": [" I tried creating a virtual environment for Python 3.6 which is the only fix."]}, {"number": 23371, "title": "Tensorflow GPU setup issue on Ryzen 5 + 2080 RTX GPU + Ubuntu 18.04", "body": "Hi Team,\r\n\r\nI am trying to setup tensorflow-gpu on my new machine with configuration AMD Ryzen 5 2600 + 2080 RTX GPU + Ubuntu 18.04. I have tried tensorflow-gpu docker, tensorflow build from source.\r\n\r\nIn docker image, it was exiting after first epoch without any error. \r\n\r\nAnd in other approach we are getting following errors:\r\n\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory.\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation MatMul: Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0 ]. Make sure the device specification refers to a valid device.\r\n\t [[node MatMul (defined at test.py:6)  = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/device:GPU:0\"](a, b)]]\r\n\r\n\r\nPlease assist with right steps to setup tensorflow gpu on Ryzen 5 + 2080 RTX GPU + Ubuntu 18.04 machine. Quick response will be really appreciated.\r\n\r\nRegards,\r\nAnkit Aggarwal\r\n", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 23370, "title": "WIP: Fix max_pool_with_argmax behavior on GPU", "body": "The PR is still not done yet.", "comments": ["Nagging Assignee @facaiy: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 23369, "title": "KeyError: 'BlockLSTM' when using tf.train.import_meta_graph()", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\npip install tensorflow-gpu\r\n- TensorFlow version (use command below):\r\n1.11\r\n- Python version:\r\npython 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\nCUDA 9.0, cuDNN 7.31\r\n- GPU model and memory:\r\nTITAN X (Pascal), 12G\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\nv1.11.0-0-gc19e29306c 1.11.0\r\n\r\n**Describe the current behavior**\r\n\r\nI'm using two kinds of LSTM operations like below.\r\n```python\r\n    def __bi_lstm(self, inputs, lengths, rnn_size, keep_prob=0.5, scope='bi-lstm'):\r\n        \"\"\"Apply bi-directional LSTM\r\n        \"\"\"\r\n        with tf.variable_scope(scope):\r\n            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\r\n            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\r\n            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw,\r\n                                                                        cell_bw,\r\n                                                                        inputs,\r\n                                                                        sequence_length=lengths,\r\n                                                                        dtype=tf.float32)\r\n            outputs = tf.concat([output_fw, output_bw], axis=-1)\r\n            return tf.nn.dropout(outputs, keep_prob)\r\n\r\n    def __bi_lstm_fused(self, inputs, lengths, rnn_size, keep_prob=0.5, scope='bi-lstm-fused'):\r\n        \"\"\"Apply bi-directional LSTM block fused\r\n        \"\"\"\r\n        with tf.variable_scope(scope):\r\n            t = tf.transpose(inputs, perm=[1, 0, 2])  # Need time-major\r\n            lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(rnn_size)\r\n            lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(rnn_size)\r\n            lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\r\n            output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=lengths)\r\n            output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=lengths)\r\n            outputs = tf.concat([output_fw, output_bw], axis=-1)\r\n            outputs = tf.transpose(outputs, perm=[1, 0, 2])\r\n            return tf.nn.dropout(outputs, keep_prob)\r\n```\r\ni trained a model and save it via saver.save().\r\nand then, tried to import_meta_data\r\n\r\n```python\r\nloader = tf.train.import_meta_graph(meta_file, clear_devices=True)\r\n....\r\n```\r\n\r\nin case tf.contrib.rnn.LSTMCell(), everything goes fine. \r\nbut, when it comes to use tf.contrib.rnn.LSTMBlockFusedCell(), \r\nthere is an key error.\r\n\r\n```bash\r\n Traceback (most recent call last):\r\n  File \"export.py\", line 55, in <module>\r\n    export(args)\r\n  File \"export.py\", line 13, in export\r\n    loader = tf.train.import_meta_graph(meta_file, clear_devices=True)\r\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1666, in import_meta_graph\r\n    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\r\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1688, in _import_meta_graph_with_return_elements\r\n    **kwargs))\r\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\r\n    return_elements=return_elements)\r\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 391, in import_graph_def\r\n    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 158, in _RemoveDefaultAttrs\r\n    op_def = op_dict[node.op]\r\nKeyError: 'BlockLSTM'\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["The contrib ops are lazily imported, so you need to add a line just saying \"tf.contrib.rnn\" before importing the metagraph.", "> The contrib ops are lazily imported, so you need to add a line just saying \"tf.contrib.rnn\" before importing the metagraph.\r\n\r\nWhat he said is  right. So you have to do  the following\r\n```\r\nimport tensorflow as tf\r\ntf.contrib.rnn\r\nsess = tf.Session()\r\nimported_meta = tf.train.import_meta_graph(\"model_dir/model.ckpt-0.meta\")  \r\n```", "I am having exact the same issue in a C++ program that loads a frozen graph and attempts to do inference. How I could import those contrib ops in a C++ program?", "On a C++ program you need to dlopen the .so file containing the contrib ops. If you search for .so / dll it in the contrib/ directory of the python tf installation (inside site_packages or wherever else your tf was installed) you can find it.", "i had the same issue and solved it.\r\n\r\nhttps://github.com/dsindex/etagger/blob/master/inference/cc/src/TFUtil.cc\r\n\r\n* for LSTMBlockFusedCell()\r\n$ rnn_path=`python -c \"import tensorflow; print(tensorflow.contrib.rnn.__path__[0])\"`\r\n$ rnn_ops_lib=${rnn_path}/python/ops/_lstm_ops.so\r\n$ cp -rf ${rnn_ops_lib} ${TENSORFLOW_BUILD_DIR}\r\n$ export LD_LIBRARY_PATH=${TENSORFLOW_BUILD_DIR}:$LD_LIBRARY_PATH\r\n\r\n\r\n", "> On a C++ program you need to dlopen the .so file containing the contrib ops. If you search for .so / dll it in the contrib/ directory of the python tf installation (inside site_packages or wherever else your tf was installed) you can find it.\r\n\r\nCan you (or anyone who has done this) provide more details? I tried that and the error remains.\r\n\r\nError is: `Op type not registered 'Resampler' in binary running on <computer-name>`\r\n\r\nSo I added this code:\r\n\r\n```\r\nvoid LoadResamplerOps() {\r\n    std::cout << \"Attempting dlopen(\" << kResamplerOpsLibraryPath << \")\" << std::endl;\r\n    void *handle =  dlopen(kResamplerOpsLibraryPath.c_str(), RTLD_NOW);\r\n    if (!handle) {\r\n        std::cerr << \"Error opening \" << kResamplerOpsLibraryPath << \": \" << dlerror() << std::endl;\r\n        return;\r\n    }\r\n    // Clear any existing error if file loaded successfully\r\n    dlerror();\r\n    std::cout << kResamplerOpsLibraryPath << \" loaded successfully\" << std::endl;\r\n}\r\n```\r\nI successfully load the `.so` file but still get the same error.  I'm guessing after `dlopen` I have to do some additional things, but I am not sure what they would be.", "It sometimes happens when the op is pinned to a device (like GPU), but the\nop doesn't on that device.\n\nDid you run the graph successfully before exporting? Are you running the\nimport on the same machine?\n\nOn Tue, Jun 25, 2019 at 1:50 PM Apollys <notifications@github.com> wrote:\n\n> On a C++ program you need to dlopen the .so file containing the contrib\n> ops. If you search for .so / dll it in the contrib/ directory of the python\n> tf installation (inside site_packages or wherever else your tf was\n> installed) you can find it.\n>\n> Can you (or anyone who has done this) provide more details? I tried that\n> and the error remains.\n>\n> Error is: Op type not registered 'Resampler' in binary running on\n> <computer-name>\n>\n> So I added this code:\n>\n> void LoadResamplerOps() {\n>     std::cout << \"Attempting dlopen(\" << kResamplerOpsLibraryPath << \")\" << std::endl;\n>     void *handle =  dlopen(kResamplerOpsLibraryPath.c_str(), RTLD_NOW);\n>     if (!handle) {\n>         std::cerr << \"Error opening \" << kResamplerOpsLibraryPath << \": \" << dlerror() << std::endl;\n>         return;\n>     }\n>     // Clear any existing error if file loaded successfully\n>     dlerror();\n>     std::cout << kResamplerOpsLibraryPath << \" loaded successfully\" << std::endl;\n> }\n>\n> I successfully load the .so file but still get the same error. I'm\n> guessing after dlopen I have to do some additional things, but I am not\n> sure what they would be.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23369?email_source=notifications&email_token=AE75E3PIBYQDD636VIBEC7LP4KAJBA5CNFSM4GAFPLX2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYRRCSI#issuecomment-505614665>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AE75E3KHFNI46IFLUK7JVETP4KAJBANCNFSM4GAFPLXQ>\n> .\n>\n"]}, {"number": 23368, "title": "tensorflow\\contrib\\coder\\python\\ops\\_coder_ops.so not found", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nsample from matterport/maskrcnn\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10 x64\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):\r\nconda list\r\ntensorflow-gpu            1.10.0                   py36_0    aaronzs\r\ntensorflow-gpu            1.11.0                    <pip>\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nb'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\r\n- Python version:3.6.7\r\n- CUDA/cuDNN version:9.0/7.1.4\r\n- GPU model and memory:GeForce GTX 1060 3GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI tired to train a sample in the project from [https://github.com/matterport/Mask_RCNN](url)\r\nthen there is an error showed up.\r\nI tried the basic functions of tensorflow like session they works fine.\r\nbut when I used train something about DLL showed up\r\nit said that it can't find something like program input  \"? MakeShape@TensorShapeUtils@tensorflow@@SA?AVStatus@2@V?$Span@$$CBH@absl@@PEAVTensorShape@2@@Z(in DLL C:\\Users\\willy_sung\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\contrib\\coder\\python\\ops_coder_ops.so)\"\r\n**Describe the expected behavior**\r\nthere should be no error \r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nthere is no error until this line\r\n`train(model)`\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nLoading weights  C:\\HSIR\\Mask_RCNN\\mask_rcnn_coco.h5\r\n2018-10-30 15:35:55.918873: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-10-30 15:35:56.255762: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1405] Found device 0 with properties:\r\nname: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.759\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.00GiB freeMemory: 2.42GiB\r\n2018-10-30 15:35:56.255976: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-10-30 15:35:56.948830: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-10-30 15:35:56.948953: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971]      0\r\n2018-10-30 15:35:56.953167: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:984] 0:   N\r\n2018-10-30 15:35:56.954126: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2125 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTraining network heads\r\nTraceback (most recent call last):\r\n  File \"balloon.py\", line 364, in <module>\r\n    train(model)\r\n  File \"balloon.py\", line 199, in train\r\n    layers='heads')\r\n  File \"C:\\HSIR\\Mask_RCNN\\mrcnn\\model.py\", line 2341, in train\r\n    histogram_freq=0, write_graph=True, write_images=False),\r\n  File \"C:\\Users\\willy_sung\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\keras\\callbacks.py\", line 745, in __init__\r\n    from tensorflow.contrib.tensorboard.plugins import projector\r\n  File \"C:\\Users\\willy_sung\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\", line 31, in <module>\r\n    from tensorflow.contrib import coder\r\n  File \"C:\\Users\\willy_sung\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\contrib\\coder\\__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib.coder.python.layers.entropybottleneck import *\r\n  File \"C:\\Users\\willy_sung\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\contrib\\coder\\python\\layers\\entropybottleneck.py\", line 24, in <module>\r\n    from tensorflow.contrib.coder.python.ops import coder_ops\r\n  File \"C:\\Users\\willy_sung\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\contrib\\coder\\python\\ops\\coder_ops.py\", line 30, in <module>\r\n    resource_loader.get_path_to_datafile(\"_coder_ops.so\"))\r\n  File \"C:\\Users\\willy_sung\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\contrib\\util\\loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"C:\\Users\\willy_sung\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py\", line 56, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: C:\\Users\\willy_sung\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\contrib\\coder\\python\\ops\\_coder_ops.so not found", "comments": ["Not able to access your project from the link you provided https://github.com/matterport/Mask_RCNN).\r\nCan you please update it? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I don't understand why you can't access but I fix the problem with a bigger GPU", "Thanks for the update. Can you please share your updated configuration which helped you solve your issue?", "@ymodak  I used GPU with  3 GB, then my professor bought GTX 1060 with 6GB memory which solves the problem.  ", "I solve this problem at window10 platform:\r\nchange D:\\anaconda\\Lib to D:\\anaconda\\lib; then pip install --force-reinstall tensorflow-gpu==1.9.0", "I meet this problem when i was using Tensorflow-gpu 1.10.0\uff1bBut this problem doesn't appear when I update the tensorflow to 1.11.0."]}, {"number": 23367, "title": "How to visualizing a model structure in tensorboard when eager execution is open? ", "body": "tensorflow-gpu == 1.10.0\r\npython == 3.5\r\n\r\nHello, i want to visualize a model structure in tensorboard when eager execution is open, but i can not find one available solution to do this. In graph mode, we can use tf.summary.writer('', sess) to save model structure, but how to do as same as this under eager mode? ", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nPlease refer the below links. I request you to post questions in the Stack Overflow as we strictly encourage users to submit a bug/feature request here. Thank you !\r\n\r\n[Embedding visualization with eager execution](https://github.com/tensorflow/tensorboard/issues/1421).\r\n[Eager and Tensorboard graphs.](https://stackoverflow.com/questions/50257614/tensorflow-eager-and-tensorboard-graphs)\r\n[Tensorboard: Graph visualization](https://www.tensorflow.org/guide/graph_viz)", "Thanks very much.", "@murdockhou what was your conclusion? Is it in fact impossible to visualize graph in eager mode - as stated in the SO answer?\r\n\r\nI found [tf.contrib.summary.graph](https://www.tensorflow.org/versions/r1.13/api_docs/python/tf/contrib/summary/graph) function. Unfortunately its  documentation is unclear. [Here](https://stackoverflow.com/questions/54560627/param-for-tf-contrib-summary-graph) is SO request for elaboration.\r\n\r\n@nfelt - can you advise here as the last editor?", "Answered on StackOverflow here: https://stackoverflow.com/a/54913635/1179226  The documentation does describe this parameter, but in eager mode there is no single overall graph by definition, so there isn't really anything TensorBoard can currently visualize that matches what you are asking for."]}, {"number": 23366, "title": "Tensorflow Lite Andriod bug", "body": "hey guys, i'm very new for Tensorflow and Tensorflow lite.\r\nI followed the the guide from TensorFlow For Poets, which i see from official site of Tensorflow Lite https://www.tensorflow.org/lite/\r\n\r\nI am finished very quickly the first guide TensorFlow For Poets, and then i tried to set up Tensorflow on my andriod device(I have a Arm-RK3399, which i have been installed andorid). The first 3 Steps were no problem, but after I opend andriod studio, it became very slow, as in the guide, i choosed Tensorflow-for-poets-2/android/tflite from the working directory and then i didn't get a \"gradle Sync\" popup, but in android studio i saw the Gradle was Syncing, and also configure building Cpp. but I waited almost 1 hour, it was still building. I also tried changing to offline work, but it didn't help. \r\n\r\nIs that because I used Linux in Virtual Machine caused this ?  or I should used android Phone? \r\n\r\nThx\r\nJason\r\n\r\n\r\n", "comments": ["@JasonQRY If you are using linux in Virtual Machine, try giving it more RAM (approx. 4 gigs or more). Android Studio is itself a very heavy software and takes a lot of time even on real systems. If possible, try building Gradle on a real system instead of virtual machine.\r\nLet me know how it proceeds :)", "> @JasonQRY If you are using linux in Virtual Machine, try giving it more RAM (approx. 4 gigs or more). Android Studio is itself a very heavy software and takes a lot of time even on real systems. If possible, try building Gradle on a real system instead of virtual machine.\r\n> Let me know how it proceeds :)\r\n\r\nI tried actually on real system and it worked, i just donwloaded the App from Tensorflow, and import to Android Studio."]}]