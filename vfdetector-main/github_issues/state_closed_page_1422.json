[{"number": 10316, "title": "[OpenCL] Cleans control_flow_ops.cc file", "body": "Mainly re-order code to follow general rule:\r\n- CPU Operation registration\r\n- GPU Operation registration\r\n- SYCL Operation registration", "comments": ["Can one of the admins verify this patch?", "is there only reordering in this PR?\r\nI think we can skip the reordering for now.\r\nWe can close the PR if there are no actual code changes.", "Reverted ordering in 4691140\r\nThere are other changes in the file ( eg registration for Merge and RefMerge )", "Jenkins, test this please."]}, {"number": 10315, "title": "tf.image.central_crop returns zero dimension shaped tensors ", "body": "* I have written custom code.\r\n* OS platform\r\nc++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\r\nCopyright (C) 2013 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n* Tensorflow version\r\n1.1.0-rc2\r\n* GPU model\r\nWed May 31 11:49:29 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro K2200        On   | 0000:04:00.0      On |                  N/A |\r\n| 42%   37C    P8     1W /  39W |    422MiB /  4032MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      6147    G   /usr/lib/xorg/Xorg                             213MiB |\r\n|    0     16477    G   cinnamon                                        81MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\nProblem definition:\r\nGiven an image of shape (240, 320, 3), and a central_fraction of 0.33\r\ntf.image.central_crop returns an image of shape (0, 0, 3), which is wrong.\r\nMoreover, such tensors of zero dimension results in the cryptic error\r\n\r\ncould not set cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\r\n\r\nwhen these zero sized tensors are passed to downstream ops.\r\n\r\nSource code to reproduce the problem:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimg_shape = (240, 320, 3)\r\n\r\nimg_ph = tf.placeholder(shape=img_shape, dtype=tf.uint8)\r\ncentral_fraction = 0.33\r\ncrop_op = tf.image.central_crop(img_ph, central_fraction)\r\n\r\nwith tf.Session() as sess:\r\n  img = np.zeros((240, 320, 3), dtype=np.uint8)\r\n  feed_dict = {img_ph: img}\r\n  cropped = sess.run(crop_op, feed_dict=feed_dict)\r\n  print(\"cropped shape: {}\".format(cropped.shape))\r\n```\r\n\r\n", "comments": ["Thanks for the report and the simple reproducible instructions @fumin \r\n\r\n@shlens : Mind taking a look? IIUC, this might have been a bug since `central_crop` was introduced in de7b961da6cea7692d9f36eeec1c7f220c15af3f - where if [`fraction_offset`](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/python/ops/image_ops_impl.py#L390) computes to 2, then `bbox_h_size` and `bbox_w_size` will be 0.\r\n", "I believe the issue is that the function tries to be general in order to handle cases when the input shape is not known at compile time. This might be the case if the image were the result of decode_jpeg, for instance. In this case, the spatial dimensions of the output shape are unknown.\r\n\r\nIt would be possible to create a special case handling such that if the shape is fully specified in the input, then the shape will be set in the output.", "I think the issue is that the calculation of\r\n```\r\nfraction_offset = int(1 / ((1 - central_fraction) / 2.0))\r\n```\r\nwill magnify the deviation if the accuracy of central_fraction is low. Thus causing `(240, 320, 3)` * 0.33 to be `(0, 0, 3)`\r\n\r\nCreated a PR #12239 for this issue."]}, {"number": 10314, "title": "[OpenCL] Cleans StridedSlice Op", "body": "Small PR's that clean OpenCL related code.", "comments": ["Jenkins, test this please.", "Linux XLA fail seems to be unrelated", "Jenkins, test this please.\r\n", "Jenkins, test this please", "Test failure is a known issue, PR safe to merge."]}, {"number": 10313, "title": "[OpenCL] Cleans Shape Op", "body": "First in series of small PR's that clean OpenCL related code.", "comments": ["Can one of the admins verify this patch?", "Can we undo the move, to make review easier?\r\nIf we are only moving code, we can close this PR.", "There are some changes beside re-ordering 609e8c7", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Makefile failure unrelated. merging."]}, {"number": 10312, "title": "Fix comments error in mnist_replica.py where only one ps is used with\u2026", "body": "\u2026 two works by default.", "comments": ["Can one of the admins verify this patch?", "I have changed the format.", "Thanks, @cxxgtxy !"]}, {"number": 10311, "title": "Different arg names for conv1d", "body": "### System information\r\n- **TensorFlow version (use command below)**: r1.1\r\n\r\n### Describe the problem\r\nIn latest verion, tf.nn.conv1d and tf.nn.conv2d has different arg names for filter and stride.\r\ntf.nn.conv1d: \r\n    filters: A 3D Tensor. Must have the same type as input.\r\n    stride: An integer. The number of entries by which the filter is moved right at each step.\r\ntf.nn.conv2d:\r\n    filter: A Tensor. Must have the same type as input. A 4-D tensor of shape [filter_height, filter_width, in_channels, out_channels]\r\n    strides: A list of ints. 1-D tensor of length 4. The stride of the sliding window for each dimension of input. The dimension order is determined by the value of data_format, see below for details.\r\n\r\nShould it be unified to same name?", "comments": ["The naming for [conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) and [conv3d](https://www.tensorflow.org/api_docs/python/tf/nn/conv3d) seems to match: `input, filter, strides, padding`\r\nThis should be used in [conv1d](https://www.tensorflow.org/api_docs/python/tf/nn/conv1d) as well: `value -> input, filters -> filter, stride -> strides`", "Is this only about naming or description as well?", "Ah, this discrepancy is unfortunate. Thanks for pointing it out.\r\n\r\nAlas, since this now part of the 1.x release - we can't get rid of the old names. However, we can perhaps add \"input\", \"filter\" and \"strides\" as arguments to conv1d.\r\n\r\nI'll take a stab at that.\r\n\r\n(Though, perhaps `stride` still makes sense since it is a single integer, not a list as in `conv2d` and `conv3d`?)", "Actually, on further reflection - while this discrepancy is unfortunate - I don't think it is worth fixing.\r\n\r\nWe have to maintain backward compatibility of the API for all 1.x releases. In order to accept either the old or new arguments as named parameters, the signature of `conv1d` will have to change from:\r\n\r\n```\r\n def conv1d(value, filters, stride, padding,                                                                                                  \r\n           use_cudnn_on_gpu=None, data_format=None,\r\n           name=None)\r\n```\r\n\r\nto something like:\r\n\r\n```\r\ndef conv1d(value=None, filters=None, stride=None, padding=None,                                                                                                  \r\n           use_cudnn_on_gpu=None, data_format=None,\r\n           name=None, input=None, filter=None, strides=None)\r\n```\r\n\r\nWith a bunch of logic in the new function to ensure that:\r\n- At least one of `input` or `value` is not `None`\r\n- Same for one of `filter` or `filters` and one of `stride` or `strides`\r\n- `padding` is not `None`\r\n- If `value` is specified, prefer that over `input`\r\netc.\r\n\r\nI fear that doing so will add more confusion than it helps resolve.\r\n\r\nGiven that, I'm proposing that we live with this unfortunate historical artifact.\r\n\r\nLet me know if anyone feels particularly strongly, otherwise I'll go ahead and close this issue in a day or two.", "+1 to not fixing the issue", "This should be given the label \"stalled\" so that this can be fixed in 2.x", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 10310, "title": "Don't unnecessarily reset y domain on line charts", "body": "Fixes #8994", "comments": ["Can one of the admins verify this patch?", "/CC @dandelionmane @chihuahua would be great to get this merged for the 1.2 release!", "Any update here?", "Chi, can you review this?", "@taion, I am very sorry for having neglected this PR. tensorboard has at this point into a separate repository. Could you please recreate the PR for the tensorflow/tensorboard repo and reference issue tensorflow/tensorboard#36? @dandelionmane can review the PR. We promise to be more responsive."]}, {"number": 10309, "title": "Fix TensorBoard demo data", "body": "https://github.com/tensorflow/tensorflow/commit/d97706437932443ae22082ada49d3ac054b77304 suggests that the demo TensorBoard was supposed to go away entirely, but `tensorboard/DEVELOPMENT.md` still says to use it.\r\n\r\nBetter that it shows scalars and histograms than not?", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "/CC @dandelionmane", "No longer relevant per https://github.com/tensorflow/tensorboard/pull/58"]}, {"number": 10308, "title": "Batch norm docs -> r1.2", "body": "Fixed up the documentation for tf.contrib.layers.batch_norm and tf.layers.batch_normalization explaining how to include the update ops in the train_op. Also simplified the existing tf.layers.batch_normalization documentation to the least error-prone option (adding to the train_op).\r\n\r\nPiperOrigin-RevId: 156609483\r\n\r\nSee https://github.com/tensorflow/tensorflow/commit/675f0f9073e9e6ac6df8018e972f2b38f2c4c23c for the same change in master.", "comments": []}, {"number": 10307, "title": "Update docker to cudnn6.", "body": "", "comments": ["I think python 3.4 is causing issues in your builds.\r\nCould you check what is going on?", "@damienmg @davidzchen  we are getting the following, but we don't see any cuda configuration being hard-coded to cudnn 5. Any idea why we are seeing the error? Thanks.\r\nERROR: /var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/external/local_config_cuda/cuda/BUILD:1302:1: declared output 'external/local_config_cuda/cuda/lib/libcudnn.so.5' is a dangling symbolic link.", "@tensorflow-jenkins test this please", "The variable that is used in cuda_configure.bzl starts with an underscore\nso that would be _TF_CUDNN_VERSION.\n\nOn Sat, Jun 3, 2017, 7:29 AM gunan <notifications@github.com> wrote:\n\n> *@gunan* requested changes on this pull request.\n> ------------------------------\n>\n> In configure\n> <https://github.com/tensorflow/tensorflow/pull/10307#discussion_r119979902>\n> :\n>\n> > @@ -522,6 +522,12 @@ while true; do\n>    CUDA_TOOLKIT_PATH=\"\"\n>  done\n>\n> +# Set default CUDA version if not set\n> +if [ -z \"$TF_CUDA_VERSION\" ]; then\n> +  TF_CUDA_VERSION=\"8.0\"\n> +  export TF_CUDA_VERSION\n>\n> I think I know what is going on.\n> Simply exporting it wont help. You need the value written to bazelrc, with\n> this command:\n> write_action_env_to_bazelrc \"TF_CUDA_VERSION\" \"$TF_CUDA_VERSION\"\n> However, you need to make sure that we do not have two conflicting values\n> are not written, so it may complicate things a little.\n> ------------------------------\n>\n> In configure\n> <https://github.com/tensorflow/tensorflow/pull/10307#discussion_r119979905>\n> :\n>\n> > @@ -626,6 +632,12 @@ while true; do\n>    CUDNN_INSTALL_PATH=\"\"\n>  done\n>\n> +# Set default CUDNN version if not set\n> +if [ -z \"$TF_CUDNN_VERSION\" ]; then\n>\n> same as above, this has to be written to bazelrc.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/10307#pullrequestreview-41913693>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADjHf2LUHFmgomPx05q3q5rC6BeB8ilQks5sAO9WgaJpZM4Nq--O>\n> .\n>\n", "we seems to also set _TF_CUDNN_VERSION to \"TF_CUDNN_VERSION\" :)"]}, {"number": 10306, "title": "Build fails on ppc64le", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.04 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 359d6f9716c0bb9bd8201ce600da98b0481a8049\r\n- **Bazel version (if compiling from source)**:  0.4.5-2017-05-25 (@255953740)\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: `bazel build --verbose_failures --show_package_location  //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Describe the problem\r\nOn a ppc64le machine running Ubuntu 17.04 I am not able to build tensorflow.\r\n\r\n### Source code / logs\r\n```\r\nERROR: /home/brosa/.cache/bazel/_bazel_brosa/141a2b9f209d04ad1bc4d9433836a54c/external/io_bazel_rules_closure/java/io/bazel/rules/closure/webfiles/server/BUILD:54:1: Generating SOY v2 Java files @io_bazel_rules_closure//java/io/bazel/rules/closure/webfiles/server:listing_files failed: bash failed: error executing command\r\n  (cd /home/brosa/.cache/bazel/_bazel_brosa/141a2b9f209d04ad1bc4d9433836a54c/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/brosa/bazel/output:/home/brosa/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/com_google_template_soy/SoyParseInfoGenerator --outputDirectory=bazel-out/host/genfiles/external/io_bazel_rules_closure/java/io/bazel/rules/closure/webfiles/server --javaPackage=io.bazel.rules.closure.webfiles.server --javaClassNameSource=filename --allowExternalCalls=1 $(cat bazel-out/host/genfiles/external/io_bazel_rules_closure/java/io/bazel/rules/closure/webfiles/server/listing_files__srcs) $(cat bazel-out/host/genfiles/external/io_bazel_rules_closure/java/io/bazel/rules/closure/webfiles/server/listing_files__deps)'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nUnrecognized option: -client\r\nError: Could not create the Java Virtual Machine.\r\nError: A fatal exception has occurred. Program will exit.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n\r\n```", "comments": ["That error seems to be coming from your JDK installation\r\n\r\nCould you describe your bazel installation process and the JDK you're using (required by bazel)? For example, `java -version`?\r\n", "I can compile on 97c6203bb3f3978ac67920c66b6234ef82051c57 .\r\n\r\njava version:\r\nopenjdk version \"1.8.0_131\"\r\n```\r\nOpenJDK Runtime Environment (build 1.8.0_131-8u131-b11-0ubuntu1.17.04.1-b11)\r\nOpenJDK 64-Bit Server VM (build 25.131-b11, mixed mode)\r\n```\r\nThis version does not recognize `-client`:\r\n```\r\njava -client\r\nUnrecognized option: -client\r\nError: Could not create the Java Virtual Machine.\r\nError: A fatal exception has occurred. Program will exit.\r\n```\r\nBazel was built from source following the instructions here: https://github.com/PPC64/tensorflow-ppc64-doc\r\n\r\n", "I tried running \r\n`bazel --host_jvm_args=-XX:+IgnoreUnrecognizedVMOptions  build --config=opt --verbose_failures --show_package_location  //tensorflow/tools/pip_package:build_pip_package`\r\nas a way to ignore the issue with `-client`, but I still get the same error.", "If I config my jdk installation to ignore the error ( `sudo sed -i \"\\$a-client IGNORE\" /usr/lib/jvm/java-1.8.0-openjdk-ppc64el/jre/lib/ppc64le/jvm.cfg` ), I got a different error:\r\n\r\n```\r\nERROR: /home/brosa/tensorflow-upstream/tensorflow/tensorboard/components/tf_color_scale/BUILD:36:1: Executing genrule //tensorflow/tensorboard/components/tf_color_scale:ts failed: bash failed: error executing command\r\n  (cd /home/brosa/.cache/bazel/_bazel_brosa/141a2b9f209d04ad1bc4d9433836a54c/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/brosa/bazel/output:/home/brosa/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/genfiles/external/com_microsoft_typescript/tsc.sh --inlineSourceMap --inlineSources --noResolve --declaration --module es6 --outDir bazel-out/local-opt/genfiles/tensorflow/tensorboard/components/tf_color_scale external/com_microsoft_typescript/lib.es6.d.ts external/org_definitelytyped/polymer.d.ts external/org_definitelytyped/webcomponents.js.d.ts bazel-out/local-opt/genfiles/tensorflow/tensorboard/components/tf_imports/d3.d.ts bazel-out/local-opt/genfiles/tensorflow/tensorboard/components/tf_color_scale/bundle.ts'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 126.\r\nbazel-out/host/genfiles/external/com_microsoft_typescript/tsc.sh: line 6: /home/brosa/.cache/bazel/_bazel_brosa/141a2b9f209d04ad1bc4d9433836a54c/execroot/org_tensorflow/external/org_nodejs/bin/node: cannot execute binary file: Exec format error\r\nbazel-out/host/genfiles/external/com_microsoft_typescript/tsc.sh: line 6: /home/brosa/.cache/bazel/_bazel_brosa/141a2b9f209d04ad1bc4d9433836a54c/execroot/org_tensorflow/external/org_nodejs/bin/node: Success\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 42.819s, Critical Path: 26.22s\r\n```\r\n\r\nLooking closer:\r\n\r\n> file home/brosa/.cache/bazel/_bazel_brosa/141a2b9f209d04ad1bc4d9433836a54c/execroot/org_tensorflow/external/org_nodejs/bin/node\r\n> \r\n> /home/brosa/.cache/bazel/_bazel_brosa/141a2b9f209d04ad1bc4d9433836a54c/execroot/org_tensorflow/external/org_nodejs/bin/node: **ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2**, for GNU/Linux 2.6.9, BuildID[sha1]=fb12043414130d6f6928b657f7fdb80264a784c2, not stripped\r\n\r\nIt seems that `external/org_nodejs/bin/node` is an architecture-dependent (x64) binary...\r\n", "@brunoalr, please refer this [issue](https://github.com/tensorflow/tensorflow/issues/10001). This issue also talks about the error with tsc.sh, may work for you too.", "@npanpaliya thanks for pointing to that issue. Unfortunately I still can't compile it.", "I updated the issue's name to make clear it occurs on a ppc64le (little endian) rather than a ppc64 (big endian) machine.", "Alright, so I managed to build TensorFlow **without** TensorBoard (https://stackoverflow.com/questions/43119802/can-i-build-tensorflow-without-android-and-without-tensorboard).\r\n\r\nThat seems to be a strong indication that the issue is caused by distributing architecture-dependent binaries (e.g. x64 binaries for nodejs and protoc) to satifisfy some build dependencies.\r\n\r\nSummarizing: it is necessary to check whether the installed jdk supports the \"-client\" flag and a) upload binaries compiled to different targets or b) implement a fallback solution that uses local binaries instead.", "As we do not have access to ppc machines, we do not have official support on this platform.\r\nWe will be happy to accept a Pull request resolving this issue.\r\nTherefore, I will mark this as Contributions Welcome.", "Looks like the reported initial issue was resolved?\r\nAnd it seems to me the reported issues may be caused by tensorboard."]}, {"number": 10305, "title": "Improve windows bazel python test suite.", "body": "- Create new tags, no_windows and no_windows_gpu\r\n- Instead of a separate maintained list, use bazel tags to exclude tests.\r\n- Tag all the python tests that are known to have issues in windows.", "comments": ["Running windows bazel presubmit here:\r\nhttp://ci.tensorflow.org/job/tensorflow-pr-win-bazel/18/", "new link http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/19/", "@meteorcloudy There are some build errors.\r\nhttp://ci.tensorflow.org/job/tensorflow-pr-win-bazel/19/console\r\n\r\nCould you help us debug these?", "Here is another try\r\nhttp://ci.tensorflow.org/job/tensorflow-pr-win-bazel/20/", "@gunan @yifeif Thank you for making this change! This is great!\r\nFrom the latest run, it seems only `//py_test_dir/tensorflow/python/debug:examples_test` is failing. And it turns out to be a shell test!\r\nSo, adding `--test_lang_filters=py` as a test option should solve this problem.", "Another try with all the changes requested:\r\nhttp://ci.tensorflow.org/job/tensorflow-pr-win-bazel/21/"]}, {"number": 10304, "title": "Batch norm docs -> r1.1", "body": "Fixed up the documentation for tf.contrib.layers.batch_norm and tf.layers.batch_normalization explaining how to include the update ops in the train_op. Also simplified the existing tf.layers.batch_normalization documentation to the least error-prone option (adding to the train_op).\r\n\r\nPiperOrigin-RevId: 156609483\r\n\r\nSee https://github.com/tensorflow/tensorflow/commit/675f0f9073e9e6ac6df8018e972f2b38f2c4c23c for the same change in master.", "comments": ["We probably wont touch this branch anymore, as 1.2 is around the corner.\r\nIs it OK if we only cherrypick this into r1.2?", "Yes, please recreate pulling this change into r1.2."]}, {"number": 10303, "title": "TensorflowDebugger does not dump Stack/Pack/Concat nodes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.1\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n8.0/5.1.5\r\n- **GPU model and memory**:\r\nTitan X Pascal\r\n- **Exact command to reproduce**:\r\n```\r\nimport sys\r\nimport tensorflow as tf\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\nbase = tf.ones([10], dtype=tf.float32, name='base')\r\nstacked = tf.stack([base, base], name='stacked')\r\nconcat = tf.concat([[base], [base]], axis=0, name='concat')\r\n\r\nsession = tf.Session()\r\nsession = tf_debug.LocalCLIDebugWrapperSession(session)\r\n\r\nwith session.as_default():\r\n    res = session.run([ stacked, concat])\r\nprint res\r\n```\r\n\r\n### Describe the problem\r\n\r\nWhen using the TensorflowDebugger with stacked/concated, the stacked/concated nodes do not appear in the set of dumped nodes once a run has completed.  In addition nodes that fed into these nodes are not dumped.", "comments": ["@caisq : Mind taking a look?", "@blake-varden This is not a bug. The reason why you see no data dumped is because every node is constant folded in the graph set up by your code. `tf.ones` defines a TF constant. So all the downstream tensors like `stacked` and `concat` are effectively constant. TF's graph optimization knows that and folds all nodes into one for each fetched tensor.\r\n\r\nIf you replace `tf.ones` with a `tf.Variable`, you'll see the data dumped:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\nbase = tf.Variable(np.ones([10]), dtype=tf.float32, name=\"base\")\r\nstacked = tf.stack([base, base], name='stacked')\r\nconcat = tf.concat([[base], [base]], axis=0, name='concat')\r\n\r\nsession = tf.Session()\r\nsession.run(tf.global_variables_initializer())\r\n\r\nsession = tf_debug.LocalCLIDebugWrapperSession(session)\r\nres = session.run([ stacked, concat])\r\n```\r\n\r\nThis behavior is documented in a relatively obscure place:\r\nhttps://www.tensorflow.org/api_docs/python/tfdbg/watch_graph\r\n\r\n> N.B.: 1. Under certain circumstances, the Tensor may not get actually watched (e.g., if the node of the Tensor is constant-folded during runtime). \r\n\r\nFor more on constant folding in TF, see:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L83\r\nand\r\nhttps://www.tensorflow.org/api_docs/python/tf/OptimizerOptions\r\n\r\nTFDBG is working as intended as I just checked in the `tensorflow/tensorflow:nightly` docker image.", "Re-opening the issue as a doc bug. I plan to add a Q&A item for this behavior."]}, {"number": 10302, "title": "Exclude Python test files from CMake PIP package.", "body": "", "comments": []}, {"number": 10301, "title": "TensorBoard Embeddings fail if a \"+\" character is in LOG_DIR subdirectory filename", "body": "### System information\r\n- **Occurs in Tutorial**: DandelionMane's TF Dev Summit 2017 TensorBoard Tutorial\r\n  - If learning_rate >= 1 (not that you would want to do that)\r\n- **OS Platform**: Windows 10\r\n- **TensorFlow install**: Binary\r\n- **TensorFlow version**: 1.1.0\r\n- **Exact command to reproduce**: Use a plus sign in the filename of one of LOG_DIR's subdirectories\r\n\r\n### Problem\r\nThe tensorboard embeddings tab throws an \"Error fetching projector config\" if a + sign is used in a log_dir subdirectory; that is, if the projector_config.pbtxt is in that subdirectory.", "comments": ["@dsmilkov Maybe we need to URLencode the projector config filename?", "This issue has been migrated to https://github.com/tensorflow/tensorboard/issues/73."]}, {"number": 10300, "title": "Upgrade TF ci build and docker files to use bazel 0.5.0", "body": "", "comments": ["bazel 0.5.0 included a fix for bazelbuild/bazel#2759 which required #8880\r\nWhat has to happen for #10236 or rather #10318 to get resolved?", "This PR upgrades our ubuntu images.\r\nBut our MacOS and Windows machines have to be manually upgraded.\r\nI will approve and merge your PR once the upgrades are complete."]}, {"number": 10299, "title": "document how to use selective_registration and the use of __ANDROID_FULL_TYPE__", "body": "Every developer wants to use his own model. \r\nMost app built for Mobile will crash at runtime, because those model won't just use the sparse subset of ops shipped with the aar available from jcenter \r\n\r\nWhich means, that mobile developers need a way to easily and painlessly cross compile tenserflow with the right ops for their custom models. \r\n\r\nThe print_header_for_selecrtive_registration.py  script is a good step in the right direction for this but it is not documented. \r\n\r\nThe documentation is completely lacking. Please document how to cross-compile tenserflow for mobile with the right types, and the right ops. With want command ? what files should we modify, where ? \r\n\r\nI spent days looking at the tensorflow code/build files trying things and I still could not build a binary that would make that annoying \"Op wasn't registered issue\" away\r\n\r\nFix this !\u00a0This is not an individual problem, look at the amount of questions and issues about \"tenserflow and Op wasn't registered\" on the internet.\r\n\r\nbtw, the tensorflow aar from jcenter is completely useless as it can only be used to build the demo app.\r\nIt would be better to put the demo apk on Google play, it would not mislead developers into thinking they can easily build apps with it\r\n \r\n\r\n", "comments": ["CC/ @petewarden @andrewharp ", "@cwhipkey re: selective registration docs.\r\n\r\n@Lakedaemon If there are useful ops that you think should be included in the default set please feel free to send a PR adding them. It's our intention for the AAR to be useful for the majority of common models and a stepping stone before developers build their own size-optimized TF library.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@petewarden @andrewharp any updates on documentation for selective registration?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@petewarden @andrewharp any updates on documentation for selective registration?\r\n\r\n", "close due to inactivity. "]}, {"number": 10298, "title": "PhiloxRandom: Fix race in GPU fill function", "body": "The PhiloxRandom fill kernel for the GPU had race conditions that caused the\r\noutputs to be non-deterministic. In particular, the code previously executed\r\nwith N GPU threads (# thread contexts per GPU), but it would only advance the\r\nfill addresses by N-1 stride in each step. This incorrect stride caused the\r\n0th and N-1st threads to write to the same memory locations, racing for which\r\nwas last to write their common locations. Make the stride equal to the number\r\nof threads to eliminate the race.\r\n\r\nBONUS: By fixing this race, PhiloxRandom constant-sized GPU initializers now\r\nmatch CPU initializers.", "comments": ["Can one of the admins verify this patch?", "Would be great if you could also add a test that would fail before your change and works after.", "@ekelsen: I updated the existing `random_ops_test.py` to manifest the race condition", "Haha, good catch!  LGTM", "Jenkins, test this please"]}, {"number": 10297, "title": "docker tensorflow python - FileNotFoundError: [WinError 3] The system cannot find the path specified: ''", "body": "getting below error while running following command on docker. kindly assist..\r\n\r\n$ python retrain.py \\\r\n> --bottleneck_dir= tf_files/bottlenecks \\\r\n> --how_many_training_steps=500 \\\r\n> --model_dir=tf_files/inception \\\r\n> --summaries_dir=tf_files/training_summaries/basic \\\r\n> --output_graph=tf_files/retrained_graph.pb \\\r\n> --output_labels=tf_files/retrained_labels.txt \\\r\n> --image_dir=tf_files/images/\r\nLooking for images in 'cat'\r\nLooking for images in 'dog'\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"retrain.py\", line 1063, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\Sharjeel.Riaz\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"retrain.py\", line 809, in main\r\n    jpeg_data_tensor, bottleneck_tensor)\r\n  File \"retrain.py\", line 434, in cache_bottlenecks\r\n    ensure_dir_exists(bottleneck_dir)\r\n  File \"retrain.py\", line 316, in ensure_dir_exists\r\n    os.makedirs(dir_name)\r\n  File \"C:\\Users\\Sharjeel.Riaz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\os.py\", line 241, in makedirs\r\n    mkdir(name, mode)\r\nFileNotFoundError: [WinError 3] The system cannot find the path specified: ''", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 10296, "title": "Test CMake entries against filetree", "body": "There were some invalid entries in `tensorflow/contrib/cmake/tf_python.cmake` which are removed as of #10294 and which were discovered alongside #10264 @drpngx where I proposed extracting all file glob entries from all CMake files for better management as well as automated testing for their validity to keep things safe and sound.", "comments": ["/CC @gunan @mrry ", "For multiple reasons, bazel will stay as our primary build system.\r\ncmake wont be promoted to core.\r\nI am not against the test itself, I am happy to accept a test as you described as a contribution, and run it under contrib/cmake. But it is not a priority for us.", "Feel free to describe your design before you send a PR if you'd like some clarification.", "@mrry PR #14877 already covers the extraction. I'll try writing the tests, too.\r\nFirst, ensure to include existing subpackages of name `python`, `python/ops`, `python/kernels` and `python/layers`. Second, ensure not to include non-existent packages.\r\nI'll try to assemble a PR tomorrow or so.", "Since the Bazel build is the ground truth, I'd guess the easiest approach would probably be to add a Bazel test that compares the list of submodules in the built PIP artifact to the list of submodules in `tensorflow/contrib/cmake/python_modules.txt`. ", "@mrry My original intent was to test `python_modules.txt` etc. against the filetree.", "Maybe @gunan can tell where to place the tests?", "We could of course run the tests directly as part of the `tf_python.cmake` build.\r\nBut I wonder if a separate test might be good to have.", "Sorry, missed this.\r\nI agree with @mrry that this should be a bazel test, as that is our ground truth.\r\nWe should add a BUILD file under contrib/cmake, and have a py_test for this in that BUILD file. It will be picked up by our contrib builds automatically.", "Will do @gunan, already sent the PR #15166", "When the `tf_python.cmake` tests are merged, I will look into the other `cmake` files.\r\n@gunan Which of them would probably benefit the most from (extraction and) testing?", "If you mean comparison to bazel built pip package, any cmakefile with a list in them can benefit.\r\nHowever, if you mean just comparing to the file tree, we already run the full cmake build itself, so i dont think we need an extra test for comparing cmakefile entries to the files in the tree.\r\n\r\nResponse to deleted comment:\r\n`tf_py_test` is just a convenience wrapper around `py_test` rule which hides some complexity for a few of our options like xla and grpc. In this case, we definitely do not need it.", "> we already run the full cmake build itself\r\n\r\nSo, why didn't it catch the [invalid entries](https://github.com/tensorflow/tensorflow/pull/15368/files) in `tf_python.cmake` ?", "OK, maybe I can rephrase this.\r\nAs python code simply needs to be copied by cmake, and no compilation or other processing is needed, old files that do not exist anymore are just noop statements, they will try copying but will immediately return success because there is nothing to copy.\r\nFor anything else, the build will fail because it will try compiling a non-existent file, or something similar.\r\n\r\nEither way, either the entries are noops, or get caught by the build.\r\nThe change you have should be able to catch all noops, anything else should be caught by the cmake build.", "@gunan Thanks for the clarification.\r\nSo, `tf_python.cmake` and `tf_tests.cmake` would benefit, others wouldn't, right?\r\nI updated #15590 accordingly.", "Good catch, the test blacklist in tf_tests.cmake may also contain old entries."]}, {"number": 10295, "title": "LRN and other fixes", "body": "* Fixed bug in LRN (Mahmoud)\r\n* Optimized slice op using OpenMP (Niranjan)\r\n* Optimization for NHWC (Amin)", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Hi, is someone looking at this?", "@andydavis1: could you also get someone to check the CLA problem? As far as I know, all contributors have signed the CLA.", "Are all contributors ok with this being contributed? If so, we're ok to merge. CLAbot cannot distinguish this, so if we have checked, we can merge over its protestations.", "I am ok with this being contributed. ", "I am also ok with this contribution.", "@mdfaijul Mind confirming that you're ok with contributing this? Thanks!", "I am okay with this contribution.", "@tensorflow-jenkins test this please.", "Looks like there's an issue with Jenkins:\r\n\r\n> \r\n> Building image tf-make-base...\r\n> Error checking context: 'no permission to read from '/var/lib/jenkins/workspace/tensorflow-pull-requests-makefile/tensorflow/contrib/makefile/downloads/eigen/unsupported/test/openglsupport.cpp''.\r\n> ERROR: docker build failed. Dockerfile is at /var/lib/jenkins/workspace/tensorflow-pull-requests-makefile/tensorflow/contrib/makefile/../../../tensorflow/contrib/makefile/Dockerfile\r\n> Build step 'Execute shell' marked build as failure", "@martinwicke @jhseu @andydavis1 Who would be the right person to follow up with for the Jenkins failure?", "Possibly transient failure, trying again. \r\n\r\nJenkins, test this please.", "Doesn't look like it went away\r\n\r\n> \r\n> Building image tf-make-base...\r\n> Error checking context: 'no permission to read from '/var/lib/jenkins/workspace/tensorflow-pull-requests-makefile/tensorflow/contrib/makefile/downloads/eigen/unsupported/test/openglsupport.cpp''.\r\n> ERROR: docker build failed. Dockerfile is at /var/lib/jenkins/workspace/tensorflow-pull-requests-makefile/tensorflow/contrib/makefile/../../../tensorflow/contrib/makefile/Dockerfile\r\n> Build step 'Execute shell' marked build as failure\r\n> [Set GitHub commit status (universal)] ERROR on repos [] (sha:83b7987) with context:tensorflow-pull-requests-makefile\r\n> Unable to get pull request builder trigger!!", "@tensorflow-jenkins test this please.", "I see tensorboard related failures - don't think they are related to us.\r\n\r\n> //bazel_pip/tensorflow/tensorboard/backend/event_processing:directory_watcher_test FAILED in 2.7s\r\n>   /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/testlogs/bazel_pip/tensorflow/tensorboard/backend/event_processing/directory_watcher_test/test.log\r\n> //bazel_pip/tensorflow/tensorboard/backend/event_processing:event_accumulator_test FAILED in 2.7s\r\n>   /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/testlogs/bazel_pip/tensorflow/tensorboard/backend/event_processing/event_accumulator_test/test.log\r\n> //bazel_pip/tensorflow/tensorboard/backend/event_processing:event_file_inspector_test FAILED in 2.5s\r\n>   /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/testlogs/bazel_pip/tensorflow/tensorboard/backend/event_processing/event_file_inspector_test/test.log\r\n> //bazel_pip/tensorflow/tensorboard/backend/event_processing:event_file_loader_test FAILED in 2.7s\r\n>   /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/testlogs/bazel_pip/tensorflow/tensorboard/backend/event_processing/event_file_loader_test/test.log\r\n> //bazel_pip/tensorflow/tensorboard/backend/event_processing:event_multiplexer_test FAILED in 2.4s\r\n>   /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/testlogs/bazel_pip/tensorflow/tensorboard/backend/event_processing/event_multiplexer_test/test.log\r\n> //bazel_pip/tensorflow/tensorboard/backend/event_processing:reservoir_test FAILED in 2.5s\r\n>   /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/testlogs/bazel_pip/tensorflow/tensorboard/backend/event_processing/reservoir_test/test.log\r\n> //bazel_pip/tensorflow/tensorboard/backend:http_util_test                FAILED in 2.7s\r\n>   /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/testlogs/bazel_pip/tensorflow/tensorboard/backend/http_util_test/test.log\r\n> //bazel_pip/tensorflow/tensorboard/backend:json_util_test                FAILED in 2.5s\r\n>   /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/testlogs/bazel_pip/tensorflow/tensorboard/backend/json_util_test/test.log\r\n> \r\n> Executed 657 out of 657 tests: 649 tests pass and 8 fail locally."]}, {"number": 10294, "title": "remove some invalid entries", "body": "I noticed that some entries don't exist (anymore).\r\nThis seems to be some kind of a consistency issue.\r\n\r\nMore specifically:\r\n`tensorflow/contrib/ios_examples/camera/data`\r\n`tensorflow/contrib/session_bundle/testdata/saved_model_half_plus_two`\r\n`tensorflow/contrib/session_bundle/testdata/saved_model_half_plus_two/variables`\r\n\r\nThis is the continuation of PR #10264 @drpngx ", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "`stage_op_test` error appears unrelated."]}, {"number": 10293, "title": "fix the error in deploy hadoop documentation", "body": "there is a small error in shell commands", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please"]}, {"number": 10292, "title": "FEATURE: Label scalar summaries", "body": "Often I log scalar summaries at intervals that are meaningful in their own right \u2014\u00a0e.g. at epochs or min-batch boundaries \u2014\u00a0and report this, rather than the global step, as the x-value with something like\r\n\r\n    some_writer.add_summary(a_value, epoch_number)\r\n\r\nor\r\n\r\n    some_writer.add_summary(a_value, batch_number)\r\n\r\nBut the API has no way of indicating that these x-values are not \"steps\", and reports them in the TensorBoard UI as \"Steps\", which they are not.\r\n\r\n_It would be nice to have a way of changing the \"Steps\" label to some other string (here, for example \"Epoch\" or \"Batch\")._", "comments": ["CC @dandelionmane @jart ", "I have migrated this issue to tensorflow/tensorboard#75 because TensorBoard has moved into its own repository. Lets continue discussion there."]}, {"number": 10291, "title": "Memory violation when adding an new op", "body": "-----------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\ncommit 3bee923c9\r\n- **Bazel version (if compiling from source)**:\r\n0.4.5\r\n- **CUDA/cuDNN version**:\r\ncuda 8.0/cudnn 5.1.5\r\n- **GPU model and memory**:\r\nTesla P40 \r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI created a custom operation, it works well with bazel test  --run_under=valgrind. However, when I uses the custom operation using python api, memory violation is happend. Even though no modification of the input tensor, input tensor error comes out.\r\n\r\n### Source code / logs\r\n```\r\nnamespace tensorflow {\r\n\r\nclass InGraphAutoParallelOp : public OpKernel {\r\n public:\r\n  explicit InGraphAutoParallelOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* ctx) override {\r\n    // Get graph definition\r\n    const Tensor* meta_graph_proto_tensor;\r\n    OP_REQUIRES_OK(ctx, ctx->input(\"meta_graph_def_str\", &meta_graph_proto_tensor));\r\n    MetaGraphDef meta_graph_def;\r\n    meta_graph_def.ParseFromString(meta_graph_proto_tensor->scalar<string>()());\r\n\r\n    // Get num replicas\r\n    const Tensor* num_replicas_tensor;\r\n    OP_REQUIRES_OK(ctx, ctx->input(\"num_replicas\", &num_replicas_tensor));\r\n    int num_replicas = num_replicas_tensor->flat<int>()(0);\r\n\r\n    MetaGraphDef out_meta_graph_def;\r\n    if (num_replicas == 1) {\r\n      out_meta_graph_def = meta_graph_def;\r\n    } else {\r\n      rdag::grappler::AutoParallel auto_parallel(num_replicas);\r\n      auto_parallel.BuildGraph(meta_graph_def, out_meta_graph_def.mutable_graph_def());\r\n    }\r\n\r\n    Tensor* output_tensor = NULL;\r\n    OP_REQUIRES_OK(ctx,\r\n        ctx->allocate_output(0, TensorShape({}), &output_tensor));\r\n    CHECK(out_meta_graph_def.SerializeToString(&output_tensor->scalar<string>()()));\r\n  }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"InGraphAutoParallel\").Device(DEVICE_CPU), InGraphAutoParallelOp);\r\n\r\n \r\n```\r\n\r\nBelow is the modification of auto_parallel code in TensorFlow.\r\n\r\n```\r\n#include \"tensorflow_rdag/grappler/auto_parallel.h\"\r\n\r\n#include \"tensorflow_rdag/grappler/grappler_item_builder.h\"\r\n#include \"tensorflow_rdag/grappler/op_types.h\"\r\n#include \"tensorflow_rdag/grappler/utils.h\"\r\n\r\nnamespace tensorflow {\r\nnamespace rdag {\r\nnamespace grappler {\r\n\r\nstatic const std::set<std::string> APPLY_GRADIENT_OPS = {\"ApplyGradientDescent\",\r\n                                                     \"ApplyProximalGradientDescent\",\r\n                                                     \"ApplyAdadelta\",\r\n                                                     \"ApplyAdagrad\",\r\n                                                     \"ApplyProximalAdagrad\",\r\n                                                     \"ApplyAdagradDA\",\r\n                                                     \"ApplyFtrl\",\r\n                                                     \"ApplyMomentum\",\r\n                                                     \"ApplyAdam\",\r\n                                                     \"ApplyRMSProp\",\r\n                                                     \"ApplyCenteredRMSProp\"};\r\nstatic std::map<std::string, int> GRADIENT_POS = {{\"ApplyGradientDescent\", 2},\r\n                                             {\"ApplyProximalGradientDescent\", 4},\r\n                                             {\"ApplyAdadelta\", 6},\r\n                                             {\"ApplyAdagrad\", 3},\r\n                                             {\"ApplyProximalAdagrad\", 5},\r\n                                             {\"ApplyAdagradDA\", 3},\r\n                                             {\"ApplyFtrl\", 3},\r\n                                             {\"ApplyMomentum\", 3},\r\n                                             {\"ApplyAdam\", 9},\r\n                                             {\"ApplyRMSProp\", 7},\r\n                                             {\"ApplyCenteredRMSProp\", 8}};\r\nconst char kAutoParallelPrefix[] = \"AutoParallel\";\r\n\r\nstd::string AutoParallel::GetGradientNodeName(const std::string& apply_gradient_node_name) {\r\n  auto apply_gradients_node = all_nodes_[apply_gradient_node_name];\r\n  return apply_gradients_node->input(GRADIENT_POS[apply_gradients_node->op()]);\r\n}\r\n\r\nStatus AutoParallel::Initialize(const GrapplerItem& item) {\r\n  LOG(INFO) << \"Number of replicas: \" << num_replicas_;\r\n  item_ = &item;\r\n  graph_ = item.graph;\r\n  LOG(INFO) << \"Original graph size: \" << item.graph.node_size();\r\n  if (item.fetch.empty()) {\r\n    return Status(error::INVALID_ARGUMENT, \"No fetch nodes provided.\");\r\n  }\r\n\r\n  if (item.MainVariables().empty()) {\r\n    return Status(error::INVALID_ARGUMENT, \"No variables provided.\");\r\n  }\r\n\r\n  for (const auto& init : item.init_ops) {\r\n    VLOG(1) << \"Init node: \" << init;\r\n  }\r\n\r\n  for (const auto& fetch : item.fetch) {\r\n    VLOG(1) << \"Fetch node: \" << fetch;\r\n  }\r\n\r\n  for (const auto& var : item.MainVariables()) {\r\n    VLOG(2) << \"Variable: \" << var->name();\r\n  }\r\n\r\n  for (QueueRunnerDef def : item.queue_runners) {\r\n    queue_runners_.insert(std::make_pair(def.queue_name(), def));\r\n  }\r\n\r\n  for (VariableDef def : item.variables) {\r\n    variables_.insert(std::make_pair(def.variable_name(), def));\r\n  }\r\n\r\n  std::vector<std::string> queue_nodes;\r\n  for (int i = 0; i < graph_.node_size(); i++) {\r\n    all_nodes_.insert(\r\n        std::make_pair(graph_.node(i).name(), graph_.mutable_node(i)));\r\n    if (APPLY_GRADIENT_OPS.find(graph_.node(i).op()) !=\r\n      APPLY_GRADIENT_OPS.end()) {\r\n      apply_gradients_nodes_.insert(graph_.node(i).name());\r\n      VLOG(2) << \"Apply gradients node: \" << graph_.node(i).name();\r\n    }\r\n  }\r\n\r\n  std::set<std::string> dont_replicate_nodes;\r\n  for (const auto& variable : item.MainVariables()) {\r\n    dont_replicate_nodes.insert(variable->name());\r\n    VariableDef def = variables_[variable->name()];\r\n    dont_replicate_nodes.insert(def.initializer_name());\r\n    dont_replicate_nodes.insert(def.snapshot_name());\r\n  }\r\n\r\n  std::vector<std::string> gradient_nodes;\r\n  for (const auto& apply_gradient_node_name : apply_gradients_nodes_) {\r\n    gradient_nodes.push_back(GetGradientNodeName(apply_gradient_node_name));\r\n  }\r\n\r\n  auto train_nodes = ComputeTransitiveFanin(graph_, gradient_nodes);\r\n  LOG(INFO) << \"Number of training nodes: \" << train_nodes.size();\r\n\r\n  for (const auto& fetch_node_name : item.fetch) {\r\n    dont_replicate_nodes.insert(fetch_node_name);\r\n  }\r\n\r\n  std::vector<const NodeDef*> enqueue_dequeue_nodes;\r\n  std::vector<const NodeDef*> visitied;\r\n  for (const auto& node : train_nodes) {\r\n    if (dont_replicate_nodes.find(node->name()) == dont_replicate_nodes.end()) {\r\n      replica_nodes_.insert(node->name());\r\n      if (IsDequeueOp(*node)) {\r\n        enqueue_dequeue_nodes.push_back(node);\r\n      }\r\n    }\r\n  }\r\n  LOG(INFO) << \"Number of replica nodes: \" << replica_nodes_.size();\r\n\r\n  std::vector<const NodeDef*> input_nodes;\r\n  while (!enqueue_dequeue_nodes.empty()) {\r\n    // Pop first node in eneque_dequeue_nodes.\r\n    const NodeDef* enqueue_dequeue_node = *enqueue_dequeue_nodes.begin();\r\n    enqueue_dequeue_nodes.erase(enqueue_dequeue_nodes.begin());\r\n    if(std::find(visitied.begin(), visitied.end(), enqueue_dequeue_node) != visitied.end())\r\n      continue;\r\n    visitied.push_back(enqueue_dequeue_node);\r\n\r\n    auto temp_input_nodes = ComputeTransitiveFanin(graph_, {enqueue_dequeue_node->name()});\r\n\r\n    for (const NodeDef* input_node : temp_input_nodes) {\r\n      input_nodes.push_back(input_node);\r\n\r\n      if (IsQueueOp(*input_node)) {\r\n        QueueRunnerDef def = queue_runners_[input_node->name()];\r\n        for (int i = 0; i < def.enqueue_op_name_size(); i++) {\r\n          const auto& enqueue_op = all_nodes_[def.enqueue_op_name(i)];\r\n          input_nodes.push_back(enqueue_op);\r\n          enqueue_dequeue_nodes.push_back(enqueue_op);\r\n        }\r\n        input_nodes.push_back(all_nodes_[def.close_op_name()]);\r\n        input_nodes.push_back(all_nodes_[def.cancel_op_name()]);\r\n      } else if (IsDequeueOp(*input_node)) {\r\n        enqueue_dequeue_nodes.push_back(input_node);\r\n      }\r\n    }\r\n  }\r\n\r\n  LOG(INFO) << \"Number of input nodes: \" << input_nodes.size();\r\n\r\n  // Replicate all input pipeline nodes\r\n  for (const auto& input_node : input_nodes) {\r\n    replica_nodes_.insert(input_node->name());\r\n  }\r\n\r\n  for (const auto& node : all_nodes_) {\r\n    if (replica_nodes_.find(node.first) == replica_nodes_.end()) {\r\n      shared_nodes_.insert(node.first);\r\n    }\r\n  }\r\n  LOG(INFO) << \"Number of shared nodes: \" << shared_nodes_.size();\r\n  return Status::OK();\r\n}\r\n\r\nbool AutoParallel::NotSharedNode(const std::string& name) {\r\n  return shared_nodes_.find(name) == shared_nodes_.end();\r\n}\r\n\r\nvoid AutoParallel::AddSharedNodes(GraphDef* graph) {\r\n  std::string prefix = strings::StrCat(kAutoParallelPrefix, \"-Replica-\", 0);\r\n  for (const auto& node : shared_nodes_) {\r\n    auto new_node = graph->add_node();\r\n    *new_node = *all_nodes_[node];\r\n    for (int i = 0; i < new_node->input_size(); i++) {\r\n      if (NotSharedNode(NodeName(new_node->input(i)))) {\r\n        std::string new_name = AddPrefixToNodeName(new_node->input(i), prefix);\r\n        *new_node->mutable_input(i) = new_name;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nvoid AutoParallel::AddOneReplica(GraphDef* graph, int number) {\r\n  std::string prefix = strings::StrCat(kAutoParallelPrefix, \"-Replica-\", number);\r\n  for (const auto& node : replica_nodes_) {\r\n    auto new_node = graph->add_node();\r\n    *new_node = *all_nodes_[node];\r\n    assert(new_node != all_nodes_[node]);\r\n    if (NotSharedNode(new_node->name())) {\r\n      new_node->set_name(AddPrefixToNodeName(new_node->name(), prefix));\r\n      if (num_replicas_ > 0) {\r\n        // TODO : keep previous device setting except gpu device\r\n        // new_node->set_device(std::strings::StrCat(\"/gpu:\", number % num_replicas_));\r\n      }\r\n      for (int i = 0; i < new_node->input_size(); i++) {\r\n        if (NotSharedNode(NodeName(new_node->input(i)))) {\r\n          std::string new_name = AddPrefixToNodeName(new_node->input(i), prefix);\r\n          *new_node->mutable_input(i) = new_name;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nNodeDef* AutoParallel::AddNodeDivConst(GraphDef* graph) {\r\n  NodeDef* node = graph->add_node();\r\n  node->set_name(strings::StrCat(kAutoParallelPrefix, \"-Div-Const\"));\r\n  node->set_op(\"Const\");\r\n\r\n  AttrValue attr_data_type;\r\n  attr_data_type.set_type(DT_FLOAT);\r\n  node->mutable_attr()->insert({\"dtype\", attr_data_type});\r\n\r\n  AttrValue attr_tensor;\r\n  auto tensor = attr_tensor.mutable_tensor();\r\n  tensor->add_float_val(static_cast<float>(num_replicas_));\r\n  tensor->set_dtype(DT_FLOAT);\r\n  node->mutable_attr()->insert({\"value\", attr_tensor});\r\n  return node;\r\n}\r\n\r\nNodeDef* AutoParallel::AddNodeDiv(GraphDef* graph, const std::string& name, const std::string& input_a,\r\n                                  const std::string& input_b) {\r\n  NodeDef* node = graph->add_node();\r\n  node->set_name(strings::StrCat(kAutoParallelPrefix, \"-Div-\", name));\r\n  node->set_op(\"RealDiv\");\r\n  node->add_input(input_a);\r\n  node->add_input(input_b);\r\n  AttrValue attr_type;\r\n  attr_type.set_type(DT_FLOAT);\r\n  node->mutable_attr()->insert({\"T\", attr_type});\r\n  return node;\r\n}\r\n\r\nvoid AutoParallel::AddApplyGradientDescent(GraphDef* graph) {\r\n  std::map<std::string, NodeDef*> apply_gradients_nodes;\r\n  for (int i = 0; i < graph->node_size(); i++) {\r\n    if (APPLY_GRADIENT_OPS.find(graph->node(i).op()) !=\r\n        APPLY_GRADIENT_OPS.end()) {\r\n        apply_gradients_nodes.insert(\r\n          std::make_pair(graph->node(i).name(), graph->mutable_node(i)));\r\n    }\r\n  }\r\n\r\n  auto div_const_node = AddNodeDivConst(graph);\r\n\r\n  for (const auto& apply_gradient_node_name : apply_gradients_nodes_) {\r\n    auto apply_gradients_node = apply_gradients_nodes[apply_gradient_node_name];\r\n\r\n    // Add all gradients\r\n    NodeDef* add_node = graph->add_node();\r\n    add_node->set_name(strings::StrCat(kAutoParallelPrefix, \"-Add-\", apply_gradient_node_name));\r\n    add_node->set_op(\"AddN\");\r\n    const auto& input_name = GetGradientNodeName(apply_gradient_node_name);\r\n    for (int i = 0; i < num_replicas_; i++) {\r\n      add_node->add_input(strings::StrCat(kAutoParallelPrefix, \"-Replica-\", i, \"/\", input_name));\r\n      LOG(INFO) << \"Gradient Node Name: \" << strings::StrCat(kAutoParallelPrefix, \"-Replica-\", i, \"/\", input_name);\r\n    }\r\n    AttrValue attr_type;\r\n    attr_type.set_type(DT_FLOAT);\r\n    add_node->mutable_attr()->insert({\"T\", attr_type});\r\n    AttrValue attr_type2;\r\n    attr_type2.set_type(DT_INT32);\r\n    attr_type2.set_i(num_replicas_);\r\n    add_node->mutable_attr()->insert({\"N\", attr_type2});\r\n\r\n    // Divide by number of GPUs\r\n    auto div_node = AddNodeDiv(\r\n        graph,\r\n        apply_gradient_node_name,\r\n        add_node->name(),\r\n        div_const_node->name());\r\n\r\n    LOG(INFO) << \"Change gradient node as : \" << div_node->name();\r\n    *apply_gradients_node->mutable_input(GRADIENT_POS[apply_gradients_node->op()]) =\r\n        div_node->name();\r\n  }\r\n}\r\n\r\nvoid AutoParallel::BuildGraph(const MetaGraphDef& meta_graph, GraphDef* graph) {\r\n  ItemConfig cfg;\r\n  std::unique_ptr<GrapplerItem> grappler_item = GrapplerItemFromMetaGraphDef(\r\n      \"graph_to_optimize\", meta_graph, cfg);\r\n  Initialize(*grappler_item);\r\n\r\n  //GraphDef* out_graph_def = out_graph.mutable_graph_def();\r\n  AddSharedNodes(graph);\r\n  for (int i = 0; i < num_replicas_; i++) {\r\n    AddOneReplica(graph, i);\r\n  }\r\n  AddApplyGradientDescent(graph);\r\n  //\r\n  *(graph->mutable_library()) = meta_graph.graph_def().library();\r\n  // LOG(INFO) << \"Parallelized graph size: \" << out_graph_def->node_size();\r\n\r\n  // SetMetaGraphForQueueRunners(out_graph, replicated_queues);\r\n  // SetMetaGraphForVariables(out_graph, variables);\r\n  //return graph;\r\n}\r\n\r\n}  // end namespace grappler\r\n}  // end namespace rdag\r\n}  // end namespace tensorflow\r\n\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10290, "title": "tf.split should allow negative axis arguments", "body": "`tf.concat` does, for instance.", "comments": ["Are you sure about `tf.concat`? The documentation at [core/ops/array_ops.cc#L369](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/array_ops.cc#L369) doesn't say so. This might be a consistency issue.", "Yep, just confirmed in a shell.  I'll fix the documentation for `tf.concat`.", "@vrv @cwhipkey Fixed by someone else two hours before I submitted my CL."]}, {"number": 10289, "title": "Cannot bazel build tensorflow pip package from source (/usr/bin/env: 'python': No such file or directory)", "body": "------------------------\r\n### System information\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- CUDA Toolkit v8.0\r\n- cuDNN 5\r\n- gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\n- Bazel 0.5.0 (from repo)\r\n- Anaconda Python 3.6\r\n```\r\nfrancesco@gpu-box:~/tensorflow$ python\r\nPython 3.6.0 |Anaconda custom (64-bit)| (default, Dec 23 2016, 12:22:00) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> \r\n```\r\n\r\n\r\n------------------------\r\n\r\nHello,\r\n\r\nI am trying to build tensor flow pip package from source. I can install and use tensorflow with GPU using a precompiled binary just fine. However, in this case, I need to build from source.\r\n\r\nHere is the following steps I take:\r\n1. git clone https://github.com/tensorflow/tensorflow.git\r\n2. cd tensorflow\r\n3. ./configure\r\n\r\n```\r\nfrancesco@gpu-box:~/tensorflow$ ./configure \r\nPlease specify the location of python. [Default is /home/francesco/anaconda3/bin/python]: \r\nFound possible Python library paths:\r\n  /home/francesco/anaconda3/lib/python3.6/site-packages/\r\n  /home/francesco/anaconda3/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/francesco/anaconda3/lib/python3.6/site-packages/]\r\n\r\nUsing python library path: /home/francesco/anaconda3/lib/python3.6/site-packages/\r\nDo you wish to build TensorFlow with MKL support? [y/N] n\r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] y\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n\r\nNo XLA JIT support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] n\r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N] n\r\nnvcc will be used as CUDA compiler\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the cuDNN version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\nDo you wish to build TensorFlow with MPI support? [y/N] n\r\nMPI support will not be enabled for TensorFlow\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\n\r\n```\r\n\r\nUnfortunately, when I then build with bazel, it runs into an error. It seems like it cannot find python?\r\n\r\n```\r\nfrancesco@gpu-box:~/tensorflow$ bazel build -c opt --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: /home/francesco/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.\r\nWARNING: /home/francesco/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.\r\nINFO: Found 1 target...\r\nINFO: From Compiling external/snappy/snappy-sinksource.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nINFO: From Compiling external/snappy/snappy-c.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nINFO: From Compiling external/snappy/snappy.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nexternal/snappy/snappy.cc: In member function 'void snappy::SnappySinkAllocator::Flush(size_t)':\r\nexternal/snappy/snappy.cc:1403:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < blocks_.size(); ++i) {\r\n                       ^\r\nIn file included from external/snappy/snappy-internal.h:34:0,\r\n                 from external/snappy/snappy.cc:30:\r\nexternal/snappy/snappy.cc: In instantiation of 'bool snappy::SnappyScatteredWriter<Allocator>::AppendFromSelf(size_t, size_t) [with Allocator = snappy::SnappySinkAllocator; size_t = long unsigned int]':\r\nexternal/snappy/snappy.cc:715:13:   required from 'void snappy::SnappyDecompressor::DecompressAllTags(Writer*) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>]'\r\nexternal/snappy/snappy.cc:799:3:   required from 'bool snappy::InternalUncompressAllTags(snappy::SnappyDecompressor*, Writer*, snappy::uint32) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>; snappy::uint32 = unsigned int]'\r\nexternal/snappy/snappy.cc:1460:78:   required from here\r\nexternal/snappy/snappy.cc:1316:34: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (PREDICT_TRUE(offset - 1u < op_ptr_ - op_base_ && op_end <= op_limit_)) {\r\n                                  ^\r\nexternal/snappy/snappy-stubs-internal.h:80:25: note: in definition of macro 'PREDICT_TRUE'\r\n #define PREDICT_TRUE(x) x\r\n                         ^\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nERROR: /home/francesco/.cache/bazel/_bazel_francesco/7b3bdb053a374c3fec955b526c0e6446/external/highwayhash/BUILD.bazel:8:1: C++ compilation of rule '@highwayhash//:sip_hash' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/francesco/.cache/bazel/_bazel_francesco/7b3bdb053a374c3fec955b526c0e6446/execroot/tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/francesco/anaconda3/bin/python \\\r\n    PYTHON_LIB_PATH=/home/francesco/anaconda3/lib/python3.6/site-packages/ \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n    TF_CUDA_VERSION=8.0 \\\r\n    TF_CUDNN_VERSION=5 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/local_linux-py3-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.pic.d '-frandom-seed=bazel-out/local_linux-py3-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.pic.o' -fPIC -iquote external/highwayhash -iquote bazel-out/local_linux-py3-opt/genfiles/external/highwayhash -iquote external/bazel_tools -iquote bazel-out/local_linux-py3-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c external/highwayhash/highwayhash/sip_hash.cc -o bazel-out/local_linux-py3-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.\r\n/usr/bin/env: 'python': No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 44.289s, Critical Path: 5.54s\r\n```\r\n\r\nBoth my python_bin_path and python_lib_path are correct, so I do not understand.\r\n\r\nAny advice?\r\n\r\nRegards", "comments": ["I am pretty certain the cause is using anaconda python, instead of ubuntu python.\r\nIs your python bin path included in your \"path\" environment variable?", "```\r\nfrancesco@gpu-box:~$ env\r\nSHELL=/bin/bash\r\nLD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64\r\nPATH=/home/francesco/anaconda3/bin:/usr/local/cuda-8.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\r\nPWD=/home/francesco\r\nHOME=/home/francesco\r\n_=/usr/bin/env\r\n```\r\n\r\nYes it's included, AFAIK", "@damienmg Could you help look into this?\r\nhighwayhash//:siphash seems to be a cc rule, but I am not sure why we see the failure here.\r\nmaybe there is some backwards incompatible change with bazel 0.5?", "Can you try with `--action_env PATH`?", "Nope.\r\nTried variations like `--action_env PATH`, `--action_env=PATH`, `--action_env=$PATH` to no avail.\r\n", "`--action_env PATH=\"$PATH\"`?", "Hi,\r\n\r\nThat worked. Ran into another issue `ModuleNotFoundError: No module named 'tensorflow.python.pywrap_tensorflow_internal'`when importing the tensorflow in python, but that's only because I was running the shell in the git directory :)\r\n\r\nWill close the issue. Thanks for the help.", "Got snagged by this too. Anaconda is very common, recommend adding a side-note of this flag in the installation instructions", "I compiled TF 1.13.1 with bazel 0.21.0, and I had to add `--noincompatible_strict_action_env` on top of `action_env`.", "@hsgkim , Thanks! Compiled TF with this command:\r\n```bash\r\nbazel build --config=opt --config=cuda --noincompatible_strict_action_env //tensorflow/tools/pip_package:build_pip_package\r\n```", "Ubuntu 20.04.\r\n\r\nIt's work for me\r\n\r\n`sudo ln -s /usr/bin/python3 /usr/bin/python`\r\n", "> Ubuntu 20.04.\r\n> \r\n> It's work for me\r\n> \r\n> `sudo ln -s /usr/bin/python3 /usr/bin/python`\r\n\r\nThank you @deslum \r\nThe command worked for me too.\r\nUbuntu 21.04"]}, {"number": 10288, "title": "[XLA][Feature] - Pass config flags for LLVM runtime.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**:  ('v1.0.0-1783-g4c3bb1a', '1.0.0')\r\n- **Bazel version (if compiling from source)**:  0.4.5\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: -\r\n\r\n### Problem description\r\nAs part of my Google Summer of Code project, I am trying to build TensorFlow with Polly-enabled LLVM. To do this, I have written my own BUILD file which runs TensorFlow with a custom repository of LLVM that has Polly checked out as well. I have managed to get a clean build and am now looking to incorporate Polly's passes in the Optimization pipeline of XLA. \r\n\r\nIn XLA, the llvm module passes are registered [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/compiler_functor.cc#L214).\r\n\r\nPolly register's its passes in LLVM through the following steps  \r\n\r\n1)\r\n```\r\nstatic llvm::RegisterStandardPasses RegisterPollyOptimizerEarly(\r\n    llvm::PassManagerBuilder::EP_ModuleOptimizerEarly,\r\n    registerPollyEarlyAsPossiblePasses);\r\n```\r\nCorresponding file - ```<polly-src>/lib/Support/RegisterPasses.cpp```.  \r\n\r\n2)\r\n\r\n```\r\npolly::initializePollyPasses(Registry); \r\n```\r\n\r\nCorresponding file - ```<polly-src>/lib/Polly.cpp```\r\n\r\nI have built the object files for both these files. But I want to check if Polly is actually being invoked in the pipeline, and so my question is -\r\n- Are these steps enough to use Polly in the bazel build of TensorFlow?\r\n- How can I pass configuration flags to LLVM in TensorFlow to check for Polly usage?\r\n\r\nAs a reference, please find my BUILD file [here](https://gitlab.com/annanay25/tensorflow/blob/master/third_party/llvm/llvm.BUILD).\r\n\r\ncc @phawkins @eliben", "comments": ["Thank you for working to bring LLVM Polly optimizations to TensorFlow! Since @phawkins owns our llvm.BUILD file, I'll assign to him, since helping you accomplish this will probably be quite beneficial to the community.", "Thank you so much! @jart \r\n\r\nUpdate: I have figured out the answer to the second question -\r\nHow can I pass configuration flags to LLVM in TensorFlow to check for Polly usage?\r\n\r\nIts by adding ``` TF_XLA_FLAGS=\"--xla_cpu_llvm_cl_opts=-polly\" ``` to the python command.  \r\n\r\nHowever, the ```-polly``` flag is not being recognized by llvm yet, and that means that the pass manager has not yet been linked correctly. I will need some help regarding the same.\r\n\r\nThanks.", "I've been working w/ Annanay to get the llvm build files modified -- he's done some great work. Annanay - can you share your repo here? It'll be a lot easier for people to help you if they can reproduce your work easily.", "Hi @brianretford,\r\nThanks for this.\r\n\r\nThese are the steps to reproduce my state right now:\r\n- Clone my tensorflow [repository](https://gitlab.com/annanay25/tensorflow) or \r\n  1. copy the llvm build [file](https://gitlab.com/annanay25/tensorflow/blob/master/third_party/llvm/llvm.BUILD) \r\n  2. edit the xla build file to add [this line](https://gitlab.com/annanay25/tensorflow/blob/master/tensorflow/compiler/xla/service/BUILD#L1110) to your local tensorflow repository.\r\n- Clone this version of [llvm](https://gitlab.com/annanay25/llvm-tensorflow) - it is rolled back to the required commit and has polly checked out as well.\r\n- Modify the ```tensorflow/tensorflow/workspace.bzl``` file and change the [llvm rule](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L496-L508) by replacing it with this:\r\n\r\n```\r\n  native.new_local_repository (\r\n       name = \"llvm\",\r\n       path = \"<path to llvm clone>\",\r\n       build_file = str(Label(\"//third_party/llvm:llvm.BUILD\")),\r\n   )\r\n```\r\n- Run ```bazel build --verbose_failures -c opt //tensorflow/tools/pip_package:build_pip_package``` and ``` bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg```\r\n in the tensorflow root folder. \r\n\r\n- Navigate to a folder with a python file that uses tensorflow. \r\n  Run ```TF_XLA_FLAGS=\"--xla_cpu_llvm_cl_opts=-disable-inlined-alloca-merging\"  python <filename.py> --xla=true```. It should run without any errors.\r\n  Run \r\n```TF_XLA_FLAGS=\"--xla_cpu_llvm_cl_opts=-polly\"  python <filename.py> --xla=true```. The command should fail with \"unrecognized flag -polly\".\r\n\r\n\r\nThanks.\r\n", "Please check #10471."]}, {"number": 10287, "title": "Windows 8.1 Anaconda Tensorflow GPU -- BLACKSCREEN", "body": "Followed instructions on https://www.tensorflow.org/install/install_windows\r\nWhen I get to\r\n\r\nactivate tensorflow-gpu \r\n $ python\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\nScreen goes Black. When I move the cursor it moves, and then resets to the center every 10 seconds or so and disappears into the blackness, and doesnt go back to normal screen.\r\n\r\n### System information\r\n- Windows 8.1 Pro\r\nused\r\n- pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.1.0-cp35-cp35m-win_amd64.whl \r\n\r\n- Cuda toolkit 8.0 \r\n- Cudnn 5.1\r\n- 4G NVidia GT 750M:\r\n\r\n python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nb'unkown' 1.1.0\r\n\r\n\r\nLet me know what else I should provide\r\n", "comments": ["If you have s single GPU, problems like this can happen, because TF and your display compete for the same GPUs resources. TF by default tries to allocate as much GPU memory as possible. You can try tuning that. You can try to change that behaviour with this:\r\nhttps://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth", "I tried both allow_growth  and the fraction config parameters down until 0.1 but still getting a black screen..  a bit confused as to why that is the case. I can't log anything, since as soon as I hit enter on sess = tf.Session(config = config) the screen crashes.  Any alternatives? I thought my gpu was compatible, and all I was computing is to print hello world ..\r\n\r\nI have a laptop with intel integrated gpu, but from my search it seems I can use that as a display whilst I compute with the nvidia GPU. ", "The issue sounds like a problem with either your GPU, or your driver, then.\r\nWe cannot reproduce the issue you are seeing in any of your machines, so you can try stackoverflow to see how you may be able to troubleshoot your GPU on your laptop.", "Ok. Thank you, I'll have a look into that on stackoverflow.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I got a similar problem recently on Ubuntu 18.04 LTS with TensorFlow-GPU 1.12. The driver, CUDA, cuDNN are the latest. For training even simple small model, the screen turned black but the computer was fine (as I could still listen to the music from the computer). Still have no clue what is going wrong. I had a RTX 2080 GPU."]}]