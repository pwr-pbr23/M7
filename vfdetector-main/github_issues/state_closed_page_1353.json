[{"number": 12490, "title": "Expose new CRF classes and function.", "body": "#12056 introduced a new tensor-based CRF decoding. This PR exposes the new names at the module level.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 12489, "title": "error: token \"\"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\" is not valid in preprocessor expressions", "body": " bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nLinux Ubuntu 16.04\r\nBazel version (if compiling from source):0.5.2\r\nCUDA/cuDNN version:9.0/7.0\r\nGPU model and memory:GTX 965m\r\n\r\nERROR: /home/zhouzl/tensorflow-master/tensorflow/contrib/seq2seq/BUILD:51:1: error while parsing .d file: /home/zhouzl/.cache/bazel/_bazel_zhouzl/418dff85d676df3fe7a9d3de8f68c1df/execroot/org_tensorflow/bazel-out/local_linux-opt/bin/tensorflow/contrib/seq2seq/_objs/python/ops/_beam_search_ops_gpu/tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.pic.d (No such file or directory).\r\nIn file included from /usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/common_functions.h:50:0,\r\n                 from /usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/cuda_runtime.h:115,\r\n                 from <command-line>:0:\r\n/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token \"\"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\" is not valid in preprocessor expressions\r\n #define __CUDACC_VER__ \"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\r\n                        ^\r\n/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token \"\"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\" is not valid in preprocessor expressions\r\n #define __CUDACC_VER__ \"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\r\n                        ^\r\n/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token \"\"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\" is not valid in preprocessor expressions\r\n #define __CUDACC_VER__ \"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\r\n                        ^\r\n/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token \"\"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\" is not valid in preprocessor expressions\r\n #define __CUDACC_VER__ \"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\r\n                        ^\r\n/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token \"\"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\" is not valid in preprocessor expressions\r\n #define __CUDACC_VER__ \"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\r\n                        ^\r\n/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token \"\"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\" is not valid in preprocessor expressions\r\n #define __CUDACC_VER__ \"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\r\n                        ^\r\n/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token \"\"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\" is not valid in preprocessor expressions\r\n #define __CUDACC_VER__ \"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\r\n                        ^\r\n/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token \"\"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\" is not valid in preprocessor expressions\r\n #define __CUDACC_VER__ \"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\r\n                        ^\r\n/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token \"\"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\" is not valid in preprocessor expressions\r\n #define __CUDACC_VER__ \"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\r\n\r\n", "comments": ["Now\uff0cThe error has changed\r\n\r\nERROR: /home/zhouzl/.cache/bazel/_bazel_zhouzl/418dff85d676df3fe7a9d3de8f68c1df/external/nccl_archive/BUILD:33:1: output 'external/nccl_archive/_objs/nccl/external/nccl_archive/src/broadcast.cu.pic.o' was not created.\r\nERROR: /home/zhouzl/.cache/bazel/_bazel_zhouzl/418dff85d676df3fe7a9d3de8f68c1df/external/nccl_archive/BUILD:33:1: not all outputs were created or valid.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n", "At global scope:\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nINFO: From Compiling external/nccl_archive/src/reduce_scatter.cu.cc:\r\nexternal/nccl_archive/src/common_kernel.h(42): error: class \"__half\" has no member \"x\"\r\n\r\nexternal/nccl_archive/src/common_kernel.h(42): error: class \"__half\" has no member \"x\"\r\n\r\nexternal/nccl_archive/src/common_kernel.h(55): error: class \"__half\" has no member \"x\"\r\n\r\nexternal/nccl_archive/src/common_kernel.h(55): error: class \"__half\" has no member \"x\"\r\n\r\nexternal/nccl_archive/src/copy_kernel.h(28): error: class \"__half\" has no member \"x\"\r\n\r\nexternal/nccl_archive/src/copy_kernel.h(28): error: class \"__half\" has no member \"x\"\r\n\r\n6 errors detected in the compilation of \"/tmp/tmpxft_00004436_00000000-6_reduce_scatter.cu.cpp1.ii\".\r\nERROR: /home/zhouzl/.cache/bazel/_bazel_zhouzl/418dff85d676df3fe7a9d3de8f68c1df/external/nccl_archive/BUILD:33:1: output 'external/nccl_archive/_objs/nccl/external/nccl_archive/src/reduce_scatter.cu.pic.o' was not created.\r\nERROR: /home/zhouzl/.cache/bazel/_bazel_zhouzl/418dff85d676df3fe7a9d3de8f68c1df/external/nccl_archive/BUILD:33:1: not all outputs were created or valid.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n", "@cwhipkey @martinwicke Looks like a NCCL build issue...", "It looks like for cuda 9 support we need to update NCCL version to get this commit:\r\n  https://github.com/NVIDIA/nccl/commit/29a1a916dc14bb2c00feed3d4820d51fa85be1e6\r\n\r\nI think you can try that with cuda 9 by changing the workspace.bzl file:\r\nhttps://github.com/tensorflow/tensorflow/blob/084d29e67a72e369958c18ae6abfe2752fcddcbf/tensorflow/workspace.bzl#L618\r\n\r\nand update the url, sha256, and strip_prefix.  (url and strip prefix should refer to the new commit, sha256 is the hash of the downloaded archive)", "One also needs to update Eigen that supports cuda 9 for the original problem mentioned in this issue.\r\nTry this in workspace.bzl file-\r\n`   \r\n          \"http://bazel-mirror.storage.googleapis.com/bitbucket.org/ncluehr/eigen/get/6cc72014d888.tar.gz\",\r\n          \"https://bitbucket.org/ncluehr/eigen/get/6cc72014d888.tar.gz\",          \r\n       ],\r\n      sha256 = \"549fc01af565d185123016c12baf997eeb462b088c1885a4c3dfaf2a5c82d677\",\r\n      strip_prefix = \"ncluehr-eigen-6cc72014d888\",`", "hi\uff0ci met the same problems when trying to install tensorflow from source code on\r\nLinux Ubuntu 16.04\r\nBazel version :0.5.2\r\nCUDA/cuDNN version:9.0/5.0\r\nI fixed the problem on \"external/nccl_archive/src/copy_kernel.h(28): error: class \"__half\" has no member \"x\"\", but encountered another as follow:\r\n\r\n/usr/local/cuda-9.0/bin/..//include/crt/common_functions.h:64:24: error: token \"\"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\" is not valid in preprocessor expressions\r\n #define __CUDACC_VER__ \"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\r\n\r\nI did some research on internet and found some solutions for this problem on Boost. But things are different on tensorflow, Could somebody help me with this? Thx!!!!!!!\r\n", "@pangzhan27 , for this issue, you will have to update the NCCL and Eigen versions being downloaded by tensorflow's build. The versions are mentioned in workspace.bzl. But if you are using tensorflow 1.4, then it already has updated versions that work with CUDA-9. If not, please refer workspace.bzl from tensorflow's v1.4.0 tag and copy the Eigen and NCCL versions and corresponding sha256 values. It should work.", "@npanpaliya  Thank you for your advice. The problem has been solved. It seems like that cuda9.0 only support tensorflow r1.4 by now. I used to install tensorflow r1.3 from source code but failed. When i change the version to r1.4, it works out.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 12488, "title": "extract common function for start abort rendezvous", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "Jenkins, test this please", "Looks like this breaks the tests. Mind taking a look?", "@jhseu I'm sorry, I have fixed it. ", "@tensorflow-jenkins test this please."]}, {"number": 12487, "title": "Make scatter_* kernels to be multiple thread and gain 8x speedup.", "body": "Relate to [12358](https://github.com/tensorflow/tensorflow/issues/12358).\r\nUse the lock to deal with the duplicate. After some test, the results show the atomic_flag and spin lock are the best choice and the performance is close  to the lock-free mode.\r\nBenchmark with tfprof, the result shows about 8x speedup.\r\n### Env\r\n- **CPU**\r\n32-cores\r\n- **MEM**\r\n126G\r\n### Parameters\r\n- ref: 10000000*100\r\n- indices: 128000\r\n- updates: 128000*100\r\n```\r\n# orignial\r\nScatterSub                    4400.00MB (65.69%, 6.66%),        39.50ms (43.19%, 9.00%),            0us (47.10%, 0.00%),        39.50ms (43.14%, 9.11%)\r\n# lock-free\r\nScatterSub                    4400.00MB (65.69%, 6.66%),         3.84ms (50.64%, 2.20%),            0us (46.37%, 0.00%),         3.84ms (50.76%, 2.27%)\r\n# with lock\r\nScatterSub                    4400.00MB (65.69%, 6.66%),         4.70ms (50.06%, 2.81%),            0us (45.62%, 0.00%),         4.70ms (50.19%, 2.90%)\r\n```", "comments": ["Can one of the admins verify this patch?", "@ekelsen code updated.", "The code is much cleaner, thank you .  I think you could collapse the two loop bodies into one and pass the assignment function in.  Then no code is duplicated.  What do you think?", "The strategy of string assignment is memcpy which is different with the other type. if collapse the two loop bodies into one, there shuold be a judgement inside the loop. maybe it's no so good.", "If you pass the assignment op into the function there wouldn't be a conditional inside the loop.", "@ekelsen done, thanks.", "@ekelsen Could you review the code if you have time?", "@ekelsen any luck with this?", "LGTM", "Thanks! Jenkins, test this please.", "Here's the error:\r\n```\r\nFAIL: //tensorflow/python/kernel_tests:embedding_ops_test (shard 16 of 20) (see /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/testlogs/tensorflow/python/kernel_tests/embedding_ops_test/shard_16_of_20/test.log).\r\nINFO: From Testing //tensorflow/python/kernel_tests:embedding_ops_test (shard 16 of 20):\r\n==================== Test output for //tensorflow/python/kernel_tests:embedding_ops_test (shard 16 of 20):\r\n2017-09-18 20:25:27.451882: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n.F\r\n======================================================================\r\nFAIL: testRandom (__main__.ScatterAddSubTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/embedding_ops_test.py\", line 103, in testRandom\r\n    self._TestCase(_AsLong(list(shape)), list(indices))\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/embedding_ops_test.py\", line 86, in _TestCase\r\n    self.assertTrue(all((p_init == result).ravel()))\r\nAssertionError: False is not true\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.715s\r\n\r\nFAILED (failures=1)\r\n================================================================================\r\nPASS: //tensorflow/python/kernel_tests:embedding_ops_test (shard 14 of 20)\r\nPASS: //tensorflow/python/kernel_tests:gradient_correctness_test\r\nPASS: //tensorflow/python/kernel_tests:embedding_ops_test (shard 15 of 20)\r\nPASS: //tensorflow/python/kernel_tests:identity_op_py_test\r\nPASS: //tensorflow/python/kernel_tests:identity_n_op_py_test\r\nPASS: //tensorflow/python/kernel_tests:in_topk_op_test\r\nPASS: //tensorflow/python/kernel_tests:io_ops_test\r\nFAIL: //tensorflow/python/kernel_tests:embedding_ops_test (shard 17 of 20) (see /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/testlogs/tensorflow/python/kernel_tests/embedding_ops_test/shard_17_of_20/test.log).\r\nINFO: From Testing //tensorflow/python/kernel_tests:embedding_ops_test (shard 17 of 20):\r\n==================== Test output for //tensorflow/python/kernel_tests:embedding_ops_test (shard 17 of 20):\r\n2017-09-18 20:25:27.213108: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n.F\r\n======================================================================\r\nFAIL: testSubRandom (__main__.ScatterAddSubTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/embedding_ops_test.py\", line 110, in testSubRandom\r\n    self._TestCase(_AsLong(list(shape)), list(indices), state_ops.scatter_sub)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/embedding_ops_test.py\", line 86, in _TestCase\r\n    self.assertTrue(all((p_init == result).ravel()))\r\nAssertionError: False is not true\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 2.722s\r\n```\r\n\r\nNote that it doesn't fail on all shards.", "After debug, it turns out the float-point precision problem. multi thread leads to different calculation order which generate different-precision result.\r\nThe [stackoverflow question](https://stackoverflow.com/questions/7365790/how-can-floating-point-calculations-be-made-deterministic) explain that.\r\nShould we take care of the calculation order to make sure the precision same all the time?", "The documentation for the op does not guarantee ordering, but I would prefer the op to be deterministic if it can be done without much performance hit and memory overhead.", "The extra calculation and memory usage is determined by the size of indices. Maybe it's not good to make the op deterministic.", "@tensorflow-jenkins test this please", "Use the assertAllClose with 1e-5 precision to check the result.", "Jenkins, test this please.", "Minor point: there is no need to set `rtol=0` AFAIK, I think it's a bit more idiomatic to omit that.", "MacOS CPU Tests ERROR:\r\n```\r\n./tensorflow/core/kernels/scatter_functor.h:160:22: error: expected ';' at end of declaration\r\n    Index result = -1 GUARDED_BY(mu);\r\n```\r\nwhat's going on?", "Jenkins, test this please.", "Timeout in `embedding_ops_test` on Windows, trying again.\r\n\r\nJenkins, test this please.", "@drpngx Could you paste the error log about CPU test ?", "It's in `//tensorflow/cc/gradients:nn_grad_test`:\r\n```\r\n[RUN      ] NNGradTest.MaxPoolGradV2Helper\r\ntensorflow/cc/gradients/nn_grad_test.cc:39: Failure\r\nExpected: (max_error) < (2e-4), actual: 0.446481 vs 0.0002\r\n[  FAILED  ] NNGradTest.MaxPoolGradV2Helper (3 ms)\r\n```", "@drpngx  I think that's a pre-existing flake. ", "@tensorflow-jenkins test this please.", "7 tests failed. Look for\r\n\r\n```\r\n19:25:01 292/297 Test #105: C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/kernel_tests/embedding_ops_test.py ....................................***Timeout 600.28 sec\r\n```\r\nin the [log](https://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/4452/consoleFull).", "What's the difference between windows and linux environment?", "Jenkins, test this please.", "@nolanliou can you look at the windows tests", "Actually, it looks like a lot of timeouts? If it's not due some flakiness, then it means that this op slows down significantly.", "The linux CPU tests is all passed while the windows is timeout. I have changed the atomic_flag to mutex, please have another try.", "Jenkins, test this please.", "@nolanliou can you rebase and check if the test failures are fixed, if not take a look at them. That would help us move the merge forward. Thanks for the help.", "Rebase done. there are some problems in windows test, but I don't have accessable windows environment to do the test. ", "Jenkins, test this please.", "Jenkins, test this please.\r\n", "Jenkins, test this please\r\n", "Looks like the windows issues are directly related to this change.\r\nEspecially the `190 - C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/kernel_tests/scatter_ops_test.py` failure. Looks like the test times out with this change.\r\n\r\nMaybe a deadlock in windows?", "Yes, there is something wrong in windows, I'll fix it when have time.", "@nolanliou any update? I'll mark stalled for now.", "@nolanliou this is almost ready to go. Any chance to fix the tests?", "Sorry, I don't have time to fix the bug recently.", "No worries, ping us on that thread when it's ready to go. Thanks!", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@nolanliou any chance you might have time to finish this up? It would be nice to get such a speedup in. Thanks.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Closing this out because it's been stale for a while. Please reopen with the issues addressed if you'd like to merge this in. Thanks!"]}, {"number": 12485, "title": "Update multinomial.py", "body": "The previous calculation of log_prob is unstable if the probs of Multinomial are close to zeros, the math_ops.log will result in nan values.", "comments": ["Can one of the admins verify this patch?", "@YiMX, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jvdillon, @tensorflower-gardener and @yifeif to be potential reviewers.", "Hi, after some testing, I found that log_prob computed from math_ops.reduce_logsumexp with logits can be stable and counts is trainable in this case.", "@YiMX did you find a helper function in `distribution_utils`?", "@drpngx I did not find any helper functions in ```distribution_utils```.  This issue should be same as you using categorical, ```categorical.py``` used some helper functions like ```embed_check_integer_casting_closed```, but it solves the problem via sparse_softmax_cross_entropy_with_logits", "@ebrevdo ping", "Hi folks. It looks like this issue was already spotted and we have a fix about to go in.  Our fix uses tf.nn.log_softmax which is slightly more efficient than this PR.  Again, my apologies for not intervening sooner (I was only just made aware of this PR).", "@jvdillon is the other fix merged? Should we close this PR?", "@gunan  It was merged, I will close this PR."]}, {"number": 12484, "title": "Modification and Uniformisation of setup.py files", "body": "Hello everyone,\r\n\r\nAs you may find on PyPI: <https://pypi.python.org/pypi/tensorflow/json>, tensorflow is not reported as a package which is compatible with python 3. Which doesn't prevent to be able to install it with pip properly. it just mess up any API call to the PyPI API (which is not good for all our friends using CI tools).\r\n\r\nThe reason is actually quite simple: *some classifiers were missing*.\r\n\r\n```python\r\n############ Actual Classifiers as declared on PyPI for TF 1.3.0 ############ \r\n\"classifiers\": [\r\n      \"Development Status :: 4 - Beta\",\r\n      \"Intended Audience :: Developers\",\r\n      \"Intended Audience :: Education\",\r\n      \"Intended Audience :: Science/Research\",\r\n      \"License :: OSI Approved :: Apache Software License\",\r\n      \"Programming Language :: Python :: 2.7\",  #### ONLY PYTHON 2.7 is flagged\r\n      \"Topic :: Scientific/Engineering :: Mathematics\",\r\n      \"Topic :: Software Development :: Libraries\",\r\n      \"Topic :: Software Development :: Libraries :: Python Modules\"\r\n    ]\r\n\r\n####### The proposed changes to uniformised the two setup.py and allow Py3 compatibility #######\r\nclassifiers=[\r\n       \"Development Status :: 4 - Beta\",\r\n        \r\n        'Intended Audience :: Developers',\r\n        'Intended Audience :: Education',\r\n        'Intended Audience :: Science/Research',\r\n        \r\n        'License :: OSI Approved :: Apache Software License',\r\n        \r\n        'Programming Language :: Python :: 2',\r\n        'Programming Language :: Python :: 2.7',\r\n        'Programming Language :: Python :: 3',\r\n        'Programming Language :: Python :: 3.6',\r\n        \r\n        'Topic :: Scientific/Engineering',\r\n        'Topic :: Scientific/Engineering :: Mathematics',\r\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\r\n        'Topic :: Software Development',\r\n        'Topic :: Software Development :: Libraries',  \r\n        'Topic :: Software Development :: Libraries :: Python Modules',\r\n    ],\r\n```\r\n\r\nSo the main objective of this PR is basically to fix the aforesaid issues in the two setup.py i was able to find.\r\n\r\nI also took the opportunity to uniformise the two setup.py files. They now declare the same audiences and topics, I hope it will be fine. Else, I can still revert the changes and just keep the changes related to Python 3 compatibility.\r\n\r\nPlease have a great day,\r\n\r\nBest Regards,\r\n\r\nJonathan DEKHTIAR", "comments": ["Can one of the admins verify this patch?", "@DEKHTIARJonathan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @vrv to be potential reviewers.", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 12483, "title": "Fix serveral typos in docs.", "body": "Signed-off-by: Ti Zhou <tizhou1986@gmail.com>\r\n\r\nFix serveral typos in docs.\r\n\r\nIncluding:\r\ntensorflow/docs_src/programmers_guide/threading_and_queues.md\r\ntensorflow/docs_src/api_guides/python/reading_data.md", "comments": ["Can one of the admins verify this patch?", "@tizhou86, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @mrry and @tensorflower-gardener to be potential reviewers."]}, {"number": 12482, "title": "Failure to build on OS/X", "body": "I have the following error when building on OS/X:\r\n\r\n```\r\n./tensorflow/core/platform/default/mutex.h:25:10: fatal error: 'nsync_cv.h' file not found\r\n#include \"nsync_cv.h\"\r\n         ^\r\n1 error generated.\r\n```\r\n\r\nEarlier on I get the following warning:\r\n```\r\nWARNING: /Users/davidn/workspace/tensorflowview/tensorflow/tensorflow/core/BUILD:1632:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /Users/davidn/workspace/tensorflowview/tensorflow/tensorflow/core/BUILD:1632:1.\r\n```\r\n", "comments": ["was my issue.  a dependency was introduced elsewhere to 'nsync' that was not covered by my existing dependencies.  needed to add it.\r\n", "Hi David,\r\n\r\nCould you describe how to resolve the problem? I encounter the same one. Thanks.", "Hi @DavidNorman,\r\n\r\nI got the same mistake. How did you solve the issue? Thanks in advance.\r\n\r\nBazel installed from: Source\r\nBazel version: 0.5.1\r\nTensorFlow installed from: Source\r\nTensorflow version: 1.3\r\nuname-a: Linux raspberrypi 4.9.41-v7+ SMP Tue Aug 8 16:00:15 BST 2017 armv7l GNU/Linux\r\nCUDA/cuDNN version: N/A", "Same issue for me as well. Any solution?", "Hi,\r\nI came up with a solution. Taking into account that I was able to compile everything from source on different machines, I just changed the file mutex.h for the following code:\r\n\r\n```\r\n/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an \"AS IS\" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n==============================================================================*/\r\n\r\n#ifndef TENSORFLOW_PLATFORM_DEFAULT_MUTEX_H_\r\n#define TENSORFLOW_PLATFORM_DEFAULT_MUTEX_H_\r\n\r\n// IWYU pragma: private, include \"third_party/tensorflow/core/platform/mutex.h\"\r\n// IWYU pragma: friend third_party/tensorflow/core/platform/mutex.h\r\n\r\n#include <chrono>\r\n#include <condition_variable>\r\n#include <mutex>\r\n#include \"tensorflow/core/platform/thread_annotations.h\"\r\nnamespace tensorflow {\r\n\r\n#undef mutex_lock\r\n\r\nenum LinkerInitialized { LINKER_INITIALIZED };\r\n\r\n// A class that wraps around the std::mutex implementation, only adding an\r\n// additional LinkerInitialized constructor interface.\r\nclass LOCKABLE mutex : public std::mutex {\r\n public:\r\n  mutex() {}\r\n  // The default implementation of std::mutex is safe to use after the linker\r\n  // initializations\r\n  explicit mutex(LinkerInitialized x) {}\r\n\r\n  void lock() ACQUIRE() { std::mutex::lock(); }\r\n  bool try_lock() EXCLUSIVE_TRYLOCK_FUNCTION(true) {\r\n    return std::mutex::try_lock();\r\n  };\r\n  void unlock() RELEASE() { std::mutex::unlock(); }\r\n};\r\n\r\nclass SCOPED_LOCKABLE mutex_lock : public std::unique_lock<std::mutex> {\r\n public:\r\n  mutex_lock(class mutex& m) ACQUIRE(m) : std::unique_lock<std::mutex>(m) {}\r\n  mutex_lock(class mutex& m, std::try_to_lock_t t) ACQUIRE(m)\r\n      : std::unique_lock<std::mutex>(m, t) {}\r\n  mutex_lock(mutex_lock&& ml) noexcept\r\n      : std::unique_lock<std::mutex>(std::move(ml)) {}\r\n  ~mutex_lock() RELEASE() {}\r\n};\r\n\r\n// Catch bug where variable name is omitted, e.g. mutex_lock (mu);\r\n#define mutex_lock(x) static_assert(0, \"mutex_lock_decl_missing_var_name\");\r\n\r\nusing std::condition_variable;\r\n\r\ninline ConditionResult WaitForMilliseconds(mutex_lock* mu,\r\n                                           condition_variable* cv, int64 ms) {\r\n  std::cv_status s = cv->wait_for(*mu, std::chrono::milliseconds(ms));\r\n  return (s == std::cv_status::timeout) ? kCond_Timeout : kCond_MaybeNotified;\r\n}\r\n\r\n}  // namespace tensorflow\r\n\r\n#endif  // TENSORFLOW_PLATFORM_DEFAULT_MUTEX_H_\r\n```\r\n\r\nCan you edit your post and show specs of machine, tensorflow version & bazel. Thanks (Just to detect where can be the mistake).", "Hi @alc1218,\r\n\r\nYour file works. Thank you very much!", "This file has changed in the latest code base. May be updating will help? Essentially thats what is done by alc1218 above - replace old file with new one. ", "@alc1218's solution seems to work. Anyways, here are the details of my machine.\r\n\r\nMachine: macOS Sierra 10.12.6\r\nBazel: 0.5.3 with Homebrew\r\nTensorflow: 1.3 from source using \r\n`git clone --recursive https://github.com/tensorflow/tensorflow`", "same issue on linux centos:\r\ntensorflow/tensorflow/core/platform/default/mutex.h:25:22: fatal error: nsync_cv.h: No such file or directory", "+1, same issue when compiled custom reader op, as below:\r\n\r\n```#make\r\nScanning dependencies of target binary_parser_op\r\n[ 50%] Building CXX object CMakeFiles/binary_parser_op.dir/cc/dense_parser_op.cc.o\r\nIn file included from /usr/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/platform/mutex.h:31:0,\r\n                 from /usr/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/op.h:32,\r\n                 from /home/admin/src/dolphin/dolphin-yarn-parent/dolphin-tensorflow-extends/tf_parser/cc/dense_parser_op.cc:1:\r\n/usr/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/platform/default/mutex.h:25:22: fatal error: nsync_cv.h: No such file or directory\r\n #include \"nsync_cv.h\"```", "I changed the file `tensorflow/core/platform/default/mutex.h` by adding the relative path as prefix of `nsync_cv.h` and `nsync_mu.h` to fix the issue, as follows:\r\n\r\n```\r\n#include \"external/nsync/public/nsync_cv.h\"\r\n#include \"external/nsync/public/nsync_mu.h\"\r\n```", "copying `nsync*.h` from `dist-packages/external/nsync/public` (or `https://github.com/google/nsync/tree/master/public`) to `/usr/include` also does the trick without modifying tensorflow files.", "@adelmanm method sounds reasonable. However I changed our build system to clone the github and include the directories instead of messing up the system includes. I hope tensorflow includes this as a thirdparty dependency in future release.", "@DavidNorman Could you elaborate more on what you did to fix the issue? I'm having the same issues, and the resolutions here won't work for me because the libraries are compiled with the `nsync` reference as-is.", "Had the same issue:\r\nsearch for nsync_cv.h file using:\r\n`sudo find / -name nsync_cv.h`\r\nthen add it in the file `mutex.h`\r\n`#include \"tensorflow/contrib/makefile/downloads/nsync/public/nsync_cv.h\"`\r\n`#include \"tensorflow/contrib/makefile/downloads/nsync/public/nsync_mu.h\"`\r\n\r\nIf you get linker error after above step try this: https://github.com/tensorflow/tensorflow/issues/12904#issuecomment-328234880", "Had the same issue, and when I try to follow the instructions given above, I come across another problem..\r\n:undefined reference to 'nsync::nsync_mu_lock(nsync::nsync_mu_s_*)'\r\n:undefined reference to 'nsync::nsync_mu_init(nsync::nsync_mu_s_*)'\r\n:undefined reference to 'nsync::nsync_mu_unlock(nsync::nsync_mu_s_*)'", "the undefined reference is probably because $HOST_NSYNC_LIB and $TARGET_NSYNC_LIB are unset in your env. \r\nYou could try this patch assuming you are building for iOS\r\n\r\ndiff --git a/tensorflow/contrib/makefile/Makefile b/tensorflow/contrib/makefile/Makefile\r\nindex 525cf2cd4..e4893dde2 100644\r\n--- a/tensorflow/contrib/makefile/Makefile\r\n+++ b/tensorflow/contrib/makefile/Makefile\r\n@@ -86,7 +86,7 @@ endif\r\nHOST_INCLUDES += -I/usr/local/include\r\n\r\nHOST_LIBS := \r\n-$(HOST_NSYNC_LIB) \r\n+$(MAKEFILE_DIR)/downloads/nsync/builds/default.macos.c++11/nsync.a \r\n-lstdc++ \r\n-lprotobuf \r\n-lpthread \r\n@@ -169,7 +169,7 @@ endif\r\nINCLUDES += -I/usr/local/include\r\n\r\nLIBS := \r\n-$(TARGET_NSYNC_LIB) \r\n+$(MAKEFILE_DIR)/downloads/nsync/builds/lipo.ios.c++11/nsync.a \r\n-lstdc++ \r\n-lprotobuf \r\n-lz \\\r\n\r\n", "I was able to fix the issue by adding `/usr/local/lib/python3.6/site-packages/tensorflow/include/external/nsync/public` to my header search path (TF v1.4)", "I feel as though this issue should be re-opened, because a fresh clone of master (as of b20ec5c461031f9375274cf026a7dfff0f903acc) results in the Aforementioned NSync compilation bug when trying to integrate libtensorflow into other apps - the only working solution was to use @alc1218 's variant of mutex.h - which is a breaking change to the codebase. \r\n\r\nThank you."]}, {"number": 12481, "title": "add lvalue ref for auto deduction, and avoid to copy object.", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please."]}, {"number": 12480, "title": "Use Tensorflow in conjunction with PyTorch/Theano", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 0.12.1\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: 5.1\r\n- **GPU model and memory**: 1080-Ti\r\n- **Exact command to reproduce**: None\r\n\r\n### Describe the problem\r\nI'm trying to use Tensorflow in conjunction with PyTorch (I built the model in Tensorflow to generate vector representations and PyTorch trains on top of those). However, the problem is that PyTorch runs out of memory because TF will replicate the model in ALL available CUDA devices. In this case CUDA_VISIBLE_DEVICES is not helpful, and I tried GPU device tf.device(\"/gpu:0\") but Tensorflow still fills up all GPUs' memory.\r\n\r\nIs there some way to actually limit Tensorflow's GPU usage to one and free up the other for other DL libraries like PyTorch or Theano?", "comments": ["You can try to allocate a `fraction` of memory like:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/08ed32dbb9e8f67eec9efce3807b5bdb3933eb2f/tensorflow/core/protobuf/config.proto#L21", "Something like:\r\n\r\n```python\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.25)\r\n\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n```", "This has been [answered on Stack Overflow](https://stackoverflow.com/q/34199233/3574081).", "Thank you. Sorry about the duplication. "]}, {"number": 12479, "title": "find worker session by handle for worker", "body": "extract common template method to find worker session by handle, and remove duplicated implements.", "comments": ["@horance-liu, thanks for your PR! By analyzing the history of the files in this pull request, we identified @suharshs, @mrry and @saeta to be potential reviewers.", "Can one of the admins verify this patch?", "@horance-liu did you address all comments?", "@mrry ping for review", "@mrry can you check the changes please?", "@martinwicke My [comment from 8/27](https://github.com/tensorflow/tensorflow/pull/12479#discussion_r135431498) is still unresolved.", "Closing as stalled."]}, {"number": 12478, "title": "Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)", "body": "INFO:tensorflow:Creating bottleneck at /tmp/bottleneck\\dandelion\\10043234166_e6dd915111_n.jpg_inception_v3.txt\r\n2017-08-22 12:36:45.710892: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\framework\\op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\r\n2017-08-22 12:36:46.351813: E c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:359] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2017-08-22 12:36:46.351943: E c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:366] error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2017-08-22 12:36:46.354865: E c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:326] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-08-22 12:36:46.356828: F c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\kernels\\conv_ops.cc:671] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "I had tried to run the label_image.py in terminal but the issue was solved after I updated to Tensorflow 1.3", "Good to hear the issue is resolved. I'll close for now.", "When I use the keras==1.2.0, I have got the same problem. Fortunately,  the issue was solved after I upgrade the tensorflow from 1.2.0 to 1.3.0 ."]}, {"number": 12477, "title": "replace loop with ranged-for from worker's get_status", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 12476, "title": "replace explicit type decl with auto deduction for device manager", "body": "", "comments": ["@horance-liu, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @saeta to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for the pull request, though!"]}, {"number": 12474, "title": "CUDA 9RC + cuDNN7", "body": "Things have moved forward.  I strongly suggest building from head with CUDA 9 and cuDNN 7.  All of the necessary codes **should** be in the TF 1.4 tag but given we are still working on new features for FP16, I would build from head if that is of interest.  I do not like to share anything I have not personally tested as I know how frustrating trying to get things to compile can be.\r\n\r\n\r\n**Everything below this line is OUT DATED as of 19-OCT**\r\n\r\nThis is an **unofficial** and **very** not supported patch to make it possible to compile TensorFlow with CUDA9RC and cuDNN 7 or CUDA8 + cuDNN 7.\r\n\r\nDuring testing on V100 (Volta) and ResNet-50 FP32  using CUDA 9RC + cuDNN 7 was significantly faster than CUDA 8 + cuDNN 6, which was not a surprise.  I am about to test on P100s.  I am sharing this patch informally so those that are interested can play with cuDNN 7 as well as CUDA 9RC before we have the official release.  As we have more interesting code, e.g FP16 models, I will share it in this issue.   I expect NVIDIA will start to submit official cuDNN 7 patches very soon.  \r\n\r\n**Note:**  This patch may work on more recent versions of TensorFlow but it will likely bit rot so keep that in mind.  Apply the cuDNN 7 patch and then fast-forwarding the branch might be the best approach.  My git skills are not strong so do what you think is best.  \r\n\r\n1. Download the patches\r\n- [0001-CUDA-9.0-and-cuDNN-7.0-support.patch](https://storage.googleapis.com/tf-performance/public/cuda9rc_patch/0001-CUDA-9.0-and-cuDNN-7.0-support.patch)\r\n- [eigen.f3a22f35b044.cuda9.diff](https://storage.googleapis.com/tf-performance/public/cuda9rc_patch/eigen.f3a22f35b044.cuda9.diff)\r\n\r\n2. Clone the tensorflow repo\r\n```https://github.com/tensorflow/tensorflow.git```\r\n\r\n3. Checkout the revision that the TensorFlow patch can apply to:\r\n```git checkout db596594b5653b43fcb558a4753b39904bb62cbd~```\r\n\r\n4. Apply the TensorFlow patch:\r\n```git apply ~/Downloads/0001-CUDA-9.0-and-cuDNN-7.0-support.patch```\r\n\r\n5. Run ./configure. When it asks for the CUDA version, put 9.0 (or 8). When it asks for the cudnn version, put 7.0.1 (Entering '7' worked fine for me). Make sure you put the paths to the right Cuda and cudnn versions and have your ldconfig or LD_LIBARY_PATH set to point to the CUDA 9 folder.\r\n```./configure```\r\n\r\n6. Attempt to build TensorFlow, so that Eigen is downloaded. **This build will fail if building for CUDA9RC but will succeed for CUDA8**\r\n ```bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package```\r\n\r\n7. Apply the Eigen patch:\r\n```bash\r\n    cd -P bazel-out/../../../external/eigen_archive\r\n    patch -p1 < ~/Downloads/eigen.f3a22f35b044.cuda9.diff\r\n```\r\n\r\n8. Build TensorFlow successfully\r\n```bash\r\n    cd -\r\n    bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nI have run this process myself on Ubuntu 14.04 with Python 2.7.\r\n\r\nThank for NVIDIA for the early patch and @reedwm who created most of these instructions.  \r\n\r\nIf you are using Python 2.7 and gcc 4.8+ here is a [.whl](https://storage.googleapis.com/tf-performance/tf_binary/gcc4_8/tensorflow-1.4.HEAD.b9ac2d7eb.AUG_1.CUDA9rc-cp27-none-linux_x86_64.whl) where I followed the instructions above and [one with CUDA 8 and cuDNN 7](https://storage.googleapis.com/tf-performance/tf_binary/gcc4_8/tensorflow-1.4.HEAD.b9ac2d7eb.AUG_1.CUDA8_cuDNN7-cp27-none-linux_x86_64.whl) which I have yet to test.  Stress again, this was created by me and not the TensorFlow build team.  My/our goal is to engage with anyone that wants to try this out and try to have a little fun.  :-)\r\n", "comments": ["Thanks Toby. Looking forward to the performance numbers on P100. ", "Quick Question (sorry if it's not the right place to ask, and if I'm missing something really obvious on NVIDIA's Website): Do CUDA 9 & cuDNN 7 provide better performance on non-Volta (Pascal) GPUs? Are these noticeable when training Neural Nets?", "In my testing cuDNN 7 (with CUDA 8) it does seem to have some improved\nKernels. cuDNN 6 did not seem to have much of an impact  I saw a speed up\nin Single GPU performance for VGG16 and ResNet when testing on P100s.  I\nhave not tested on K80s.  We are still working through any minor issues\nwith TensorFlow.  The upgrades are not always plug and play.  This is a\nfine place to ask, I just did not want answer compile questions for\nplatforms I am not using for this very unofficial patch.  :-)\n\nOn Thu, Aug 24, 2017 at 12:43 AM, Tanmay Bakshi <notifications@github.com>\nwrote:\n\n> Quick Question (sorry if it's not the right place to ask, and if I'm\n> missing something really obvious on NVIDIA's Website): Do CUDA 9 & cuDNN 7\n> provide better performance on non-Volta (Pascal) GPUs? Are these noticeable\n> when training Neural Nets?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12474#issuecomment-324558781>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZesiHeQSoFmVaTYQEy_lHu3vbmUAXDks5sbSmPgaJpZM4O-GMz>\n> .\n>\n", "I'm seeing a bit of a different result. With CUDA 8 & cuDNN 6, training a simple but deep Keras CNN model took me 225 seconds, but with CUDA 9 & cuDNN 7, it took 241 seconds (on a P100).", "My test was CUDA 8 + cuDNN 7 and CUDA 8 + cuDNN 6.  For CUDA 9RC + cuDNN 7, testing not apples to apples as I was using a slightly different versions of TF than I did for CUDA 8 + cuDNN 7 but I think likely perform the same, compared to CUDA 8 + cuDNN 7 Resnet-50 was about the same and I saw a regression with VGG16  at 8xP100 SMX running replicated nccl.  With 1 GPU I saw some differences but not large enough to be certain.  There are things to sort out", "@terrylewis60 It looks like you are subscribed to the issue.  If you unsubscribe you will not get notifications.  I would also greatly appreciate it if you removed the profanity and childish name calling.  You are free to express yourself, although you are likely outside the community guidelines.  It just seems rather silly to complain about something you subscribed to.  Thank you.", "Any updates on this? While the patch works with the specific tf-commit (also with Python3), the tf version says __version__ is 1.2.1-rc1, so rather old (that version was incompatible with current bazel 0.5.3, so bazel needs to be downgraded in order to build the tf-patch version.) The cudnn 7 changes so far are not included in HEAD. Any possibility for a branch that supports the most recent released upstream software/drivers?", "The commit point is Aug 1st, which is partially post 1.3. The patch works\nwith the 1.3 branch/tag as well.  The PRs are in progress from NVIDIA.\nThe eigen PR for CUDA 9RC is in review by the external eigen team.\n\nOn Aug 26, 2017 1:45 AM, \"Dominik Schl\u00f6sser\" <notifications@github.com>\nwrote:\n\n> Any updates on this? While the patch works with the specific tf-commit\n> (also with Python3), the tf version says *version* is 1.2.1-rc1, so\n> rather old (that version was incompatible with current bazel 0.5.3, so\n> bazel needs to be downgraded in order to build the tf-patch version.) The\n> cudnn 7 changes so far are not included in HEAD. Any possibility for a\n> branch that supports the most recent released upstream software/drivers?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12474#issuecomment-325103656>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZeslZ2gAsAXT2MmYkdgvLNHyoZoeWYks5sb9sjgaJpZM4O-GMz>\n> .\n>\n", "Bazel 5.3 had issues which as silly as it sounds were beyond our control\nand immediate knowledge.  Our build servers do not run latest bazel.  I\nalso like to stay a step behind.  I think I was the first to hit the issue\nhaving accidentally upgraded 1 hour after the release.  I wasted half a\nday, which meant making up that time into the night.  I understand the\nfrustration.\n\nOn Aug 26, 2017 6:22 AM, \"Toby Boyd\" <tobyboyd@google.com> wrote:\n\nThe commit point is Aug 1st, which is partially post 1.3. The patch works\nwith the 1.3 branch/tag as well.  The PRs are in progress from NVIDIA.\nThe eigen PR for CUDA 9RC is in review by the external eigen team.\n\nOn Aug 26, 2017 1:45 AM, \"Dominik Schl\u00f6sser\" <notifications@github.com>\nwrote:\n\n> Any updates on this? While the patch works with the specific tf-commit\n> (also with Python3), the tf version says *version* is 1.2.1-rc1, so\n> rather old (that version was incompatible with current bazel 0.5.3, so\n> bazel needs to be downgraded in order to build the tf-patch version.) The\n> cudnn 7 changes so far are not included in HEAD. Any possibility for a\n> branch that supports the most recent released upstream software/drivers?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12474#issuecomment-325103656>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZeslZ2gAsAXT2MmYkdgvLNHyoZoeWYks5sb9sjgaJpZM4O-GMz>\n> .\n>\n", "The patch almost works with r1.3:  it fails on tensorflow/workspace.bzl:\r\n`error: patch failed: tensorflow/workspace.bzl:664`\r\n`error: tensorflow/workspace.bzl: patch does not apply`\r\n[cmake anyone?  ;-) ]\r\n", "I tested the patch on the 1.3 TAG v1.3.0-rc2 because I was doing\nperformance tests with the various versions.  With the env from the\noriginal post.  The 1.3 final is not that different, you can look at the\nlog but I am fairly sure it was a few small cherry picks.  I am a bit\nsurprised it did not apply.  You can always apply to the TAG and then\nmerge.  If the merging does not get too messy you could merge all the way\nto head.  Then make a branch, share it, and bask in some tasty karma.  :-)\n\nOn Sat, Aug 26, 2017 at 7:11 AM, Dominik Schl\u00f6sser <notifications@github.com\n> wrote:\n\n> The patch almost works with r1.3: it fails on tensorflow/workspace.bzl:\n> error: patch failed: tensorflow/workspace.bzl:664\n> error: tensorflow/workspace.bzl: patch does not apply\n> [cmake anyone? ;-) ]\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12474#issuecomment-325131010>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZeskl8RRNrxl1I77IU4JokByOjQm1lks5scCd7gaJpZM4O-GMz>\n> .\n>\n", "For now I patched the patch so that it works with release r1.3:\r\n[0002-TF-1.3-CUDA-9.0-and-cuDNN-7.0-support.patch.txt](https://github.com/tensorflow/tensorflow/files/1253794/0002-TF-1.3-CUDA-9.0-and-cuDNN-7.0-support.patch.txt)\r\n\r\n(note: the attached patch-file needs to be renamed, the extension `.txt` needs to be removed.)\r\n\r\nThis patch will only work with r1.3.  (workspace.bzl changes almost on a daily basis, which breaks the patch). ", "Big thank you.\n\nOn Aug 26, 2017 9:20 AM, \"Dominik Schl\u00f6sser\" <notifications@github.com>\nwrote:\n\n> For now I patched the patch so that it works with release r1.3:\n> 0002-TF-1.3-CUDA-9.0-and-cuDNN-7.0-support.patch.txt\n> <https://github.com/tensorflow/tensorflow/files/1253794/0002-TF-1.3-CUDA-9.0-and-cuDNN-7.0-support.patch.txt>\n>\n> (note: the attached patch-file needs to be renamed, the extension .txt\n> needs to be removed.)\n>\n> This patch will only work with r1.3. (workspace.bzl changes almost on a\n> daily basis, which breaks the patch).\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12474#issuecomment-325143008>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZeslTuD_Ccy3os6rQHIYeqRHrzqtr8ks5scEXNgaJpZM4O-GMz>\n> .\n>\n", "Could somebody please specify the required bazel version for this patch to work on r1.3?", "I've built with tf r1.3, 0002-patch and bazel 0.5.3 (latest bazel release). The original 0001-patch (first post) did not build with bazel 0.5.3, but required a downgrade to 0.5.2.", "Thank you @domschl !", "Hi @tfboyd, how does the following command work?\r\n``cd -P bazel-out/../../../external/eigen_archive``\r\n\r\nBecause there is no \"external/eigen_archive\" folder in tensorflow repo. There is only \"third_party/eigen3/Eigen\", so this command and your next patch command does not work. \r\n", "You need to try to build tf once. That pulls the external dependencies like eigen3. ", "The real PRs are very close and I think some people have used those to get\nthings working as well.  Just an FYI. When they are all approved I will\nupdate and close this issue.  Dominik is correct you have to build (then\nfail) then do the Eigen step then build again.\n\nOn Wed, Aug 30, 2017 at 12:16 PM, Dominik Schl\u00f6sser <\nnotifications@github.com> wrote:\n\n> You need to try to build tf once. That pulls the external dependencies\n> like eigen3.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12474#issuecomment-326090505>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZespskw2egUJi4PHAJkekDNbyPACEbks5sdbT0gaJpZM4O-GMz>\n> .\n>\n", "Hi @tfboyd, I tried to use your patches, but after the first build step, there is no \"external/eigen_archive\" folder at all. It means the command \"cd -P bazel-out/../../../external/eigen_archive\" failed. So when this folder is generated?", "@hfutxrg  it must be broken now.  It will be in the official build in a few weeks or less.  Step #6 that mentions EIGEN is what creates that folder.  But if it did not work for you that is ok, this was for fun and it worked for a few people.  ", "Hi @tfboyd, how did you benchmark the performance of P100 and V100? I used the benchmarks mentioned in https://www.tensorflow.org/performance/benchmarks. But it seems the performance results are incorrect. With real imagenet dataset, I got 905 images/sec with 1 P100 and 1234 images/sec with 4 P100. But that benchmark website has 219 images/sec with 1 P100 and 852 images/sec. So there are huge difference here. \r\n\r\nMy command to run the benchmark is:\r\n`` export CUDA_VISIBLE_DEVICES=0``\r\n``python tf_cnn_benchmarks.py \\``\r\n   `` --model_name=resnet50 \\``\r\n   `` --batch_size=64 \\``\r\n    ``--num_batches=2000 \\``\r\n   `` --num_gpus=1 \\``\r\n   `` --data_dir=/cm/shared/scratch/database/tensorflow \\``\r\n    ``--data_name=\"imagenet\"``\r\n\r\nIs there anything wrong with my command? Also that benchmark website has similar performance with synthetic data, but if I remove \"--data_dir\", the performance is 12866 images/sec which is incorrect. Any comments and suggestions are appreciated!\r\n\r\nThanks,\r\nRengan", "You are running the trivial model not resnet50.   ```--model=resnet50```  I was totally confused for a second.  I recently retested and has NVIDIA test on TensorFow 1.3 and there numbers are slightly different now on the DGX-1 but no more than 3-4%.  We made a few tweaks to the model as we worked to ensure it converged to match the paper.  Nothing huge just small changes.  You can also use the benchmarks from the[ staging branch](https://github.com/tensorflow/benchmarks/tree/tf_benchmark_stage) but it requires 1.4.   tf_cnn_benchmarks does not worry about backwards compatibility as it is kind of a sandbox, just FYI.  I am working on a better way to publish updated code and accept external PRs more often.  It is more work than it looks.", "Thanks @tfboyd. That's a stupid mistake. After I changed to the correct option, the benchmark works correctly now. \r\nAnother question, how do we run the benchmark with 16-bit floating points (float16)? I didn't see any options to enable that.", "fp16 is not ready just yet.  We still need batch_norm updated to support it and another push from the script.  Give me a few more weeks and I should have everything you need.  I (and we) very much want to share the code with you as soon as it functions even if it has some issues.  It will also be hard to use this patch as it is getting very stale so if all goes well in a few weeks, I hope less, we can have everything (Cuda9RC, cuDNN 7 and fp16 with something to play with) official.\r\n\r\nZero worries on the mistake there are a bunch of flags and it is so easy to make a mistaken.  ", "Hi @tfboyd , in the benchmark code, one way for variable update is called \"independent\". This should never not be used in real training, right? Because in real training, all GPUs have to communicate each other to update the gradient. The page https://www.tensorflow.org/performance/performance_models also didn't mention \"independent\" variable update.", "@hfutxrg Correct independent means graidents are not shared between the towers.  Only useful for testing hardware and no a \"legit\" way to test actual performance.  \r\n\r\nFYI, it looks like cuDNN 7 is mostly merged.  I have not tested it myself yet but just FYI.  Trying to keep the thread updated as I have time.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/12503\r\n\r\nand CUDA 9\r\nhttps://github.com/tensorflow/tensorflow/pull/12502\r\n\r\nThere will be followup PRs and I hope to publish FP16 examples very soon.  There is a review still in progress for batch norm.  \r\n", "Will this be 1.4.0?\r\nestimated release date? please?", "1.4 binaries will most likely be CUDA 8 + cuDNN 6.  I suspect (which is not an estimate) that 1.5 will be built with CUDA 9 + cuDNN 7.  I am testing the build now and working through some issues with CUDA 9 and cuDNN 7.  Maybe I am doing something wrong, I will find out tomorrow.  I really want to have it CUDA 8 and cuDNN 7 building in 1.4 source but if we miss the release train we miss it.  There is no date for 1.4 but I suspect it will RC before the end of October.  It is being gated on finishing some features but again it most likely will not have prebuilt binaries for CUDA 9 and cuDNN 7.  \r\n\r\nFYI. I will close this issue when it builds from source with no \"tricks\".  :-)", "@tfboyd Thanks for your great help for integrating cuda rc 9 + cudnn 7.  So, if I have a V100 machine, I should be able to compile tensorflow from the source codes (1.3 branch) based on your patches above and enjoy the speed up from V100, right? \r\n\r\nBtw, will your patch work with the official cuda 9 (release few days ago) and cudnn 7? Or you will update the patch or you already did that?\r\n\r\nThanks again.\r\n", "@tfboyd\r\n\r\nThere are always errors for me to command **eigen-...-diff** file.\r\n\r\nI can't command the code **cd -P bazel-out/../../../external/eigen_archive** correctly. I tried when terminal was in tensorflow file and tensorflow1.3 file. But it doesn't work. Could you please tell me what\r\n**bazel-out/../../../** means and what the file**/../../../** is? Can I just substitute the exact file of **bazel-out/../../../** ?\r\n\r\nI had tried to use this way: command **cd -P bazel-out/(+the exact position of )external/eigen_archive** and then I execute the patch, the errors just happened at each **@@ -236,7 +236,7@@**(for example, others are just the same form) and the program says **can't find file to patch at input line xx**.\r\n\r\nDo you know why these happen?", "@zhangb223    @chunfuchen \r\n\r\nI suggest taking TensorFlow from head and not using this patch.  I will update the original comment.\r\n\r\nCUDA 9 with no other changes speeds up V100s over CUDA 8 in my testing of ResNet50 FP32.  I hope to share the teams FP16 code and some results but we are still working through a variety of items.  Technial the tf_cnn_benchmarks.py code currently published will do FP16 but I want to make sure it is running great with great convergence (high accuracy that matches or almost matches the FP32, e.e. ~75%) and scaling.  \r\n", "I marked this closed but as always (or mostly) I am happy to answer questions and share status when I have time.  ", "Hello @tfboyd\r\n\r\nI want to offer more information about the first error . When I execute ( cd -P tensorflow1.3/lib/python2.7/.../external/eigen_archive ) I can enter the path. However, if I add bazel-out after -P, the terminal reports: No such file or dictionary. So I want to know the function of bazel-out.\r\n\r\nIs it a path or a command? I can\u2019t find bazel-out in the bazel package.\r\n\r\n\r\nThanks!", "Hi, I am a newbie. I want some help regarding tensorflow installation. \r\nMy system has\r\nOS - Windows 10 \r\nGPU - 940MX\r\nI installed CudaTK 9 and updated driver and CuDNN 7 sucessfully.\r\neven in Anaconda, tensorflow installation is also successful it shown. but still missing \"dll\" issues remain same. \r\nI tried to install CUdatk8 but its telling h/w not supported for it. please help me with a suitable link or some suggestions so that I can make it work.\r\nPC is Lenovo IdeaPad 320.\r\nThank you for your kind support\r\n\r\n ", "Hi, my gcc version is 5.4.0 (Ubuntu 16.04), so I compile and install gcc 5.3.0 into /opt/gcc-5.3.0 ( gcc dependentments have already write into /etc/ld.so.conf.d/gcc-5.3.0.conf, and $HOME/.bashrc except gcc), and I have soft connection all /opt/gcc-5.3.0/bin to /usr/bin except the file whose name begins with 'x86_64-unknown-linux-' because I do not know the name in /usr/bin.\r\n\r\nSo that, I fail `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` with `gcc: Error trying to exec 'cc1plus': Execvp: No Such File or Directory`\r\n\r\nMy question is, do I have to compile it with 5.3.0?\r\n\r\nAnybody could help me?", "@Arnab0989 did you find some solution? Because I have exactly the same situation and I didn't find solution except that to install the old nvidia driver + cudnn6 + cuda8", "Looking through the benchmark code and it mentions NCCL not supporting FP16, however [Caffe has NCCL with FP16 support](https://github.com/caffe2/caffe2/blob/6f684818014513e6295305ff45d207143ad3815b/caffe2/contrib/nccl/cuda_nccl_gpu.cc).\r\n\r\nWill this be added to Tensorflow for 1.5? - I'm currently getting worse results with FP16 on internal results. Current Hypotheses are it is either NCCL or TF.Layers. However, I've implemented a custom getter which I thought would circumvent the tf.layers issues. ", "@joeyearsley \r\n\r\nCan you provide more specifics?  I assume V100 and where you are seeing FP16 worse than FP32 as far as throughput.  And single or multi-gpu as well as which CUDA and cuDNN.  That will help me to understand if and where the problem might be as well as get the right assistance and provide useful guidance.  \r\n", "@Arnab0989 and @plgkm6  I suggest opening another issue to get windows help.  This was just tracking getting CUDA 9 and cuDNN 7 support in the source.  I know that is a little nuanced but you will get more attention for the windows problem outside this thread with a separate issue.  I believe the team is still working though build issues with windows but I am not positive.", "@tfboyd Thanks for the quick reply, it's an internal benchmark which mirrors the TF benchmark but with medical images. \r\n\r\nGPUS: 8 P100s\r\nCUDA: 9\r\ncuDNN: 7\r\nMGPU type: NCCL Replicated\r\n\r\nGeneral computation time, for example I can double my batch size to 8 (max that fits in memory) but I only get 85 img/s vs 100 img/s with FP32 and batch size of 4.\r\n\r\nModel is in `NCHW` format and uses `Fused BN`.", "@joeyearsley \r\n\r\nI would not advise FP16 on P100s for training unless it happens to work better for your situation.  It is ok for testing that it works.  I believe the issues is the V100 has the ability to do FP16 but do the accumulation in FP32.  P100 cannot do that so you have to cast and do other \"stuff\".  Apologies for being vague I am sharing what I have heard but I do not have direct experience.  I am pretty sure I am generally correct but my details are likely off in some way.  I will open an issue in the benchmark repo and link it here later today.  Maybe I can get more data from the perf team. ", "/CC @chsigg, who is planning on implementing FP16 nccl support. Not sure if this will be in by TF1.5 though.\r\n\r\n@joeyearsley As @tfboyd mentioned, P100s do not get a huge gain from fp16, and there is currently no nccl support for fp16. I don't think `tf.layers` would slow things down in fp16. It might be nccl-related; you could try running without nccl for both fp16 and fp32 and compare.", "@tfboyd @reedwm Thank you for the quick replies! \r\n\r\nHaving ran that test (with and without FP16 on a PS) it seems that it is mostly the casting and other stuff.\r\n\r\nOnce again, many thanks! "]}, {"number": 12473, "title": "[bug] gradients of scatter_nd_add return None ", "body": "The gradient of scatter_nd_add  always return None.  If the gradient is not implemented, it should raise an exception. \r\n\r\n```python\r\n\r\nimport tensorflow as tf\r\nimport numpy as np \r\nimport matplotlib.pyplot as plt\r\nrng = np.random\r\n\r\n\r\n\r\nx = tf.Variable(np.random.random((2, 4)).astype('float32'))\r\nindice = tf.Variable(np.random.randint(0, 4, size=(2, 4, )), dtype=tf.int32)\r\n\r\n\r\n\r\n\r\nx_val = np.random.random((2, 4))\r\nindice_val = np.random.randint(0, 4, size=(2, 4, ))\r\nval_val = np.random.random((2, 4))\r\n\r\n\r\n# tf Graph Input\r\nX = tf.placeholder(\"float\")\r\nY = tf.placeholder(\"float\")\r\n\r\n\r\n# Set model weights\r\nW = tf.Variable(np.random.random((2, 4)).astype('float32'), name=\"weight\")\r\nb = tf.Variable(np.random.random((2, )).astype('float32'), name=\"bias\")\r\n\r\n# Construct a linear model\r\npred = tf.add(tf.multiply(X, W), b)\r\n\r\ny = tf.scatter_nd_add(x, indice, pred)\r\n\r\n\r\ngrad = tf.gradients(y, [W,b])\r\n\r\n```", "comments": ["@narphorium, @ebrevdo  Can you take a look?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I believe this just requires removing the @NoGradient registration for ScatterNdAdd/Sub.  Are you interested in submitting a PR for this, + a unit test?", "`scatter_nd_add` has been changed into [tensor_scatter_nd_add](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add).\r\nAlso, in Tensorflow 2, placeholder has been deprecated, please use the latest Tensorflow compatible code see[ Migration Guide](https://www.tensorflow.org/guide/migrate).", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 12472, "title": "Feature request: EASGD", "body": "EASGD is a very useful algorithm for distributed asynchronous training, which based on an elastic force which links the parameters of workers with a center variable stored by the parameter server. This allows the local variables to fluctuate further from the center variable, which in theory allows for more exploration of the parameter space. \r\n\r\nIn practice I find it usually outperforms ordinary ASGD.  I have been working on it  for some days and I can make a pull request if you are interested in it.\r\n\r\n## The link address of the paper\r\n[Deep learning with Elastic Averaging SGD](https://arxiv.org/pdf/1412.6651.pdf)\r\n\r\n", "comments": ["We currently don't have anybody working on this. It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Thanks!", "Hi @ali01,\r\nI need some suggestions. Since the local variables and global variables are always not same, do you think it is important or not to know the global training loss/accuracy related to global variables?", "@jinxin0924 I'm currently working on implementing ADAG. I'm stuck because I don't know how to create local variables that aren't communicated with the parameter server.   Do you have any code the demos this?", "@tmulc18 \r\nI use CustomGetter to create local variables and global variables at the same time. You may take a loot at [Model Average implementation](https://github.com/tensorflow/tensorflow/pull/11581/files), they create local variables in the optimizer.", "Could this issue please be closed since the relevant changes appear to have been merged? Thanks!"]}, {"number": 12471, "title": "replace prune subgraph algorithm with auto deduction for ranged-for", "body": "", "comments": ["@horance-liu, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @mrry and @josh11b to be potential reviewers.", "Can one of the admins verify this patch?", "(Closing for the same reason as the other change)."]}, {"number": 12470, "title": "replace auto deduction for master session close member function", "body": "", "comments": ["@horance-liu, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @suharshs and @saeta to be potential reviewers.", "Can one of the admins verify this patch?", "(Closing for the same reason as the other change)"]}, {"number": 12469, "title": "replace auto deduction for master reset member function", "body": "", "comments": ["Can one of the admins verify this patch?", "@horance-liu, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @saeta and @suharshs to be potential reviewers.", "(Closing for the same reason as the other change)"]}, {"number": 12468, "title": "external/eigen_archive/unsupported/Eigen/CXX11/Tensor:84:26: fatal error: cuda_runtime.h: No such file or directory", "body": "## System information\r\nAfter run the tf_env_collect.sh in my terminal, i get this infomation:\r\n\r\n== cat /etc/issue ===============================================\r\nLinux saners 4.10.0-32-generic #36~16.04.1-Ubuntu SMP Wed Aug 9 09:19:02 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux saners 4.10.0-32-generic #36~16.04.1-Ubuntu SMP Wed Aug 9 09:19:02 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.4)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc1-1204-g084d29e\r\ntf.COMPILER_VERSION = v1.3.0-rc1-1204-g084d29e\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/include:/usr/local/cuda-8.0/include:$LD_LIBRARY_PATH\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue Aug 22 09:32:48 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.82                 Driver Version: 375.82                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro K620         Off  | 0000:01:00.0      On |                  N/A |\r\n| 34%   40C    P0     2W /  30W |    291MiB /  1999MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1037    G   /usr/lib/xorg/Xorg                             142MiB |\r\n|    0      1867    G   compiz                                          58MiB |\r\n|    0      2340    G   ...el-token=36B9BD8BE2E6AD02534077E8E73C38D9    89MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n\r\nTensorflow version:('v1.3.0-rc1-1204-g084d29e', '1.3.0')\r\n\r\n##  Describe the problem\r\n\r\n when i  use \"bazel build //tensorflow/examples/android:tensorflow_demo\"  in terminal i get the error:\r\n\r\nERROR: /home/saners/tensorflow/tensorflow/core/kernels/BUILD:4581:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: arm-linux-androideabi-gcc failed: error executing command external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables ... (remaining 77 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:0,\r\n                 from ./tensorflow/core/kernels/bias_op_gpu.h:21,\r\n                 from tensorflow/core/kernels/bias_op.cc:30:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/Tensor:84:26: fatal error: cuda_runtime.h: No such file or directory\r\n #include <cuda_runtime.h>\r\n                          ^\r\ncompilation terminated.\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\r\n\r\nmy tensorflow has installed normally,but i don't know how to solve this problem.\r\nAnyone can help me ? ", "comments": ["I'm facing the same problem. Any updates on this?\r\n", "I met the same problem too, sometimes it has this error\r\n\r\nERROR: /home/work/tensorflow/tensorflow/core/BUILD:992:1: C++ compilation of rule '//tensorflow/core:android_tensorflow_lib_lite' failed (Exit 1).\r\nIn file included from ./tensorflow/core/platform/cupti_wrapper.h:24:0,\r\n                 from tensorflow/core/platform/default/gpu_tracer.cc:27:\r\n./tensorflow/core/platform/default/gpu/cupti_wrapper.h:26:45: fatal error: cuda/extras/CUPTI/include/cupti.h: No such file or directory\r\n #include \"cuda/extras/CUPTI/include/cupti.h\"\r\n\r\nor\r\n\r\nfatal error: cuda/include/cuda.h: No such file or directory\r\n #include \"cuda/include/cuda.h\"\r\n\r\nbut eventually\r\n\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/Tensor:84:26: fatal error: cuda_runtime.h: No such file or directory\r\n#include <cuda_runtime.h>", "I have exactly the same issue. Could @andrewharp @benoitsteiner please have a look? Thank you very much!", "anyone solved the problem ? please show us ! thank you very much!", "I got the same issue.", "I got the same error! help!!\r\n**sys**\r\nUbuntu x64 14.04\r\nbazel:0.7.0\r\ntf:1.4\r\ncuda:8\r\njava:8\r\n\r\n\r\n**error log**\r\nINFO: Found 1 target...\r\nERROR: /home/mt-srcb/phzhang/DLplatform/tensorflow-1.4.0/tensorflow/core/BUILD:993:1: C++ compilation of rule '//tensorflow/core:android_tensorflow_lib_lite' failed (Exit 1).\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:0,\r\n                 from ./tensorflow/core/kernels/conv_2d.h:19,\r\n                 from tensorflow/core/kernels/pooling_ops_common.cc:24:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/Tensor:84:26: fatal error: cuda_runtime.h: No such file or directory\r\n #include <cuda_runtime.h>\r\n                          ^\r\ncompilation terminated.\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Any news on this issue ?\r\nI'm still facing it !", "Could you try again master?\r\nThere was a recent PR that I suspect may fix this.", "I just updated to master now, and this issue is still present.", "@Pelups could you confirm the commit you are synced to in TF repository?\r\nCould you provide detailed information about your setup, bazel version, operating system, compiler version? If you can, please provide all the information in the issue template, including the command you are running.", "Yes I'm synced with the repository.\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): master\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.5.4\r\n- GCC/Compiler version (if compiling from source): 4.8\r\n- CUDA/cuDNN version: 8.0/6.0\r\n- GPU model and memory: 2 x GXT 1080\r\n- Exact command to reproduce: bazel build -c opt //tensorflow/examples/android:tensorflow_demo\r\n\r\nThank you for your help.", "@petewarden @andrewharp have you seen this issue on android before?\r\nI think this may be a separate issue than the initial report that we may want to investigate further.\r\n\r\n@Pelups I still cannot see the commit you are synced to. Could you share the output of this command:\r\n```\r\n$ git describe --long --first-parent --tags\r\n```", "@gunan \r\nThis is the output : \r\n`v1.4.0-8-gd70a2c2`", "From what I can see from your commit, you are NOT synced to github tensorflow/tensorflow repository master. Your tags say you are 8 commits ahead of 1.4 tag, which is at this point more than two months old.\r\n\r\nPlease try cloning `tensorflow/tensorflow` repository again, and building from head. If you want to use your own client, please see git documentation on how to fetch/pull the latest version of the code, and merge them into your working branch.", "I don't know how the above command works, but after cloning the Tensorflow repository, using it on the master branch gives me:\r\n`v1.1.0-rc2-1971-gab2c63a`\r\n\r\nAnd I still have the same issue.", "Can you share your full terminal output using pastebin?\r\nThe reason I am asking so many questions is, the issue reported here was definitively solved at head.\r\nSo I just want to check if yours is a similar problem.", "Here is the output of the command \r\n`bazel build -c opt //tensorflow/examples/android:tensorflow_demo`\r\nexecuted on the master branch of the repo.\r\n\r\n[output.txt](https://github.com/tensorflow/tensorflow/files/1625394/output.txt)\r\n\r\nThank you for your help", "@Pelups the android demo should not need CUDA.\r\nIs it possible during configuration you configured TF to build with CUDA? That would add `--config=cuda` automatically to the build.\r\n\r\n@nluehr is it possible that the update we made to eigen broke the build for cuda 8?", "@gunan you were right. I tried building TF without cuda and it works !\r\nThank you for your help."]}, {"number": 12467, "title": "Error in estimator.py ", "body": "Hi, \r\nI am running the tutorial code A Guide to TF Layers: Building a Convolutional Neural Network on API r.1.3\r\nhttps://www.tensorflow.org/tutorials/layers\r\n\r\nMy code is here. \r\nhttps://gist.github.com/Po-Hsuan-Huang/91e31d59fd3aa07f40272b75fe2a924d\r\n\r\nThe error shows:\r\n```\r\nrunfile('/Users/pohsuanhuang/Documents/workspace/tensorflow_models/NMIST/cnn_mnist.py', wdir='/Users/pohsuanhuang/Documents/workspace/tensorflow_models/NMIST')\r\nExtracting MNIST-data/train-images-idx3-ubyte.gz\r\nExtracting MNIST-data/train-labels-idx1-ubyte.gz\r\nExtracting MNIST-data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST-data/t10k-labels-idx1-ubyte.gz\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_model_dir': '/tmp/mnist_convnet_model', '_save_summary_steps': 100}\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-c9b70e26f791>\", line 1, in <module>\r\n    runfile('/Users/pohsuanhuang/Documents/workspace/tensorflow_models/NMIST/cnn_mnist.py', wdir='/Users/pohsuanhuang/Documents/workspace/tensorflow_models/NMIST')\r\n\r\n  File \"/Users/pohsuanhuang/miniconda/envs/tensorflow/lib/python2.7/site-packages/spyder/utils/site/sitecustomize.py\", line 866, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"/Users/pohsuanhuang/miniconda/envs/tensorflow/lib/python2.7/site-packages/spyder/utils/site/sitecustomize.py\", line 94, in execfile\r\n    builtins.execfile(filename, *where)\r\n\r\n  File \"/Users/pohsuanhuang/Documents/workspace/tensorflow_models/NMIST/cnn_mnist.py\", line 129, in <module>\r\n    main(None)\r\n\r\n  File \"/Users/pohsuanhuang/Documents/workspace/tensorflow_models/NMIST/cnn_mnist.py\", line 117, in main\r\n    hooks=[logging_hook])\r\n\r\n  File \"/Users/pohsuanhuang/miniconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 241, in train\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n\r\n  File \"/Users/pohsuanhuang/miniconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 630, in _train_model\r\n    model_fn_lib.ModeKeys.TRAIN)\r\n\r\n  File \"/Users/pohsuanhuang/miniconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 615, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n\r\n  File \"/Users/pohsuanhuang/Documents/workspace/tensorflow_models/NMIST/cnn_mnist.py\", line 24, in cnn_model_fn\r\n    input_layer = tf.reshape(features, [-1, 28, 28, 1])\r\n\r\n  File \"/Users/pohsuanhuang/miniconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2619, in reshape\r\n    name=name)\r\n\r\n  File \"/Users/pohsuanhuang/miniconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 493, in apply_op\r\n    raise err\r\n\r\nTypeError: Failed to convert object of type <type 'dict'> to Tensor. Contents: {'x': <tf.Tensor 'random_shuffle_queue_DequeueMany:1' shape=(100, 784) dtype=float32>}. Consider casting elements to a supported type.\r\n```\r\n\r\nI traced down a little bit, and found the function `estimator._call_input_fn()`  does not use parameter 'mode' at all, thus not able to create a tuple comprises features and labels. Is it the tutorial that needs to be modified, or there is some problem with this function. I don't understand why `mode ` is unused here. \r\n\r\nThanks ! Sorry if this should not be posted here.\r\n\r\n```\r\ndef _call_input_fn(self, input_fn, mode):\r\n    \"\"\"Calls the input function.\r\n\r\n    Args:\r\n      input_fn: The input function.\r\n      mode: ModeKeys\r\n\r\n    Returns:\r\n      Either features or (features, labels) where features and labels are:\r\n        features - `Tensor` or dictionary of string feature name to `Tensor`.\r\n        labels - `Tensor` or dictionary of `Tensor` with labels.\r\n\r\n    Raises:\r\n      ValueError: if input_fn takes invalid arguments.\r\n    \"\"\"\r\n    del mode  # unused\r\n    input_fn_args = util.fn_args(input_fn)\r\n    kwargs = {}\r\n    if 'params' in input_fn_args:\r\n      kwargs['params'] = self.params\r\n    if 'config' in input_fn_args:\r\n      kwargs['config'] = self.config\r\n    with ops.device('/cpu:0'):\r\n      return input_fn(**kwargs)\r\n\r\n```", "comments": []}, {"number": 12466, "title": "tensorflow contrib modules break when running in a pyc only environment", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nAmazon Linux 2016.09\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary, no gpu, virtualenv method\r\n- **TensorFlow version (use command below)**:\r\n1.2.1\r\n- **Python version**: \r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\npython -c \"from tensorflow.contrib.layers import fully_connected\"\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nWhen building a python install for a disk space constrained environment, it's fairly common to first compile all .py files to .pyc files, and then only install the .pyc files, to save a few megabytes of space. When attempting this for tensorflow, it became unable to import it's contrib modules (traceback below). From some testing I did, this appears to be due to differing behavior in this snippet from the `get_path_to_datafile` function in `python/platform/resource_loader.py`: `data_files_path = _os.path.dirname(_inspect.getfile(_sys._getframe(1)))`\r\n\r\nwhen the .py files are present, this snippet produces an absolute path. When only the .pyc files are present, it produces a relative path, which is then concatenated with another path, as the `get_path_to_datafile` function is called twice.\r\n\r\n### Source code / logs\r\n```\r\nFile \"./tensorflow/contrib/__init__.py\", line 27, in <module>\r\nFile \"./tensorflow/contrib/cudnn_rnn/__init__.py\", line 28, in <module>\r\nFile \"./tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 34, in <module>\r\nFile \"./tensorflow/contrib/util/loader.py\", line 55, in load_op_library\r\nFile \"./tensorflow/python/framework/load_library.py\", line 64, in load_op_library\r\nNotFoundError: ./tensorflow/contrib/util/./tensorflow/contrib/cudnn_rnn/python/ops/_cudnn_rnn_ops.so: cannot open shared object file: No such file or directory\r\n```", "comments": ["Does this problem still exist at TF 1.4?", "The issue still exists in tensorflow 1.4.0. The stack trace looks to have changed slightly, but the error is very similar.\r\n\r\n```\r\n>>> from tensorflow.contrib.layers import fully_connected\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"./tensorflow/contrib/__init__.py\", line 28, in <module>\r\n\r\n  File \"./tensorflow/contrib/cudnn_rnn/__init__.py\", line 33, in <module>\r\n\r\n  File \"./tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 21, in <module>\r\n  File \"./tensorflow/contrib/rnn/__init__.py\", line 83, in <module>\r\n\r\n  File \"./tensorflow/contrib/rnn/python/ops/gru_ops.py\", line 33, in <module>\r\n  File \"./tensorflow/contrib/util/loader.py\", line 55, in load_op_library\r\n  File \"./tensorflow/python/framework/load_library.py\", line 56, in load_op_library\r\n  File \"./tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\ntensorflow.python.framework.errors_impl.NotFoundError: ./tensorflow/contrib/util/./tensorflow/contrib/rnn/python/ops/_gru_ops.so: cannot open shared object file: No such file or directory\r\n```", "This appears to be because pyc files store the paths relative to where they were generated from.  So if you run the python command that generates the pyc from the root directory in an environment with the same root, it will give you full paths and work properly. That being said, it would be great if we could make this work no matter what the relative paths were.\r\n", "@annarev, could you look into making getfile() more robust to only pyc environments? I think we have the same issue if you try to put a PYTHONPATH with a relative path specified i.e.  ../foo/bar/tensorflow ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 12465, "title": "Tensorflow Debugger eats disk space with RNNs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**:  3.5\r\n\r\n### Describe the problem\r\n\r\nI have an RNN that I'm trying to debug using `LocalCLIDebugWrapperSession` which runs on very long sequences (4800 steps). Even for a basic LSTM cell that I apply via `tf.nn.dynamic_rnn`, `tfdbg` uses a lot of disk space (>5 GB sometimes). When I accidentally collected gradient infromation via `tf.contrib.layers.optimize_loss(..., summaries=[\"gradients\", ...])`, this ballooned even more to >70 GB (which promptly crashed my laptop).\r\n\r\nFrom inspecting the `tfdbg` dump in `/tmp/`, it looks like this is because tfdbg` dumps out information for each time step. Especially with `tf.contrib.layers.optimize_loss` capturing gradients, this means that there are hundreds (thousands?) of small files being created on each time step.\r\n\r\nIn some sense, this is expected behavior (each time step represents a group of TF operations), but using 70 GB seems like a pretty sharp-edged API that's easy to mis-use. I'm not sure what to really do here -- maybe there's a way to somehow compress these files or to combine all these small files across time steps into one large file? It could also just be a documentation problem.", "comments": ["This possibility of filling large amount of disk space while using tfdbg to debug certain models is documented in this FAQ section: https://www.tensorflow.org/programmers_guide/debugger#frequently_asked_questions,\r\n\r\nunder the heading \r\nQ: The model I am debugging is very large. The data dumped by tfdbg fills up the free space of my disk. What can I do?\r\n\r\nBut I'm considering adding a configurable cap on how much disk space tfdbg consumes by default.\r\n\r\n", "Huh... I somehow missed that FAQ section \ud83d\ude28.\r\n\r\nOne other problem with having hundreds of thousands of small files is that the tfdbg UI regularly freezes whenever I try to use it. Looking at strace, it looks like it's looping through all the files it created\r\n\r\n```\r\n...\r\nnewfstatat(14, \"Sum_0_DebugIdentity_1503415213363935\", {st_mode=S_IFREG|0664, st_size=15705, ...}, AT_SYMLINK_NOFOLLOW) = 0\r\nunlinkat(14, \"Sum_0_DebugIdentity_1503415213363935\", 0) = 0\r\nnewfstatat(14, \"Sum_1_0_DebugIdentity_1503415270902805\", {st_mode=S_IFREG|0664, st_size=15709, ...}, AT_SYMLINK_NOFOLLOW) = 0\r\nunlinkat(14, \"Sum_1_0_DebugIdentity_1503415270902805\", 0) = 0\r\n...\r\n```\r\n\r\nSo I guess this is really a feature request I'm asking for: is there a way to output data into a handful of very large files, rather than a huge number of small files? At least that way, `tfdbg` wouldn't be hang looping making syscalls for each file.", "+1, found that it was very slow once turn on tfdbg dump by `DumpingDebugWrapperSession`, and I post the comparison info about my example training(300 steps), as below:\r\n\r\nturn off tfdbg dump:\r\nreal\t0m9.759s\r\nuser\t0m8.315s\r\nsys\t0m1.625s\r\n\r\nturn on tfdbg dump:\r\nreal\t2m24.094s\r\nuser\t2m19.819s\r\nsys\t0m12.707s", "Nagging Assignee @caisq: It has been 377 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@harshini-gadige What is the right way to move forward here? AFAIK, this issue still exists and we've been waiting on a response from @caisq for over a year now (!) on whether this is a bug or expected behavior.\r\n\r\nAFAIK, the core of the issue is still valid: for RNNs, we get hundreds of thousands of small files, which make `tfdbg` super painful to use because it iterates through all of them. I can think of a couple of remediation steps (e.g. use a handful of large files instead of a huge number of small files), although I don't understand `tfdbg` well enough to know what's realistic."]}, {"number": 12464, "title": "Testing out the build", "body": "", "comments": ["@jhseu, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @yongtang and @vrv to be potential reviewers.", "Jenkins, test this please"]}, {"number": 12463, "title": "Change required cuDNN from v5.1 to v6.0 for Linux in the docs", "body": "This fix changes the required cuDNN from v5.1 to v6.0 for Linux in `tensorflow/docs_src/install/install_linux.md` as the build is linked against libcudnn.so.6 now (not libcudnn.so.5)\r\n\r\nThis fix fixed #12416.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Note that I made a similar fix to the Windows docs on the 1.3 branch in #12383. AFAIK, it hasn't appeared on the website yet, but it's probably important to cherrypick this into the 1.3 branch as well."]}, {"number": 12462, "title": "fix gdr compiling error", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "CMake build aborts again, seems to be unrelated.", "@jhseu Jonathan, could you merge this when you have time? Thanks!", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please", "CI server seems to fail on fetching Git.", "@jhseu Can we just merge this? The CI seems to be constantly broken, and the fix is straight forward."]}, {"number": 12461, "title": "README link to Linux GPU Python 3.5 nightly build is broken ", "body": "I figured you wouldn't need the form information. The link:\r\n\r\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.3.0-cp35-cp35m-linux_x86_64.whl\r\n\r\nMy response:\r\n\r\n```\r\nHTTP ERROR 404\r\n\r\nProblem accessing /view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.3.0-cp35-cp35m-linux_x86_64.whl. Reason:\r\n\r\n    Not Found\r\nPowered by Jetty://\r\n```\r\n", "comments": ["The link worked for me. Please re-open if issue persists..."]}, {"number": 12460, "title": "TensorFlow Android 1.2 - Adds READ_PHONE_STATE permission", "body": "", "comments": ["I am building with:\r\n     compile 'org.tensorflow:tensorflow-android:1.2.0'\r\nMy app was rejected by Play Store due to having the READ_PHONE_STATE permission and no privacy policy. However I don't have this permission anywhere in my app, and tracked it down to TensorFlow adding it.\r\nJudging by this thread, I am not alone:\r\nhttps://stackoverflow.com/questions/44864081/tensorflow-android-library-asking-for-read-phone-state", "@meavydev If you add targetSdkVersion = 23 to your build.gradle and/or minSdkVersion = 19, does this solve the issue? READ_PHONE_STATE is automatically requested by the application target/minSdkVersion are < 4. We set this in AndroidManifest.xml, but only compileSdkVersion in the build.gradle, in case you've copied that for your app.\r\n\r\nIf you only get this issue when including the TF AAR, then we'll need to figure out how to add it to the AAR build as well. Disabling manifest exporting in the [android_tensorflow_inference_java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/BUILD#L58) android_library rule that makes up the basis for the AAR might help, but otherwise we could explicitly add a manifest with higher target levels..", "I am only using the AAR, not building from source.\r\nI am building with target SDK set to 24 and build tools on 24, due to a tool that only works with that setup.\r\nMy minSDK is set to 16.\r\nGiven it sounds like it's an AAR issue, I can try a build increasing the minSDK, but it was running at this level for some AR glasses.", "I guess this just needs a min-sdk like #20454 ", "@PaulWoitaschek I'm having this issue. And users are wondering why I need this permission (I don't). Any update on this?", "This should be fixed by b148d228886e53f71401c332b8d48d45467dad47", "Thanks for the update. The workaround for now is to add this to your manifest:\r\n```\r\n<uses-permission\r\n        android:name=\"android.permission.READ_PHONE_STATE\"\r\n        tools:node=\"remove\" />\r\n```", "While checking why my app required READ_PHONE_STATE I found out some things I want to share:\r\n- crobertsbmw is absolutely right, the workaround is very helpful!\r\n- the fix mentioned by PaulWoitaschek was merged before release of 1.10.0 lib, which still does NOT contain the fix and asks for the permission in question. (I assume a merge readded the code snippet)\r\n- the actual master commit [bf23e5f](https://github.com/tensorflow/tensorflow/tree/bf23e5f0f6e75a35521b7765da9863dbc24d8a71) does again contain the fix and does not require the READ_PHONE_STATE permission\r\n\r\nI hope the permission is fixed by next release of tensorflow-lite lib 1.11.0\r\n\r\n", "I don't understand what happened here:\r\nhttps://github.com/tensorflow/tensorflow/commits/master/tensorflow/contrib/lite/java/AndroidManifest.xml\r\n\r\n602b03d5fb521aef6561a5c131075d915907357e My commit from Jul 16, 2018\r\nb148d228886e53f71401c332b8d48d45467dad47 jdduke authored and tensorflower-gardener committed 23 days ago\r\n\r\nTwo consecutive commits added the same thing to the same file? What am I missing?", "That was a thing I ran into as well.\r\nThere are different folders with different manifests. Your commit downgraded the targetSdk for the Android TFLite sample app, as jddukes commit added sdk configuration to `tensorflow/contrib/android/cmake/src/main/AndroidManifest.xml`, which I assume to be the source for building the library.\r\n\r\nThe library manifest got fixed once, but lost the sdk configuration in a commit, somewhere before building version 1.10.0.\r\n\r\nSomebody then noticed the mistake, as it is fixed for the mentioned master commit.\r\n ", "Nagging Assignee @andrewharp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "1.11.0 is released but not to jcenter. Can you please push the update to jcenter too?", "@PaulWoitaschek can you please update whether the update has been pushed to jcenter?", "https://bintray.com/google/tensorflow/tensorflow-lite\r\n\r\nOnly 1.10.0", "@wt-huang pingpong", "@PaulWoitaschek thanks for the confirmation. Any ETA as to when the updated 1.11.0 will be pushed to jcenter? ", "I am not involved into that, I just contributed the fix.", "@PaulWoitaschek @meavydev Closing this issue for now as the fix is in for 1.10.0 and 1.11.0 will be pushed to jcenter when the release is ready. Feel free to open a new issue if any error comes up.", "But 1.10.1?"]}, {"number": 12459, "title": "Respect `optimize` parameter in `zeros_like`", "body": "This fix address the issue in #12436 where `optimize` flag was not respected when shape is fully defined for `zeros_like`.\r\n\r\nThis fix fixes #12436.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "The PR has been rebased to fix the merging conflict and pushed. Please take a look.", "Jenkins, test this please", "Jenkins, test this please.", "Error on MacOS was a lot larger than on other platforms:\r\n\r\n```\r\n[ RUN      ] NNGradTest.MaxPoolGradHelper\r\ntensorflow/cc/gradients/nn_grad_test.cc:39: Failure\r\nExpected: (max_error) < (2e-4), actual: 0.134928 vs 0.0002\r\n```\r\n\r\nSeems unrelated?", "Yay! All green."]}]