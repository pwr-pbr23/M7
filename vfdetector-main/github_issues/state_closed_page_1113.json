[{"number": 19856, "title": "problem in tf.data.TFRecordDataset", "body": "**Describe the problem**\r\nI use the tf.data.TFRecordDataset API to read the tfrecord file. Everything is ok when batch_size is set to 1, but when the batch_size is greater than 1, the code is crashed. the error is below:\r\n\r\n_Traceback (most recent call last):\r\n  File \"F:/kaggle/JD_Fashion_AI/train.py\", line 125, in <module>\r\n    main()\r\n  File \"F:/kaggle/JD_Fashion_AI/train.py\", line 122, in main\r\n    run_training()\r\n  File \"F:/kaggle/JD_Fashion_AI/train.py\", line 98, in run_training\r\n    sess.run(label)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 200704 values, but the requested shape has 150528\r\n\t [[Node: Reshape = Reshape[T=DT_UINT8, Tshape=DT_INT32](DecodeRaw, Reshape/shape)]]\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,224,224,3], [?,?], [?]], output_types=[DT_UINT8, DT_UINT8, DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n_\r\n\r\nnote: i have totally 50000 images, i originally set batch_size to 16, so the num_per_epoch is 3125.\r\n\r\n**Source code**\r\n`def _parse_function(record):\r\n\r\n    features = {'image': tf.FixedLenFeature([], tf.string), 'label': tf.FixedLenFeature([], tf.string),\r\n                'id': tf.FixedLenFeature([], tf.string)}\r\n    example = tf.parse_single_example(record, features=features)\r\n    image = tf.decode_raw(example['image'], tf.uint8)\r\n    image = tf.reshape(image, [224, 224, 3])\r\n    label = tf.subtract(tf.decode_raw(example['label'], tf.uint8), 48)\r\n    id = tf.cast(example['id'], tf.string)\r\n\r\n    return image, label, id\r\n\r\n\r\ndef run_training():\r\n\r\n    # learning_rate = tf.placeholder(dtype=tf.float32)\r\n    train_filenames = ['I:/JD_fashion_AI/train_tfrecords/train.tfrecords']\r\n    validation_filenames = ['I:/JD_fashion_AI/valid_tfrecords/valid.tfrecords']\r\n    train_datasets = tf.data.TFRecordDataset(filenames=train_filenames)\r\n    validation_datasets = tf.data.TFRecordDataset(filenames=validation_filenames)\r\n    train_datasets = train_datasets.map(_parse_function).shuffle(buffer_size=1000).batch(16).repeat()\r\n    validation_datasets = validation_datasets.map(_parse_function).batch(1).repeat(1)\r\n    iterator = tf.data.Iterator.from_structure(train_datasets.output_types,\r\n                                               train_datasets.output_shapes)\r\n    image, label, id = iterator.get_next()\r\n    training_init_op = iterator.make_initializer(train_datasets)\r\n    validation_init_op = iterator.make_initializer(validation_datasets)\r\n\r\n    init = tf.global_variables_initializer()\r\n\r\n    with tf.Session() as sess:\r\n\r\n        sess.run(init)\r\n        step = 0\r\n        for _ in range(18):\r\n            sess.run(training_init_op)  # must initialize the iterator to read the data flow\r\n            for _ in range(3125):\r\n                \r\n                sess.run(label)\r\n                print(step)\r\n`\r\nanyone else know what happened ?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have solved my problem. There is something wrong with my training data. The depth of normal image is 3, but there exist several images with depth of 4. Hence, input tensor is not match  with my default size 150528 (150528 = 224 x 224 x 3).  ", "Looks like this issue is resolved, so I'll close it. Thanks for posting an update!"]}, {"number": 19855, "title": "[Faster RCNN Resnet object detection] Tensorflow crash by using full of RAM memory", "body": "### System information\r\n- **What is the top-level directory of the model you are using**:\r\nmodels/research/object_detection/\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 17.04\r\n- **TensorFlow installed from (source or binary)**:\r\nconda install\r\n- **TensorFlow version (use command below)**:\r\n1.7.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nCUDA\u00ae Toolkit 9.0; cuDNN v7.1\r\n- **GPU model and memory**:\r\nGForce GTX 1080Ti (11GB)\r\n- **Exact command to reproduce**:\r\n```\r\npython train.py --logtostderr --train_dir=training_point/ --pipeline_config_path=training/faster_rcnn_inception_resnet_v2_atrous_coco.config\r\n```\r\n```\r\npython eval.py --logtostderr --pipeline_config_path=training/faster_rcnn_inception_resnet_v2_atrous_coco.config --checkpoint_dir=training_point/ --eval_dir=eval/\r\n```\r\n```\r\ntensorboard --logdir=training:training_point/,testing:eval/\r\n```\r\n### Describe the problem\r\nI retrain faster_RCNN_resnet101_coco to detect some object. But it consumes all of my available RAM's memory.\r\nMy dataset contains 1000 images. My RAM's memory is 32GB.\r\nBecause of this issue, the OS always kill my training process after 7k iterations.\r\nWhy does tensorflow use too many RAM's memory like that? What is the solution to solve this issue?\r\nThanks you for your help!\r\n", "comments": ["Could you share the following:\r\n- Pipeline config file you used for training\r\n- image resolution(s) of your dataset", "Hey, what is the size of your dataset in GB? The inbuilt data loader usually loads all the data at the beginning of loading it for faster training. If you are having RAM issue than you have to make your own data loader which will only load fix amount of image at a time."]}, {"number": 19854, "title": "TensorFlowException: Op type not registered 'NonMaxSuppressionV3' in binary running on localhost.", "body": "```\r\n### System information\r\nLinux 4.4.0-127-generic #153-Ubuntu SMP Sat May 19 10:58:46 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 4.4.0-127-generic #153-Ubuntu SMP Sat May 19 10:58:46 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                          1.14.2                \r\nprotobuf                       3.5.2.post1           \r\ntensorflow                     1.8.0                 \r\ntensorflow-serving-api-python3 1.4.0                 \r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n\r\n- **Exact command to reproduce**: I run my android application on Android Studio\r\n```\r\n\r\n### Describe the problem\r\nI have an android application where for instance I make an initialization of my frozen model. \r\nIt seems that there is a problem with the ops. I get this exception:\r\n\r\n`java.lang.RuntimeException: Unable to start activity ComponentInfo{myapp/myapp.MainActivity}: \r\norg.tensorflow.TensorFlowException: Op type not registered 'NonMaxSuppressionV3' in binary running on localhost. Make sure the Op and Kernel are registered in the binary running in this process.\r\n\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2666)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2727)\r\n        at android.app.ActivityThread.-wrap12(ActivityThread.java)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1478)\r\n        at android.os.Handler.dispatchMessage(Handler.java:102)\r\n        at android.os.Looper.loop(Looper.java:154)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6121)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:890)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:780)\r\n     Caused by: org.tensorflow.TensorFlowException: Op type not registered 'NonMaxSuppressionV3' in binary running on localhost. Make sure the Op and Kernel are registered in the binary running in this process.\r\n        at org.tensorflow.Graph.importGraphDef(Native Method)\r\n        at org.tensorflow.Graph.importGraphDef(Graph.java:130)\r\n        at org.tensorflow.Graph.importGraphDef(Graph.java:114)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:559)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)\r\n        at myapp.ActivityInference.<init>(ActivityInference.java:22)\r\n        at myapp.MainActivity.onCreate(MainActivity.java:21)\r\n        at android.app.Activity.performCreate(Activity.java:6692)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1118)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2619)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2727)\u00a0\r\n        at android.app.ActivityThread.-wrap12(ActivityThread.java)\u00a0\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1478)\u00a0\r\n        at android.os.Handler.dispatchMessage(Handler.java:102)\u00a0\r\n        at android.os.Looper.loop(Looper.java:154)\u00a0\r\n        at android.app.ActivityThread.main(ActivityThread.java:6121)\u00a0\r\n        at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:890)\u00a0\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:780)\u00a0` \r\n\r\nI have checked the BUILD file in tensorflow/core/kernels/ and there is defined the kernel library:\r\n`tf_kernel_library(\r\n    name = \"non_max_suppression_op\",\r\n    prefix = \"non_max_suppression_op\",\r\n    deps = IMAGE_DEPS,\r\n)`\r\nI am new to this so I really do not know how I can fix it.\r\nI am working on a pre-trained frozen model.\r\n\r\n### Source code / logs\r\nAndroid code:\r\n\r\n\r\n```\r\npackage myapp;\r\nimport android.content.Context;\r\nimport android.content.res.AssetManager;\r\n\r\nimport org.tensorflow.contrib.android.TensorFlowInferenceInterface;\r\n\r\n\r\npublic class ActivityInference {\r\n\r\n    private static ActivityInference activityInferenceInstance;\r\n    private TensorFlowInferenceInterface inferenceInterface;\r\n    private static AssetManager assetManager;\r\n    private static final String MODEL_FILE = \"file:///android_asset/frozen_inference_graph.pb\";\r\n    private static final String INPUT_NODE = \"input\";\r\n\r\n    public ActivityInference(final Context context) {\r\n        this.assetManager = context.getAssets();\r\n        inferenceInterface = new TensorFlowInferenceInterface(assetManager, MODEL_FILE);\r\n\r\n    }\r\n\r\n    public static ActivityInference getInstance(final Context context){\r\n        if (activityInferenceInstance == null)\r\n        {\r\n            activityInferenceInstance = new ActivityInference(context);\r\n        }\r\n        return activityInferenceInstance;\r\n    }\r\n}\r\n```\r\n\r\nand my main activity:\r\n\r\n```\r\npackage myapp;\r\n\r\nimport android.support.v7.app.AppCompatActivity;\r\nimport android.os.Bundle;\r\nimport android.widget.TextView;\r\n\r\n\r\npublic class MainActivity extends AppCompatActivity {\r\n\r\n    private ActivityInference activityInference;\r\n\r\n    @Override\r\n    protected void onCreate(Bundle savedInstanceState) {\r\n        super.onCreate(savedInstanceState);\r\n        setContentView(R.layout.activity_main);\r\n        final TextView textView = (TextView) findViewById(R.id.textView);\r\n        activityInference = new ActivityInference(getApplicationContext());\r\n    }\r\n}\r\n```", "comments": ["@andrewharp  Would you mind to see this issue I raised please? I really need some help. Thanks. ", "you should basically add the missing op and rebuild the library manually as mentioned in the issue you referenced. Did you manage to do it ? I'm facing the same problem with the same missing op after training my network with the `mobilenet V2 SSDlite` for object detection", "@achraf-boussaada  No I did not, when I do it with tflite it works but with mobile version of mobile it does not, and I can not rebuild the library.", "Well I managed to build the library and use it with `Tensorflow mobile` but the results were disappointing (almost 2 seconds inference for object detection). Did you get better results when using `tflite` ? \r\n\r\nto get it to work on `Tensorflow mobile` I cloned `Tensorflow` with this line to avoid protobuf compilation errors: \r\n`git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git `\r\n\r\nthen proceeded normally building the library from source and followed the instruction used in the issue you referenced and it worked for me.\r\n\r\n**Note :** create a `virtualenv` and clone and build `tensorflow` from there, it isolates it and help avoid any conflict problems.", "@achraf-boussaada  Well I am working on windows, so it seems a little complicated. \r\nAnd I am trying to include my model mobile to be able to do the TF Detect but I get this error, although I have defined cmake in the gradle build file.\r\nYes the mobile version seems slower than the tflite of course. But the confidence is less when it'a about the tflite.", "@achraf-boussaada  You have your own model mobile or you are using their model? When it's with their model it works perfectly, but when I put my mobile model it does not do the job and raises this exception.", "@japer21  hmm ! good to know. I took the `mobilenet V2 SSDlite` checkpoint and I trained my custom model with it. I had also disappointing results when I used `tflite` with a model trained with `Mobilenet V1 SSD` (inference time was way better with Tensorflow mobile- not only me encountered this issue-). That's why I'm holding off about using `tflite` and I'm waiting until it's stable enough to switch. Even the developers said it's better to use `TF mobile` when planning on production not only testing.", "@achraf-boussaada You use android on which OS system please? I am on Windows and with the most recent and stable version of andrfoid tensorflow it does not work.\r\nWould you mind explaining me a little more of the steps you took to make it working after training your model? Thanks in advance.", "@achraf-boussaada In fact, the model I am using has been trained with Inception and not mobilenet V2, maybe that can cause also a problem.", "frist of all it doesn't matter which OS you're using when working with android. It only matters for model training, afterwards you're going to freeze your model and add that file to your android project. I'm working with MacOS (you can always create a virtual machine and use ubuntu since windows has some compatibility issues with tensorflow from source and so you can benefit from better results while using bazel).\r\n\r\nI'm using the [Object detection demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) from tensorflow because I need to detect objects in realtime, so all I had to do is add my new model to the asset folder and do minor changes in the code. As it seems from your code snippet you're trying to write an app from scratch and since you're using Inception I guess you're trying to classify images. So take a look at the demo I referenced you can test your model there and have a good idea how things by classification works.", "@achraf-boussaada Did you change something to make it work without giving the exception `org.tensorflow.TensorFlowException: Op type not registered 'NonMaxSuppressionV3' in binary running on localhost. Make sure the Op and Kernel are registered in the binary running in this process.`  ?  I am also using that demo for Object detection because I need to detect specific objects, and while using my own model it does not work.", "I built the library manually for different cpu architectures as I mentioned in my earlier comment to add the missing `op`, after that i added the library to my project. my project structure looks like this : \r\n(the libraries should be placed in the `libs` directory in your project)\r\n\r\n> --jniLibs\r\n> ----arm64-v8\r\n> --------libtensorflow_inference.so\r\n> ----armeabi-v7a\r\n> --------libtensorflow_inference.so\r\n> ----x86\r\n> --------libtensorflow_inference.so\r\n> ----x86_64\r\n> --------libtensorflow_inference.so\r\n> ----libandroid_tensorflow_inference_java.jar\r\n\r\nyou should download the `libandroid_tensorflow_inference_java.jar` from the last successful [nightlybuild](https://ci.tensorflow.org/view/Nightly/job/nightly-android/) referenced in their Object Detection Demo Readme but I don't know why for about a month now the link don't work anymore. Try to open a new issue about it, because I can't attach it here  and I don't know if there is a new link for it.\r\n\r\nyou need to add this to your gradle file : \r\n\r\n```\r\nandroid{\r\n...\r\n    sourceSets {\r\n        main {\r\n            jniLibs.srcDirs = ['libs']\r\n        }\r\n    }\r\n...\r\n}\r\n```\r\nall of this will basically replace this line : \r\n`implementation 'org.tensorflow:tensorflow-android:+'`\r\nso after adding the libraries delete that line to avoid conflict. And that should be it", "@achraf-boussaada  This link https://ci.tensorflow.org/view/Nightly/job/nightly-android/ seems broken as it gives me 404 Error. I will try these steps thanks", "yep it's broken. It worked a month ago and in all documentations I saw, they still referencing that link. Maybe they restructured their Jenkins. Good luck and keep me updated , maybe I can help.", "@achraf-boussaada You did it using these commands?\r\n\r\n```\r\nAn example of command-line usage is:\r\n  bazel build tensorflow/python/tools:print_selective_registration_header && \\\r\n  bazel-bin/tensorflow/python/tools/print_selective_registration_header \\\r\n    --graphs=path/to/graph.pb > ops_to_register.h\r\n\r\nThen when compiling tensorflow, include ops_to_register.h in the include search\r\npath and pass -DSELECTIVE_REGISTRATION and -DSUPPORT_SELECTIVE_REGISTRATION\r\n - see core/framework/selective_registration.h for more details.\r\n\r\nWhen compiling for Android:\r\n  bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" \\\r\n    --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" \\\r\n    //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n    --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a\r\n```", "yep ! I used those commands. I got an error at first but when I cloned the project using this : \r\n`git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git`\r\nit resolved my issue and it worked. Make sure to compile the library for all cpu architectures.", "I'm closing this issue since it's not been updated for a while, and seems to be fixed from the latest comments."]}, {"number": 19853, "title": "There seems to be a wrong network implementation in tensorflow/example/speech_commands/models.py", "body": "There may be a mistake I found in `tensorflow/tensorflow/example/speech_commands/models.py`:\r\nThe function `create_conv_model()` is to create a \"cnn-trad-fpool3\" network with 2 max pool layers, but the second max pool layer is missing now.  The code can run normally, but I'm not sure if it is  the right \"cnn-trad-fpool3\" network.  \r\n\r\nI put part of the code of `tensorflow/tensorflow/exmaple/speech_commands/models.py` here below and added some comments  to explain my points.\r\n```python\r\ndef create_conv_model(fingerprint_input, model_settings, is_training):\r\n  \"\"\"Builds a standard convolutional model.\r\n  This is roughly the network labeled as 'cnn-trad-fpool3' in the\r\n  'Convolutional Neural Networks for Small-footprint Keyword Spotting' paper:\r\n  http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf\r\n  Here's the layout of the graph:\r\n  (fingerprint_input)\r\n          v\r\n      [Conv2D]<-(weights)\r\n          v\r\n      [BiasAdd]<-(bias)\r\n          v\r\n        [Relu]\r\n          v\r\n      [MaxPool]\r\n          v\r\n      [Conv2D]<-(weights)\r\n          v\r\n      [BiasAdd]<-(bias)\r\n          v\r\n        [Relu]\r\n          v\r\n      [MaxPool]\r\n          v\r\n      [MatMul]<-(weights)\r\n          v\r\n      [BiasAdd]<-(bias)\r\n          v\r\n  This produces fairly good quality results, but can involve a large number of\r\n  weight parameters and computations. For a cheaper alternative from the same\r\n  paper with slightly less accuracy, see 'low_latency_conv' below.\r\n  During training, dropout nodes are introduced after each relu, controlled by a\r\n  placeholder.\r\n  Args:\r\n    fingerprint_input: TensorFlow node that will output audio feature vectors.\r\n    model_settings: Dictionary of information about the model.\r\n    is_training: Whether the model is going to be used for training.\r\n  Returns:\r\n    TensorFlow node outputting logits results, and optionally a dropout\r\n    placeholder.\r\n  \"\"\"\r\n  if is_training:\r\n    dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')\r\n  input_frequency_size = model_settings['dct_coefficient_count']\r\n  input_time_size = model_settings['spectrogram_length']\r\n  fingerprint_4d = tf.reshape(fingerprint_input,\r\n                              [-1, input_time_size, input_frequency_size, 1])\r\n  first_filter_width = 8\r\n  first_filter_height = 20\r\n  first_filter_count = 64\r\n  first_weights = tf.Variable(\r\n      tf.truncated_normal(\r\n          [first_filter_height, first_filter_width, 1, first_filter_count],\r\n          stddev=0.01))\r\n  first_bias = tf.Variable(tf.zeros([first_filter_count]))\r\n  first_conv = tf.nn.conv2d(fingerprint_4d, first_weights, [1, 1, 1, 1],\r\n                            'SAME') + first_bias\r\n  first_relu = tf.nn.relu(first_conv)\r\n  if is_training:\r\n    first_dropout = tf.nn.dropout(first_relu, dropout_prob)\r\n  else:\r\n    first_dropout = first_relu\r\n  # the first and only max pool operation in the whole network.\r\n  max_pool = tf.nn.max_pool(first_dropout, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME') \r\n  second_filter_width = 4\r\n  second_filter_height = 10\r\n  second_filter_count = 64\r\n  second_weights = tf.Variable(\r\n      tf.truncated_normal(\r\n          [\r\n              second_filter_height, second_filter_width, first_filter_count,\r\n              second_filter_count\r\n          ],\r\n          stddev=0.01))\r\n  second_bias = tf.Variable(tf.zeros([second_filter_count]))\r\n  second_conv = tf.nn.conv2d(max_pool, second_weights, [1, 1, 1, 1],\r\n                             'SAME') + second_bias\r\n  second_relu = tf.nn.relu(second_conv)\r\n  if is_training:\r\n    second_dropout = tf.nn.dropout(second_relu, dropout_prob)\r\n  else:\r\n    second_dropout = second_relu\r\n  # there could be another one max pool operation as the comments said in the beginning:\r\n  # another_max_pool =tf.nn.max_pool(second_dropout, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')\r\n  # and the variable name \"second_dropout\" below should be changed to \"another_max_pool\".\r\n  second_conv_shape = second_dropout.get_shape()\r\n  second_conv_output_width = second_conv_shape[2]\r\n  second_conv_output_height = second_conv_shape[1]\r\n  second_conv_element_count = int(\r\n      second_conv_output_width * second_conv_output_height *\r\n      second_filter_count)\r\n  flattened_second_conv = tf.reshape(second_dropout,\r\n                                     [-1, second_conv_element_count])\r\n  label_count = model_settings['label_count']\r\n  final_fc_weights = tf.Variable(\r\n      tf.truncated_normal(\r\n          [second_conv_element_count, label_count], stddev=0.01))\r\n  final_fc_bias = tf.Variable(tf.zeros([label_count]))\r\n  final_fc = tf.matmul(flattened_second_conv, final_fc_weights) + final_fc_bias\r\n  if is_training:\r\n    return final_fc, dropout_prob\r\n  else:\r\n    return final_fc\r\n```\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  No. I read the source code and run the it on my own computer.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  \r\nN/A\r\n- **TensorFlow installed from (source or binary)**:\r\nN/A\r\n- **TensorFlow version (use command below)**:  \r\nv1.9.0-rc0\r\n- **Python version**: \r\nN/A\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nN/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thanks for replying to my issue, I have updated my comment and added more details.", "@petewarden can you please take a look?\r\n", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks for spotting this! It does look like a bug, sorry about that. Since we've been using that conv model as a baseline (for example in https://arxiv.org/abs/1804.03209 ) it might be awkward to fix it at this stage. I would welcome a PR with a comment added noting this issue, and possibly an alternative conv_with_maxpool implementation if it does produce noticeably better results. Closing for now, but please refer to this if you do get a chance to work on that."]}, {"number": 19852, "title": "How can I use beam search together with CustomHelper when decoding in a seq2seq model structure?", "body": "", "comments": ["I know there is a BeamSearchDecoder, but no parameter called helper in this function. Does anyone have idea how to do that? Thanks for the help!", "Nagging Assignee @cy89: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@lukaszkaiser can you offer any advice, please?", "Sorry, I have no experience with this code, I've never used it. I can only recommend switching to newer things like the tensor2tensor library, but maybe someone can find a real answer?", "Nagging Assignee @cy89: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19851, "title": "Protobuff file in Android example - \"TF Detect\" app", "body": "**OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**\r\nLinux Ubuntu 16.04\r\n**TensorFlow installed from (source or binary):**\r\nSource.\r\n**TensorFlow version (use command below):**\r\n1.8.0\r\n**Python version:**\r\n2.7.12\r\n**Bazel version (if compiling from source):**\r\n0.13.0\r\n**GCC/Compiler version (if compiling from source):**\r\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)\r\n\r\nSorry if this is wrong place to ask, since my question is not related to any bug/error. \r\nI have already asked on StackOverflow and nobody has given me an answer, so I decided to ask it here. \r\n\r\nIn android example https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android , there is \"ssd_mobilenet_v1_android_export.pb\" that gets stored in assets folderT once gradle is run. The model seems to be trained using COCO dataset.\r\n\r\nI would like to know where I can find original .cfg and .weights files of the CNN in order to do my own training. I could not find anything online.\r\n\r\nThank you in advance.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Closing this: I think stackoverflow is still your best bet for this kind of support, sorry!"]}, {"number": 19850, "title": "Computed output size would be negative. Qunatized graph error", "body": "Some what similar to issue [10458](https://github.com/tensorflow/tensorflow/issues/10458).\r\nI generated my frozen graph and quantized it for deploying it. But this is the error message I am getting when trying to deploy it as a apk.\r\n\r\n```\r\nInvalid argument: Computed output size would be negative: -32 [input_size: 84, effective_filter_size: 117, stride: 1]\r\nInvalid argument: Computed output size would be negative: -32 [input_size: 85, effective_filter_size: 118, stride: 1]\r\nInvalid argument:Computed output size would be negative: -32 [input_size: 86, effective_filter_size: 119, stride: 1]\r\nFailed to run TensorFlow inference with inputs:[inputTensor, dropout_keep_prob], outputs:[output/softmax]\r\n```", "comments": ["classifier.inputSize = \"chnage it the size of the inputTensor\";"]}, {"number": 19849, "title": "XLA Compile error: Operation has no attr named '_XlaCompile'", "body": "### System information\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: pip (Command: pip install tensorflow-gpu)\r\n- **TensorFlow version**: v1.8.0-0-g93bc2e2072  1.8.0\r\n- **Python version**: Python 3.6.4 :: Anaconda, Inc.\r\n- **CUDA/cuDNN version**: CUDA Version 9.0.176 / cuDNN: 7.0.5.15\r\n- **Bazel version**: N/A\r\n- **GPU model and memory**: GeForce GTX 1080 / 8 GB\r\n- **Exact command to reproduce**: python filename.py\r\n- **Have I written custom code**: No\r\n\r\n### Describe the problem\r\nI am running the below code. I am getting the XLA Compiler error as mentioned below. I am having Tensorflow GPU version, but I did try with CPU version and I was getting the error. But when I downgraded Tensorflow (CPU) to 1.3 I was not getting any error. \r\nPlease can somebody help me in this.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\ninput_tensor = tf.placeholder(dtype=tf.float32, shape=[None, 16, 16, 3])\r\nprint(input_tensor.shape)\r\n\r\nconv_filter = tf.get_variable('conv_filter', shape=[2, 2, 3, 6], dtype=tf.float32)\r\nconv1 = tf.nn.conv2d(input_tensor, conv_filter, strides=[1, 2, 2, 1], padding='SAME')\r\nprint(conv1.shape)\r\n\r\ndeconv_filter = tf.get_variable('deconv_filter', shape=[2, 2, 6, 3], dtype=tf.float32)\r\n\r\ndeconv = tf.nn.conv2d_transpose(input_tensor, filter=deconv_filter,\r\n    output_shape=tf.shape(input_tensor),\r\n    strides=[1, 2, 2, 1],\r\n    padding='SAME')\r\nprint(deconv.shape)\r\n\r\nt = tf.reduce_mean(deconv)\r\ng = tf.train.AdamOptimizer(0.01).minimize(t)\r\n```\r\n\r\nError:\r\n```\r\n(?, 16, 16, 3)\r\n(?, 8, 8, 6)\r\n(?, 16, 16, 3)     # <<<<< This should have printed (?, ?, ?, ?)\r\n\r\nTraceback (most recent call last):\r\n  File \"/my/path/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2327, in get_attr\r\n    c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'conv2d_transpose' has no attr named '_XlaCompile'.\r\n\r\nTraceback (most recent call last):\r\n  File \"/my/path/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 380, in _MaybeCompile\r\n    xla_compile = op.get_attr(\"_XlaCompile\")\r\n  File \"/my/path/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2331, in get_attr\r\n    raise ValueError(str(e))\r\nValueError: Operation 'conv2d_transpose' has no attr named '_XlaCompile'.\r\n```\r\n\r\nThe error is coming in the line `g = tf.train.AdamOptimizer(0.01).minimize(t)`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler , I have added the required fields. Also, in stackoverflow, I have asked the question here: [Stackoverflow question](https://stackoverflow.com/questions/50756848/xla-compile-error-operation-has-no-attr-named-xlacompile)", "@tatatodd could you take a look?\r\n", "I am really sorry. The problem is happening because of mismatch in the shape of filter and strides associated with it to produce the required output shape with padding. I should have set deconvolution filter shape of [1, 1, 3, 3] and strides at [1, 1, 1, 1] with 'VALID' padding. Then the above example would work.\r\n\r\nMy sincere apologies in wasting the time of developers. "]}, {"number": 19848, "title": "ModuleNotFoundError: No module named 'tensorflow.python.training'", "body": "No Module Found for Tensorflow, \r\n\r\nI have installed tensorflow-gpu(version=1.5) in a conda virtual env and the cuda version 9.1 \r\n\r\nThe packages in the virtual environment  \r\n......................................installed packages......................................\r\nspyder (3.2.4)\r\nSQLAlchemy (1.1.13)\r\nstatsmodels (0.8.0)\r\nsympy (1.1.1)\r\ntables (3.4.2)\r\ntblib (1.3.2)\r\ntensorflow-gpu (1.5.0)\r\ntensorflow-tensorboard (1.5.1)\r\nterminado (0.6)\r\ntestpath (0.3.1)\r\ntoolz (0.8.2)\r\n------------------------------------------------------------------------------------------------\r\n\r\nI run the following program,\r\n`python train_tripletloss.py --logs_base_dir ~/logs/facenet/ --models_base_dir ~/models/facenet/ --data_dir /home/socian/Documents/facenet/data/lfw --image_size 250 --model_def src.models.inception_resnet_v1 --lfw_dir /home/socian/Documents/facenet/data/lfw_182 --optimizer RMSPROP --learning_rate 0.01 --weight_decay 1e-4 --max_nrof_epochs 500`\r\n\r\n\r\n**but it shows the error \r\nTraceback (most recent call last):\r\n  File \"train_tripletloss.py\", line 39, in <module>\r\n    from src import facenet\r\n  File \"/home/tareq/rabindra/facenet/src/facenet.py\", line 37, in <module>\r\n    from tensorflow.python.training import training\r\nModuleNotFoundError: No module named 'tensorflow.python.training'**\r\n\r\nThen I have checkout  the problem in python shell,\r\n\r\n >>> import tensorflow as tf\r\n```\r\n>>> from tensorflow.python.training import training\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow.python.training'\r\n>>> from tensorflow.python.data import dataset\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: cannot import name 'dataset'\r\n>>> \r\n```\r\n\r\n\r\n`\r\n\r\n\r\n\r\n\r\n", "comments": ["Some of the modules have changed in version 1.5 like instead of using \r\n`from tensorflow.python.data import dataset`\r\nyou can use \r\n`tf.data.Dataset`\r\n\r\nFor other API info visit this [link.](https://www.tensorflow.org/versions/r1.5/api_docs/python/tf/data/Dataset)", "Nagging Assignee @reedwm: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "As @nikhilranjan7 stated, internal modules can change between TensorFlow releases. See [this link](https://www.tensorflow.org/programmers_guide/version_compat) for version compatibility info."]}, {"number": 19847, "title": "win10 c++ LNK1189\uff1alibrary limit of 65535 objects exceeded\tpywrap_tensorflow_internal", "body": "**System information:**\r\n\u00b7 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n\r\n\u00b7 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10-x64\r\n\r\n\u00b7 TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/tensorflow.git\r\n\r\n\u00b7 TensorFlow version (use command below): r1.4, r1.5, r1.6 r1.7 and r1.8, command :\r\nv1.8's command : git checkout -b v1.8 -f origin/r1.8\r\nv1.7's command : git checkout -b v1.7 -f origin/r1.7\r\nv1.6's command : git checkout -b v1.6 -f origin/r1.6\r\nand so on include v1.4 and v1.5\r\n\r\n\u00b7 Python version: Anaconda3 - python3.6\r\n\r\n\u00b7 Bazel version (if compiling from source): I used CMAKE 3.11.1\r\n\r\n\u00b7 GCC/Compiler version (if compiling from source): both Visual Studio 2015 and Visual Studio 2015' MSBuild\r\n\r\n\u00b7 CUDA/cuDNN version: CUDA9.0, cudnn-9.0-win10-7.1\r\n\r\n\u00b7 GPU model and memory: GTX-860m with 2Gb Memory\r\n\r\nExact command to reproduce:\r\n\"\r\nD:\\ProgramData\\tensorflow\\tensorflow\\contrib\\cmake\\build> cmake .. -A x64 -DCMAKE_BUILD_TYPE=Debug -DSWIG_EXECUTABLE=D:/soft/TensorflowSoft/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=D:/ProgramData/Anaconda3/python.exe -DPYTHON_LIBRARIES=D:/ProgramData/Anaconda3/libs/python36.lib -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"D:\\soft\\TensorflowSoft\\cudnn\" -G \"Visual Studio 14 2015\"\r\n\r\nD:\\ProgramData\\tensorflow\\tensorflow\\contrib\\cmake\\build> set PreferredToolArchitecture=x64\r\n\r\nD:\\ProgramData\\tensorflow\\tensorflow\\contrib\\cmake\\build> MSBuild /p:Configuration=Release ALL_BUILD.vcxproj\r\n\"\r\n\r\n**Describe the problem :**\r\nI build TensorFlow-CPU-r1.8 successfully (win10-x64 + Anaconda3-python3.6 + VS2017(not VS2015) + tensorflow1.8 + cmake3.11.1 + SwigWin3.0.12).\r\n\r\nBut when I build tensorflow-GPU version(both tensorflow-r1.7 and r1.8), the only error has occurred: \"error LNK1189\uff1alibrary limit of 65535 objects exceeded, pywrap_tensorflow_internal\".\r\n (If I use VS2017, it shows error: \" the compiler is not supported for CUDA 9.0\" , so I switch Visual Studio version to VS2015, and CMAKE command is work successfully.)\r\n\r\nAfter that, I switched to lower versions TensorFlow-r1.6 to TensorFlow-r1.5, but Link error occurred\uff1a\r\n\r\n\u201clibprotobufd.lib(text_format.obj) : error LNK2019: unresolved external symbol __std_reverse_trivially_swappable_8 referenced in function \"void _\r\n_cdecl std::_Reverse_unchecked1<class google::protobuf::Message const * *>(class google::protobuf::Message const * * const,class google::protobuf:\r\n:Message const * * const,struct std::integral_constant<unsigned __int64,8>)\" (??$_Reverse_unchecked1@PEAPEBVMessage@protobuf@google@@@std@@yaxqeap\r\nEBVMessage@protobuf@google@@0u?$integral_constant@_K$07@0@@z) [D:\\tf\\tensorflowGPU\\tensorflow\\contrib\\cmake\\build\\proto_text.vcxproj]\r\nlibprotobufd.lib(wire_format.obj) : error LNK2001: unresolved external symbol __std_reverse_trivially_swappable_8 [D:\\tf\\tensorflowGPU\\tensorflo\r\nw\\contrib\\cmake\\build\\proto_text.vcxproj]\r\nD:\\tf\\tensorflowGPU\\tensorflow\\contrib\\cmake\\build\\Debug\\proto_text.exe : fatal error LNK1120: 1 unresolved externals [D:\\tf\\tensorflowGPU\\tenso\r\nrflow\\contrib\\cmake\\build\\proto_text.vcxproj]\u201d\r\n\r\nI don't know how to solve those errors...", "comments": ["the only error is :\" LNK1189\uff1alibrary limit of 65535 objects exceeded\tpywrap_tensorflow_internal\"", "It should be working OK for our builds on those  branches.\r\n@av8ramit do you know if the bjuilds had a breaking cherrypick?", "@gunan  I build **Debug** version not Release version", "@gunan  I build CPU-Debug OK\uff0cbut **GPU-Debug** had only this Error.", "I have the same problem, successed in CPU-Debug, but failed in GPU-Debug with this error. Have you solved the problems?"]}, {"number": 19846, "title": "Update RELEASE.md for tfdbg bug fix in 1.9.0", "body": "", "comments": ["Can you also merge to 1.9? That way it will get propagated to master."]}, {"number": 19845, "title": "New feature: Large Model Support contrib module for training large models", "body": "This PR proposes a new module, named`lms`, in `contrib`, which helps TensorFlow with training large models that cannot be fit into GPU memory.\r\n \r\nInput is a computational graph defined by users, and our module automatically adds swap-in and swap-out nodes to the graph for transferring tensors from GPUs to the host and vice versa. The computational graph is statically modified. Hence, it needs to be done before a TensorFlow session actually starts.\r\n \r\nWith this PR and a Power machine coupled with P100 NVIDIA GPU (16GB memory), we are able to train ResNet-50 with a mini-batch size of 800 (~4x larger than the one without this PR), [3DUnet]( https://github.com/ellisdg/3DUnetCNN) with full image sizes (192^3 images). Performance degradation is small ranging from 10% to 30% depending on neural networks and mini-batch sizes for training.", "comments": ["OK, this is a pretty large CL. It will take some time to think about this.\r\n\r\nOne quick high-level comment: \"lms\" should be expanded to something like large_model to make it more readable.", "@yuefengz WDYT?", "@yuefengz @benoitsteiner could you comment on this?", "@allenlavoie Allen, could you take a look?", "One high-level question is how this relates to Grappler's memory optimizer. There is a swapping heuristic which is on by default: https://github.com/tensorflow/tensorflow/blob/36568a821a13379b79d74586ff20bdd3e0da102b/tensorflow/core/grappler/optimizers/memory_optimizer.cc#L1109\r\n\r\nIn general we've been leaning toward optimizing things by default when possible rather than providing opt-in utilities. Is there something here we could merge into Grappler to reach more people without requiring them to configure it? (But if there are optimizations too experimental/aggressive to turn on by default, adding an option to RewriterConfig is possible.)\r\n\r\nHaving rewrites in Python is also going to limit their impact significantly. Grappler gets run in quite a few places, for example when defining graph functions while executing eagerly.\r\n\r\nThe second issue is that contrib is going away, and we're trying to reduce rather than increase the number of contrib projects. @martinwicke would have a bit more context on this, but I think the short summary is that we'd prefer things that can't/won't be merged into core to live in their own repos. But if these rewrites could be contributed to Grappler, maybe the documentation/examples could live with Grappler too (rather than in a contrib/ directory)?", "I am sorry this was left lingering for so long, but we will not accept new projects to contrib (see also github.com/tensorflow/community/pull/18). \r\n\r\nWe would prefer this be maintained in its own repo, or merged into grappler. The latter is decidedly preferred since it'll make sure this gets used as much as possible.", "Thank @allenlavoie and @martinwicke for your comments!\r\nWe understand the situation. We will move this PR to our own repository.\r\nIdeas in the PR can be merged into grappler, but at the moment we don't have enough time to do so.", "The use of Power seems really interesting. AFAIK, Power employs NVLink between CPU and GPU so the swapping is significantly faster than PCIe. Could you shed some light on its performance characteristics as most TF users are not familiar with the Power platform? We will be interested to take over this project if we are convinced that it could be more valuable to TF users combined with the alternative hardware platform.", "@martinwicke @allenlavoie  The graph modifications with this are being done statically before the model is run in a session. These graph modifications could likely be done at the grappler level, but we would probably want them turned off by default and have the modification tuneables (number of tensors to swap, how soon to trigger the swap-ins, etc) be set by the RewriterConfig. They were initially written at the Python level to allow faster prototyping, experimentation, and research.\r\n\r\nIn practice we've seen the swapping accomplished by this module far out perform the swapping that was in the memory optimizer in grappler as of TF 1.8.  Using TensorFlow High Performance Models (HPM) we were able to measure the memory gains in terms of both batch size and image size. With Resnet50 and Resnet152 we are able to train with 5x and 4.6x the batch size before running out of memory. We also modified GoogleNet in HPM to allow the image resolution to be changed. We were able to train with 2.5x higher image resolutions before going OOM. Using a 3DUnet model for 3D image segmentation we were able to achieve 2.4x the 3D image resolution.\r\n\r\nTo understand the benefit of moving these modifications to the memory_optimizer in Grappler it would be helpful to know the future role of grappler. TensorFlow 2.0 will likely have eager execution enabled by default. In such a mode, the optimizations in Grappler are N/A, correct? Is Grappler's role in 2.0 going to be limited to \"production\" runs where eager is turned off and the graph is available?  What is the future direction for the other optimizations in Grappler given the general unavailability of the graph in eager mode?\r\n\r\nAs for @byronyi's questions about the POWER architecture, yes, it does have NVIDIA NVLink connections between the CPU and GPU as compared to PCIe Gen3 connections between CPU and GPU that other architectures have. The POWER architecture also has a much faster bus between system memory and the CPU. The combination of these faster buses allows this type of tensor swapping to run with far less overhead than on PCIe connected GPUs. A case study investigating this exists if you want more information about the model accuracy gains this tensor swapping produces with 3D MRIs and how it performs on different architectures.\r\n\r\nhttps://developer.ibm.com/linuxonpower/2018/07/27/tensorflow-large-model-support-case-study-3d-image-segmentation/", "Grappler optimizations will be available in 2.0 for any code which is inside a function (decorated with `@defun`). We believe that will be most code, so graph-level optimizations will definitely be a thing.", "Nagging Assignee @drpngx: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The TensorFlow Large Model Support contribution has been changed to a separate module and placed in its own github repository, https://github.com/IBM/tensorflow-large-model-support", "Nice! Thank you for updating."]}, {"number": 19844, "title": "Added the tutorials link", "body": "The very first time users would like to have a clear navigation where they can find the tutorials regarding the additional resources where they can learn the specific tasks in TensorFlow.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Ignoring unrelated Mac errors:\r\n\r\n```\r\nWARNING: /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/absl_py/WORKSPACE:1: Workspace name in /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/absl_py/WORKSPACE (@io_abseil_py) does not match the name given in the repository's definition (@absl_py); this will cause a build error in future versions\r\nERROR: /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/local_config_cc/BUILD:50:5: in apple_cc_toolchain rule @local_config_cc//:cc-compiler-ios_arm64: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, try: \"bazel clean --expunge\" to re-run Xcode configuration\r\nERROR: /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/local_config_cc/BUILD:50:5: in apple_cc_toolchain rule @local_config_cc//:cc-compiler-darwin_x86_64: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, try: \"bazel clean --expunge\" to re-run Xcode configuration\r\nERROR: /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/local_config_cc/BUILD:50:5: in apple_cc_toolchain rule @local_config_cc//:cc-compiler-ios_i386: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, try: \"bazel clean --expunge\" to re-run Xcode configuration\r\nERROR: /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/local_config_cc/BUILD:50:5: in apple_cc_toolchain rule @local_config_cc//:cc-compiler-ios_x86_64: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, try: \"bazel clean --expunge\" to re-run Xcode configuration\r\nERROR: /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/local_config_cc/BUILD:50:5: in apple_cc_toolchain rule @local_config_cc//:cc-compiler-watchos_armv7k: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, try: \"bazel clean --expunge\" to re-run Xcode configuration\r\nERROR: /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/local_config_cc/BUILD:50:5: in apple_cc_toolchain rule @local_config_cc//:cc-compiler-watchos_i386: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, try: \"bazel clean --expunge\" to re-run Xcode configuration\r\nERROR: /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/local_config_cc/BUILD:50:5: in apple_cc_toolchain rule @local_config_cc//:cc-compiler-ios_armv7: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, try: \"bazel clean --expunge\" to re-run Xcode configuration\r\nERROR: /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/local_config_cc/BUILD:50:5: in apple_cc_toolchain rule @local_config_cc//:cc-compiler-tvos_arm64: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, try: \"bazel clean --expunge\" to re-run Xcode configuration\r\nERROR: /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/local_config_cc/BUILD:50:5: in apple_cc_toolchain rule @local_config_cc//:cc-compiler-tvos_x86_64: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, try: \"bazel clean --expunge\" to re-run Xcode configuration\r\nERROR: /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/local_config_cc/BUILD:50:5: in apple_cc_toolchain rule @local_config_cc//:cc-compiler-armeabi-v7a: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, try: \"bazel clean --expunge\" to re-run Xcode configuration\r\nAnalyzing: 570 targets (299 packages loaded)\r\nAnalyzing: 570 targets (315 packages loaded)\r\nAnalyzing: 570 targets (319 packages loaded)\r\nWARNING: /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/grpc/WORKSPACE:1: Workspace name in /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/grpc/WORKSPACE (@com_github_grpc_grpc) does not match the name given in the repository's definition (@grpc); this will cause a build error in future versions\r\n```"]}, {"number": 19843, "title": " cpu and gpu Dockerfiles for ppc64le", "body": "Adding Dockerfile.cpu.ppc64le and Dockerfile.gpu.ppc64le to enable the ability\r\nto do builds using docker on ppc64le. Also enables the ability to run\r\nci_sanity.sh (from ci_build.sh) on ppc64le.\r\n\r\nModified ci_build.sh and ci_parameterized_build.sh to accept container types\r\nthat start with cpu or gpu.\r\n\r\nAdded install_bazel_from_source.sh and install_buildifier_from_source.sh install\r\nscripts to avoid installing x86 versions of the binaries. These scripts could be\r\nused by other platforms in the future.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I tried to update just 2 files with the correct copyright year, but git drag along a bunch of changes. Let me close and open a new pull request. ", "Note, pull request https://github.com/tensorflow/tensorflow/pull/20102  replaces this pull request"]}, {"number": 19842, "title": "Merge r1.9 back into master.", "body": "", "comments": ["Ready to merge."]}, {"number": 19841, "title": "Consolidate `tf.data` release notes.", "body": "Amit: Let me know if I should put these somewhere else instead.", "comments": []}, {"number": 19840, "title": "TF that is build from r1.9 - crashes with _gru_ops.so: undefined symbol: _ZN15stream_executor6Stream12ThenBlasGemmENS_4blas9TransposeES2_yyyfRKNS_12DeviceMemoryIfEEiS6_ifPS4_i ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I have used unchanged code from branch r1.9, commit e1436b2952c7600c8ac88114210381db0398be16\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:branch r1.9, commit e1436b2952c7600c8ac88114210381db0398be16\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.10.1\r\n- **GCC/Compiler version (if compiling from source)**: 4.8\r\n- **CUDA/cuDNN version**: 9.2/7.2\r\n- **GPU model and memory**: V100, 16Gb\r\n- **Exact command to reproduce**:\r\n\r\n```bash\r\ngit clone https://github.com/tensorflow/benchmarks.git\r\ncd benchmarks/scripts/tf_cnn_benchmarks\r\ngit checkout 551caecb936312690d6bc8c8c2e2562089e2c200\r\npython3 tf_cnn_benchmarks.py --data_format=NCHW --batch_size=256 --num_batches=100 --model=resnet50 --optimizer=momentum --variable_update=replicated --nodistortions --hierarchical_copy=True --gradient_repacking=8 --datasets_use_prefetch=False --display_every=10 --gpu_thread_mode=gpu_shared --num_gpus=8 --use_fp16=True\r\n```\r\n\r\n### Results:\r\n```\r\npython3 tf_cnn_benchmarks.py --data_format=NCHW --batch_size=256 --num_batches=100 --model=resnet50 --optimizer=momentum --variable_update=replicated --nodistortions --hierarchical_copy=True --gradient_repacking=8 --datasets_use_prefetch=False --display_every=10 --gpu_thread_mode=gpu_shared --num_gpus=8 --use_fp16=True\r\n/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"tf_cnn_benchmarks.py\", line 27, in <module>\r\n    import benchmark_cnn\r\n  File \"/home/vkovalevskyi/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 48, in <module>\r\n    import data_utils\r\n  File \"/home/vkovalevskyi/benchmarks/scripts/tf_cnn_benchmarks/data_utils.py\", line 21, in <module>\r\n    from tensorflow.contrib.data.python.ops import batching\r\n  File \"/home/vkovalevskyi/.local/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 35, in <module>\r\n    from tensorflow.contrib import cudnn_rnn\r\n  File \"/home/vkovalevskyi/.local/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/__init__.py\", line 34, in <module>\r\n    from tensorflow.contrib.cudnn_rnn.python.layers import *\r\n  File \"/home/vkovalevskyi/.local/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/__init__.py\", line 23, in <module>\r\n    from tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import *\r\n  File \"/home/vkovalevskyi/.local/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 20, in <module>\r\n    from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\r\n  File \"/home/vkovalevskyi/.local/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 22, in <module>\r\n    from tensorflow.contrib.rnn.python.ops import lstm_ops\r\n  File \"/home/vkovalevskyi/.local/lib/python3.6/site-packages/tensorflow/contrib/rnn/__init__.py\", line 88, in <module>\r\n    from tensorflow.contrib.rnn.python.ops.gru_ops import *\r\n  File \"/home/vkovalevskyi/.local/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/gru_ops.py\", line 33, in <module>\r\n    resource_loader.get_path_to_datafile(\"_gru_ops.so\"))\r\n  File \"/home/vkovalevskyi/.local/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/home/vkovalevskyi/.local/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 56, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/vkovalevskyi/.local/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/_gru_ops.so: undefined symbol: _ZN15stream_executor6Stream12ThenBlasGemmENS_4blas9TransposeES2_yyyfRKNS_12DeviceMemoryIfEEiS6_ifPS4_i\r\n```", "comments": ["Same here under macos following saudet's lead on #19676\r\n\r\n```\r\n$ python3 -c 'import tensorflow.contrib.eager as tfe'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/Users/freewilll/code/machine-learning/tensorflow/venv/lib/python3.4/site-packages/tensorflow/contrib/__init__.py\", line 35, in <module>\r\n    from tensorflow.contrib import cudnn_rnn\r\n  File \"/Users/freewilll/code/machine-learning/tensorflow/venv/lib/python3.4/site-packages/tensorflow/contrib/cudnn_rnn/__init__.py\", line 34, in <module>\r\n    from tensorflow.contrib.cudnn_rnn.python.layers import *\r\n  File \"/Users/freewilll/code/machine-learning/tensorflow/venv/lib/python3.4/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/__init__.py\", line 23, in <module>\r\n    from tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import *\r\n  File \"/Users/freewilll/code/machine-learning/tensorflow/venv/lib/python3.4/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 20, in <module>\r\n    from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\r\n  File \"/Users/freewilll/code/machine-learning/tensorflow/venv/lib/python3.4/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 22, in <module>\r\n    from tensorflow.contrib.rnn.python.ops import lstm_ops\r\n  File \"/Users/freewilll/code/machine-learning/tensorflow/venv/lib/python3.4/site-packages/tensorflow/contrib/rnn/__init__.py\", line 88, in <module>\r\n    from tensorflow.contrib.rnn.python.ops.gru_ops import *\r\n  File \"/Users/freewilll/code/machine-learning/tensorflow/venv/lib/python3.4/site-packages/tensorflow/contrib/rnn/python/ops/gru_ops.py\", line 33, in <module>\r\n    resource_loader.get_path_to_datafile(\"_gru_ops.so\"))\r\n  File \"/Users/freewilll/code/machine-learning/tensorflow/venv/lib/python3.4/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/Users/freewilll/code/machine-learning/tensorflow/venv/lib/python3.4/site-packages/tensorflow/python/framework/load_library.py\", line 56, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/freewilll/code/machine-learning/tensorflow/venv/lib/python3.4/site-packages/tensorflow/contrib/rnn/python/ops/_gru_ops.so, 6): Symbol not found: __ZN15stream_executor6Stream12ThenBlasGemmENS_4blas9TransposeES2_yyydRKNS_12DeviceMemoryIdEEiS6_idPS4_i\r\n  Referenced from: /Users/freewilll/code/machine-learning/tensorflow/venv/lib/python3.4/site-packages/tensorflow/contrib/rnn/python/ops/_gru_ops.so\r\n  Expected in: flat namespace\r\n in /Users/freewilll/code/machine-learning/tensorflow/venv/lib/python3.4/site-packages/tensorflow/contrib/rnn/python/ops/_gru_ops.so\r\n ```", "same bug on RC1:\r\n\r\n```\r\nTraceback (most recent call last):\r\n File \"tf_cnn_benchmarks.py\", line 27, in <module>\r\n   import benchmark_cnn\r\n File \"/home/builder/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 48, in <module>\r\n   import data_utils\r\n File \"/home/builder/benchmarks/scripts/tf_cnn_benchmarks/data_utils.py\", line 21, in <module>\r\n   from tensorflow.contrib.data.python.ops import batching\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/__init__.py\", line 35, in <module>\r\n   from tensorflow.contrib import cudnn_rnn\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/__init__.py\", line 34, in <module>\r\n   from tensorflow.contrib.cudnn_rnn.python.layers import *\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/__init__.py\", line 23, in <module>\r\n   from tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import *\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 20, in <module>\r\n   from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 22, in <module>\r\n   from tensorflow.contrib.rnn.python.ops import lstm_ops\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/rnn/__init__.py\", line 88, in <module>\r\n   from tensorflow.contrib.rnn.python.ops.gru_ops import *\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/gru_ops.py\", line 33, in <module>\r\n   resource_loader.get_path_to_datafile(\"_gru_ops.so\"))\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n   ret = load_library.load_op_library(path)\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py\", line 56, in load_op_library\r\n   lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/_\r\ngru_ops.so: undefined symbol: _ZN15stream_executor6Stream12ThenBlasGemmENS_4blas9TransposeES2_yyyfRKNS_12DeviceMemoryIfEEiS6_ifPS4_i\r\nTraceback (most recent call last):\r\n```", "I was not able to reproduce in the Docker image [nvidia/cuda:9.2-cudnn7-devel-ubuntu18.04](https://hub.docker.com/r/nvidia/cuda/) with identical settings. tf_cnn_benchmarks worked fine when i ran it.\r\n\r\n@gunan, any ideas what the issue could be?", "No idea off the top of my head.\r\nAre you using TF built from sources, or are you using any of our release packages?", "@reedwm @gunan I was building from source. I have recreated builder machine and now it works fine (looks like). Not sure what was the root case on the original builder. Probably some strange combination of dependancies that was impacting 1.9 but not 1.8.", "Closing, since looks like latest r1.9 works for me.", "reopening, same on v1.9.0-rc2:\r\n\r\n/usr/lib/python2.7/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated.\r\nIn future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n File \"tf_cnn_benchmarks.py\", line 27, in <module>\r\n   import benchmark_cnn\r\n File \"/home/builder/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 48, in <module>\r\n   import data_utils\r\n File \"/home/builder/benchmarks/scripts/tf_cnn_benchmarks/data_utils.py\", line 21, in <module>\r\n   from tensorflow.contrib.data.python.ops import batching\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/__init__.py\", line 36, in <module>\r\n   from tensorflow.contrib import cudnn_rnn\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/__init__.py\", line 34, in <module>\r\n   from tensorflow.contrib.cudnn_rnn.python.layers import *\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/__init__.py\", line 23, in <module>\r\n   from tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import *\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 20, in <module>\r\n   from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 22, in <module>\r\n   from tensorflow.contrib.rnn.python.ops import lstm_ops\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/rnn/__init__.py\", line 89, in <module>\r\n   from tensorflow.contrib.rnn.python.ops.gru_ops import *\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/gru_ops.py\", line 33, in <module>\r\n   resource_loader.get_path_to_datafile(\"_gru_ops.so\"))\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n   ret = load_library.load_op_library(path)\r\n File \"/home/builder/.local/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py\", line 56, in load_op_library\r\n   lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/builder/.local/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/_gru_ops.so: undefined s\r\nymbol: _ZN15stream_executor6Stream12ThenBlasGemmENS_4blas9TransposeES2_yyyfRKNS_12DeviceMemoryIfEEiS6_ifPS4_i\r\nTraceback (most recent call last):", "Which commit worked (you said some r1.9 worked)? ", "And to confirm, this is gcc 4.8, CUDA 9.2, Ubuntu 18.04 still", "@martinwicke my mistake it did not work before :(\r\n\r\nour updated builder:\r\nCUDA 9.2, \r\nGCC: Ubuntu 5.4.0-6ubuntu1~16.04.10\r\nUbuntu 16.04.10", "I can provide binary if needed", "I have also similar problem with tensorflow 1.8.0, whenever I import/use tensorflow.contrib.\r\n\r\n**Code to generate error,**\r\nimport tensorflow.contrib\r\n\r\n**My system has,**\r\nCUDA 9.0\r\nUbuntu 16.04.4 LTS\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n", "I will close the bug, I was able to correctly compile the TF 1.9, now bazel build requires different way of triggering the build in order to find all the required CUDA libraries, on top of this monolithic build almost always shows that build is ok even though some of the libs were not found. Combinations of these 2 things makes it hard to identify the root cause and quickly come up with the solution.", "Same error at latest upload: \r\n\r\nhttps://github.com/tensorflow/serving/blob/012e0b992d4b6ff003e0bc02226681ae526c9059/tensorflow_serving/tools/docker/Dockerfile.devel-gpu\r\n\r\nIs there some way to build from a tag that is stable? \r\n\r\n", "I am trying 1.9.0 https://raw.githubusercontent.com/tensorflow/serving/1.9.0/tensorflow_serving/tools/docker/Dockerfile.devel-gpu \r\n\r\nHad to make the following changes: set the branch to r1.9\r\nComment out the stubs to prevent cuInit fails like in https://github.com/tensorflow/serving/issues/1015\r\n```\r\n...\r\nARG TF_SERVING_VERSION_GIT_BRANCH=r1.9\r\n...\r\n#RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1\r\n#ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs:${LD_LIBRARY_PATH}\r\n...\r\n```\r\nthen \r\n```\r\nln -s Dockerfile.devel-gpu Dockerfile\r\nnvidia-docker build -t tensorflow-serving-r1.9-gpu .\r\n```\r\nThis leads to a fully working build! \r\n\r\nPlease, any way of building that requires less manual intervention to create a stable GPU build is welcome!", "@rdwrt It won't build \r\n\r\n> E: Packages were downgraded and -y was used without --allow-downgrades.\r\n", "Maybe with a --no-cache when running the docker build.\n\nOn Thu, Jul 26, 2018, 12:21 Alex Chaplianka <notifications@github.com>\nwrote:\n\n> @rdwrt <https://github.com/rdwrt> It won't build\n>\n> E: Packages were downgraded and -y was used without --allow-downgrades.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19840#issuecomment-408049780>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAFJrw5fXg3ezvIWncPK2pYo09-mlsvBks5uKZgSgaJpZM4Ue44I>\n> .\n>\n", "I had the same error in the title earlier when I built (on CentOS7 with CUDA 9.0) with --config=monolithic, after removing the flag the problem went away. The installation steps are documented in https://gist.github.com/gentaiscool/a628fab5cd98953af7f46b69463394b3", "I'm getting the same error, not related to tf-serving. Just building tensorflow with\r\n```\r\nbazel build \\\r\n  --config=opt \\\r\n  --config=cuda \\\r\n  --config=mkl \\\r\n  //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nspecs:\r\n- Bazel 0.15.2\r\n- Python 3.5.5\r\n- TensorFlow 1.9 and 1.10 (tried both, same result)\r\n- Ubuntu 16.04\r\n- gcc 5.4.0\r\n- cuda 9.2\r\n- with and without --config=monolithic (tried both)\r\n\r\nError after successful build:\r\n```\r\nIn [4]: import tensorflow.contrib\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-4-9ce6c8f43c36> in <module>()\r\n----> 1 import tensorflow.contrib\r\n...\r\nNotFoundError: ~/miniconda3/envs/35/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/_lstm_ops.so: undefined symbol: _ZN15stream_executor6Stream12ThenBlasGemmENS_4blas9TransposeES2_yyyfRKNS_12DeviceMemoryIfEEiS6_ifPS4_i\r\n```\r\n\r\n\r\nNot sure if this is relevant but I get this warning when the tensorflow build starts:\r\n`WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.`\r\n\r\n\r\n\r\nEdit: I found that I only get this error when building with TensorRT support. I'm using TensorRT 4.0.1.6-1+cuda9.2\r\n\r\nEdit: Problem solved by install pycuda with `pip install pycuda` before importing tensorflow.contrib", "> I will close the bug, I was able to correctly compile the TF 1.9, now bazel build requires different way of triggering the build in order to find all the required CUDA libraries, on top of this monolithic build almost always shows that build is ok even though some of the libs were not found. Combinations of these 2 things makes it hard to identify the root cause and quickly come up with the solution.\r\n\r\n@b0noI : could you please elaborate on that? What is the new different way that bazel now requires?\r\n(instructions in https://www.tensorflow.org/install/install_sources seem to be unchanged.)\r\n\r\nWe also hit exactly this error with `--monolithic` builds; we are forced to do this because of problematic libprotobuf behavior when loaded by different shared libraries (like described in https://github.com/protocolbuffers/protobuf/issues/1489#issuecomment-217241765). \r\n\r\n(Perhaps there exists an easier solution which would link only protobuf statically, we haven't really looked carefully yet at the corresponding tf-bazel rules; if someone knows a way to do this off top of their head, I would really appreciate a hint!)", "I also encountered this on a monolithic build of TensorFlow `v1.11.0`.\r\n\r\nIt's pretty easily and consistently reproducible:\r\n- Build within an official tf docker image (eg: `tensorflow/tensorflow:1.11.0-devel-gpu`) with `--monolithic`\r\n- Install the pip package\r\n- Import `tensorflow.contrib` --> observe it fail due to missing symbols\r\n\r\nInvestigating a bit, I found that a simple fix is to add the following entry to [tf_version_script.lds](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tf_version_script.lds):\r\n\r\n```\r\n*stream_executor*;\r\n```", "@ethereon the right way!", "@ethereon Thanks, trying that patch out with our tensorflow debian packages for Pop!\\_OS", "@ethereon's workaround worked for me (v1.11, cuda+monolithic+mkl+opt)"]}, {"number": 19839, "title": "Branch 199651315", "body": "", "comments": []}, {"number": 19838, "title": "optimize_for_inference_lib.optimize_for_inference produces an invalid graph", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource.\r\n- **TensorFlow version (use command below)**:\r\n1.8.0\r\n- **Python version**: \r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n0.13.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)\r\n- **CUDA/cuDNN version**:\r\n9.0.176/7.0.5.15\r\n- **GPU model and memory**:\r\n1080 Ti\r\n- **Exact command to reproduce**:\r\n```\r\ncat > test.py <<EOF && python test.py\r\nimport tensorflow as tf\r\n\r\nfrom collections import namedtuple\r\nfrom tensorflow.python.tools import optimize_for_inference_lib\r\n\r\n\r\ndef main():\r\n    with tf.Graph().as_default(), tf.Session() as session:\r\n        input = tf.placeholder(shape=[10], dtype=tf.float32)\r\n        output = top_k(input)\r\n\r\n        graph_def = session.graph.as_graph_def()\r\n\r\n    input_nodes = [input]\r\n    output_nodes = [output.values, output.indices]\r\n\r\n    graph_def = tf.graph_util.convert_variables_to_constants(\r\n        session, graph_def, [_get_node_name(t) for t in output_nodes]\r\n    )\r\n\r\n    with tf.Graph().as_default():\r\n        tf.import_graph_def(graph_def)  # OK\r\n\r\n    graph_def = optimize_for_inference_lib.optimize_for_inference(\r\n        input_graph_def=graph_def,\r\n        input_node_names=[_get_node_name(t) for t in input_nodes],\r\n        output_node_names=[_get_node_name(t) for t in output_nodes],\r\n        placeholder_type_enum=[node.dtype.as_datatype_enum for node in input_nodes]\r\n    )\r\n\r\n    with tf.Graph().as_default():\r\n        tf.import_graph_def(graph_def)  # ERROR\r\n\r\n\r\nTopKResult = namedtuple('TopKResult', ['values', 'indices'])\r\n\r\n\r\ndef top_k(input, k=1, sorted=True, name=None):\r\n    \"\"\"\r\n    A version of tf.nn.top_k tolerant to k == 0 and k < tf.shape(input)[-1].\r\n    \"\"\"\r\n    k = tf.minimum(k, tf.shape(input)[-1])\r\n\r\n    return tf.cond(\r\n        tf.equal(k, 0),\r\n        lambda: TopKResult(\r\n            values=tf.zeros(\r\n                shape=tf.concat([tf.shape(input)[:-1], [0]], axis=0),\r\n                dtype=input.dtype\r\n            ),\r\n            indices=tf.zeros(\r\n                shape=tf.concat([tf.shape(input)[:-1], [0]], axis=0),\r\n                dtype=tf.int32\r\n            )\r\n        ),\r\n        lambda: TopKResult(**tf.nn.top_k(input, k, sorted, name)._asdict())\r\n    )\r\n\r\n\r\ndef _get_node_name(tensor):\r\n    assert tensor.name.endswith(':0')\r\n    return tensor.name[:-len(':0')]\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\nEOF\r\n```\r\n\r\n### Describe the problem\r\n\r\n`optimize_for_inference_lib.optimize_for_inference` produces an invalid graph for the graph generated by the above script. The returned GraphDef cannot be imported:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 66, in <module>\r\n    main()\r\n  File \"test.py\", line 32, in main\r\n    tf.import_graph_def(graph_def)  # ERROR\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 493, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: import/cond/zeros_1/Const = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: 0>](import/cond/Switch:1)\r\n```\r\n", "comments": ["I met the same problem when i use the .pb produced by quantize_graph ", "same problem with latest tensorflow", "optimized it using optimize_for_inference (works error)\r\n\r\n```\r\n${ROOT}/bazel-bin/tensorflow/python/tools/optimize_for_inference  \\\r\n--input=$graph \\\r\n--output=optimized_for_inference_graph.pb \\\r\n--frozen_graph=True \\\r\n--input_names=\"input\" \\\r\n--output_names=\"softmax_output\"\r\n\r\n$ ./summarize_graph.sh optimized_for_inference_graph.pb\r\nFound 1 possible inputs: (name=input, type=float(1), shape=None)\r\nNo variables spotted.\r\nFound 1 possible outputs: (name=softmax_output, op=Softmax)\r\nFound 3156821 (3.16M) const parameters, 0 (0) variable parameters, and 0 control_edges\r\nOp types used: 221 Const, 28 Mul, 27 Reshape, 26 Pack, 25 Add, 18 Sub, 14 ConcatV2, 12 Range, 10 StridedSlice, 10 Cast, 8 Transpose, 8 FloorDiv, 8 GatherV2, 8 Less, 7 Enter, 7 RealDiv, 6 Shape, 6 Prod, 6 MatMul, 5 BiasAdd, 5 Fill, 4 Log, 4 Maximum, 4 ExpandDims, 3 ListDiff, 3 Conv2D, 3 Merge, 3 Select, 3 NextIteration, 3 GreaterEqual, 3 Switch, 2 SplitV, 2 Softmax, 2 Size, 2 Rsqrt, 2 FusedBatchNorm, 2 Pad, 2 Neg, 2 LinSpace, 2 TensorArrayV3, 2 Mean, 2 MaxPool, 1 TensorArrayScatterV3, 1 TensorArrayReadV3, 1 TensorArrayGatherV3, 1 TensorArraySizeV3, 1 Sum, 1 TensorArrayWriteV3, 1 Cos, 1 ComplexAbs, 1 Square, 1 Split, 1 Sigmoid, 1 Exit, 1 FloorMod, 1 RandomStandardNormal, 1 RFFT, 1 Placeholder, 1 PadV2, 1 Minimum, 1 LoopCond, 1 LogicalAnd\r\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\r\nbazel run tensorflow/tools/benchmark:benchmark_model -- --graph=optimized_for_inference_graph.pb --show_flops --input_layer=input --input_layer_type=float --input_layer_shape= --output_layer=softmax_output\r\n\r\n ./summarize_graph.sh frozen_graph.pb\r\nFound 1 possible inputs: (name=input, type=float(1), shape=[?,480000])\r\nNo variables spotted.\r\nFound 1 possible outputs: (name=softmax_output, op=Softmax)\r\nFound 3156821 (3.16M) const parameters, 0 (0) variable parameters, and 129 control_edges\r\nOp types used: 221 Const, 32 Identity, 28 Mul, 27 Reshape, 26 Pack, 25 Add, 18 Sub, 14 ConcatV2, 12 Range, 10 StridedSlice, 10 Cast, 8 FloorDiv, 8 GatherV2, 8 Transpose, 8 Less, 7 Enter, 7 RealDiv, 6 Shape, 6 Prod, 6 MatMul, 5 BiasAdd, 5 Fill, 4 Log, 4 Maximum, 4 ExpandDims, 3 ListDiff, 3 Conv2D, 3 Merge, 3 Select, 3 NextIteration, 3 GreaterEqual, 3 Switch, 2 SplitV, 2 Softmax, 2 Size, 2 Rsqrt, 2 FusedBatchNorm, 2 Pad, 2 Neg, 2 LinSpace, 2 TensorArrayV3, 2 Mean, 2 MaxPool, 1 TensorArrayScatterV3, 1 TensorArrayReadV3, 1 TensorArrayGatherV3, 1 TensorArraySizeV3, 1 Sum, 1 TensorArrayWriteV3, 1 Cos, 1 ComplexAbs, 1 Square, 1 Split, 1 Sigmoid, 1 Exit, 1 FloorMod, 1 RandomStandardNormal, 1 RFFT, 1 Placeholder, 1 PadV2, 1 Minimum, 1 LoopCond, 1 LogicalAnd\r\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\r\n```\r\n\r\nAfter using optimized_for_inference bin, the input shape changed. tf version 1.9", "I observed a similar problem. The following code works perfectly well with `use_optimize_for_inference = False` but gives the error \r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node optimized/cond/add/y}} = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 1>](optimized/cond/Switch:1)\r\n```\r\nif I use the optimize for inference routine. The code:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.tools import optimize_for_inference_lib\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\n\r\nuse_optimize_for_inference = True\r\n\r\n# Set up a simple test graph\r\na = tf.Variable(tf.ones(3, dtype=tf.float32))\r\nx = tf.placeholder(tf.float32, [3], name='x')\r\nswitch = tf.placeholder(tf.bool, name='switch')\r\n\r\ny = a * x\r\ny = tf.cond(switch, lambda: y + 1.0, lambda: y)\r\ny = tf.identity(y, 'result')\r\n\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    # Save the graph to disk\r\n    tf.train.write_graph(sess.graph.as_graph_def(), '.', 'model.pbtxt', as_text=True)\r\n    saver.save(sess, './model.ckpt')\r\n\r\n    # Freeze the trained model\r\n    output_graph = 'frozen_model.pb'\r\n    freeze_graph.freeze_graph('./model.pbtxt', '', False, './model.ckpt',\r\n                              'result', 'save/restore_all', 'save/Const:0', output_graph, True, '')\r\n\r\n    # Reload the frozen graph from disk\r\n    frozen_graph = tf.GraphDef()\r\n    with tf.gfile.Open(output_graph, \"rb\") as f:\r\n        frozen_graph.ParseFromString(f.read())\r\n\r\n    # optimize for inference\r\n    input_names = ['x', 'switch']\r\n    output_names = ['result']\r\n    new_graph = optimize_for_inference_lib.optimize_for_inference(\r\n        frozen_graph, input_names, output_names, [tf.float32.as_datatype_enum, tf.bool.as_datatype_enum])\r\n\r\n    # Write the optimized graph to disk\r\n    output_graph_inference = 'frozen_model_inference.pb'\r\n    with tf.gfile.GFile(output_graph_inference, \"wb\") as f:\r\n        f.write(new_graph.SerializeToString())\r\n\r\n    # Attempt loading the frozen graph: This fails if we optimize the graph for inference\r\n    print('Loading the saved graph...')\r\n    optimized_graph_def = tf.GraphDef()\r\n    with tf.gfile.GFile(output_graph_inference if use_optimize_for_inference else output_graph, \"rb\") as f:\r\n        optimized_graph_def.ParseFromString(f.read())\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(optimized_graph_def, name=\"optimized\")\r\n\r\n\r\n```\r\nThe problem is either related to the use of the boolean variable or the conditional. I observed it both in Tensorflow 1.11.0 (Python 3.7) and 1.8.0 (Python 3.6). For now, my solution is to just not use the optimize for inference script, as it doesn't benefit me that much in my use....", "> \r\n> \r\n> I met the same problem when i use the .pb produced by quantize_graph\r\n\r\nHi kasyoukin,\r\nI also met the same issue when use the .pb produced by quantize_graph.\r\nDid you find the solution?\r\n", "I met exactly the same problem and it seems like that the problem is caused by `tf.cond`.", "Same problem, looks like the dropout node. I was also trying to use an optimized  graph. It looks like I can used an unoptimized graph just fine.\r\n```\r\nValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: prefix/dense_0/Dropout/cond/dropout/keep_prob = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 0.2>](prefix/dense_0/Dropout/cond/Switch:1)\r\n```", "I could reproduce the issue with TF1.13.1. Thanks!\r\nHere is the error log\r\n\r\n```\r\nINFO:tensorflow:Restoring parameters from ./model.ckpt\r\nINFO:tensorflow:Froze 1 variables.\r\nINFO:tensorflow:Converted 1 variables to const ops.\r\nLoading the saved graph...\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\r\n    425         results = c_api.TF_GraphImportGraphDefWithResults(\r\n--> 426             graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\n    427         results = c_api_util.ScopedTFImportGraphDefResults(results)\r\n\r\nInvalidArgumentError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node optimized/cond/add/y}}\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\r\n    428       except errors.InvalidArgumentError as e:\r\n    429         # Convert to ValueError for backwards compatibility.\r\n--> 430         raise ValueError(str(e))\r\n    431 \r\n    432     # Create _DefinedFunctions for any imported functions.\r\n\r\nValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node optimized/cond/add/y}}\r\n```", "In my opinion  it caused by tf.cond(),but without any solutions,unless remove this node.", "Facing the a similar issue when using LSTM in my model. Any leads so far?\r\n\r\nValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node lstm_1/while/add/y}} = Const[_output_shapes=[[]], dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: 1>](lstm_1/while/Switch:1)", "I met the same problem(\"**do not match 1 inputs specified...***\") yesterday. The reason seem to be related \"tf.graph_util.remove_training_nodes\"(or this function wrapped in \"tensorflow.python.tools\").", "I am unable to repro this in 1.14 for the examples posted by @yegord and @dvicini. Please reopen with a repro if this is still not working for you. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=19838\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=19838\">No</a>\n", "> I am unable to repro this in 1.14 for the examples posted by @yegord and @dvicini. Please reopen with a repro if this is still not working for you.\r\n\r\nWhat about Tf 1.13.1  ? \r\n", "entity_tagging/bidirectional_rnn/fw/fw/cond/fw/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul/Switch' expects to be colocated with unknown node 'entity_tagging/bidirectional_rnn/fw/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/read'\r\n\r\nwhen use quantize tools cause this problem\r\nsolved this problem by delete graph_util.remove_training_nodes function", "reproduct the it:\r\n1. train a model by tf/models/official/transformer\r\n2. do freeze_graph for the model, which created at step 1\r\n3. try to import the graph_def, which created at step 2\r\n\r\n```\r\n# step 3 code\r\ninput_tensor = tf.placeholder(tf.int64, shape=(1, None), name='input_tensor')\r\nwith tf.gfile.GFile(model_file, 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    with tf.Session() as sess:\r\n        tf.import_graph_def(graph_def,{\"input_tensor\":input_tensor},name=\"\")\r\n```", "I'm having the same issue. For me it seems to occur in ```tf.keras.layers.BatchNormalization```. When I pass the bool ```training=is_training``` to the call operator the error dissapears. Any idea on why this happens?", "use tensorflow 1.14.0", "Same issue with TF 1.13.1 caused by batch_norm. \r\n`ValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node import/model/conv1/batch_norm/cond/Const}}`\r\nDisappear with TF 1.14.0"]}, {"number": 19837, "title": "freeze_graph for inference memory leak: frozen graph size 1.3GB, takes 15GB memory when inference", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (details below)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.3 \r\n- **TensorFlow installed from (source or binary)**:binary (via `pip install`)\r\n- **TensorFlow version (use command below)**:1.8.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI want to train a graph, save it, then load it later for inference-only purpose.\r\nThe way I'm doing it now:\r\nDuring training, I save the model with `tf.graph_util.convert_variables_to_constants` method and `tf.train.write_graph` which produces a `.pb` file. Its size is around 1.3 GB. \r\nThen I load the frozen graph and do some inference with some toy data. This causes memory issue by **a factor of 10**. (In activity monitor, Python takes ~15GB on the first inference)\r\n\r\nI've also tried the \"saving-loading-inference\" procedure with \r\n -`saver.save()` \r\n -`saver.restore()` \r\nand it doesn't cause the memory issue.  However, for production purpose on my end, loading a frozen graph would be strongly preferred to the \r\n\r\n### Source code / logs\r\n\r\n#### Saving graph  (for debugging purpose, I SKIPPED training and save the graph once variables are initialized)\r\n```\r\n## build the graph\r\nx = tf.placeholder(..)\r\n...\r\nsome tf operations to build graph\r\n...\r\npred = tf.nn.xw_plus_b(last_layer, W,b, \"predictions\")\r\n\r\n##save graph\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer()))\r\ngraph_as_constants = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(),\r\n                                      output_node_names=[\"predictions\"])\r\ntf.train.write_graph(graph_as_constants, model_path, as_text=False, name=\"model_graph\")\r\nsess.close()\r\n```\r\n\r\n#### Load graph\r\n```\r\nwith gfile.FastGFile(model_path, 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    tf.import_graph_def(graph_def, name='')\r\ngraph = tf.get_default_graph()\r\n```\r\n\r\n#### Try inference on some toy data:\r\n```\r\nx = graph.get_tensor_by_name(\"x:0\")\r\npred = graph.get_tensor_by_name(\"predictions:0\")\r\nwith tf.Session() as sess:\r\n    for _ in range(10):\r\n        x_test = some np array for testing\r\n        test_result = sess.run(pred, feed_dict={x:x_test})\r\n        time.sleep(30)\r\n```\r\n\r\nHere in inference, it goes through a loop with 10 iterations. In the first iteration, the sess.run(..) causes the memory leak issue (15 GB). In following iterations, memory falls back to what I would expect with the size of the model loaded.\r\n\r\n### Some details\r\n\r\n- The issue persists whether I save the frozen graph on CPU machine (macOS High Sierra 10.13.3 ) or a remote GPU machine (Ubuntu 16.04.3 LTS). \r\n- Again the graph is frozen once it's initialized. I skipped all training/testing logic for debugging purpose.\r\n- For freeze graph,  I also tried `freeze_graph` in `bazel-bin` in terminal. It still causes memory issue when loading and doing inference.\r\n- I've tried `optimize_for_inference(graph_def,...)` in the graph loading section. It doesn't solve the issue.\r\n", "comments": ["I have the same issue! Freezing and optimizing does not help", "waiting\r\n", "Nagging Assignee @jart: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hello @Camuslu , There should be some posts on Stack Overflow that address the memory problems the freeze_graph and load model .pb file: \r\n[Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 19836, "title": "Inconsistency between layer names and weight names in HDF5 saved from tf.keras nested models", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Doesn't matter\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**:  Doesn't matter\r\n\r\nSupposed you run the following code to construct a nested Sequential model with tf.keras and save it as a .h5 file.\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\n\r\nwith tf.Graph().as_default(), tf.Session():\r\n  inner_model = tf.keras.Sequential([\r\n      tf.keras.layers.Dense(4, input_shape=[3], activation='relu'),\r\n      tf.keras.layers.Dense(3, activation='tanh')])\r\n  outer_model = tf.keras.Sequential()\r\n  outer_model.add(inner_model)\r\n  outer_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\r\n  outer_model.compile(loss='binary_crossentropy', optimizer='sgd')\r\n\r\n  outer_model.save('/tmp/tf_nested_keras.h5')\r\n```\r\n\r\nInside the saved .h5 file, you can see the layers have the names:\r\n* 'dense_1',\r\n* 'dense_2' \r\n* 'dense_3'.\r\n\r\nHowever, the weight names do not match up with the layer names:\r\n* 'dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0',\r\n\r\nThis is causing issues for TensorFlow.js converters. Note that the same issue does not occur for non-tf keras.\r\n", "comments": ["cc @bileschi ", "I think the issue has been fixed in 693b339a", "@facaiy Thanks for the info. Yes, I can confirm that this issue is resolved in 1.9.0rc0. Therefore I will close this issue now."]}, {"number": 19835, "title": "tensorflow/contrib/lite module tests are failed to build", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NA\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary)**:\r\nFrom source\r\n- **TensorFlow version (use command below)**:\r\nTF-master\r\n- **Python version**: \r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n- **CUDA/cuDNN version**: \r\nNA\r\n- **GPU model and memory**:\r\nNA\r\n- **Exact command to reproduce**:\r\n`bazel test -c opt  -k --jobs 1 test_timeout 300,450,1200,3600 --build_tests_only -- //tensorflow/... -//tensorflow/compiler/...  `\r\n\r\n### Describe the problem\r\nThe TF-master build completed successfully.However while running the tests , around 80+ tests are failed to build. All of the failed tests are belongs to `tensorflow/contrib/lite `module.\r\nNow , I'm trying to understand the reason. But, if anyone knows these are known failures then please let me know.  \r\nAlso, I would like to understand more about `tensorflow/contrib/lite ` module  - Is it supported on our platform or do we need to skip the tests for this module ? or need special configuration to run the tests?\r\n\r\nI'm using below configuration to build and test TF-master :\r\nEnabled only 3 features i.e. TF_NEED_GCP=1 , TF_NEED_HDFS=1 & TF_NEED_JEMALLOC=1 , and all other are disabled. \r\n```\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: y\r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: y\r\nGoogle Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: y\r\nHadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n\r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -mcpu=native]: \"-mcpu=power8 -mtune=power8\"\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\nConfiguration finished\r\n```\r\n\r\n\r\n### Source code / logs\r\nFor more details please check the logfile.txt.\r\n\r\n[logfile.txt](https://github.com/tensorflow/tensorflow/files/2080578/logfile.txt)\r\n", "comments": ["@skye @gunan Could you please provide your comments on this.Thanks!", "I see this happening, I am guessing other failures are similar:\r\n```\r\ntensorflow/contrib/lite/tools/verifier_test.cc:44:50: error: 'string' was not declared in this scope\r\n```\r\n@angersson @andrehentz any ideas?", "Not sure what's going on. That file certainly has \"#include <string>\", and all the references seems to be to std::string. ", "I saw this happen before when the namespacing was off. \"string\" is only defined inside of certain tflite and tensorflow namespaces. Could the failing code be outside of those?", "https://github.com/tensorflow/tensorflow/commit/980c390941853649bb56c4940a46f474eb97ed80 should fix a good bunch of these.\r\n@sandipmgiri do you have logs from a more recent run?", "I had a PR that make all tflite **kernel tests** work https://github.com/tensorflow/tensorflow/pull/17700. Updated and added more fixes for new ones.", "@gunan Yesterday I re-ran the test command, please find the latest logfile. \r\nLooks like the issue is not resolved yet.\r\n\r\n[latest_logfile.txt](https://github.com/tensorflow/tensorflow/files/2104823/latest_logfile.txt)\r\n", "Looks like most of the problems are due to namespace ambiguities. The solution to these would be to replace the lines referenced in error messages to use symbols from std namespace.\r\nThis will need exhaustive iteration through the failures, fix one failure, then rerun build, then fix another failure. For example, the first issue I see is \"cerr\", the fix should be as simple as replacing the line with `std:cerr`\r\nSince we do not have access to a ppc64le machine, would you like to contribute the fix?", "Looks like we are getting same failures on X86 as well, PFA logfile_x86.txt.\r\nBut will try to fix the failures one by one, as you suggested.\r\n[logfile_x86.txt](https://github.com/tensorflow/tensorflow/files/2110045/logfile_x86.txt)\r\n", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing , since the same failure observed on X86 as well."]}, {"number": 19834, "title": "Attention Wrapper state error.", "body": "Hello! I am implementing a seq2seq model with attention for multi-step time series forecasting. I am encountering an error with implementing attention for MultiRNN cell.\r\n\r\nSystem Information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:   Yes , I have attached my code below.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)** - Linux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**: - Source\r\n\r\n- **TensorFlow version (use command below)**: -VERSION = 1.4.1\r\n\r\n- **Python version**: 2.7.12\r\n\r\n- **Bazel version (if compiling from source)**:  - NA\r\n\r\n- **GCC/Compiler version (if compiling from source)**: COMPILER_VERSION = v1.4.0-19-ga52c8d9\r\n\r\n- **CUDA/cuDNN version**: - NA\r\n\r\n- **GPU model and memory**: - NA\r\n\r\n- **Exact command to reproduce**:\r\nI am implementing a seq2seq model with attention for multi-step time series forecasting.\r\n\r\nI am facing an issue with attention wrapper state.\r\n\r\nMy code is given below.\r\n\r\nFunction for defining the decoder \r\n\r\n\r\n    def decoder_network(attention_mechanism,dropout=config.dropout ):\r\n        cells = []\r\n        for i in range(config.num_stacked_layers)\r\n    \r\n            lstm_cell = tf.contrib.rnn.LSTMCell(config.hidden_dim)\r\n            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(lstm_cell, attention_mechanism=attention_mechanism, attention_layer_size=config.hidden_dim)\r\n            cells.append(decoder_cell)\r\n    \r\n    \r\n        cell = tf.contrib.rnn.MultiRNNCell(cells)\r\n        return cell,cells\r\n\t\t\r\n\t\t\r\n\r\nUsing the above function\r\n\r\n    cell,cells = decoder_network(attention_mechanism)\r\n    new_state = cells[0].zero_state(batch_size=batch_size, dtype=tf.float32).clone(cell_state = initial_state)\r\n\r\n\tfor i, inp in enumerate(decoder_inputs):\r\n\t      output, new_state = cell(inp, state=new_state)\r\n\t\r\n\r\n\r\nHowever, I am facing the following error :\r\n\r\n    Traceback (most recent call last):\r\n      File \"seq2seq.py\", line 351, in <module>\r\n        train()\r\n      File \"seq2seq.py\", line 274, in train\r\n        rnn_model = build_train_graph(feed_previous=True)\r\n      File \"seq2seq.py\", line 247, in build_train_graph\r\n        dec_outputs, dec_memory = _basic_rnn_seq2seq(enc_inp, dec_inp, cell, Why , by , feed_previous=feed_previous)\r\n      File \"seq2seq.py\", line 165, in _basic_rnn_seq2seq\r\n        return _rnn_decoder(decoder_inputs, enc_state, attention_mechanism, Why , by ,_loop_function)\r\n      File \"seq2seq.py\", line 147, in _rnn_decoder\r\n        output, new_state = cell(inp, state=new_state)\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\r\n        return super(RNNCell, self).__call__(inputs, state)\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 575, in __call__\r\n        outputs = self.call(inputs, *args, **kwargs)\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1066, in call\r\n        cur_inp, new_state = cell(cur_inp, cur_state)\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\r\n        return super(RNNCell, self).__call__(inputs, state)\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 575, in __call__\r\n        outputs = self.call(inputs, *args, **kwargs)\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 1289, in call\r\n        \"Received type %s instead.\"  % type(state))\r\n    TypeError: Expected state to be instance of AttentionWrapperState. Received type <class 'tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple'> instead.\r\n    \r\n\r\n\r\nI have tried printing the type of new state and i got the following type\r\n\r\n`    \r\n    ('new_state', AttentionWrapperState(cell_state=(LSTMStateTuple(c=<tf.Tensor 'rnn/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/add_9:0' shape=(?, 40) dtype=float32>, h=<tf.Tensor 'rnn/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/mul_14:0' shape=(?, 40) dtype=float32>),), attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=(244, 40) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(244, 5) dtype=float32>, alignment_history=()))\r\n    `\r\nEven though new state is an instance of AttentionWrapperState it still gives an error.\r\n\r\nThanks in advance for your replies .", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thanks for replying to my issue, I have updated my comment and added more details.", "@oahziur any chance you could take a look?", "@vsai121 \r\n\r\nI think you initial state should created from `cell.zero_state(batch_size=batch_size, dtype=tf.float32)` instead of `cells[0].zero_state(batch_size=batch_size, dtype=tf.float32)`. `cells[0]` is just your first rnn layer's cell.", "@oahziur I have the same issue\r\ntried your solution `cell.zero_state(batch_size=batch_size, dtype=tf.float32).clone(cell_state=initial_state)`\r\n\r\ngot:\r\nAttributeError: 'tuple' object has no attribute 'clone'\r\n\r\n---\r\n* tf 1.8 (cpu)\r\n* python 3.5\r\n* ubuntu 16.04", "@vsai121 I solved it\r\nin `MultiRNN` `zero_state` is a tuple of the states of each layer. so:\r\n\r\n`new_state = tuple([cells.zero_state(batch_size, tf.float32)[i].clone(cell_state=initial_state[i]) for i in range(num_layers)])`\r\n", "We should consider a helper function that uses nest.map_structure to do\nthis for you.\n\nOn Mon, Jul 23, 2018 at 3:02 AM, nimroha <notifications@github.com> wrote:\n\n> @vsai121 <https://github.com/vsai121> I solved it\n> in MultiRNN zero_state is a tuple of the states of each layer. so:\n>\n> new_state = tuple([cells.zero_state(batch_size, tf.float32)[i].clone(cell_state=initial_state[i])\n> for i in range(num_layers)])\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19834#issuecomment-407005548>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyIaVTFQ5x4xScV4ozhlvLVu_0voks5uJZ8ogaJpZM4UeSWJ>\n> .\n>\n", "Nagging Assignee @ebrevdo: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "For posterity, the most general way to do this is:\r\n\r\n```\r\nnew_state = tf.contrib.framework.nest.map_structure(\r\n   lambda cell, is: cell.zero_state(batch_size, tf.float32).clone(cell_state=is),\r\n   cells, initial_state)\r\n```"]}, {"number": 19833, "title": "tensorflow version conflict", "body": "There's a class under tensorflow namely \"prepare_attention\" which doesn't work on the latest version of tensorflow but only on tensorflow 1.0.0 which can't be installed on Python 3.6\r\n\r\nMy config:\r\nWindows 10\r\nPython 3.6.5\r\n\r\nPlease suggest me a way out either with the prepare_attention or how to get Tensorflow for Python 3.6\r\n\r\nThanks!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "HIe , so yeah , After upgrading my tensorflow I now get an error whenever I try to use \"prepare_attention\" on my seq2seq , any leads ??", "Well hi there,\n\nYou have to downgrade your tensorflow version, that's the only way i found\nout after struggling for months. So, i suggest you do the same.\n\nThnx\n\nOn Thu, Sep 6, 2018, 3:12 PM Joshua Chihozhwa <notifications@github.com>\nwrote:\n\n> HIe , so yeah , After upgrading my tensorflow I now get an error whenever\n> I try to use \"prepare_attention\" on my seq2seq , any leads ??\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19833#issuecomment-419030643>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AkTrkJmDq5SwFl2aWzEfHwwzN2Zfv0uCks5uYO4PgaJpZM4UeNOj>\n> .\n>\n", "Thanks , I'll do that..\n\nOn Thu, Sep 6, 2018, 11:49 Avneesh Singh <notifications@github.com> wrote:\n\n> Well hi there,\n>\n> You have to downgrade your tensorflow version, that's the only way i found\n> out after struggling for months. So, i suggest you do the same.\n>\n> Thnx\n>\n> On Thu, Sep 6, 2018, 3:12 PM Joshua Chihozhwa <notifications@github.com>\n> wrote:\n>\n> > HIe , so yeah , After upgrading my tensorflow I now get an error whenever\n> > I try to use \"prepare_attention\" on my seq2seq , any leads ??\n> >\n> > \u2014\n> > You are receiving this because you authored the thread.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/19833#issuecomment-419030643\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/AkTrkJmDq5SwFl2aWzEfHwwzN2Zfv0uCks5uYO4PgaJpZM4UeNOj\n> >\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19833#issuecomment-419033285>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AfN-jUNp1X5uaUtMXFgCeG8SyHwbrbxoks5uYO-vgaJpZM4UeNOj>\n> .\n>\n"]}, {"number": 19831, "title": "Allow tf.summary.merge_all() assign operation name", "body": "One may need to assign name of the MergeSummary operation", "comments": ["(From API-owners) We are fine with this, but are somewhat curious if there was a motivating example where this would be needed.", "Hi @josh11b  we are trying to embed TF to our application. we are define graph with python, train the graph with a C++ application.  \r\n\r\nTensorboard is great tool to visualize graph and metrics, since we are not train with python api we can't use summary file writer.  A solution would be define summary with python and fetch in C++, so I need a name to fetch summary output, then I found that the name  is not assignable, that is the reason behind this PR.", "Ok, gotcha", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Can you look at the errors and fix them?", "Sure", "It seems to be broken on Windows. Could you check?", "@drpngx  Can you help trigger the test again, I am not sure it is related to this PR. If it persist I will try to get a windows box to debug it. thanks", "Windows related failure didn't show up this time. but instead anther internal failure ;(", "@drpngx  Anything else I could do to move this PR forward ? :)", "Testing again. We'll initiate a pull.", "`gen_collective_ops` seems to be the cause. It looks like the build is broken.", "@reeze could you pull rebase and push again? thanks.", "@drpngx  DONE ;)", "Thanks! Running tests...", "OK, these are weird errors... build errors against LLVM."]}, {"number": 19830, "title": "After encoded to tfrecords and decoded, .jpg quality descend", "body": "I'm not sure if somebody has noticed or it's just an exception that after the procedure of encoding and decoding, the .jpg loss some information. I havn't figured it out.... So strange..........\r\nHope someone can find the answer...\r\n\r\nThe part to encode:\r\n```\r\ndef encode_tfrecords(file_path, output_path):\r\n    classes = os.listdir(file_path)\r\n\r\n    with tf.python_io.TFRecordWriter(output_path) as writer:\r\n        for index, name in enumerate(classes):\r\n            class_path = file_path + name + \"/\"\r\n            for img_name in os.listdir(class_path):\r\n                img_path = class_path + img_name\r\n\r\n                img_raw = tf.gfile.FastGFile(img_path, 'rb').read()\r\n\r\n                example = tf.train.Example(features=tf.train.Features(feature={\r\n                    \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\r\n                    'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw]))\r\n                }))\r\n                writer.write(example.SerializeToString())  \r\n```\r\n\r\n\r\nThe part to decode:\r\n```\r\ndef read_and_decode(filename, output_path, iters):\r\n    filename_queue = tf.train.string_input_producer([filename])  \r\n    reader = tf.TFRecordReader()\r\n    _, serialized_example = reader.read(filename_queue) \r\n    features = tf.parse_single_example(serialized_example,\r\n                                       features={\r\n                                           'label': tf.FixedLenFeature([], tf.int64),\r\n                                           'img_raw': tf.FixedLenFeature([], tf.string),\r\n                                       }) \r\n    # image = tf.decode_raw(features['img_raw'], t.uint8)\r\n    image = tf.image.decode_jpeg(features['img_raw'], channels=3)\r\n    label  = tf.cast(features['label'], tf.int64)\r\n    with tf.Session() as sess:  \r\n        init_op = tf.global_variables_initializer()\r\n        sess.run(init_op)\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(coord=coord)\r\n        for i in range(iters):\r\n            example, l = sess.run([image, label]) \r\n            img = Image.fromarray(example, 'RGB')\r\n            img.save(output_path + str(i) + '_''Label_' + str(l) + '.jpg')\r\n        coord.request_stop()\r\n        coord.join(threads)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thx man!\r\nIt's executed in my mac OS with py 3.6.5 and tf 1.3.0. \r\nTo reproduce, you should have a directory we call file_path of such structure below:\r\n0 -- img1.jpg\r\n     img2.jpg\r\n     img3.jpg\r\n     ...\r\n1 -- img1.jpg\r\n     img2.jpg\r\n     ...\r\n2 -- ...\r\nAnd encode_tfrecords'll return a tfrecords file.\r\nThen you can use read_and_decode function to decode the tfrecords and see the pics.", "And I have another question as well, I noticed that in tf for encoding we have tf.train.BytesList, tf.train.Int64List, tf.train.FloatList, but what if we want to encode something string like. Is there any solution? I found that encoding to Bytes didn't work well", "What is the problem? jpg is lossy? You can use png instead.", "Exact, jpg is lossy, so does png. But png is less lossy than jpg. Do you know how to explain this phenomene?", "png should be lossless. jpg is lossy, by design.", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19829, "title": "large String[] create Tensor in java", "body": "i use this way to create a tensor by a large String[] in java:    \r\n* // Valid: Matrix of String tensors.\r\n   * // Each element might have a different length.\r\n   * byte[][][] matrix = new byte[2][2][];\r\n   * matrix[0][0] = \"this\".getBytes(\"UTF-8\");\r\n   * matrix[0][1] = \"is\".getBytes(\"UTF-8\");\r\n   * matrix[1][0] = \"a\".getBytes(\"UTF-8\");\r\n   * matrix[1][1] = \"matrix\".getBytes(\"UTF-8\");\r\n   * Tensor<String> m = Tensor.create(matrix, String.class);\r\n   * }</pre>\r\n\r\nhowever,it cost too much time. my String[] length of 6000+,create the tensor will cost 20+ m. how can i deal with it? thanks.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@gxsaccount If you still have this question please ask on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). There is also a larger community that reads questions there.\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 31 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19828, "title": "Add label smoothing feature in sampled softmax", "body": "", "comments": ["We need a test for that.", "So, I think what needs to happen is that the smoothing is only applied to the other targets, so the probability is divided by `num_classes - 1`. For the one-hot, you can use the `1 - p` as `on_value`, and `p / (n -1)` as `off_value`.", "@drpngx  Thanks for your advice!\r\n\r\nIndeed, I got inf cost when I tested this function on my language model, which has about 80,000 classes in total, with `label_smoothing=0.2`, and I finally found that it was due to the `remove_accidental_hits` switch. When this switch is set `True`, the `true_logit` of some sampled classes will be set `-inf` in function `_compute_sampled_logits` if they are accidentally the target, which causes `sampled_loss` to be `inf` since `label` is not zero because of label smoothing.\r\nThere are two possible solutions: 1) Considering that the total classes number is relative large, we can get approximate result even we turn off `remove_accidental_hits`. 2) Make sure that target will not be sampled.\r\n\r\nBTW, I got similar result on my language model with `label_smoothing=0.2` after I turned off `remove_accidental_hits`.", "We're about to open-source our implementation in another repo. Will keep you posted.", "Please see the logic around [here](https://github.com/tensorflow/lingvo/blob/master/lingvo/core/layers.py#L2432).", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 19827, "title": "Add label smoothing feature in sampled softmax", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!"]}, {"number": 19826, "title": "averaging pooling fail to execute in tensorflow C API for rectangle size", "body": "while square size like 96X96 is OK the rectangle sized average pooling induces failure on GPU version C API.\r\nchecked the freeze_graph.py tool and the output dimension is right. the bug should be somewhere in the GPU version implementation.\r\n\r\n2018-06-07 10:38:08.502720: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2018-06-07 10:38:09.262953: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2018-06-07 10:38:10.250411: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2018-06-07 10:38:10.290275: F tensorflow/stream_executor/cuda/cuda_dnn.cc:570] could not convert BatchDescriptor {count: 1 feature_map_count: 256 spatial: 0 2  value_min: 0.000000 value_max: 0.000000 layout: BatchDepthYX} to cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\r\nAborted (core dumped)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}]