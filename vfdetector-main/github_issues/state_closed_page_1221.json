[{"number": 16535, "title": "only merge specific kind of summary", "body": "Currently, I need to gitve `tf.summary.merge` an input arg to specify what summaries to merge. \r\nBut it will be more convinient if there is a function called `tf.summary.merge_scalar` / `tf.summary.merge_image` cause scalars and images are often logged with different frequencies, and they need to be merged seperately.", "comments": ["/CC @dandelionmane, thoughts?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@jart Feature request possibly related to tensorboard.  Thoughts?", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I agree that the `tf.summary` API has room for improvement regarding convenience. We're trying to move towards `tf.contrib.summary` and eventually `tb.summary` at this time. We sadly don't have resources to do feature work on this older API at this time."]}, {"number": 16534, "title": "warning with tf.losses.softmax_cross_entropy", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows and Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9/7\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\ntf.losses.softmax_cross_entropy calls tf.nn.softmax_cross_entropy_with_logits, in which there is a warning. It's better also provide tf.losses.softmax_cross_entropy_v2 to call tf.nn.softmax_cross_entropy_with_logits_v2.\r\n", "comments": ["Can you print out the exact message and you expected?", "The warning message reported is\r\n\r\n.../lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:691: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n\r\nFuture major versions of TensorFlow will allow gradients to flow\r\ninto the labels input on backprop by default.\r\n\r\nSee tf.nn.softmax_cross_entropy_with_logits_v2.\r\n\r\nI didn't call tf.nn.softmax_cross_entropy_with_logits directly. It was called by tf.losses.softmax_cross_entropy. It's better also provide tf.losses.softmax_cross_entropy_v2 to call tf.nn.softmax_cross_entropy_with_logits_v2 to silence the warning.", "OK, yes, that's a problem in our deprecation. The solution is to refactor the code to call a function that doesn't have the deprecated code. Would you be willing to send a PR for that?", "Added #16837 for the fix.", "Jumping in because of a doubt: what exactly does \"allow gradients to flow\r\ninto the labels input on backprop by default\" mean? \r\n\r\nI see in the merged PR that it's now necessary to include the call to `stop_gradients`, but what would happen if that call was missing? And what is the rationale behind letting gradients flow into the labels input?", "If the call was missing, gradients would be propagated to the labels, which, depending on where those labels come from, may or may not be what you want.\r\n\r\nIn general, gradients should flow through all ops back to all inputs. If you do not want this, stop_gradients should be used.", "Thanks for the clarification.\r\nIs there anywhere an example of use-case where propagation to labels is useful/necessary? (I'm just trying to figure out what is this used for)", "@GPhilo \r\nFound a couple of use-cases [here](https://stackoverflow.com/questions/49100865/why-softmax-cross-entropy-with-logits-v2-backprops-into-labels); Interesting.", "It seams odd to me, that going forward, there will be like `tf.nn.foo_v2` but no `tf.nn.foo`...\r\nI suppose this is a matter of compatibility, but since (according to the message) this will take effect in the next *major* release, why can't we keep the name?"]}, {"number": 16533, "title": "Make Lstm1d.ndlstm_base_unrolled use lstm_cell with state_is_tuple = True", "body": "This is to address the deprecation warning thrown by using `state_is_tuple` = False.", "comments": ["Please resolve conflicts", "I'm closing this pull request now since ndlstm is removed from tensorflow:master."]}, {"number": 16532, "title": "Keras/TF1.5.0 does not consider TensorBoard embeddings_freq/embeddings_metadata", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow) describe in following**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7**:\r\n- **TensorFlow installed from (source or binary) binary**:\r\n- **TensorFlow version (use command below) 1.5.0**:\r\n- **Python version 3.5.1**: \r\n- **Bazel version (if compiling from source) NOT USED**:\r\n- **GCC/Compiler version (if compiling from source) NOT USED**:\r\n- **CUDA/cuDNN version NOT USED**:\r\n- **GPU model and memory NOT USED**:\r\n- **Exact command to reproduce python imdb_fasttext.py**:\r\n\r\n### Describe the problem\r\nTF1.5.0/Keras does not consider TensorBoard embeddings_freq/embeddings_metadata.\r\n\r\nThis parameter is added in 10 month ago and removed in 5 month ago.\r\nAnd currently it does not exist \r\n-  Current code\r\n    https://github.com/tensorflow/tensorflow/blame/579125e87af201ae6b6fa872b6dc3f3ecb400de9/tensorflow/python/keras/_impl/keras/callbacks.py#L606\r\n-  Apr 12, 2017 (10month ago) (add argument)\r\n    https://github.com/tensorflow/tensorflow/commit/b8b8ebcf851df71ebb5209ae27d75e2befc50f0d\r\n- Sep 6, 2017 (5month ago) (remove argument)\r\n    https://github.com/tensorflow/tensorflow/commit/eaaa0b93852054dee086a3ed5373cf8bbe3d2fb3\r\n\r\nWhich is already supported on Keras 2.1.3.\r\nhttps://github.com/keras-team/keras/blame/7d1e0bc5872855af5bf35a725025d3bdb6f07d6c/keras/callbacks.py#L641\r\n\r\n### Source code / logs\r\nCurrent outputs are following. It reports the keyword \"embeddings_metadata\" does not exist. \r\n```\r\nC:\\Users\\sakaia\\work\\tensorflow\\keras>python imdb_fasttext.py\r\nUsing TensorFlow backend.\r\nLoading data...\r\n25000 train sequences\r\n25000 test sequences\r\nAverage train sequence length: 238\r\nAverage test sequence length: 230\r\nPad sequences (samples x time)\r\nx_train shape: (25000, 400)\r\nx_test shape: (25000, 400)\r\nBuild model...\r\nWARNING:tensorflow:From C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\p\r\nython\\keras\\_impl\\keras\\backend.py:1557: calling reduce_mean (from tensorflow.py\r\nthon.ops.math_ops) with keep_dims is deprecated and will be removed in a future\r\nversion.\r\nInstructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\nTraceback (most recent call last):\r\n  File \"imdb_fasttext.py\", line 146, in <module>\r\n    embeddings_metadata= embeddingsMetadata\r\nTypeError: __init__() got an unexpected keyword argument 'embeddings_metadata'\r\n```\r\n\r\nSource code is follows (based on https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py)\r\n```Python\r\n'''This example demonstrates the use of fasttext for text classification\r\n\r\nBased on Joulin et al's paper:\r\n\r\nBags of Tricks for Efficient Text Classification\r\nhttps://arxiv.org/abs/1607.01759\r\n\r\nResults on IMDB datasets with uni and bi-gram embeddings:\r\n    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\r\n    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.\r\n'''\r\n\r\nfrom __future__ import print_function\r\nimport numpy as np\r\n\r\n# Changed from keras to tensorflow.python.keras\r\nfrom tensorflow.python.keras.preprocessing import sequence\r\nfrom tensorflow.python.keras.models import Sequential\r\nfrom tensorflow.python.keras.layers import Dense\r\nfrom tensorflow.python.keras.layers import Embedding\r\nfrom tensorflow.python.keras.layers import GlobalAveragePooling1D\r\nfrom tensorflow.python.keras.callbacks import TensorBoard\r\n# Followings are workaround for https://github.com/tensorflow/tensorflow/issues/16358\r\nfrom keras.datasets import imdb\r\n\r\ndef create_ngram_set(input_list, ngram_value=2):\r\n    \"\"\"\r\n    Extract a set of n-grams from a list of integers.\r\n\r\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\r\n    {(4, 9), (4, 1), (1, 4), (9, 4)}\r\n\r\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\r\n    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\r\n    \"\"\"\r\n    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\r\n\r\n\r\ndef add_ngram(sequences, token_indice, ngram_range=2):\r\n    \"\"\"\r\n    Augment the input list of list (sequences) by appending n-grams values.\r\n\r\n    Example: adding bi-gram\r\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\r\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\r\n    >>> add_ngram(sequences, token_indice, ngram_range=2)\r\n    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\r\n\r\n    Example: adding tri-gram\r\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\r\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\r\n    >>> add_ngram(sequences, token_indice, ngram_range=3)\r\n    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\r\n    \"\"\"\r\n    new_sequences = []\r\n    for input_list in sequences:\r\n        new_list = input_list[:]\r\n        for i in range(len(new_list) - ngram_range + 1):\r\n            for ngram_value in range(2, ngram_range + 1):\r\n                ngram = tuple(new_list[i:i + ngram_value])\r\n                if ngram in token_indice:\r\n                    new_list.append(token_indice[ngram])\r\n        new_sequences.append(new_list)\r\n\r\n    return new_sequences\r\n\r\n# Set parameters:\r\n# ngram_range = 2 will add bi-grams features\r\nngram_range = 1\r\nmax_features = 20000\r\nmaxlen = 400\r\nbatch_size = 32\r\nembedding_dims = 50\r\nepochs = 5\r\n\r\nprint('Loading data...')\r\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\r\nprint(len(x_train), 'train sequences')\r\nprint(len(x_test), 'test sequences')\r\nprint('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\r\nprint('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\r\n\r\nif ngram_range > 1:\r\n    print('Adding {}-gram features'.format(ngram_range))\r\n    # Create set of unique n-gram from the training set.\r\n    ngram_set = set()\r\n    for input_list in x_train:\r\n        for i in range(2, ngram_range + 1):\r\n            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\r\n            ngram_set.update(set_of_ngram)\r\n\r\n    # Dictionary mapping n-gram token to a unique integer.\r\n    # Integer values are greater than max_features in order\r\n    # to avoid collision with existing features.\r\n    start_index = max_features + 1\r\n    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\r\n    indice_token = {token_indice[k]: k for k in token_indice}\r\n\r\n    # max_features is the highest integer that could be found in the dataset.\r\n    max_features = np.max(list(indice_token.keys())) + 1\r\n\r\n    # Augmenting x_train and x_test with n-grams features\r\n    x_train = add_ngram(x_train, token_indice, ngram_range)\r\n    x_test = add_ngram(x_test, token_indice, ngram_range)\r\n    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\r\n    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\r\n\r\nprint('Pad sequences (samples x time)')\r\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\r\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\r\nprint('x_train shape:', x_train.shape)\r\nprint('x_test shape:', x_test.shape)\r\n\r\nprint('Build model...')\r\nmodel = Sequential()\r\n\r\n# we start off with an efficient embedding layer which maps\r\n# our vocab indices into embedding_dims dimensions\r\nmodel.add(Embedding(max_features,\r\n                    embedding_dims,\r\n                    input_length=maxlen))\r\n\r\n# we add a GlobalAveragePooling1D, which will average the embeddings\r\n# of all words in the document\r\nmodel.add(GlobalAveragePooling1D())\r\n\r\n# We project onto a single unit output layer, and squash it with a sigmoid:\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\nembeddingsMetadata = {'embedding': 'metadata.tsv'}\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          validation_data=(x_test, y_test),\r\n# problem occured here!!!\r\n          callbacks=[TensorBoard(log_dir=\".\", histogram_freq=1, embeddings_freq=1,\r\n                                 embeddings_metadata= embeddingsMetadata\r\n                                ),\r\n          ])\r\n```\r\n", "comments": ["Right, it looks like it was intentional. Maybe @fchollet can comment.", "Same issue here, but for all parameters related to `embeddings`(frequency, layer names, metadata - all them complain that such argument doesn't exist):\r\n\r\n> ---------------------------------------------------------------------------\r\n> TypeError                                 Traceback (most recent call last)\r\n> <ipython-input-132-e85937547af4> in <module>()\r\n>      63                  embeddings_freq=1,\r\n>      64                  embeddings_layer_names=None,\r\n> ---> 65                  embeddings_metadata=None\r\n>      66                               )\r\n>      67 ]\r\n> \r\n> TypeError: __init__() got an unexpected keyword argument 'embeddings_freq'\r\n\r\nMy callback function was:\r\n\r\n> tf.keras.callbacks.TensorBoard(log_dir=log_dir,\r\n>                  histogram_freq=0,\r\n>                  batch_size=32,\r\n>                  write_graph=True,\r\n>                  write_grads=True,\r\n>                  write_images=True,\r\n>                  embeddings_freq=1,\r\n>                  embeddings_layer_names=None,\r\n>                  embeddings_metadata=None\r\n>                               )", "@drpngx @fchollet any updates on this open issue?", "@ricardobarroslourenco got same error.\r\nTypeError: __init__() got an unexpected keyword argument 'embeddings_metadata'\r\n\r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/_impl/keras/callbacks.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/_impl/keras/callbacks.py)\r\n\r\nComments of TensorBoard shows embeddings_metadata param,\r\nwhile the __init__ do not support.\r\n`\r\n@tf_export('keras.callbacks.TensorBoard')\r\nclass TensorBoard(Callback):\r\n  # pylint: disable=line-too-long\r\n  \"\"\"Tensorboard basic visualizations.\r\n  This callback writes a log for TensorBoard, which allows\r\n  you to visualize dynamic graphs of your training and test\r\n  metrics, as well as activation histograms for the different\r\n  layers in your model.\r\n  TensorBoard is a visualization tool provided with TensorFlow.\r\n  If you have installed TensorFlow with pip, you should be able\r\n  to launch TensorBoard from the command line:\r\n  ```sh\r\n  tensorboard --logdir=/full_path_to_your_logs\r\n  ```\r\n  You can find more information about TensorBoard\r\n  [here](https://www.tensorflow.org/get_started/summaries_and_tensorboard).\r\n  Arguments:\r\n      log_dir: the path of the directory where to save the log\r\n          files to be parsed by TensorBoard.\r\n      histogram_freq: frequency (in epochs) at which to compute activation\r\n          and weight histograms for the layers of the model. If set to 0,\r\n          histograms won't be computed. Validation data (or split) must be\r\n          specified for histogram visualizations.\r\n      write_graph: whether to visualize the graph in TensorBoard.\r\n          The log file can become quite large when\r\n          write_graph is set to True.\r\n      write_grads: whether to visualize gradient histograms in TensorBoard.\r\n          `histogram_freq` must be greater than 0.\r\n      batch_size: size of batch of inputs to feed to the network\r\n          for histograms computation.\r\n      write_images: whether to write model weights to visualize as\r\n          image in TensorBoard.\r\n      embeddings_freq: frequency (in epochs) at which selected embedding\r\n          layers will be saved.\r\n      embeddings_layer_names: a list of names of layers to keep eye on. If\r\n          None or empty list all the embedding layer will be watched.\r\n      embeddings_metadata: a dictionary which maps layer name to a file name\r\n          in which metadata for this embedding layer is saved. See the\r\n          [details](https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional)\r\n          about metadata files format. In case if the same metadata file is\r\n          used for all embedding layers, string can be passed.\r\n  \"\"\"\r\n\r\n  # pylint: enable=line-too-long\r\n\r\n  def __init__(self,\r\n               log_dir='./logs',\r\n               histogram_freq=0,\r\n               batch_size=32,\r\n               write_graph=True,\r\n               write_grads=False,\r\n               write_images=False):\r\n    super(TensorBoard, self).__init__()\r\n`", "Closing due to staleness. Please check with the latest version of TensorFlow. Feel free to reopen if the issue still persists. Thanks!"]}, {"number": 16531, "title": "Running problems when building a VGG model through tflearn on Ubuntu to train your own data", "body": "\r\n  File \"<ipython-input-15-fc398a1d9321>\", line 1, in <module>\r\n    runfile('/home/lab326/songpeng/anacoda\u9879\u76ee/tflearn-vgg1.py', wdir='/home/lab326/songpeng/anacoda\u9879\u76ee')\r\n\r\n  File \"/home/lab326/anaconda3/lib/python3.5/site-packages/spyder/utils/site/sitecustomize.py\", line 705, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"/home/lab326/anaconda3/lib/python3.5/site-packages/spyder/utils/site/sitecustomize.py\", line 102, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"/home/lab326/songpeng/anacoda\u9879\u76ee/tflearn-vgg1.py\", line 59, in <module>\r\n    files_extension=['.jpg'], filter_channel=True)\r\n\r\n  File \"/home/lab326/anaconda3/lib/python3.5/site-packages/tflearn/data_utils.py\", line 512, in image_preloader\r\n    flags=files_extension, filter_channel=filter_channel)\r\n\r\n  File \"/home/lab326/anaconda3/lib/python3.5/site-packages/tflearn/data_utils.py\", line 732, in directory_to_samples\r\n    classes = sorted(os.walk(directory).__next__()[1])\r\n\r\nStopIteration\r\n\r\n\r\n\r\nfrom tflearn.data_utils import image_preloader\r\n\r\nX, Y = image_preloader(files_list, image_shape=(224, 224), mode='folder',\r\n                       categorical_labels=True, normalize=False,\r\n                       files_extension=['.jpg'], filter_channel=True)", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 16530, "title": "Merge pull request #1 from tensorflow/master", "body": "Update from tensorflow origin", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 16529, "title": "Merge pull request #1 from tensorflow/master", "body": "Update from tensorflow origin", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 16528, "title": "Merge pull request #2 from tensorflow/master", "body": "Update from tensorflow origin", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 16527, "title": "Only trying to load nnapi when we want to.", "body": "Only call `NNAPIExists()` when we want to use nnapi to get ride of the unnecessary `unable to open library libneuralnetwork.so` error.", "comments": ["Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Reviewer @aselle: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?"]}, {"number": 16526, "title": "TF1.5 configure overwrites export TF_CUDA_VERSION=9.1", "body": "It seems `configure` overwrites the BASH flag:\r\n```bash\r\nexport TF_CUDA_VERSION=\"9.1\"\r\n```\r\nHere is the 1.5 branch of my [tensorflow.sh](https://github.com/ahundt/robotics_setup/blob/tf-1.5/tensorflow.sh) install script.\r\n\r\nHere are the key code lines from the [tensorflow.sh specific hash to reproduce this error](https://github.com/ahundt/robotics_setup/blob/8a7940f74945f09d2a6bfc1a08d06f156a4c79f9/tensorflow.sh) install script,  and 161 at the time of writing:\r\n```\r\n    # line 134 at the time of writing\r\n    export TF_NEED_CUDA=1\r\n    export TF_CUDA_VERSION=\"9.1\"\r\n    export TF_CUDNN_VERSION=7\r\n    export CUDA_TOOLKIT_PATH=/usr/local/cuda-9.1/targets/x86_64-linux/lib/\r\n    # ...snip... to line 161 at the time of writing\r\n    yes \"\" | ./configure\r\n```\r\n\r\nI believe the problem is either in the `configure` script, [configure.py set_tf_cuda_version() ask_cuda_version](https://github.com/tensorflow/tensorflow/blob/r1.5/configure.py#L805) or in the call to [set_tf_cuda_version](https://github.com/tensorflow/tensorflow/blob/r1.5/configure.py#L1320), none of which seem to check the environment variable first and possibly overwrite it.\r\n\r\nThe result is the following error:\r\n```\r\nInvalid path to CUDA 9.0 toolkit. /usr/local/cuda/lib64/libcudart.so.9.0 cannot be found\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]:\r\n\r\nPlease specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nInvalid path to CUDA 9.0 toolkit. /usr/local/cuda/lib64/libcudart.so.9.0 cannot be found\r\nTraceback (most recent call last):\r\n  File \"configure.py\", line 1365, in <module>\r\n    main()\r\n  File \"configure.py\", line 1320, in main\r\n    set_tf_cuda_version(environ_cp)\r\n  File \"configure.py\", line 850, in set_tf_cuda_version\r\n    _DEFAULT_PROMPT_ASK_ATTEMPTS)\r\n__main__.UserInputError: Invalid TF_CUDA_SETTING setting was provided 10 times in a row. Assuming to be a scripting mistake.\r\n-> [1]\r\nahundt@femur|~/src/robotics_setup on tf-1.5!?\r\n\u00b1 ls /usr/local/cuda/lib64/\r\nstubs                  libcusparse.so.9.1     libnppist.so.9.1\r\nlibaccinj64.so         libcusparse.so.9.1.85  libnppist.so.9.1.85\r\nlibaccinj64.so.9.1     libcusparse_static.a   libnppist_static.a\r\nlibaccinj64.so.9.1.85  libnppc.so             libnppisu.so\r\nlibcublas_device.a     libnppc.so.9.1         libnppisu.so.9.1\r\nlibcublas.so           libnppc.so.9.1.85      libnppisu.so.9.1.85\r\nlibcublas.so.9.1       libnppc_static.a       libnppisu_static.a\r\nlibcublas.so.9.1.128   libnppial.so           libnppitc.so\r\nlibcublas.so.9.1.85    libnppial.so.9.1       libnppitc.so.9.1\r\nlibcublas_static.a     libnppial.so.9.1.85    libnppitc.so.9.1.85\r\nlibcudadevrt.a         libnppial_static.a     libnppitc_static.a\r\nlibcudart.so           libnppicc.so           libnpps.so\r\nlibcudart.so.9.1       libnppicc.so.9.1       libnpps.so.9.1\r\nlibcudart.so.9.1.85    libnppicc.so.9.1.85    libnpps.so.9.1.85\r\nlibcudart_static.a     libnppicc_static.a     libnpps_static.a\r\nlibcufft.so            libnppicom.so          libnvblas.so\r\nlibcufft.so.9.1        libnppicom.so.9.1      libnvblas.so.9.1\r\nlibcufft.so.9.1.85     libnppicom.so.9.1.85   libnvblas.so.9.1.128\r\nlibcufft_static.a      libnppicom_static.a    libnvblas.so.9.1.85\r\nlibcufftw.so           libnppidei.so          libnvgraph.so\r\nlibcufftw.so.9.1       libnppidei.so.9.1      libnvgraph.so.9.1\r\nlibcufftw.so.9.1.85    libnppidei.so.9.1.85   libnvgraph.so.9.1.85\r\nlibcufftw_static.a     libnppidei_static.a    libnvgraph_static.a\r\nlibcuinj64.so          libnppif.so            libnvrtc-builtins.so\r\nlibcuinj64.so.9.1      libnppif.so.9.1        libnvrtc-builtins.so.9.1\r\nlibcuinj64.so.9.1.85   libnppif.so.9.1.85     libnvrtc-builtins.so.9.1.85\r\nlibculibos.a           libnppif_static.a      libnvrtc.so\r\nlibcurand.so           libnppig.so            libnvrtc.so.9.1\r\nlibcurand.so.9.1       libnppig.so.9.1        libnvrtc.so.9.1.85\r\nlibcurand.so.9.1.85    libnppig.so.9.1.85     libnvToolsExt.so\r\nlibcurand_static.a     libnppig_static.a      libnvToolsExt.so.1\r\nlibcusolver.so         libnppim.so            libnvToolsExt.so.1.0.0\r\nlibcusolver.so.9.1     libnppim.so.9.1        libOpenCL.so\r\nlibcusolver.so.9.1.85  libnppim.so.9.1.85     libOpenCL.so.1\r\nlibcusolver_static.a   libnppim_static.a      libOpenCL.so.1.0\r\nlibcusparse.so         libnppist.so           libOpenCL.so.1.0.0\r\n```", "comments": ["I managed to hack around this by manually setting [configure.py](https://github.com/tensorflow/tensorflow/blob/master/configure.py#L39):\r\n```\r\n_DEFAULT_CUDA_VERSION = '9.0'\r\n```\r\nto\r\n```\r\n_DEFAULT_CUDA_VERSION = '9.1'\r\n```\r\n\r\nHowever, it would be nice if the script was updated to read the export variables correctly again. \ud83d\udc4d ", "I'm using MPI as well and I'm trying to figure out how to an additional hack from https://github.com/tensorflow/tensorflow/issues/11903#issuecomment-332718012 to avoid an MPI error. Here is a partial solution:\r\n`export CC_OPT_FLAGS=\"-DOMPI_SKIP_MPICXX=1 -march=native\"`", "Sounds like a good idea to me. Feel free to send a PR and CC @gunan.\r\n\r\nI prefer config options as suggested by in #16518.", "I agree, config options would be even better", "@case540 Could you take a look.\r\n\r\nOur aim is in the end you will only need to set config options, and no need to run configure.\r\nMike, what is the environment variable to use for setting the CUDA version we use for the configure script?", "oops, missed this.\r\n\r\nTF_CUDA_VERSION is the correct env variable to be setting. I'm not able to reproduce this error. The configure script appears to look in the correct path for CUDA. Note, that IF configure.py script does not find CUDA, then it will set the TF_CUDA_VERSION env variable to empty string (and then reask user again where it should look for CUDA).", "were you able to try with 9.1 on tf 1.5? Perhaps something was updated already but before it would look for 9.0 anyway.\r\n\r\nI also have an automated setup script so asking would ideally be avoidable if at all possible.", "Hmm, taking another look at this, it seems like if you set either of CUDA_TOOLKIT_PATH  or TF_CUDA_VERSION  incorrectly, then both of those ENV variables will be reset and ununsed.\r\n\r\nIt could be the case that CUDA_TOOLKIT_PATH was incorrectly set, then both those env variables were set to the defaults, and some default cuda path with default cuda 9.0 version was used.\r\n\r\nIf this is the case, then it's more of an issue of a very confusing error. I dont think the CUDA_VERSION variable should ever be reset by the configure script, since if a user sets a specific version we should use that version probably", "On the other hand, since...\r\n\r\nyes \"\" | ./configure\r\n\r\n... is run by the above script, it kinda just skips through all of the prompts. I think a potential solution to this is to make a much friendly way for scripts to run configure.py. Such just add actual python args to the script instead of just setting env variables and then skipping over user prompts lol", ">  I think a potential solution to this is to make a much [friendlier] way for scripts to run configure.py\r\n\r\nAdding python args and instructions to use them would be an excellent solution, I agree!", "Closing this issue due to staleness. Please check with the latest version of TensorFlow. Feel free to reopen if issue still persists. Thanks!"]}, {"number": 16525, "title": "Fix incorrect docs for DecodeVideoOp", "body": "This fix fixes incorrect docs for DecodeVideoOp\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 16524, "title": "MKL: Reverting the switch to max_pool_v2 in python", "body": "A prior commit https://github.com/tensorflow/tensorflow/pull/14983 changed python interface to call max_pool_v2 causing failure in MKL build. Currently MKL doesn't support max_pool_v2. Reverting  the commit  for now, will change it back when MKL implementation is complete.", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 16523, "title": "Feature request: tf.Print should either print (not log), or accept a log level", "body": "### System information\r\n- **Have I written custom code**: no\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: 1.4.1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Feature Request\r\nCurrently tf.Print produces INFO logs.\r\nIf log level is set to get rid of INFO logs (given the noise level), it renders tf.Print as non-functional.\r\n\r\nIt would be nice to have tf.Print work either by making it actually work like print(), i.e. it is not a mechanism to generate logs, or if it should remain that way, then have it accept a log level optional parameter.", "comments": ["/CC @mrry, what do you think of this proposal?", "Seems reasonable. I'd lean towards the \"making it actually work like `print()`\" option, and consider adding a new API (perhaps in `tf.contrib`) that offers more options.", "Marking as contributions welcome for adding the contrib op.", "I believe this is addressed by https://github.com/tensorflow/tensorflow/pull/15173 ?", "From a quick read of the history on that bug, I don't think it does. I think from there print is still based on using the logging infrastructure and/or stderr, whereas `print()` by default writes unconditionally to stdout, which would be the desired behavior.", "Related: #15953", "Hi @nikhilk ! tf.print does not print logs anymore. To get logs printed , you can use [tf.compat.v1.loggin_set_verbosity](https://www.tensorflow.org/api_docs/python/tf/compat/v1/logging/set_verbosity) . Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "In fact `tf.print` does now accept a log level, too: https://www.tensorflow.org/api_docs/python/tf/print\r\n\r\n> The output stream, logging level, or file to print to. Defaults to sys.stderr, but sys.stdout, tf.compat.v1.logging.info, tf.compat.v1.logging.warning, tf.compat.v1.logging.error, absl.logging.info, absl.logging.warning and absl.logging.error are also supported. To print to a file, pass a string started with \"file://\" followed by the file path, e.g., \"file:///tmp/foo.out\".\r\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 16522, "title": "TF 1.5 fails on Windows 10 with \"Security check failure or stack buffer overrun\"", "body": "The error is reproducible on simple MNIST tutorial from https://www.tensorflow.org/tutorials/layers\r\nI can upload the minidump, if necessary.\r\n\r\n**TF version**\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nb'unknown' 1.5.0\r\n\r\n**The TF log is**\r\nExtracting MNIST-data\\train-images-idx3-ubyte.gz\r\nExtracting MNIST-data\\train-labels-idx1-ubyte.gz\r\nExtracting MNIST-data\\t10k-images-idx3-ubyte.gz\r\nExtracting MNIST-data\\t10k-labels-idx1-ubyte.gz\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_model_dir': '.\\\\mnist_convnet_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F02AEF5860>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\n2018-01-28 19:09:17.733213: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1105] Found device 0 with properties:\r\nname: GeForce GTX 770 major: 3 minor: 0 memoryClockRate(GHz): 1.1105\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 2.00GiB freeMemory: 1.64GiB\r\n2018-01-28 19:09:17.733455: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 770, pci bus id: 0000:01:00.0, compute capability: 3.0)\r\n2018-01-28 19:09:19.732272: E C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2018-01-28 19:09:19.732452: E C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:389] error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2018-01-28 19:09:19.732900: E C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2018-01-28 19:09:19.733034: F C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\kernels\\conv_ops.cc:717] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)\r\n\r\n**Exception analysis from WinDbg**\r\n(42f0.1198): Security check failure or stack buffer overrun - code c0000409 (!!! second chance !!!)\r\n*** WARNING: Unable to verify checksum for C:\\Users\\nobody\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd\r\n*** ERROR: Symbol file could not be found.  Defaulted to export symbols for C:\\Users\\nobody\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd - \r\nucrtbase!abort+0x4e:\r\n00007ffc`d74eb70e cd29            int     29h\r\n0:034> !analyze -v\r\n*** WARNING: Unable to verify checksum for C:\\Users\\nobody\\Anaconda3\\python36.dll\r\n*** WARNING: Unable to verify checksum for python.exe\r\n*** WARNING: Unable to verify checksum for C:\\Users\\nobody\\Anaconda3\\lib\\site-packages\\numexpr\\interpreter.cp36-win_amd64.pyd\r\n*** ERROR: Symbol file could not be found.  Defaulted to export symbols for C:\\Users\\nobody\\Anaconda3\\lib\\site-packages\\numexpr\\interpreter.cp36-win_amd64.pyd - \r\n*** ERROR: Symbol file could not be found.  Defaulted to export symbols for C:\\WINDOWS\\SYSTEM32\\nvcuda.dll - \r\nGetUrlPageData2 (WinHttp) failed: 12002.\r\n\r\nKEY_VALUES_STRING: 1\r\n\r\n\r\nTIMELINE_ANALYSIS: 1\r\n\r\nTimeline: !analyze.Start\r\n    Name: <blank>\r\n    Time: 2018-01-28T17:19:28.120Z\r\n    Diff: 120 mSec\r\n\r\nTimeline: Dump.Current\r\n    Name: <blank>\r\n    Time: 2018-01-28T17:19:28.0Z\r\n    Diff: 0 mSec\r\n\r\nTimeline: Process.Start\r\n    Name: <blank>\r\n    Time: 2018-01-28T17:18:34.0Z\r\n    Diff: 54000 mSec\r\n\r\nTimeline: OS.Boot\r\n    Name: <blank>\r\n    Time: 2018-01-12T10:16:05.0Z\r\n    Diff: 1407803000 mSec\r\n\r\n\r\nDUMP_CLASS: 2\r\n\r\nDUMP_QUALIFIER: 0\r\n\r\nFAULTING_IP: \r\nucrtbase!abort+4e\r\n00007ffc`d74eb70e cd29            int     29h\r\n\r\nEXCEPTION_RECORD:  (.exr -1)\r\nExceptionAddress: 00007ffcd74eb70e (ucrtbase!abort+0x000000000000004e)\r\n   ExceptionCode: c0000409 (Security check failure or stack buffer overrun)\r\n  ExceptionFlags: 00000001\r\nNumberParameters: 1\r\n   Parameter[0]: 0000000000000007\r\nSubcode: 0x7 FAST_FAIL_FATAL_APP_EXIT\r\n\r\nFAULTING_THREAD:  00004cb0\r\n\r\nPROCESS_NAME:  python.exe\r\n\r\nERROR_CODE: (NTSTATUS) 0xc0000409 - The system detected an overrun of a stack-based buffer in this application. This overrun could potentially allow a malicious user to gain control of this application.\r\n\r\nEXCEPTION_CODE: (NTSTATUS) 0xc0000409 - The system detected an overrun of a stack-based buffer in this application. This overrun could potentially allow a malicious user to gain control of this application.\r\n\r\nEXCEPTION_CODE_STR:  c0000409\r\n\r\nEXCEPTION_PARAMETER1:  0000000000000007\r\n\r\nWATSON_BKT_PROCSTAMP:  5a5e439a\r\n\r\nWATSON_BKT_PROCVER:  3.6.4150.1013\r\n\r\nPROCESS_VER_PRODUCT:  Python\r\n\r\nWATSON_BKT_MODULE:  ucrtbase.dll\r\n\r\nWATSON_BKT_MODSTAMP:  70f70cc4\r\n\r\nWATSON_BKT_MODOFFSET:  6b70e\r\n\r\nWATSON_BKT_MODVER:  10.0.16299.15\r\n\r\nMODULE_VER_PRODUCT:  Microsoft\u00ae Windows\u00ae Operating System\r\n\r\nBUILD_VERSION_STRING:  10.0.16299.15 (WinBuild.160101.0800)\r\n\r\nMODLIST_WITH_TSCHKSUM_HASH:  d39c6bc550850a285511eb4ca73187e2bfcc8d5c\r\n\r\nMODLIST_SHA1_HASH:  c6d888e2558848c801875139c919412641504368\r\n\r\nNTGLOBALFLAG:  70\r\n\r\nAPPLICATION_VERIFIER_FLAGS:  0\r\n\r\nPRODUCT_TYPE:  1\r\n\r\nSUITE_MASK:  272\r\n\r\nDUMP_TYPE:  fe\r\n\r\nANALYSIS_SESSION_HOST:  DESKTOP-V7DQHVH\r\n\r\nANALYSIS_SESSION_TIME:  01-28-2018 19:19:28.0120\r\n\r\nANALYSIS_VERSION: 10.0.17061.1000 amd64fre\r\n\r\nTHREAD_ATTRIBUTES: \r\nOS_LOCALE:  ENU\r\n\r\nPROBLEM_CLASSES: \r\n\r\n    ID:     [0n277]\r\n    Type:   [FAIL_FAST]\r\n    Class:  Primary\r\n    Scope:  DEFAULT_BUCKET_ID (Failure Bucket ID prefix)\r\n            BUCKET_ID\r\n    Name:   Add\r\n    Data:   Omit\r\n    PID:    [Unspecified]\r\n    TID:    [Unspecified]\r\n    Frame:  [0]\r\n\r\n    ID:     [0n266]\r\n    Type:   [FATAL_APP_EXIT]\r\n    Class:  Addendum\r\n    Scope:  DEFAULT_BUCKET_ID (Failure Bucket ID prefix)\r\n            BUCKET_ID\r\n    Name:   Add\r\n    Data:   Omit\r\n    PID:    [Unspecified]\r\n    TID:    [Unspecified]\r\n    Frame:  [0]\r\n\r\nBUGCHECK_STR:  FAIL_FAST_FATAL_APP_EXIT\r\n\r\nDEFAULT_BUCKET_ID:  FAIL_FAST_FATAL_APP_EXIT\r\n\r\nPRIMARY_PROBLEM_CLASS:  FAIL_FAST\r\n\r\nLAST_CONTROL_TRANSFER:  from 00007ffc623b447f to 00007ffcd74eb70e\r\n\r\nSTACK_TEXT:  \r\n000000be`c3a3da80 00007ffc`623b447f : 000000be`00000003 00000000`00000003 000000be`c3a3dbf0 000001f6`6f45afa0 : ucrtbase!abort+0x4e\r\n000000be`c3a3dab0 00007ffc`62dd4ea9 : 000000be`c3a3e640 00000000`00000000 00000000`00000000 000001f6`00000001 : _pywrap_tensorflow_internal!tensorflow::internal::LogMessageFatal::~LogMessageFatal+0x4f\r\n000000be`c3a3daf0 00007ffc`62db3d62 : 00000000`00000001 000000be`c3a3ec50 000001f6`8ebb94e8 000000be`c3a3f6f0 : _pywrap_tensorflow_internal!tensorflow::LaunchConv2DOp<Eigen::GpuDevice,float>::operator()+0x18c9\r\n000000be`c3a3eb50 00007ffc`6251e5b4 : 000001f6`6f942d30 000001f6`6f461900 000001f6`6f461900 000000be`c3a3f0a0 : _pywrap_tensorflow_internal!tensorflow::Conv2DOp<Eigen::GpuDevice,float>::Compute+0x742\r\n000000be`c3a3eda0 00007ffc`6251ddca : ffffffff`fffffffe 000000be`c3a3f050 000001f6`6f3ddc80 000001f6`6fcd2db0 : _pywrap_tensorflow_internal!tensorflow::BaseGPUDevice::ComputeHelper+0x4f4\r\n000000be`c3a3ef70 00007ffc`623edaa5 : 00000000`00000000 00000000`00000000 000000be`c3a3f100 00000000`00000000 : _pywrap_tensorflow_internal!tensorflow::BaseGPUDevice::Compute+0x18a\r\n000000be`c3a3f000 00007ffc`623f1068 : 000001f6`6f8dde30 00007ffc`62384a75 000001f6`6f8dde30 000001f6`6f4635c0 : _pywrap_tensorflow_internal!tensorflow::NewLocalExecutor+0x1065\r\n000000be`c3a3f9c0 00007ffc`62384e79 : 000001f6`6f8dde30 00000000`00000000 000000be`c3a3fa58 000001f6`6f4633f0 : _pywrap_tensorflow_internal!?_Copy@?$_Func_impl@V?$_Binder@U_Unforced@std@@P8ExecutorState@?A0x4f36ba0d@tensorflow@@EAAXUTaggedNode@345@_J@ZQEAV345@AEBU6345@AEA_J@std@@V?$allocator@H@2@X$$V@std@@EEBAPEAV?$_Func_base@X$$V@2@PEAX@Z+0x78\r\n000000be`c3a3fa10 00007ffc`62385020 : 000001f6`6f8dde30 00007ffc`00001198 00007ffc`aa528b40 00000000`00000000 : _pywrap_tensorflow_internal!Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop+0x3d9\r\n000000be`c3a3faa0 00007ffc`623b6ab5 : 000001f6`6f480000 00007ffc`00000000 000001f6`6f4843c0 00000000`00000000 : _pywrap_tensorflow_internal!Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop+0x580\r\n000000be`c3a3fae0 00007ffc`623b6a09 : 000001f6`6f4845b0 00000000`00000000 00000000`00000000 00000000`00000000 : _pywrap_tensorflow_internal!tensorflow::WindowsFileSystem::Utf8ToWideChar+0x175\r\n000000be`c3a3fb20 00007ffc`d749d885 : 00000000`fc960000 000001f6`6f4845b0 00000000`00000000 00000000`00000000 : _pywrap_tensorflow_internal!tensorflow::WindowsFileSystem::Utf8ToWideChar+0xc9\r\n000000be`c3a3fb50 00007ffc`d9c21fe4 : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : ucrtbase!thread_start<unsigned int (__cdecl*)(void * __ptr64)>+0x35\r\n000000be`c3a3fb80 00007ffc`da2eef91 : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : KERNEL32!BaseThreadInitThunk+0x14\r\n000000be`c3a3fbb0 00000000`00000000 : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : ntdll!RtlUserThreadStart+0x21\r\n\r\nTHREAD_SHA1_HASH_MOD_FUNC:  a995353e639442c8476b335aedd6ecded1b41211\r\nTHREAD_SHA1_HASH_MOD_FUNC_OFFSET:  b900a9e6afe48f2fd80e72effda8ef8748a73296\r\nTHREAD_SHA1_HASH_MOD:  216c770e1f463298a8482ae892957bad45a82609\r\nFOLLOWUP_IP: \r\nucrtbase!abort+4e\r\n00007ffc`d74eb70e cd29            int     29h\r\nFAULT_INSTR_CODE:  15ba29cd\r\nSYMBOL_STACK_INDEX:  0\r\nSYMBOL_NAME:  ucrtbase!abort+4e\r\nFOLLOWUP_NAME:  MachineOwner\r\nMODULE_NAME: ucrtbase\r\nIMAGE_NAME:  ucrtbase.dll\r\nDEBUG_FLR_IMAGE_TIMESTAMP:  70f70cc4\r\nSTACK_COMMAND:  ~34s ; .cxr ; kb\r\nBUCKET_ID:  FAIL_FAST_FATAL_APP_EXIT_ucrtbase!abort+4e\r\nFAILURE_EXCEPTION_CODE:  c0000409\r\nFAILURE_IMAGE_NAME:  ucrtbase.dll\r\nBUCKET_ID_IMAGE_STR:  ucrtbase.dll\r\nFAILURE_MODULE_NAME:  ucrtbase\r\nBUCKET_ID_MODULE_STR:  ucrtbase\r\nFAILURE_FUNCTION_NAME:  abort\r\nBUCKET_ID_FUNCTION_STR:  abort\r\nBUCKET_ID_OFFSET:  4e\r\nBUCKET_ID_MODTIMEDATESTAMP:  70f70cc4\r\nBUCKET_ID_MODCHECKSUM:  fbc7a\r\nBUCKET_ID_MODVER_STR:  10.0.16299.15\r\nBUCKET_ID_PREFIX_STR:  FAIL_FAST_FATAL_APP_EXIT_\r\nFAILURE_PROBLEM_CLASS:  FAIL_FAST\r\nFAILURE_SYMBOL_NAME:  ucrtbase.dll!abort\r\nFAILURE_BUCKET_ID:  FAIL_FAST_FATAL_APP_EXIT_c0000409_ucrtbase.dll!abort\r\nWATSON_STAGEONE_URL:  http://watson.microsoft.com/StageOne/python.exe/3.6.4150.1013/5a5e439a/ucrtbase.dll/10.0.16299.15/70f70cc4/c0000409/0006b70e.htm?Retriage=1\r\nTARGET_TIME:  2018-01-28T17:19:44.000Z\r\nOSBUILD:  16299\r\nOSSERVICEPACK:  15\r\nSERVICEPACK_NUMBER: 0\r\nOS_REVISION: 0\r\nOSPLATFORM_TYPE:  x64\r\nOSNAME:  Windows 10\r\nOSEDITION:  Windows 10 WinNt SingleUserTS\r\nUSER_LCID:  0\r\nOSBUILD_TIMESTAMP:  1976-06-22 09:45:20\r\nBUILDDATESTAMP_STR:  160101.0800\r\nBUILDLAB_STR:  WinBuild\r\nBUILDOSVER_STR:  10.0.16299.15\r\nANALYSIS_SESSION_ELAPSED_TIME:  9166\r\nANALYSIS_SOURCE:  UM\r\nFAILURE_ID_HASH_STRING:  um:fail_fast_fatal_app_exit_c0000409_ucrtbase.dll!abort\r\nFAILURE_ID_HASH:  {e31753ac-c98a-8055-3663-47e707543d20}", "comments": ["Did you build it yourself or did you download the pip package?\r\n\r\nCan you try to run other operators that run on GPU to make sure they work?\r\n\r\n@yzhwang could that be due to a recent change?", "I am using pip version of TF\r\nIt looks like there are two issues\r\nFirst one was related to CUDNN. I've reinstalled NVidia drivers and CUDA from scratch and now it's gone. The model work fine for me.\r\nHowever, the buffer overflow is probably still there. I believe it was caused by this last string from TF log:\r\nF C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\kernels\\conv_ops.cc:717] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)\r\nRight now I can't reproduce the error, but I still have the minidump from the last run.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "@sn2234 I believe that the check failure of GetConvolveAlgorithms() is not the root cause. I think the first error CUDNN_STATUS_NOT_INITIALIZED indicates that cudnn is not correctly installed or loaded. I think GetConvolveAlgorithms() failed simply because cudnn is not there to provide the algorithm enums.\r\nTo find out the root cause of CUDNN_STATUS_NOT_INITIALIZED, could you provide more info as how you installed TensorFlow and CUDA/cudnn? Also your hardware info too. Thanks!", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you."]}, {"number": 16521, "title": "Failed to load native Tensorflow runtine", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information : \r\nraspberry pi raspbian 4.9.24-v27\r\nTensorflow installed from source (version 1.5)\r\nPython version : 3.5.4\r\ngcc 4.9.2\r\nPython was build from sources\r\nBazel version 0.9.0\r\n\r\nFollowin this link to build bazel and tensor flow (with last version of bazel and tf) : \r\n<img width=\"437\" alt=\"image\" src=\"https://user-images.githubusercontent.com/30391054/35484892-572990c2-0457-11e8-8980-6389246bb2d0.png\">\r\nwith minimal support (only jmalloc)\r\n\r\n\r\n### Describe the problem\r\nWhen trying import tensorflow in python3 i have the following error message \ud83d\udc4d \r\nimportError tensorflow/python/_pywrap_tensorflow_internal.so undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE\r\n\r\nfailed to load the native Tensorflow runtime\r\n\r\nI am in doubt to revert to python3.4 and using the tensorflow available without compiling ...\r\n\r\nThanks in advance.\r\n", "comments": ["/CC @petewarden", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16520, "title": "Periodic resample operation gradients and optimization", "body": "- Added gradient for periodic_resample operation.\r\n- Optimization: implemented incremental index computing.\r\n- Implemented intra-op parallelism for this operation\r\n- Make output shape of periodic_resample operation fully defined, when possible.", "comments": ["@vchigrin: I've taken a look at the changes and they seem quite reasonable and beneficial. Thanks a lot for making the PR. If they pass the python tests (as you mentioned in the previous thread), then I think this is good to go.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@vchigrin could you please provide some benchmark numbers to document the performance improvements?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@vchigrin any progress?", "Sorry for the delay, I am very busy at my primary work these times (it isn't related to Tensorflow). I will update PR this weekend.", "About benchmark. Tested it on CPU Intel(R) Core(TM) i7-4930K CPU @ 3.40GHz with following script \r\nhttps://pastebin.com/RvVw52YX\r\n\r\nBefore testing i modified code so it used only single thread for this operation (since previous version also was single-threaded). \r\n\r\nProgram output before these changes:\r\n```\r\n  Average resample time 563.567590714 ms, max time 579.554796219 ms\r\n```\r\nAfter:\r\n```\r\n  Average resample time 83.5015463829 ms, max time 96.137046814 ms\r\n```\r\n", "@vchigrin Thanks for the info, looks good.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@vchigrin Thanks for adding the test. It looks like there are still a few linter and build formatting errors:\r\n\r\nhttps://source.cloud.google.com/results/invocations/55177de0-a0c5-41c5-b2d2-dc9c922dc888/log", "Fixed, sorry for making such mistakes..", "@vchigrin no worries. :-)\r\n", "Nagging Assignee @rmlarsen: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Timeout, trying again."]}, {"number": 16519, "title": "Fix some typos in the documation (tensorflow/docs_src)", "body": "This fix fixes some typos in the documentation (tensorflow/docs_src/**.md)", "comments": []}, {"number": 16518, "title": "./configure [--help|-h] does not work", "body": "\r\n```\r\n./configure --help\r\n```\r\n\r\nstarts some interactive configuration.\r\n\r\nIt should output a list of possible configuration options or indicate where to find more information.\r\n\r\nIt should *not* start interactive configuration.", "comments": ["@alexanderkjeldaas you can use `yes|./configure` to get the defaults. I think it's fine to have the prompts be overridden by command-line options.\r\n\r\n/CC @gunan ", "Created a PR [#16571](https://github.com/tensorflow/tensorflow/pull/16571) to try to address this issue. Free free to comment if any we needs to further improve.", "@alexanderkjeldaas Could you please confirm if this issue can be closed with the closed PR #16571 ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/16518\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/16518\">No</a>\n"]}, {"number": 16517, "title": "ou must feed a value for placeholder tensor 'import/Placeholder when i test my frozen model", "body": "Hello , I trying create mobile app for object recognition for my own created model. I fallow this tutorial https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/#2\r\nBut when i even get a testing model from step 3 i get error \r\n\r\n```\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'import/Placeholder' with dtype float\r\n\t [[Node: import/Placeholder = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\n\r\nI'm aware that is wrong node problem, but i trying with other and always i get failure \r\n\r\nMy code for model creation \r\n```\r\n\r\nx = tf.placeholder(tf.float32,\r\n                   shape=[None, cons.IMAGE_SIZE, cons.IMAGE_SIZE, 3], name=\"x\")\r\ny_ = tf.placeholder(tf.float32, shape=[None, cons.LABELS_NUMB], name=\"labels\")\r\n\r\nK = 4\r\nL = 8\r\nM = 12\r\nN = 200\r\n\r\nx_image = tf.reshape(x, [-1, cons.IMAGE_SIZE, cons.IMAGE_SIZE, 3])\r\ntf.summary.image('input', x_image, 3)\r\nprint(\"X image \")\r\nprint(tf.shape(x_image))\r\n\r\n################## first ##############\r\n\r\nW_conv1 = weight_variable([5, 5, 3, 32], \"weight1\")\r\nb_conv1 = bias_variable([32], \"bias1\")\r\n\r\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\r\nh_pool1 = max_pool_2x2(h_conv1)\r\nprint(\"W_conv1 \")\r\nprint(tf.shape(W_conv1))\r\nprint(\"b_conv1 \")\r\nprint(tf.shape(b_conv1))\r\nprint(\"h_conv1 \")\r\nprint(tf.shape(h_conv1))\r\nprint(\"h_pool1 \")\r\nprint(tf.shape(h_pool1))\r\n\r\n################## second ##############\r\n\r\nW_conv2 = weight_variable([5, 5, 32, 64], \"weight2\")\r\nb_conv2 = bias_variable([64], \"bias2\")\r\n\r\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\r\ntf.summary.histogram(\"activations\", h_conv2)\r\n\r\n\r\nh_pool2 = max_pool_2x2(h_conv2)\r\n\r\nprint(\"W_conv2 \")\r\nprint(tf.shape(W_conv2))\r\nprint(\"b_conv2 \")\r\nprint(tf.shape(b_conv2))\r\nprint(\"h_conv2 \")\r\nprint(tf.shape(h_conv2))\r\nprint(\"h_pool2 \")\r\nprint(tf.shape(h_pool2))\r\n\r\n################## fully connected 3 ##############\r\n\r\nW_fc1 = weight_variable([8 * 8 * 64, 1024], \"Weight3\")\r\nb_fc1 = bias_variable([1024], \"bias3\")\r\n\r\nh_pool2_flat = tf.reshape(h_pool2, [-1, 8 * 8 * 64])\r\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\r\n\r\nprint(\"W_fc1 \")\r\nprint(tf.shape(W_fc1))\r\nprint(\"b_fc1 \")\r\nprint(tf.shape(b_fc1))\r\nprint(\"h_pool2_flat \")\r\nprint(tf.shape(h_pool2_flat))\r\nprint(\"h_fc1 \")\r\nprint(tf.shape(h_fc1))\r\n\r\n################ dropout  4 #################\r\n\r\nkeep_prob = tf.placeholder(tf.float32)\r\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\r\n\r\n################## fully connected 5 ##############\r\n\r\nW_fc2 = weight_variable([1024, cons.LABELS_NUMB], \"weight5\")\r\nb_fc2 = bias_variable([cons.LABELS_NUMB], \"bias5\")\r\n\r\nY = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\r\ntf.summary.histogram(\"final\", Y)\r\n\r\n\r\nprint(\"W_fc2 \")\r\nprint(tf.shape(W_fc2))\r\nprint(\"b_fc2 \")\r\nprint(tf.shape(b_fc2))\r\nprint(\"Y \")\r\nprint(tf.shape(Y))\r\n\r\nwith tf.name_scope(\"cross_entropy\"):\r\n    cross_entropy = tf.reduce_mean(\r\n        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=Y))\r\n    tf.summary.scalar(\"xent\", cross_entropy)\r\n\r\nwith tf.name_scope(\"train_step\"):\r\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\n\r\nwith tf.name_scope(\"Acuracy\"):\r\n    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(y_, 1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n    tf.summary.scalar(\"accuracy\", accuracy)\r\n\r\nsumm = tf.summary.merge_all()\r\nsaver = tf.train.Saver()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    writer = tf.summary.FileWriter(\"/home/damian/api/mnist_demo/10\")\r\n    writer.add_graph(sess.graph)\r\n    for i in range(1000):\r\n        img, lb = fileCreation.next_batch(100, images32, labels)\r\n        if i % 5 == 0:\r\n            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 1.0})\r\n        writer.add_summary(s, i)\r\n        if i % 100 == 0:\r\n            saver.save(sess, '/home/damian/api/checkpoint/my_test_model', global_step=i)\r\n            train_accuracy = accuracy.eval(feed_dict={\r\n                x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 1.0})\r\n            print('step %d, training accuracy %g' % (i, train_accuracy))\r\n        train_step.run(feed_dict={x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 0.5})\r\n\r\n    print('test accuracy %g' % accuracy.eval(feed_dict={x: test_images32,\r\n                                                        y_: fileCreation.dense_to_one_hot(test_labels,\r\n                                                                                          cons.LABELS_NUMB),\r\n                                                        keep_prob: 0.1}))\r\n```\r\n\r\n\r\nI'm freeze this model with and i point output_node to 'final'\r\nnext i call script from tutorial\r\n\r\n\r\n```\r\npython -m scripts.label_image \\\r\n  --graph=tf_files/frozen_model3.pb  \\\r\n  --input_layer=x \\\r\n  --output_layer=final \\\r\n  --image=tf_files/stop.png  \\\r\n  --input_height=32 \\\r\n  --input_width=32 \\\r\n  --input_mean=16  \\\r\n  --input_std=16  \r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16516, "title": "0.6.0", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 16515, "title": "Unable to install/ upgrade tensorflow1.5 on window10", "body": "Exception:\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\commands\\install.py\", line 335, in run\r\n    wb.build(autobuilding=True)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\wheel.py\", line 749, in build\r\n    self.requirement_set.prepare_files(self.finder)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\req\\req_set.py\", line 380, in prepare_files\r\n    ignore_dependencies=self.ignore_dependencies))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\req\\req_set.py\", line 554, in _prepare_file\r\n    require_hashes\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\req\\req_install.py\", line 278, in populate_link\r\n    self.link = finder.find_requirement(self, upgrade)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\index.py\", line 465, in find_requirement\r\n    all_candidates = self.find_all_candidates(req.name)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\index.py\", line 423, in find_all_candidates\r\n    for page in self._get_pages(url_locations, project_name):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\index.py\", line 568, in _get_pages\r\n    page = self._get_page(location)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\index.py\", line 683, in _get_page\r\n    return HTMLPage.get_page(link, session=self.session)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\index.py\", line 811, in get_page\r\n    inst = cls(resp.content, resp.url, resp.headers)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\index.py\", line 731, in __init__\r\n    namespaceHTMLElements=False,\r\nTypeError: parse() got an unexpected keyword argument 'transport_encoding'", "comments": ["Try to uninstall old tf packages before you upgrade", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "\r\n\r\n\r\n\r\n\r\n------------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64-bit\r\n- **TensorFlow installed from (source or binary)**: using conda-forge\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: pip install --ignore-installed --upgrade tensorflow\r\n\r\n### Describe the problem\r\nI am looking to upgrade my current tensorflow version from 1.2.1 to the latest 1.5. following the article: https://developers.googleblog.com/2018/01/announcing-tensorflow-15.html.\r\n\r\nI have tested it on my ubuntu machine and it worked fine, but got errors while installing on my windows machine.\r\n\r\n### Source code / logs\r\n\r\nPS C:\\WINDOWS\\system32>  pip install --ignore-installed --upgrade tensorflow\r\nCollecting tensorflow\r\n  Downloading tensorflow-1.5.0-cp36-cp36m-win_amd64.whl (31.1MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 31.1MB 97kB/s\r\nCollecting tensorflow-tensorboard<1.6.0,>=1.5.0 (from tensorflow)\r\n  Downloading tensorflow_tensorboard-1.5.0-py3-none-any.whl (3.0MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.0MB 1.2MB/s\r\nCollecting absl-py>=0.1.6 (from tensorflow)\r\n  Downloading absl-py-0.1.10.tar.gz (79kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 1.3MB/s\r\nCollecting numpy>=1.12.1 (from tensorflow)\r\n  Downloading numpy-1.14.0-cp36-none-win_amd64.whl (13.4MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13.4MB 984kB/s\r\nCollecting six>=1.10.0 (from tensorflow)\r\n  Using cached six-1.11.0-py2.py3-none-any.whl\r\nCollecting protobuf>=3.4.0 (from tensorflow)\r\n  Downloading protobuf-3.5.1-py2.py3-none-any.whl (388kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 389kB 1.6MB/s\r\nCollecting wheel>=0.26 (from tensorflow)\r\n  Downloading wheel-0.30.0-py2.py3-none-any.whl (49kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 560kB/s\r\nCollecting bleach==1.5.0 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)\r\n  Using cached bleach-1.5.0-py2.py3-none-any.whl\r\nCollecting werkzeug>=0.11.10 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)\r\n  Downloading Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 327kB 249kB/s\r\nCollecting markdown>=2.6.8 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)\r\n  Downloading Markdown-2.6.11-py2.py3-none-any.whl (78kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 1.1MB/s\r\nCollecting futures>=3.1.1 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)\r\n  Downloading futures-3.1.1.tar.gz\r\nCollecting html5lib==0.9999999 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)\r\nCollecting setuptools (from protobuf>=3.4.0->tensorflow)\r\n  Downloading setuptools-38.4.1-py2.py3-none-any.whl (489kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 491kB 2.2MB/s\r\nBuilding wheels for collected packages: absl-py, futures\r\n  Running setup.py bdist_wheel for absl-py ... done\r\n  Stored in directory: C:\\Users\\Shravankumar147\\AppData\\Local\\pip\\Cache\\wheels\\45\\07\\0e\\6880381ca521796cf6cc18ba4ab502c2\r\n232e5777099b4df4ae\r\n  Running setup.py bdist_wheel for futures ... done\r\n  Stored in directory: C:\\Users\\Shravankumar147\\AppData\\Local\\pip\\Cache\\wheels\\ad\\79\\48\\b32521764d59b16fd1bc0ffd5862f6d3\r\nbf770c7d73ea1fb12a\r\nSuccessfully built absl-py futures\r\nInstalling collected packages: wheel, six, html5lib, bleach, werkzeug, markdown, futures, setuptools, protobuf, numpy, t\r\nensorflow-tensorboard, absl-py, tensorflow\r\nException:\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\commands\\install.py\", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\req\\req_set.py\", line 784, in install\r\n    **kwargs\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\req\\req_install.py\", line 851, in install\r\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\req\\req_install.py\", line 1064, in move_wheel_files\r\n    isolated=self.isolated,\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\wheel.py\", line 345, in move_wheel_files\r\n    clobber(source, lib_dir, True)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\wheel.py\", line 323, in clobber\r\n    shutil.copyfile(srcfile, destfile)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\shutil.py\", line 121, in copyfile\r\n    with open(dst, 'wb') as fdst:\r\nPermissionError: [Errno 13] Permission denied: 'C:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\numpy\\\\core\\\\multiarray.\r\ncp36-win_amd64.pyd'\r\n\r\nHope this information helps to understand the problem, let me know if anything else required. ", "@machinelearning147 I think you are using windows 10. Open the powershell/ cmd with adminstrative right by search it from start->right click\r\n\r\nthen run `pip install --upgrade tensorflow`  again, problem will be solved", "Thank you, issue resolved with \r\n\r\n`pip install tensorflow==1.5`"]}, {"number": 16514, "title": "Possibility to compile Tensorflow Native C++ Library to support Windows 10 UWP?", "body": "Hello all,\r\n\r\nWhen I was checking Deep Learning framework for Windows 10 UWP development, somehow I came into this: https://anyline.com/news/tensorflow-implem/\r\n\r\nIt was not hard to see their UWP example app as well: https://github.com/Anyline/anyline-ocr-examples-windows-uwp\r\n\r\nWhich really brings up the question: Is it possible to compile the Tensorflow C++ library to add support for Windows 10 UWP development now? Or at least to get some codes enough for the Model Evaluation to work? Since it has been achieved by others. Some similar cases are like OpenCV, Numpy, etc. It can be really tricky solving dependencies and other unknown issues, but since it's done before there's hope?\r\n\r\nMany thanks in advance!", "comments": []}, {"number": 16513, "title": "TF1.5.0 not working with CUDA 8.0", "body": "After upgrading to TF 1.5.0, when I import tensorflow, it raises:\r\n\r\n```\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n```\r\n\r\n- System: Ubuntu 14.04.5 LTS (64 bit)\r\n- Python: 2.7.6\r\n- TensorFlow: tensorflow-gpu-1.5.0\r\n- GPU: GeForce GTX TITAN\r\n- CUDA: 8.0", "comments": ["Facing the exact same Issue!", "The prebuilt tensorflow binaries support CUDA 9.0 - https://github.com/tensorflow/tensorflow/releases/tag/v1.5.0\r\n\r\nYou have 3 options:\r\n\r\n1. Compile from source and link against CUDA 8.0.\r\n2. Upgrade your CUDA to 9.0\r\n3. Continue to user TF 1.4.\r\n\r\n", "@mtngld is correct."]}, {"number": 16512, "title": "how to install ffmpeg in tensorflow 1.4 binary", "body": "hi\r\ni want to install ffmpeg in tensorflow 1.4 binary , python 3.5 on ubuntu 16.04 , please help me how do i do ? \r\nthe output type python -c \"from tensorflow.contrib import ffmpeg\" is ok dont have anly error , but i dont know why : \r\nfrom tensorflow.contrib import ffmpeg\r\n\r\ni get error , \r\n>>>  from tensorflow.contrib import ffmpeg\r\n  File \"<stdin>\", line 1\r\n    from tensorflow.contrib import ffmpeg\r\n    ^\r\nIndentationError: unexpected indent\r\n", "comments": ["ffmpeg is not officially supported by TF, you need to build from source and set this function on", "You have to use `tf.contrib.ffmpeg` to use functionality from there."]}, {"number": 16511, "title": "Update LICENSE", "body": "2017 => 2018", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 16510, "title": "Feature Request: Make lstm2d.separable_lstm accept Dynamic Batch Sizes", "body": "Apparently, lstm2d.separable_lstm doesn't accept dynamic batch sizes (number of images). Whenever I set the shape of a `placeholder` to `(None, height, width, depth)` to be fed into the network , I get this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"...\\tensorflow\\contrib\\ndlstm\\python\\lstm2d.py\", line 159, in separable_lstm\r\n    hidden = horizontal_lstm(images, nhidden)\r\n  File \"...\\tensorflow\\contrib\\ndlstm\\python\\lstm2d.py\", line 82, in horizontal_lstm\r\n    sequence = images_to_sequence(images)\r\n  File \"...\\tensorflow\\contrib\\ndlstm\\python\\lstm2d.py\", line 47, in images_to_sequence\r\n    [width, num_image_batches * height, depth])\r\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'int'\r\n```\r\n\r\nI guess it would be nice to have `separable_lstm` accept dynamic batch sizes so it can be used effectively.", "comments": ["Proposal: \r\n\r\nModify the implementation of `images_to_sequence` and `sequence_to_images` such that they'll accept dynamic batch sizes. I have already pushed [it](https://github.com/selcouthlyBlue/tensorflow/commit/7fcc9393e0b9dbc1b70fe746dfabca424c320b38) into my forked copy of tensorflow. The gist of it is instead of multiplying the `height` of the images to the `batch_size`, I multiplied it to the `width` of the images.  ", "I tried it for my own project and it's taking long to get to a step with a batch size greater than 1 in training. Any other ideas on how to make the batch size for ndlstm dynamic?", "How about using tf.shape to get dynamic shape? I mean to modify _shape function: \r\n\r\ns = array_ops.shape(input_)\r\nreturn s[0], s[1],.... \r\n\r\nDoes it work? ", "Applying tf.shape worked although I applied it in only one part of the lstm2d like so:\r\n\r\n```\r\ndef images_to_sequence(tensor):\r\n  _, _, width, depth = _shape(tensor)\r\n  s = array_ops.shape(tensor)\r\n  batch_size, height = s[0], s[1]\r\n  transposed = array_ops.transpose(tensor, [2, 0, 1, 3])\r\n  return array_ops.reshape(transposed,\r\n                           [width, batch_size * height, depth])\r\n```\r\n\r\nWith that, `sequence_to_images` was also modified to take in a known dimension (height) of the tensor:\r\n\r\n```\r\ndef sequence_to_images(tensor, height):\r\n  width, num_batches, depth = _shape(tensor)\r\n  if num_batches is None:\r\n    num_batches = -1\r\n  else:\r\n    num_batches = num_batches // height\r\n  reshaped = array_ops.reshape(tensor,\r\n                               [width, num_batches, height, depth])\r\n  return array_ops.transpose(reshaped, [1, 2, 0, 3])\r\n```\r\n\r\nI already pushed these changes."]}, {"number": 16509, "title": "Fix typo", "body": "", "comments": []}, {"number": 16508, "title": "Check more cpu features for Clang on Windows", "body": "Clang on Windows will define `__SSE__`, `__SSE2__` and other macros.\r\n\r\n#15990", "comments": []}, {"number": 16507, "title": "ResourceExhaustedError, when running UNET", "body": "My computer has a gpu GeForce 940MX installed. It has the Memory bandwidth 16.02 GB/s. I'm trying to train LUNA dataset using UNET model using following code.\r\n\r\n\tfrom __future__ import print_function\r\n\r\n\timport numpy as np\r\n\tfrom keras.models import Model\r\n\tfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\r\n\tfrom keras.layers import concatenate\r\n\tfrom keras.optimizers import Adam\r\n\tfrom keras.optimizers import SGD\r\n\tfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\r\n\tfrom keras import backend as K\r\n\r\n\r\n\tK.set_image_dim_ordering('th')  # Theano dimension ordering in this code\r\n\r\n\timg_rows = 512\r\n\timg_cols = 512\r\n\r\n\tsmooth = 1.\r\n\r\n\r\n\tdef dice_coef(y_true, y_pred):\r\n\t\ty_true_f = K.flatten(y_true)\r\n\t\ty_pred_f = K.flatten(y_pred)\r\n\t\tintersection = K.sum(y_true_f * y_pred_f)\r\n\t\treturn (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\r\n\r\n\tdef dice_coef_np(y_true,y_pred):\r\n\t\ty_true_f = y_true.flatten()\r\n\t\ty_pred_f = y_pred.flatten()\r\n\t\tintersection = np.sum(y_true_f * y_pred_f)\r\n\t\treturn (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\r\n\r\n\tdef dice_coef_loss(y_true, y_pred):\r\n\t\treturn -dice_coef(y_true, y_pred)\r\n\r\n\r\n\tdef get_unet():\r\n\t\tinputs = Input((1,img_rows, img_cols))\r\n\t\tconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\r\n\t\tconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\r\n\t\tpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\r\n\r\n\t\tconv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\r\n\t\tconv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\r\n\t\tpool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\r\n\r\n\t\tconv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\r\n\t\tconv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\r\n\t\tpool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\r\n\r\n\t\tconv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\r\n\t\tconv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\r\n\t\tpool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\r\n\r\n\t\tconv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\r\n\t\tconv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\r\n\r\n\t\t#up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\r\n\t\tup6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=1)\r\n\t\tconv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\r\n\t\tconv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\r\n\r\n\t\t#up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\r\n\t\tup7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=1)\r\n\t\tconv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\r\n\t\tconv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\r\n\r\n\t\t#up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\r\n\t\tup8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=1)\r\n\t\tconv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\r\n\t\tconv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\r\n\r\n\t\t#up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\r\n\t\tup9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=1)\r\n\t\tconv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\r\n\t\tconv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\r\n\r\n\t\tconv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\r\n\r\n\t\tmodel = Model(inputs=inputs, outputs=conv10)\r\n\r\n\t\tmodel.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])\r\n\r\n\t\treturn model\r\n\r\n\r\n\tdef train_and_predict(use_existing):\r\n\t\tprint('-'*30)\r\n\t\tprint('Loading and preprocessing train data...')\r\n\t\tprint('-'*30)\r\n\t\timgs_train = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"trainImages.npy\").astype(np.float32)\r\n\t\timgs_mask_train = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"trainMasks.npy\").astype(np.float32)\r\n\r\n\t\timgs_test = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"testImages.npy\").astype(np.float32)\r\n\t\timgs_mask_test_true = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"testMasks.npy\").astype(np.float32)\r\n\t\t\r\n\t\tmean = np.mean(imgs_train)  # mean for data centering\r\n\t\tstd = np.std(imgs_train)  # std for data normalization\r\n\r\n\t\timgs_train -= mean  # images should already be standardized, but just in case\r\n\t\timgs_train /= std\r\n\r\n\t\tprint('-'*30)\r\n\t\tprint('Creating and compiling model...')\r\n\t\tprint('-'*30)\r\n\t\tmodel = get_unet()\r\n\t\t# Saving weights to unet.hdf5 at checkpoints\r\n\t\tmodel_checkpoint = ModelCheckpoint('unet.hdf5', monitor='loss', save_best_only=True)\r\n\t\t#\r\n\t\t# Should we load existing weights? \r\n\t\t# Set argument for call to train_and_predict to true at end of script\r\n\t\tif use_existing:\r\n\t\t\tmodel.load_weights('./unet.hdf5')\r\n\t\t\t\r\n\t\t# \r\n\t\t# The final results for this tutorial were produced using a multi-GPU\r\n\t\t# machine using TitanX's.\r\n\t\t# For a home GPU computation benchmark, on my home set up with a GTX970 \r\n\t\t# I was able to run 20 epochs with a training set size of 320 and \r\n\t\t# batch size of 2 in about an hour. I started getting reseasonable masks \r\n\t\t# after about 3 hours of training. \r\n\t\t#\r\n\t\tprint('-'*30)\r\n\t\tprint('Fitting model...')\r\n\t\tprint('-'*30)\r\n\t\tmodel.fit(imgs_train, imgs_mask_train, batch_size=50, epochs=10, verbose=1, shuffle=True,\r\n\t\t\t\t  callbacks=[model_checkpoint])\r\n\r\n\t\t# loading best weights from training session\r\n\t\tprint('-'*30)\r\n\t\tprint('Loading saved weights...')\r\n\t\tprint('-'*30)\r\n\t\tmodel.load_weights('./unet.hdf5')\r\n\r\n\t\tprint('-'*30)\r\n\t\tprint('Predicting masks on test data...')\r\n\t\tprint('-'*30)\r\n\t\tnum_test = len(imgs_test)\r\n\t\timgs_mask_test = np.ndarray([num_test,1,512,512],dtype=np.float32)\r\n\t\tfor i in range(num_test):\r\n\t\t\timgs_mask_test[i] = model.predict([imgs_test[i:i+1]], verbose=0)[0]\r\n\t\tnp.save('masksTestPredicted.npy', imgs_mask_test)\r\n\t\tmean = 0.0\r\n\t\tfor i in range(num_test):\r\n\t\t\tmean+=dice_coef_np(imgs_mask_test_true[i,0], imgs_mask_test[i,0])\r\n\t\tmean/=num_test\r\n\t\tprint(\"Mean Dice Coeff : \",mean)\r\n\r\n\tif __name__ == '__main__':\r\n\t\ttrain_and_predict(False)\r\n\t\t\r\nBut when running it using GPU I'm getting the following error.\r\n\r\n\tWarning (from warnings module):\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\h5py\\__init__.py\", line 36\r\n\t\tfrom ._conv import register_converters as _register_converters\r\n\tFutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n\tUsing TensorFlow backend.\r\n\t------------------------------\r\n\tLoading and preprocessing train data...\r\n\t------------------------------\r\n\t------------------------------\r\n\tCreating and compiling model...\r\n\t------------------------------\r\n\t------------------------------\r\n\tFitting model...\r\n\t------------------------------\r\n\tEpoch 1/10\r\n\tTraceback (most recent call last):\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1327, in _do_call\r\n\t\treturn fn(*args)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1306, in _run_fn\r\n\t\tstatus, run_metadata)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\contextlib.py\", line 66, in __exit__\r\n\t\tnext(self.gen)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n\t\tpywrap_tensorflow.TF_GetCode(status))\r\n\ttensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]\r\n\t\t [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]\r\n\t\t [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3022_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n\tDuring handling of the above exception, another exception occurred:\r\n\r\n\tTraceback (most recent call last):\r\n\t  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 150, in <module>\r\n\t\ttrain_and_predict(False)\r\n\t  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 127, in train_and_predict\r\n\t\tcallbacks=[model_checkpoint])\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\engine\\training.py\", line 1657, in fit\r\n\t\tvalidation_steps=validation_steps)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\engine\\training.py\", line 1213, in _fit_loop\r\n\t\touts = f(ins_batch)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2357, in __call__\r\n\t\t**self.session_kwargs)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\r\n\t\trun_metadata_ptr)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1124, in _run\r\n\t\tfeed_dict_tensor, options, run_metadata)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1321, in _do_run\r\n\t\toptions, run_metadata)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _do_call\r\n\t\traise type(e)(node_def, op, message)\r\n\ttensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]\r\n\t\t [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]\r\n\t\t [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3022_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n\tCaused by op 'conv2d_1/convolution', defined at:\r\n\t  File \"<string>\", line 1, in <module>\r\n\t  File \"C:\\Research\\Python_installation\\lib\\idlelib\\run.py\", line 124, in main\r\n\t\tret = method(*args, **kwargs)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\idlelib\\run.py\", line 351, in runcode\r\n\t\texec(code, self.locals)\r\n\t  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 150, in <module>\r\n\t\ttrain_and_predict(False)\r\n\t  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 106, in train_and_predict\r\n\t\tmodel = get_unet()\r\n\t  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 39, in get_unet\r\n\t\tconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in __call__\r\n\t\toutput = self.call(inputs, **kwargs)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 164, in call\r\n\t\tdilation_rate=self.dilation_rate)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3195, in conv2d\r\n\t\tdata_format=tf_data_format)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 672, in convolution\r\n\t\top=op)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 338, in with_space_to_batch\r\n\t\treturn op(input, num_spatial_dims, padding)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 664, in op\r\n\t\tname=name)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 131, in _non_atrous_convolution\r\n\t\tname=name)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 397, in conv2d\r\n\t\tdata_format=data_format, name=name)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n\t\top_def=op_def)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\r\n\t\toriginal_op=self._default_original_op, op_def=op_def)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n\t\tself._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n\tResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[50,32,512,512]\r\n\t\t [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]\r\n\t\t [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3022_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n\r\nCan someone please kindly explain me the reason behind this error, ResourceExhaustedError. Is it because that the memory of GPU is not enough to load the dataset. This worked fine without GPU. But took around 6 hours to finish one epoch", "comments": ["Yes your GPU memory is not sufficient to load the model, it would be better if you feed the dataset in batches!", "As @rootally stated, you ran out of GPU memory. Ask on StackOverflow if you have more questions."]}, {"number": 16506, "title": "Feature request: Have Estimator display Loss and Metrics for Every Epoch and not Every Step", "body": "Most of the papers I\u2019ve read measure the time it takes to train a model with every epoch and not every step. If it isn\u2019t possible to display the loss only for every epoch, I think it would be nice to print when an epoch has passed.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}]