[{"number": 36283, "title": "fix the issue of py_func", "body": "in py_func, an error was throwed if the input tensor of the function is sparse_tensor or ragged_tensor as in the issue: https://github.com/tensorflow/tensorflow/issues/36278\r\nFix it by convert them into dense tensor.", "comments": ["Yes the user would need to do this conversion.\n\nOn Tue, Jan 28, 2020 at 6:18 PM Leslie-Fang <notifications@github.com>\nwrote:\n\n> *@Leslie-Fang* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/ops/script_ops.py\n> <https://github.com/tensorflow/tensorflow/pull/36283#discussion_r372159272>\n> :\n>\n> > @@ -116,6 +119,10 @@ def _convert(self, value, dtype):\n>        # TODO(akshayka): Make it possible to return a list of both Tensors and\n>        # Nones from an EagerPyFunc.\n>        return constant_op.constant(0.0, dtype=dtype)\n> +    if sparse_tensor.is_sparse(value):\n> +      value = sparse_ops.sparse_tensor_to_dense(value)\n>\n> Thanks @alextp <https://github.com/alextp> If we return the sparse_tensor\n> as a list of 3 eager_tensors. Does the user needs convert the 3\n> eager_tensors back into the sparse_tensor? Do you have any suggestions?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/36283?email_source=notifications&email_token=AAABHRLEACXXUCI7RL32SZTRADROJA5CNFSM4KMQ4UE2YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCTM7ETY#discussion_r372159272>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNLNMVKCAA3ZYRYHH3RADROJANCNFSM4KMQ4UEQ>\n> .\n>\n\n\n-- \n - Alex\n", "Close this PR and will make a new one later. Since the PR needs to change the function's return types."]}, {"number": 36282, "title": "What (tf) happened to and in release 1.15.1?", "body": "There was a release of tensorflow-1.15.1 available for a very short time (on Jan 24 2020) that caused a `TypeError: gradients_v2() got an unexpected keyword argument 'colocate_gradients_with_ops'`.\r\n\r\nExact same code that failed with above error on v1.15.1 works with v1.15.0 and v1.15.2.\r\n\r\nNow, the release tag 1.15.1 (https://github.com/tensorflow/tensorflow/tags) is not even available anymore on github! To me that seems very fishy to say the least! So of course I am wondering what happened here?", "comments": ["My speculation - 1.15.1 was tagged with something major wrong and they had to delete the tag so that no one tried to build it. The same thing happen with 1.13.0 when the master branch was tagged instead of the r1.13 branch. Once a tag is used and picked up in forks, you can't reuse it.\r\n\r\nSo 1.15.1 is history. Looking at the commit history for the r1.15 branch I see this: https://github.com/tensorflow/tensorflow/commit/a6d897351e483dfd0418e5cad2900ad9ef24188c\r\n\r\nThat makes me think that 1.15.1 was built using the TensorFlow 2.X build rules, hence the runtime error.  ", "1.15.1 was build using v2 behavior by mistake. So we removed all the pips and tags to make sure it won't get installed and we won't get issues to debug in the future for something we already know we caused as an error.\r\n\r\nApologies for the confusion.\r\n\r\nClosing issue."]}, {"number": 36281, "title": "unsupported operand type(s) for *: 'int' and 'NoneType' in training_v2", "body": "\r\n**System information**\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.1.0\r\n\r\nRunning a simple evaluation of a Lenet Model with MNIST dataset (from TFDS) with a large batch size (e.g. due to using many workers) leads to \r\n\r\n```\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: 2 root error(s) found.\r\n  (0) Out of range:  End of sequence\r\n\t [[node IteratorGetNext_2 (defined at git/tensorflow_tests/train_distributed.py:212) ]]\r\n\t [[metrics/accuracy/div_no_nan/allreduce_1/CollectiveReduce/_70]]\r\n  (1) Out of range:  End of sequence\r\n\t [[node IteratorGetNext_2 (defined at git/tensorflow_tests/train_distributed.py:212) ]]\r\n```\r\n\r\nand\r\n\r\n```\r\nFile \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 152, in run_one_epoch\r\n    total_epochs * steps_per_epoch))\r\nTypeError: unsupported operand type(s) for *: 'int' and 'NoneType'\r\n```\r\n\r\nThe bug is pretty obvious in https://github.com/tensorflow/tensorflow/blob/7b0cfadc895d74aec99875a23ed296506c98e88c/tensorflow/python/keras/engine/training_v2.py#L152 where `steps_per_epoch` is used although it can be (and is) None, see also https://github.com/tensorflow/tensorflow/blob/7b0cfadc895d74aec99875a23ed296506c98e88c/tensorflow/python/keras/engine/training_v2.py#L135", "comments": ["@Flamefire Could you please provide us with simple standalone code to reproduce the issue in our environment, Thanks.", "I'm having trouble reproducing this with a reduced sample. What I did was using the MultiWorkerMirroredStrategy with a simple MNIST test and 2 nodes. For some reason the dataset was not able to produce a single batch and raised OutOfRangeError which lead to the above linked handling code. As the step is still zero the 2nd branch (see link above) is taken and steps_per_epoch is used without checking it for None (which it still is)\r\n\r\nI see if I can come up with some sample code which is small enough but the issue is obvious from reading the code.", "FYI I saw this same failure, but in my case the cause was that I was running with a very small dataset (puny laptop testing...), such that the validation dataset (which I was creating as just the last 10% of the overall set) did not fill a full batch (and I was discarding the remainder, so probably feeding an empty validation set). @Flamefire you mentioned \u201cwith a large batch size\u201d \u2014 might this be your situation as well?", "@gthb I guess so, yes. But there still is a bug in the code as a value which can be None is used expecting it to be not None.", "I guess you are using TF 2.1\r\n\r\nI upgrade to version V2.2.0.rc3. It solved the problem! (This version is not using `training_v2` anymore, btw)\r\n\r\nsrc: https://github.com/tensorflow/tensorflow/releases\r\n\r\nThanks!", "It happened to me while trying to check tf.data.Dataset elements :\r\n`for (image, label) in train_ds.take(3):`\r\n-----   `print(f'{image.numpy().shape} and {label.numpy().shape}')`\r\nstrangely, I restarted the kernel and the error just disappeared. ", "Reviewing the code for v2.3.0-rc0 I conclude that this was fixed", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36281\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36281\">No</a>\n"]}, {"number": 36280, "title": "Keras: Unable to load SavedModel that includes tf.keras.backend.not_equal call", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **No**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.1.0-rc2**\r\n- Python version: **3.7.4**\r\n\r\n**Describe the current behavior**\r\nLoading a Keras model including a call to `tf.keras.backend.not_equal` gives an error:\r\n`ValueError: Could not find matching function to call loaded from the SavedModel`.\r\n\r\nWorks fine on 2.0.0.\r\n\r\n**Describe the expected behavior**\r\nIt should be possible to load a model that includes a call to `tf.keras.backend.not_equal`.\r\n\r\n**Code to reproduce the issue**\r\n``` python\r\nimport tensorflow as tf\r\n\r\ninp = tf.keras.layers.Input((1,1))\r\nmask = tf.keras.backend.not_equal(inp, 0)\r\ngru = tf.keras.layers.GRU(1, name='my-output')(inp, mask=mask)\r\n\r\nmodel = tf.keras.Model(inp, gru)\r\n\r\nmodel.save('my-saved-model')\r\nmodel2 = tf.keras.models.load_model('my-saved-model')\r\n```\r\n\r\n**Other info / logs**\r\nThe snippet above results in the following traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/gustavg/.PyCharm2019.3/config/scratches/scratch_90.py\", line 10, in <module>\r\n    model2 = tf.keras.models.load_model('my-saved-model')\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 150, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 89, in load\r\n    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 552, in load_internal\r\n    export_dir)\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 119, in __init__\r\n    self._finalize()\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 157, in _finalize\r\n    created_layers={layer.name: layer for layer in node.layers})\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1903, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1851, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 773, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py\", line 59, in return_outputs_and_add_losses\r\n    outputs, losses = fn(inputs, *args, **kwargs)\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/gustavg/.virtualenvs/dq2/lib/python3.7/site-packages/tensorflow_core/python/saved_model/function_deserialization.py\", line 262, in restored_function_body\r\n    \"\\n\\n\".join(signature_descriptions)))\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (1 total):\r\n    * Tensor(\"inputs:0\", shape=(None, 1, 1), dtype=float32)\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 1 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (1 total):\r\n    * [TensorSpec(shape=(None, 1, 1), dtype=tf.float32, name='inputs/0')]\r\n  Keyword arguments: {}\r\n```\r\n", "comments": ["Was able to reproduce the issue. Please find the [Gist](https://colab.research.google.com/gist/Saduf2019/4e7c6c479f6124be187dbda990df259c/untitled8.ipynb) here. Thanks!", "The code snippet executes successfully with latest tf nightly version '2.2.0-dev20200129'. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36280\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36280\">No</a>\n"]}, {"number": 36279, "title": "Dimension is expanded for Keras scalar input in eager mode", "body": "**System information**\r\n- Have I written custom code: **YES**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**\r\n- TensorFlow installed from (source or binary): **pip**\r\n- TensorFlow version (use command below): **v2.1.0-rc2-17-ge5bf8de410 2.1.0**\r\n- Python version: **Python 3.7.3**\r\n- CUDA/cuDNN version: **CUDA v10.1 / cuDNN 7.6.4** \r\n- GPU model and memory: **NVIDIA GeForce GTX 1060 with Max-Q Design 6 GB**\r\n\r\n**Describe the current behavior**\r\n\r\nIf input layer for keras model is scalar and model is executing eagerly `model.compile(run_eagerly=True)` then\r\n\r\n- The shape of the input tensor is expanded up to 1 dimension (`TensorShape([16, 1])` instead of `TensorShape([16])` including the batch dimension if batch size is 16.\r\n- Warning is shown `WARNING:tensorflow:Model was constructed with shape Tensor(\"the_input:0\", shape=(16,), dtype=float32) for input (16,), but it was re-called on a Tensor with incompatible shape (16, 1).`\r\n\r\nIf `run_eagerly=False` then the shape is as expected.\r\n\r\nThis is pretty critical in our case because we use scalar input for `label_length` argument of [tf.nn.ctc_loss](https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss) which requires tensor of shape `[batch_size]` and fails with really vague error if shape is `[batch_size,1]`\r\n\r\n**Describe the expected behavior**\r\nThe shape should be kept as specified - `TensorShape([16])` - and no warning is shown.\r\n\r\n**Code to reproduce the issue**\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sys\r\n\r\n\r\nbatch_size = 16\r\ninput_batch = np.ones((batch_size,)).astype(np.float32)\r\n\r\n\r\nclass TestSequence(tf.keras.utils.Sequence):\r\n    def __len__(self):\r\n        return 1\r\n\r\n    def __getitem__(self, idx):\r\n        return input_batch, input_batch\r\n\r\n# Dummy layer to show the tensor shape\r\nclass PrintLayer(tf.keras.layers.Layer):\r\n    def call(self, inputs):\r\n        tf.print(inputs.shape, output_stream=sys.stderr)\r\n        return inputs\r\n\r\n\r\n# The result is the same if shape=() is used to specify scalar input\r\ninput_layer = tf.keras.layers.Input(\r\n    name='the_input', batch_shape=(batch_size,), dtype='float32')\r\nx = PrintLayer()(input_layer)\r\noutput_layer = tf.keras.layers.ReLU()(x)\r\nmodel = tf.keras.models.Model(inputs=[input_layer], outputs=[output_layer])\r\n\r\n# If run_eagerly=False then the shapes are correct\r\nmodel.compile(loss='mse', run_eagerly=True)\r\n\r\nmodel.fit(TestSequence(), epochs=3)\r\n```\r\n**Other info / logs**\r\n\r\nI've tracked the source of this change up to the following line (callstack below)\r\n\r\n```\r\nstandardize_single_array, training_utils.py:454\r\n<listcomp>, training_utils.py:529\r\nstandardize_input_data, training_utils.py:529\r\n_standardize_tensors, training.py:2410\r\n_standardize_user_data, training.py:2383\r\ntrain_on_batch, training_v2_utils.py:416\r\nwrapper, api.py:258\r\n_call_for_each_replica, distribute_lib.py:2164\r\ncall_for_each_replica, distribute_lib.py:1819\r\nexperimental_run_v2, distribute_lib.py:763\r\ndistributed_function, training_v2_utils.py:85\r\nexecution_function, training_v2_utils.py:98\r\nrun_one_epoch, training_v2.py:128\r\nfit, training_v2.py:342\r\nfit, training.py:819\r\n```\r\n\r\nSeems like the expected shapes are not passed if the eager mode is on (see [this line](https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/engine/training.py#L2391)) and hence the error.\r\n\r\nHere is a [link to gist in colab](https://gist.github.com/PavelKovalets/2e0bb75fe234d355773f291dd8c4bdb3).\r\n", "comments": ["i am able to replicate the issue, please find the [gist](https://colab.research.google.com/gist/Saduf2019/f954acee0b06d7b1dbdaabc751ea69a0/untitled11.ipynb) here.", "@PavelKovalets This is expected behavior in both Tensorflow Keras and Keras and is being clearly mentioned [here](https://github.com/keras-team/keras/issues/12058#issuecomment-455453006). ", "@gowthamkpr as for me it is quite strange that the behaviour differs when eager is turned on and off. So if the expected behaviour is to always expand dims of a scalar input, why the shape is not changed if I run it with `run_eagerly=False` - it is `TensorShape([16])`? Shouldn't it be consistent?\r\n\r\nAnd by the way does it mean that it is not possible to have scalar input? If so (which is at least seems like an artificial limitation), then maybe it should be mentioned somewhere in documentation and in the [related issue](https://github.com/tensorflow/tensorflow/issues/14384). ", "@PavelKovalets Thanks for the issue! This is fixed in the latest nightly `pip install -U tf-nightly`\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36279\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36279\">No</a>\n"]}, {"number": 36278, "title": "Support tf.SparseTensor and tf.RaggedTensor in tf.py_function", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tensorflow==2.1.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`tf.py_function` executes eagerly and therefore does support to mix python and TF functions.\r\nCreating instances of  tf.SparseTensor and tf.RaggedTensor is not a problem.\r\nBut if there are in the return statement tf.pyfunction crashes, because I assume its expecting a tf.Tensor.\r\n\r\nExample:\r\n\r\n````\r\ndef eager_py_ragged(arg1_eager):\r\n\r\n    return tf.RaggedTensor.from_tensor(arg1_eager, dtype=tf.int32)\r\n\r\narg1_const = tf.constant(1, dtype=tf.int32)\r\n\r\ntf.py_function(eager_py_sparse, [arg1_const], [tf.int32])\r\n# tf.python.framework.errors_impl.InvalidArgumentError:\r\n# ValueError: Attempt to convert a value (<tensorflow.python.framework.sparse_tensor.SparseTensor object at X)\r\n# with an unsupported type (<class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>) to a Tensor.\r\n````\r\n\r\n\r\n**Will this change the current api? How?**\r\nI dont think so.\r\n**Who will benefit with this feature?**\r\nPeople building their `tf.data` pipeline with `tf.pyfunction`.\r\n**Any Other info.**\r\nI will create more `tf.py_function` related stories. Since this are distinct features from my POV, there multiple feature requests. I hope this is ok.", "comments": ["@VitaliKaiser  Create a PR to convert the sparse tensor and ragged tensor into the uniform tensor", "Updates on this?", "In https://github.com/tensorflow/tensorflow/issues/27679#issuecomment-522578000, I showed how py_function could be extended to handle composite tensor inputs and outputs (and other nested structures, like dicts, tuples, lists, etc).  If you have bandwidth to work on a PR that adds that to TensorFlow (with tests etc.), then it would be very welcome; otherwise, you could just use the `new_py_function` that I defined there, which wraps `tf.py_function`.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Definitely still not working @google-ml-butler", "Please reopen it. Tensorflow should  support it"]}, {"number": 36277, "title": "fix the assert_shapes issue", "body": "fix the assert_shapes issue https://github.com/tensorflow/tensorflow/issues/36268 when input is sparse tensor", "comments": ["@alextp Could you help to review this one also?", "Hi @jaingaurav, Thanks for the review. Theoretically we can. But I am afraid there are much changes we need to make since in the following logic of  assert_shapes function all takes eager_tensor as default input.\r\nAnyway, I will try to do it and back to a review. Thanks.", "@jaingaurav Push up another fix. It seems much of the functions already support sparse_tensors. Please help to have a review again.\r\nAnd this fix has passed the unit test\r\n```\r\nbazel --output_user_root=$build_dir test --config=mkl --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mavx512f --copt=-mavx512cd  //tensorflow/python/kernel_tests:check_ops_test\r\n```\r\nBut I am not sure if any corner case exists.", "Hi @jaingaurav , Sorry to interrupt. But any update :\uff09\uff1f", "> Thanks for the work. Overall, it seems the correct fix is to avoid any `ops.convert_to_tensor` calls altogether since they seems somewhat unnecessary.\r\n\r\nYes, I believe for other functions are already compatible to the input of sparse tensor.   `ops.convert_to_tensor` is the only one we need to focus. ", "@Leslie-Fang Can you please address the reviewer comments and resolve conflicts? Thanks!", "Close this one and figure out a new fix."]}, {"number": 36275, "title": "Build on MacOS for Select Operators TFLite error", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5\r\n- TensorFlow installed from (source or binary): installed from source\r\n- TensorFlow version (use command below):  v2.0.1\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source):  0.26.1\r\n- GCC/Compiler version (if compiling from source):  Apple clang version 11.0.0 (clang-1100.0.33.17)\r\n\r\n**Describe the current behavior**\r\n\r\nWhen build by using  _tensorflow/contrib/makefile/build_all_ios_with_tflite.sh_ the following error occur:\r\n\r\n...\r\ntensorflow/lite/minimal_logging_android.cc:18:10: fatal error: 'android/log.h' file not found\r\n#include <android/log.h>\r\n         ^~~~~~~~~~~~~~~\r\n1 error generated.\r\nmake: *** [/Users/jay/Documents/tensorflow2/tensorflow/contrib/makefile/gen/obj/ios_ARM64/tensorflow/lite/minimal_logging_android.o] Error 1\r\n....\r\n**Describe the expected behavior**\r\n\r\nThe build is completed and generates files, libtensorflow-lite.a, libprotobuf.a, nsync.a, for using in XCode.\r\n\r\n**Code to reproduce the issue**\r\n\r\n- run ./configure \r\n-- setup WORKSPACE \r\n-- choose 'build TensorFlow with iOS support'\r\n- run tensorflow/contrib/makefile/build_all_ios_with_tflite.sh\r\n\r\nthe result of setting up Android WORKSPACE in .tf_configure.bazelrc  is\r\n\r\n...\r\nbuild --action_env ANDROID_NDK_HOME=\"/Users/jay/Library/Android/sdk/ndk/17.2.4988734\"\r\nbuild --action_env ANDROID_NDK_API_LEVEL=\"21\"\r\nbuild --action_env ANDROID_BUILD_TOOLS_VERSION=\"29.0.2\"\r\nbuild --action_env ANDROID_SDK_API_LEVEL=\"27\"\r\nbuild --action_env ANDROID_SDK_HOME=\"/Users/jay/Library/Android/sdk\"\r\n...\r\n\r\n", "comments": ["Over to @yyoon to help.", "@rangsaritv The contribute directory is removed since r2.1 release.\r\nAlso there is an official way of building iOS TFLite using bazel.\r\nhttps://www.tensorflow.org/lite/guide/build_ios\r\n\r\nBefore going though, you need to install some Python packages.\r\nhttps://www.tensorflow.org/install/source\r\n", "@terryheo Thank you very much for your help. :)"]}, {"number": 36273, "title": "Add custom_objects in the function layer_test", "body": "Add a custom_objects argument to layer_test, to enable running layer_test for custom layers.\r\n\r\nreference: https://github.com/tensorflow/tensorflow/issues/36272", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36273) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36273) for more info**.\n\n<!-- ok -->", "Hi Mokke,\r\n\r\nJust realized I accidentally commented on the issue instead of this PR:\r\nThanks for the PR! I have two thoughts about this:\r\n\r\nlayer_test is a totally internal method, we don't want external users relying on it. This change seems explicitly designed for external users of Keras implementing their own layers and trying to use layer_test.\r\n\r\nThe layer_test utility isn't an effective test of layer behavior, it just makes sure certain things don't crash. You would be better served writing your own set of unit tests that actually verify layer behavior (and that make sure training, evaluation, etc. work correctly).\r\n\r\nThat said, I'll add @yhliang2018 to see if she thinks that there are some sort of testing utilities we should be exposing to external users in Keras' API.\r\n\r\n", "@MokkeMeguru Can you please check tomerk's comments and keep us posted? Thanks!", "I waited   @yhliang2018  's comment. \r\nBut I want to say \"If We Want To Update Some Layers are Something Wrong, We Need To Pass Your UNITTEST At Least.\" (refs. https://github.com/tensorflow/tensorflow/issues/18222)\r\nSo this pull request gives you (and us) comfortable development of Tensorflow by removing ridiculous requests. ", "Thanks for the PR! I think it's okay to add an extra arg to `layer_test` to make it work better with custom layers. But the two points mentioned by @tomerk still apply: we may not expose it as an public API, and for your custom layers, you may need to test more to make it behave as expected. ", "response? Should I tell you anything more?\r\nI want to cooperate with this PR.", "@MokkeMeguru Can you please address the reviewers comments and resolve conflicts? Thanks!", "Thanks, @tomerk  !  I updated the document about custom_objects", "I fixed import error for organizing imports.", "I don't know why this pr is not accepted by Unity Sanity with pylint error. \r\nCan you help me?\r\n\r\nlogs.\r\n\r\n```\r\n=== Sanity check step 2 of 15: do_pylint (Python 3 pylint) ===\r\n\r\nERROR_WHITELIST=\"^tensorflow/python/framework/function_test\\.py.*\\[E1123.*noinline ^tensorflow/python/platform/default/_gfile\\.py.*\\[E0301.*non-iterator ^tensorflow/python/platform/default/_googletest\\.py.*\\[E0102.*function\\salready\\sdefined ^tensorflow/python/feature_column/feature_column_test\\.py.*\\[E0110.*abstract-class-instantiated ^tensorflow/contrib/layers/python/layers/feature_column\\.py.*\\[E0110.*abstract-class-instantiated ^tensorflow/contrib/eager/python/evaluator\\.py.*\\[E0202.*method-hidden ^tensorflow/contrib/eager/python/metrics_impl\\.py.*\\[E0202.*method-hidden ^tensorflow/contrib/rate/rate\\.py.*\\[E0202.*method-hidden ^tensorflow/python/training/tracking/tracking\\.py.*\\[E0202.*method-hidden ^tensorflow/python/platform/gfile\\.py.*\\[E0301.*non-iterator ^tensorflow/python/keras/callbacks\\.py.*\\[E1133.*not-an-iterable ^tensorflow/python/keras/engine/base_layer.py.*\\[E0203.*access-member-before-definition ^tensorflow/python/keras/layers/recurrent\\.py.*\\[E0203.*access-member-before-definition ^tensorflow/python/kernel_tests/constant_op_eager_test.py.*\\[E0303.*invalid-length-returned ^tensorflow/python/keras/utils/data_utils.py.*\\[E1102.*not-callable ^tensorflow/python/autograph/.*_py3_test\\.py.*\\[E0001.*syntax-error ^tensorflow/python/keras/preprocessing/image\\.py.*\\[E0240.*Inconsistent method resolution \"\r\nRunning pylint on 2595 files with 32 parallel jobs...\r\n\r\n\r\npylint took 160 s\r\n\r\nFound a whitelisted error:\r\n  tensorflow/python/autograph/pyct/static_analysis/activity_py3_test.py:52: [E0001(syntax-error), ] invalid syntax\r\nFound a whitelisted error:\r\n  tensorflow/python/autograph/impl/api_test.py:450: [E0202(method-hidden), ApiTest.test_converted_call_callable_metaclass.TestMetaclass.__call__] An attribute defined in api_test line 460 hides this method\r\nFound a whitelisted error:\r\n  tensorflow/python/distribute/multi_worker_test_base.py:196: [E1120(no-value-for-parameter), create_in_process_cluster] No value for argument 'reason' in unbound method call\r\nFound a whitelisted error:\r\n  tensorflow/python/kernel_tests/constant_op_eager_test.py:250: [E0303(invalid-length-returned), ConstantTest.testInvalidLength.BadList.__len__] __len__ does not return non-negative integer\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3293: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_invalid_type] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3293: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_invalid_type] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3300: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_one_feature_column] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3300: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_one_feature_column] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3310: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_two_feature_columns] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3310: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_two_feature_columns] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3311: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_two_feature_columns] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3311: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_two_feature_columns] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3323: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_equal_keys_different_parse_spec] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3323: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_equal_keys_different_parse_spec] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3324: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_equal_keys_different_parse_spec] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3324: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_equal_keys_different_parse_spec] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3331: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_equal_keys_equal_parse_spec] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3331: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_equal_keys_equal_parse_spec] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3332: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_equal_keys_equal_parse_spec] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3332: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_equal_keys_equal_parse_spec] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3345: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_multiple_features_dict] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3345: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_multiple_features_dict] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3346: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_multiple_features_dict] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/feature_column/feature_column_test.py:3346: [E0110(abstract-class-instantiated), MakeParseExampleSpecTest.test_multiple_features_dict] Abstract class '_TestFeatureColumn' with abstract methods instantiated\r\nFound a whitelisted error:\r\n  tensorflow/python/keras/saving/saved_model/json_utils.py:37: [E0202(method-hidden), Encoder.default] An attribute defined in json.encoder line 158 hides this method\r\nFound a whitelisted error:\r\n  tensorflow/python/keras/utils/data_utils.py:604: [E1102(not-callable), terminate_keras_multiprocessing_pools] multiprocessing.active_children is not callable\r\nFound a whitelisted error:\r\n  tensorflow/python/keras/utils/data_utils.py:896: [E1102(not-callable), init_pool_generator] multiprocessing.current_process is not callable\r\nFound a whitelisted error:\r\n  tensorflow/python/keras/utils/version_utils_test.py:120: [E0110(abstract-class-instantiated), SplitUtilsTest.test_user_provided_metaclass] Abstract class 'AbstractModel' with abstract methods instantiated\r\n\r\nFAIL: Found 2 non-whitelisted pylint errors:\r\ntensorflow/python/keras/testing_utils.py:201: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\n\r\ntensorflow/python/keras/testing_utils.py:257: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\n```", "this means we should fix the code, \r\n\r\n```\r\n  recovered_model = keras.models.Model.from_config(\r\n    model_config, custom_objects)\r\n```\r\nto \r\n\r\n```\r\n  recovered_model = keras.models.Model.from_config(\r\n      model_config, custom_objects)\r\n```\r\n? (two space to four space)", "I updated PR for passing your lint test.\r\nOther tests were passed.", "I don't know why windows build will be crash.\r\nI watched its log, but this error is no related to my PR.", "I updated the description because I *think* the curly braces in the code snippets are causing issues with the merging.\r\n\r\n@gbaned how do we restart the merging?\r\nHere is the code snippet I removed:\r\n\r\nUsage:\r\n```\r\ntesting_utils.layer_test(\r\n    SyncBatchNormalization,\r\n    # keras.layers.BatchNormalization,\r\n    kwargs={\r\n      'momentum': 0.9,\r\n      'epsilon': 0.1,\r\n      'gamma_regularizer': keras.regularizers.l2(0.01),\r\n      'beta_regularizer': keras.regularizers.l2(0.01)\r\n    },\r\n    input_shape=(3, 4, 2),\r\n    custom_objects={'SyncBatchNormalization': SyncBatchNormalization})\r\n```\r\n", "Thanks a lot!"]}, {"number": 36272, "title": "Able to Custom Layer's unit test", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI want to write a correct batch normalization in MultiGPU/TPU.\r\nI wrote some [code](https://gist.github.com/MokkeMeguru/35af0c7ddba511f6a268e7c78fdba2d6), but I cannot do your unit test.\r\n\r\n**Will this change the current api? How?**\r\nAdd any argument for add custom layer in the function [layer_test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/testing_utils.py#L75-L78)\r\n\r\n**Who will benefit with this feature?**\r\n1. Someone wants to fix an official layer.\r\n2. Someone wants to create some layers.\r\n \r\n**Any Other info.**\r\n", "comments": ["So, we can change the layer_test function as follows\r\n```python\r\n\r\ndef layer_test(layer_cls, kwargs=None, input_shape=None, input_dtype=None,\r\n               input_data=None, expected_output=None,\r\n               expected_output_dtype=None, expected_output_shape=None,\r\n               validate_training=True, adapt_data=None,\r\n               custom_objects=None): # add here\r\n # ...\r\n recovered_model = keras.models.Model.from_config(model_config, custom_objects) # change here\r\n# ...\r\n recovered_model = keras.models.Sequential.from_config(model_config, custom_objects)\r\n```", "Usage:\r\n```python\r\ntesting_utils.layer_test(\r\n    SyncBatchNormalization,\r\n    # keras.layers.BatchNormalization,\r\n    kwargs={\r\n      'momentum': 0.9,\r\n      'epsilon': 0.1,\r\n      'gamma_regularizer': keras.regularizers.l2(0.01),\r\n      'beta_regularizer': keras.regularizers.l2(0.01)\r\n    },\r\n    input_shape=(3, 4, 2),\r\n    custom_objects={'SyncBatchNormalization': SyncBatchNormalization})\r\n```", "Hi Mokke,\r\nThanks for the PR! I have two thoughts about this:\r\n\r\n1. layer_test is a totally internal method, we don't want external users relying on it. This change seems explicitly designed for external users of Keras implementing their own layers and trying to use layer_test.\r\n\r\n2. The layer_test utility isn't an effective test of layer behavior, it just makes sure certain things don't crash. You would be better served writing your own set of unit tests that actually verify layer behavior (and that make sure training, evaluation, etc. work correctly).\r\n\r\nThat said, I'll add @yhliang2018 to see if she thinks that there are some sort of testing utilities we should be exposing to external users in Keras' API.", "Hi @MokkeMeguru you could have another test_serialization unit test which can cover this, for example, see [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/losses_test.py#L166-L170)\r\n\r\nClosing it for now. Let us know if you have other questions!"]}, {"number": 36271, "title": "Clarify conversion used for Quantization", "body": "In response to the issue Explain #35072 opened by @petewarden \r\n\r\n> how int8 input and output quantization conversion works in TensorFlow Lite \r\n\r\nThe converter uses the Standard score to standardize the input stats. The standardization is interchangeably known and used here as Quantization. \r\n", "comments": ["@talumbau  @BeenKim I see that I mistakenly added 3 reviewers even though one was enough for such a trivial Pull. Please consider helping me in sorting it out. Kindly review it or (as alexalami has already approved it) you could also remove yourself from the reviewers' list. :) \r\nIt would definitely save you all a lot of redundant worry. ", "Not sure if you meant to add me here - not sure I'm relevant. :)", "Yeah that's what I apologise for. Please cancel my review request as it has already been reviewed.<0_0>"]}, {"number": 36270, "title": "TFLite FP16 network Fully connected layer failing on gpu in android", "body": "Using tensorflow 2.1 on both python as well as android. Quantized the model to fp16 in python and successfully run inference tests with python interpretor on ubuntu/mac.\r\n\r\nBut when I deployed the same network on android edge device with tflite java 2.1 pre-built and enabled gpu delegate, its failing with below error. \r\n\r\n\r\ntensorflow/lite/kernels/fully_connected.cc:105 filter->type != kTfLiteFloat32 (10 != 1)\r\n    Node number 187 (FULLY_CONNECTED) failed to prepare.\r\n    \r\n    tensorflow/lite/kernels/conv.cc:272 bias->type != input_type (1\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:336)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:82)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:48)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:175)", "comments": ["@jkparuchuri,\r\nIs this still an issue?\r\n\r\nCould you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36270\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36270\">No</a>\n"]}, {"number": 36269, "title": "ModuleNotFoundError: No module named 'tensorflow.contrib'", "body": "Hi. Can anyone help me with this existing problem?\r\n\r\nThis is the error.\r\n2020-01-28 02:46:01.200715: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 48, in <module>\r\n    from tensorflow.contrib import framework as contrib_framework\r\n**ModuleNotFoundError: No module named 'tensorflow.contrib'**\r\n\r\nIt is from train.py in Tensorflow:\r\n**from tensorflow.contrib import framework as contrib_framework** line 48\r\n\r\nTensorflow Version: 1.14.0\r\nPython3 Version: 3.7.3\r\n\r\nI am currently coding in Raspberry Pi.", "comments": ["@KoizumiNao Could you please provide us with simple standalone code to reproduce the issue in our environment, Thanks", "I tried to train data using this line in the terminal:\r\n**python3 train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config**", "Same problem but on local machine: \r\n**python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_inception_v2_coco.config**", "@KoizumiNao  Please refer to this work around [#31350](https://github.com/tensorflow/tensorflow/issues/31350#issuecomment-520958043). And let us know if the issue still exist.", "I found the solution. I installed **1.13.1** of TensorFlow instead. And I tried to run by python3. There's a lot of depreciation warnings. Thank you for the solution, sir."]}, {"number": 36268, "title": "tf.debugging.assert_shapes() does not work for SparseTensor", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.15\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\n`tf.debugging.assert_shapes` cannot be used with sparse tensors.\r\n\r\n**Describe the expected behavior**\r\n`tf.debugging.assert_shapes` should allow you to mix and match dense and sparse tensors when checking for dimensional consistency.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nA = tf.range(3)\r\ntf.debugging.assert_shapes(((A, [3]),))  # works\r\n# raises \"ValueError: Attempt to convert a value (...) with an unsupported type (<class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>) to a Tensor.\r\ntf.debugging.assert_shapes(((tf.sparse.from_dense(A), [3]),))\r\n```", "comments": ["@terhorst  I believe the input type of sparse_tensor is not the typical use-case for tf.debugging.assert_shapes. Neverthless it shouldn't raise the error.\r\nI have made a PR to fix it: https://github.com/tensorflow/tensorflow/pull/36277\r\nPlease have a check if needed.", "This fixes it, but converting the sparse tensor to dense solely for the purpose of checking its shape seems quite wasteful to me.", "Hi @terhorst, I think this function is used for debugging only, right? If so, the efficiency shouldn't be the major problem. And I think this fix changes the least number of code which has lower possibility to induce other issues.  ", "In my case, the whole reason for using a SparseTensor in the first place is\nthat it cannot fit into memory as dense. With this patch, assert_shapes()\nwill produce an OOM or some other form of crash, which seems like a\nregression since that is strictly more annoying and harder to debug than\nmismatched dimensions. But, it's your call; my only point in submitting\nthis issue was to bring it to your attention, and I don't have the\nbandwidth to learn how to patch it myself.\n\nOn Tue, Jan 28, 2020 at 8:25 PM Leslie-Fang <notifications@github.com>\nwrote:\n\n> Hi @terhorst <https://github.com/terhorst>, I think this function is used\n> for debugging only, right? If so, the efficiency shouldn't be the major\n> problem. And I think this fix changes the least number of code which has\n> lower possibility to induce other issues.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36268?email_source=notifications&email_token=AAAEOHAT2RLP3LNW6REXZF3RADLJRA5CNFSM4KMLQMXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEKFT6RA#issuecomment-579551044>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAAEOHEWTU3R6XWWVPRF2E3RADLJRANCNFSM4KMLQMXA>\n> .\n>\n\n\n-- \nJonathan\nterhorst@gmail.com\n", "I have tried on colab with TF version 2.1.0-rc2, 2.2.0-dev20200128 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/76389a872dc7528d8ba29760a345c37b/untitled596.ipynb).Thanks!", "I am able to reproduce the isue with TF 2.3-rc1, nightly version(`2.4.0-dev20200721`).PLease, find the gist [here](https://colab.research.google.com/gist/ravikyram/4315d3365fcc9c7170084261951e80eb/untitled162.ipynb).Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36268\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36268\">No</a>\n"]}, {"number": 36267, "title": "[ROCm] Reverting ROCm to use MIOpen Find Mode APIs (be default) for convolution", "body": "This PR reverts ROCm to use MIOpen Find Mode APIs (be default) for convolution.  The use of MIOpen Immediate Mode API (instead of the Find Mode APIs) can be specified by the setting the env var `TF_ROCM_USE_IMMEDIATE_MODE=1`.\r\n\r\nAlmost all of the changes in this PR are within code that is specific to the ROCm platform, so this PR should not have any impact on non ROCm builds.\r\n\r\n----------------\r\n\r\n\r\n/cc @chsigg @whchung ", "comments": ["@deven-amd Can you please resolve conflicts? Thanks!", "@gbaned, I have rebased the PR to resolve the merge conflicts", "@gbaned, gentle ping", "gentle ping", "gentle ping", "@deven-amd Can you please resolve conflicts? Thanks!", "@gbaned rebase done to remove merge conflicts.\r\n\r\n@cheshire please re-approve.\r\n\r\nthanks again", "@deven-amd Still, conflicts appearing, can you please resolve those? Thanks!", "@gbaned , it seems like this PR did get merged (but not marked as such) and is now causing merge conflicts with itself. Assuming this is correct, how do we go about marking this PR as \"merged\".\r\n\r\nthanks", "It was merged in c6667ea, but I'm not aware of any way to mark this PR as merged manually. I think the only option is to just close it. "]}, {"number": 36266, "title": "Check failed: 0 <= new_num_elements", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n```\r\n== check python ===================================================\r\npython version: 3.7.3\r\npython branch:\r\npython build version: ('default', 'Jun 27 2019 23:31:30')\r\npython compiler version: GCC 5.5.0\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #1 SMP Tue Jul 30 17:17:50 UTC 2019\r\nos release version: 4.9.184-0.1.ac.235.83.329.metal1.x86_64\r\nos platform: Linux-4.9.184-0.1.ac.235.83.329.metal1.x86_64-x86_64-with-redhat-5.3-Tikanga\r\nlinux distribution: ('Red Hat Enterprise Linux Server', '5.3', 'Tikanga')\r\nlinux os distribution: ('redhat', '5.3', 'Tikanga')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='dev-dsk-jsahewal-1b-01f89cec.us-east-1.amazon.com', release='4.9.184-0.1.ac.235.83.329.metal1.x86_64', version='#1 SMP Tue Jul 30 17:17:50 UTC 2019', machine='x86_64', processor='x86_64')\r\narchitecture: ('64bit', 'ELF')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Homebrew gcc 5.5.0_4) 5.5.0\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy                             1.18.1\r\nprotobuf                          3.9.2\r\ntensorflow-cpu                    2.1.0\r\ntensorflow-estimator              2.1.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.1.0\r\ntf.version.GIT_VERSION = v2.1.0-rc2-17-ge5bf8de\r\ntf.version.COMPILER_VERSION = 7.3.1 20180303\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 145: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 7, 3, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n```\r\n\r\n**Describe the current behavior**\r\n```\r\n2020-01-27 22:56:12.796098: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2800045000 Hz\r\n2020-01-27 22:56:12.797977: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3bad450 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-27 22:56:12.798009: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-01-27 22:56:12.806071: F tensorflow/core/framework/tensor_shape.cc:353] Check failed: 0 <= new_num_elements (0 vs. -4523975925047186833)\r\n[1]    10698 abort      python3 demo.py\r\n```\r\n\r\n**Describe the expected behavior**\r\nShould not abort\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import feature_column\r\nfrom tensorflow.keras import layers\r\n\r\n# A utility method to create a tf.data dataset from a Pandas Dataframe\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n    dataframe = dataframe.copy()\r\n    labels = dataframe.pop('target')\r\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n    if shuffle:\r\n        ds = ds.shuffle(buffer_size=len(dataframe))\r\n    ds = ds.batch(batch_size)\r\n    return ds\r\n\r\nURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\ndataframe = pd.read_csv(URL)\r\n\r\n# Replicate the dataFrame to make it a bit bigger\r\ntemp = dataframe.copy()\r\nfor _ in range(10):\r\n    dataframe = pd.concat([dataframe, temp.copy()])\r\n\r\n# Replicate the columns to make it a bit bigger\r\ncol_lst = dataframe.columns.tolist()\r\nfor i in range(10):\r\n    for ind, col in enumerate(col_lst):\r\n        dataframe[f'{i+1}_{ind}'] = dataframe[col]\r\n\r\nbatch_size = 5 # A small batch sized is used for demonstration purposes\r\ndataframe_ds = df_to_dataset(dataframe, batch_size=batch_size)\r\n```", "comments": ["You are allocating a tensor with too many elements, and there is a check that prevents that from happening.", "But, shape of the dataFrame is just (3333, 154) after replicating rows and columns. And, I have around 64 GB RAM machine with 50 GB free.", "Didn't reproduce this issue with commit-id: 0c9a5414d4ac21a52ddd2ee441afe69113bd8bc4 in master branch.", "I'm having a similar issue. Everything's ok if I create a dataset from a `pd.DataFrame` containing 100k samples. However, if I open a `pd.DataFrame` containing 1k samples, then duplicate it to 2k samples, it crashes.\r\n\r\nI'm working on TF 2.0.\r\n\r\nedit: I have found a strange workaround.\r\n\r\n- In the simple case I add each column's value to a dict that I pass to `tf.data.Dataset`:\r\n```python\r\nfor c in dataframe.columns:\r\n    input_dict[c] = dataframe[c]\r\ndataset = tf.data.Dataset.from_tensor_slices(input_dict)\r\n```\r\nAnd it works fine.\r\n- In the problematic case I duplicate the dataframe using `dataframe = pd.concat((dataframe, dataframe))` before adding values as in the first case\r\n- __The working workaround__ is to add the values using:\r\n```python\r\nfor c in dataframe.columns:\r\n    input_dict[c] = dataframe[c].tolist() * 2\r\ndataset = tf.data.Dataset.from_tensor_slices(input_dict)\r\n```\r\nwhich works!\r\nNote that trying to add the column cast into a list will fail if the column contains a dict or something that cannot be converted into a Tensor. But duplicating the list does not throw this error...\r\n\r\n@jayantsahewal This does not completely solve your problem since you perform more complicated dataframe manipulation but it could be a starting point.", "This trick will skip the issue:\r\n\r\n```python3\r\n# if df is merged  from several data source\r\n# such as df=pd.concat(df_a, df_b)\r\n# create tensor will crash\r\ntf.data.Dataset.from_tensor_slices((dict(df), target))\r\n\r\n# but copy the df will fix the issue\r\ndf.to_csv('tmp', index=False)\r\ndf1 = pd.read_csv('tmp')\r\ntf.data.Dataset.from_tensor_slices((dict(df1), target))\r\n```\r\n\r\nI think this may cause by tensor shape computation, although i dont have time to dig deeper.\r\n\r\n", "Thanks @valsworthen and @littlebeandog I will try out these solutions.", "I also had this issue doing cross-fold validation and concatenating folds together to form the training set. Concatenated dataframes seem to fail. A simple solution that worked for me was to just reset the dataframe index after concatenating.\r\n\r\ndf = pd.concat(df_a, df_b)\r\ndf = df.reset_index()", "There's a possibility of shape checks failing when the defined batch size is too large to be supported by the GPU VRAM.", "I did not face any problem in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/2c1773099db1ab3469399e1ed55426ad/35650.ipynb).", "Seems to no longer be reproducible", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36266\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36266\">No</a>\n"]}, {"number": 36265, "title": "[TF-Micro]  Adding option to specify the toolchain paths as part of the generate projects build process", "body": "to build make projects with the full path to the build tools run \r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile generate_hello_world_make_project  TENSORFLOW_ROOT=<path_to_tensorflow_root>\r\n\r\nThe makefiles generated int he project build will now explicitly set the compiler flags.\r\n\r\nThe make template files can now set\r\n\r\nTARGET_TOOLCHAIN_PREFIX := \r\nTARGET_TOOLCHAIN_ROOT := \r\n\r\nas well for their specific platforms. See updated bluepill as an example.\r\n\r\n", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36265) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36265) for more info**.\n\n<!-- ok -->", "has an issue. will resubmit."]}, {"number": 36264, "title": "Fix flatbuffers link error", "body": "Fix flatbuffers link error\r\n\r\nThis commit:\r\ntensorflow@adf6e22#diff-4bdae93a605044a27f9df49a92bcf398L129\r\n\r\nRemoved the linkopts section from the cc_binary 'flatc' from\r\nthird_party/flatbuffers/BUILD.bazel. That caused builds to fail with:\r\n\r\n/usr/bin/ld: bazel-out/host/bin/external/flatbuffers/src/libflatbuffers.a(idl_parser.o): undefined reference to symbol 'cos@@GLIBC_2.17'\r\n/dt7/lib/powerpc64le-linux-gnu/libm.so.6: error adding symbols: DSO missing from command line\r\ncollect2: error: ld returned 1 exit status\r\n\r\n(This is GPU builds on ppc64le)\r\n\r\nI tried passing `--linkopt=-lm` on the bazel build command but it still failed.\r\nRestoring the original linkopts section does resolve this problem. (and it\r\nseems better to have the linkopt here than globally for the entire build)\r\n\r\nA simple `linkopts = [\"-lm\"],` works for ppc64le, but I figured for windows and\r\nother platforms it was better to restore the original values.", "comments": ["FYI @petewarden as this changes your recent commit."]}, {"number": 36263, "title": "[ROCm] Adding ROCm support for CTC Loss", "body": "/cc @whchung @chsigg ", "comments": []}, {"number": 36262, "title": "Possible incorrect implementation of inception_v3.py", "body": "\"Mixed 0\" between lines 138-152 is the implementation of the figure 5 in the paper. There should be 4 branches that does and have the layer name as :\r\n\r\nbranch 1x1 = 1x1 conv\r\nbranch pool = pooling + 1x1 conv\r\nbranch 3x3 = 1x1 conv + 3x3 conv\r\nbranch 5x5 = 1x1 conv + 3x3 conv +3x3 conv\r\n\r\nThe last one is is named as branch3x3dbl in the implementation and its fine doing two 3x3 convs to substitute 5x5 conv, but what should be branch 3x3 is named as branch 5x5 and involves a 5x5 convolution which contradicts the paper. So lines 140 and 141 should be rewritten as:\r\n```\r\n  branch3x3 = conv2d_bn(x, 48, 1, 1)\r\n  branch3x3 = conv2d_bn(branch3x3, 64, 3, 3)  \r\n```\r\n\r\nThis is not limited to this section and followup codes have the same wrong implementations. I think it should be taken care of. The idea of avoiding bigger convolutions is the main point of this paper and 5x5 convolutions are taking part in the code regardless of this fact. \r\n\r\n\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nPlease, provide simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (Windown 10-64 bit):\r\n- TensorFlow installed from (source or binary): binary from conda\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: GTX 1070\r\n\r\n**Describe the current behavior**\r\n\"Mixed 0\" between lines 138-152 is the implementation of the figure 5 in the paper. There should be 4 branches that does and have the layer name as :\r\n\r\nbranch 1x1 = 1x1 conv\r\nbranch pool = pooling + 1x1 conv\r\nbranch 3x3 = 1x1 conv + 3x3 conv\r\nbranch 5x5 = 1x1 conv + 3x3 conv +3x3 conv\r\n\r\nThe last one is is named as branch3x3dbl in the implementation and its fine doing two 3x3 convs to substitute 5x5 conv, but what should be branch 3x3 is named as branch 5x5 and involves a 5x5 convolution which contradicts the paper. So lines 140 and 141 should be rewritten as:\r\n\r\n```\r\n\r\n  branch3x3 = conv2d_bn(x, 48, 1, 1)\r\n  branch3x3 = conv2d_bn(branch3x3, 64, 3, 3) \r\n```\r\n\r\nThis is not limited to this section and followup codes have the same wrong implementations. I think it should be taken care of. The idea of avoiding bigger convolutions is the main point of this paper and 5x5 convolutions are taking part in the code regardless of this fact.\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "@colt18 \r\nRequest you to provide colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@colt18 \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36261, "title": "Remove 2.1.x Python 2 docker images from spec", "body": null, "comments": []}, {"number": 36260, "title": "Remove 2.0.x Python 2 docker images", "body": null, "comments": []}, {"number": 36259, "title": "Error registering Keras serializable loss", "body": "**System information**\r\n- Have I written custom code): yes\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.1\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nUsing the `tf.keras.utils.register_keras_serializable` decorator does not allow correct serialization/restoration of a custom `tf.keras.losses.Loss` subclass without using a `custom_objects` argument.\r\n\r\n**Describe the expected behavior**\r\n\r\nUsing `tf.keras.utils.register_keras_serializable` allows serialization/restoration of custom losses without using a `custom_objects` argument.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\n@tf.keras.utils.register_keras_serializable()\r\nclass CustomLoss(tf.keras.losses.MeanSquaredError):\r\n    pass\r\n\r\n\r\nmodel = tf.keras.Sequential([tf.keras.layers.Dense(units=1, input_shape=(1,))])\r\nmodel.compile(optimizer='sgd', loss=CustomLoss())\r\nmodel.save('model')\r\n\r\n# ValueError: Unknown loss function: CustomLoss\r\ntf.keras.models.load_model('model', compile=True)\r\n```\r\n\r\n**Other info / logs**\r\nRelevant traceback:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/fd242bad45bd9778d61ad116001fd4b191e51c30/tensorflow/python/keras/saving/save.py#L190\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/fd242bad45bd9778d61ad116001fd4b191e51c30/tensorflow/python/keras/saving/saved_model/load.py#L114-L115\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/fd242bad45bd9778d61ad116001fd4b191e51c30/tensorflow/python/keras/saving/saving_utils.py#L259\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/fd242bad45bd9778d61ad116001fd4b191e51c30/tensorflow/python/keras/losses.py#L1301-L1305\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/fd242bad45bd9778d61ad116001fd4b191e51c30/tensorflow/python/keras/utils/generic_utils.py#L361-L362\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/fd242bad45bd9778d61ad116001fd4b191e51c30/tensorflow/python/keras/utils/generic_utils.py#L321", "comments": ["I have tried on colab with TF version 2.1.0-rc2, 2.2.0-dev20200128 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/55d11b811078c362ff6e49379da72353/untitled597.ipynb).Thanks!", "I found this bug too and fixed in #37018", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36259\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36259\">No</a>\n"]}, {"number": 36258, "title": "Stop publishing nightly python 2 images, and note future devel Python 2 removal", "body": "This change removes Python 2 docker images from our publishing specification,\nin accordance with Python 2's EOL and the end of new Python 2 TensorFlow\npackages in PyPI.\n\nPlease do not merge until the announcement email has been sent to\ndevelopers@tensorflow.org.", "comments": []}, {"number": 36257, "title": "Remove 1.15.x Python 2 docker images", "body": "This change removes Python 2 images from our Dockerfile spec definition. A\nsimilar change is on the way for master, as well as an annoucement email\nfurther clarifying our plans.", "comments": ["Approving but probably will have to be merged only when we do a new patch release.\r\n\r\nDo we need a similar one for `r2.0` and for `r2.1` branches?", ">Do we need a similar one for r2.0 and for r2.1 branches?\r\n\r\nYes. I'll prepare those now."]}, {"number": 36256, "title": "Tensorflow 2.1 Jacobian fails when using Keras LeakyReLU Layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. See attached test case.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Unsure\r\n- TensorFlow installed from (source or binary): Binary installed from pip\r\n- TensorFlow version (use command below): ('v2.1.0-rc2-17-ge5bf8de410', '2.1.0')\r\n- Python version: Python 2.7.15 and Python 3.7.0\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nComputing the Jacobian using the Gradient tape function fails when a the LeakyReLU Keras layer is used.\r\n\r\n**Describe the expected behavior**\r\nIt was expected to calculate the Jacobian of the model with respect to the trainable parameters.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow\r\n\r\nmodel = tensorflow.keras.models.Sequential()\r\nmodel.add(tensorflow.keras.layers.Dense(2,\r\n                                        input_dim=1,\r\n                                        use_bias=True))\r\nmodel.add(tensorflow.keras.layers.LeakyReLU())\r\n#model.add(tensorflow.keras.layers.ReLU())\r\nmodel.add(tensorflow.keras.layers.Dense(1,\r\n                                        use_bias=True))\r\n\r\ninputs = tensorflow.random.uniform((100,1), 1.0, -1.0)\r\n\r\nwith tensorflow.GradientTape(watch_accessed_variables=False) as tape:\r\n\ttape.watch(model.trainable_weights)\r\n\ttemp = model(inputs)\r\n\r\njacobian = tape.jacobian(temp, model.trainable_weights)\r\n```\r\n\r\n**Other info / logs**\r\nIt produces the following error messages.\r\n\r\n*** Python 2.7.15 Error Output ***\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 18, in <module>\r\n    jacobian = tape.jacobian(temp, model.trainable_weights)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/eager/backprop.py\", line 1121, in jacobian\r\n    sys.exc_info()[2])\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/eager/backprop.py\", line 1113, in jacobian\r\n    parallel_iterations=parallel_iterations)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py\", line 189, in pfor\r\n    return f()\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 2362, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n\r\n    /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:183 f  *\r\n        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n    /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:256 _pfor_impl\r\n        outputs.append(converter.convert(loop_fn_output))\r\n    /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1280 convert\r\n        output = self._convert_helper(y)\r\n    /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1460 _convert_helper\r\n        (y_op.type, y_op, converted_inputs))\r\n\r\n    ValueError: No converter defined for LeakyReluGrad\r\n    name: \"loop_body/LeakyReluGrad\"\r\n    op: \"LeakyReluGrad\"\r\n    input: \"loop_body/MatMul\"\r\n    input: \"loop_body/LeakyReluGrad/features\"\r\n    attr {\r\n      key: \"T\"\r\n      value {\r\n        type: DT_FLOAT\r\n      }\r\n    }\r\n    attr {\r\n      key: \"alpha\"\r\n      value {\r\n        f: 0.300000011921\r\n      }\r\n    }\r\n    \r\n    inputs: [WrappedTensor(t=<tf.Tensor 'loop_body/MatMul/pfor/Reshape_1:0' shape=(100, 100, 2) dtype=float32>, is_stacked=True, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'loop_body/LeakyReluGrad/features:0' shape=(100, 2) dtype=float32>, is_stacked=False, is_sparse_stacked=False)]. \r\n    Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower\r\n\r\nEncountered an exception while vectorizing the jacobian computation. Vectorization can be disabled by setting experimental_use_pfor to False.\r\n```\r\n\r\n*** Python 3.7.0 Error Output***\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\", line 1113, in jacobian\r\n    parallel_iterations=parallel_iterations)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py\", line 189, in pfor\r\n    return f()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2362, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n\r\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:183 f  *\r\n        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:256 _pfor_impl\r\n        outputs.append(converter.convert(loop_fn_output))\r\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1280 convert\r\n        output = self._convert_helper(y)\r\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1460 _convert_helper\r\n        (y_op.type, y_op, converted_inputs))\r\n\r\n    ValueError: No converter defined for LeakyReluGrad\r\n    name: \"loop_body/LeakyReluGrad\"\r\n    op: \"LeakyReluGrad\"\r\n    input: \"loop_body/MatMul\"\r\n    input: \"loop_body/LeakyReluGrad/features\"\r\n    attr {\r\n      key: \"T\"\r\n      value {\r\n        type: DT_FLOAT\r\n      }\r\n    }\r\n    attr {\r\n      key: \"alpha\"\r\n      value {\r\n        f: 0.30000001192092896\r\n      }\r\n    }\r\n    \r\n    inputs: [WrappedTensor(t=<tf.Tensor 'loop_body/MatMul/pfor/Reshape_1:0' shape=(100, 100, 2) dtype=float32>, is_stacked=True, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'loop_body/LeakyReluGrad/features:0' shape=(100, 2) dtype=float32>, is_stacked=False, is_sparse_stacked=False)]. \r\n    Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 18, in <module>\r\n    jacobian = tape.jacobian(temp, model.trainable_weights)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\", line 1121, in jacobian\r\n    sys.exc_info()[2])\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/six.py\", line 702, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\", line 1113, in jacobian\r\n    parallel_iterations=parallel_iterations)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py\", line 189, in pfor\r\n    return f()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2362, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n\r\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:183 f  *\r\n        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:256 _pfor_impl\r\n        outputs.append(converter.convert(loop_fn_output))\r\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1280 convert\r\n        output = self._convert_helper(y)\r\n    /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1460 _convert_helper\r\n        (y_op.type, y_op, converted_inputs))\r\n\r\n    ValueError: No converter defined for LeakyReluGrad\r\n    name: \"loop_body/LeakyReluGrad\"\r\n    op: \"LeakyReluGrad\"\r\n    input: \"loop_body/MatMul\"\r\n    input: \"loop_body/LeakyReluGrad/features\"\r\n    attr {\r\n      key: \"T\"\r\n      value {\r\n        type: DT_FLOAT\r\n      }\r\n    }\r\n    attr {\r\n      key: \"alpha\"\r\n      value {\r\n        f: 0.30000001192092896\r\n      }\r\n    }\r\n    \r\n    inputs: [WrappedTensor(t=<tf.Tensor 'loop_body/MatMul/pfor/Reshape_1:0' shape=(100, 100, 2) dtype=float32>, is_stacked=True, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'loop_body/LeakyReluGrad/features:0' shape=(100, 2) dtype=float32>, is_stacked=False, is_sparse_stacked=False)]. \r\n    Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower\r\n\r\nEncountered an exception while vectorizing the jacobian computation. Vectorization can be disabled by setting experimental_use_pfor to False.\r\n```", "comments": ["@cianciosa \r\nI have tried on colab with TF version 2.1.0-rc2 .Please find the gist for [python3](https://colab.research.google.com/gist/ravikyram/2b9d8404b0c8e2b1969602e563db4dc0/untitled598.ipynb) and [python2](https://colab.research.google.com/gist/ravikyram/9b62bd4eaf07da2912ac14990f93a34a/untitled599.ipynb) here. Is this the expected behavior?. Thanks!", "Since a LeakyReLU function has a well defined although discontinuous first derivative, I would have expected that computing the jacobian would proceed without throwing an exception.\r\n\r\nIf you comment out the LeakyReLU line and uncomment the normal ReLU line,\r\n```\r\n#model.add(tensorflow.keras.layers.LeakyReLU())\r\nmodel.add(tensorflow.keras.layers.ReLU())\r\n```\r\nthen you would see the behavior more a kin to what I would have expected.", "@cianciosa I cannot reproduce the error when I ran your code with recent `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/2da0fa8376324480ef1b7abea655b48e/untitled61.ipynb) is the gist for your reference. Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "Verified it's fixed via tf_nightly-2.2.0.dev20200402.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36256\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36256\">No</a>\n"]}, {"number": 36255, "title": "GradientTape does not track intermediate variables inside a tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- TensorFlow installed from (source or binary): \r\nBinary\r\n- TensorFlow version (use command below):\r\n2.2.0-dev20200123\r\n- Python version:\r\n3.7.3\r\n- CUDA/cuDNN version: 10.1 / \r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 6\r\n#define CUDNN_PATCHLEVEL 4\r\n- GPU model and memory:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro P3200        On   | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   54C    P3    24W /  N/A |   5911MiB /  6078MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1326      G   /usr/lib/xorg/Xorg                           282MiB |\r\n|    0      2272      G   kwin_x11                                      53MiB |\r\n|    0      2279      G   /usr/bin/krunner                             105MiB |\r\n|    0      2281      G   /usr/bin/plasmashell                          96MiB |\r\n|    0      3136      C   ...ilso4/anaconda3/envs/fermi21/bin/python  5363MiB |\r\n|    0      4764      G   ...pycharm-community-2019.3.1/jbr/bin/java     2MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n**Describe the current behavior**\r\nCan't compute derivatives of output wrt intermediate variables created inside a tf.function wrapped function. \r\n\r\nIf tf.function wrapper is removed the function behaves as expected.\r\n\r\n**Describe the expected behavior**\r\nCan compute these derivatives. \r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\ny = tf.Variable(2.)\r\nz = tf.Variable(3.)\r\nx = tf.random.normal((1,))\r\n\r\n@tf.function # remove this to function\r\ndef call(x, y, z):\r\n    a = x * y\r\n    b = a * z\r\n    return a, b\r\n\r\nwith tf.GradientTape(True) as g:\r\n    g.watch(x)\r\n    a, b = call(x, y, z)\r\n\r\ngx, ga =  g.gradient(b, [x, a])\r\nprint(ga)\r\nprint(gx)\r\n```\r\n\r\n**Other info / logs**\r\n2.2.0-dev20200123\r\nNone\r\ntf.Tensor([6.], shape=(1,), dtype=float32)\r\n\r\n", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/5702221d4122ecf0ff681e032cb61657/36255.ipynb). Thanks!", "Any word on how to deal with this? Seems like the intermediate vars are untracked is there an easy workaround?", "Technically, you need to watch `b` if you intend to take gradients with respect to it. As it is now, the code only works without tf.function by accident. A different equation that doesn't entirely depend on `x` would give you bad results.\r\n\r\nA workaround is to move the gradient tape inside the function so that you can watch `b`:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ny = tf.Variable(2.)\r\nz = tf.Variable(3.)\r\nx = tf.random.normal((1,))\r\n\r\n@tf.function\r\ndef call(x, y, z):\r\n    with tf.GradientTape(True) as g:\r\n        g.watch(x)\r\n        a = x * y\r\n        g.watch(a)\r\n        b = a * z\r\n    gx, ga = g.gradient(b, [x, a])\r\n    return gx, ga\r\n\r\ngx, ga = call(x, y, z)\r\nprint(ga)\r\nprint(gx)\r\n```\r\n\r\nThe tape should probably raise an error instead of silently returning None.\r\n@allenlavoie ", "@xmax1,\r\nSorry for the delayed response. The **`derivative`** could be calculated now properly, instead of returning **`None`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/ec665c9f3d24f086dcaaf500a9c2511b/gh_36255.ipynb) of the working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36255\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36255\">No</a>\n"]}, {"number": 36254, "title": "Dropout problem in tensorflow 1.14", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 on two different PCs\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.12 on one system and 1.14 on another system\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9.0 (for tf1.12) and 10.0 (for tf1.14)\r\n- GPU model and memory: Nvidia GTX 1080 Ti (for tf1.12) and RTX 2080 Ti (for tf1.14)\r\n\r\n**Describe the current behavior**\r\nI defined a simple one-layer LSTM network with a dropout layer. The code is below and the input is a sequence of shape (105, 15), grouped into 1024 samples per batch.\r\n\r\n```\r\ndef keras_model(input_shape):\r\n    from tensorflow.keras.layers import Input, CuDNNLSTM, Dense, Dropout\r\n\r\n    inputs = Input(shape=input_shape)\r\n    x = CuDNNLSTM(64, return_sequences=False)(inputs)\r\n    x = Dropout(0.1)(x)\r\n    outputs = Dense(NC, activation='softmax')(x)\r\n\r\n    return tf.keras.Model(inputs, outputs)\r\n```\r\n\r\n It worked under tf1.12 but it is not working under tf1.14 and I get below log:\r\n\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_1 (InputLayer)         [(None, None, 15)]        0\r\n_________________________________________________________________\r\ncu_dnnlstm (CuDNNLSTM)       (None, 64)                20736\r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 64)                0\r\n_________________________________________________________________\r\ndense (Dense)                (None, 7)                 455\r\n=================================================================\r\nTotal params: 21,191\r\nTrainable params: 21,191\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n\r\nEpoch 1/1000\r\nTraceback (most recent call last):\r\n  File \"run_rnn2.py\", line 653, in <module>\r\n    doWork(a)\r\n  File \"run_rnn2.py\", line 502, in doWork\r\n    validation_steps=val_steps, verbose=verbose, callbacks=CB)\r\n  File \"D:\\shahriar\\tf1.14_py3.6.5\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 780, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"D:\\shahriar\\tf1.14_py3.6.5\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 274, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"D:\\shahriar\\tf1.14_py3.6.5\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3292, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"D:\\shahriar\\tf1.14_py3.6.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1458, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: The second input must be a scalar, but it has shape [1024]\r\n         [[{{node dropout/cond/Switch}}]]\r\n         [[metrics/acc/Identity/_89]]\r\n  (1) Invalid argument: The second input must be a scalar, but it has shape [1024]\r\n         [[{{node dropout/cond/Switch}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n\r\nWhat can be the reason?\r\nI can not upgrade my tensorflow to another version. I need to have tf1.14 work with the same code as tf1.12 under python 3.6.5.\r\n\r\n", "comments": ["@shahriar49 \r\nLooks like code is incomplete. Can you please provide complete code snippet to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@ravikyram \r\nThe whole code is lengthy and I didn't think it helps that much, and it is run on some proprietary data that I can not share. Isn't there any clue that somebody can get from the error log that I got? I expect experts to be able to spot the problem easily.", "We may not be able to provide solution to your problem without inspecting the minimal code repro.\r\nIf you can help us with a toy example which works in 1 TF version and/or fails in another we will be happy to assist.  Thanks!\r\n", "@ymodak \r\nI found the problem and realized that it was because of passing sample weights in addition to samples and targets in the training dataset. It is still a question for me why tensorflow 1.12 and 1.14 act differently in this case, but it is understandable."]}, {"number": 36253, "title": "How is the support for tensorflow 2.0 and keras in this?", "body": "Need to understand if this support tensorflow 2.0 and tfx serving directly?", "comments": ["It looks like you haven't used a template to create this issue. Please submit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. Thanks!", "@Akpadhy,\r\nAny updates regarding this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36252, "title": "TF 2.1 libtensorflow library does not build under Windows", "body": "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNone\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version:\r\nmaster or 2.1 labeled version\r\n- Python version:\r\n3.6.8\r\n- Installed using virtualenv? pip? conda?:\r\nPartially Pip as specified in manual at \r\n- Bazel version (if compiling from source):\r\nbazel 1.2.1\r\n- GCC/Compiler version (if compiling from source):\r\nMSVC 2017 or MSVC 2019\r\n- CUDA/cuDNN version:\r\nNone\r\n- GPU model and memory:\r\nNone\r\n\r\nlibtensorflow does not build. Tried 2.1 version and current (27 jan 2020) master version\r\n\r\nUsed\r\nbazel build --define=no_tensorflow_py_deps=true --config opt //tensorflow/tools/lib_package:libtensorflow\r\n\r\nUsed either set BAZEL_VS= {{ path to VS2019 }} or {{ path to VS2017 }}\r\n\r\nError is always:\r\n\r\nERROR: /tensorflow/tensorflow/core/framework/BUILD:575:1: C++ compilation of rule '//tensorflow/core/framework:bfloat16' failed (Exit 2)\r\n\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/Tensor/TensorBlock.h(1028): error C2061: syntax error: identifier 'Kind'\r\n\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/Tensor/TensorBlock.h(1134): note: see reference to class template instantiation 'Eigen::internal::StridedLinearBufferCopy<Scalar,IndexType>' being compiled\r\n\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/Tensor/TensorBlock.h(1037): error C2061: syntax error: identifier 'Kind'\r\n\r\nIt seems related to the Eigen library header file(s) missing in includes ? \r\n\r\n", "comments": ["See also related issue https://github.com/tensorflow/tensorflow/issues/36064", "@goldiegadde @mihaimaruseac did we see a similar issue in 2.1 branch?\r\nOn master, my builds seem to be healthy.", "I don't remember having this issue at the time.", "Dear all, the issue persists; I am using \r\n[https://github.com/sitting-duck/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows](url) as the steps for creating a build on windows, since there is no good documentation. If you have a better description of steps needed; please provide them.", "Any update?", "@pkanwar23 ", "Any update?", "I am sorry, I have been knee deep in another task lately, and have not been able to respond.\r\nI will try to look into this tomorrow.", "[Latest reasons](https://github.com/tensorflow/tensorflow/issues/39905#issuecomment-641588284) why nothing happens for last 6 months for Windows DLL for TF2.0, TF2.1 and TF2.2", "We need to invite more discussion on the future of WINDOW support for tensorflow.  6 months with no progress but challenges. **Google needs to hire those WHO CLIAM HERE are able to address these challenges IMMEDIATELY.** ", "The latest commit to address this problem is to get more Windows developers to spend MORE TIME trying [new Bazel  (from 2.0 to 3.0)](https://github.com/tensorflow/tensorflow/commit/b4b83222d470afbf0b83d12b0824c0f056235655)\r\n", "@josdewitte \r\nCan you please try on the latest stable version 2.4.1 and let us know if this is still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36252\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36252\">No</a>\n"]}]