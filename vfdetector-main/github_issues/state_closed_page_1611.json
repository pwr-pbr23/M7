[{"number": 4581, "title": "How can user control the communication in distributed tensorflow", "body": "sorry to trouble.\nWhen I try to train model with ps and worker. I found that TensorFlow passed the whole gradients of each op between ps and each worker. \nIf I explicitly calculate the gradient of inputs, which is defined as placeholder, TensorFlow passed these gradients. While the batch size of the inputs is too big, it makes the time of communication too long.\nIs there any way to choose the parameters which user want to pass between ps and worker.\n", "comments": ["Thanks for the question @zhougr1993!\n\nWe primarily use github issues to track bugs and installation problems.  This is a question better suited for StackOverflow.  Please ask it there and tag it with the `tensorflow` tag.  Thanks!\n"]}, {"number": 4580, "title": "Support older versions of git", "body": "The Parameter -C is unknown in git 1.7.1 but --exec-path= can be used.\n", "comments": ["@Phhere, thanks for your PR! By analyzing the annotation information on this pull request, we identified @meteorcloudy and @aselle to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Andrew, mind taking a look?\n", "Thank you for the patch.\n\nYou should squash your two commits into one using `git rebase -i` since the first commit is incorrect (it confused me at first.\n\nAlso, there is an internal committed change that will be sync'd soon that adds a \n`tensorflow/tools/git/gen_git_source.sh`. It would be great if you could make the change to that (which is used for the Makefile case to avoid the python dependency).\n", "Ok, I will rebase it and wait for the internal change\n", "I think the new version is ok\n", "Jenkins, test this please.\n", "@aselle , are we good to go?\n", "(I don't think that the gen_git_source.sh in its present form requires modifications)\n"]}, {"number": 4579, "title": "A Convolutional Neural Network Model Could not be loaded", "body": "I have saved the model using Keras' model.save('my_model.h5') it's working. However, when I try to load the model in a different project I get this error message: ValueError: Tensor(\"cond/pred_id:0\", dtype=bool) must be from the same graph as Tensor(\"dropout_1/mul_1:0\", shape=(?, 1, 256), dtype=float32).\nCould this be a bug? Any idea?\n", "comments": ["Looks like you should file it against Keras.  Feel free to reopen if you have a minimum repro program that shows this is a TF bug.\n"]}, {"number": 4578, "title": "Default weights_initializer for tf.contrib.layers.convolution2d should be xavier_initializer_conv2d", "body": "Hi, \n\nI notice that in [tf.contrib.layers.convolution2d](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L354), [tf.contrib.layers.convolution2d_in_plane](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L469), [tf.contrib.layers.convolution2d_transpose](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L573), [tf.contrib.layers.separable_convolution2d](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L1097), the default weights_initializer is tf.contrib.layers.xavier_initializer, rather than tf.contrib.layers.xavier_initializer_conv2d. Is it better to use tf.contrib.layers.xavier_initializer_conv2d?\n\nThanks!\n", "comments": ["xavier_initializer_conv2d = xavier_initializer\n", "Got it. Thanks!\n"]}, {"number": 4577, "title": "Unable to run example trainer using GPU", "body": "Hi experts,\n\nI am building tensorflow from source and using virtualenv. I followed all the steps in \nhttps://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#installing-from-sources\nbut when i try to run the example trainer, i get a Floating point exception core dump error.\nPlease let me know if you need further information and how I can get it for you.\n\nMuch appreciated!\n- Macchakaran\n\n(tensorflow) macchakaran@macchakaran-pc:~/Work/tensorflow$ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5.1.5 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: GeForce GTX 1070\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.683\npciBusID 0000:01:00.0\nTotal memory: 7.92GiB\nFree memory: 7.47GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\nFloating point exception (core dumped)\n", "comments": ["Please fill out the question template when you click on \"New issue\"  --- this will help us a lot in figuring out what the issue is.\n", "Automatically closing due to lack of recent activity. Please reopen when additional information becomes available. Thanks!\n"]}, {"number": 4576, "title": "python/ops/variable_scope.py, variable_scope() is not backward compatible", "body": "https://github.com/tensorflow/tensorflow/commit/3d1ee95e612b1987e664ca46a7c584872d36dde9#diff-57dece591b2185b1c01f712d53baef96L1089\n\nIt used to accept \"\" as name_of_scope, newer version TF will produce an error.\n\n```\nif default_name is None and not name_or_scope:\n      raise TypeError(\"If default_name is None then name_or_scope is required\")     \n```\n\nBy reading the newer code, I believe disallowing \"\" as scope name wasn't your team's intension.\nA simple fix could make this function completely backward compatible.\n", "comments": ["Indeed, it looks like it was meant to be \"... and not name_or_scope is None\", right? Would you be willing to send a PR correcting this?\n", "I think it should be \"... and ~~not~~ name_or_scope is None\".\nPR is up:\nhttps://github.com/tensorflow/tensorflow/pull/4619\n"]}, {"number": 4575, "title": "Temporary workaround for iOS build problem", "body": "The download_dependencies.sh script fails, because the pattern it's searching for in workspace.bzl has changed but the workspace file itself hasn't been updated yet. This is a minimal fix to allow users to continue building for iOS until we get the syncing issue sorted out.\n", "comments": ["@petewarden, thanks for your PR! By analyzing the annotation information on this pull request, we identified @jart, @martinwicke and @tensorflower-gardener to be potential reviewers\n", "By the way, are you aware that the Eigen version you specified in the download file is not the same as the one in the [workspace.bzl](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl) file? But it is the same version we're using internally. :\\\n", "test this please\n", "http://ci.tensorflow.org/job/tensorflow-master-makefile/\nand this PR fixed the master makefile build.\n"]}, {"number": 4574, "title": "translate.py in the seq2seq model is missing in sitepackages for python3.5 cpu only release", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nsearched github issues for \"translate.py\" no hits\n### Environment info\n\nOperating System:\nUbuntu 16.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nNA, CPU only\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.10.0\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nNA\n### What other attempted solutions have you tried?\n\nNA\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["Can you let us know which exact file this refers to?\n", "tensorflow/tensorflow/models/rnn/translate/translate.py\n", "Please assign to someone with more build expertise, that's not me.\n", "@lukaszkaiser - oh right, I blindly assigned based on the prefix `tf/models/rnn` :)  Re-assigning to @gunan.\n", "It is possible we omit models on purpose.\n@martinwicke Could you confirm?\n", "Yes it would seem like you omitted it from the pip release which would perhaps make sense because it's supposed to be run as a \"main\" ie its not part of a library. And I mean its no real problem for me to just get the file from \"master\" but the problem is that the translate.py file that I get from master may not be compatible with the release version that I installed via pip.\n\nDo you see what I mean? So I think that files like tensorflow/tensorflow/models/rnn/translate/translate.py should be accessible in the pip-installed version of tensorflow to make sure your pip installation of tensorflow is on the same version as the provided models.\n", "You can always get the files corresponding to the binary releases by\nchecking out the corresponding branch in the repo. Eg for the 0.10 release,\ncheck out the r0.10 branch.\nOn Thu, Sep 29, 2016 at 09:38 patrickbreen notifications@github.com wrote:\n\n> Yes it would seem like you omitted it from the pip release which would\n> perhaps make sense because it's supposed to be run as a \"main\" ie its not\n> part of a library. And I mean its no real problem for me to just get the\n> file from \"master\" but the problem is that the translate.py file that I get\n> from master may not be compatible with the release version that I installed\n> via pip.\n> \n> Do you see what I mean? So I think that files like\n> tensorflow/tensorflow/models/rnn/translate/translate.py should be\n> accessible in the pip-installed version of tensorflow to make sure your pip\n> installation of tensorflow is on the same version as the provided models.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4574#issuecomment-250467804,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_QRncSifFABzf8ARe7n2BpMwvy0-ks5qu79hgaJpZM4KF-v8\n> .\n", "So marking this as working as intended, and using the same files from the release branch should give you compatible files.\nClosing this issue.\n"]}, {"number": 4573, "title": "ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support.", "body": "My CUDA version is 8.0 and cuDNN is 5.1. I have run many cuda examples without any problem.\n\nBut I stuck in following problem for two days.\n\nFirst I run the configure as below\n`devymex@DL-LAB:~/Software/tensorflow$ ./configure \n~/Software/tensorflow ~/Software/tensorflow\nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] N\nNo Hadoop File System support will be enabled for TensorFlow\nFound possible Python library paths:\n  /usr/local/lib/python2.7/dist-packages\n  /usr/lib/python2.7/dist-packages\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\n\n/usr/local/lib/python2.7/dist-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: \nWARNING: Running Bazel server needs to be killed, because the startup options are different.\nSending SIGTERM to previous Bazel server (pid=5498)... done.\n.\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.\nINFO: All external dependencies fetched successfully.\nConfiguration finished\n`\n\nThen build tensorflow by bazel:\n\n`bazel build -c opt --config=cuda\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\nSending SIGTERM to previous Bazel server (pid=20618)... done.\n.\nERROR: /home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):\n    File \"/home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/BUILD\", line 4\n        error_gpu_disabled()\n    File \"/home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/error_gpu_disabled.bzl\", line 3, in error_gpu_disabled\n        fail(\"ERROR: Building with --config=c...\")\nERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\nUnhandled exception thrown during build; message: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@e06068b3' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@de03db55', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@2228d81e', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@2342a6cb')\nINFO: Elapsed time: 0.539s\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@e06068b3' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@de03db55', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@2228d81e', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@2342a6cb')\n    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)\n    at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalStateException: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/BUILD\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:179)\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.findCrosstoolConfiguration(CrosstoolConfigurationLoader.java:239)\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.readCrosstool(CrosstoolConfigurationLoader.java:281)\n    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.createParameters(CppConfigurationLoader.java:128)\n    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:73)\n    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:48)\n    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction.compute(ConfigurationFragmentFunction.java:78)\n    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1016)\n    ... 4 more\nCaused by: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/BUILD\n    at com.google.devtools.build.lib.packages.Package.makeNoSuchTargetException(Package.java:559)\n    at com.google.devtools.build.lib.packages.Package.getTarget(Package.java:543)\n    at com.google.devtools.build.lib.skyframe.SkyframePackageLoaderWithValueEnvironment.getTarget(SkyframePackageLoaderWithValueEnvironment.java:71)\n    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction$ConfigurationBuilderEnvironment.getTarget(ConfigurationFragmentFunction.java:193)\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:177)\n    ... 11 more\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@e06068b3' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@de03db55', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@2228d81e', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@2342a6cb')\n    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)\n    at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalStateException: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/BUILD\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:179)\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.findCrosstoolConfiguration(CrosstoolConfigurationLoader.java:239)\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.readCrosstool(CrosstoolConfigurationLoader.java:281)\n    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.createParameters(CppConfigurationLoader.java:128)\n    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:73)\n    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:48)\n    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction.compute(ConfigurationFragmentFunction.java:78)\n    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1016)\n    ... 4 more\nCaused by: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/devymex/.cache/bazel/_bazel_devymex/05af4cc48fb50d1cc8f7e879f4c1ce83/external/local_config_cuda/crosstool/BUILD\n    at com.google.devtools.build.lib.packages.Package.makeNoSuchTargetException(Package.java:559)\n    at com.google.devtools.build.lib.packages.Package.getTarget(Package.java:543)\n    at com.google.devtools.build.lib.skyframe.SkyframePackageLoaderWithValueEnvironment.getTarget(SkyframePackageLoaderWithValueEnvironment.java:71)\n    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction$ConfigurationBuilderEnvironment.getTarget(ConfigurationFragmentFunction.java:193)\n    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:177)\n    ... 11 more\n`\n\nPlease help me to get rid of the annoying problem, thanks!\n", "comments": ["This is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorflow` tag.\n"]}, {"number": 4572, "title": "upgrade protobuf to 3.1.0", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@kashif, thanks for your PR! By analyzing the annotation information on this pull request, we identified @mrry, @jart and @keveman to be potential reviewers\n", "For issues #4561 and #4371\n", "Mr. Jenkins: test this please\n", "@jart can you kindly try to run jenkins again?\n", "@jhseu done\n", "Jenkins, test this please\n", "Can you merge or rebase with the latest master? Tests are failing due to being on an old change.\n", "Jenkins, test this please\n"]}, {"number": 4571, "title": "Tensorflow website stating CUDNN v5 required, but my version working only on v5.1", "body": "https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#requirements\nstates that cuDNN v5 is required. So I tried to compile the basic example. This error was shown.\n_Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5103 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration._\n\nIt worked when I upgraded cuDNN to v5.1. Please update the said webpage if possible. \n\nOperating System: Ubuntu Linux 14.04 LTS\n\nInstalled version of CUDA and cuDNN: \n-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13 Sep 24 20:03 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       17 Sep 24 20:03 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.3\n-rwxr-xr-x 1 root root 59909104 Sep 24 19:10 /usr/local/cuda/lib64/libcudnn.so.5.0.5\n-rwxr-xr-x 1 root root 60696704 Jun 10 13:48 /usr/local/cuda/lib64/libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 59715990 Jun 10 13:48 /usr/local/cuda/lib64/libcudnn_static.a\n\n**Tensorflow version:**\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.10.0\n", "comments": ["@zheng-xq Could you take a look at this? Thanks.\n", "We should update the website with this information.\r\nAll our builds have been using cudnn 5.1 for a while now.", "Documentation is now updated."]}, {"number": 4570, "title": "[tutorials] Fixes code style", "body": "", "comments": ["@chanis, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @ilblackdragon and @tensorflower-gardener to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Mr. Jenkins test this please\n"]}, {"number": 4569, "title": "fix Gfile method size() to Size()", "body": "Gfile object has method Size(), not size()\n\nSigned-off-by: yaoning yaoning@unitedstack.com\n", "comments": ["@mslovy, thanks for your PR! By analyzing the annotation information on this pull request, we identified @rohan100jain, @vrv and @keveman to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Are you sure?\n\nGfile now just wraps FileIO, which defines it as size(): https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/file_io.py#L83\n", "It was Size() in branch r0.10.  If you want to send a PR for that branch, please do, after signing the CLA.  However, we since changed it to size() in r0.11 / master, so I'm going to close this.\n\nhttps://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/python/platform/gfile.py#L179 \n"]}, {"number": 4568, "title": "Download dependencies to install tensorflow from source on Mac", "body": "### Environment info\n\nOperating System: OS X 10.11.6\n\nInstalled version of CUDA and cuDNN: \nNo\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`) :\n   6218ac2be3cc530da866ec32da4cb86c6ac5bb85\n2. The output of `bazel version`\n   Build label: 0.3.1-homebrew\n   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n   Build time: Thu Aug 4 09:58:27 2016 (1470304707)\n   Build timestamp: 1470304707\n   Build timestamp as int: 1470304707\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\ntensorflow/contrib/makefile/download_dependencies.sh\n### What other attempted solutions have you tried?\n\nI thought it was some problem that I havent install the tar in my Mac, but it didnt help\n### Logs or other output that would be helpful\n\nWhen i run the command line like above, I got some error which indicate it failed to download eigen.\n\ndownloading http://bitbucket.org/eigen/eigen/get/\" + eigen_version + \".tar.gz\ntar: Unrecognized archive format\ntar: Error exit delayed from previous errors.\n\nCan you help me?  I am new to tensorflow\n", "comments": ["Seems to be the same as: https://github.com/tensorflow/tensorflow/issues/4567\n\nAlso, anbai106 -- is there any reason you need to use the makefile in particular, or could you build the canonical way (using Bazel)? https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#prepare-environment-for-mac-os-x \n", "Hi, thanks for your answer, actually, I just follow the tutorial for ios application:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile\nand also, I install tensorflow with Docker container, So if I want to continue to the IOS application implement, what should I do, just follow the canonical way with bazel or with the source installation, or the docker version???\nThank you in advance:)\nHao\n"]}, {"number": 4567, "title": "tensorflow/contrib/makefile/download_dependencies.sh fails to parse workspace.bzl", "body": "Hi, \n\nIn download_dependencies.sh we grep tensorflow/workspace.bzl in order to determine which Eigen to download. We do this using the command `grep -o 'http.*bitbucket.org/eigen/eigen/.*tar\\.gz' tensorflow/workspace.bzl` which we then proceed to try and curl and untar (ln22).  \n\nDue to the new format of workspace.bzl, that grep returns `http://bitbucket.org/eigen/eigen/get/\" + eigen_version + \".tar.gz` which will download a 404 from bitbucket. Can we please update the makefile to account for this?\n\nThanks,\nTomas\n", "comments": ["I see the same error on both Mac OS 10.11.16 and Raspbian. Clean git checkout of Tensorflow in both cases.\n\n$ uname -a\nDarwin XXXX.local 15.6.0 Darwin Kernel Version 15.6.0: Mon Aug 29 20:21:34 PDT 2016; root:xnu-3248.60.11~1/RELEASE_X86_64 x86_64\n\n$ tensorflow/contrib/makefile/download_dependencies.sh\ndownloading http://bitbucket.org/eigen/eigen/get/\" + eigen_version + \".tar.gz\ntar: Unrecognized archive format\ntar: Error exit delayed from previous errors.\nRipley:tensorflow andrew\n\n$ uname -a\nLinux raspberrypi 4.1.19+ #858 Tue Mar 15 15:52:03 GMT 2016 armv6l GNU/Linux\n\n$ tensorflow/contrib/makefile/download_dependencies.sh\ndownloading http://bitbucket.org/eigen/eigen/get/\" + eigen_version + \".tar.gz\n\ngzip: stdin: not in gzip format\ntar: Child returned status 1\ntar: Error is not recoverable: exiting now\n\nEdited:\nFor those who just want to get it to work, change the EIGEN_URL line in download_dependencies.sh to:\n\nEIGEN_URL=\"http://bitbucket.org/eigen/eigen/get/46ee714e25d5.tar.gz\"\n", "It's fixed in master, commit 1d9ca7bbea8f41d38f5dc57fd3fe4ee15c4aab04.\n"]}, {"number": 4566, "title": "Allow user to control amount of GPU memory consumed", "body": "Environment:\n~tensorflow 0.10 \n~NVIDIA K80 GPU server. \n~ubuntu 14.04\n\nI do not change the code which is download from this repository.\nWhen I run ptb_word_lm.py, it takes up all my GPU memory (from device gpu:0 to device gpu:15). Is it a bug? How could I fix it?\n\nThanks a lot in advance!\nSwind\n", "comments": ["Do you have 16 GPUs in your machine? Once a GPU is used by TF, it will grab all its memory.\n", "Actually I have 8 K80 physical GPUs which can be used as 16 GPUs.  GPU:0 is used by TF, but it will grab all GPUs' memory.(from gpu:0 to gpu:15) \n", "check out http://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory and CUDA_VISIBLE_DEVICES\n", "Thank you for your response, CUDA_VISIBLE_DEVICES works.\nBut I think it is still a question (or a bug in a way) about TF 0.10, I hope it will be solved in the following versions of TF. For example, I hope that the code\nwith tf.device('/gpu:13'):\ncan control the usage of gpu memory. It seems that this code just controls the usage of gpu calculation now. When I use this code, ptb_word_lm.py just use gpu:13 for calculation but takes up all my GPU memory from gpu:0 to gpu:15.\n\nThanks again\n", "@zheng-xg, I've made this into a feature request, retitled and assigned to you based on the previous comment by  @DC-Swind.\n", "@zheng-xq \n", "What's wrong with using `CUDA_VISIBLE_DEVICES`? There may be performance implications if you wait to initialize GPU memory pool until device is declared\n", "To control which GPU you want to use, use CUDA_VISIBLE_DEVICES\n\nTo control how much GPU memory you want to use: per_process_gpu_memory_fraction\n\nhttps://github.com/tensorflow/tensorflow/blob/d24e64227653ab3cab7405c79ca166063a42f0ff/tensorflow/python/kernel_tests/sparse_xent_op_test.py#L262\n", "We should probably add this to the documentation... maybe here? https://www.tensorflow.org/versions/r0.11/how_tos/using_gpu/index.html\nOr is it already in it, and I just can't find it.\n", "Can you require a specific amount of bytes instead of a fraction? I don't know what GPU will be used, and would like to reserve 100 MB when starting a session.", "I'll second Carlthome's request here. Working with multiple machines with (very) differing amount of GPU memory is hard, since it seems I can only set it to a fraction of the available memory, causing it to fail on our GPU monster that also run other tasks.\r\n\r\nAlternatively, is there a way to grab the total GPU memory? CUDA calls get blocked by StreamExecutor, and I cannot seem to find a way to achieve this using the latter. "]}, {"number": 4565, "title": "Add camera permission for iOS 10", "body": "To avoid shutting the  application down on iOS 10,\nadding `NSCameraUsageDescription` to `Info.plist` is necessary.\n\nhttps://developer.apple.com/library/content/documentation/General/Reference/InfoPlistKeyReference/Articles/CocoaKeys.html#//apple_ref/doc/uid/TP40009251-SW24\n\n> Important: To protect user privacy, an iOS app linked on or after\n> iOS 10.0, and which accesses the device\u2019s camera, must statically\n> declare the intent to do so. Include the NSCameraUsageDescription key\n> in your app\u2019s Info.plist file and provide a purpose string for this\n> key. If your app attempts to access the device\u2019s camera without a\n> corresponding purpose string, your app exits.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your contribution :)\n\nMr. Jenkins: test this please\n"]}, {"number": 4564, "title": "Fix header path of iOS examples for eigen", "body": "I failed to build iOS example projects.\nIt seems download destination path for eigen library is changed\nfrom `tensorflow/contrib/makefile/downloads/eigen-latest`\nto `tensorflow/contrib/makefile/downloads/eigen`.\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/download_dependencies.sh#L49\n\nFixing `Header Search Paths` solves the problem.\n", "comments": ["@tyfkda, thanks for your PR! By analyzing the annotation information on this pull request, we identified @petewarden to be a potential reviewer\n", "Can one of the admins verify this patch?\n", "I think these changes make sense to me, Jenkins test this please.\n"]}, {"number": 4563, "title": "Fix to download eigen library", "body": "URL for eigen library is extracted from `tensorflow/workspace.bzl`,\nbut expression is used to construct the URL in the code,\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L22\nso extracted URL using simple `grep` is not correct.\n\nThis change get version code for eigen library from the file,\nand construct URL with it.\n\nIt looks cmake handles the URL in same way:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/external/eigen.cmake#L13\n", "comments": ["@tyfkda, thanks for your PR! By analyzing the annotation information on this pull request, we identified @jart, @martinwicke and @tensorflower-gardener to be potential reviewers\n", "Can one of the admins verify this patch?\n", "I find the change which ignores `eigen_version`\nis done in the commit: 65038b084059cf934df50fa86dba5b0e765f9d65\n", "I have updated the commit\nsince conflict occurred with #4575 .\n", "Thank you for your contribution. Ideally I would like to update this by having it pull the URL out of the workspace.bzl file the same way it gets pulled out of other files.\n", "@jart \nI'm not sure what is your expected way,\n\n1.\nUnlike URL for other libraries in download_dependencies.sh (`GEMMLOWP_URL`, `GOOGLETEST_URL`, `PROTOBUF_URL`, `RE2_URL`), single `grep` cannot extract `EIGEN_URL`, because the URL is declared as\n\n``` py\n    url = \"http://bitbucket.org/eigen/eigen/get/\" + eigen_version + \".tar.gz\",\n```\n\nin [workspace.bzl](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L22).\n\n2.\nUnlike CMake, [string(REGEX MATCH ...)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/external/eigen.cmake#L13) cannot use in Makefile.\n\n3.\nDo you mean it is preferable to modify the declaration for eigen library in same way in workspace.bzl, not to use variable\n\n``` py\n  native.new_http_archive(\n    name = \"eigen_archive\",\n    url = \"http://bitbucket.org/eigen/eigen/get/46ee714e25d5.tar.gz\",\n    sha256 = \"d2ba02303c20d6ddc1a922f7e0e176ef841514545e053388845359aa62176912\",\n    strip_prefix = \"eigen-eigen-46ee714e25d5\",\n    build_file = str(Label(\"//:eigen.BUILD\")),\n  )\n```\n\nto enable extraction for eigen URL with single `grep`?\n", "Jenkins, test this please\n"]}, {"number": 4562, "title": "Cherrypicks for r0.11rc release", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @mrry, @ageron and @lilac to be potential reviewers\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "Cherry-pick, ignoring CLA.\n", "@googlebot I authored the commit, and am okay with it being contributed.\n", "Jenkins, test this please\n", "Can you also pull in https://github.com/tensorflow/tensorflow/commit/1d9ca7bbea8f41d38f5dc57fd3fe4ee15c4aab04 and https://github.com/tensorflow/tensorflow/commit/ff2bea2684ccce59547eb9acd02462cc0d2ba1e1? They're needed to fix the makefile build.\n", "Done, now waiting for all tests.\n", "tensorflow/python/kernel_tests:io_ops_test is still flaky.\nJonathan, did we have a fix for that?\n", "Yeah, https://github.com/tensorflow/tensorflow/commit/a112b988d0976493adab04b9cb2af771882a4904\n", "All changes are cherrypicked from master.\ntherefore ignoring CLA, assuming CLA is all taken care of in master branch.\n"]}, {"number": 4561, "title": "protobuf needs to be updated", "body": "@kamal94 noticed in https://github.com/tensorflow/tensorflow/issues/4371#issuecomment-249340251 that the version of protobuf we're referencing contains HOST_CFG in its protobuf.bzl file. We need to upgrade that.\n", "comments": ["Is this fixed by #4572 ?", "Probably."]}, {"number": 4560, "title": "[issue4339]Add adjust_gamma for image", "body": "Add \"Gamma Correction\" Image Adjustment [issue4339](https://github.com/tensorflow/tensorflow/issues/4339) according to **scikit-image** [adjust_gamma](http://scikit-image.org/docs/dev/api/skimage.exposure.html#skimage.exposure.adjust_gamma)\n\nChange list:\n- Add `adjust_gamma` in `image_ops.py`\n- Add corresponding tests in `image_ops_test.py`\n  - test_adjust_gamma_one\n  - test_adjust_gamma_zero\n  - test_adjust_gamma_less_one\n  - test_adjust_gamma_greater_one\n- Add dtype range and limits for numpy dtype\n\nCompare to scikit-image:\n- [adjust_gamma](https://github.com/scikit-image/scikit-image/blob/master/skimage/exposure/exposure.py#L309)\n- [Test Gamma Correction](https://github.com/scikit-image/scikit-image/blob/master/skimage/exposure/tests/test_exposure.py#L308)\n- [dtype range](https://github.com/scikit-image/scikit-image/blob/master/skimage/util/dtype.py#L8)\n- [dtype limits](https://github.com/scikit-image/scikit-image/blob/master/skimage/util/dtype.py#L32)\n\nAn explanation of [Gamma Correction](https://en.wikipedia.org/wiki/Gamma_correction) in Wikipedia\n", "comments": ["Can one of the admins verify this patch?\n", "@DjangoPeng, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @girving and @vrv to be potential reviewers\n", "@girving @vrv Please check!\nBTW, would we support the pow of `uint8, uint16` and `tf.int32 / tf.float32` in the future? ?\n", "Vijay, please review or reassign if you know someone more appropriate.\n", "@vrv  could u review the PR, or reassign others @jhseu \n", "Hmm, the other good reviewers for this change are out-of-office right now and shlens will only have time to review probably next week. I'm hesitant to do the review because of my lack of context on image ops. Thanks for your patience.\n", "Can one of the admins verify this patch?\n", "A mistake of deleting branch. Restore it now\n", "@jhseu @vrv @girving Can one of you review the PR?\n", "@shlens friendly ping :)\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "@shlens Do you know how to get the error log of `Linux GPU FAILURE`? I don't know why it fail, but I remember it succeeded last time. :(\n", "Jenkins, test this please.\n", "@shlens Does changes approved means that the PR can be merged now? \n", "Jenkins, test this please.\n"]}, {"number": 4559, "title": "Update version string to 0.11.rc0", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @omtcyfz and @keveman to be potential reviewers\n", "Just to be sure: you checked the warning output of the update_version.sh, right?\n", "Yes, and manually updated all the files here.\nThe following files were in the warning:\ntensorflow/tools/dist_test/Dockerfile.local:39:0.10.0\ntensorflow/contrib/cmake/setup.py:29:0.10.0\ntensorflow/g3doc/get_started/os_setup.md:399:0.10.0\n"]}, {"number": 4558, "title": "Fix cmake/external to use new include paths.", "body": "Fixes the breakage from #4555.\n", "comments": ["@mrry, thanks for your PR! By analyzing the annotation information on this pull request, we identified @ageron and @lilac to be potential reviewers\n"]}, {"number": 4557, "title": "using GraphDef in C API?", "body": "I'm looking over the C API to turn a GraphDef into a TF_Graph for use with TF_NewSessionWithGraph, but I can't figure out how that's supposed to be done.\n\nIt kind of seems like you can't unless you use the deprecated TF_ExtendGraph API? But that's deprecated, so I'm not sure what to do.\n", "comments": ["https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h#L754 :)\n\nIt's currently being worked on though :)\n", "The `TF_ImportGraphDef()` implementation is part of pending PR #4555, so should be available very soon: https://github.com/tensorflow/tensorflow/pull/4555/commits/22221698a3ecd43024f84cd6c468ddd00955f920 (commit)\n", "Ah! I missed that! Thanks for the info.\n"]}, {"number": 4556, "title": "Make division_past_test medium", "body": "It's flaky with 60s timeout.\n", "comments": ["@martinwicke, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @gunan and @itsmeolivia to be potential reviewers\n", "Eh. Merging. The only test worth waiting for on this one is the sanity.\n"]}, {"number": 4555, "title": "Branch 134070499", "body": "", "comments": ["cmake failure:\n", "[ 23%] No install step for 'grpc'\n[ 23%] Completed 'grpc'\n[ 23%] Built target grpc\n", "I'm tempted to say ignore the CMake failure and I'll fix it. Sound good?\n", "Hard to resist.\n", "I have a fix in the works... the Bazel simplification also moved some of the header paths for our external dependencies, so we need a corresponding change in the CMake build.\n"]}, {"number": 4554, "title": "TensorFlow Binary Segmentation", "body": "I've written the following [segmentor](https://docs.google.com/document/d/1Mnfrcu5AU47rRoPwweQv8kRGzTPJaXwJf7zd3pksXK4/edit?usp=sharing) and I can't get the accuracy to work. In fact I'm always getting accuracy of 0.0 whatever the size of my sample.\n\nI think the problem is at the sigmoid layer at the end of U() function where a tensor of continuous elements between 0 and 1 (conv10) is further compared to a binary tensor and therefore there's no chance of getting any equality between the two.\n", "comments": ["Since this issue is not a bug or feature request, please ask this type of question on stackoverflow with the tag #tensorflow. Thank you.\n"]}, {"number": 4553, "title": "I got a problem when I ran the flower demo of slim", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System:\nUbuntu 14.04\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nCuda 7.5   cuDNN 5.0\nIf installed from binary pip package, provide:\nNo, we installed tensorflow with the source code\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n   754048a0453a04a761e112ae5d99c149eb9910dd\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nWe ran the flower demo alone, everything is ok, however, with the slim I got the error like this:\n![image](https://cloud.githubusercontent.com/assets/10766005/18788663/5d1d63ce-81da-11e6-91fa-0a34ae52e130.png)\n I think something wrong with this file /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/image_ops.py\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\nthink you!\n", "comments": ["Please include more details and this is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorflow` tag.\n"]}, {"number": 4552, "title": "Feature request: better handling of concurrent execution to prevent unnecessary out of memory failures ", "body": "I have a model in which one operation happens to be very memory intensive and cannot be executed at once. I thought that by splitting the operation manually and executing parts of it individually (all on the GPU), I would get around the OOM problem. Unfortunately, it appears that TF aggressively tries to schedule as many ops to run in parallel as possible, even if that results in an OOM failure. I describe my specific issue further in this [Stack Overflow post](http://stackoverflow.com/questions/39643250/best-way-to-split-computation-too-large-to-fit-into-memory).\n\nIt seems to me that this problem should be reasonably easy to solve, and is rather a serious current limitation of TF. The basic principle is that a computation should not fail if it's possible, in some sequential order, to execute it in memory. I'm not talking about situations in which TF has to be clever about splitting a seemingly atomic operations in parts. That I understand can be quite difficult and requires \"art\" on the part of the programmer. What I'm describing is a situation in which the computation graph, _as described_, can in fact be executed without running out of memory, but because TF is (overly) aggressive in scheduling independent ops concurrently, it runs out of memory.\n\nThe solution seems rather straight-forward. Before putting yet another op onto the GPU (or whatever hardware), TF should check if that would cause the memory to run out. If so, it should check if there's a cyclic dependency on the current op getting executed, i.e. it would be impossible for it to move forward unless the current op is executed. If so then that's a legitimate problem and it would fail with an OOM error. On the other hand, if it's possible for it to wait until other ops are executed before trying again, then it should simply wait until more memory is available. I.e. it should have a way to run independent operations sequentially.\n\nI'm not familiar enough with the internals of TF to know how difficult a change like this would be, but as it currently stands, it seems like it's preventing a large swath of models from getting executed that otherwise could get executed.\n\nOn a related note, the dynamic_rnn op already seems to do this, where it's able to shuffle memory back and forth until the computation is done, and so perhaps there's an existing partial solution that I'm not aware of?\n", "comments": ["Quick update. I discovered that using `map_fn` and setting `parallel_iterations` to a small number addresses my current needs. Nonetheless, I think it would be very helpful if there was a general mechanism by which TF prevented (unnecessary) failures due to running out of memory.\n", "@yuanbyu was working on related stuff. The issue is that independent ops schedule to execute concurrently even if that exceeds device memory budget\n", "@yuanbyu Could you take a look at this? Thanks.\n", "#2126 is a catch all for improvements to TensorFlow placement. I'm closing this issue and redirecting this query there. Thanks.", "Memory efficient execution is an open research area, and it's good to have a concrete application in mind to have a practical solution. For instance I found that \"out of memory\" was often caused by bad execution order, rather than executing too many of them concurrently.\r\n\r\nIE consider computational graph below\r\n![caterpillar](https://cloud.githubusercontent.com/assets/23068/22432653/7dc7bf20-e6ca-11e6-879c-14880a4d26d3.png)\r\n\r\nTF schedules things as soon as they are ready to execute, so if \"square\" op takes long to compute, it'll compute all the \"circles\", and then keep them in memory, and then compute the squares. This needs O(n) memory where `n` is the length of chain, whereas memory-optimal execution need O(1) memory and will go \"circle->square->circle->square\".\r\n\r\nThe general problem is known in computer science as \"one-shot black pebbling\" first formalized in http://graal.ens-lyon.fr/~lmarchal/scheduling/sethi_complete_register_allocation.pdf\r\n\r\nIt is provably [hard](https://arxiv.org/abs/1109.4910) to compute, but things can look better for concrete models.\r\n\r\nAnother set of memory-constrained applications can be enabled by discarding some intermediate values and recomputing them later. That's corresponds to dropping \"one-shot\" in the \"one-shot pebbling game\", and in deep learning this comes up in approaches like training neural networks where gradients don't fit in [memory](https://arxiv.org/abs/1604.06174)\r\n\r\nRegarding your proposal, it sounds like you are proposing to add memory swapping to more ops in TensorFlow?\r\n\r\nThis is what the implementation does with it right now:\r\nhttps://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/core/kernels/stack_ops.cc#L215\r\n\r\nThere's a trade off between transfer and recomputation. For instance, I found that on Titan X Pascal it's 7-10x faster to recompute a large cheap op like `mul` or `concat` on GPU rather than fetch previous result from main memory.\r\n\r\nTo summarize, you can reduce memory by picking a better execution order, by recomputing parts of the graph on demand, or by swapping things out to CPU memory. Can you be more specific with the model you are trying to run to make sure that swapping is indeed the correct thing here?", "I think we may be talking about two distinct levels of abstractions. I understand that \"solving\" the time-optimal or space-optimal memory execution order problem may be difficult. What I'm asking for is something that would prevent TF from executing an op if that would result in an OOM error. Perhaps I'm making an undue assumption: I'm assuming that TF can estimate the memory requirement of a single op. If that's the case, then, given the current state of available memory, it should be able to determine whether executing another op will lead to an OOM or not. If it would, then my desired behavior would be that it just waits. I.e. it sets up a schedule that enforces sequentiality if need be (with some timeout heuristics, etc) to prevent OOM errors.\r\n\r\nOr are you saying that it already does that, but finds itself in an unrecoverable state where computing the next necessary op is simply impossible? I'm somewhat doubtful that's the case, at least in my current graph.", "@alquraishi it wasn't easy to determine whether executing an op would OOM in the current framework, since you're already in the execution of the OpKernel when you find out you run out of memory, and the framework doesn't really handle re-executing an OpKernel well (it may be possible, though).\r\n\r\nI think there are two possible approaches to solve this (none of which I'm too involved in at the moment, but something I've thought about):\r\n\r\n* Once we get C++ shape inference in the runtime (right now it just backs the python shape inference framework), you would be able to know at runtime the approximate output memory requirements of every operation, and so you could indeed know ahead of time whether an operation could run.  This wouldn't include temporary memories required by libraries like cudnn), but it would probably be the low hanging fruit optimization.\r\n\r\n* XLA's compiled approach needs to know the sizes of tensors at compile time, providing the information to the memory and instruction scheduler for XLA to possibly do the right thing.  I'm more optimistic about this approach working well, since the standard executor is very dynamic, and XLA might have a better time of accomplishing this goal. @tatatodd  might have more to say on this.\r\n\r\n* It would be also nice to have the recompuatation/memory use tradeoff in the above solutions.", "@alquraishi estimating memory needed is hard, but simple heuristic might be to use an upper bound instead of true memory requirement. IE, suppose the largest op output is 1GB. Then you assume that once memory hits 11GB on TitanX, any new op executed will cause OOM and output needs to be placed on CPU.\r\n\r\nHence, once you hit 11GB, all GPU kernels place their output in main RAM until GPU RAM is reclaimed. This will avoid OOM crash at the expense of introducing a bunch of GPU->CPU and CPU->GPU memory copies needed to run GPU kernels.\r\n\r\nThis is a \"run-time\" approach mirroring what dynamic_rnn does. A competing approach is static graph analysis which would determine which ops are likely to hit this scenario, and pin those ops to CPU device. This means CPU kernel implementation will be used, and extra GPU<->CPU memory copies are avoided.\r\n\r\nThe later could be done purely on client side, by doing a profiling run, examining `RunMetadata`, and then creating new graph with `tf.device` annotations to place things on CPU or GPU", "Yes these would all be reasonable solutions. The problem I'm having right now is that I'm unable to run the model _at all_, which is a big step down from having to do some extra GPU<->CPU copying. I've tried to manually enforce sequential computations using `control_dependencies`, and it's alleviated the problem somewhat, but there are still many models I can't run that I would be able to otherwise.", "Closing since I think the issue as-is is too open ended.  Would be great to improve the situation, but I don't think leaving the issue open will encourage the necessary research."]}]