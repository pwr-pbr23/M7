[{"number": 4129, "title": "Remove unused cache files from Docker image", "body": "Closes #4116\n", "comments": ["Based on discussion at #4116 it sounds like this is working as intended. I'm going to close this PR so we have just one place to talk about whether we want to make this change (#4116). If we decide on that thread that we want to remove the cache, we can reopen this PR.\n"]}, {"number": 4128, "title": "Docs for Inception Model", "body": "TensorFlow 0.10.0rc0 CPU on Linux.\n\nThis is about the Inception model and example that is included with TensorFlow:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/imagenet/classify_image.py\n\nI am using this version of the Inception model:\n\nhttp://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\n\nI'm not sure whether to ask these questions here or on StackOverflow, but there are some related questions there that haven't been answered, and this is also sort of a request for improving the TensorFlow docs, so I hope it's OK that I ask the questions here.\n\nMy questions are:\n\n(1) The softmax classifier apparently outputs an array of length 1008 - but there is only 1000 classes in the data-set. Why is there a difference? How should I adjust for this difference?\n\n(2) What is the size of the input JPEG-images supposed to be, and is the Inception model rescaling them automatically?\n\n(3) I get a deprecation warning. I don't know what is causing this. Does it mean that this Inception model will stop working in the future?\n\n> /home/foo/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py:1811: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n>  result_shape.insert(dim, 1)\n> W tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\n\n(4) The sample code uses `tf.gfile.Exists()` which just wraps `os.path.exists()` so it seems to be completely redundant. Why is it used?\n\n(5) The sample code also uses `tf.gfile.FastGFile()` which seems to be the non-thread-safe version of TensorFlow's file-class. Why not use Python's built-in `open()` instead?\n\n(6) The function `run_inference_on_image()` creates a new TensorFlow session for each image that is being processed. Is the TensorFlow session so lightweight that this is the preferred way of doing it? Or is it better to create a single session and use it repeatedly?\n\nThanks.\n", "comments": ["(1) I don't know. \n(2) See classify_image.py, the images are automatically resized to 299 x 299, the size expected.\n(3) Yes, the model will need to be upgraded at some point or it will fail.\n(4&5) Portability & future-proofness\n(6) This is done to make the example simple.  If you really want to do inference on a lot of images, it is better to create a single session and use it repeatedly.\n", "Thanks for the quick answer.\n\n(1) Just to clarify the question a bit. After having loaded the graph in my own code, if I print the softmax-tensor then the shape is (1, 1008):\n\n```\nprint(graph.get_tensor_by_name('softmax:0'))\n```\n\n> Tensor(\"softmax:0\", shape=(1, 1008), dtype=float32)\n\nBut in the file `imagenet_2012_challenge_label_map_proto.pbtxt` the class-numbers range from 1 to 1000 (both inclusive). So there's apparently only 1000 possible classes in the data-set, but the softmax has 1008 output-classes. It seems strange to me and it must mean that we should ignore 8 of the classes from the softmax-ouput - but which ones?\n\n(2) I can't seem to find the image resizing in `classify_image.py` so I wonder if the resizing is done implicitly inside the TensorFlow graph that is being restored? It's also not clear what happens to the images; are they resized or cropped? using what algorithm? what if they're too small, are they then just padded?\n\n(3) Would it be possible to always have an Inception model compatible with the latest TensorFlow version and stored under e.g.:\n\nhttp://download.tensorflow.org/models/image/imagenet/inception-latest.tgz\n\nThen I won't need to make any changes to my code when it is updated, as it will just be downloaded automatically. Or what would be the best way of doing this?\n\nThanks again.\n", "(1) It turns out that the 1008 size softmax output is an artifact of dimension back-compatibility with a older, Google-internal system.  Newer versions of the inception model have 1001 output classes, where one is an \"other\" class used in training.  You shouldn't need to pay any attention to the extra 8 outputs.\n\n(2) Oops.  The public version of  [classify_image.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/imagenet/classify_image.py) doesn't include an explicit image resize, nor the dimensions.  I guess someone thought it would make a better tutorial to hide that stuff inside the graph.  If you look at [label_image/main.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc) used in the C++ API example, you can see the explicit jpg decoding and resizing.  If you dump the graph protobuf in text format (huge!) you can see nodes at the top of the graph doing the same thing.\n\n(3)  Acknowledged, but this might be a community support issue.  We're pretty busy right now.\n", "See https://research.googleblog.com/2016/08/improving-inception-and-image.html for the latest imagenet model.\n", "Thanks again for the quick answer!\n\n## (1)\n\nI suppose that when the Inception model outputs a class-number of zero it really means \"other\" and class-numbers between 1001-1007 are for back-compatibility. But I think the code breaks if the top-5 includes one of these special classes, because there is no mapping from these special class-numbers to ImageNet uid's in the file `imagenet_2012_challenge_label_map_proto.pbtxt` I think a comment in the code would be good.\n\n## (2)\n\nActually it is much more confusing to omit the information on image rescaling; the file `classify_image.py` doesn't even mention that images are rescaled to 299x299 pixels. I looked in the C++ file you linked to and it appears to be using `ResizeBilinear()`, which I can't really find in the source-code, but I guess it's bilinear interpolation. But it's still not clear whether the images are squeezed, cropped or padded if they're not exactly square. I tried doing the resizing manually in an image editor and using these images as input to the Inception model. With this image the Inception model does pretty well in most variants, but you can easily imagine that if e.g. a very wide or very high image is squeezed then the Inception model will no longer perform as well. Similarly with cropping where e.g. an essential part of the image is removed. Perhaps the best solution would be to pad the image so it's square and then resize?\n\n### Original image\n\n![parrot](https://cloud.githubusercontent.com/assets/13588114/18245844/e5765950-7367-11e6-83cc-2f9b378125b4.jpg)\n\n### Squeezed image, Inception result: Macaw 97.0%\n\n![parrot_squeezed](https://cloud.githubusercontent.com/assets/13588114/18245414/5f6c6252-7365-11e6-9cf5-aa20565379bf.jpg)\n\n### Cropped image version 1, Inception result: Macaw 97.4%\n\n![parrot_cropped1](https://cloud.githubusercontent.com/assets/13588114/18245415/5f7460e2-7365-11e6-9f33-35e8143ba7e2.jpg)\n\n### Cropped image version 2, Inception result: Macaw 93.9%\n\n![parrot_cropped2](https://cloud.githubusercontent.com/assets/13588114/18245416/5f76646e-7365-11e6-9104-019fa5c7ca74.jpg)\n\n### Cropped image version 3, Inception result: Jacamar (another bird) 26.1%, grasshopper 10.6%\n\n![parrot_cropped3](https://cloud.githubusercontent.com/assets/13588114/18245418/5f799472-7365-11e6-8f26-39a0d07511d8.jpg)\n\n### Padded image, Inception result: Macaw 96.8%\n\n![parrot_padded](https://cloud.githubusercontent.com/assets/13588114/18245417/5f7821fa-7365-11e6-8245-35833b23851d.jpg)\n\n## (3 + more)\n\nThanks for the tip on the new Inception models. However, they're not reloaded in the same way as the Inception tutorial. It appears that you have to import the .py files, create a graph using those functions, and then reload the checkpoints in the tar-files. The Inception tutorial uses a frozen graph instead.\n\nI think it would be great if you had a standardized way of creating neural networks from a model-zoo. This ought to be a part of the TensorFlow API. The API could look something like this:\n\n```\nmodel3 = tf.models.Inception3(auto_download=True)\n\nmodel4 = tf.models.Inception4(auto_download=True)\n```\n\nThese functions would automatically download a graph and checkpoint-file for each Inception model that was working with the user's TensorFlow version (or perhaps just the latest TF version), and then return a model-object which inherits from `class ModelZoo` and provides the following functions:\n\n```\nmodel3.get_graph()  # Returns the graph for the neural network.\nmodel3.get_placeholder_name()  # Name of placeholder-variable for inputting image.\nmodel3.get_softmax()  # Return tensor for softmax-classifier.\nmodel3.get_last_layer()  # Return tensor for last layer for re-training.\nmodel3.get_class_names(class_numbers)  # Return names of the given classes.\nmodel3.inference(session=None, images)  # Perform inference on the list of images.\n                                        # Create new session if None is supplied.\nmodel3.print_scores(predictions)  # Print scores and class-names for predicted classes.\n```\n\nThis would allow users to do inference with only a few lines of code:\n\n```\nmodel = tf.models.Inception3(auto_download=True)\npredicted_labels = model.inference(images=my_images)\n```\n\nPerhaps this is what you want to do with the Slim library? I've looked at the following Notebook which is mostly well-documented (yay, finally some TensorFlow code with lots of comments! The author is apparently @nathansilberman):\n\nhttps://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb\n\nHowever, it is still quite complicated and I think a model zoo can be made even simpler with an API such as the above.\n", "@sherrym do you have some feedback on the suggestions from @Hvass-Labs, or suggestions on who might be appropriate?  I'm triaging issues this week, and this is beyond my area of expertise.  Thanks!\n", "I have finished my new tutorial on how to use the pre-trained Inception Model and I have two suggestions to the TensorFlow developers:\n\n(A) The Inception v3 model seems to have problems recognizing people. I give several examples of this in the tutorial (link below). This would seem to make the pre-trained Inception model useless in practice. I'm not sure, but I think the problem is maybe in the training-set you're using. Please consider training the Inception model on a data-set that is more relevant. Instead of having 1000 classes, maybe only have 100 classes that it could recognize more accurately and that were more relevant?\n\n(B) As mentioned above, I think the TensorFlow API for using pre-trained \"Zoo\" models could be much simpler. The current methods for loading pre-trained models are rather convoluted. If users could just do something like the following to load a good pre-trained model, then you might see a multitude of new interesting Android apps using image recognition:\n\n```\nimport tensorflow as tf\nmodel = tf.models.Inception(auto_download=True)\npredicted_labels, predicted_scores = model.classify(image=\"foo.jpg\")\n```\n\nHere's my new tutorial on the Inception Model as a Notebook and YouTube talk:\n\nhttps://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/07_Inception_Model.ipynb\n\nhttps://www.youtube.com/watch?v=ZG_hoLgNFNo\n", "(A) The training set for inception is from the ImageNet challenge http://image-net.org/download-faq. It's not an arbitrary set of training images.\n\n(B) Would you like to submit a pull request? Thanks.\n", "Thanks for the quick reply.\n\n(A) As I understand, ImageNet is a data-set used in academic competitions that has been around for several years now. I understand completely that it's a huge achievement to get so good scores on ImageNet. But fact is that the Inception v3 model you include with TensorFlow gives bizarre results when using it on some real-world images that it ought to be able to recognize. I assume the Inception models released last week give equally bizarre results because they were apparently trained on the same ImageNet data-set.\n\nIn my own tutorial that I linked to above, the Inception v3 model classified an image of Elon Musk (299x299 pixels) as maybe being a sweatshirt (20% score) or an abaya (17% score). If I resized the image to 100x100 pixels then Inception said it was maybe a sweatshirt (18%) or a cowboy-boot (16%). Inception said that Gene Wilder portraying Willy Wonka was a bow-tie (97%), and that Johnny Depp portraying Willy Wonka was maybe sunglasses (31%) or sunglass (19%), that is, two very similar class-names.\n\nI don't know what the problem is. Is it because the ImageNet-classes really belong to a hierarchy, so that sweatshirt is really a sub-class of person, and the same with bow-tie and sunglasses? But that's not something that can be deduced from the data-files with class-names that you have included with the Inception model.\n\nI would have invited the main author of the Inception model Christian Szegedy to comment on this, but I can't find his GitHub-handle.\n\n(B) I would love to make a pull-request for this - it would be an honour to make such an important contribution to TensorFlow which is quickly becoming the leading AI library! But I'm a beginner in all this and I still only understand a fraction of TensorFlow, so I think it's better for me to just issue a 'feature-request' instead :-) So please don't get annoyed by this, in the end my questions and suggestions might help improve TensorFlow.\n\nAt the moment, I try to learn the key points about TensorFlow and make tutorials aimed at beginners in both Deep Learning and TensorFlow - there's apparently a lot of us that found the Udacity course rather confusing.\n", "I found out that the problem with the strange labels predicted by the Inception model is indeed the classes and their names in the ImageNet data-set. This can be seen from e.g. the \"bow-tie\" label which causes the Inception model to classify people wearing bow-ties as \"bow-tie\" rather than e.g. \"man wearing bow-tie\"\n\nhttp://image-net.org/synset?wnid=n02883205#\n\nOne solution would be to use the neural networks for generating captions, e.g. \"Man wearing tall hat, bow-tie and purple jacket.\" But I haven't gotten around to caption-generation yet and I suspect it's quite a bit more complicated to implement.\n\nI would still argue that ImageNet's classes are not completely meaningful, and perhaps the team that made the Inception model could make a much better data-set from the vast amounts of data that google has available, so the Inception model would work much better out-of-the-box, without the need for re-training or more complicated caption-generation techniques.\n\nAs mentioned above, the Inception model thinks that a picture of Elon Musk shows either a sweat-shirt (20% score) or an abaya (17% score). If you change the resolution of the image, then Inception thinks it shows either a sweat-shirt or a cowboy-boot. If you ask people on the street what they think of the state-of-the-art in AI and image recognition developed by the world's leading scientists and engineers at google, and it gives results like this, well ... people are going to be amused :-)\n\nJust a few thoughts.\n", "The ImageNet dataset is a benchmark dataset that's been widely and successfully used for research. Inception is a model that does fairly well on this dataset and we published the code so others can build on it and improve it. As always in Machine Learning, Inception has its limitations when it comes to generalizing beyond the type of data it was trained on.\n\nWe never implied that Inception, trained on imagenet, is suited for any particular purpose (for example, recognizing images outside the categories trained on), or that its design is suitable for production environments with concerns such as oblong images, or requirements on the specific description strings.\n\nWe have released the code to train Inception in order to make it possible for people who need a network with such capabilities to train their own, and to modify the network or its preprocessing pipeline to meet their needs. This is not something that a complete beginner in machine learning should attempt, although it has been done. Users generally find the pretrained and retrainable version rather useful since it offers a way to train a custom model much faster. \n\nI will close this issue -- if there a concrete things that can be improved in the documentation, or the model itself, PRs are welcome.\n", "Thanks again for the comments, I appreciate it. This thread may answer questions that others might have in the future so I think it has been a useful discussion.\n", "I agree. It is clearly important to set the expectations for a research\nmodel such as this one.\nOn Tue, Sep 13, 2016 at 02:26 Hvass-Labs notifications@github.com wrote:\n\n> Thanks again for the comments, I appreciate it. This thread may answer\n> questions that others might have in the future so I think it has been a\n> useful discussion.\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> \n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4128#issuecomment-246625364,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_ZggnyOl4Y-OukITzZ7GxNlGvv9Oks5qpmxKgaJpZM4JxvOM\n> .\n", "@Hvass-Labs if u look at the 1000 classes used in the ImageNet dataset you will see that their are no tags related to the words - 'human', 'man', 'person'. So it is not surprising that it can't tell you that the image contains a human."]}, {"number": 4127, "title": "Tensorflow build from source fails with C++ compilation of rule '//tensorflow/core:framework_internal' failed: crosstool_wrapper_driver_is_not_gcc failed:", "body": "I am trying to build tensorflow from source (without external git access), can someone please let me know how to get this going, I have built most of the dependencies in a local system directory but end up with the following:\n\nERROR: /usr/local/tensorflow/tensorflow/core/BUILD:927:1: C++ compilation of rule '//tensorflow/core:framework_internal' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\n(cd /root/.cache/bazel/bazel_root/e86f6fce5559de9e3e13fb6adb66b858/execroot/tensorflow && \\\nexec env - \\\nPATH=/usr/local/tensorflow/protobuf/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/bin \\\nthird_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIC -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' '-frandom-seed=bazel-out/host/bin/tensorflow/core/objs/framework_internal/tensorflow/core/example/feature_util.o' -iquote . -iquote bazel-out/host/genfiles -iquote external/protobuf -iquote bazel-out/host/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/re2 -iquote bazel-out/host/genfiles/external/re2 -isystem external/protobuf/src -isystem bazel-out/host/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/highwayhash -isystem bazel-out/host/genfiles/external/highwayhash -isystem third_party/gpus/cuda/include -isystem bazel-out/host/genfiles/third_party/gpus/cuda/include -fno-exceptions -DEIGEN_AVOID_STL_ARRAY '-DGOOGLE_CUDA=1' -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -MD -MF bazel-out/host/bin/tensorflow/core/_objs/framework_internal/tensorflow/core/example/feature_util.d -c tensorflow/core/example/feature_util.cc -o bazel-out/host/bin/tensorflow/core/_objs/framework_internal/tensorflow/core/example/feature_util.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\ntensorflow/core/example/feature_util.cc: In function 'const typename tensorflow::internal::RepeatedFieldTrait::Type& tensorflow::GetFeatureValues(const string&, const tensorflow::Example&) [with FeatureType = long long int; typename tensorflow::internal::RepeatedFieldTrait::Type = google::protobuf::RepeatedField; std::string = std::basic_string]':\ntensorflow/core/example/feature_util.cc:54:67: error: invalid initialization of reference of type 'const google::protobuf::RepeatedField&' from expression of type 'const google::protobuf::RepeatedField'\nreturn example.features().feature().at(name).int64_list().value();\ntensorflow/core/example/feature_util.cc: In function 'typename tensorflow::internal::RepeatedFieldTrait::Type\\* tensorflow::GetFeatureValues(const string&, tensorflow::Example) [with FeatureType = long long int; typename tensorflow::internal::RepeatedFieldTrait::Type = google::protobuf::RepeatedField; std::string = std::basic_string]':\ntensorflow/core/example/feature_util.cc:62:23: error: cannot convert 'google::protobuf::RepeatedField' to 'google::protobuf::RepeatedField' in return\n->mutable_value();\n^\ntensorflow/core/example/feature_util.cc: In function 'const typename tensorflow::internal::RepeatedFieldTrait::Type& tensorflow::GetFeatureValues(const string&, const tensorflow::Example&) [with FeatureType = long long int; typename tensorflow::internal::RepeatedFieldTrait::Typ\ne = google::protobuf::RepeatedField; std::string = std::basic_string]':\ntensorflow/core/example/feature_util.cc:55:1: warning: control reaches end of non-void function [-Wreturn-type]\n}\n^\ntensorflow/core/example/feature_util.cc: In function 'typename tensorflow::internal::RepeatedFieldTrait::Type tensorflow::GetFeatureValues\n(const string&, tensorflow::Example*) [with FeatureType = long long int; typename tensorflow::internal::RepeatedFieldTrait::Type = google::\nprotobuf::RepeatedField; std::string = std::basic_string]':\ntensorflow/core/example/feature_util.cc:63:1: warning: control reaches end of non-void function [-Wreturn-type]\n}\n^\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n\nAny pointers is greatly appreciated, not sure what I am missing here.\n\nbash-4.2# cd /usr/local/\nbazel-0.3.0 jpeg-9a libpng-1.2.53 tensorflow\neigen-eigen-9e1b48c333aa giflib-5.1.4\n\ngmock-1.7.0 six-1.10.0 zlib-1.2.8\nfarmhash-34c13ddfab0e35422f4c3979f360635a8c050260\n\n/usr/local/tensorflow\nbash-4.2# ls\nACKNOWLEDGMENTS bazel-genfiles boost/ bzip2.BUILD gif.BUILD jpeg.BUILD navbar.md re2/ third_party zlib.BUILD\nAUTHORS bazel-out boost.BUILD configure grpc.BUILD jsoncpp.BUILD png.BUILD README.md tools\navro.BUILD bazel-tensorflow boringssl.BUILD CONTRIBUTING.md highwayhash/ LICENSE protobuf/ RELEASE.md util\nbazel-bin bazel-testlogs bower.BUILD farmhash.BUILD ISSUE_TEMPLATE.md nanopb.BUILD tensorflow WORKSPACE\n\nworkspace.bzl and CROSSTOOL have all the necessary entries for the include libraries and packages.\n", "comments": ["The best thing to do if you want to build without access to the internet would be to download the files mentioned in the WORKSPACE and tensorflow.bzl (all things grabbed by `new_http_archive` etc.), unpack them, and replace `new_http_archive` (and similar) with `new_local_repository` rules.\n\nYou shouldn't have to build these dependencies, bazel will do that for you. See the [bazel docs](https://www.bazel.io/versions/master/docs/be/workspace.html#new_local_repository) for more info.\n\nYour particular error looks like it got confused by several distinct versions of protobuf visible to the compiler.\n", "Still trying on this one.\nThere are few dependencies mentioned in tensorflow.bzl and WORKSPACE that are already installed for other applications that is the reason for using those custom paths.\n"]}, {"number": 4126, "title": "Device logging from C API not appearing", "body": "Hello,\nI'm using the C API and creating a session with the `log_device_placement` config parameter set to `true`, but am not actually seeing any logging output on STDOUT. Is there something special I need to be doing to capture and display the TensorFlow log from C?\n", "comments": ["Could you give a simple example I might be able to reproduce?\n", "@malmaud, in addition to a simple example to help us reproduce the issue, you might try adding --alsologtostderr on the command-line of your binary.\n", "Closing this out due to lack of activity.  Feel free to post on this issue if it's still a problem.  Thanks!\n"]}, {"number": 4125, "title": " do tensorflow support lmdb or hdf5 data?", "body": "Now I have lmdb and hdf5 data for image classification. And I want to use tensorflow to train some convnets. So do tensorflow support lmdb or hdf5 data?\n", "comments": ["@JulyUp tensorflow/examples/skflow/hdf5_classification.py \n", "@wangg12  Thank you very much. It is useful.\n"]}, {"number": 4124, "title": "\"ValueError: No gradients provided for any variable\" when using bidirectional_dynamic_rnn", "body": "I got the following error when using `tf.nn.bidirectional_dynamic_rnn`. \n\n```\nTraceback (most recent call last):\n  File \"main_cnn.py\", line 63, in <module>\n    main()\n  File \"main_cnn.py\", line 50, in main\n    model.run(num_epoch, learning_rate=learning_rate)\n  File \"/home/s1510032/research/programs/textsum-cnn/model.py\", line 266, in run\n    self.optim = self.opt.apply_gradients(zip(grads, params), global_step=self.global_step)\n  File \"/home/s1510032/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 298, in apply_gradients\n    (grads_and_vars,))\nValueError: No gradients provided for any variable: ((None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c570d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1dfdd58e10>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1dfdd63690>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1dfdd7c5d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c0f1d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c22890>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c35850>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103da890>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103dac50>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103dad10>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103e13d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f1038cf10>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103aabd0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e119ecf90>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e1197f150>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1d9f2c3d50>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1d9f2c3dd0>))\n\n```\n\nIf I keep everything and use `tf.nn.bidirectional_rnn` instead, this problem goes away. Here is the working code and non-working code. Loss is calculated based on outputs and that part is the same for both cases.\n\nWorking code:\n\n```\noutputs, fw_states, _ = tf.nn.bidirectional_rnn(self.lstm_fw_cell,\n                         self.lstm_bw_cell,\n                         self.sent_cnn_outputs,\n                         dtype=tf.float32)\n\n # calculate loss using outputs\n```\n\nNon-working code:\n\n```\noutputs, output_states = tf.nn.bidirectional_dynamic_rnn(self.lstm_fw_cell, \n                    self.lstm_bw_cell,\n                    self.sent_cnn_outputs,\n                    time_major=True,\n                    sequence_length=self.sequence_length,\n                    dtype=tf.float32)\n\noutputs = tf.concat(2, outputs)\noutputs = tf.unpack(outputs, 0)\n\n# calculate loss using outputs\n```\n\nI suspect the issue here could because of `tf.unpack` function, which I use to make `outputs` become a list (for using in `zip` function in python with other list. Do you have any suggestion to resolve this?\n", "comments": ["As I thought, this issue is because of the function `tf.unpack`. I had to hack my code a little bit like this instead of using `tf.unpack`\n\n`outputs = tf.split(0, max_timestep, outputs)`\n\nand when iterating through `outputs`, I just have to remember to reshape it to remove the first dimension (which is 1)\n\n```\n\nfor idx, top_h in enumerate(outputs):\n    top_h = tf.reshape(top_h, [batch_size, -1])\n```\n\nI do not know why `tf.unpack` does not work while this works, and I think there should be a better solution than this. Anyway, I am happy that it works at the moment for my code.\n", "Have you tried using `tf.unpack(outputs, num=max_timestep, axis=0)` ?\n\ni think the 0 in your unpack is to the wrong argument.  always pass the keywords for keyword arguments.\n\nplease let us know if this solves your problem.\n", "I'm closing this issue out due to inactivity. If new information is provided, I will reopen this issue.\n"]}, {"number": 4123, "title": "Branch 131799027", "body": "", "comments": []}, {"number": 4122, "title": "rnn/translate: IOError: [Errno 2] No such file or directory: '/mnt/tf1/translate/giga-fren.release2.fr.gz'", "body": "### Environment info\n\nOperating System:\n\n```\n$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 14.04.4 LTS\nRelease:    14.04\nCodename:   trusty\n```\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n`NONE`\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   `$ python -c \"import tensorflow; print(tensorflow.__version__)\"\n   0.10.0rc0`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n```\n$ cd tensorflow/tensorflow/models/rnn/translate/\n$ python translate.py --data_dir /mnt/tf1/translate\nPreparing WMT data in /mnt/tf1/translate\nDownloading http://www.statmt.org/wmt10/training-giga-fren.tar to /mnt/tf1/translate/training-giga-fren.tar\nSuccesfully downloaded training-giga-fren.tar 2595102720 bytes\nExtracting tar file /mnt/tf1/translate/training-giga-fren.tar\nUnpacking /mnt/tf1/translate/giga-fren.release2.fr.gz to /mnt/tf1/translate/giga-fren.release2.fr\nTraceback (most recent call last):\n  File \"translate.py\", line 290, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"translate.py\", line 287, in main\n    train()\n  File \"translate.py\", line 147, in train\n    FLAGS.data_dir, FLAGS.en_vocab_size, FLAGS.fr_vocab_size)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/translate/data_utils.py\", line 265, in prepare_wmt_data\n    train_path = get_wmt_enfr_train_set(data_dir)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/translate/data_utils.py\", line 83, in get_wmt_enfr_train_set\n    gunzip_file(train_path + \".fr.gz\", train_path + \".fr\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/translate/data_utils.py\", line 68, in gunzip_file\n    with gzip.open(gz_path, \"rb\") as gz_file:\n  File \"/usr/lib/python2.7/gzip.py\", line 34, in open\n    return GzipFile(filename, mode, compresslevel)\n  File \"/usr/lib/python2.7/gzip.py\", line 94, in __init__\n    fileobj = self.myfileobj = __builtin__.open(filename, mode or 'rb')\nIOError: [Errno 2] No such file or directory: '/mnt/tf1/translate/giga-fren.release2.fr.gz'\nubuntu@ip-10-169-182-86:~/tensorflow/tensorflow/tensorflow/models/rnn/translate$ ls /mnt/tf1/translate/training-giga-fren.tar \n```\n\n```\n$ ls -l /mnt/tf1/translate/training-giga-fren.tar \n-rw-rw-r-- 1 ubuntu ubuntu 2595102720 Aug 31 10:53 /mnt/tf1/translate/training-giga-fren.tar\n```\n### What other attempted solutions have you tried?\n\n```\n$ python translate.py --data_dir /mnt/tf1/translate\nPreparing WMT data in /mnt/tf1/translate\nExtracting tar file /mnt/tf1/translate/training-giga-fren.tar\n...(same error)\n```\n\n```\n$ ls -l /mnt/tf1/translate/\ntotal 5073524\n-rw-rw-r-- 1 ubuntu ubuntu 1214224978 Aug 30 19:55 giga-fren.release2.fixed.en.gz\n-rw-rw-r-- 1 ubuntu ubuntu 1380871453 Aug 29 21:43 giga-fren.release2.fixed.fr.gz\n-rw-rw-r-- 1 ubuntu ubuntu 2595102720 Aug 31 10:53 training-giga-fren.tar\n```\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["Same happens for `giga-fren.release2.en.gz`, so I have partially solved it by `symlink`\n\n``` shell\nubuntu@ip-10-169-182-86:/mnt/tf1/translate$ ls -l\ntotal 9536168\nlrwxrwxrwx 1 ubuntu ubuntu         30 Aug 31 15:17 giga-fren.release2.en.gz -> giga-fren.release2.fixed.en.gz\n-rw-rw-r-- 1 ubuntu ubuntu 1214224978 Aug 30 19:55 giga-fren.release2.fixed.en.gz\n-rw-rw-r-- 1 ubuntu ubuntu 1380871453 Aug 29 21:43 giga-fren.release2.fixed.fr.gz\n-rw-rw-r-- 1 ubuntu ubuntu 4565271815 Aug 31 15:15 giga-fren.release2.fr\nlrwxrwxrwx 1 ubuntu ubuntu         30 Aug 31 15:13 giga-fren.release2.fr.gz -> giga-fren.release2.fixed.fr.gz\n-rw-rw-r-- 1 ubuntu ubuntu 2595102720 Aug 31 10:53 training-giga-fren.tar\n```\n", "In tensorflow/models/rnn/translate/data_utils.py  line 76\n-  train_path = os.path.join(directory, \"giga-fren.release2\")\n-  train_path = os.path.join(directory, \"giga-fren.release2.fixed\")\n", "Ideal solution is that someone hosts the file that won't change it.  Not sure what else is different besides file name.\n", "I don't think this is tensorflow problem.\n\nthe files inside training-giga-fren.tar have been renamed:\nAfter extracting you get this error:\n`IOError: [Errno 2] No such file or directory: ...`\n\nls into the folder of the data downloaded, in the default example it's `/tmp/` and what you will see is this:\n\n```\ngiga-fren.release2.fixed.en.gz  \ngiga-fren.release2.fixed.fr.gz  \ntraining-giga-fren.tar \n...\n```\n\nNext, try doing this:\n\n`mv /tmp/giga-fren.release2.fixed.fr.gz /tmp/giga-fren.release2.fr.gz`\n\n`mv /tmp/giga-fren.release2.fixed.en.gz /tmp/giga-fren.release2.en.gz`\n\nWhich will recorrect name of the files into (giga-fren.release2.en or giga-fren.release2.fr) rather than the one with fixed.\n\nre turn the python translate and bam it works now \ud83d\ude04 \n\nPS: if pull-requests are allowed, then I can fix this in the main file.\n", "pull requests are welcome!\n", "In the tensorflow-0.10.0rc0 release, changing line 76 by adding '.fixed' in data_utils.py doesnt work. Instead creating the soft link for giga-fren.release2.fixed.en.gz  and giga-fren.release2.fixed.fr.gz  in the data_dir works as of today.\n", "Hello everyone,\r\nI executed the code translate.py from sequence to sequence models tutorial of tensorflow 60 hours ago and still waiting for the output.\r\nCan anyone tell me how much more time will it take to display the output\r\nSystem specifications:\r\nI installed Ubuntu 16.04 on VMware with a disk space of 100\r\n![seqop](https://cloud.githubusercontent.com/assets/20130992/22991763/68078e36-f38b-11e6-84ed-f288cfa00087.PNG)\r\n GB and RAM of 1 GB.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 4121, "title": "Lack of 'name' argument in the tf.contrib.learn.Classifier.evaluate method", "body": "Tensorflow version 0.10.0rc0 (Installed today by pip from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl)\n\nI'm working with examples from [here](https://www.tensorflow.org/versions/r0.10/tutorials/tflearn/index.html) and [here](https://www.tensorflow.org/versions/r0.10/tutorials/monitors/index.html):\n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n# Data sets\nIRIS_TRAINING = \"iris_training.csv\"\nIRIS_TEST = \"iris_test.csv\"\n\n# Load datasets.\ntraining_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TRAINING,\n                                                       target_dtype=np.int)\ntest_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TEST,\n                                                   target_dtype=np.int)\n\n# Specify that all features have real-value data\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n\n# Build 3 layer DNN with 10, 20, 10 units respectively.\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n                                            hidden_units=[10, 20, 10],\n                                            n_classes=3,\n                                            model_dir=\"/tmp/iris_model\",\n                                            config=tf.contrib.learn.RunConfig(\n                                                save_checkpoints_secs=1))\n\nvalidation_metrics = {(\"metrics/accuracy\", \"classes\"): tf.contrib.metrics.streaming_accuracy,\n                      (\"metrics/precision\", \"classes\"): tf.contrib.metrics.streaming_precision,\n                      (\"metrics/recall\", \"classes\"): tf.contrib.metrics.streaming_recall}\n\nvalidation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n    test_set.data,\n    test_set.target,\n    every_n_steps=50,\n    metrics=validation_metrics)\n\n# Fit model.\nclassifier.fit(x=training_set.data,\n               y=training_set.target,\n               steps=2000,\n               monitors=[validation_monitor])\n\n# Evaluate accuracy.\naccuracy_score = classifier.evaluate(x=test_set.data,\n                                     y=test_set.target)[\"accuracy\"]\nprint('Accuracy: {0:f}'.format(accuracy_score))\n\n# Classify two new flower samples.\nnew_samples = np.array(\n    [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)\ny = classifier.predict(new_samples)\nprint('Predictions: {}'.format(str(y)))\n```\n\nand everything is fine for now, but if I use my own model:\n\n```\ndef my_model(x, y, mode):\n    net = tf.contrib.layers.fully_connected(x, num_outputs=10)\n    net = tf.contrib.layers.fully_connected(net, num_outputs=20)\n    net = tf.contrib.layers.fully_connected(net, num_outputs=10)\n\n    logits = tf.contrib.layers.fully_connected(net, 3)\n    prediction = tf.nn.softmax(logits)\n\n    if mode == tf.contrib.learn.ModeKeys.INFER:\n        return prediction, None, None\n\n    loss = tf.contrib.losses.softmax_cross_entropy(logits, tf.one_hot(y, 3))\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n    train_op = tf.contrib.slim.learning.create_train_op(loss, optimizer)\n\n    return prediction, loss, train_op\n\n\nclassifier = tf.contrib.learn.Classifier(model_fn=my_model,\n                                         n_classes=3,\n                                         model_dir=\"/tmp/iris_model\",\n                                         config=tf.contrib.learn.RunConfig(\n                                            save_checkpoints_secs=1))\n```\n\ninstead of:\n\n```\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n                                            hidden_units=[10, 20, 10],\n                                            n_classes=3,\n                                            model_dir=\"/tmp/iris_model\",\n                                            config=tf.contrib.learn.RunConfig(\n                                                save_checkpoints_secs=1))\n```\n\nFollowing error occurs:\n\n```\nTraceback (most recent call last):\n  File \"/mnt/nfs/dnn/workspace/deep-learning/test/minimal_working_example.py\", line 70, in <module>\n    monitors=[validation_monitor])\n  File \"/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 240, in fit\n    max_steps=max_steps)\n  File \"/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 578, in _train_model\n    max_steps=max_steps)\n  File \"/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 280, in _supervised_train\n    None)\n  File \"/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/supervised_session.py\", line 270, in run\n    run_metadata=run_metadata)\n  File \"/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/recoverable_session.py\", line 54, in run\n    run_metadata=run_metadata)\n  File \"/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/coordinated_session.py\", line 70, in run\n    self._coord.join(self._coordinated_threads_to_join)\n  File \"/***/deep-learning/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 357, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/coordinated_session.py\", line 66, in run\n    return self._sess.run(*args, **kwargs)\n  File \"/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitored_session.py\", line 107, in run\n    induce_stop = monitor.step_end(monitors_step, monitor_outputs)\n  File \"/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitors.py\", line 396, in step_end\n    return self.every_n_step_end(step, output)\n  File \"/***/deep-learning/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitors.py\", line 687, in every_n_step_end\n    steps=self.eval_steps, metrics=self.metrics, name=self.name)\nTypeError: evaluate() got an unexpected keyword argument 'name'\n```\n### Solution\n\nAdd `name` argument to the Classifier.evaluate method.\n", "comments": ["I think this is fixed at head: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/classifier.py#L76\n\nSorry it hasn't made it into the last release.\n"]}, {"number": 4120, "title": "Document \"How to adding new customized file system\"", "body": "Adding customized file system is supported but not well documented:\nTF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')\ng++ -std=c++11 -shared test_file_system.cc -o test_file_system.so -fPIC -I $TF_INC\n\nAdding a section in how_tos will help.\n", "comments": ["Could you clarify the context of the problem and its solution?\n", "It is still being worked on, when it is ready we will document it.\n", "Thanks for the information @vrv\n", "Closing this due to inactivity. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 4119, "title": "Implementing gradient for user op in C++?", "body": " I'd ideally like the operation to be wholly self-contained (gradient and operation defined in same file). The official tutorial only highlights a python implementation. Does anyone know if it's possible to implement the gradient in C++, and how to go about it?\n", "comments": ["Yes, see e.g. CholeskyGrad, which was discussed in #367 \n"]}, {"number": 4118, "title": "Undefined Symbols When Compiling User Op for GPU", "body": "## Summary\n\nI can compile, but not load, a user-defined TF shared library using CUDA. The library is based off of the TF [`zero_out`](https://www.tensorflow.org/versions/master/how_tos/adding_an_op/index.html#adding-a-new-op) example. I've modified the example to support CPU and GPU devices. Upon loading the shared library I get the following error: `tensorflow.python.framework.errors.NotFoundError: zero_out.so: undefined symbol: _ZN10tensorflow7functor14ZeroOutFunctorIN5Eigen9GpuDeviceEEclERKS3_NS2_9TensorMapINS2_6TensorIKfLi1ELi1ElEELi16EEENS7_INS8_IfLi1ELi1ElEELi16EEEi`.\n\nI suspect that the problem has something to do with my particular combination of compiler and OS environment. However after trying many ideas I've hit a wall.\n## Similar Issues\n\nhttps://github.com/tensorflow/tensorflow/issues/2097: My problem looks similat to this one.\nhttps://github.com/tensorflow/tensorflow/issues/1569: There are two suggestions made in this issue:\n1. Try a `gcc-4.*` compiler\n2. Use the compiler variable `D_GLIBCXX_USE_CXX11_ABI=0`\n\nNeither of these suggestions are working for me.\n## Detailed Explanation\n\nI give details below about my environment, the source code, the steps I've taken to compile, and the error.\n### Environment\n\nOS: Ubuntu 16.04 LTS\nKernel : 4.4.0-34-generic\nCompiler: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.2) 5.4.0 20160609\nCuda: 7.5\nCudnn:  5.1.3\n\nMy problem exists whether I use a TF binary or compile my own version from source. Note that in order to successfully compile TF from source I must add the following three lines to `CROSSTOOL.tpl`:\n\n```\ncxx_flag: \"-D_MWAITXINTRIN_H_INCLUDED\"\ncxx_flag: \"-D_FORCE_INLINES\"\ncxx_builtin_include_directory: \"/usr/local/cuda-7.5/include\"\n```\n### Source Code\n\nI have three source files and a Makefile:\n#### `zero_out.h`\n\n``` c++\n#ifndef TENSORFLOW_KERNELS_ZERO_OUT_OP_H_\n#define TENSORFLOW_KERNELS_ZERO_OUT_OP_H_\n\nnamespace tensorflow {\n\nnamespace functor {\n\n// Generic helper functor for the ZeroOut Op.\ntemplate <typename Device>\nstruct ZeroOutFunctor;\n\n}  // namespace functor\n}  // namespace tensorflow\n\n#endif  // TENSORFLOW_KERNELS_ZERO_OUT_OP_H_\n```\n#### `zero_out.cc`\n\n``` c++\n#define EIGEN_USE_THREADS\n#include \"zero_out.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n\nnamespace tensorflow {\n\nREGISTER_OP(\"ZeroOut\")\n.Input(\"to_zero: float\")\n.Output(\"zeroed: float\")\n.Doc(R\"doc(\nZeros all elements of the tensor except the first.\nzeroed: A Tensor.\n  output[0] = input[0]\n  output[1:N] = 0\n)doc\");;\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace functor {\n\ntemplate <typename Device>\nstruct ZeroOutFunctor {\n  void operator()(const Device& d,\n          typename TTypes<float>::ConstFlat input,\n          typename TTypes<float>::Flat output,\n          const int N);\n};\n\ntemplate <>\nstruct ZeroOutFunctor<CPUDevice> {\n  void operator()(const CPUDevice& d,\n          typename TTypes<float>::ConstFlat input,\n          typename TTypes<float>::Flat output,\n          const int N) {\n    for (int i = 1; i < N; i++) {\n      output(i) = 0;\n    }\n\n    // Preserve the first input value if possible.\n    if (N > 0) output(0) = input(0);\n  }\n};\n} // namespace functor    \n\ntemplate <typename Device>\nclass ZeroOutOp : public OpKernel {\npublic:\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    // Grab the input tensor\n    const Tensor& input_tensor = context->input(0);\n    auto input = input_tensor.flat<float>();\n\n    // Create an output tensor\n    Tensor* output_tensor = NULL;\n    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\n                             &output_tensor));\n\n    auto output = output_tensor->template flat<float>();\n    const int N = input.size();\n    functor::ZeroOutFunctor<Device>()(context->eigen_device<Device>(),\n                      input, output, N);\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\")                 \\\n            .Device(DEVICE_CPU),        \\\n            ZeroOutOp<CPUDevice>);\n\n#if GOOGLE_CUDA\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\")                 \\\n            .Device(DEVICE_GPU),        \\\n            ZeroOutOp<GPUDevice>);\n#endif // GOOGLE_CUDA\n} // namespace tensoroflow\n\n```\n#### `zero_out_gpu.cu.cc`\n\n``` c++\n#if GOOGLE_CUDA\n\n#define EIGEN_USE_GPU\n\n#include \"zero_out.h\"\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n\n\nnamespace tensorflow {\n\nnamespace functor {\n\nusing GPUDevice = Eigen::GpuDevice;\n\n__global__ void ZeroOutKernel(const float* in, float* out, const int N) {\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N;\n       i += blockDim.x * gridDim.x) {\n    if (i == 0) {\n      out[i] = in[i];\n    } else {\n      out[i] = 0;\n    }\n  }\n}\n\ntemplate <>\nstruct ZeroOutFunctor<GPUDevice> {\n  void operator()(const GPUDevice& d,\n          typename TTypes<float>::ConstFlat input,\n          typename TTypes<float>::Flat output,\n          const int N) {\n    // How to compute the optimal block count and threads per block?\n    // tensorflow/core/util/cuda_kernel_helper.h isn;t included in the binary\n    // distribution\n    ZeroOutKernel<<<32, 256, 0, d.stream()>>>(input.data(), output.data(), N);\n  }\n};\n\ntemplate struct ZeroOutFunctor<GPUDevice>;  \n} // namespace functor \n} // namespace tensorflow\n#endif // GOOGLE_CUDA\n```\n#### `Makefile`\n\n``` make\nINCLUDE += -I /usr/local/cuda-7.5/include\nINCLUDE += -I $(shell python -c \\\n    'import tensorflow as tf; print(tf.sysconfig.get_include())')\n\nCXX = gcc -std=c++11\nCXXFLAGS =                          \\\n    -D_MWAITXINTRIN_H_INCLUDED  \\\n    -D_FORCE_INLINES            \\\n    $(INCLUDE) -fPIC -lcudart   \\\n\nNVCC = nvcc -std=c++11 -c\nNVCCFLAGS =                         \\\n    -D_MWAITXINTRIN_H_INCLUDED  \\\n    -D_FORCE_INLINES            \\\n    $(INCLUDE) -x cu -Xcompiler -fPIC\n\nLDFLAGS = -shared\nCUDA_SRCS = zero_out_gpu.cu.cc\nSRCS = zero_out.cc\nRM = rm -f\nTARGET_LIB = zero_out.so\nCUDA_OBJ = zero_out.cu.o\n\nall: $(TARGET_LIB)\n\n# This target (CPU and GPU) does not find the right symbols\n$(TARGET_LIB): $(SRCS) $(CUDA_OBJ) \n    $(CXX) $(LDFLAGS) -o $@ $^ $(CXXFLAGS) -DGOOGLE_CUDA=1\n\n# This target (CPU only) is fine\n# $(TARGET_LIB): $(SRCS) \n#   $(CXX) $(LDFLAGS) -o $@ $^ $(CXXFLAGS) -DGOOGLE_CUDA=0\n\n$(CUDA_OBJ): $(CUDA_SRCS)\n    $(NVCC) -o $@ $^ $(NVCCFLAGS) -DGOOGLE_CUDA=1\n\n.PHONY: clean\nclean:\n    -$(RM) $(TARGET_LIB)\n    -$(RM) *~\n    -$(RM) *.o\n```\n### Compilation\n\nCompilation runs without errors:\n\n``` bash\n$ make\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\nnvcc -std=c++11 -c -o zero_out.cu.o zero_out_gpu.cu.cc -D_MWAITXINTRIN_H_INCLUDED -D_FORCE_INLINES -I /usr/local/cuda-7.5/include -I /home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/include -x cu -Xcompiler -fPIC -DGOOGLE_CUDA=1\n/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/allocator.h(155): warning: missing return statement at end of non-void function \"tensorflow::Allocator::RequestedSize\"\n\n/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/allocator.h(155): warning: missing return statement at end of non-void function \"tensorflow::Allocator::RequestedSize\"\n\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\ngcc -std=c++11 -shared -o zero_out.so zero_out.cc zero_out.cu.o -D_MWAITXINTRIN_H_INCLUDED -D_FORCE_INLINES -I /usr/local/cuda-7.5/include -I /home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/include -fPIC -lcudart  -DGOOGLE_CUDA=1\n```\n## Error When Loading the Library:\n\n``` bash\n$ python -c \"import tensorflow as tf; tf.load_op_library('zero_out.so')t.so')\"\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/home/sarroff/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py\", line 75, in load_op_library\n    raise errors._make_specific_exception(None, None, error_msg, error_code)\ntensorflow.python.framework.errors.NotFoundError: zero_out.so: undefined symbol: _ZN10tensorflow7functor14ZeroOutFunctorIN5Eigen9GpuDeviceEEclERKS3_NS2_9TensorMapINS2_6TensorIKfLi1ELi1ElEELi16EEENS7_INS8_IfLi1ELi1ElEELi16EEEi\n```\n## Notes and Some Things I've Tried\n1. I have no difficulties compiling and loading a shared library for CPU-only. (See the commented target in the Makefile.)\n2. My problem persists whether using a TF bleeding-edge source (with `bazel`) or TF binary installation (with `make`)\n3. I've tried compiling the shared library with `g++`, as well as using some earlier `gcc-4.*` compilers\n4. I've tried mimicking the nvcc and gcc options provided in `CROSSTOOL.tpl`\n5. I don't have immediate access to an earlier Linux distro, otherwise I would have tried it\n\nAny help is greatly appreciated!\n", "comments": ["I believe you have to forward declare the template specialization of the functor in the .cc file (not the .cu.cc file).\n\nExample: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/adjust_contrast_op.cc#L178\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/adjust_contrast_op_gpu.cu.cc#L28\n\nSo basically forward declare the functor you have in the .cu.cc file, and I think you'll have more luck.\n\nSince it's not clear this is a bug, I'm closing this.  In the future, these types of 'requests for help' are better asked on StackOverflow.  It's better to determine if it's a real bug there, and then file the specific bug on GitHub later.  Good luck!\n", "Thanks for your help and quick response. Unfortunately the forward declaration doesn't appear to fix the problem. I've tried putting it in the .cc file and the .h file without success. I don't have complete confidence that this isn't an issue with my configuration. However if you think this problem is better suited for SO, I'll be happy to investigate there.\n\n**Edit**: I'm definitely doing something wrong. I'm able to compile, link, and load `adjust_contrast_op` as a user op. Sorry for the troubles.\n", "@woodshop Do you figure out what's wrong now, as i'm facing the similar problem. ", "Hello @songmeixu,\r\nI don't remember my exact solution. Using a forward declaration was definitely the first step to solving the problem. However the declaration exposed other linking problems that needed a resolution. Sorry I can't be more specific.", "Any suggestions to fix this issue @woodshop  @songmeixu "]}, {"number": 4117, "title": "Use shallow git clone in Dockerfiles", "body": "`git clone --depth 1 --shallow-submodules ...`\n\n_EDIT_: Looks like have to omit `--shallow-submodules` for the version of git in the image. This does not seem to affect the size or the speed.\n", "comments": ["Will that work if we're operating at a specific commit that's not HEAD for our git repos?\n", "The Dockerfile clones specifies a specific branch at clone time.\n", "Okay, can you give it a try with and without the shallow-submodules option and show us the time difference?\n", "Sorry, I should add this has two benefits:\n1. faster build\n2. more importantly: smaller Docker image. Right now it's > 3 GB.\n", "1. How much faster?\n2. How is this related to #4116  ?\n", "1. On my local machine 3.8s vs 12.1s\n2. Related; it is one more step to cutting bloat in the image. That one I hope cuts 2+ GB. This one is a more modest 50MB.\n", "@caisq, could you take a look at integrating this. the devel docker image is pretty insanely huge right now. Somebody nearby filled their disk and wedged their machine installing that docker image :).\n", "Any thoughts on this?\n", "On my todo list. Thanks.\n", "We decided not to do shallow git clone, because\n1) the space saving is not significant, compared to the space saving brought by removing bazel build cache as implemented in https://github.com/tensorflow/tensorflow/commit/8a724211a9d4871338afe874276abf1d358f799b for issue    https://github.com/tensorflow/tensorflow/issues/4116\n2) the git history may be useful for certain development purposes. \n\nClosing this issue.\n"]}, {"number": 4116, "title": "Cleanup Bazel Cache in Dockerfile", "body": "Right now I see the following in the Docker image:\n\n```\n--- /root/.cache/bazel/_bazel_root ---------------------------------------------------------------------------------------------------------------------------------------------------------\n                        /..\n    2.3GiB [##########] /68a62076e91007a7908bc42a32e4cff9\n   84.3MiB [          ] /install\n```\n\nwhich leads to a very large Docker image.\n", "comments": ["Ping? This seems to cut down the size of the image >1GB.\n", "So we actually did this on purpose; totally worth discussing whether or not it's still a good idea. \ud83d\ude09 @vrv @danmane @martinwicke as some folks who may have opinions.\n\n**Why?** The files here make it so that a freshly run container can run a `bazel build` without first having to rebuild the universe. This is explicitly trading bandwidth for computation time, which we thought was worthwhile. If someone doesn't need to fiddle with TF code, we have the non-devel builds; if they do, this saves time.\n\n@cancan101 Just to ask, how are you using the devel builds? Does saving build time help you?\n", "I am using the Docker image as the environment on a cluster. That means the first time I run a job on a new node, it has to pull the docker image.\n\nThis means I am storing 1-2GB per node of unneeded stuff and also that on each new node I have to pull 1-2GB before the node can be used.\n", "What are you doing once the machines start up in the cluster? Do they just need the TF library to be available, or are you building something from source?\n\nIf they just need TF to be available, you can switch from `gcr.io/tensorflow/tensorflow:devel` to just `gcr.io/tensorflow/tensorflow`, which is a much smaller image (even beyond dropping the bazel cache).\n", "I am using the GPU image. Is there a Dockerfile that I can build myself?\n", "`gcr.io/tensorflow/tensorflow:gpu` should be the image you're looking for. All the existing dockerfiles are in [tensorflow/tools/docker](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker).\n", "I would like to build myself since I am would like to build with compute capabilities 3.0 and 3.7.\n", "SGTM -- I think @caisq is the person to ask here, since he wrote [this build script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/parameterized_docker_build.sh). In particular, I think there's a correct set of params to pass there to get precisely what you want: maybe just `GPU` and `NO` for the first two?\n", "I'm not sure what you are suggesting here? I am currently building the Docker image gpu-devel. It looks like the other image expects to install a prebuilt wheel. I like the ability to build from scratch and not rely on e[xternal files](https://github.com/tensorflow/tensorflow/blob/891f8f73cc1967a5c2da89057884bc3dc1f9091a/tensorflow/tools/docker/Dockerfile.gpu#L45).\n\nI was trying to suggest a way of cutting down the size of this image. Perhaps there is a better image for what I want, but so far the ones I am seeing are just wrappers around prebuilt binaries.\n\nEDIT: Also if I understand that linked script, it calls `ci_parameterized_build.sh` which expects to build the wheels outside of a Docker environment?\n", "Ah, I think I probably left out a bit of detail.\n\nAs you mentioned, what you want is to build the equivalent of the `tensorflow:gpu` build, but with custom flags (different compute capability support), and without depending on the existing `.whl` file on `storage.googleapis.com`. \n\nThe way the `:gpu` image is built is by\n1. building the `tensorflow:devel-gpu` container, \n2. producing a new `.whl` file from that container, \n3. copying it to `storage.googleapis.com`, and \n4. building the `tensorflow:gpu` image with that newly-created file.\n\nWhat you want is a variant of this, where you do (1) and (2) with custom options, but then directly go to (4) without copying the file to `storage.googleapis.com` at all. \n\nI believe the linked build script does precisely that, since that's what happens in the process of doing our CI builds; the only question is getting the flags right. I was thinking you could just make use of that script, since then you don't end up owning your own set of custom build scripts.\n\nThat said, you're totally welcome to write your own if that's easier for you.\n", "The linked ci script might be overkill (?) for this. I would imagine this can be boiled down \n1. docker build gpu-devel\n2. docker run with volume to copy out the wheel\n3. docker build of the gpu image with the wheel in the build context\n\nMy initial hope was that with sufficient cleaning of the bazel cache, I could get by with just doing step 1 and having an imagine that was \"small enough\".\n", "Yeah, that makes sense. I'm not sure what the shortest path is, since squash support seems to be [stuck in committee](https://github.com/docker/docker/pull/22641).\n\nIt's actually not so bad to do this by flattening an existing container -- it mostly amounts to dropping some files and doing `docker export <big-container> | docker import - <flat-container>` (it's how we built containers for TF at one point long ago). I'm happy to sketch that out if you're interested, but you already be tired of my \"easy solutions\" from above. \ud83d\ude09 \n", "I'm closing this issue due to inactivity. I'll open this again if the issue is updated with new information.\n\nOne thing you'll be pleased to hear @cancan101 is that I managed to cut down the disk size used by Bazel by a few hundred megs in 65038b084059cf934df50fa86dba5b0e765f9d65 on September 21st.\n", "Apologies, I failed to notice this was assigned. Re-opening.\n", "Fixed by https://github.com/tensorflow/tensorflow/commit/8a724211a9d4871338afe874276abf1d358f799b\n\nThank you for the suggestion, @cancan101 \n\nDocker image size reduction is already reflected in the tensroflow/tensorflow:nightly\\* tags on Docker Hub, and will be reflected in tensorflow/tensorflow:latest\\* tags on next release.\n"]}, {"number": 4115, "title": "lstm code example clarification", "body": "otherwise the probabilities variable changes every iteration, preventing us from \"predicting the probabilities of the possible continuations of the sentence\"\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 4114, "title": "Train DNN model efficiently with sparse features and missing data", "body": "We have used TensorFlow for recommend systems to replace the logistic regression model. The train data is quite sparse and we have to do it efficiently. Now we have the problem to train the model without sparse tensor's multiplication.\n\nWe have read `word2vec` and the example of `wide_n_deep` models. Their inputs are sparse but not suitable for general recommend systems. Our inputs are sparse and we need to encode the features like ages, colleges with one-hot encoding. The examples may have different number of valid features because of unknown values and the train data looks like this.\n\n```\nlabel         gender        age        college       \n------------------------------------------------\n0             2:1           8:1       (unknown)\n1             1:1           5:1         25:1\n0             1:1           8:1       (unknown)\n```\n\nWith this dataset, we have to build `SparseTensor` object for each example data like this.\n\n```\nexample1 = tf.SparseTensor(indices=[[2], [8]], values=[1, 1], shape=[100])\nexample2 = tf.SparseTensor(indices=[[1], [5], [25]], values=[1, 1, 1], shape=[100])\nexample3 = tf.SparseTensor(indices=[[1], [8]], values=[1, 1], shape=[100])\n```\n\nFor `word2vec` and `wide_n_deep` models, we can use `tf.nn.embedding_lookup()` to lookup the variables to train and no need to fill up the whole matrix with zeros. This works for each example but we have problems if using batch because the valid `ids` has different shape to train. The code  looks like this. \n\n```\nvocabulary_size = 100\nembedding_size = 1\nembeddings = tf.Variable(tf.ones([vocabulary_size, embedding_size]))\n\nbatch_size = 3\nfeature_number = 3 # ERROR: should be 2 or 3\ntrain_inputs = tf.placeholder(tf.int32, shape=[batch_size, feature_number])\n\nbatch_data = np.array([[2, 8], [1, 5, 25], [1, 8]]) # EROOR: should be dense\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(embed, feed_dict={train_inputs: batch_data}))\n```\n\nThe code could not work until we make `example1`, `example2` and `example3` have the same number of valid featue, such as 3 in this case. This is one solution but we have to fill up the examples with zeros(notice that this is different from filling up the one-hot encoding).\n\nMaybe supporting sparse tensor's multiplication is quit more efficient. Now we have only `embedding_lookup` op for SpareTensor and I don't know how to use `embedding_lookup_sparse` or `safe_embedding_lookup_sparse` for this scenario. It could be great if anyone has any suggestion for this use case.\n\nRelated to https://github.com/tensorflow/tensorflow/issues/1241\n", "comments": ["@ebrevdo @mrry can you guys help?\n\nI thinks it's an general scenario but we have no idea to implement it with TensorFlow. We have implemented the logistic regression model with the same dataset in Spark. It iterates each example without batching and doesn't have this issue. We can also fill up the `unknown` data with zeros or default values but this is some kind of inefficient because actually we don't need to compute their variables.\n", "You can create a rank-3 SparseTensor that contains all batches.  Its indices matrix would look like, e.g. (from your example):\n\n```\n[0  2]\n[0  8]\n[1  1]\n[1  5]\n[1 25]\n[2  1]\n[2  8]\n```\n\nand its values would be a vector with the concatenated values of the 3 batch entries.\n\n`safe_embedding_lookup_sparse` can handle this kind of SparseTensor (actually i think the others can too?)\n\nDoes this answer your question?  I'm not 100% sure.\n", "(there's also the `sparse_tensor_dense_matmul` op which allows one to multiply rank-2 SparseTensors with dense matrices; but i feel that `safe_embedding_lookup_sparse` is sufficient for what you want to do)\n", "Thanks @ebrevdo for the quick response. We have considered to use the rank-3 SparseTensor instead of a batch of rank-2 SparseTensors. It's the different way to represent the train data and of course we can use it.\n\nBut the `safe_embedding_lookup_sparse` is not only the lookup method but also combining the weights sum/average of each row. I have read the [api docs](https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#embedding_lookup_sparse) and trace the unit test of `safe_embedding_lookup_sparse`.\n\n![screen shot 2016-08-30 at 12 19 02](https://cloud.githubusercontent.com/assets/2715000/18116453/89381de0-6f7a-11e6-9635-26ef332160d3.png)\n\nIt's some kind of computation but not for our model. For dense data, we compute the logist with `w * x + b` where w, x and b are dense matrix. For sparse data, just like word2vev, we use `embedding_lookup` to lookup the related variables and apply to the similar function. Now we can do that but `embedding_lookup` need the dense `ids` and we can't satisfy this because we have missing features now.\n\nWe have tried `safe_embedding_lookup_sparse` but don't know any way to do something like `w * x + b`. It would be better to have an example about using this to get the logis so that we can write the loss function to train.\n\nNot sure if `safe_embedding_lookup_sparse` can do this. If not, we need to lookup each \"reldated\"  variables and do the sparse multiplication.\n", "[Wide and deep model](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py) is the best example to handle sparse data and use `safe_embedding_lookup_sparse`. Notice that the input data doesn't have missing feature, which means each example has exactly same number of feature values.\n\nThe model handles continues columns and categorical(sparse) columns in different ways. It will create one SparseTensor for each categorical column, which is quite different from what @ebrevdo  and I mentioned above. Now we have many SparseTensors such as \"education_num\", \"capital_gain\" and \"capital_loss\" and the shape is the same `[batch_size, 1]`. This is the precondition of using `safe_embedding_lookup_sparse` because it doesn't need to aggregate anything in this use case. In each train step, we need a `for` loop to iterate all columns' outputs and it will be much slower if we have more features.\n\nSo I don't know `safe_embedding_lookup_sparse` is the solution for this problem. It works like `embedding_lookup` and we have to create many SparseTensors for each column.\n\nIt would be better to know how Google uses TensorFlow for sparse inputs internally. It's general in recommendation system and CTR scenario. Because what I want is how to handle this use cause instead of using sparse matrix multiplication, I'm gonna change the tittle and please comment if you have any practices with TensorFlow.\n", "This question, since it concerns how to use TensorFlow properly, is much better asked on StackOverflow. If you believe there is a bug or a very specific feature request, please resubmit those as more focused requests referencing this issue.\n", "@tobegit3hub  do you have resolved the problem?  ", "could tensorflow 2.0 support sparse input training and prediction now?"]}, {"number": 4113, "title": "add Rint operation", "body": "Based on previous work: https://github.com/tensorflow/tensorflow/pull/1288/files \nand from a comment on #3730. Fixes #3730.\n", "comments": ["Can one of the admins verify this patch?\n", "There are conflicts, have you rebased already?\n", "@martinwicke  @girving I addressed all the changes proposed. \n", "One thing to keep in mind is that std::round sets the rounding mode on every call and is significantly slower than std::rint, so it's nice to have this op.\n", "@Mistobaan Friendly ping so this PR doesn't get forgotten :) also it needs to be rebased.\n", "@rmlarsen can you finish this review since @girving is out?\n", "Sure. @Mistobaan let us know when this is ready for another look.\n", "@rmlarsen I was not able to compile the TF repository for a few days now due to this issue: #4312 \n", "@Mistobaan It looks like a fix for that issue was pushed today. Can you give it another try, please?\n\nhttps://github.com/tensorflow/tensorflow/commit/f66b491a06627510c1cf751fc11db2caf5aa1f25\n", "@Mistobaan friendly ping. I'd love to have this change. :)\n", "Yes, Is in my todo queue :) I will try to finish it later today.\n\nOn Mon, Sep 19, 2016 at 11:21 AM Martin Wicke notifications@github.com\nwrote:\n\n> @Mistobaan https://github.com/Mistobaan friendly ping. I'd love to have\n> this change. :)\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4113#issuecomment-248076979,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAG31wgnSZW4fWItZD16Q28aD_xM99mhks5qrtKegaJpZM4JxJ4z\n> .\n", "@Mistobaan Is this ready for review? I didn't see any updates.\n", "Not yet, I made most of the proposed changes but seems the tests\nare failing\n\nOn Monday, September 26, 2016, Rasmus Munk Larsen notifications@github.com\nwrote:\n\n> @Mistobaan https://github.com/Mistobaan Is this ready for review? I\n> didn't see any updates.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4113#issuecomment-249615398,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAG318wSmDhpZ6Wi3EfKiKC6lRaXTyWnks5qt-1NgaJpZM4JxJ4z\n> .\n\n## \n\nLinkedIn: http://linkedin.com/in/fmilo\nTwitter: @fabmilo\n\n## Github: http://github.com/Mistobaan/\n\nSimplicity, consistency, and repetition - that's how you get through. (Jack\nWelch)\nPerfection must be reached by degrees; she requires the slow hand of time\n(Voltaire)\nThe best way to predict the future is to invent it (Alan Kay)\n", "@girving @martinwicke @rmlarsen I think this is ready to review\n", "Jenkins, test this please.\n", "looks like Android does not have std::rint ... ? all the others were aborted looks like\n", "Odd. @petewarden what's the best way to deal with the lack of std::rint on android? Should we disable this op on mobile? Or can we use an alternative?\n", "As far as I can tell, `rint()` is in the C99 standard, but not the C++ standard. Looking through StackOverflow, it sounds like this is an issue for other platforms like Visual Studio on Windows too. Is it possible to write a reference implementation to achieve the same behavior on platforms where we know the function isn't present? That would allow us to support graphs that use it, even if we lose performance.\n", "@petewarden You mean putting together a base implementation like the ones suggested on: http://stackoverflow.com/questions/4572556/concise-way-to-implement-round-in-c ?\n", "@Mistobaan yes, it looks like we'll need something along those lines unfortunately. Since we can rely on the C++ standard library, that might make it a bit easier than the newlib approach though?\n", "@tensorflow-jenkins test this please\n", "std::rint undefined in android build. Probably need to include <cmath>.\n\nThe Makefile build fails with undefined tensorflow::port::ScopedSetRound::ScopedSetRound. You probably need to link it somewhere.\n", "Friendly ping.\n", "@vrv @martinwicke @petewarden  let's try this simple solution for the rint problem on android \n", "@tensorflow-jenkins test this please\n", "The failures in the CI do not seem related to the patch, do they?\n", "Test failure has been fixed at HEAD, so you should be good, modulo the review comments.\n", "addressed comments \n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "Makefile failed:\n\n/workspace/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/example/feature.pb.o /workspace/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/example/example.pb.o  -L/workspace/tensorflow/contrib/makefile/gen/protobuf-host/lib -L/usr/local/lib -lstdc++ -lprotobuf -lpthread -lm -lz -ldl -lpthread\n/workspace/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/core/threadpool.o: In function `tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}::operator()() const':\nthreadpool.cc:(.text._ZZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIFvvEEENKUlvE_clEv[_ZZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIFvvEEENKUlvE_clEv]+0x30): undefined reference to`tensorflow::port::ScopedSetRound::ScopedSetRound()'\nthreadpool.cc:(.text._ZZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIFvvEEENKUlvE_clEv[_ZZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIFvvEEENKUlvE_clEv]+0x48): undefined reference to `tensorflow::port::ScopedSetRound::~ScopedSetRound()'\nthreadpool.cc:(.text._ZZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIFvvEEENKUlvE_clEv[_ZZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIFvvEEENKUlvE_clEv]+0x75): undefined reference to`tensorflow::port::ScopedSetRound::~ScopedSetRound()'\ncollect2: error: ld returned 1 exit status\n\nAny ideas?\n", "I think you have to add the added .cc files to the list of files compiled by makefile.\n\n@petewarden to confirm where exactly.\n", "Looks like 'tensorflow/core/platform/setround.cc' isn't being linked in by the makefile. This is surprising, because I would expect the wildcard here to include it in the sources:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/Makefile#L423\n\nI also see other files in tensorflow/core/platform being compiled and linked in, looking at the logs:\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-makefile/6255/consoleFull\n\nCan you try adding $(warning $TF_CC_SRCS) around line 467 in the Makefile to show what we've calculated? If that's tricky, I can dig into it myself later today.\n", "ping for @petewarden and @Mistobaan \n", "Friendly ping -- it would be lovely to get this in.\n", "@vrv I was able to reproduce and fix the error ( setround.cc had to be manually added to a dependency text file). \n", "@tensorflow-jenkins test this please\n\n(WoohoooooO!)\n", "now the windows cmake build is failing ... I can't help with that. I don't have  a win machine. \n", "Strange, it cannot import tensorflow. Let's see if it was a transient issue.\n\nJenkins, test this please.\n", "@drpngx rebased and addressed the proposed changes.\n", "Jenkins, test this please.\n", "Something broken with the makefile, will have to try again later.\n", "@gunan it looks like the makefile issue is unrelated. Can we merge this?\n", "Yes, the failure looks like a flake. We can go ahead and merge.\n", "Is it really `std::fesetround` or is it just ```fesetround```?\r\n\r\nI received the following ERROR when building.\r\n\r\n\r\n```\r\nERROR: /local/tmp/tensorflow/tensorflow/core/BUILD:1036:1: C++ compilation of rule '//tensorflow/core:lib_internal' failed: gcc failed: error executing command /local2/tmp/brew/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/local2/tmp/brew/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 ... (remaining 104 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/core/platform/setround.cc: In constructor 'tensorflow::port::ScopedSetRound::ScopedSetRound()':\r\ntensorflow/core/platform/setround.cc:27:4: error: 'fesetround' is not a member of 'std'\r\n    std::fesetround(FE_TONEAREST);\r\n    ^\r\ntensorflow/core/platform/setround.cc:27:4: note: suggested alternative:\r\nIn file included from tensorflow/core/platform/setround.cc:19:0:\r\n/local2/tmp/brew/include/fenv.h:88:12: note:   'fesetround'\r\n extern int fesetround (int __rounding_direction) __THROW;\r\n            ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nEDIT:\r\nOn further inspection, the problem seems to be that gcc is being used to compile a C++ file.. I have opened issue #7289"]}, {"number": 4112, "title": "op LogUniformCandidateSampler should raise ValueError when range_max < num_sampled", "body": "This Op registration should have a modified Attr\n\ninstead of \n\n```\nREGISTER_OP(\"LogUniformCandidateSampler\")\n    .Input(\"true_classes: int64\")\n    .Output(\"sampled_candidates: int64\")\n    .Output(\"true_expected_count: float\")\n    .Output(\"sampled_expected_count: float\")\n    .Attr(\"num_true: int >= 1\")\n    .Attr(\"num_sampled: int >= 1\")\n```\n\nthis\n\n```\nREGISTER_OP(\"LogUniformCandidateSampler\")\n    .Input(\"true_classes: int64\")\n    .Output(\"sampled_candidates: int64\")\n    .Output(\"true_expected_count: float\")\n    .Output(\"sampled_expected_count: float\")\n    .Attr(\"num_true: int >= 1\")\n    .Attr(\"num_sampled: int <=range_max\")\n```\n\nI ran into this issue when training a word2vec with noise contrastive estimate algorithm where the size of vocabulary was smaller than the size of negative samples used to train the logistic classifier.\n\nCurrently core/kernels/range_sampler.cc throws this error\n\n```\nF tensorflow/core/kernels/range_sampler.cc:86] Check failed: batch_size + avoided_values.size() <= range_ (5 vs. 4)\n```\n\nThe error does not describe the variable which is the cause of this error.\n", "comments": []}, {"number": 4111, "title": "CUDNN_STATUS_BAD_PARAM   tensorflow/stream_executor/cuda/cuda_dnn.cc:423", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nCUDNN_STATUS_BAD_PARAM\n### Environment info\n\nOperating System:OS: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root   322936 Aug 16  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 16  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Aug 16  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 Aug 16  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Aug 16  2015 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13 Aug 30 22:56 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 root root       17 Aug 30 22:56 /usr/local/cuda/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxr-xr-x 1 root root 61272736 Jan 12  2016 /usr/local/cuda/lib64/libcudnn.so.4.0.4\n-rwxr-xr-x 1 root root 61453024 Aug 30 22:55 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 62025862 Aug 30 22:55 /usr/local/cuda/lib64/libcudnn_static.a\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n   https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\n   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\n   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\n   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\n   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n   0.10.0rc0\n   If installed from source, provide \n3. The commit hash (`git rev-parse HEAD`)\n4. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nI also used data from hd5f file, before the tf.Session I extract the data first. \nIt works for the 200000 iterations, but when it comes more than 200000 the program will be terminated.\n### What other attempted solutions have you tried?\n\nI tried to re-install cudnn 4.0 \n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\nIteration: 240640\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:423] could not set cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\nAborted (core dumped)\nStrange thing happens after the iteration:240640 every time\n", "comments": []}, {"number": 4110, "title": "r0.10: function 'repository_rule' does not exist", "body": "Hi,\n\nI previously installed tensorflow 0.8 and 0.9 on a GPU cluster with several local patches to circumvent non-standard paths for crosstool files and swig. Our users are now requesting an upgrade to 0.10 but I have the following error... Installing tensorflow on a cluster is so painful....\n\nERROR: ~/tensorflow_0.10/tensorflow/third_party/gpus/cuda_configure.bzl:415:18: function 'repository_rule' does not exist.\n\nI suppose this is related to bazel. repository_rule is an experimental feature of bazel. What version of bazel is required to install 0.10? bazel is also painful to install to so we try to avoid upgrades as much as possible. This is very costly in terms of sys admin time.\n\nverbose logs:\n\n```\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /global/software/Core/GCC/4.9.2-binutils-2.25/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5.18\nPlease specify the location where CUDA 7.5.18 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /global/software/Core/CUDA/7.5.18\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5.0\nPlease specify the location where cuDNN 5.0 library is installed. Refer to README.md for more details. [Default is /global/software/Core/CUDA/7.5.18]: /global/software/Core/cuDNN/5.0-CUDA-7.5.18\nInvalid path to cuDNN  toolkit. Neither of the following two files can be found:\n/global/software/Core/cuDNN/5.0-CUDA-7.5.18/lib64/libcudnn.so.5.0\n/global/software/Core/cuDNN/5.0-CUDA-7.5.18/libcudnn.so.5.0\n.5.0\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5   \nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /global/software/Core/CUDA/7.5.18]: /global/software/Core/cuDNN/5.0-CUDA-7.5.18\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: 3.7\nWARNING: Output base '~/.cache/bazel/_bazel_hidden/2a1b98729d2f817402a27341162b0ab6' is on NFS. This may lead to surprising failures and undetermined behavior.\n.....................\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\nERROR: ~/.cache/bazel/_bazel_hidden/2a1b98729d2f817402a27341162b0ab6/server (Directory not empty).\nWARNING: Output base '~/.cache/bazel/_bazel_hidden/2a1b98729d2f817402a27341162b0ab6' is on NFS. This may lead to surprising failures and undetermined behavior.\n..............\nERROR: ~/tensorflow_0.10/tensorflow/third_party/gpus/cuda_configure.bzl:415:18: function 'repository_rule' does not exist.\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package 'external': Extension 'third_party/gpus/cuda_configure.bzl' has errors.\nConfiguration finished\n```\n### Environment info\n\nOperating System: RHEL 6.7\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\nls /global/software/Core/CUDA/7.5.18/lib/libcud*\n/global/software/Core/CUDA/7.5.18/lib/libcudadevrt.a\n/global/software/Core/CUDA/7.5.18/lib/libcudart.so\n/global/software/Core/CUDA/7.5.18/lib/libcudart.so.7.5\n/global/software/Core/CUDA/7.5.18/lib/libcudart.so.7.5.18\n/global/software/Core/CUDA/7.5.18/lib/libcudart_static.a\n```\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n\n```\n$ git rev-parse HEAD\n6ce5b5c8298273e3861a75fb6ccde63b9dd157c5\n```\n1. The output of `bazel version`\n\nbazel is 0.2.0 but `bazel version` doesn't seem to work...\n\n```\n$ bazel version\nWARNING: Output base '~/.cache/bazel/_bazel_hidden/2a1b98729d2f817402a27341162b0ab6' is on NFS. This may lead to surprising failures and undetermined behavior.\nBuild target: bazel-out/local_linux-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n```\n", "comments": ["I think you'll need to upgrade your version of bazel to 0.3.0 -- they added a bug fix for cuda configuration that requires a new feature.\n", "Thank you @vrv for your quick reply. So for 0.8, the latest bazel was too new, now it's too old... The configure script should check for the right one I guess...\n\nThe TF build scripts need patching anyway when using non-standard paths. One example, in `third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl`:\n\n```\nCURRENT_DIR = os.path.dirname(sys.argv[0])\nNVCC_PATH = CURRENT_DIR + '/../../../cuda/bin/nvcc'\nLLVM_HOST_COMPILER_PATH = ('/usr/bin/gcc')\n```\n\n:(\nI just dream of autotools here.\n", "I had an old version of bazel as well, because my brew on OSX 10.12.3 needed upgrading:\r\n```\r\nbrew update\r\nbrew upgrade bazel\r\n```"]}, {"number": 4109, "title": "Clarify that Mac works with cuDNN5.", "body": "", "comments": ["Keeping unix binary installation recommendation to v4 as we are still using v4 for this release. Removing v2 as suggested by Vijay. \n@vrv could you take another look? I would like to get this updated so people stop running into #3826 . Thanks!\n", "LGTM\n\nWe can make the same change but with the v4->v5 changes in master :)\n"]}, {"number": 4108, "title": "Issue3963", "body": "Added threadpool.h to the BUILD script so it can be installed in the public #include directory with a binary pip install. Fixes issue https://github.com/tensorflow/tensorflow/issues/3963\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I should be part of the CLA for Hewlett Packard Enterprise.\n", "From our page about Corp CLAs: \"The Corporate CLA is only signed once per corporation. As part of signing the CLA, they designate a Google Group that identifies who at their company is authorized to contribute to Google projects.\" -- do you know whether you've been added to the appropriate google group, and is the email used on your commits the one that is in that group?\n", "I went through the legal review process to be allowed to contribute to tensorflow, so I thought all was set, but let me check on the Google Group.\n", "@kbrems Friendly ping so this PR doesn't get forgotten :)\n", "Sigh. The contribution has been approved by our legal dept. but they have to add me to the google group on the CLA and that is taking forever...\n", "My email address should now be on the google group list for the CLA for Hewlett Packard Enterprise.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n\n(woohoo!!!)\n"]}, {"number": 4107, "title": " While training first TensorFlow neural net model", "body": "I build tensorflow for Mac OSX from sources following the Download and Setup guide from tensorflow.org. \n\n```\n$ which python\n/usr/local/bin/python\n$ pip --version\npip 8.1.2 from /usr/local/lib/python2.7/site-packages (python 2.7)\n```\n\nCreated the pip package and installed it with NO GPU support. \n\n```\n$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\n$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n\n# The name of the .whl file will depend on your platform.\n$ sudo pip install /tmp/tensorflow_pkg/tensorflow-0.10.0rc0-py2-none-any.whl\n```\n\nWhen attempting to Train first neural net model from the root of the source tree:\n\n```\n$ cd tensorflow/models/image/mnist\n$ python convolutional.py\n\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\n\nTraceback (most recent call last):\n  File \"convolutional.py\", line 326, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"convolutional.py\", line 243, in main\n    batch = tf.Variable(0, dtype=data_type())\n\nTypeError: __init__() got an unexpected keyword argument 'dtype'\n```\n", "comments": ["This keyword argument has been available since https://github.com/tensorflow/tensorflow/commit/60dfe8852cfd9565e622e423098bdad746dc3aee, which includes TensorFlow versions 0.8 and later.\n\nCan you try the following at a Python shell and report the results?\n- `import tensorflow as tf; print tf.__version__`\n- `help(tf.Variable)`\n", "```\n>>> import tensorflow as tf; print tf.__version__\n0.7.1\n```\n", "You'll need to upgrade your version of tensorflow to 0.8+, or check out the source code at the 0.7.1 tree, not master.\n"]}, {"number": 4106, "title": "Feature request: ability to sample from seq2seq decoder given input", "body": "Given some particular encoder input, it would be great to be able to sample sequences from the embedding_tied_rnn_seq2seq decoder. To my understanding, at present, this is possible with the tied_rnn_seqw2seq decoder, by passing in a custom loop_function, but it isn't possible in the embedding version. It appears that only argmax is supported with the feed_previous parameter. This would be useful in dialog/conversation applications, and perhaps others.\n\n.\n", "comments": ["We're still working on seq2seq. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 4105, "title": "ERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path. ", "body": "### Environment info\n\nOperating System:\nOS  10.10.5\n\nInstalled version of CUDA and cuDNN: \n\n```\n$ ls -l /usr/local/cuda/lib/libcud*\n-rwxr-xr-x  1 root           wheel      8280 Apr 13 01:02 /usr/local/cuda/lib/libcuda.dylib\nlrwxr-xr-x  1 root           wheel        45 Apr 13 01:03 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-7.5/lib/libcudadevrt.a\nlrwxr-xr-x  1 root           wheel        50 Apr 13 01:03 /usr/local/cuda/lib/libcudart.7.5.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.7.5.dylib\nlrwxr-xr-x  1 root           wheel        46 Apr 13 01:03 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.dylib\nlrwxr-xr-x  1 root           wheel        49 Apr 13 01:03 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart_static.a\n-rwxr-xr-x@ 1 production204  staff  60108616 Feb  8  2016 /usr/local/cuda/lib/libcudnn.4.dylib\nlrwxr-xr-x  1 root           admin        47 Aug 29 18:08 /usr/local/cuda/lib/libcudnn.5.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudnn.5.dylib\nlrwxr-xr-x  1 root           admin        45 Aug 29 18:08 /usr/local/cuda/lib/libcudnn.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudnn.dylib\n-rw-r--r--@ 1 production204  staff  59311504 Feb  8  2016 /usr/local/cuda/lib/libcudnn_static.a\n```\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. \n   (can't get that far, but i'm using 0.10)\n\n```\n>>> import tensorflow\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally\nSegmentation fault: 11\n\n```\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n\n```\n4c49dbebef05442c7e72d6129a30574fcd13f0e1\n```\n1. The output of `bazel version`\n\n```\n$ bazel version\nBuild label: 0.3.1-homebrew\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Aug 4 09:59:58 2016 (1470304798)\nBuild timestamp: 1470304798\nBuild timestamp as int: 1470304798\n```\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n```\n$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nINFO: Elapsed time: 0.076s\n\n```\n### What other attempted solutions have you tried?\n- Downgrading to cuDNN4, switching between 4 and 5\n- Re-installing bazel\n- Modifying CROSSTOOL file according to various threads\n- Manually linking CUDA libraries during `./configure` to not use symlinked libraries\n- Various other hacks over the last week \ud83d\ude2d\n", "comments": ["I got this recently too, I was somehow successful by just re-running ./configure and then immediately running bazel build, but I'm not sure what's going on.\n", "Following that @vrv I just tried de-installing TF completely and then re-running with the same ./configure settings:\n\n```\n$ ./configure\nPlease specify the location of python. [Default is /Library/Frameworks/Python.framework/Versions/2.7/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] y\nGoogle Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\n  /Library/Python/2.7/site-packages\nPlease input the desired Python library path to use.  Default is [/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]\n\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: 3.0\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.\nINFO: Waiting for response from Bazel server (pid 63884)...\nWARNING: /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/external/boringssl_git/WORKSPACE:1: Workspace name in /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/external/boringssl_git/WORKSPACE (@boringssl) does not match the name given in the repository's definition (@boringssl_git); this will cause a build error in future versions.\nINFO: All external dependencies fetched successfully.\nConfiguration finished\n```\n\nThen a simplified build command:\n\n```\n$ bazel build -c opt --config=cuda tensorflow/...\n```\n\nAfter a _very_ verbose and lengthy compile attempt, I received this error message (it caused OS X Terminal app to hang permanently as well, so I couldn't copy-paste, had to take a screenshot):\n\nhttps://www.dropbox.com/s/riu5f4n5aj1opmk/Screenshot%202016-08-30%2016.33.48.png?dl=0 \n\nYet again, some sort of a `CROSSTOOL` issue. I've made sure to pull and update my local TF often over the week that I've been trying to build, as I've seen lots of activity related to this component of TF. \n", "Yes if you are developing TF it happens quite often. I think it might be something related with the caching system of bazel. After few hours it resets and I have to re run the configure again.\n", "+cc @damienmg \n\nThis seems to be a bug in Bazel. (Edit: to clarify: I meant the occasional `no such package '@local_config_cuda//crosstool': BUILD file not found on package path.` error).\n\nNext time this happens, can you take a look at directory `bazel-tensorflow/external/local_config_cuda/crosstool` and let me know which files are there?\n", "@davidzchen thanks for your reply.\n\nThis error or some version of it is consistent:\n\n```\ntensorflow$ cd bazel-tensorflow/external/local_config_cuda/crosstool\n\nproduction204@Trevors-MacBook-Pro crosstool$ ls -l\ntotal 32\n-rwxr-xr-x  1 production204  wheel   925 Aug 30 14:45 BUILD\n-rwxr-xr-x  1 production204  wheel  9068 Aug 30 14:45 CROSSTOOL\ndrwxr-xr-x  3 production204  wheel   102 Aug 30 14:45 clang\n\nproduction204@Trevors-MacBook-Pro crosstool$ \n\n```\n", "I just saw your screenshot, and that appears to be a different problem than the `no such package '@local_config_cuda//crosstool': BUILD file not found on package path.` error, which seems to be a caching issue.\n\nDo you mean that the `crosstool_wrapper_driver_is_not_gcc` error occurs consistently? Is it causing your Terminal.app to hang every time? If it reproduces consistently, you re-run the command with `--verbose_failures`?\n", "I meant that my TF builds seem to fail related to crosstool consistently, probably my naivete on the specifics for me to think that `crosstool_wrapper_driver_is_not_gcc` and `CROSSTOOL` could be the same problem! \n", "No problem. The naming could be a bit confusing. The `@local_config_cuda//crosstool` error may be an issue with Bazel's caching; I have ran into it a couple of times myself, and it usually goes away after I re-run `./configure`.\n\nWere you able to reproduce the `crosstool_wrapper_driver_is_not_gcc` error again? Looking at your screenshot, it looks like the `headerpad_max_install_names` flag is not spelled correctly for some reason since it is complaining about `eaderpad_max_install_names`. Did you change this flag in your CROSSTOOL file?\n", "I am getting the same missing crosstool on Linux. The strange thing is that there isn't even a bazel-tensorflow directory:\n\n```\n[david@SQUIDS tensorflow]$ ls\nACKNOWLEDGMENTS  avro.BUILD   boringssl.BUILD  bzip2.BUILD  CONTRIBUTING.md  farmhash.BUILD  gmock.BUILD  ISSUE_TEMPLATE.md  jsoncpp.BUILD  nanopb.BUILD  png.BUILD      README.md   six.BUILD   third_party  util       zlib.BUILD\nAUTHORS          boost.BUILD  bower.BUILD      configure    eigen.BUILD      gif.BUILD       grpc.BUILD   jpeg.BUILD         LICENSE        navbar.md     _python_build  RELEASE.md  tensorflow  tools        WORKSPACE\n```\n\nbazel is 0.3.1, and I have ran ./configure four times now.\n", "I do see that `eaderpad_max_install_names` reference in the screenshot above, however, after searching through all the files and folders in my TF directory I don't see that text anywhere, only `headerpad_max_install_names`. I don't know why the `h` would be getting clipped off. \n\nUpdate: Same error message related to `crosstool_wrapper_driver_is_not_gcc`. The `'@local_config_cuda//crosstool': BUILD file not found on package path.` error seems to have gone away after doing a `pip uninstall` on TF and then re-installing. \n\n```\nINFO: From Linking tensorflow/cc/ops/io_ops_gen_cc [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nINFO: From Linking tensorflow/cc/ops/random_ops_gen_cc [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nINFO: From Linking tensorflow/cc/ops/parsing_ops_gen_cc [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nINFO: From Linking tensorflow/cc/ops/sparse_ops_gen_cc [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nINFO: From Linking tensorflow/cc/ops/logging_ops_gen_cc [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nINFO: From Linking tensorflow/cc/ops/string_ops_gen_cc [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nINFO: From Linking tensorflow/cc/ops/user_ops_gen_cc [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nINFO: From Linking tensorflow/cc/ops/candidate_sampling_ops_gen_cc [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nINFO: From Linking tensorflow/cc/ops/control_flow_ops_gen_cc [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nINFO: From Linking tensorflow/cc/ops/image_ops_gen_cc [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nINFO: From Linking tensorflow/cc/ops/array_ops_gen_cc [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nINFO: From Linking tensorflow/cc/ops/linalg_ops_gen_cc [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nINFO: From Linking tensorflow/cc/ops/no_op_gen_cc [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nINFO: From Linking tensorflow/cc/ops/training_ops_gen_cc [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nERROR: /Users/production204/Github/tensorflow/tensorflow/cc/BUILD:179:1: Executing genrule //tensorflow/cc:io_ops_genrule failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 5.\ndyld: Library not loaded: @rpath/libcudart.7.5.dylib\n  Referenced from: /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/execroot/tensorflow/bazel-out/host/bin/tensorflow/cc/ops/io_ops_gen_cc\n  Reason: image not found\n/bin/bash: line 1: 44071 Trace/BPT trap: 5       bazel-out/host/bin/tensorflow/cc/ops/io_ops_gen_cc bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/io_ops.h bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/io_ops.cc 0\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 3015.469s, Critical Path: 3002.51s\n```\n\nThe complete log was too big for pastebin, here it is on Dropbox: https://www.dropbox.com/home/Documents%20Dropbox?preview=TW-TF-error-log-083116.txt\n\nThen, I run the same above commands, but with --verbose_failures (hard to imagine it being more verbose that the previous log, which was almost 15,000 lines!), final error message was:\n\n```\nERROR: /Users/production204/Github/tensorflow/tensorflow/cc/BUILD:179:1: Executing genrule //tensorflow/cc:training_ops_genrule failed: bash failed: error executing command \n  (cd /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/execroot/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/cuda/bin:/Library/Frameworks/Python.framework/Versions/2.7/bin:/usr/local/bin:usr/local/sbin:/usr/local/mysql/bin:/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \\\n    TMPDIR=/var/folders/h3/pn9k79xn6qd9jgksqbkpn3l80000gn/T/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/cc/ops/training_ops_gen_cc bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.h bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.cc 0'): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 5.\ndyld: Library not loaded: @rpath/libcudart.7.5.dylib\n  Referenced from: /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/execroot/tensorflow/bazel-out/host/bin/tensorflow/cc/ops/training_ops_gen_cc\n  Reason: image not found\n/bin/bash: line 1: 74845 Trace/BPT trap: 5       bazel-out/host/bin/tensorflow/cc/ops/training_ops_gen_cc bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.h bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.cc 0\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 3111.405s, Critical Path: 3097.65s\n\nproduction204@Trevors-MacBook-Pro tensorflow $ \n```\n\nHere's the complete log: https://www.dropbox.com/s/nozqcscnc9ho5uz/TW-TF-error-log-083116--verbose_failures.txt?dl=0 \n", "@Dapid That is interesting. How did you run the `./configure` script? Can you paste the output\n\n@damienmg Is there currently a way to inspect the contents of `/external` after running `bazel fetch` but before the Bazel output directories get symlinked?\n\n@trevorwelch FWIW, most of the noise in the output are compiler warnings. The `dyld: Library not loaded: @rpath/libcudart.7.5.dylib` error is interesting. Can you verify whether the file `bazel-bin/tensorflow/cc/tutorials_example_trainer.runfiles/local_config_cuda/cuda/lib/libcudart.7.5.dylib` exists? If not, what files are under the `bazel-bin/tensorflow/cc/tutorials_example_trainer.runfiles/local_config_cuda/cuda/lib` directory?\n", "@davidzchen \n\n```\n./configure \nPlease specify the location of python. [Default is /home/david/.virtualenvs/py35/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /bin/gcc]: /usr/local/cuda/bin/gcc\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \nlibcudnn.so resolves to libcudnn.4\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: 5.0\nINFO: Reading 'startup' options from /home/david/.bazelrc: --batch\nWarning: ignoring LD_PRELOAD in environment.\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\nINFO: Reading 'startup' options from /home/david/.bazelrc: --batch\nWarning: ignoring LD_PRELOAD in environment.\nWARNING: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/boringssl_git/WORKSPACE:1: Workspace name in /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/boringssl_git/WORKSPACE (@boringssl) does not match the name given in the repository's definition (@boringssl_git); this will cause a build error in future versions.\nINFO: All external dependencies fetched successfully.\nConfiguration finished\n```\n\n```\ngit log\ncommit 6ce5b5c8298273e3861a75fb6ccde63b9dd157c5\nAuthor: Sanders Kleinfeld <sandersk@users.noreply.github.com>\nDate:   Sun Aug 28 01:00:52 2016 -0400\n```\n\nOn branch r0.10.\n\nIf I leave the default GCC it is created, but the build fails because it is incompatible with CUDA.\n", "@davidzchen \nIt does exist:\n\n```\ntensorflow$ cd bazel-bin/tensorflow/cc/tutorials_example_trainer.runfiles/local_config_cuda/cuda/lib/\n\nlib$ ls -l\ntotal 40\nlrwxr-xr-x  1 production204  wheel  126 Aug 31 11:03 libcublas.7.5.dylib -> /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/external/local_config_cuda/cuda/lib/libcublas.7.5.dylib\nlrwxr-xr-x  1 production204  wheel  126 Aug 31 11:03 libcudart.7.5.dylib -> /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/external/local_config_cuda/cuda/lib/libcudart.7.5.dylib\nlrwxr-xr-x  1 production204  wheel  123 Aug 31 11:03 libcudnn.5.dylib -> /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/external/local_config_cuda/cuda/lib/libcudnn.5.dylib\nlrwxr-xr-x  1 production204  wheel  125 Aug 31 11:03 libcufft.7.5.dylib -> /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/external/local_config_cuda/cuda/lib/libcufft.7.5.dylib\nlrwxr-xr-x  1 production204  wheel  126 Aug 31 11:03 libcurand.7.5.dylib -> /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/external/local_config_cuda/cuda/lib/libcurand.7.5.dylib\n\nlib$ \n```\n", "Whoa, I'm running into this, too, but on master with OS X 10.11.6.\n\n./configure:\n\n```\n/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: 3.0\n```\n\nHere's my file listings. All of the symlinks work and the files are all Mach-O\u00a0so no weird accidental ELF or something: https://gist.github.com/jmhodges/a5de9cc5760333f5b57040d1947ec190\n\nThis was after going to sleep and coming back to this just now. Last night, I was debugging [a different error condition](https://github.com/tensorflow/tensorflow/pull/4145#issuecomment-244038488) and just came back to find my builds no longer working. I thought it was me hand-hacking in extra linkopts (`-L/usr/local/cuda/lib`, specifically) into various BUILD files trying to get the dyld error fixed.\n", "I can confirm this also building on a Debian (sid, uptodate of today) system.\n", "I've found I can induce this by Ctrl-C'ing in the middle of a fresh `bazel build`.\n", "Ubuntu-16.04, CUDA 8, java 1.8.0_101, bazel 0.3.1\n\nBuilding from master today\n\nStarted ./configure in a virtual instance in VirtualBox, did a CTRL-C because it was taking too long. Went home, fired up the instance again, deleted tensorflow repo, cloned it again.\n\nDid ./configure again with same options as before, it worked well except one warning at the beginning:\n\n```\nFound stale PID file (pid=20777). Server probably died abruptly, continuing...\n```\n\nIgnored it, and did the command to build for GPU:\n\n```\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\n\nAnd it failed immediately:\n\n```\nINFO: Waiting for response from Bazel server (pid 9635)...\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nINFO: Elapsed time: 4.667s\n```\n\nEDIT:\n\nTried `bazel clean` and try again. `bazel clean --expunge` and try again ./configure and `bazel build`. Nothing helps. Fails the same way always. :(\n", "There are two issues being discussed in this thread. @trevorwelch, let's move the `Library not loaded: @rpath/libcudart.7.5.dylib` discussion over to #4187. \n\nFor those experiencing the `'@local_config_cuda//crosstool': BUILD file not found` issue:\n- If the Bazel output directories (i.e. `bazel-tensorflow`, etc.) exist, what is the output of `ls -l bazel-tensorflow/external/local_config_cuda/crosstool`?\n- If not, what is the output of `ls -l $(bazel info output_base)/external/local_config_cuda/crosstool`?\n\nIn the meantime, I am still trying to reproduce this.\n", "i experience the same issue. tf 0.10. mac os el capitain. bazel 0.3.0\n", "@asimonov - Can you print the contents of the `local_config_cuda/crosstool` directory as I mentioned in my comment above?\n", "```\nroot@machine-learning:/vagrant/packages/tensorflow# ls *bazel*\nls: cannot access '*bazel*': No such file or directory\nroot@machine-learning:/vagrant/packages/tensorflow# ls -l $(bazel info output_base)/external/local_config_cuda/crosstool\nls: cannot access '/root/.cache/bazel/_bazel_root/b0bb79a433b74dfa52314ef9af1d2ddd/external/local_config_cuda/crosstool': No such file or directory\n```\n\n[contents of bazel cache after BUILD file not found](https://gist.github.com/FlorinAndrei/c706fa420d118ecdd29c3016ae13cea1)\n\nHere's how to reproduce it:\n\nClone this repo: https://github.com/FlorinAndrei/ml-setup\n\nCheckout the ubuntu1604 branch, then launch the virtual machine and run the ansible installer, then compile TF by hand:\n\n```\ngit clone https://github.com/FlorinAndrei/ml-setup\ncd ml-setup\ngit checkout ubuntu1604\nvagrant up\nvagrant ssh\n\nsudo su -\ncp /vagrant/bash_profile_example /root/.bash_profile\nexit\nsudo su -\n\ncd /vagrant\n# this will take a long time\nansible-playbook -i inventory main.yml\nexit\nsudo su -\n\ncd /vagrant/packages/tensorflow\n./configure\n\n# Hit ENTER on every question except:\n# Do you wish to build TensorFlow with GPU support? (answer: y)\n# Please specify a list of comma-separated Cuda compute capabilities you want to build with. (answer: 6.1)\n\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\n\nHowever, if you delete the tensorflow repo, re-clone and try again, it starts compiling:\n\n```\ncd\nrm -rf /vagrant/packages/tensorflow\ncd /vagrant\nansible-playbook -i inventory 40-tensorflow.yml\n\ncd /vagrant/packages/tensorflow\n./configure\n\n# Hit ENTER on every question except:\n# Do you wish to build TensorFlow with GPU support? (answer: y)\n# Please specify a list of comma-separated Cuda compute capabilities you want to build with. (answer: 6.1)\n```\n\n[contents of bazel cache after ./configure](https://gist.github.com/FlorinAndrei/38b886ae3aa46760ca8228cc889971ca)\n\n```\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\n\nAnd now it starts compiling.\n\nEDIT: Even on second try, it still fails to compile all the way to the end, but that seems like a different issue, which I've opened here:\n\nhttps://github.com/tensorflow/tensorflow/issues/4190\n", "David,\n\nI cannot find local_config_cuda/crosstool directory anywhere in tensorflow directory.\n\nKind Regards,\nAlexey\n\n> On 3 Sep 2016, at 20:59, David Z. Chen notifications@github.com wrote:\n> \n> @asimonov https://github.com/asimonov - Can you print the contents of the local_config_cuda/crosstool directory as I mentioned in my comment above?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub https://github.com/tensorflow/tensorflow/issues/4105#issuecomment-244566755, or mute the thread https://github.com/notifications/unsubscribe-auth/AAOr9jkI6txztNytQvsTCtcTUtlP5lrgks5qmdGbgaJpZM4Jw5M2.\n", "@davidzchen At `bazel-tensorflow/external/local_config_cuda/crosstool`, I'm getting a broken symlink to\n\n```\n~/.cache/bazel/_bazel_user/d217f35631206796f447d50c6f1d6243/external/local_config_cuda/crosstool\n```\n\nMaybe worth noting that there does exist a `cuda` directory at\n\n```\n~/.cache/bazel/_bazel_/d217f35631206796f447d50c6f1d6243/external/local_config_cuda/cuda\n```\n\nAnd so the symlink to it at `bazel-tensorflow/external/local_config_cuda/cuda` is valid.\n", "@asimonov @rasmi - Does the contents of your `local_config_cuda/cuda` directory look similar to that of the [directory listing](https://gist.github.com/FlorinAndrei/c706fa420d118ecdd29c3016ae13cea1) in @FlorinAndrei's gist?\n", "@davidzchen -- assuming you mean @jmhodges gist, yes. Just a bunch of cuda library files. Sure enough, running `./configure` and building again produces the crosstool directory with `BUILD`, `CROSSTOOL`, and a `clang/bin/crosstool_wrapper_driver_is_not_gcc` files. I was getting errors with `crosstool_wrapper_driver_is_not_gcc` [earlier](https://github.com/tensorflow/tensorflow/issues/190#issuecomment-244636243) when using gcc 4.9.1, and now I'm getting [entirely unrelated errors](https://github.com/google/highwayhash/issues/23) in `highwayhash` when using gcc 4.7.2. Not sure if that has anything to do with it or if that's helpful, since this error seems further upstream from these compilation errors.\n", "@jmhodges \n@rasmi \n@davidzchen \n\nhere my solution :)\n\nhttps://github.com/JimmyKon/tensorflow_build_issue_fix\n", "On the error of  `'@local_config_cuda//crosstool': BUILD file not found`:\n\n```\n$ bazel info output_base\nWarning: ignoring LD_PRELOAD in environment.\n.\n/home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63\n$ ls -l $(bazel info output_base)/external/local_config_cuda/crosstool\nWarning: ignoring LD_PRELOAD in environment.\ntotal 20\n-rwxrwxr-x. 1 david david  925 Sep  1 13:38 BUILD\ndrwxrwxr-x. 3 david david 4096 Sep  1 13:38 clang\n-rwxrwxr-x. 1 david david 8870 Sep  1 13:38 CROSSTOOL\n```\n\nNote that this only happens when I set a non default GCC, but since it is incompatible with CUDA I can only build for CPU.\n", "same here, when `ls -l $(bazel info output_base)/external/local_config_cuda`, there is noly `cuda`directory, no `crosstool` dir.\n", "i solved this by careful setting the cuda/cudnn/gcc version when using ./configure\n", "This is what I have:\n\nAlexPro:tensorflow alex$ bazel info output_base\n/private/var/tmp/_bazel_alex/9a8b7d02e7f6ce832d52efe09806ba70\n\nAlexPro:tensorflow alex$ ls -la /private/var/tmp/_bazel_alex/9a8b7d02e7f6ce832d52efe09806ba70/external/local_config_cuda\ntotal 8\ndrwxr-xr-x    4 alex  wheel   136  6 Sep 07:10 .\ndrwxr-xr-x  168 alex  wheel  5712  6 Sep 07:10 ..\n-rw-r--r--    1 alex  wheel   116  6 Sep 07:10 WORKSPACE\ndrwxr-xr-x    9 alex  wheel   306  6 Sep 07:10 cuda\n\n> On 6 Sep 2016, at 02:51, Wei Wu notifications@github.com wrote:\n> \n> same here, when ls -l $(bazel info output_base)/external/local_config_cuda, there is noly cuda directory, no crosstool.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub https://github.com/tensorflow/tensorflow/issues/4105#issuecomment-244831674, or mute the thread https://github.com/notifications/unsubscribe-auth/AAOr9tiufEt_WZPBpsHOl0ZZjhBgyzE2ks5qnMcogaJpZM4Jw5M2.\n", "@tornadomeet can you show exactly what you used in configure?\n", "Odd. It appears that even though you are running the `./configure` script to build with GPU support, `cuda_configure` seems to think that GPU support is disabled.\n\nThe way that `cuda_configure` determines whether GPU support is enabled is whether `TF_NEED_CUDA` is set to `\"1\"`, which is what the `./configure` script sets if you answered `y` to whether to build with GPU support.\n\nIf you are consistently reproducing the `'@local_config_cuda//crosstool': BUILD file not found` error, can you apply [this patch](https://gist.github.com/davidzchen/165e6ec139fcd9245b5721c8bf2fdd53), then run your `./configure` script and paste the debug (warning) messages from `cuda_configure.bzl` in the output?\n", "I've solved the same problem by putting `bazel  build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` into the last line of `configure`.\nLooks like bazel configuration does not complete on `bazel fetch //...` in some cases.\n`source configure` before running `bazer build ...` might work too.\n", "@nsuke That seems to indicate that Bazel is re-fetching if you run `bazel build` separately from the `configure` script, which seems to be a bug. To help us better understand what is going on, can you apply the [patch](https://gist.github.com/davidzchen/165e6ec139fcd9245b5721c8bf2fdd53) that I linked in my comment above, reproduce the `'@local_config_cuda//crosstool': BUILD file not found` error and paste the output that you get? In particular, I am interested in:\n1. What the debug (`WARNING`) messages say when the `bazel fetch` command is running, which will tell us whether `cuda_configure` thinks GPU support is enabled and what the value of `TF_NEED_CUDA` is.\n2. Whether the same debug messages are also being printed when you run `bazel build`, which will tell us whether Bazel is re-fetching during build.\n\n@damienmg Are there any flags that can be used to enable some additional logging to understand what `fetch` is doing in this case?\n", "@Dapid i do not saved the configure info, can we get this anywhere(except the info output to the screen during configure)?\n", "Anyone solved the issue? please help\nOS: Ubuntu 16.04, Cuda toolkit 8.0, cudnn-8.0-linux-x64-v5.1, GPU nvidia 1070\n\nother settings:\nCUDA_HOME=/usr/local/cuda-8.0\nLD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64\nPATH=/usr/local/cuda-8.0/bin:/home/gopi/bin:/home/gopi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n\nMy command window shown below .\n\ngopi@gp:/home/tensorflow$ sudo ./configure \nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python2.7\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\nNo Google Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /usr/local/lib/python2.7/dist-packages\n  /usr/lib/python2.7/dist-packages\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\n/usr/local/lib/python2.7/dist-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc-5\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda-8.0\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1.5\nPlease specify the location where cuDNN 5.1.5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda-8.0]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n\n.\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.\nINFO: All external dependencies fetched successfully.\nConfiguration finished\ngopi@gp:/home/tensorflow$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\n", "@gopi77 \n\nI met this error once after upgrade my gcc on my mac.\nyou can reinstall bazel and execute \"bazel clean\" before compile.\n", "@gopi77 - Is there a reason why you are running `./configure` as sudo? Can you try running it without sudo?\n\n@Dapid @nsuke - Can one of you apply the patch I linked in [my comment above](https://github.com/tensorflow/tensorflow/issues/4105#issuecomment-244868298), reproduce the failure, and paste the output that you get as described so that we can get some more state on why the error is happening?\n", "Hi JimmyKon \nI reinstalled bazel, even got fresh clone of tensorflow in different area. But the errors remain\ngopi@gp:~/tensorflow$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n.\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\n\nAnyone succeeded with this combination ?\nOS: Ubuntu 16.04, Cuda toolkit 8.0, cudnn-8.0-linux-x64-v5.1, GPU nvidia 1070, nvidia driver 370.23.\n\nRepeated installation of ubuntu, toolkit, nvidia drivers, tensorflow many times in the last 3 weeks. still no luck with GPU based run.\nBut cuda toolkit examples are working fine Ex: bilateral filter and also CPU only version of tensorflow binary working fine.\n\nDavidzchen, \nI got python error and it disappeared using sudo ./configure.\n\nNote: I mentioned compute capability as 6.1 (not visible in previous mail) for my nvidia 1070 card \n\nRegards\nGopi. J\n", "the error of ./configure without sudo shown below >>>\n\ngopi@gp:~/tensorflow$ ./configure\nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\nNo Google Cloud Platform support will be enabled for TensorFlow\n./configure: line 66: tensorflow/tools/swig/swig_path: Permission denied\nFound possible Python library paths:\n  /usr/local/lib/python2.7/dist-packages\n  /usr/lib/python2.7/dist-packages\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\n\n/usr/local/lib/python2.7/dist-packages\n./util/python/python_config.sh: line 154: tools/bazel.rc: Permission denied\n", "@davidzchen in my case, it turned out that it works fine as long as I don't modify files under `third_party/gpus` between configure and build.\n(It was due to #4233 if that matters)\n\nHere's failure log with your patch:\n\n```\n$ ./configure\n...\nWARNING: /home/nsuke/tensorflow/third_party/gpus/cuda_configure.bzl:119:5: TF_NEED_CUDA: 1.\nWARNING: /home/nsuke/tensorflow/third_party/gpus/cuda_configure.bzl:360:3: cuda_configure: _create_cuda_repository.\n...\n$ touch third_party/gpus/cuda_configure.bzl\n$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nWARNING: /home/nsuke/tensorflow/third_party/gpus/cuda_configure.bzl:122:5: TF_NEED_CUDA not set.\nWARNING: /home/nsuke/tensorflow/third_party/gpus/cuda_configure.bzl:304:3: cuda_configure: _create_dummy_repository.\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nINFO: Elapsed time: 0.553s\n```\n", "@gopi77 Seems that you have some permissions issue in the directory where you have cloned the tensorflow repo. Can you fix your permission issues and try again?\n\n@nsuke Thanks, the debug messages and your explanation are very helpful. Yes, if you modify files under `third_party/gpus`, then Bazel will re-fetch. Since the environment variables read by `cuda_configure` (such as `TF_NEED_CUDA`, etc.) are no longer set (as they were set by the `configure` script), then `cuda_configure()` will run with all default values, in which case, it will configure with GPU support disabled. Is #4233 the reason why you were modifying files under `third_party/gpus` between running `./configure` and `bazel build`?\n", "I have just pulled the latest version of tensorflow, re-ran ./config and build now works. Not sure if you have applied any fixes...\n\n> On 6 Sep 2016, at 23:07, David Z. Chen notifications@github.com wrote:\n> \n> @gopi77 https://github.com/gopi77 Seems that you have some permissions issue in the directory where you have cloned the tensorflow repo. Can you fix your permission issues and try again?\n> \n> @nsuke https://github.com/nsuke Thanks, the debug messages and your explanation are very helpful. Yes, if you modify files under third_party/gpus, then Bazel will re-fetch. Since the environment variables read by cuda_configure (such as TF_NEED_CUDA, etc.) are no longer set (as they were set by the configure script), then cuda_configure() will run with all default values, in which case, it will configure with GPU support disabled. Is #4233 https://github.com/tensorflow/tensorflow/pull/4233 the reason why you were modifying files under third_party/gpus between running ./configure and bazel build?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub https://github.com/tensorflow/tensorflow/issues/4105#issuecomment-245110873, or mute the thread https://github.com/notifications/unsubscribe-auth/AAOr9oiejZAFujyPC-q9KHbbiqlmUvzOks5qneQTgaJpZM4Jw5M2.\n", "@asimonov There have not been any fixes that went in for `cuda_configure` since 4c49dbebef05442c7e72d6129a30574fcd13f0e1, which went in 10 days ago and should be related to this issue.\n\nAll: If you are experiencing the `'@local_config_cuda//crosstool': BUILD file not found` issue, the most likely cause of the problem is that `cuda_configure` is being re-run during `bazel build` and is re-configuring with GPU support disabled.\n1. **Are you modifying any files under `third_party/gpus` between running `./configure` and `bazel build`?** If so, then the reason why you are experiencing the error is because Bazel is re-fetching as explained [here](https://github.com/tensorflow/tensorflow/issues/4105#issuecomment-245110873). Please explain why you need to do so, and we'll see if we can come up with a solution to avoid that.\n2. **If not, please follow the instructions [here](https://github.com/tensorflow/tensorflow/issues/4105#issuecomment-244868298)** so that we can have more state in order to debug why the re-fetch is happening.\n", "interesting\u2026\n\nI just did a pull, re-ran configure then checked that crosstool is there:\n\nAlexPro:tensorflow alex$ ls -l $(bazel info output_base)/external/local_config_cuda\ntotal 8\n-rw-r--r--   1 alex  wheel  116  7 Sep 00:05 WORKSPACE\ndrwxr-xr-x   5 alex  wheel  170  7 Sep 00:05 crosstool\ndrwxr-xr-x  11 alex  wheel  374  7 Sep 00:05 cuda\n\nthen started the build\n\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n\nI went through but failed later with 3 errors. Did not have time to check why. But at least the crosstool issue was gone.\n\nInterestingly when i started build second time it failed with same crosstool issue.\n\n> On 7 Sep 2016, at 02:20, David Z. Chen notifications@github.com wrote:\n> \n> @asimonov https://github.com/asimonov There have not been any fixes that went in for cuda_configure since 4c49dbe https://github.com/tensorflow/tensorflow/commit/4c49dbebef05442c7e72d6129a30574fcd13f0e1, which went in 10 days ago and should be related to this issue.\n> \n> All: If you are experiencing the '@local_config_cuda//crosstool': BUILD file not found issue, the most likely cause of the problem is that cuda_configure is being re-run during bazel build and is re-configuring with GPU support disabled.\n> \n> Are you modifying any files under third_party/gpus between running ./configure and bazel build? If so, then the reason why you are experiencing the error is because Bazel is re-fetching as explained here https://github.com/tensorflow/tensorflow/issues/4105#issuecomment-245110873. Please explain why you need to do so, and we'll see if we can come up with a solution to avoid that.\n> If not, please follow the instructions here https://github.com/tensorflow/tensorflow/issues/4105#issuecomment-244868298 so that I can have more state on why the error is happening.\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub https://github.com/tensorflow/tensorflow/issues/4105#issuecomment-245146614, or mute the thread https://github.com/notifications/unsubscribe-auth/AAOr9sr0Pn6-eGfqbw6p_9DdyW0AydGyks5qnhF0gaJpZM4Jw5M2.\n", "ah, just found the build error:\n\nERROR: /Users/alex/dev/tensorflow/tensorflow/stream_executor/BUILD:5:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign ... (remaining 112 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n\nso it does look like the same issue. but this time it was going further (first try). before i would be failing straight away.\n\n> On 7 Sep 2016, at 07:56, Alexey Simonov alexey.simonov@gmail.com wrote:\n> \n> interesting\u2026\n> \n> I just did a pull, re-ran configure then checked that crosstool is there:\n> \n> AlexPro:tensorflow alex$ ls -l $(bazel info output_base)/external/local_config_cuda\n> total 8\n> -rw-r--r--   1 alex  wheel  116  7 Sep 00:05 WORKSPACE\n> drwxr-xr-x   5 alex  wheel  170  7 Sep 00:05 crosstool\n> drwxr-xr-x  11 alex  wheel  374  7 Sep 00:05 cuda\n> \n> then started the build\n> \n> bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n> \n> I went through but failed later with 3 errors. Did not have time to check why. But at least the crosstool issue was gone.\n> \n> Interestingly when i started build second time it failed with same crosstool issue.\n> \n> > On 7 Sep 2016, at 02:20, David Z. Chen <notifications@github.com <mailto:notifications@github.com>> wrote:\n> > \n> > @asimonov https://github.com/asimonov There have not been any fixes that went in for cuda_configure since 4c49dbe https://github.com/tensorflow/tensorflow/commit/4c49dbebef05442c7e72d6129a30574fcd13f0e1, which went in 10 days ago and should be related to this issue.\n> > \n> > All: If you are experiencing the '@local_config_cuda//crosstool': BUILD file not found issue, the most likely cause of the problem is that cuda_configure is being re-run during bazel build and is re-configuring with GPU support disabled.\n> > \n> > Are you modifying any files under third_party/gpus between running ./configure and bazel build? If so, then the reason why you are experiencing the error is because Bazel is re-fetching as explained here https://github.com/tensorflow/tensorflow/issues/4105#issuecomment-245110873. Please explain why you need to do so, and we'll see if we can come up with a solution to avoid that.\n> > If not, please follow the instructions here https://github.com/tensorflow/tensorflow/issues/4105#issuecomment-244868298 so that I can have more state on why the error is happening.\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub https://github.com/tensorflow/tensorflow/issues/4105#issuecomment-245146614, or mute the thread https://github.com/notifications/unsubscribe-auth/AAOr9sr0Pn6-eGfqbw6p_9DdyW0AydGyks5qnhF0gaJpZM4Jw5M2.\n", "@davidzchen I updated the repo (branch r0.10), applied the patch, and here are the results:\n\n```\n[david@SQUIDS tensorflow]$ ./configure \nPlease specify the location of python. [Default is /home/david/.virtualenvs/py35/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/local/cuda/bin/gcc\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \nlibcudnn.so resolves to libcudnn.5\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: 5.0\nWarning: ignoring LD_PRELOAD in environment.\n.\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\nWarning: ignoring LD_PRELOAD in environment.\n.\nWARNING: /home/david/gits/tensorflow/third_party/gpus/cuda_configure.bzl:95:5: TF_NEED_CUDA: 1.\nWARNING: /home/david/gits/tensorflow/third_party/gpus/cuda_configure.bzl:336:3: cuda_configure: _create_cuda_repository.\nWARNING: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/boringssl_git/WORKSPACE:1: Workspace name in /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/boringssl_git/WORKSPACE (@boringssl) does not match the name given in the repository's definition (@boringssl_git); this will cause a build error in future versions.\nINFO: All external dependencies fetched successfully.\nConfiguration finished\n[david@SQUIDS tensorflow]$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nWarning: ignoring LD_PRELOAD in environment.\nWARNING: /home/david/gits/tensorflow/third_party/gpus/cuda_configure.bzl:95:5: TF_NEED_CUDA: 1.\nWARNING: /home/david/gits/tensorflow/third_party/gpus/cuda_configure.bzl:336:3: cuda_configure: _create_cuda_repository.\nWARNING: /home/david/gits/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\nWARNING: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/bit_depth.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/gemmlowp.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/map.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/output_stages.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/profiling/instrumentation.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/profiling/profiler.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nINFO: Found 1 target...\nERROR: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/zlib_archive/BUILD:7:1: C++ compilation of rule '@zlib_archive//:zlib' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 35 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nIn file included from external/zlib_archive/zlib-1.2.8/zconf.h:395:0,\n                 from external/zlib_archive/zlib-1.2.8/zlib.h:34,\n                 from external/zlib_archive/zlib-1.2.8/zutil.h:22,\n                 from external/zlib_archive/zlib-1.2.8/deflate.h:16,\n                 from external/zlib_archive/zlib-1.2.8/deflate.c:52:\n/usr/include/limits.h:123:26: error: no include path in which to search for limits.h\n # include_next <limits.h>\n                          ^\nIn file included from external/zlib_archive/zlib-1.2.8/zconf.h:421:0,\n                 from external/zlib_archive/zlib-1.2.8/zlib.h:34,\n                 from external/zlib_archive/zlib-1.2.8/zutil.h:22,\n                 from external/zlib_archive/zlib-1.2.8/deflate.h:16,\n                 from external/zlib_archive/zlib-1.2.8/deflate.c:52:\n/usr/include/sys/types.h:146:20: fatal error: stddef.h: No such file or directory\n #include <stddef.h>\n                    ^\ncompilation terminated.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 2.827s, Critical Path: 0.40s\n[david@SQUIDS tensorflow]$ bazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package\nWarning: ignoring LD_PRELOAD in environment.\nWARNING: /home/david/gits/tensorflow/third_party/gpus/cuda_configure.bzl:95:5: TF_NEED_CUDA: 1.\nWARNING: /home/david/gits/tensorflow/third_party/gpus/cuda_configure.bzl:336:3: cuda_configure: _create_cuda_repository.\nWARNING: /home/david/gits/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\nWARNING: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/bit_depth.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/gemmlowp.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/map.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/output_stages.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/profiling/instrumentation.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/profiling/profiler.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nINFO: Found 1 target...\nERROR: /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/external/re2/BUILD:9:1: C++ compilation of rule '@re2//:re2' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /home/david/.cache/bazel/_bazel_david/47d00ffdd2fc0515138a34f138cebd63/execroot/tensorflow && \\\n  exec env - \\\n    LD_LIBRARY_PATH=/usr/lib64/openmpi/lib:/home/david/.local/modeller9.14/lib/x86_64-intel8:/opt/cudnn/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64 \\\n    PATH=/usr/lib64/openmpi/bin:/home/david/.local/bin:/home/david/.local/hmmer3.1/bin:/home/david/.virtualenvs/py35/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/data/rosetta_weekly/release_15_19/main/source/bin:/usr/local/cuda/bin:/home/david/.local/bin:/home/david/bin \\\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/local_linux-py3-opt/bin/external/re2/_objs/re2/external/re2/util/strutil.pic.d '-frandom-seed=bazel-out/local_linux-py3-opt/bin/external/re2/_objs/re2/external/re2/util/strutil.pic.o' -fPIC -iquote external/re2 -iquote bazel-out/local_linux-py3-opt/genfiles/external/re2 -iquote external/bazel_tools -iquote bazel-out/local_linux-py3-opt/genfiles/external/bazel_tools -isystem external/re2 -isystem bazel-out/local_linux-py3-opt/genfiles/external/re2 -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c external/re2/util/strutil.cc -o bazel-out/local_linux-py3-opt/bin/external/re2/_objs/re2/external/re2/util/strutil.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nIn file included from external/re2/util/util.h:9:0,\n                 from external/re2/util/strutil.cc:5:\n/usr/include/stdio.h:33:21: fatal error: stddef.h: No such file or directory\n # include <stddef.h>\n                     ^\ncompilation terminated.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 0.834s, Critical Path: 0.36s\n\n```\n", "Hi\n\nI tried cloning the latest code and followed the steps as pe https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#installing-from-sources\n\nNow the ./configure & bazel build are success, but some other failure, pasted below.\n\ngopi@gp:~/tensorflow$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n....\n....\n....\nTarget //tensorflow/cc:tutorials_example_trainer up-to-date:\n  bazel-bin/tensorflow/cc/tutorials_example_trainer\nINFO: Elapsed time: 1142.181s, Critical Path: 1109.17s\n\ngopi@gp:~/tensorflow$ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5.1.5 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: GeForce GTX 1070\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\npciBusID 0000:01:00.0\nTotal memory: 7.92GiB\nFree memory: 7.43GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\nFloating point exception (core dumped)\n", "Not entirely sure if it is the same issue, but I'm also receiving a BUILD file not found on package path error.\n\n`~$ bazel test //home/Documents/magenta-master/magenta\nERROR: no such package 'home/Documents/magenta-master/magenta': BUILD file not found on package path.`\n\nI\u1e3f using Bazel 0.3.1 on Ubuntu 16.04 \n", "`ERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.`\n\nafter restart system, even with the recomendations above\n\n\"bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\" is taking too long to execute, i've closed after 1 hour\nOS: Ubuntu 16.04, Cuda toolkit 8.0, cudnn-8.0-linux-x64-v5.1, GPU nvidia 1070, Bazel 3.1\n", "It got worse. Before, if I deleted/recloned the repo and tried again, it would compile. Now it always fails.\n\n```\nroot@machine-learning:/vagrant/packages/tensorflow# bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nINFO: Waiting for response from Bazel server (pid 17980)...\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nINFO: Elapsed time: 8.671s\n```\n\n```\n# git rev-parse HEAD\nwarning: refname 'HEAD' is ambiguous.\n1cb04a68d24187d4d02689e2bc8aba9b7abee154\n```\n\n```\nroot@machine-learning:/vagrant/packages/tensorflow# ls *bazel*\nls: cannot access '*bazel*': No such file or directory\nroot@machine-learning:/vagrant/packages/tensorflow# ls -l $(bazel info output_base)/external/local_config_cuda/crosstool\nls: cannot access '/root/.cache/bazel/_bazel_root/b0bb79a433b74dfa52314ef9af1d2ddd/external/local_config_cuda/crosstool': No such file or directory\nroot@machine-learning:/vagrant/packages/tensorflow# tree $(bazel info output_base)/external/local_config_cuda          \n/root/.cache/bazel/_bazel_root/b0bb79a433b74dfa52314ef9af1d2ddd/external/local_config_cuda\n|-- cuda\n|   |-- BUILD\n|   |-- build_defs.bzl\n|   |-- cuda_config.h\n|   |-- extras\n|   |   `-- CUPTI\n|   |       |-- include\n|   |       |   `-- cupti.h\n|   |       `-- lib64\n|   |           `-- libcupti.so\n|   |-- include\n|   |   |-- cublas.h\n|   |   |-- cuda.h\n|   |   `-- cudnn.h\n|   |-- lib64\n|   |   |-- libcublas.so\n|   |   |-- libcudart.so\n|   |   |-- libcudart_static.a\n|   |   |-- libcudnn.so\n|   |   |-- libcufft.so\n|   |   `-- libcurand.so\n|   `-- platform.bzl\n`-- WORKSPACE\n```\n", "@davidzchen \n\n> Are you modifying any files under third_party/gpus between running ./configure and bazel build?\n\nYes, that makes sense. I'm modifying `bazel-tensorflow/external/protobuf/BUILD` to fix [this issue here](https://github.com/tensorflow/tensorflow/pull/3097#issuecomment-244814825) and `third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl` to fix [this issue here](https://github.com/tensorflow/tensorflow/issues/190#issuecomment-244636243). I got it to compile after re-running `./configure` and `bazel build`.\n", "All the compilation failures I've seen before only happen on the master branch. None of these issues persists when I switch to the r0.10 branch. Last night I was able to complete a build, Ubuntu 16.04, CUDA 8RC, cuDNN 5.1, nvidia-driver-370, python-2.7, and compute capability 6.1 (for Pascal GPU) - as soon as I switched to r0.10.\n", "Hi,\n\nI am getting a similar error when running ./configure. \"ERROR: The specified --crosstool_top '@local_config_cuda//crosstool:CROSSTOOL' is not a valid cc_toolchain_suite rule.\"\nMy configuration : \nexport TF_NEED_GCP=0\nexport TF_NEED_CUDA=1\nexport TF_CUDA_VERSION=7.5\nexport TF_CUDNN_VERSION=5\n\nIs this related to this issue ? I switched to r0.10 branch but still able to reproduce the issue.\n", "@anirudh2290 which version of bazel are you using ?\nI've got the same error with this commit and after: bazelbuild/bazel@0d32fc88d6d179bedef4a04bc22c44583365b859\n0.3.1 should be fine.\n", "@nsuke thank you. that was the problem. reverting to a previous commit worked. \n", "@anirudh2290 yes this is a recent change in Bazel. @davidzchen we should not longer use a filegroup to refer to crosstool but use cc_toolchain_suite\n", "@damienmg Understood. I have added those changes to #4285\n", "Got this problem ERROR: no such package '@local_config_cuda//crosstool':  again today.\nAfter various attempts the below steps worked.\n1.  sudo apt-get upgrade bazel\n2. ./configure (i didn't repeat this step >> $ git clone https://github.com/tensorflow/tensorflow)\n3. # To build with GPU support:\n   \n   bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n4. mkdir _python_build\n   cd _python_build\n   ln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/\\* .\n   ln -s ../tensorflow/tools/pip_package/\\* .\n   python setup.py develop\n5. cd tensorflow/models/image/mnist\n   python convolutional.py\n", "meet the same issue.\n", "Hi. I'm getting \"ERROR: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\" as well with:\n- bazel: https://github.com/bazelbuild/bazel/releases/download/0.3.1/bazel-0.3.1-installer-linux-x86_64.sh\n- tensorflow: commit a6c5f8e4e013e54fed8dfcf49fb6de365f018022 (patched according to https://github.com/tensorflow/tensorflow/issues/4312#issuecomment-246109583 to avoid the ios/android errors)\n- cuda: http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1404/x86_64/cuda-repo-ubuntu1404_7.5-18_amd64.deb\n- python 3.5 as package from: sudo add-apt-repository -y ppa:fkrull/deadsnakes; sudo apt-get update; \n  sudo apt-get -y install python3.5\n- forced to use wheel>0.25.0\n\nThe issue for me happens deterministically, if I run tensorflow ./configure while trying to avoid interactive questions:\n\n # set vars to avoid interactive\nexport PYTHON_BIN_PATH=/usr/bin/python\n ## No way to confirm the following default value to ./util/python/python_config.sh without actually hitting the Return key: :-((\n ##   /usr/lib/python3/dist-packages\nexport TF_NEED_GCP=n\nexport TF_NEED_CUDA=y\nexport GCC_HOST_COMPILER_PATH=/usr/bin/gcc\nexport TF_CUDA_VERSION=7.5\nexport CUDA_TOOLKIT_PATH=$CUDA_HOME\nexport TF_CUDNN_VERSION=4\nexport CUDNN_INSTALL_PATH=$CUDA_HOME\nexport TF_CUDA_COMPUTE_CAPABILITIES=3.0\n\nIf I run ./configure without setting the above variables, ie.:\n\nubuntu@aws17:~/tensorflow$ ./configure\n~/tensorflow ~/tensorflow\nPlease specify the location of python. [Default is /usr/bin/python]:\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\nNo Google Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /usr/local/lib/python3.5/dist-packages\n  /usr/lib/python3/dist-packages\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]\n\n/usr/local/lib/python3.5/dist-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]:\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]:\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\nlibcudnn.so resolves to libcudnn.4\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n\nthen the compilation works.\n\nNote that there is probably no way to pass blank values (indicating \"use the default\" as opposed to undefined values indicating \"I have not answered yet\") for several of the variables. So ./configure in the interactive mode is getting various things blank while the less interactive ./configure has these values filled.\n", "@martinwicke  A similar error now appears during the building process:\n\n```\n/home/kamal/.cache/bazel/_bazel_kamal/f9ae4eca457b390bb2ebe780caca64e0/external/protobuf/BUILD:333:1: Linking of rule '@protobuf//:protoc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /home/kamal/.cache/bazel/_bazel_kamal/f9ae4eca457b390bb2ebe780caca64e0/execroot/tensorflow && \\\n  exec env - \\\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/external/protobuf/protoc bazel-out/host/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.o bazel-out/host/bin/external/protobuf/libprotoc_lib.a bazel-out/host/bin/external/protobuf/libprotobuf.a bazel-out/host/bin/external/protobuf/libprotobuf_lite.a -lpthread -lstdc++ -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,-S -Wl,--gc-sections): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nbazel-out/host/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.o: In function `main':\nmain.cc:(.text.startup.main+0x2ad): undefined reference to `vtable for google::protobuf::compiler::php::Generator'\nmain.cc:(.text.startup.main+0x5fc): undefined reference to `vtable for google::protobuf::compiler::php::Generator'\nmain.cc:(.text.startup.main+0x707): undefined reference to `vtable for google::protobuf::compiler::php::Generator'\ncollect2: error: ld returned 1 exit status\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n```\n\nI think this might be related to    4316aeb\n", "Hi,\n\nFacing similar issues for tensorflow build.\nI am trying Tensoflow from the source and receive build error for \na) C++ compilation of rule '@grpc//:gpr' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc \n\nb) ERROR: I/O error while writing action log: No space left on device.\n\nEnviornment:\n\nCuda 8.0\nCuDNN 5\nUbuntu 16.04\nbazel 0.31\nNvidia K80\nAzure VM N6 (56 GB Memory)\n\nTried ./configure and build several times. Configure is successful but build fails. \n\nAlso tried, bazel clean, bazel clean --explunge and ran the build with reduced number of jobs\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n\nbut the error continues..\n\nLooked at this thread and also \n\nhttps://github.com/tensorflow/tensorflow/issues/190\n\nHere is the full error message:\n\n52adb8ea4f53b1b72067611e8a7eb020/external/grpc/BUILD:69:1: C++ compilation of rule '@grpc//:gpr' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 38 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nERROR: I/O error while writing action log: No space left on device.\njava.util.logging.ErrorManager: 2\njava.io.IOException: No space left on device\n        at java.io.FileOutputStream.writeBytes(Native Method)\n        at java.io.FileOutputStream.write(FileOutputStream.java:326)\n        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n        at java.util.logging.FileHandler$MeteredStream.flush(FileHandler.java:196)\n        at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:297)\n        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)\n        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)\n        at java.util.logging.StreamHandler.flush(StreamHandler.java:259)\n        at java.util.logging.FileHandler.publish(FileHandler.java:683)\n        at java.util.logging.Logger.log(Logger.java:738)\n        at java.util.logging.Logger.doLog(Logger.java:765)\n        at java.util.logging.Logger.log(Logger.java:788)\n        at java.util.logging.Logger.info(Logger.java:1489)\n        at com.google.devtools.build.lib.profiler.AutoProfiler$LoggingElapsedTimeReceiver.accept(AutoProfiler.java:315)\n        at com.google.devtools.build.lib.profiler.AutoProfiler$SequencedElapsedTimeReceiver.accept(AutoProfiler.java:262)\n        at com.google.devtools.build.lib.profiler.AutoProfiler.completeAndGetElapsedTimeNanos(AutoProfiler.java:226)\n        at com.google.devtools.build.lib.buildtool.ExecutionTool.saveCaches(ExecutionTool.java:725)\n        at com.google.devtools.build.lib.buildtool.ExecutionTool.executeBuild(ExecutionTool.java:470)\n        at com.google.devtools.build.lib.buildtool.BuildTool.buildTargets(BuildTool.java:201)\n        at com.google.devtools.build.lib.buildtool.BuildTool.processRequest(BuildTool.java:333)\n        at com.google.devtools.build.lib.runtime.commands.BuildCommand.exec(BuildCommand.java:69)\n        at com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.execExclusively(BlazeCommandDispatcher.java:488)\n        at com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.exec(BlazeCommandDispatcher.java:324)\n        at com.google.devtools.build.lib.runtime.CommandExecutor.exec(CommandExecutor.java:49)\n        at com.google.devtools.build.lib.server.RPCService.executeRequest(RPCService.java:70)\n", "@sskgit - This looks like b) no space left on device is the reason for a) compilation of a file fails. You'll notice the `bazel-*` symlinks in the tensorflow directory point to `$HOME/.cache/bazel/...`, so check you've got enough space on the partition you have `$HOME` mounted to. You'll need about 10GB, possibly more.\n", "@darrengarvey Thanks for your response.\n\nI tried sudo df and here is the output\n\ndf\nFilesystem     1K-blocks     Used Available Use% Mounted on\nudev            28837212        0  28837212   0% /dev\ntmpfs            5771196     9244   5761952   1% /run\n/dev/sda1       29711408 29329736    365288  99% /\ntmpfs           28855968        0  28855968   0% /dev/shm\ntmpfs               5120        0      5120   0% /run/lock\ntmpfs           28855968        0  28855968   0% /sys/fs/cgroup\nnone                  64        0        64   0% /etc/network/interfaces.dynamic.d\n/dev/sdb1      356513788   135524 356378264   1% /mnt\ntmpfs            5771196        0   5771196   0% /run/user/1000\n\nIt shows mounted on / has almost no space, and I think $VM/Username (i.e. $HOME) is on the same mount /, as there is no $HOME mounted on, in the output. Is this the right command?\n\nAt / (used space)\n\ndu -sch\n2.3G    .\n2.3G    total\n\nAt $HOME (used space)\n\ndu -sch\n4.4G    .\n4.4G    total\n\nSo, I am not sure what is occupying the remaining space? Total storage on this VM is 380 GB. \n\nIs there any way to get rid of bazel logs? Is it causing the space issue? If so, where? \nAlso, if it is how do I free up some space? \n\nThanks\n", "@sskgit Running `bazel clean --expunge` should remove all of the generated files for the workspace. How much free space do you have after running that command?\n\n@martinwicke Sorry for the late reply. I have been on call for a good part of the past week. That looks like a linker error in protobuf and does not seem related to this particular change. `crosstool_wrapper_driver_is_not_gcc` is a wrapper script that calls `gcc`. Is this on a newer version of protobuf?\n", "@davidzchen Thanks for your response. \n\nused bazel clean --expunge (this cleaned 500MB space) and re-configured tensorflow using ./configure\n\nRun the bazel build again..\n\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n\nERROR: /$HOME/Downloads/tensorflow/tensorflow/core/kernels/BUILD:1710:1: error while parsing .d file: /$HOME/.cache/bazel/_bazel_gpuadmin/52adb8ea4f53b1b72067611e8a7eb020/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/depth_space_ops_gpu/tensorflow/core/kernels/depthtospace_op_gpu.cu.pic.d (No such file or directory).\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\n<built-in>: fatal error: when writing output to : No space left on device\ncompilation terminated.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n\nAt $HOME:\ndu -sch\n4.6G    .\n4.6G    total\n\nAt /:\ndu -sch\n2.3G    .\n2.3G    total\n\ndf -h\nFilesystem      Size  Used Avail Use% Mounted on\nudev             28G     0   28G   0% /dev\ntmpfs           5.6G  9.1M  5.5G   1% /run\n**/dev/sda1        29G   29G  9.8M 100% /**\ntmpfs            28G     0   28G   0% /dev/shm\ntmpfs           5.0M     0  5.0M   0% /run/lock\ntmpfs            28G     0   28G   0% /sys/fs/cgroup\nnone             64K     0   64K   0% /etc/network/interfaces.dynamic.d\n/dev/sdb1       340G  133M  340G   1% /mnt\ntmpfs           5.6G     0  5.6G   0% /run/user/1000\n\nThis is a fairly new machine with few installer software (<200MB)\n", "@sskgit Interesting. Can you open a bug at https://github.com/bazelbuild/bazel for this issue? Thanks.\n", "@davidzchen Opened an issue with bazel as well. I have tried the build almost 10 times in last couple of days. Still trying to figure why the build fails and how do I complete the build successfully. \n", "Space issue caused my Tensorflow build to fail. Clearing some space on the / mount made the build successful and Tensorflow now works as expected.\n\nThanks everyone for your help!\n", "This is happening for me too. I already have TF installed and working for GPU via the runfile but I wanted to compile it for optimizations. I get:\r\n\r\n```\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"/home/user/bin/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1039\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"/home/user/bin/tensorflow/third_party/gpus/cuda_configure.bzl\", line 976, in _create_local_cuda_repository\r\n                _host_compiler_includes(repository_ctx, cc)\r\n        File \"/home/user/bin/tensorflow/third_party/gpus/cuda_configure.bzl\", line 145, in _host_compiler_includes\r\n                get_cxx_inc_directories(repository_ctx, cc)\r\n        File \"/home/user/bin/tensorflow/third_party/gpus/cuda_configure.bzl\", line 120, in get_cxx_inc_directories\r\n                set(includes_cpp)\r\ndepsets cannot contain mutable items\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//crosstool': Traceback (most recent call last):\r\n        File \"/home/user/bin/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1039\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"/home/user/bin/tensorflow/third_party/gpus/cuda_configure.bzl\", line 976, in _create_local_cuda_repository\r\n                _host_compiler_includes(repository_ctx, cc)\r\n        File \"/home/user/bin/tensorflow/third_party/gpus/cuda_configure.bzl\", line 145, in _host_compiler_includes\r\n                get_cxx_inc_directories(repository_ctx, cc)\r\n        File \"/home/user/bin/tensorflow/third_party/gpus/cuda_configure.bzl\", line 120, in get_cxx_inc_directories\r\n                set(includes_cpp)\r\ndepsets cannot contain mutable items\r\nINFO: Elapsed time: 4.869s\r\nFAILED: Build did NOT complete successfully (3 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n```\r\n\r\nSteps to reproduce:\r\n\r\n```\r\ngit clone --recurse-submodules https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\n$ ./configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nPlease specify the location of python. [Default is /home/user/.pyenv/versions/3.6.2/bin/python]: \r\nFound possible Python library paths:\r\n/home/user/.pyenv/versions/3.6.2/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is /home/user/.pyenv/versions/3.6.2/lib/python3.6/site-packages\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: y\r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]: n\r\nNo OpenCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: \r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda-8.0\r\n\"Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: \r\nPlease specify the location where cuDNN 6 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda-8.0]:/usr/lib/x86_64-linux-gnu \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\nConfiguration finished\r\n$ bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=cuda -k //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nIf you notice that I've done something wrong, please let me know. I saw someone before mention something about `LD_LIBRARY_PATH` which I have set to `/usr/local/cuda-8.0/lib64`. I figured it was fine since as I said, I already have TF installed and running from the runfile download. I would like to be able to compile from source though. ", "I\u2019m also facing the same challenge - unable to build tensor flow on a gpu server.  Below given are the details.  OS is Ubuntu 16.04LTS\r\n\r\n```\r\nuser@gpu-devbox:~/Workouts/tensorflow$ python --version\r\nPython 2.7.12\r\n```\r\n```\r\nuser@gpu-devbox:~/Workouts/tensorflow$ gcc --version\r\ngcc (Ubuntu 5.4.0-6ubuntu1-16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n```\r\nuser@gpu-devbox:~/Workouts/tensorflow$ bazel version\r\nBuild label: 0.5.3\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Jul 28 08:34:59 2017 (1501230899)\r\nBuild timestamp: 1501230899\r\nBuild timestamp as int: 1501230899\r\n```\r\n```\r\nuser@gpu-devbox:~/Workouts/tensorflow$ ./configure \r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nFound possible Python library paths:\r\n/usr/local/lib/python2.7/dist-packages\r\n/usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: \r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]: \r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]: \r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: \r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: \r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]: \r\nNo OpenCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: Y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: \r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\"Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: \r\nPlease specify the location where cuDNN 6 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1,6.1,6.1,6.1]6.1\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\nConfiguration finished\r\n```\r\n```\r\nuser@gpu-devbox:~/Workouts/tensorflow$ echo $CUDA_HOME\r\n/usr/local/cuda-8.0\r\n\r\nuser@gpu-devbox:~/Workouts/tensorflow$ echo $LD_LIBRARY_PATH\r\n/usr/local/cuda-8.0/lib64\r\n```\r\n```\r\nuser@gpu-devbox:~/Workouts/tensorflow$ bazel build --config=opt --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" ./tensorflow/tools/pip_package:build_pip_package \r\n.......\r\nERROR: no such package '@local_config_cuda//crosstool': Traceback (most recent call last):\r\n    File \"/home/u19061/Workouts/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1039\r\n        _create_local_cuda_repository(repository_ctx)\r\n    File \"/home/user/Workouts/tensorflow/third_party/gpus/cuda_configure.bzl\", line 976, in _create_local_cuda_repository\r\n        _host_compiler_includes(repository_ctx, cc)\r\n    File \"/home/user/Workouts/tensorflow/third_party/gpus/cuda_configure.bzl\", line 145, in _host_compiler_includes\r\n        get_cxx_inc_directories(repository_ctx, cc)\r\n    File \"/home/user/Workouts/tensorflow/third_party/gpus/cuda_configure.bzl\", line 120, in get_cxx_inc_directories\r\n        set(includes_cpp)\r\ndepsets cannot contain mutable items\r\nINFO: Elapsed time: 5.488s\r\nFAILED: Build did NOT complete successfully (3 packages loaded)\r\n```", "Also -when checked the cache doesn\u2019t contain crosstool - Am I missing something here?\r\n```\r\nuser@devbox:~/Workouts/tensorflow$ ls -l $(bazel info output_base)/external/local_config_cuda/crosstool\r\ntotal 4\r\n-rwxrwxr-x 1 user user 1267 Aug  1 16:32 BUILD\r\n```", "@itssujeeth #11949 fixes the issue when building tensorflow with gpu support using bazel 0.5.3."]}, {"number": 4104, "title": "Feature request: Tensor decomposition", "body": "( Related to issue https://github.com/tensorflow/tensorflow/issues/2207 )\n\nTensors can be used for a whole lot more than implementing a NN. One of the useful operations in multidimensional arrays is its decomposition into smaller components. Tucker / CP native implementations would allow the use of tensorflow in other ML contexts...\n\nrefs: \nhttps://en.wikipedia.org/wiki/Tensor_rank_decomposition\nhttps://en.wikipedia.org/wiki/Higher-order_singular_value_decomposition\n", "comments": ["@015988 Thanks for the suggestion. I agree that TensorFlow would be well suited for the class of algorithms you mention, but this is probably out of scope for core TensorFlow at this point, and should probably live in an independent repository. \n", "@015988 I've recently implemented tucker and CP decompositions in TensorFlow ([tf-decompose](https://github.com/ebigelow/tf-decompose)). Any contributions there would be welcome!", "this would be extremely useful for big data DR in molecular dynamics @rmlarsen would it be possible to reopen? hitting lots of memory issues. tensor decomposition belongs in tf.linalg"]}, {"number": 4103, "title": "Build failure while following TensorFlow from source following TF documentation", "body": "### Problem\n\nTensorFlow build  of the pip package with CUDA support using the following command, taken from the [TensorFlow documentation](https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#installing-from-sources), fails:\n\n`bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\n### Error message\n\n```\n/usr/include/string.h: In function 'void* __mempcpy_inline(void*, const void*, size_t)':\n/usr/include/string.h:652:42: error: 'memcpy' was not declared in this scope\n   return (char *) memcpy (__dest, __src, __n) + __n;\n                                          ^\nERROR: /home/marek/src/tensorflow/tensorflow/contrib/rnn/BUILD:63:1: output 'tensorflow/contrib/rnn/_objs/python/ops/_gru_ops_gpu/tensorflow/contrib/rnn/kernels/gru_ops_gpu.cu.pic.o' was not created.\nERROR: /home/marek/src/tensorflow/tensorflow/contrib/rnn/BUILD:63:1: not all outputs were created.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n```\n### Environment\n- OS: Ubuntu 16.04 (64-bit)\n- CUDA 8.0\n- GCC 4.9.3\n- Java 1.8 (r101_b13)\n- Python 2.7.12 (Anaconda 4.1.1)\n- Bazel 0.3.1\n### Related problems for CUDA-based frameworks reported on StackOverflow and Github\n\nhttps://github.com/tensorflow/tensorflow/issues/1346\nhttps://github.com/torch/torch7/issues/670\nhttps://github.com/BVLC/caffe/issues/4046\nhttps://github.com/opencv/opencv/issues/6500\n### Solutions tried (none of them worked)\n- downgrading to CUDA 7.5 and CUDNN 4\n- changing downgrading and upgrading GCC away from 4.9.3\n- changing GCC and cmake options to include `-D_FORCE_INLINES` flag (see related Github issues above)\n### Solution that worked\n\nBuild succeeds when the `-c opt` option is removed from the Bazel build command. That is, the final command should be\n\n`bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package`\n", "comments": ["I am stumped. Omitting -c opt is certainly unpleasant. We're upgrading our test servers to 16.04, so we should either hit this soon, or it's very mysterious indeed.\n", "Well, first time post on github. @mkolod @martinwicke the problem may be solved by 'sed -i \"s/nvccopts = ''/nvccopts = '-D_FORCE_INLINES '/g\" './third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl'', which replaces the empty nvccopts with a starting flag '-D_FORCE_INLINES ' from tensorflow root.\n\nI successfully compiled tensorflow with this solution.\n\nHowever, my environment differences are \nCUDA 7.5\nGCC 4.9.4\nPython 3.5.2 :: Anaconda 4.1.1 (64-bit)\n\nanother theoretical solution may be \n'bazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package --per_file_copt=//tensorflow/.*.cu.cc@-D_FORCE_INLINES', which does roughly similar thing as above. However, I haven't checked this solution.\nAnyway, the point is to add an extra flag of '-D_FORCE_INLINES' to nvcc compiler\n"]}, {"number": 4102, "title": "ImportError: cannot import name pywrap_tensorflow", "body": "I have upgraded `protobuf` as explained: `Successfully installed protobuf-3.0.0 setuptools-26.1.1`\n\nThen\n\n``` shell\nadmin@macbookproloreto:~$ python\nPython 2.7.10 (default, Oct 23 2015, 19:19:21) \n[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Library/Python/2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 52, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/Library/Python/2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n  File \"/Library/Python/2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n  File \"/Library/Python/2.7/site-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n  File \"/Library/Python/2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 22, in <module>\n    serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB2\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3')\nTypeError: __init__() got an unexpected keyword argument 'syntax'\n>>> \n```\n", "comments": ["Did you try:\n\n> Solved with\n> \n> sudo pip uninstall six\n> sudo pip install six --upgrade --target=\"/usr/lib/python2.7/dist-packages\"\n\nFrom:  https://github.com/tensorflow/tensorflow/issues/2746 \n\nI had this problem at one point, can't remember how I solved it but often times the Linux related troubleshooting with TF can help on OS X. \n", "@trevorwelch thanks I thinks that this problem is related to System Integrity Protection in El Capitan (rootless) that I have disabled (from disk utility at reboot)\n\nI have tried then again with `sudo -H`:\n\n``` shell\nadmin@macbookproloreto:~/Developmemt/ParisiLabs/ML/models$ sudo -H pip uninstall six\nDEPRECATION: Uninstalling a distutils installed project (six) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\nUninstalling six-1.4.1:\n  /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six-1.4.1-py2.7.egg-info\nProceed (y/n)? y\n  Successfully uninstalled six-1.4.1\nadmin@macbookproloreto:~/Developmemt/ParisiLabs/ML/models$ sudo -H pip install six --upgrade --target=\"/usr/lib/python2.7/dist-packages\"\nCollecting six\n  Downloading six-1.10.0-py2.py3-none-any.whl\nInstalling collected packages: six\nSuccessfully installed six-1.10.0\nadmin@macbookproloreto:~/Developmemt/ParisiLabs/ML/models$ python\nPython 2.7.10 (default, Oct 23 2015, 19:19:21) \n[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Library/Python/2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 52, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/Library/Python/2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n  File \"/Library/Python/2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n  File \"/Library/Python/2.7/site-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n  File \"/Library/Python/2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 22, in <module>\n    serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB2\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3')\nTypeError: __init__() got an unexpected keyword argument 'syntax'\n>>> exit()\n```\n", "Closing as stale. Feel free to reopen if this issues is a problem today.", "Still an issue!"]}, {"number": 4101, "title": "Numerical issues with automatic matrix derivatives", "body": "While it is my impression that automatic differentiation generally enjoys numerical stability, this does not seem to extend directly to complicated expressions involving matrix calculus. \nIn particular, for objectives involving matrix inverses the error can grow disproportionately large, leading to e.g. scipy optimization routines failing to converge. \nIn my case I see TF with absolute element-wise gradient errors (comparing to the numerical Jacobian from tf.test.compute_gradient) around 10e-4 when my analytical gradient (also written using TF, only explicitly) gets errors near 10e-15. \nThe problem likely boils down to the TF graph computing the inverses naively and not exploiting e.g. Cholesky and/or triangle structure or reframing multiplications as more stable linear system solves.\n\nI'm not sure I can readily provide a MWE as the code is somewhat complicated, but it is basically Gaussian process regression with a heavily parameterized kernel function. \nThe objective itself has a form similar to what is found in the snippet below, with the callable instance `kernel` holding several tuneable parameters. \n\n```\n   #kernel computations\n    self.K = self.kernel(self.Xtrain) + self.kernel.diag(self.Xtrain) + self.kernel.noise(self.Xtrain)  +    self.kernel.jitter(Xtrain)\n    self.Ktest = self.kernel(self.Xtest) + self.kernel.noise(self.Xtest)  + self.kernel.jitter(self.Xtest)\n    self.Kx = self.kernel(self.Xtrain, self.Xtest)\n    self.L = tf.cholesky(self.K)\n\n    self.Ly = tf.matrix_triangular_solve(self.L, self.ytrain, lower = True)\n    self.LKx = tf.matrix_triangular_solve(self.L, self.Kx, lower = True)\n    self.llk = -0.5*tf.reduce_sum(self.Ly**2.) - 0.5*logdet_chol(self.L) \n    self.objective = - self.llk\n```\n\nContinued matrix calculus support was one of the primary reasons I picked TF over the alternatives, so I am really hoping this is something that can be fixed in the future.\n", "comments": ["Are your gradients computed in single or double precision? 10e-4 doesn't sound surprising if your gradient computation is in single precision. Could you perhaps boil this down to a reproducible example program and include your analytical gradient computation for comparison? If you peel off one step at a time in your objective, can you identify a step that is particularly inaccurate, or is it a gradual loss across the chain of matrix gradients?\n\nI agree with your general point that long chains of matrix operations (especially inverses and solves) can cause a loss of accuracy (especially in single precision). And since TensorFlow is not really a computer algebra system, the gradients are constructed by \"naively\" composing the individual op gradients using the chain rule without any algebraic simplification  / graph rewriting that could possibly improve accuracy.\n\nI don't think your comment about the gradient implementations is accurate: All the gradient ops in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py use triangular and general matrix solves rather than forming and multiplying by the inverse (apart from determinant where the gradient is a scalar times the inverse). The Cholesky gradient uses a blocked algorithm, but also uses proper backsolves at the core. https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/core/kernels/cholesky_grad.cc?rcl=131328798&l=108\n\nPlease have a look at those functions and let us know if you find something that you think could be improved, or even better submit a pull request with improvements to the code.\n", "@Bonnevie I assume that your logdet_chol is something like this:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/operator_pd_cholesky.py#L101\n", "@Bonnevie any more thoughts on this?\n", "@rmlarsen yes, my logdet_chol function is comparable (the linked package looks interesting). And I moved to double precision without effect.\n\nHmm, it sounds like the tensorflow gradient code takes care to keep things optimized, although I'm guessing that propagated matrices aren't checked for i.e. triangularity? \n\nHow does the automatic differentiation engine handle matrix-matrix derivatives, and submatrices/indexings of complicated matrix expressions? Does it calculate the entire matrix efficiently and then index, or does it try to calculate the submatrix independently using simpler routines? \n\nI have tried writing out the gradients mathematically in the following file, along with my working hypothesis about what's causing the problem:\n[gradients.pdf](https://github.com/tensorflow/tensorflow/files/463609/gradients.pdf)\n\nI think it's only for particular choices of ell that this is a real problem. My problem is not exactly as written, there are a few more moving parts.  \n", "@Bonnevie No, we do not do any dynamic checks for structure or sparsity in matrices, only functions like matrix_triangular_solve or cholesky_solve explicitly presuppose triangular structure.\n\nThanks for providing the analytic gradients and your analysis. I will take a look sometime in the coming week and respond here.\n", "To answer your question about the auto-diff engine: The engine itself only knows how to apply the chain rule to compose the \"gradients\" of entire ops to derive the gradient of all trainable variables in a model w.r.t. the loss:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients.py#L307\n\nThe entire gradient for matrix operations is computed in whole by hand-written gradient functions and then composed. See, e.g. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py#L33\n\nThe algebraic simplifications you do in your derivation simply cannot be done automatically in the current system, and would require manually writing a function for the gradient of log-determinant (or your entire log-likelihood function). That being said, I'm taking a second look at the linear solvers to see if perhaps adding a few steps of iterative refinement would help with the accuracy of the gradients.\n", "@Bonnevie Would you mind providing the condition numbers and norms of the special input cases that are causing issues (or perhaps even links to the matrices themselves)?\n", "Is there a way to compute the logdet of a matrix on the  GPU and be able to backprop on it?\n", "@alexmonroe87 currently most of the higher level linear algebra ops (cholesky, determinant, matrix_inverse,  matrix_solve etc.) are CPU only. There has been some interest in contributing GPU implementations based on the cuSolver library, but no recent progress. See https://github.com/tensorflow/tensorflow/issues/2217\n", "Closing due to inactivity.\n"]}, {"number": 4100, "title": "About tf.slim \uff0cAny plan to release pretrained checkpoint?", "body": "About tf.contrib. slim \uff0cAny plan to release pretrained checkpoint?\n", "comments": ["Inception v3 and many other networks are available using the new TF-Slim at\n[TF-Slim Image Models](https://github.com/tensorflow/models/tree/master/slim)\n", "Check the blog post\nhttps://research.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html\n"]}]