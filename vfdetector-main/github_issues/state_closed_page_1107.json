[{"number": 20045, "title": "Fix: DepthwiseConv2D fails when bias is enabled", "body": "Fixes #20023.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Closing because of issue with cla."]}, {"number": 20044, "title": "contrib: autograph/constrained_optimization: minor spelling tweaks", "body": "", "comments": ["Test failures are unrelated."]}, {"number": 20042, "title": "Failed to convert to Tensorflow Lite - TensorFlowMax, TensorFlowMinimum, TensorFlowSum not supported", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.4\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.14.0-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.1.0 (clang-902.0.39.2)\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=.tfmodel --output_file=/Users/plun/Downloads/mobilenet_v2.tflite --input_shape=1,224,224,3 --input_array=input_1 --output_array=reshape_2/Reshape --inference_type=FLOAT\r\n\r\nI am trying to convert a MobileNetV2 model to Tensorflow Lite. I was able to convert the model here: https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models, but when I try to convert a model trained by myself using Keras, the following error occurs:\r\n\r\n\"Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: **TensorFlowMax**, **TensorFlowMinimum**, **TensorFlowSum**.\"\r\n\r\nI ran `convert_variables_to_constants` to freeze the model, and ` bazel-bin/tensorflow/contrib/lite/toco/toco` to do conversion. I inspected the structure of the frozen graph, and found that TensorFlowMax, TensorFlowMinimum, TensorFlowSum are in relu layers and softmax layer. Are they really unsupported yet, or did I miss some steps?\r\n\r\nComplete console output:\r\n![screen shot 2018-06-14 at 4 39 54 pm](https://user-images.githubusercontent.com/24245729/41443425-9e22e676-6ff1-11e8-90eb-583c9e209698.png)\r\n", "comments": ["I found out how to avoid using these layers. \r\n\r\nKeras may use **TensorFlowMinimum** for relu6. In its source code:\r\n<img width=\"754\" alt=\"screen shot 2018-06-15 at 9 47 44 am\" src=\"https://user-images.githubusercontent.com/24245729/41481524-297a2e1a-7087-11e8-8521-eda7b4135b19.png\">\r\nKeras implements it as a combination of a normal relu and a minimum layer. To avoid this, use tf.nn.relu6 instead (also use a Keras Lambda layer to wrap it).\r\n\r\nKeras may use **TensorFlowMax** and **TensorFlowSum** for softmax. In its source code:\r\n<img width=\"870\" alt=\"screen shot 2018-06-15 at 9 52 21 am\" src=\"https://user-images.githubusercontent.com/24245729/41481604-7f9149e6-7087-11e8-8a71-4bfd0c23abc0.png\">\r\nI forgot to reshape the input tensor, and the dimension of it was greater than 2, so Keras breaks it down to several sublayers. We can avoid this by reshaping the tensor before softmax.", "got the same, in my implementation it was clipvalue operation, Ive converted it to min(relu,6), but it has to be relu6 op instead"]}, {"number": 20041, "title": "Failed to create CUPTI subcriber when profiling on servers with no GPU's", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nCustom i guess? I took the first example on the [Using GPUs](https://www.tensorflow.org/programmers_guide/using_gpu) page and added profiling to it (code below).\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nRed Hat Enterprise Linux Server release 6.7 (Santiago)\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nSource\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\n>>> import tensorflow as tf\r\n>>> print(tf.GIT_VERSION, tf.VERSION)\r\n('unknown', '1.8.0')\r\n\r\n\r\n- **Python version**:\r\n\r\nBuilt for both version of python same outcome for both\r\n2.7.14\r\n3.6.5\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\n0.11.1\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n\r\n4.9.3\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\nCUDA: 9.1.85\r\ncuDNN: 7.0.5\r\n\r\n- **GPU model and memory**:\r\n\r\nMy GPU test server has the following GPU's and driver version\r\n\r\n```bash\r\n$ nvidia-smi\r\nThu Jun 14 12:58:15 2018\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K40m          On   | 00000000:0B:00.0 Off |                    0 |\r\n| N/A   24C    P8    19W / 235W |    207MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K40m          On   | 00000000:81:00.0 Off |                    0 |\r\n| N/A   25C    P8    20W / 235W |    207MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n- **Exact command to reproduce**:\r\n\r\nRun the following script with TF built with GPU support on a server without GPU's with the cuda stub and cupti extras library path's in your LD_LIBRARY_PATH environment variable.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport os\r\ntrace_dir = '/tmp/tf_trace'\r\n\r\nbuilder = tf.profiler.ProfileOptionBuilder\r\nopts = builder(builder.time_and_memory()).order_by('micros').build()\r\n\r\nwith tf.contrib.tfprof.ProfileContext(trace_dir, trace_steps=[], dump_steps=[]) as pctx:\r\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n    c = tf.matmul(a, b)\r\n    with tf.Session() as sess:\r\n        pctx.trace_next_step()\r\n        pctx.dump_next_step()\r\n        _ = sess.run(c)\r\n        pctx.profiler.profile_operations(options=opts)\r\n\r\n```\r\n\r\nWorks perfect on servers with GPU's fails on servers without GPU's\r\n\r\n### Describe the problem\r\n\r\nI am attempting to make a single, portable version of TensorFlow 1.8.0 with XLA and GPU support. Everything works as expected on servers with GPU's.\r\nThe only thing that\u2019s not working on servers without GPU's is when you run a session in a profiler context, you get the below message.\r\n\r\nThe error seems fairly straight forward. The calls to the libcupti are failing since there are no GPU's or cuda drivers installed.\r\n\r\nHow feasible would it be to have TensorFlow fallback(or do a pre-check) to CPU only profiling if TensorFlow was built with GPU support but there are no GPU's on the servers?\r\n\r\n\r\n### Source code / logs\r\n\r\nNote: I manually truncated the paths of the log files up until the lib directory.\r\n\r\n$ python ~/tmp/tfprof.py\r\n2018-06-14 17:32:07.348377: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-06-14 17:32:07.354865: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUresult(-1)\r\n2018-06-14 17:32:07.354921: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:145] kernel driver does not appear to be running on this host (HOST_NAME): /proc/driver/nvidia/version does not exist\r\nNone\r\n2018-06-14 17:32:07.360597: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcupti.so.9.1 locally\r\n2018-06-14 17:32:07.460813: E tensorflow/core/platform/default/device_tracer.cc:134] cuda call ActivityRegisterCallbacks(BufferRequested, BufferCompleted) failed 15\r\nFailed to create CUPTI subcriber.\r\nTraceback (most recent call last):\r\n  File \"lib/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"/home/o594256/tmp/tf18/lib/tensorflow/python/client/session.py\", line 1308, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"lib/tensorflow/python/client/session.py\", line 1411, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create CUPTI subcriber.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/tmp/tfprof.py\", line 24, in <module>\r\n    _ = sess.run(c)\r\n  File \"lib/tensorflow/python/profiler/profile_context.py\", line 74, in _profiled_run\r\n    fetches, feed_dict, options, run_metadata)\r\n  File \"lib/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"lib/tensorflow/python/client/session.py\", line 1136, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"lib/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    run_metadata)\r\n  File \"lib/tensorflow/python/client/session.py\", line 1337, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create CUPTI subcriber.\r\n", "comments": ["I got the same problem with you, with tensorflow 1.9.0 and cuda 10.0.\r\nHow do you fix the problem? I am trying to update the tensorflow now.", "the same problem occured in tensorflow 1.12.0 and cuda 9.0 , how can I fix it?", "I got the same problem in tensorflow 1.5.0 and cuda 10.1 , how can I fix the problem?", "This answer from @trisolaran from https://github.com/tensorflow/tensorflow/issues/35860 helped me to solve many issues:\r\n\r\n> Adding options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\" to /etc/modprobe.d/nvidia-kernel-common.conf\r\n> and reboot should resolve the permission issue.\r\n\r\nTry it...", "Yes the above comment helped to resolve the issue. Thank you, closing this issue."]}, {"number": 20040, "title": "Same loss different/incorrect results", "body": "TF version: 1.8\r\nInstallation: tensorflow/tensorflow:latest-gpu\r\nOS details: \r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 16.04.4 LTS\r\nRelease:\t16.04\r\nCodename:\txenial\r\n\r\nAWS instance: p2.xlarge\r\nGPU details:\r\n```\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 16086859869116902206\r\n, name: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 11285974221\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n  }\r\n}\r\nincarnation: 13890740079777279899\r\nphysical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\"\r\n]\r\n```\r\n\r\nCustom code:\r\nThis works\r\n```python\r\nmodel.compile(loss='sparse_categorical_crossentropy',\r\n              optimizer=keras.optimizers.Adam(lr=0.01, clipnorm=1., amsgrad=True),\r\n              metrics=['acc'])\r\n```\r\nThis works too\r\n```python\r\nmodel.compile(loss=keras.losses.sparse_categorical_crossentropy,\r\n              optimizer=keras.optimizers.Adam(lr=0.01, clipnorm=1., amsgrad=True),\r\n              metrics=['acc'])\r\n```\r\nThis doesn't work. The loss value looks fine but the accuracy stays at 0 or very low value.\r\n```python\r\ndef sparse_cate_loss(y_true, y_pred):\r\n    return keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\r\n\r\nmodel.compile(loss=sparse_cate_loss,\r\n              optimizer=keras.optimizers.Adam(lr=0.01, clipnorm=1., amsgrad=True),\r\n              metrics=['acc'])\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20039, "title": "contrib.graph_editor.transform.copy_op_handler: fix for pruning", "body": "tf.contrib.graph_editor.transform.copy_op_handler: change default copy_shape to False so graph_replace can work for network pruning where new weight shape does not match old weight shape", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "signed it.\n\nOn Thu, Jun 14, 2018 at 4:27 PM, googlebot <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here (e.g. I\n> signed it!) and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google.com/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/20039#issuecomment-397426220>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AOfPMhkaXOOFrJhVBd2UTDPLhvpzfk5pks5t8scggaJpZM4UomhK>\n> .\n>\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "signed it again.", "CLAs look good, thanks!\n\n<!-- ok -->", "resolves #20026 ", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20038, "title": "Do not depend on boringssl for big-endian architectures.", "body": "A recent commit migrated TensorFlow from grpc_unsecure (and grpc++_unsecure)\r\nto their secure variants. These secure variants depend on BoringSSL.\r\nUnfortunately, BoringSSL does not work on big-endian architectures.\r\n\r\nThis commit abstracts the grpc dependency behind a couple cc_library rules,\r\nand plumbs through the logic to conditionally build without BoringSSL based\r\non the target architecture.\r\n\r\nFixes #20014", "comments": ["@namrata-ibm because I don't have the appropriate hardware to test this upon, can you try this out and see if it fixes the issue?\r\n\r\nThanks!", "@saeta, I tried above changes(including \"cpu\": \"s390x\" change), however build still failed at gRPC secure:\r\n```\r\nERROR: /home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/external/grpc/BUILD:1347:1: C++ compilation of rule '@grpc//:grpc_secure' failed (Exit 1)\r\nIn file included from external/boringssl/src/include/openssl/rsa.h:60:0,\r\n                 from external/grpc/src/core/lib/security/credentials/jwt/json_token.h:25,\r\n                 from external/grpc/src/core/lib/security/credentials/jwt/jwt_credentials.h:25,\r\n                 from external/grpc/src/core/lib/security/credentials/jwt/jwt_credentials.cc:21:\r\nexternal/boringssl/src/include/openssl/base.h:114:2: error: #error \"Unknown target CPU\"\r\n #error \"Unknown target CPU\"\r\n```\r\n\r\nPlease let me know if any more changes need to be tested.", "@namrata-ibm Whoops; thanks for catching the typo; should be fixed now.\r\n\r\nTo help debug where the dependency is coming in, can you run the following command:\r\n\r\n```\r\nbazel query 'allpaths(//tensorflow:libtensorflow_cc.so, @grpc//:grpc_secure)'\r\n```\r\n\r\nThanks!\r\n-Brennan", "Hi Brennan,\r\nThe Bazel query output:\r\n\r\n```\r\ntest@3bedc6a2c1e8:~/tensorflow$ bazel query 'allpaths(//tensorflow:libtensorflow_cc.so, @grpc//:grpc_secure)'\r\n...................................................\r\nWARNING: /home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/external/grpc/WORKSPACE:1: Workspace name in /home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/external/grpc/WORKSPACE (@com_github_grpc_grpc) does not match the name given in the repository's definition (@grpc); this will cause a build error in future versions\r\nWARNING: /home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\n\r\n//tensorflow:libtensorflow_cc.so\r\n//tensorflow/c/eager:c_api\r\n//tensorflow/core/distributed_runtime/rpc/eager:grpc_eager_client\r\n//tensorflow/core/distributed_runtime/rpc/eager:eager_grpc_server_lib\r\n//tensorflow/core/distributed_runtime/rpc/eager:grpc_eager_service_impl\r\n//tensorflow/core/distributed_runtime/rpc/eager:grpc_eager_service\r\n//tensorflow/core/distributed_runtime/eager:eager_service_impl\r\n//tensorflow/core/common_runtime/eager:execute\r\n//tensorflow/core/common_runtime/eager:eager_operation\r\n//tensorflow/core/common_runtime/eager:copy_to_device_node\r\n//tensorflow/core/common_runtime/eager:tensor_handle\r\n//tensorflow/core/common_runtime/eager:context\r\n//tensorflow/core/distributed_runtime/rpc:grpc_server_lib\r\n//tensorflow/core/distributed_runtime/rpc:rpc_rendezvous_mgr\r\n//tensorflow/core/distributed_runtime/rpc:grpc_worker_service\r\n//tensorflow/core/distributed_runtime/rpc:grpc_tensor_coding\r\n//tensorflow/core/distributed_runtime:worker\r\n//tensorflow/core/distributed_runtime/rpc:grpc_worker_cache\r\n//tensorflow/core/distributed_runtime/rpc:grpc_remote_worker\r\n//tensorflow/core/distributed_runtime/rpc:grpc_worker_service_impl\r\n//tensorflow/core/distributed_runtime/rpc:grpc_state\r\n//tensorflow/core/distributed_runtime/rpc:grpc_client_cq_tag\r\n//tensorflow/core/distributed_runtime/rpc:grpc_master_service\r\n//tensorflow/core/distributed_runtime/rpc:grpc_master_service_impl\r\n//tensorflow/core/distributed_runtime/rpc:grpc_call\r\n//tensorflow/core/distributed_runtime/rpc:grpc_channel\r\n//tensorflow/core/distributed_runtime/rpc:grpc_util\r\n//tensorflow/core/distributed_runtime:session_mgr\r\n//tensorflow/core/distributed_runtime:rpc_collective_executor_mgr\r\n//tensorflow/core/distributed_runtime:base_rendezvous_mgr\r\n//tensorflow/core/distributed_runtime:worker_session\r\n//tensorflow/core/distributed_runtime:graph_mgr\r\n//tensorflow/core/debug:debug\r\n//tensorflow/core/debug:debugger_state_impl\r\n//tensorflow/core:debug_ops_op_lib\r\n//tensorflow/core/kernels:debug_ops\r\n//tensorflow/core/debug:debug_io_utils\r\n//tensorflow/core/debug:debug_service_proto_cc\r\n//tensorflow:grpc++\r\n//tensorflow:grpc\r\n//external:grpc_lib\r\n@grpc//:grpc++\r\n@grpc//:grpc++_base\r\n@grpc//:grpc\r\n@grpc//:grpc_transport_chttp2_server_secure\r\n@grpc//:grpc_transport_chttp2_client_secure\r\n@grpc//:grpc_lb_policy_grpclb_secure\r\n@grpc//:grpc_secure\r\n```", "Ah, bazel query wont help with us here because it takes all branches when it sees an if statement.", "@saeta Maybe it is due to the change in workspace.bzl here:\r\nhttps://github.com/tensorflow/tensorflow/commit/ba9422a8adba18fc97cc1923002b7db8ca63dcfe#diff-455a4c7f8e22d7c514e8c2caa27506c5", "@namrata-ibm Please try again? Thanks!", "Yup.. Build completed successfully .. "]}, {"number": 20037, "title": "[Intel MKL] Bootstrapping MKL+GPU test", "body": "Adding a test for both MKL and GPU, as well as a script to launch it. \r\n\r\n@gunan These will run on the GPU machines in the MKL CI. Let me know if this is what you are looking for.", "comments": []}, {"number": 20036, "title": "Update debugger.md", "body": "Error: homebrew/dupes was deprecated. This tap is now empty as all its formulae were migrated.\r\ninstead use: brew install ncurses", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 20035, "title": "MKL DNN: Fix allocation ID for MKL", "body": "The allocation in the graph pass has been changed recently in non MKL version (original TF version) of code. Since MKL related code does more allocation in graph pass, thus allocation ID in MKL is also have been changed. Now it is fixed.\r\nIt would be nice, if someone changes allocation ID in non MKL version (TF only version), they can also change the MKL version of allocation ID. Both the allocation IDs (non MKL and MKL) are in the same location of the function. That way, both tests (MKL version and non MKL version) will be passed.\r\n Otherwise, we have to create a PR for MKL unit test every time someone changes the allocation ID in the non MKL version of code.", "comments": ["@tatianashp This will fix the //tensorflow/core:common_runtime_direct_session_with_tracking_alloc_test in the MKL builds. But it will break again when someone changes the TF allocation ID.\r\n\r\nSee #18240"]}, {"number": 20034, "title": "Improve S3 filesystem read performance", "body": "Avoid creating a new std::stringstream and copying data into it every time S3RandomAccessFile::Read() is called. Fixes #14572.\r\n\r\nI was seeing the same issue in #14572 when using an S3 path for my Tensorboard logdir and when reading datasets from S3. Looking at the S3 code, I noticed that S3RandomAccessFile was copying data read from an Aws::IOStream into a temporary std::stringstream, and then copying it back into the final buffer (`scratch`).\r\n\r\nBased on the comments for `RandomAccessFile::Read()`, it seems that the data from S3 could be directly written into `scratch` without a copy. I also think the stringstream was allocating lots of small chunks of memory to store the data read from the S3 files, which was probably the bigger performance hit.\r\n\r\nI updated the code and rebuilt Tensorflow, and now Tensorboard loads tfevent files from S3 much more quickly.\r\n\r\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 33 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@drpngx - I noticed the MacOS and Windows checks failed, but I'm not seeing any errors in the logs. Let me know if there's anything I need to do before this can be merged.", "This looks fine, this is ready for import internally. Will try to get to it later.", "Yay!"]}, {"number": 20033, "title": "Invalid Python example in baseline.py", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 20032, "title": "Failed to find any matching files for slim_pretrained\\inception_v1.ckpt", "body": " Unsuccessful TensorSliceReader constructor: Failed to\r\nfind any matching files for slim_pretrained\\inception_v1.ckpt\r\n         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_\r\nFLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:\r\n0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_\r\nand_slices)]]", "comments": ["Hi @falahgs. You need to follow the template when opening an issue. Provide a description so people understand the problem more clearly. ", "@marshalhayes  thanks for help ...It has been solved\r\n", "If this has been resolved, please close it :)", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20031, "title": "Handle checking of all ServingInputReceiver fields in single function", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}, {"number": 20030, "title": "Restrict contrast_factor as scalar for AdjustContrastv2 shape", "body": "The contrast_factor should be scalar for AdjustContrastv2.\r\n\r\nThis fix adds the scalar restriction in the shape function.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Nagging Reviewer @rmlarsen: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @rmlarsen: It has been 29 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20029, "title": "fix decode_png function.", "body": "Problem: In the case of palette png & args channels=0, force into grayscale.\r\nhttps://github.com/tensorflow/tensorflow/issues/20028", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@yoya Thanks for sending this PR. Can you please sign the CLA?", "OK. signed the CLA, a while ago.\r\nGoogle account: yoya@awm.jp\r\nGithub account: yoya\r\n![2018-06-15 13 22 26](https://user-images.githubusercontent.com/26040/41449968-4b3e207a-709f-11e8-87b7-0c67b2ba247d.png)\r\n![2018-06-15 13 21 19](https://user-images.githubusercontent.com/26040/41449976-5333241a-709f-11e8-8a6d-f041d7af884a.png)\r\n\r\n", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thank you for telling me how to add tests.\r\nPlease wait for a while.\r\n\r\nI'll add the follwing two image files & test settings.\r\n- lena_palette.png\r\n- lena_palette_trns.png", "Sounds good, thanks. Please let me know when you're done.", "https://github.com/tensorflow/tensorflow/pull/20029/commits/b28b82052525517beff238d2c53bde0171a4a59f\r\nI added two PNG palette image files to the test coverage.", "https://github.com/tensorflow/tensorflow/pull/20029/commits/1f31116a081afbf6794c5247a0b3b8ac120896b0\r\nExcuse me. I added a line break.", "Jenkins test this please (I think this should work...)", "@caisq can you help me get this committed? I'm not familiar with the PR process", "Jenkins test this please"]}, {"number": 20028, "title": "decode_png function. palette png & channels=0, force into grayscale.", "body": "tf.image.decode_png function.\r\nIn the case of palette png & args channels=0, force into grayscale.\r\n\r\n# sample\r\n\r\n```\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\nsrc_png_data = tf.read_file(\"rgb_png8.png\")\r\nimage = tf.image.decode_png(src_png_data, 0)\r\ndst_png_data =  tf.image.encode_png(image)\r\nwith open(\"output.png\", 'wb') as f:\r\n        f.write(dst_png_data.eval())\r\n```\r\n- input:rgb_png8.png <= Palette PNG has RGB.\r\n- output: output.png => Grayscale PNG\r\n\r\n# correctness except for this\r\n\r\ncase 1)\r\n- input <= Palette PNG has RGB.\r\n- output: => RGB PNG\r\n\r\ncase 2)\r\n- input <= Palette PNG has RGB & A(tRNS)\r\n- output: => RGBA PNG\r\n\r\ncase Otherwise)\r\n-  same as before\r\n\r\n# environment\r\n\r\n- macOS high sierra.\r\n- TensorFlow 1.8\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "all N/A on this issue. but I show these information.\r\n\r\n- macOS High Sierra (10.13.5)\r\n- pip3 install tensorflow (1.8.0)\r\n- Python  3.6.5\r\n- Bazel 0.14.1\r\n- (omit)\r\n- no CUDA\r\n- no GPU\r\n\r\n```\r\n(tensorflow) :tensorflow% wget https://raw.githubusercontent.com/tensorflow/tensorflow/master/tools/tf_env_collect.sh\r\n(tensorflow) :tensorflow% sh tf_env_collect.sh\r\n(tensorflow) :tensorflow% cat  tf_env.txt\r\n== cat /etc/issue ===============================================\r\nDarwin yoyaMBP2016late 17.6.0 Darwin Kernel Version 17.6.0: Tue May  8 15:22:16 PDT 2018; root:xnu-4570.61.1~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.13.5\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.0.0 (clang-900.0.38)\r\nTarget: x86_64-apple-darwin17.6.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin yoyaMBP2016late 17.6.0 Darwin Kernel Version 17.6.0: Tue May  8 15:22:16 PDT 2018; root:xnu-4570.61.1~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy          1.14.3\r\nprotobuf       3.5.2.post1\r\ntensorflow     1.8.0\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```", "@rmlarsen do you know about the image ops or can redirect to someone who does? Thanks.", "rgb_png8.png is here. (color-type=3)\r\n![rgb_png8.png](https://user-images.githubusercontent.com/26040/42313738-65c10f7c-807e-11e8-8f11-9db821e3c3cc.png)\r\n", "Some checks haven\u2019t completed yet.\r\n- https://github.com/tensorflow/tensorflow/pull/20029\r\n![image](https://user-images.githubusercontent.com/26040/43012100-67ba8390-8c80-11e8-9db3-54b4ad6999c9.png)\r\nIs there anything I should do ? ", "Nagging Assignee @rmlarsen: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I confirmed \"merged\" status. > https://github.com/tensorflow/tensorflow/pull/20029\r\nThank you all!"]}, {"number": 20027, "title": "Fix git_tag_override option in gen_git_source.py.", "body": "This fix was committed to the r1.8 branch but never to master.\r\nAdding this fix to master branch.", "comments": []}, {"number": 20026, "title": "tf.contrib.graph_editor Shape error", "body": "Using tf.contrib.graph_editor for pruning, specifically the graph_replace function, raises ValueErrors.\r\n\r\nAs a trivial example, consider a 2-layer network with weight matrices W0 [3,4] and W1 [4,1] that we want to prune one node from and replace with new weight matrices W0' [3,3] and W1' [3,1]. From the documentation, this appears be straight-forward to do using graph_replace and the resulting graph in theory shouldn't have any shape incompatibilities.\r\n\r\nThe documentation should be updated to clarify this limitation and a proper function for use with pruning should be identified if it exists.", "comments": ["@purpledog can you please clarify if there's another function in graph_editor that could be used for network pruning?", "From the description you gave it should be fine. would you mind sending some code?", "It appears the problem is in the copy_op_handler function where it copies the shape of the old op to the new one:\r\n```\r\ndef copy_op_handler(info, op, new_inputs, copy_shape=True):\r\n...\r\n  if copy_shape:\r\n    for t, t_ in zip(op.outputs, op_.outputs):\r\n      t_.set_shape(t.get_shape())\r\n...\r\n```\r\nI changed the default copy_shape to False and now it works. None of the calls to copy_op_handler seem to pass this argument though - is there a reason why it defaults to True?\r\n", "Here's a self-contained example:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import graph_editor as ge\r\n\r\ndef prune_test():\r\n    # make a simple graph\r\n    x = tf.placeholder(tf.float32, shape=[1, 2], name='x')\r\n    w0 = tf.Variable(np.array([[1, 2, 3, 4], [6, 7, 8, 9]], dtype=np.float32), name='w0')\r\n    x1 = tf.matmul(x, w0, name='x1')\r\n    w1 = tf.Variable(np.array([[-1], [2], [-3], [4]], dtype=np.float32), name='w1')\r\n    y = tf.matmul(x1, w1, name='y')\r\n\r\n    def vars_to_tensors(v_list):\r\n        def rsub(string, sub):\r\n            return string[:len(string)-len(sub)] if string.endswith(sub) else string\r\n        graph = tf.get_default_graph()\r\n        nodes = [n for n in graph.as_graph_def().node if n.op == 'VariableV2']\r\n        nodes = [next(n for n in nodes if rsub(v.name, ':0') == rsub(n.name, ':0')) for v in v_list]\r\n        return [graph.get_tensor_by_name(node.name + ':0') for node in nodes]\r\n\r\n    w0p = tf.Variable(np.array([[1, 2, 3], [6, 7, 8]], dtype=np.float32), name='w0_pruned')\r\n    w1p = tf.Variable(np.array([[-1], [2], [-3]], dtype=np.float32), name='w1_pruned')\r\n\r\n    # get tensors since graph_replace doesn't work with variables directly\r\n    [w0, w1, w0p, w1p] = vars_to_tensors([w0, w1, w0p, w1p])\r\n    replacements = {w0: w0p, w1: w1p}\r\n    yp = ge.graph_replace(y, replacements)\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        output, pruned_output = sess.run([y, yp], feed_dict={x: [[4., 7.]]})\r\n    print(output)\r\n    print(pruned_output)\r\n```\r\n\r\nWith the default value for copy_shape in copy_op_handler set to True, this fails with a shape error. With it set to False, this runs as expected.", "I added a [pull request](https://github.com/tensorflow/tensorflow/pull/20039) for this if you think the default value should be changed @purpledog \r\nI may be missing some situation where copying the shape is preferable though. Perhaps you could weigh in on that use case.", "Thanks Jason. This flags was useful at some point two years ago but I can't remember why and it might not be useful anymore. Your change is very much welcome. If it passes the test and make your life easier, then so be it!", "Nagging Assignee @purpledog: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "resolved by #20039 "]}, {"number": 20025, "title": "Fix missing header in aarch64 Nvidia Jetson", "body": "fix missing header in aarch64 Nvidia Jetson\r\n\r\nThis issue is GOOGLE_L4T Marco is 'not defined' on Jetson(build on Jetson) or config file. It will compile these section, then error happened.\r\n\r\nSee https://github.com/tensorflow/tensorflow/issues/19956", "comments": []}, {"number": 20024, "title": "How to get activation values of fully connected layers?", "body": "I want to extract the neural activation from the Fully connected layers. In caffe i was doing like this net.blobs[layer_name].data\r\n\r\nHow can i do the same in tensorflow?\r\n", "comments": ["Perhaps [@google/prettytensor#6](https://github.com/google/prettytensor/issues/6) could be helpful even though it's not from this repository. ", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20023, "title": "tf.keras.layers.DepthwiseConv2D fail when bias is enabled", "body": "It seems that a little mistake is present in tf.keras.layers.DepthwiseConv2D, in the call procedure:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/17d6639b550cdcedf31ee01bd6eb26c592aeac42/tensorflow/python/keras/layers/convolutional.py#L1727\r\n\r\nIn my understanding self.bias should be replaced by self.use_bias on this line, as self.bias is a tensor, wheras self.use_bias is a boolean value.\r\n\r\nAfter replacing it, the DepthwiseConv2D seems to works fine here.\r\n@fchollet what do you think ?\r\n\r\n\r\nHave I written custom code N/A\r\nOS Platform and Distribution N/A\r\nTensorFlow installed from N/A\r\nTensorFlow version N/A\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce"]}, {"number": 20022, "title": "Keras fit method not working with dataset iterator", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.9-rc0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: `model.fit(get_iterator,steps_per_epoch=2,batch_size=2,epochs=2,shuffle =True,verbose=1)`\r\nand\r\n`model.fit(get_iterator,get_iterator,steps_per_epoch=2,batch_size=2,epochs=2,shuffle =True,verbose=1)`\r\n\r\n### Describe the problem\r\nWhen I pass one dataset iterator to `fit` method, I get:\r\n \r\n\r\n> Please provide data as a list or tuple of 2 elements  - input and target pair. Received Tensor(\"IteratorGetNext_4:0\", shape=(2, ?), dtype=float32)\r\n\r\n\r\nWhen I pass two iterators I get the error:\r\n\r\n> ValueError: You passed a dataset or dataset iterator (<tensorflow.python.data.ops.iterator_ops.Iterator object at 0x000001FEABE88748>) as input `x` to your model. In that case, you should not specify a target (`y`) argument, since the dataset or dataset iterator generates both input data and target data. Received: <tensorflow.python.data.ops.iterator_ops.Iterator object at 0x000001FEABE88748>\r\n\r\nWhen I create a new dataset after zipping the original x and y data set and pass that to `fit `I get the error described in https://github.com/tensorflow/tensorflow/issues/19912\r\n\r\nAccording to 1.9-rc0 method release notes iterators should be usable with keras training methods. Please provide a solution or provide clarification in the documentation.\r\n\r\n### Source code / logs\r\n```\r\ndataset= tf.contrib.data.make_csv_dataset(file_name,48,select_columns= ['Load_residential_multi_0','Load_residential_multi_1'],shuffle=False)\r\ndataset = dataset.map(lambda x: tf.stack(list(x.values())))\r\nget_iterator = dataset.make_one_shot_iterator()\r\nget_batch = get_iterator.get_next()\r\n\r\n#Building and training a single layer model using Keras (Available within TensorFlow)\r\nmodel = Sequential() \r\n#Input Layer\r\nmodel.add(InputLayer(input_shape=(48,),name='InputLayer'))#,input_tensor =dataset\r\n#model.add(BatchNormalization(axis=1))  #Normalizing values\r\n#Layer1 \r\nmodel.add(Dense(units=5,activation='relu',name='FeedForward1'))  #Add a feed forward layer\r\n#Layer2 \r\nmodel.add(Dense(units=5,activation='relu',name='FeedForward2'))  #Add a feed forward layer\r\n#Output layer \r\nmodel.add(Dense(units=48,name='OutputLayer'))\r\n\r\n#Specify los function and optimizer\r\nmodel.compile(loss='mse',optimizer='adam',metrics=['mae'])\r\n\r\n#Summarize model\r\nmodel.summary()\r\n#Train the model\r\nmodel.fit(get_iterator,steps_per_epoch=2,batch_size=2,epochs=2,shuffle =True,verbose=1)\r\n#model.fit(get_iterator,get_iterator,steps_per_epoch=2,batch_size=2,epochs=2,shuffle =True,verbose=1)\r\n```", "comments": ["I'm trying the same thing,  I made it work using class inheritance from keras.Model\r\n\r\nSomeone please check if the functional API makes sense in this case since it needs the InputLayer (correct me if I'm wrong)\r\n\r\n", "@sebamlu Can you please share the code snippet that you used to make it work?", "Sure, I'm using TFRecordDataset instead of tf.contrib.data.make_csv_dataset, but should make no difference.\r\n\r\n\r\n```\r\n    with tf.variable_scope('feeding_data'):\r\n        train_set = tf.data.TFRecordDataset(glob(DATA_DIR + '/*train*.tfrecord'))\r\n        train_set = train_set.repeat(epochs)\r\n        train_set = train_set.shuffle(1000)\r\n        train_set = train_set.map(decode_data, num_parallel_calls=num_map_threads)\r\n        train_set = train_set.batch(batch_size)\r\n\r\n        test_set = tf.data.TFRecordDataset(glob(DATA_DIR + '/*test*.tfrecord'))\r\n        test_set = test_set.shuffle(buffer_size=1000)  # Enables stochastic batch validation\r\n        test_set = test_set.map(decode_data, num_parallel_calls=num_map_threads)\r\n        test_set = test_set.batch(batch_size)\r\n\r\n        training_iterator = train_set.make_one_shot_iterator()\r\n        validation_iterator = test_set.make_initializable_iterator()  \r\n\r\n    model = BaseCnnModel(num_classes=n_classes)\r\n    model.compile(\r\n        optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9),\r\n        loss='categorical_crossentropy',\r\n        metrics=['accuracy'],\r\n    )\r\n\r\n    model.fit(\r\n        x=training_iterator,\r\n        validation_data=validation_iterator,\r\n        steps_per_epoch=40,\r\n        epochs=epochs,\r\n        validation_steps=2,\r\n    )\r\n\r\n```\r\n\r\nand the model\r\n\r\n> class BaseCnnModel(keras.Model):\r\n> \r\n>     def __init__(self, num_classes, dropout_rate=0.5):\r\n>         super(BaseCnnModel, self).__init__(name='bcnn')\r\n> \r\n>         self.cnn1 = keras.layers.Conv2D(12, 3, activation=tf.nn.relu)\r\n>         self.maxp1 = keras.layers.MaxPooling2D((2, 2))\r\n>         self.cnn2 = keras.layers.Conv2D(16, 3, activation=tf.nn.relu)\r\n>         self.maxpl2 = keras.layers.MaxPooling2D((2, 2))\r\n>         self.cnn3 = keras.layers.Conv2D(16, 3, activation=tf.nn.relu)\r\n>         self.maxpl3 = keras.layers.MaxPooling2D((2, 2))\r\n>         self.flat = keras.layers.Flatten()\r\n>         self.dense1 = keras.layers.Dense(512, activation=tf.nn.relu)\r\n>         self.drop1 = keras.layers.Dropout(dropout_rate)\r\n>         self.dense2 = keras.layers.Dense(num_classes, activation=None)\r\n> \r\n>     def call(self, inputs):\r\n>         x = self.cnn1(inputs)\r\n>         x = self.maxp1(x)\r\n>         x = self.cnn2(x)\r\n>         x = self.maxpl2(x)\r\n>         x = self.cnn3(x)\r\n>         x = self.maxpl3(x)\r\n>         x = self.flat(x)\r\n>         x = self.dense1(x)\r\n>         x = self.drop1(x) # I assume that keras makes this work properly on evaluation..\r\n>         return self.dense2(x)\r\n> \r\n\r\n\r\nAs you can see i never defined an input shape for the model\r\n", "@sebamlu thanks for sharing. So you are saying this doesn't work when you defined the Keras model without using the class?\r\nAnyway I will try putting my model inside a class and see if it works with the `tf.contrib.data.make_csv`", "@sibyjackgrove From [https://github.com/keras-team/keras/blob/master/examples/mnist_tfrecord.py](url)\r\n\r\n```\r\nInput Tensors also have important disadvantages. In\r\nparticular, Input Tensors are fixed at model construction\r\nbecause rewiring networks is not yet supported.\r\nFor this reason, changing the data input source means\r\nmodel weights must be saved and the model rebuilt\r\nfrom scratch to connect the new input data.\r\nvalidation cannot currently be performed as training\r\nprogresses, and must be performed after training completes.\r\n....\r\n\r\n```\r\nI don't see this working with two iterators if you define an InputTensor", "I created the model as you suggested, \r\n```\r\nclass BaseModel(Model):\r\n    def __init__(self, num_features):\r\n        super(BaseModel, self).__init__(name='bcnn')\r\n\r\n        self.dense1 = Dense(12, activation=tf.nn.relu)\r\n        self.dense2 = Dense(12, activation=tf.nn.relu)\r\n        self.dense3 = Dense(num_features, activation=None)\r\n\r\n    def call(self, inputs):\r\n        x = self.dense1(inputs)\r\n        x = self.dense2(x)\r\n        return self.dense3(x)\r\nmodel = BaseModel(num_features=48)\r\nmodel.compile(loss='mse',optimizer='adam',metrics=['mae'])\r\n```\r\nBut I am getting the same error as before.", "`Please provide data as a list or tuple of 2 elements - input and target pair. Received Tensor(\"IteratorGetNext_4:0\", shape=(2, ?), dtype=float32)`\r\n\r\nI read your error and I think that the problem is with your map function, tf.stack returns a tensor and you need to return a tuple or list like: (features_tensor, target_label_tensor),  hope this helps.", "I need the tf.stack to flatten the columns from the CSV file. Anyway I removed the `tf.stack` as shown below: \r\n```\r\ndataset= tf.contrib.data.make_csv_dataset(file_name,48,select_columns=['Load_residential_multi_0','Load_residential_multi_1'],shuffle=False)\r\ndataset = dataset.batch(2)\r\nget_iterator = dataset.make_one_shot_iterator()\r\nget_batch = get_iterator.get_next()\r\n```\r\nBut I get pretty much the same error except that the  `tf.contrib.data.make_csv_dataset` is now returning an ordered dictionary.\r\n\r\n> ValueError: Please provide data as a list or tuple of 2 elements  - input and target pair. Received OrderedDict([('Load_residential_multi_0', <tf.Tensor 'IteratorGetNext_8:0' shape=(?, ?) dtype=float32>), ('Load_residential_multi_1', <tf.Tensor 'IteratorGetNext_8:1' shape=(?, ?) dtype=float32>)])\r\n\r\nMaybe the tf.contrib.data.make_csv_dataset just can't work with Keras model. I wish they would clarify.", "Duplicate of #19912 "]}, {"number": 20021, "title": "[BUG] TensorFlow crashed when using tf.cast() in dataset's map() function", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\npip\r\n- **TensorFlow version (use command below)**:\r\n1.8.0 GPU\r\n- **Python version**: \r\n3.6.4\r\n- **Bazel version (if compiling from source)**:\r\nNo\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNo\r\n- **CUDA/cuDNN version**:\r\n9.0/7\r\n- **GPU model and memory**:\r\nGPU: GPU 0: GeForce GTX 1080 Ti x2\r\nRAM: 16GB\r\n- **Exact command to reproduce**:\r\nNo\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nWhen I using TensorFlow eager execution with gpu, I want to read some data, so I use `tf.data.Dataset`. Then I want to pre-process the data before training, then I use `dataset.map()` and `tfe.py_func()`. Also, I use `dataset.prefetch(1)` to read data with pipelining,\r\nEverything is fine so far. But when I using `tf.cast()` in map function, the program will crash.\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.eager.python import tfe\r\ntf.enable_eager_execution()\r\n\r\n\r\ndef main():\r\n    data = tf.range(0, 100, dtype=tf.float32)\r\n\r\n    def map_fn(n):\r\n        def fn(nn):\r\n            int_tensor = tf.constant(1, tf.int32)\r\n            float_tensor = tf.cast(int_tensor, tf.float32)\r\n            return float_tensor\r\n        return tfe.py_func(fn, [n], [tf.float32])\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices(data)\r\n    dataset = dataset.map(map_fn).prefetch(1)\r\n\r\n    for i in dataset:\r\n        print(i)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nrun the code:\r\n```shell\r\n$ python xxx.py\r\n```\r\n\r\noutput:\r\n> 2018-06-14 13:29:48.784627: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2018-06-14 13:29:48.925618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:17:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.45GiB\r\n2018-06-14 13:29:49.063957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:65:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.44GiB\r\n2018-06-14 13:29:49.064088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1\r\n2018-06-14 13:29:49.449500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-14 13:29:49.449552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 \r\n2018-06-14 13:29:49.449562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y \r\n2018-06-14 13:29:49.449570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N \r\n2018-06-14 13:29:49.449834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10108 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2018-06-14 13:29:49.543343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10107 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n[1]    84561 segmentation fault (core dumped)  python dataset_eager_bug.py", "comments": ["Akshay: I've run a few versions of this, and think it could be a bug in `tfe.py_func()`, so could you please take a look? Here are some notes:\r\n\r\n* The crash only happens (1) when a GPU is available, and (2) when `Dataset.prefetch()` is used.\r\n* The crash doesn't seem to depend on using `tf.cast()`. Replacing `fn` with `lambda _: tf.constant([1., 2., 3.]) + tf.constant([4., 5., 6.])` gives the same failure.\r\n* The crashing stack suggests that we are treating an `EagerTensor` that is actually in GPU memory as if it were in host memory:\r\n\r\n```\r\n    @     ...\r\n    @     0x55e3661fb60c        400  tensorflow::TF_TensorToPyArray()\r\n    @     0x55e3661fd2e3         96  tensorflow::TensorToNdarray()\r\n    @     0x55e3661c30ea         96  EagerTensor_numpy()\r\n    @     0x55e36295a4f3        320  PyEval_EvalFrameEx\r\n    @     ...\r\n```\r\n\r\n* When `Dataset.prefetch()` is used, the `tf.cast()` kernel runs on GPU. When the `Dataset.prefetch()` is removed, the `tf.cast()` kernel runs on CPU. The main difference between these two  cases is that when `Dataset.prefetch()` is used, the body of the `py_func` will run on a background thread, so maybe some thread-local state isn't configured?\r\n* Even though the `tf.cast()` kernel runs on GPU (when `Dataset.prefetch()` is used), the `TensorHandle` for its result has `nullptr` for its device, which appears to be why `EagerTensor_numpy()` thinks it is safe to copy.\r\n\r\n/CC @asimshankar, since the `TensorHandle::device()` method has comments with TODOs against his name, and so he might know where the skeletons are :).", "Thanks for the notes, Derek. I'll dig in further.", "Thanks for the report @DHZS and investigation @mrry .\r\nWhile we dig into an appropriate fix, a quick workaround would be to explicitly include a `with tf.device(\"CPU:0\")` in your py func, or just add a `.cpu()` to the outputs to ensure that the returned tensors are in CPU. So something like:\r\n\r\n```python\r\ndef map_fn(n):\r\n    def fn(nn):\r\n        int_tensor = tf.constant(1, tf.int32)\r\n        float_tensor = tf.cast(int_tensor, tf.float32)\r\n        return float_tensor.cpu()  # Notice the .cpu()\r\n    return tfe.py_func(fn, [n], [tf.float32])\r\n```", "Thank you @akshayka, it works for me. \r\nBy the way, I have a related issue https://github.com/tensorflow/tensorflow/issues/19945, maybe it's helpful for you to find the bug."]}, {"number": 20020, "title": "Update version strings for 1.9.0-rc1.", "body": "", "comments": []}, {"number": 20019, "title": "tflite mult_thread", "body": "Describe the problem\r\nHow can we use tflite code in android to test the mult_thread testing.\r\nThe set thread code doen't work.\r\n\r\nSource code / logs\r\npublic void setNumThreads(int num_threads) {\r\nif (tflite != null)\r\ntflite.setNumThreads(num_threads);\r\n}", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: No\r\nOS Platform and Distribution: Android studio3.0.1 ndkr14, API21-21\r\nTensorFlow installed from: GitHub  https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite\r\nTensorFlow version\r\nBazel version:0.14.0\r\nCUDA/cuDNN version\r\n", "Similar to #18658 ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Seems to be similar to the other issue where we will track this. So closing this."]}, {"number": 20018, "title": "tflite multi_thread", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nHow can we use tflite code in android to test the mult_thread testing.\r\nThe set thread code doen't work.\r\n\r\n### Source code / logs\r\npublic void setNumThreads(int num_threads) {\r\n    if (tflite != null)\r\n      tflite.setNumThreads(num_threads);\r\n  }\r\n", "comments": []}, {"number": 20017, "title": "Sync package version of double-conversion between bazel and cmake", "body": "This fix tries to sync package version of double-conversion between bazel and cmake.\r\n\r\nThe double-conversion package was added in #12102 and was reverted in PR #15133. At that time the package version was 5664746 for both bazel and cmake.\r\n\r\nLater on, the double-conversion was re-introduced in PR #18746. The package version of double-conversion in bazel has been advanced to `3992066a95b823efc8ccc1baf82a1cfc73f6e9b8` but the version in cmake remains the old `5664746`.\r\n\r\nThis fix updates the double-conversion version in cmake so that it is synced with the version (`3992066a95b823efc8ccc1baf82a1cfc73f6e9b8`) used in bazel.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20016, "title": "Fail to convert from .pb to .toco", "body": "The error message is:\r\n2018-06-14 20:36:34.054067: F tensorflow/contrib/lite/toco/tooling_util.cc:939] Check failed: name.substr(colon_pos + 1).find_first_not_of(\"0123456789\") == string::npos (1 vs. 18446744073709551615)Array name must only have digits after colon\r\nAborted (core dumped)\r\n\r\nDoes anyone know what this message is about?\r\nThanks\r\n\r\n----\r\nupdate:\r\nSorry about wrong typing in the issue title. It should be \"Fail to convert from .pb to .tflite\"\r\nAdd info:\r\n1. Have I written custom code\r\nNo, I use the original toco.\r\n2. OS Platform and Distribution\r\nUbuntu 16.04 \r\n3. TensorFlow installed from\r\nCloned from tensorflow official github\r\n4. TensorFlow version\r\n1.8.0\r\n5. Bazel version\r\nBuild label: 0.14.0\r\n6. CUDA/cuDNN version\r\nI didn't use GPU.\r\n7. GPU model and memory\r\nN/A\r\n8. Exact command to reproduce\r\nI'm sorry that the .pb model I tried to transform is a privately built model, and I have no right to\r\nupload it online. \r\nThe commands transforming the model is just like the Basic example given in\r\n\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md\".\r\nExcept the model name and output node names were changed to that of my model.\r\n\r\nThanks.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Could you please try again on the nightly download, we think this is now fixed. Otherwise provide repro instructions.", "@wenching33 does it work for you now?", "@cy89 No, the error still exists.\r\nActually, the .pb model I try to transfer to .tflite is transferred from .caffemodel using MMdnn.\r\nI am not sure whether some error already exists after MMdnn conversion.", "@wenching33 May you provide the following, please:\r\n- Model file\r\n- Reproducible instructions\r\n- Error encountered\r\n\r\nThanks :) ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20015, "title": "Feature request: Length normalisation  in ctc_beam_search_decoder", "body": "I  read that length normalisation improves the result of beam search. So, is there any plan of adding length normalisation in ctc_beam_search_decoder? \r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version:N/A\r\nBazel version:N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory:N/A\r\nExact command to reproduce:N/A\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I think length normalization only makes sense when using beam search in combination with a language model (LM).\r\nIn case a character-level LM is used, a labeling with say L characters gets scored L times. Each scoring is done with a value between p=0..1. Therefore, the LM-score for a long text is usually too low and length normalization makes sense in this case. \r\nBottom line: as ctc_beam_search_decoder has no LM included, there is no need for length normalization.\r\n"]}]