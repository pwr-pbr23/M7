[{"number": 14636, "title": "Tensorflow Python ", "body": "Hi This is my code for tensorflow_serve python client :\r\n\r\n```\r\ndata = f.read()\r\ndata = base64.urlsafe_b64encode(data)\r\nrequest = predict_pb2.PredictRequest()\r\nrequest.model_spec.name = 'test'\r\nrequest.model_spec.signature_name = 'serving_default'\r\ndata = tf.contrib.util.make_tensor_proto(data,shape=[1])\r\n\r\nreq=request.inputs['input'].CopyFrom(data)\r\n```\r\ncould you please do me a favor and tell me how to convert(change) :\r\n```\r\n\r\ndtype: DT_STRING\r\ntensor_shape {\r\ndim {\r\nsize: 1\r\n }\r\n}\r\nstring_val: \"_9j_4AAQSkZJRgABAQEAYABgAAD_4QAWRXhpZgAASUk\r\n```\r\n\r\nto :\r\n```\r\n\r\ninputs: {\r\ndtype: DT_STRING\r\ntensor_shape {\r\ndim {\r\nsize: 1\r\n }\r\n}\r\nstring_val: \"_9j_4AAQSkZJRgABAQEAYABgAAD_4QAWRXhpZgAASUk\r\n```\r\n", "comments": ["Feel sorry for maintainers overwhelmed by tons of issues like this. \ud83d\ude4a ", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14635, "title": "Disable for generated_examples_zip_test in open-source", "body": "", "comments": []}, {"number": 14634, "title": "1.4.1 patch release update version string", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 14633, "title": "[FEATURE REQUEST] Report uninitialized data iterators.", "body": "At https://github.com/GPflow/GPflow/issues/561 we found that, iterators are not variables :), therefore `report_uninitalized_variables` fails with:\r\n\r\n```\r\n...\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py in is_variable_initialized(ref, name)\r\n    182     A `Tensor` of type `bool`.\r\n    183   \"\"\"\r\n--> 184   if ref.dtype._is_ref_dtype:\r\n    185     return gen_state_ops.is_variable_initialized(ref=ref, name=name)\r\n    186   # Handle resource variables.\r\n\r\nAttributeError: 'Iterator' object has no attribute 'dtype'\r\n```\r\n\r\n```\r\nimport numpy as np\r\nfrom tensorflow import data\r\nDataset = data.Dataset\r\npl = tf.placeholder(tf.float32)\r\ndata = Dataset.from_tensor_slices(pl)\r\niterator = data.make_initializable_iterator()\r\nbatch = iterator.get_next()\r\n\r\n## In fact it fails for TensorFlow 1.3 and 1.4\r\nsess.run(tf.report_uninitialized_variables([iterator]))\r\n```\r\n\r\nFrankly, it must be very easy to adapt `report_unitialized_variables` to report data's unitialized iterators. This would be helpful to avoid re-initializing datasets each time when we call `session.run` in GPflow. Right now, it provides initializing by demand feature.\r\n\r\nBest,\r\nArtem Artemev", "comments": ["I'm a little unclear on the motivation here... why not initialize the iterator once at the beginning of the session (or use `Dataset.make_one_shot_iterator()`)?", "@mrry, The motivation is that the same iterator can be used with different data sources and another iterators can also be involved, hence we need to re-initialize only iterators and variables for which sources were altered. In other words, we need to be able to filter initialized variables at model's variables (&iterators) list. In [GPflow](https://github.com/GPflow/GPflow), the model can look as:\r\n\r\n```\r\nIn [6]: model = gpflow.models.SVGP(X=np.random.randn(10, 1),\r\n   ...:                                 Y=np.random.randn(10, 1),\r\n   ...:                                 kern=gpflow.kernels.RBF(1),\r\n   ...:                                 likelihood=gpflow.likelihoods.Gaussian(),\r\n   ...:                                 Z=np.random.randn(10, 1),\r\n   ...:                                 minibatch_size=1)\r\n```\r\nand X, Y will be transformed into minibatch interators, numpy arrays are passed to the placeholders. User is able to change this values any time.", "@mrry, any thoughts on it? :)", "Same request applies to `tf.is_variable_initialized`. It would be great if it could work with `tf.data.Iterator`.", "@mrry, @cy89 ^^^\r\n", "https://github.com/GPflow/GPflow/issues/568 another one related issue in GPflow", "@mrry, @cy89 any further thoughts on this?", "Sorry for the delay. I'm not convinced that we should add this to the API, because it seems possible to keep track of the initialized state in the calling code if need be. Moreover, it would be inherently racy, because another thread could come in and initialize the iterator between reading its initialized status and re-initializing it.", "@mrry Hello Derek. Thank you for your answer.\r\n\r\nI strongly disagree, because the same thinking can be applied to variables - why does tensorflow need `report_initializables` or `is_variable_initialized` if user can track their statuses easily. In fact, it is not easy, because our users can exploit many sessions, which means that we have to track statuses of all variables (iterators) for users sessions.\r\n\r\nI would prefer to have general solution for checking either initiliazed tensor or not, w/o diving into details of what it is variable or data iterator."]}, {"number": 14632, "title": "libtensorflow_cc.so linker issues with release 1.4 Undefined reference ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n  Yes, code similar to the example label_image.cc\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n Linux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nFrom source\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.4.0\r\n\r\n- **Python version**: \r\n3.6.1\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.7\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n\r\n- **GPU model and memory**:\r\nN/A\r\n\r\n- **Exact command to reproduce**:\r\nbuild libtensorflow_cc.so from command:\r\nbazel build --config=opt --config=mkl --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 //tensorflow:libtensorflow_cc.so //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nAfter successfully build the library, try to compile my code with the library using following gcc command:\r\n\r\ng++ -I/usr/local/include -L/usr/local/lib -ltensorflow -std=c++11 rtclassifier.cc\r\n\r\nresult in 'undefined reference to `tensorflow::GraphDef::GraphDef()' error\r\n\r\n### Source code / logs\r\n\r\n/tmp/ccxBMZph.o: In function `LoadGraph(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unique_ptr<tensorflow::Session, std::default_delete<tensorflow::Session> >*)':\r\ntl_classifier.cc:(.text+0x90): undefined reference to `tensorflow::GraphDef::GraphDef()'\r\ntl_classifier.cc:(.text+0xa4): undefined reference to `tensorflow::Env::Default()'\r\ntl_classifier.cc:(.text+0xc4): undefined reference to `tensorflow::ReadBinaryProto(tensorflow::Env*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::MessageLite*)'\r\ntl_classifier.cc:(.text+0x15e): undefined reference to `tensorflow::SessionOptions::SessionOptions()'\r\ntl_classifier.cc:(.text+0x16d): undefined reference to `tensorflow::NewSession(tensorflow::SessionOptions const&)'\r\ntl_classifier.cc:(.text+0x22a): undefined reference to `tensorflow::GraphDef::~GraphDef()'\r\ntl_classifier.cc:(.text+0x2b7): undefined reference to `tensorflow::GraphDef::~GraphDef()'\r\n/tmp/ccxBMZph.o: In function `ReadTensorFromImageFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, int, float, float, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*)':\r\ntl_classifier.cc:(.text+0x332): undefined reference to `tensorflow::Scope::NewRootScope()'\r\ntl_classifier.cc:(.text+0x3dd): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\ntl_classifier.cc:(.text+0x3fd): undefined reference to `tensorflow::ops::ReadFile::ReadFile(tensorflow::Scope const&, tensorflow::Input)'\r\ntl_classifier.cc:(.text+0x40c): undefined reference to `tensorflow::Scope::~Scope()'\r\ntl_classifier.cc:(.text+0x504): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\ntl_classifier.cc:(.text+0x528): undefined reference to `tensorflow::ops::DecodePng::DecodePng(tensorflow::Scope const&, tensorflow::Input, tensorflow::ops::DecodePng::Attrs const&)'\r\ntl_classifier.cc:(.text+0x587): undefined reference to `tensorflow::Scope::~Scope()'\r\ntl_classifier.cc:(.text+0x671): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\ntl_classifier.cc:(.text+0x691): undefined reference to `tensorflow::ops::DecodeGif::DecodeGif(tensorflow::Scope const&, tensorflow::Input)'\r\ntl_classifier.cc:(.text+0x6f4): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\ntl_classifier.cc:(.text+0x714): undefined reference to `tensorflow::ops::Squeeze::Squeeze(tensorflow::Scope const&, tensorflow::Input)'\r\ntl_classifier.cc:(.text+0x773): undefined reference to `tensorflow::Scope::~Scope()'\r\ntl_classifier.cc:(.text+0x7be): undefined reference to `tensorflow::Scope::~Scope()'\r\ntl_classifier.cc:(.text+0x867): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\ntl_classifier.cc:(.text+0x88b): undefined reference to `tensorflow::ops::DecodeJpeg::DecodeJpeg(tensorflow::Scope const&, tensorflow::Input, tensorflow::ops::DecodeJpeg::Attrs const&)'\r\ntl_classifier.cc:(.text+0x8ea): undefined reference to `tensorflow::Scope::~Scope()'\r\ntl_classifier.cc:(.text+0x97a): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\ntl_classifier.cc:(.text+0x99c): undefined reference to `tensorflow::ops::Cast::Cast(tensorflow::Scope const&, tensorflow::Input, tensorflow::DataType)'\r\ntl_classifier.cc:(.text+0x9ab): undefined reference to `tensorflow::Scope::~Scope()'\r\ntl_classifier.cc:(.text+0xa38): undefined reference to `tensorflow::ops::ExpandDims::ExpandDims(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)'\r\ntl_classifier.cc:(.text+0xaea): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\ntl_classifier.cc:(.text+0xb0a): undefined reference to `tensorflow::ops::Const(tensorflow::Scope const&, tensorflow::Input::Initializer const&)'\r\ntl_classifier.cc:(.text+0xb60): undefined reference to `tensorflow::ops::ResizeBilinear::ResizeBilinear(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)'\r\ntl_classifier.cc:(.text+0xb9c): undefined reference to `tensorflow::Scope::~Scope()'\r\ntl_classifier.cc:(.text+0xc9c): undefined reference to `tensorflow::ops::Subtract::Subtract(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)'\r\ntl_classifier.cc:(.text+0xcd5): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\ntl_classifier.cc:(.text+0xcf9): undefined reference to `tensorflow::ops::Div::Div(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)'\r\ntl_classifier.cc:(.text+0xd17): undefined reference to `tensorflow::Scope::~Scope()'\r\ntl_classifier.cc:(.text+0xdbb): undefined reference to `tensorflow::GraphDef::GraphDef()'\r\ntl_classifier.cc:(.text+0xddb): undefined reference to `tensorflow::Scope::ToGraphDef(tensorflow::GraphDef*) const'\r\ntl_classifier.cc:(.text+0xe3e): undefined reference to `tensorflow::SessionOptions::SessionOptions()'\r\ntl_classifier.cc:(.text+0xe4d): undefined reference to `tensorflow::NewSession(tensorflow::SessionOptions const&)'\r\ntl_classifier.cc:(.text+0x109e): undefined reference to `tensorflow::GraphDef::~GraphDef()'\r\ntl_classifier.cc:(.text+0x1116): undefined reference to `tensorflow::Scope::~Scope()'\r\ntl_classifier.cc:(.text+0x1175): undefined reference to `tensorflow::Scope::~Scope()'\r\ntl_classifier.cc:(.text+0x11b4): undefined reference to `tensorflow::Scope::~Scope()'\r\ntl_classifier.cc:(.text+0x1216): undefined reference to `tensorflow::Scope::~Scope()'\r\ntl_classifier.cc:(.text+0x1275): undefined reference to `tensorflow::Scope::~Scope()'\r\n/tmp/ccxBMZph.o:tl_classifier.cc:(.text+0x12d7): more undefined references to `tensorflow::Scope::~Scope()' follow\r\n/tmp/ccxBMZph.o: In function `ReadTensorFromImageFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, int, float, float, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*)':\r\ntl_classifier.cc:(.text+0x15ee): undefined reference to `tensorflow::GraphDef::~GraphDef()'\r\ntl_classifier.cc:(.text+0x167a): undefined reference to `tensorflow::Scope::~Scope()'\r\n/tmp/ccxBMZph.o: In function `GetTopLabels(std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> > const&, int, tensorflow::Tensor*, tensorflow::Tensor*)':\r\ntl_classifier.cc:(.text+0x18fe): undefined reference to `tensorflow::Scope::NewRootScope()'\r\ntl_classifier.cc:(.text+0x1999): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\ntl_classifier.cc:(.text+0x19bd): undefined reference to `tensorflow::ops::TopK::TopK(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)'\r\ntl_classifier.cc:(.text+0x19db): undefined reference to `tensorflow::Scope::~Scope()'\r\ntl_classifier.cc:(.text+0x1a08): undefined reference to `tensorflow::GraphDef::GraphDef()'\r\ntl_classifier.cc:(.text+0x1a28): undefined reference to `tensorflow::Scope::ToGraphDef(tensorflow::GraphDef*) const'\r\ntl_classifier.cc:(.text+0x1a8b): undefined reference to `tensorflow::SessionOptions::SessionOptions()'\r\ntl_classifier.cc:(.text+0x1a9a): undefined reference to `tensorflow::NewSession(tensorflow::SessionOptions const&)'\r\ntl_classifier.cc:(.text+0x1d7e): undefined reference to `tensorflow::GraphDef::~GraphDef()'\r\ntl_classifier.cc:(.text+0x1d9c): undefined reference to `tensorflow::Scope::~Scope()'\r\ntl_classifier.cc:(.text+0x1de4): undefined reference to `tensorflow::Scope::~Scope()'\r\ntl_classifier.cc:(.text+0x1f1a): undefined reference to `tensorflow::GraphDef::~GraphDef()'\r\ntl_classifier.cc:(.text+0x1f3d): undefined reference to `tensorflow::Scope::~Scope()'\r\n/tmp/ccxBMZph.o: In function `PrintTopLabels(std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)':\r\ntl_classifier.cc:(.text+0x1ff4): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'\r\ntl_classifier.cc:(.text+0x201c): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'\r\ntl_classifier.cc:(.text+0x2081): undefined reference to `tensorflow::Tensor::Tensor()'\r\ntl_classifier.cc:(.text+0x2090): undefined reference to `tensorflow::Tensor::Tensor()'\r\ntl_classifier.cc:(.text+0x21ed): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'\r\ntl_classifier.cc:(.text+0x225a): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'\r\ntl_classifier.cc:(.text+0x2284): undefined reference to `tensorflow::Tensor::~Tensor()'\r\ntl_classifier.cc:(.text+0x2293): undefined reference to `tensorflow::Tensor::~Tensor()'\r\ntl_classifier.cc:(.text+0x22e2): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'\r\n", "comments": ["Does it work with `-ltensorflow_cc` rather than `-ltensorflow` (i.e. the C API rather than the C++ API)? ", "@allenlavoie No, it does not work with -ltensorflow_cc. I did not try to link with C library -ltensorflow, since the code call some functions not in the C API. I did test C library with some sample code, and it works. Thanks!\r\n", "Great, sounds like this isn't a bug, so I'm closing.", "Re-opening, since apparently you were referring to the C++ API. Sorry about that!\r\n\r\nAs @asimshankar suggested, you can define protocol buffers by linking in libtensorflow_framework.so. Everything else should come from libtensorflow_cc.so (or whatever you've renamed it to). ", "@allenlavoie Follow @asimshankar suggestion I am able to link with libtensorflow_cc successfully. Issue can be closed. Thanks!", "it works for me. \r\ng++  ...  -ltensorflow_cc  -ltensorflow_framework\r\n\r\n Thanks a lot!", "> it works for me.\r\n> g++ ... -ltensorflow_cc -ltensorflow_framework\r\n> \r\n> Thanks a lot!\r\n\r\nyes,it works done !! thak~\r\n", "i have the same problem with current tensorflow master (f5e0f9d520260b3a6009a9e792cf2316dc37b8e3 22 Mar 2019), tried to link against bazel build tensorflow.dll.if.lib (dll was successfully build) , source is basically the LabelImage sample. errors below. It does not matter if i use tensorflow_cc.dll.if.lib instead\r\n\r\nmissing functions are some ops (tensorflow::ops::ResizeBilinear,ExpandDims,Placeholder...) and things like \r\ntensorflow::Operation::Operation, ...\r\n\r\nthere is no [lib]tensorflow_framework to build anymore... \r\n\r\nlist below\r\n----------\r\n error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::Operation::Operation(class tensorflow::Node *)\" (??0Operation@tensorflow@@QEAA@PEAVNode@1@@Z)\r\n error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::Input::Initializer::Initializer(class std::initializer_list<struct tensorflow::Input::Initializer> const &)\" (??0Initializer@Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z)\r\n error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::Scope::~Scope(void)\" (??1Scope@tensorflow@@QEAA@XZ)\r\n error LNK2001: unresolved external symbol \"public: static class tensorflow::Scope __cdecl tensorflow::Scope::NewRootScope(void)\" (?NewRootScope@Scope@tensorflow@@SA?AV12@XZ)\r\n error LNK2001: unresolved external symbol \"public: class tensorflow::Status __cdecl tensorflow::Scope::ToGraphDef(class tensorflow::GraphDef *)const \" (?ToGraphDef@Scope@tensorflow@@QEBA?AVStatus@2@PEAVGraphDef@2@@Z)\r\n error LNK2001: unresolved external symbol \"private: class tensorflow::Scope __cdecl tensorflow::Scope::WithOpNameImpl(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const \" (?WithOpNameImpl@Scope@tensorflow@@AEBA?AV12@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)\r\n error LNK2001: unresolved external symbol \"class tensorflow::Output __cdecl tensorflow::ops::Const(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)\" (?Const@ops@tensorflow@@YA?AVOutput@2@AEBVScope@2@AEBUInitializer@Input@2@@Z)\r\n error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::ResizeBilinear::ResizeBilinear(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0ResizeBilinear@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\r\n error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::ExpandDims::ExpandDims(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0ExpandDims@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\r\n error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::Placeholder::Placeholder(class tensorflow::Scope const &,enum tensorflow::DataType)\" (??0Placeholder@ops@tensorflow@@QEAA@AEBVScope@2@W4DataType@2@@Z)\r\n error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::Cast::Cast(class tensorflow::Scope const &,class tensorflow::Input,enum tensorflow::DataType)\" (??0Cast@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@W4DataType@2@@Z)\r\n error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::Div::Div(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0Div@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\r\n error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::Subtract::Subtract(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0Subtract@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\r\n error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::TopK::TopK(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0TopK@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\r\n error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::SessionOptions::SessionOptions(void)\" (??0SessionOptions@tensorflow@@QEAA@XZ)\r\n error LNK2001: unresolved external symbol \"class tensorflow::Status __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &,class tensorflow::Session * *)\" (?NewSession@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@PEAPEAVSession@1@@Z)\r\n error LNK2001: unresolved external symbol \"class tensorflow::Session * __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &)\" (?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@Z)", "> i have the same problem with current tensorflow master ([f5e0f9d](https://github.com/tensorflow/tensorflow/commit/f5e0f9d520260b3a6009a9e792cf2316dc37b8e3) 22 Mar 2019), tried to link against bazel build tensorflow.dll.if.lib (dll was successfully build) , source is basically the LabelImage sample. errors below. It does not matter if i use tensorflow_cc.dll.if.lib instead\r\n> \r\n> missing functions are some ops (tensorflow::ops::ResizeBilinear,ExpandDims,Placeholder...) and things like\r\n> tensorflow::Operation::Operation, ...\r\n> \r\n> there is no [lib]tensorflow_framework to build anymore...\r\n> \r\n> ## list below\r\n> error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::Operation::Operation(class tensorflow::Node *)\" (??0Operation@tensorflow@@QEAA@PEAVNode@1@@z)\r\n> error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::Input::Initializer::Initializer(class std::initializer_list const &)\" (??0Initializer@Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@z)\r\n> error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::Scope::~Scope(void)\" (??1Scope@tensorflow@@QEAA@XZ)\r\n> error LNK2001: unresolved external symbol \"public: static class tensorflow::Scope __cdecl tensorflow::Scope::NewRootScope(void)\" (?NewRootScope@Scope@tensorflow@@sa?AV12@XZ)\r\n> error LNK2001: unresolved external symbol \"public: class tensorflow::Status __cdecl tensorflow::Scope::ToGraphDef(class tensorflow::GraphDef *)const \" (?ToGraphDef@Scope@tensorflow@@qeba?AVStatus@2@PEAVGraphDef@2@@z)\r\n> error LNK2001: unresolved external symbol \"private: class tensorflow::Scope __cdecl tensorflow::Scope::WithOpNameImpl(class std::basic_string<char,struct std::char_traits,class std::allocator > const &)const \" (?WithOpNameImpl@Scope@tensorflow@@aeba?AV12@AEBV?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@@z)\r\n> error LNK2001: unresolved external symbol \"class tensorflow::Output __cdecl tensorflow::ops::Const(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)\" (?Const@ops@tensorflow@@YA?AVOutput@2@AEBVScope@2@AEBUInitializer@Input@2@@z)\r\n> error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::ResizeBilinear::ResizeBilinear(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0ResizeBilinear@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\r\n> error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::ExpandDims::ExpandDims(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0ExpandDims@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\r\n> error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::Placeholder::Placeholder(class tensorflow::Scope const &,enum tensorflow::DataType)\" (??0Placeholder@ops@tensorflow@@QEAA@AEBVScope@2@W4DataType@2@@z)\r\n> error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::Cast::Cast(class tensorflow::Scope const &,class tensorflow::Input,enum tensorflow::DataType)\" (??0Cast@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@W4DataType@2@@z)\r\n> error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::Div::Div(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0Div@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\r\n> error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::Subtract::Subtract(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0Subtract@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\r\n> error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::ops::TopK::TopK(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0TopK@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\r\n> error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::SessionOptions::SessionOptions(void)\" (??0SessionOptions@tensorflow@@QEAA@XZ)\r\n> error LNK2001: unresolved external symbol \"class tensorflow::Status __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &,class tensorflow::Session * *)\" (?NewSession@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@PEAPEAVSession@1@@z)\r\n> error LNK2001: unresolved external symbol \"class tensorflow::Session * __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &)\" (?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@z)\r\n\r\nhave u solved this problem? i've encountered this problem on windows , successfully generate libtensorflow.dll, but still receive some link errors. "]}, {"number": 14631, "title": "Fixing download_dependencies.sh bugs for generating TFLite iOS exmaples.", "body": "Root cause: The script downloads files for building TFLite for iOS\r\nexample. It writes to `downloads/` directory and conflicts with the\r\nvisibility rule \"**/*\" in BUILD", "comments": ["Can one of the admins verify this patch?", "@petewarden @aselle Could you review this?", "Fixing more bugs in this PR:\r\n* `echo cp` is a mistake in the script\r\n* The files are flattened in the same structure, so use `models/` instead of `models/models/`. \r\n\r\nTo verify this:\r\n* `git clean -fdx` to clean all local files. \r\n* run `tensorflow/contrib/lite/download_dependencies.sh`\r\n* Verify that you see \"download_dependencies.sh completed successfully\", so the script is completed. \r\n* Verify these files are downloaded to correct location. \r\n * tensorflow/contrib/lite/examples/ios/camera/data/labels.txt\r\n * tensorflow/contrib/lite/examples/ios/camera/data/mobilenet_quant_v1_224.tflite\r\n * tensorflow/contrib/lite/examples/ios/simple/data/labels.txt\r\n * tensorflow/contrib/lite/examples/ios/simple/data/mobilenet_v1_1.0_224.tflite", "This still didn't work. I'm going to withdraw the PR for now and resend it later. "]}, {"number": 14630, "title": "Branch 175983704", "body": "", "comments": ["Ignoring Jenkins failures since we're switching to the internal testing infra."]}, {"number": 14629, "title": "Documentation error in attention_wrapper.py", "body": "In tf.contrib.seq2seq.attention_wrapper.py file, in line 295, it should be [batch_size, 1, max_time], instead of [batch_time, ...], hope to fix it soon!! Thanks !!", "comments": ["@zzw922cn I'm a little confused by your suggestion. The current line 295 is about the input shapes. But I suspect that you mean the description of the output shape, which should perhaps be \"[batch_size, 1, max_time]\" instead of the current \"[batch_time, 1, max_time]\" (which doesn't make sense because batch_time doesn't occur elsewhere). Have I got that right?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "@cy89 yes!!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Marking as contributions welcome to fix the comment.", "Created a PR #16642 to try to address this issue, could you please take a look? @reedwm "]}, {"number": 14628, "title": "Simplify imports", "body": "", "comments": ["Can one of the admins verify this patch?", "This is likely intentional and this change would modify the imports."]}, {"number": 14627, "title": "R1.4", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 14626, "title": "[FR] Target for Cortex-M7", "body": "With the recent availability of TensorFlow Lite, it seems now possible to target embedded systems other than mobiles. [Cortex-M7](https://developer.arm.com/products/processors/cortex-m/cortex-m7) is the highest performing processor in the Cortex MCU family has ICs from [several](http://www.st.com/en/microcontrollers/stm32-high-performance-mcus.html?querycriteria=productId=SC2154) [big](https://www.microchip.com/design-centers/32-bit/sam-32-bit-mcus/sam-s-mcus) [names](https://www.nxp.com/products/processors-and-microcontrollers/arm-based-processors-and-mcus/kinetis-cortex-m-mcus/v-seriesreal-time-ctlm0-plus-m4-m7/kinetis-kv5x-240-mhz-motor-control-and-power-conversion-ethernet-mcus-based-on-arm-cortex-m7:KV5x). Would it be feasible to get a target for the M7? For those interested in playing around with one, I'd suggest the ST [NUCLEO-F767ZI](http://www.st.com/content/st_com/en/products/evaluation-tools/product-evaluation-tools/mcu-eval-tools/stm32-mcu-eval-tools/stm32-mcu-nucleo/nucleo-f767zi.html) as it is affordable and available (I got one at an event.) Its bigger brother, the [NUCLEO-H743ZI](http://www.st.com/content/st_com/en/products/evaluation-tools/product-evaluation-tools/mcu-eval-tools/stm32-mcu-eval-tools/stm32-mcu-nucleo/nucleo-h743zi.html), doesn't seem to be available just yet.", "comments": ["Deploying to embedded systems is definitely in the roadmap. Absent a proper Bazel toolchain for the M7, the easiest path is to modify the provided makefile. I think the M7 doesn't have NEON support, so you'd also need to change the code in kernels/*.cc to use either reference ops or neon-free versions. We are hoping to make this process easier in the future.", "Thanks. I started poking around and it will take a fair bit of time for me to get familiar with the codebase in order to do any such port. Hopefully a new process would make that easier, or someone else in the community has the chops and desire!", "Removing myself (it was a mis-click)", "Any updates on the new process to add new platforms?", "The makefile can now do a basic (no op) build. Just do make CROSS=stm32f7\r\n", "@beriberikix have you tried this lately?  As @aselle mentions, the Makefile build has been getting some updates and \"make -f tensorflow/contrib/lite/Makefile CROSS=stm32f7 micro\" has worked for me for building an arm library suitable for deployment on embedded systems.", "Nagging Assignees @aselle, @rockyrhodes: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks @asellem, @rockyrhodes! I'd like to give it a go. Do you have any suggestions of an example to try? Or better yet, has anyone written up a quickstart? For example, I don't see a Makefile in the `lite` directory. I'm assuming there's some process to generate it but I couldn't figure out how.", "Pete Warden just checked in some new code that you should try.  Take a look at the README.md file here and give it a go: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/experimental/micro\r\n\r\nThis code is still pretty specific for one particular micro platform, but we're planning on expanding support to others.", "in tensorflow 2.x, complie tflite for stm32f7 is always error. Anyone success in building tflite 2.x for stm32f7?"]}, {"number": 14625, "title": "float16 support for separable_convolutions", "body": "### System information\r\n- **Have I written custom code**:\r\nNo\r\n- **OS Platform and Distribution **:\r\n16.04\r\n- **TensorFlow installed from**:\r\nfrom pip package\r\n- **TensorFlow version**:\r\n1.4.0\r\n- **Python version**: \r\n3.5.2\r\n- **CUDA/cuDNN version**:\r\nCUDA 8.0, cuDNN 6.0\r\n- **GPU model and memory**:\r\n1080Ti with 12GB Memory\r\n- **Exact command to reproduce**:\r\n\r\nIt appears that half precision support is still not available for separable_convolutions even with tf 1.4.0 release.\r\n\r\n```\r\ninputs_32 = tf.placeholder(tf.float32, shape=(1,16,16,3))\r\ninputs_16 = tf.placeholder(tf.float16, shape=(1,16,16,3))\r\nslim.separable_conv2d(inputs_32,16,[3,3],depth_multiplier=1)\r\nslim.separable_conv2d(inputs_16,16,[3,3],depth_multiplier=1)\r\n```\r\n\r\nThe first call succeeds (as expected), while the second fails with the following error:\r\n`TypeError: Value passed to parameter 'input' has DataType float16 not in list of allowed values: float32, float64`\r\n\r\nI also tried that with tf.nn.separable_conv2d(.), but with same results.\r\n\r\n**Full Error Traceback**\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-6-61a1fd560923> in <module>()\r\n----> 1 slim.separable_conv2d(inputs_16,16,[3,3],depth_multiplier=1)\r\n\r\n~/.virtualenvs/p3.5-tf1.4/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)\r\n    179       current_args = current_scope[key_func].copy()\r\n    180       current_args.update(kwargs)\r\n--> 181     return func(*args, **current_args)\r\n    182   _add_op(func)\r\n    183   setattr(func_with_args, '_key_op', _key_op(func))\r\n\r\n~/.virtualenvs/p3.5-tf1.4/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py in separable_convolution2d(inputs, num_outputs, kernel_size, depth_multiplier, stride, padding, data_format, rate, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)\r\n   2500           _scope=sc,\r\n   2501           _reuse=reuse)\r\n-> 2502       outputs = layer.apply(inputs)\r\n   2503 \r\n   2504       # Add variables to collections.\r\n\r\n~/.virtualenvs/p3.5-tf1.4/lib/python3.5/site-packages/tensorflow/python/layers/base.py in apply(self, inputs, *args, **kwargs)\r\n    669       Output tensor(s).\r\n    670     \"\"\"\r\n--> 671     return self.__call__(inputs, *args, **kwargs)\r\n    672 \r\n    673   def _add_inbound_node(self,\r\n\r\n~/.virtualenvs/p3.5-tf1.4/lib/python3.5/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    573         if in_graph_mode:\r\n    574           self._assert_input_compatibility(inputs)\r\n--> 575         outputs = self.call(inputs, *args, **kwargs)\r\n    576 \r\n    577         if outputs is None:\r\n\r\n~/.virtualenvs/p3.5-tf1.4/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py in call(self, inputs)\r\n    982         padding=self.padding.upper(),\r\n    983         rate=self.dilation_rate,\r\n--> 984         data_format=utils.convert_data_format(self.data_format, ndim=4))\r\n    985 \r\n    986     if self.use_bias:\r\n\r\n~/.virtualenvs/p3.5-tf1.4/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py in separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, rate, name, data_format)\r\n    488         padding=padding,\r\n    489         data_format=data_format,\r\n--> 490         op=op)\r\n    491 \r\n    492     return nn_ops.conv2d(\r\n\r\n~/.virtualenvs/p3.5-tf1.4/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py in with_space_to_batch(input, dilation_rate, padding, op, filter_shape, spatial_dims, data_format)\r\n    343                              spatial_dims=spatial_dims,\r\n    344                              data_format=data_format)\r\n--> 345   return new_op(input, None)\r\n    346 \r\n    347 \r\n\r\n~/.virtualenvs/p3.5-tf1.4/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py in __call__(self, inp, filter)\r\n    497 \r\n    498   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin\r\n--> 499     return self.call(inp, filter)\r\n    500 \r\n    501 \r\n\r\n~/.virtualenvs/p3.5-tf1.4/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py in <lambda>(inp, _)\r\n    334 \r\n    335   def build_op(num_spatial_dims, padding):\r\n--> 336     return lambda inp, _: op(inp, num_spatial_dims, padding)\r\n    337 \r\n    338   new_op = _WithSpaceToBatch(input_shape,\r\n\r\n~/.virtualenvs/p3.5-tf1.4/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py in op(input_converted, _, padding)\r\n    480           padding=padding,\r\n    481           data_format=data_format,\r\n--> 482           name=\"depthwise\")\r\n    483 \r\n    484     depthwise = nn_ops.with_space_to_batch(\r\n\r\n~/.virtualenvs/p3.5-tf1.4/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py in depthwise_conv2d_native(input, filter, strides, padding, data_format, name)\r\n   1159     _, _, _op = _op_def_lib._apply_op_helper(\r\n   1160         \"DepthwiseConv2dNative\", input=input, filter=filter, strides=strides,\r\n-> 1161         padding=padding, data_format=data_format, name=name)\r\n   1162     _result = _op.outputs[:]\r\n   1163     _inputs_flat = _op.inputs\r\n\r\n~/.virtualenvs/p3.5-tf1.4/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    607               _SatisfiesTypeConstraint(base_type,\r\n    608                                        _Attr(op_def, input_arg.type_attr),\r\n--> 609                                        param_name=input_name)\r\n    610             attrs[input_arg.type_attr] = attr_value\r\n    611             inferred_from[input_arg.type_attr] = input_name\r\n\r\n~/.virtualenvs/p3.5-tf1.4/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)\r\n     58           \"allowed values: %s\" %\r\n     59           (param_name, dtypes.as_dtype(dtype).name,\r\n---> 60            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\n     61 \r\n     62 \r\n\r\nTypeError: Value passed to parameter 'input' has DataType float16 not in list of allowed values: float32, float64\r\n```\r\n\r\n", "comments": ["I'm assuming this is a Feature Request for float16 in slim.separable_conv2d. Is that right?", "@sguada can you please comment on plans for float16 support in slim, particularly for separable convs?", "Yes, this is a feature request. My initial understanding was that float16 was already available for most common layers so I wasn't sure whether to treat it as a feature request or a bug.\r\n\r\nSo yes, I would like to know if there are any current plans for float16 in either slim.separable_conv2d or tf.nn.separable_conv2d.", "@afroze100 I think ` tf.layers.separable_conv2d` has supported float16 in master branch, you can test it on [tf-nightly](https://github.com/tensorflow/tensorflow#installation).", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Float16 support for similar op tf.nn.depthwise_conv2d as well would be great, on top of  tf.nn.separable_conv2d/tf.layers.separable_conv2d. Thanks,", "@rudrapoudel Would you like to open a new issue for the feature request?", "Marking contributions welcome.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Looks like the issue has been resolved in the latest tf. Will close the issue but feel free to reopen if needed."]}, {"number": 14624, "title": "Bug: using regularizer for shared variables in tf.cond branches ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.1\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nSetting the `regularizer` of a shared variable in `tf.cond` branches gives an unexpected behaviour - only one copy of the regularization op is added to `tf.GraphKeys.REGULARIZATION_LOSSES`. This is different from adding operations to a collection explicitly. And optimizing the regularization loss doesn't raise an error.\r\n\r\n\r\n### Source code\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.layers.python.layers import regularizers\r\n\r\n\r\ndef regularised_model(is_training):\r\n    scope_name = 'foo'\r\n    with tf.variable_scope(scope_name) as scope:\r\n        if is_training:\r\n            scope.reuse_variables()\r\n        y = tf.get_variable(\r\n            name='y', shape=(),\r\n            initializer=tf.constant_initializer(1.0),\r\n            regularizer=regularizers.l2_regularizer(scale=0.1, scope=scope_name))\r\n        reg = tf.nn.l2_loss(y) * 0.1\r\n        tf.add_to_collection('test_collection', reg)\r\n    return y\r\n\r\nbinary_flag = tf.placeholder(dtype=tf.bool, shape=())\r\ny_cond = tf.cond(binary_flag,\r\n                 lambda: regularised_model(False),\r\n                 lambda: regularised_model(True))\r\n\r\nreg_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\r\nassert(len(reg_loss) == 1)\r\noutput = y_cond + reg_loss[0]\r\nopt = tf.train.AdamOptimizer(0.1).minimize(output)\r\n\r\nreg_test = tf.get_collection('test_collection')\r\nassert(len(reg_test) == 2)\r\noutput_reg_true = y_cond + reg_test[0]\r\noutput_reg_false = y_cond + reg_test[1]\r\n\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    print(sess.run(output_reg_true, feed_dict={binary_flag: True})) # 1.05\r\n    print(sess.run(output_reg_false, feed_dict={binary_flag: False})) # 1.05\r\n\r\n    print(sess.run(output, feed_dict={binary_flag: True})) # 1.05\r\n    \r\n    print(sess.run([opt], feed_dict={binary_flag: True})) # [None]\r\n    print(sess.run([opt], feed_dict={binary_flag: False})) # [None]\r\n\r\n    print(sess.run(output, feed_dict={binary_flag: False})) # Error: Retval[0] does not have value\r\n\r\n```\r\n### Log\r\n```\r\n1.05\r\n1.05\r\n1.05\r\n[None]\r\n[None]\r\nTraceback (most recent call last):\r\n  File \"test_tf_cond.py\", line 43, in <module>\r\n    print(sess.run(output, feed_dict={binary_flag: False})) # Error: Retval[0] does not have value\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "So do you found any workaround for this?\r\nThanks"]}, {"number": 14623, "title": "[BUG] tf.while_loop creates a seg-fault when setting parallel_iterations to high values", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: both\r\n- **TensorFlow version (use command below)**: 1.3 / 1.4\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 8\r\n- **GPU model and memory**: Tesla P100-PCIE-16GB\r\n- **Exact command to reproduce**:\r\n### Describe the problem\r\n\r\nThe following code crashes on all of my test systems, with tensorflow 1.3 or 1.4.\r\nDoesn't matter if build with MKL or without or using the pip-version.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nnum_dim = 20\r\nFORMAT = tf.float64 #32 or 64 does not matter\r\nn = 3500 #reducing the number results in normal execution\r\n\r\ndef simpeLoop(alpha):\r\n      i = tf.constant(0)\r\n      m0 = tf.zeros([num_dim, 1], dtype=FORMAT)\r\n      cond = lambda i, m: i < n\r\n      def body(ic, vec): \r\n            #a meaningless example, summing up the first num_dim elements of a vector \r\n            op = alpha[ic * num_dim:(ic + 1) * num_dim, :]\r\n            # with tf.control_dependencies([op]): #if you uncomment this, it will not fault!\r\n            return ic + 1, vec + op\r\n      loop = tf.while_loop(cond, body, [i, m0], parallel_iterations=10**4, back_prop=False)\r\n      return loop[1]\r\n\r\nwith tf.device('/cpu:0'):\r\n      alpha = tf.placeholder(FORMAT, [None, 1], name=\"alpha\")\r\n      fdict = {\r\n          alpha: np.random.rand(n * num_dim, 1),\r\n      }\r\n      op = simpeLoop(alpha)\r\n      op = tf.reduce_sum(op) #not necessary for the seg fault\r\n      init = tf.global_variables_initializer()\r\n\r\nconfig = tf.ConfigProto()\r\nthreads = 1\r\nconfig.intra_op_parallelism_threads = threads\r\nconfig.inter_op_parallelism_threads = threads\r\nsess = tf.Session(config=config)\r\nsess.run(init, feed_dict=fdict)\r\nprint(\"init\")\r\nprint(sess.run(op, feed_dict=fdict))\r\n```\r\nAdding the control_dependencies results in normal execution.\r\n\r\nThe documentation states \r\n\r\n> The maximum number of parallel iterations can be controlled by parallel_iterations, which gives users some control over memory consumption and execution order\r\n\r\nso I would expect parallel_iterations to be some kind of upper bound. \r\nSmall tests like presented in  #12937 showed that increasing the parallel_iterations number results in higher performance. But setting it to high results in a seg fault.\r\n \r\n\r\n\r\n\r\n", "comments": ["Makes sense. The execution engine probably runs into a stack overflow or something. `10**4` is a lot!\r\n\r\nJust for fun, you could plot the runtime in seconds as a function of `parallell_iterations` from 1 to the max before you segfault? \ud83d\ude03", "I guess its still an issue but I didn't have time to make the plot. ", "Digging a bit into the core code, It might be that the implementation of executor.cc is the problem:\r\nThe [FrameState](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.cc#L935\r\n) has an attribute called iterations, which is always resized to max_parallel_iterations:\r\n[define](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.cc#L1001) and [resize](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.cc#L2310).\r\nIterations is a [gtl::inlined_vector](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/gtl/inlined_vector.h#L330) which sets its internal storage when resized. \r\nIterations is only a vector of pointer, so 10**4 should not be a problem. \r\n\r\nBut maybe this provides a starting point for someone with a better overview of the internal of tf.\r\n", "Assigning to @skye, who knows much more about control flow than I do.", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!"]}, {"number": 14622, "title": "ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory", "body": "I'm trying to run Tensorflow-gpu through virtualenv via pip3 in Ubuntu 16.04. I have installed Cuda-9.0 and cuDNN v7.0.3, then tested both and they are working fine. However, when attempting to import Tensorflow in Python I get the following error:\r\n\r\n> Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n\r\nI located libcublas.so in /usr/local/cuda/lib64. However, I see that it references a new version of the library (9.0.176), My question is whether something simple can be done (like creating a symbolic link to the library with the name of libcublas.so.8.0) or I have to wait for an update in TF that can run with Cuda9/cuDNN 7\r\n\r\nCheers!\r\n", "comments": ["+1\r\n", "i have met the same problem. it seems that cuda 9.0 is not supported for Tensorflow-gpu through virtualenv for now. You may try to install through source code. ", "+1\r\n\r\nas @pangzhan27 mentioned TF doesn't support CUDA 9.0 yet. \r\nhttps://devtalk.nvidia.com/default/topic/1026198/cuda-9-0-importerror-libcublas-so-8-0/\\\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/12052", "+1", "+1", "+1\r\n", "+1", "+1", "+1\r\n", "You might want to try this [tensorflow-gpu-v1.4-cuda-v9-cudnn-v7.whl](https://github.com/mind/wheels/releases/tag/tf1.4-gpu-cuda9) wheel as well as\r\n```bash\r\nsudo echo \"/usr/local/cuda-9.0/lib64\" >> /etc/ld.so.conf.d/cuda.conf\r\nsudo ldconfig\r\n```\r\nas pointed out in the [nvidia forums](https://devtalk.nvidia.com/default/topic/845363/libcublas-so-7-0-cannot-open-shared-object-file/). Seems to work on an Ubuntu 16.04 machine.", "+1", "+1", "+1", "@gunan FYI. Closing since @mamrehn appears to have posted a working resolution. We should fix this if possible.", "Does this work with the new nightly builds? It doesn't seem to work with the `tf-nightly-gpu`", "@bfan1256 At least it works with the latest tensorflow v1.4.1 ([tensorflow-gpu-v1.4.1-cuda-v9-cudnn-v7.whl](https://github.com/mind/wheels/releases/tag/tf1.4.1-gpu-cuda9)). I'm afraid you'd have to look into their compilation process and adapt it to the nightly version yourself to test it.", "Tensorflow doesn't support Cuda 9.0 yet. \r\n\r\nTry to use Cuda-8.0 by updating the LD_LIBRARY_PATH with the following\r\nsudo ldconfig /usr/local/cuda-8.0/lib64", "+1 waiting for Cuda 9.0 support", "Our nightlies are all built against cuda 9 now.\r\nWe do not have windows GPU nightlies yet, but if you build from master you can build and run TF with CUDA 9 on windows, too.", "@gunan thanks! Works like a charm with nightly on cuda-9.1", "Does supporting CUDA 9 also means removing support for CUDA 8? I am getting this error with the latest build\r\n```\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/abhishs8/Research/AdversarialAttacks/.env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/abhishs8/Research/AdversarialAttacks/.env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/abhishs8/Research/AdversarialAttacks/.env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/abhishs8/Research/AdversarialAttacks/.env/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/abhishs8/Research/AdversarialAttacks/.env/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n```", "For prebuilt binaries, yes. The latest prebuilt binaries only support cuda\n9.0. If you need to use with cuda 8.0, you either need to use an old\nversion or build from source.\n\nOn Jan 9, 2018 5:56 PM, \"abhishek singh\" <notifications@github.com> wrote:\n\n> Does supporting CUDA 9 also means removing support for CUDA 8? I am\n> getting this error with the latest build\n>\n> ImportError: Traceback (most recent call last):\n>   File \"/home/abhishs8/Research/AdversarialAttacks/.env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n>     from tensorflow.python.pywrap_tensorflow_internal import *\n>   File \"/home/abhishs8/Research/AdversarialAttacks/.env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n>     _pywrap_tensorflow_internal = swig_import_helper()\n>   File \"/home/abhishs8/Research/AdversarialAttacks/.env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n>   File \"/home/abhishs8/Research/AdversarialAttacks/.env/lib/python3.6/imp.py\", line 243, in load_module\n>     return load_dynamic(name, filename, file)\n>   File \"/home/abhishs8/Research/AdversarialAttacks/.env/lib/python3.6/imp.py\", line 343, in load_dynamic\n>     return _load(spec)\n> ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14622#issuecomment-356474760>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOegpf9SQleYVQpTr7raJIjErvi6Zks5tJBjTgaJpZM4Qgikl>\n> .\n>\n", "Had the exact same problem. But building from source with bazel as per [the official instructions](https://www.tensorflow.org/install/install_sources) works well. I now run TensorFlow 1.5 with CUDA 9.0 and cuDNN 7.0 on Ubuntu 17.10.", "+1\r\nI have same problem no matter what i install cuda-8.0 or cuda-9.1\r\n\r\nSolove the problem by recopy cudnn/lib64 to cuda/lib64  in cuda-8.0 ver", "With tensoflow-gpu (1.5.0) + cuda 9.0 + ubuntu 16.04 in virtualenv: the same error when importing tensorflow in the virtualenv's python3 interpreter. I think it is because it is not enough to set the `LD_LIBRARY_PATH` properly; You should run the `ldconfig` after installing the cuda toolkit. So I put the cuda lib path `/usr/local/cuda-9.0/lib64` (in my config) into the newly created file `/etc/ld.so.conf.d/cuda.conf`, issued an `ldconfig` and now it works finally.", "On my computer I had :\r\n\r\n    cudnn5.1,   \r\n    cuda8.0\r\n\r\n\r\nWhen importing tensorflow, I had the same error ImportError: \r\n\r\n> libcublas.so.8.0: cannot open shared object file: No such file or\r\n> directory\r\n\r\nand even \r\n\r\n> libcublas.so.9.0: cannot open shared object file: No such file or\r\n> directory\r\n\r\n\r\nTo overcome the second issue I installed `tensorflow 1.4` instead of 1.6 \r\n and for the first I did\r\n$ export PATH=\"$PATH:/usr/local/cuda-8.0/bin\" \r\n$ export LD_LIBRARY_PATH=\"/usr/local/cuda-8.0/lib64\"\r\n\r\n\r\nBut then I had another issue : \r\n\r\n> libcudnn.so.6: cannot open shared object file: No such file or\r\n> directory\r\n\r\nThis was because I had cudnn5.1 as required but actually it needs cudnn6\r\n\r\nHere are the steps to uninstall cudnn5.1 and install cudnn6:\r\n\r\n1- Uninstall cudnn 5.1\r\n    rm -f /usr/include/cudnn.h\r\n    rm -f /usr/lib/x86_64-linux-gnu/libcudnn\r\n    rm -f /usr/local/cuda-/lib64/libcudnn\r\n\r\n2- Install cudnn6\r\n\r\nAfter having subscribed to nvdia, go to here https://developer.nvidia.com/rdp/cudnn-download and download cudnn6 for cuda8 and then go to the folder where you downloaded the cudnn and do :\r\n\r\n    $ tar xvzf cudnn-8.0-linux-x64-v5.1-ga.tgz\r\n    $ sudo cp -P cuda/include/cudnn.h /usr/local/cuda/include\r\n    $ sudo cp -P cuda/lib64/libcudnn /usr/local/cuda/lib64\r\n    $ sudo chmod a+r /usr/local/cuda/include/cudnn.h \r\n    /usr/local/cuda/lib64/libcudnn*\r\n\r\nNow you should have tensorflow\r\n\r\nTry it on typing in the console:\r\n\r\n    $python\r\n    import tensorflow\r\n\r\nIf you want to work in anaconda and the error persists , try :\r\n\r\n    $jupyter notebook --generate-config\r\n\r\nthen you can find the name of the directory where you have your config file (Ill call it\r\n\r\n) and open /jupyter_notebook_config.py and add at the top :\r\n\r\n    import os\r\n    c = get_config()\r\n    os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-8.0/lib64:usr/local/cuda-8.0/lib64/libcudart.so.8.0'\r\n    c.Spawner.env.update('LD_LIBRARY_PATH')\r\n\r\nNow it should work...\r\n\r\nHere are the websites that helped me:\r\n\r\nhttps://askubuntu.com/questions/952075/how-to-upgrade-tensorflow-to-v1-3-cudnn-cuda-upgrade\r\nhttps://developer.nvidia.com/rdp/cudnn-download\r\nhttps://medium.com/@ikekramer/installing-cuda-8-0-and-cudnn-5-1-on-ubuntu-16-04-6b9f284f6e77\r\nhttps://stackoverflow.com/questions/43984135/tensorflow-gpu-can-not-be-called-from-jupyterhub-jupyter-notebook-why\r\n"]}, {"number": 14621, "title": "Tensorflow Lite Error: when convert frozen Graphdef  to flatbuffer format (.lite) ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:master lastest\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:0.7\r\n\r\n### Describe the problem\r\nTensorflow Lite Error: when convert frozen Graphdef  to flatbuffer format (.lite) \r\n\r\nERROR\uff1a\r\n2017-11-16 19:38:54.835542: F tensorflow/contrib/lite/toco/tflite/export.cc:294] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: ArgMax.\r\n\r\nCan anyone tell me how to solve this problem? \r\n\r\n", "comments": ["Unfortunately TF Lite doesn't support ArgMax yet. See [the list of supported ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md) \r\n\r\nUntil it gets added, your best bet is to change your training script to avoid using ArgMax. You can also set --output_array in toco in a way that avoids the argmax node, but that might not be what you want anyway.", "ArgMax is supported now. I'm closing this, but feel free to reopen if you still encounter problems."]}, {"number": 14620, "title": "tf r1.4 bazel build error: nsync?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.4\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**: 0.7.0 \r\n- **GCC/Compiler version (if compiling from source)**: ****gcc (Ubuntu 5.4.1-2ubuntu1~16.04) 5.4.1 20160904\r\n- **CUDA/cuDNN version**: 8.0 / 6 \r\n\r\n- **Exact command to reproduce**:\r\n    sudo bazel build -c opt --config=cuda --copt=\"-mtune=native\" --copt=\"-O3\" tensorflow:libtensorflow_cc.so tensorflow:libtensorflow.so --genrule_strategy=standalone --spawn_strategy=standalone\r\n\r\n### Problem\r\nI use tensorflow c++ api to run some RL applications.\r\nWhen I run my project, I get this error. It was okay with tensorflow r1.3 or older versions.\r\n\r\n/home/joonho/workspace/rai/deepLearning/tensorflow/tensorflow/core/platform/default/mutex.h:25:22: fatal error: nsync_cv.h: No such file or directory\r\n #include \"nsync_cv.h\"\r\n                      ^\r\n\r\nI think there's linking error. I clearly have the headers in my environment.  \r\nHow can I fix this? is this bug in tf r1.4 or am I doing something wrong?\r\n\r\njoonho@joonho-HP-Z440-Workstation:~$ locate nsync_cv.h\r\n/home/joonho/.cache/bazel/_bazel_root/4eb2082608889a2b4334c89631226226/external/nsync/public/nsync_cv.h\r\n/home/joonho/.virtualenvs/tensorflow/lib/python3.5/site-packages/external/nsync/public/nsync_cv.h\r\n/home/joonho/.virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/include/external/nsync/public/nsync_cv.h\r\n", "comments": ["I get across the same issue.", "Can you try with bazel 0.8?", "I resolve this by adding one more parameter which was found on the document for new ops... However I've forgotten which one...Seems that there is a lag between code implementation and document.", "@drpngx I tried with the latest version of Bazel and still the same error occurs.", "@WayneWang12  if you remember which op it is and how you fixed it, please let us know.\r\n/CC @MarkDaoust ", "It seems like you're not the only one hitting this error.\r\n\r\nA few people say  [these instructions](https://github.com/tensorflow/tensorflow/issues/12482#issuecomment-328829250) fix the problem.\r\n", "@drpngx Hi, I'm following this:\r\n```\r\nTF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')\r\nTF_LIB=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')\r\ng++ -std=c++11 -shared zero_out.cc -o zero_out.so -fPIC -I$TF_INC -I$TF_INC/external/nsync/public -L$TF_LIB -ltensorflow_framework -O2\r\n```\r\nThis solved my problem on ubuntu. As for mac os, I suggest you to try instructions on this document: https://www.tensorflow.org/extend/adding_an_op . Below there is some for mac os.", "Thanks @WayneWang12. So the fix is documented already, and people keep missing the instructions? [No, there's no mention of `nsync` in adding an op.] Anyone have good ideas on where we should add links to these instructions so people are more likely to find them?\r\n  ", "@MarkDaoust Hi Mark, I suggest there should be instructions on the [read.me](https://github.com/tensorflow/models/blob/master/tutorials/embedding/README.md) because I followed the instruction from [word2vec](https://www.tensorflow.org/tutorials/word2vec) here but the operation is different from [Adding an op](https://www.tensorflow.org/extend/adding_an_op). It took me a while to find the real solution.\r\n\r\nI think there might be some more places but I've only used the part of word2vec to build a recomendation system. Later if I learn more about tensorflow I'll try to report more if I get across to it.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I get this warning during tensorflow installation:\r\n\r\nWARNING: ~/tensorflow/tensorflow/core/BUILD:1781:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in ~/tensorflow/tensorflow/tensorflow.bzl:1044:30.\r\n\r\nThen when I run tensorflow in C++, it complains that the nsync library cannot be found, so I get and error:\r\n\r\n/usr/local/include/google/tensorflow/tensorflow/core/platform/default/mutex.h:25:22: fatal error: nsync_cv.h: No such file or directory"]}, {"number": 14619, "title": "Undefined symbols \"_cblas_sgemm\" on iOS", "body": "Hello,\r\n\r\nI downloaded the latest release 1.4.0 and builded it for iOS with build_all_ios.sh, and can successfully build libtensorflow-core.a, libprotobuf-lite.a and libprotobuf.a, and added the libtensorflow-core.a with -force_load\r\n\r\nbut error happened when link these libs to iOS project, the detail info as follows, can anybody know how to fix it? thanks\r\nUndefined symbols for architecture arm64:\r\n  \"_cblas_sgemm\", referenced from:\r\n      tensorflow::Conv2DUsingGemmOp<float, tensorflow::(anonymous namespace)::Im2ColConvFunctor<float, float, float, FastGemmFunctor<float, float, float> > >::Compute(tensorflow::OpKernelContext*) in libtensorflow-core.a(conv_ops_using_gemm.o)\r\n      tensorflow::FusedResizeConv2DUsingGemmOp<float, tensorflow::(anonymous namespace)::FusedResizeAndPadConvFunctor<float, float, float, FastGemmFunctor<float, float, float>, (tensorflow::(anonymous namespace)::SamplingMode)0>, true>::Compute(tensorflow::OpKernelContext*) in libtensorflow-core.a(conv_ops_fused.o)\r\n      tensorflow::FusedResizeConv2DUsingGemmOp<float, tensorflow::(anonymous namespace)::FusedResizeAndPadConvFunctor<float, float, float, FastGemmFunctor<float, float, float>, (tensorflow::(anonymous namespace)::SamplingMode)1>, false>::Compute(tensorflow::OpKernelContext*) in libtensorflow-core.a(conv_ops_fused.o)\r\nld: symbol(s) not found for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n", "comments": ["I don't think this is a lite issue. Maybe the iOS project is missing the Accelerate framework?", "Yes, it works after adding Accelerate framework\r\nThanks so much!!"]}, {"number": 14618, "title": "Error in tf.image.extract_glimpse documentation", "body": "Hi,\r\nI think there is an error into the documentation of the **tf.image.extract_glimpse** into the **offsets** definition:\r\n\r\n**offsets**: A Tensor of type float32. A 2-D integer tensor of shape [batch_size, 2] containing the **x, y** locations of the center of each window.\r\n\r\nAre you sure it is not **y,x** instead? Also into the previous field:\r\n\r\n**size**: A Tensor of type int32. A 1-D tensor of 2 elements containing the size of the glimpses to extract. **The glimpse height must be specified first, following by the glimpse width.**\r\n\r\nSo for **size** you place height (y) **before** width (x) which is the usual thing to do, but in **offsets** you require to have x (width) before y (height) which I think is wrong.\r\n\r\nThanks,\r\nAndrea", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce"]}, {"number": 14617, "title": "ValueError: graph_def is invalid at node u'decode/DecodeJpeg': Input tensor 'image_feed:0' Cannot convert a tensor of type float32 to an input of type string.", "body": "when i try to quantize the graph after optimize, i get the error \r\nValueError: graph_def is invalid at node u'decode/DecodeJpeg': Input tensor 'image_feed:0' Cannot convert a tensor of type float32 to an input of type string.\r\n\r\npython ./tensorflow/tools/quantization/quantize_graph.py \\\r\n--input=/Users/jie/tensorflow/models/models/im2txt/im2txt/model/train/optimize_graph.pb \\\r\n--output=/Users/jie/tensorflow/models/models/im2txt/im2txt/model/train/rounded_graph.pb \\\r\n--output_node_names=lstm/initial_state,softmax,lstm/state \\\r\n--mode=weights_rounded \r\n\r\n\r\n\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "i met the similar issue. i am using mac os 10.12.6. python 3.6.3. i am using the following command to get the quantized model from my retained inception v3 model.\r\n\r\n./transform_graph --in_graph=/path-to-model/retrained_inceptionv3_model.pb --out_graph=/path-to-model/quantized_retrained_inceptionv3_model.pb --inputs='DecodeJpeg/contents' --outputs='final_result' --transforms='strip_unused_nodes(type=float, shape=\"1,299,299,3\") fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights'\r\n\r\nand i use the generated model to do prediction of a image.\r\n\r\nthe output error is:\r\ngraph_def is invalid at node 'DecodeJpeg': Input tensor 'DecodeJpeg/contents:0' Cannot convert a tensor of type float32 to an input of type string\r\n\r\nplease help! thanks!\r\n\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "The tensorflow graph being supplied as an input is custom code. We need much information in cases like these to determine if it's a bug. Sometimes issues like these happen when migrating pbtxt files between TF versions. I can reopen if you can help us understand the bug. Otherwise please try  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow)."]}, {"number": 14616, "title": "help me with tensorflow!", "body": "wxf@wxf-K56L:~$ source activate tensorflow\r\n(tensorflow) wxf@wxf-K56L:~$ python\r\nPython 2.7.14 |Anaconda, Inc.| (default, Nov  8 2017, 22:44:41) \r\n[GCC 7.2.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/wxf/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/wxf/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/wxf/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/wxf/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/wxf/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/wxf/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\n", "comments": ["You did not include cuda library in your env.", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 14615, "title": "For Android, Tensorflow Lite  C++ interface and static library", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**: master latest\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.7\r\n\r\n### Describe the problem\r\nFor Android, Tensorflow Lite currently only supports Java interface calls, how do I use the C++ interface and static library? have any reference materials or guidance ? @aselle @petewarden\r\n", "comments": ["Can you please look at our C++ API documentation here ?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/apis.md", "Also, please take a look at how the JNI layer is implemented to provide Java support as an example of using the c++ api. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 14614, "title": "Which c++ source code Should I modify to allocate GPU memory dynamically?", "body": "I am using Spark with tensorflow. Spark make many tensorflow process, so I want tensorflow to allocate GPU memory dynamically. \r\nAdding the below code is **not workin**g for me\r\n```\r\nconfig = tf.ConfigProto() \r\nconfig.gpu_options.allow_growth=True \r\nsess = tf.Session(config=config)\r\n```\r\nAlthough I found  the way to set gpu memory fraction in 'tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc' ,\r\nI want to set 'allow_growth' in c++ source.\r\nWhich part shoud I modify?", "comments": ["solve it. \r\nchange the source code manually and build it again by bazel.", "@rayjang could you please describe in more details how did you do this?\r\ntensorflow::ConfigProto configProto;\r\n configProto.gpu_options();                   // ----> it gives to me only gpu_options without any recieved arrguments.\r\n\r\nthanks in advance.", "@kerolos:\r\ntensorflow::ConfigProto config;\r\nconfig.mutable_gpu_options()->set_allow_growth(...);"]}, {"number": 14613, "title": "tf.data.Iterator.from_string_handle() breaking behaviour in r1.4 compared to r1.3.1", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: +\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04\r\n- **GPU model and memory**: NVIDIA\u00ae Tesla\u00ae K80 (GCE)\r\n- **Exact command to reproduce**:\r\n\r\n### Context:\r\nSame setup/context as in the previous issue https://github.com/tensorflow/tensorflow/issues/12859 including this fix https://github.com/tensorflow/tensorflow/issues/12859#issuecomment-327783827.\r\nIn addition the training is now in a Multi-task Learning mode, so feedable contrib Iterator was used to accommodate different datasets for each task.\r\nSimplified snippet:\r\n~~~python\r\n        self.handle = tf.placeholder(tf.string, shape=[])\r\n\r\n        ...\r\n\r\n        # targets -> Multi-task training targets.\r\n        # datasets -> dict of Multi-task target tf.Datasets.\r\n        for target in self.targets:\r\n            self.datasets[target] = self.create_TFDataset()\r\n\r\n        self.iterator = tf.contrib.data.Iterator.from_string_handle(\r\n            self.handle,\r\n            self.datasets[targets[-1]].output_types,\r\n            self.datasets[targets[-1]].output_shapes)\r\n\r\n        # iter_init_ops and iter_handles -> init_ops & handles per each task.\r\n        for target in self.targets:\r\n            self.iterators[target] = self.datasets[target].make_initializable_iterator()\r\n            self.iter_init_ops[target] = self.iterators[target].initializer\r\n            self.iter_handles[target] = self.iterators[target].string_handle()\r\n\r\n        ...\r\n        # Within tf.train.MonitoredTrainingSession as mon_sess.\r\n        for target in targets:\r\n            # Get all target datasets handles.\r\n            handle[target] = mon_sess._coordinated_creator.tf_sess.run(\r\n                training_model.iter_handles[target])\r\n            # Init all target datasets.\r\n            mon_sess._coordinated_creator.tf_sess.run(training_model.iter_init_ops[target])\r\n\r\n        ...\r\n        # Training step for a specific target.\r\n        input_feed = {self.handle:handle[target]}\r\n        output_feed = [\r\n            self.update_ops[target],\r\n            self.losses[target], \r\n            self.metrics[target],\r\n        ]\r\n        outputs = session.run(output_feed, input_feed, options=options,\r\n                                             run_metadata=run_metadata)\r\n~~~\r\n### Problem:\r\nThis system worked flawlessly for tf 1.3 & tf 1.3.1. \r\nAfter a planned update this week to tf 1.4 the following Error would appear at absolute random (it might appear after 5 seconds or after a few hours of training):\r\n~~~console\r\n...\r\nW tensorflow/core/framework/op_kernel.cc:1192] Unavailable: Endpoint read failed\r\n\t [[Node: Model/Generate_BiRNN/BiRNN_Logic/bidirectional_rnn/fw/carry_w_S543 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:ps/replica:0/task:1/device:CPU:0\", send_device_incarnation=-458934800929363750, tensor_name=\"edge_9_Model/Generate_BiRNN/BiRNN_Logic/bidirectional_rnn/fw/carry_w\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\r\n\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/Select_G315 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device_incarnation=3243841103411587778, tensor_name=\"edge_4692_Training_Graph/Model/TARGET/TARGET_Metrics/Select\", tensor_type=DT_DOUBLE, _device=\"/job:worker/replica:0/task:0/device:CPU:0\"]()]]\r\n...\r\nW tensorflow/core/framework/op_kernel.cc:1192] Not found: Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\r\n\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]\r\n\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\r\n...\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\r\n\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]\r\n\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"dist_train.py\", line 684, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"dist_train.py\", line 640, in main\r\n    create_job()\r\n  File \"dist_train.py\", line 628, in create_job\r\n    run_worker(server, cluster)\r\n  File \"dist_train.py\", line 429, in run_worker\r\n    profile=profile\r\n  File \"/HARNN.py\", line 1576, in step_dist_gpu\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 521, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 892, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 967, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 952, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1024, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 827, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\r\n\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]\r\n\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nCaused by op 'Training_Graph/Model/IteratorFromStringHandle', defined at:\r\n  File \"dist_train.py\", line 684, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"dist_train.py\", line 640, in main\r\n    create_job()\r\n  File \"dist_train.py\", line 628, in create_job\r\n    run_worker(server, cluster)\r\n  File \"dist_train.py\", line 272, in run_worker\r\n    training_model.build_graph()\r\n  File \"/HARNN.py\", line 221, in build_graph\r\n    self._init_dataset()\r\n  File \"/HARNN.py\", line 329, in _init_dataset\r\n    self.datasets[self.targets[-1]].output_shapes)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 189, in from_string_handle\r\n    output_shapes=nest.flatten(output_shapes))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 662, in iterator_from_string_handle\r\n    output_types=output_types, output_shapes=output_shapes, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\r\n\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]\r\n\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\r\n~~~\r\n\r\n### Statement:\r\nWhat exactly have changed in tf.data.Iterator between versions r1.4 and r1.3.1 that causes such an unpleasant behaviour? What can be done to counteract `NotFoundError`?", "comments": ["I don't think there was any change that would cause this error. From the error message though it looks like the handle might have been captured (using `Iterator.to_string_handle()`) in a different session from which it is being used. Since you're using `tf.train.MonitoredTrainingSession`, is it possible that the session has been recreated? If that's the case, you'll need to make sure to recompute the handle\u2014and potentially reinitialize the iterator\u2014in the new session.\r\n\r\nA possible workaround, which might or might not work depending on the actual cause of the problem, would be to set a `shared_name` when you create the iterator. I believe that should make the handle string stable across restarts of the session (as long as it's not in a task that could have failed).", "Thanks for your suggestion @mrry!\r\n\r\n*MonitoredTrainingSession*: As far as I can tell sessions are not recreated and both `.from_string_handle()` and `string_handle()` are invoked before session is even created. I'm just running this on k8s on GKE with 10 GPUs in a data-parallel distributed fashion.\r\n\r\n*shared_name*: Your suggestion worked for a tf r1.4 binary! Thanks a lot. By creating shared name for each task it avoided `NotFoundError`.\r\n\r\nAlso, I tried to run it with tf r1.4 built from sources, but I keep getting `CUDA_ERROR_ILLEGAL_ADDRESS` at random during the training, too. But I assume this is probably not related to this issue.\r\n\r\nIn general my issue is solved, but obviously this is a hack, and not an intended behavior. Exactly the same code runs smoothly on r1.3.1, but for r1.4 it needs a `shared_name` to be specified. So I leave this up to you to keep it or close it.", "Actually, @mrry, as I've been training past few days with r1.4, I caught `tf.train.MonitoredTrainingSession` occasionally and silently restarting sessions without raising any errors! \r\n~~~console\r\n# Few hours of training\r\n...\r\n2017-11-19 02:01:35.079083: I tensorflow/core/distributed_runtime/master_session.cc:1004] \r\nStart master session fc7b867194458c34 with config: \r\ngpu_options { allow_growth: true } \r\nallow_soft_placement: true \r\ngraph_options { optimizer_options { global_jit_level: ON_1 } }\r\n 2017-11-19 02:06:11.963118: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 23453 of 125000\r\n2017-11-19 02:06:21.963333: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 47689 of 125000\r\n2017-11-19 02:06:31.963203: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 70957 of 125000\r\n2017-11-19 02:06:41.963338: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 95095 of 125000\r\n2017-11-19 02:06:51.963216: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 119359 of 125000\r\n2017-11-19 02:06:54.333142: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled. \r\n...\r\n# Few hours of training\r\n~~~\r\nAnd this would happen on any worker without any traces at random moments during training.\r\nSo I think the Error Handling might have changed since the latest release.\r\nThis is not good, and makes my models sad(\r\nCan I be of any help in determining the cause? \r\n", "@mrry @cy89 \r\nI'm getting following behaviour without any warnings or errors, only upper-mentioned `Start master session ...` message. \r\n![image](https://user-images.githubusercontent.com/7069222/33189189-55b3b19a-d055-11e7-8e61-5361f88354fb.png)\r\nI would like to prevent or at least debug this.", "@mrry have you seen this kind of behavior before, or might you have any hints?\r\n", "I'm at a loss to explain why the restarts are having such an effect on your training job. I believe `MonitoredTrainingSession` has always attempted to restart the underlying session transparently when certain types of failure happen, and the worker for which `is_chief` is `True` will attempt to reinitialize the parameters from the latest checkpoint if one is available. I doubt the dataset initialization is the cause, although your setup will reinitialize the iterator at each of these restarts, so you might need to shuffle the data to ensure that you're not iterating over the data in the same order each time and thus overfitting somehow.\r\n\r\nCan you share more details about the training setup?\r\n\r\n/cc @ispirmustafa in case there were any recent changes to `MonitoredSession` that could explain a change in behavior.", "I'm too not aware of any change to MonitoredSession that is related. \r\nBTW, following code segment accesses to private parts of MonitoredSession which may work in an unexpected way:\r\n```\r\n        for target in targets:\r\n            # Get all target datasets handles.\r\n            handle[target] = mon_sess._coordinated_creator.tf_sess.run(\r\n                training_model.iter_handles[target])\r\n            # Init all target datasets.\r\n            mon_sess._coordinated_creator.tf_sess.run(training_model.iter_init_ops[target])\r\n\r\n```\r\nInstead you should use session run hooks to make it proper:\r\n```\r\n    class InitializerHook(tf.train.SessionRunHook):\r\n      def after_create_session(self, session, coord):\r\n        for target in targets:\r\n            # Get all target datasets handles.\r\n            handle[target] = session.run(training_model.iter_handles[target])\r\n            # Init all target datasets.\r\n            session.run(training_model.iter_init_ops[target])\r\n   with MonitoredTrainingSession(hooks=[InitializerHook()], ...):\r\n```", "@mrry I shared the graph just to illustrate that `MonitoredTrainingSession` is initialized repeatedly during a training. The graph looks that way because the checkpoint is being saved rarely. The problem is that there are no errors whatsoever. \r\nIn the beginning I thought it maybe restarting due to OOMs and just not showing the errors, but I checked with bigger models and tf catches OOMs just fine.\r\nMy setup is an alpha k8s cluster on GCP that I used successfully since it's release this summer:\r\n```\r\n# Cluster settings.\r\nCLUSTER_NAME=#####\r\nNUM_NODES=10\r\nZONE=us-west1-b\r\nPROJECT=#####\r\nCLUSTER_VERSION=1.7.8-gke.0\r\nDISK_SIZE=75\r\nIMAGE_TYPE=COS\r\nMACHINE_TYPE=n1-highmem-4\r\nTYPE=nvidia-tesla-k80\r\nCOUNT=1\r\n\r\n# NodePool Settings.\r\nNP_NAME=ps-cpu\r\nNP_NUM_NODES=1\r\nNP_MACHINE_TYPE=n1-highmem-2\r\n\r\nsudo gcloud alpha container clusters create ${CLUSTER_NAME} \\\r\n    --num-nodes=${NUM_NODES} \\\r\n    --zone=${ZONE} \\\r\n    --project=${PROJECT} \\\r\n    --cluster-version=${CLUSTER_VERSION} \\\r\n    --disk-size=${DISK_SIZE} \\\r\n    --image-type=${IMAGE_TYPE} \\\r\n    --machine-type=${MACHINE_TYPE} \\\r\n    --accelerator=type=${TYPE},count=${COUNT} \\\r\n    --enable-kubernetes-alpha \\\r\n    --scopes storage-full\r\n\r\nsudo gcloud alpha config set container/cluster ${CLUSTER_NAME}\r\n\r\nsudo gcloud alpha container clusters get-credentials ${CLUSTER_NAME} \\\r\n    --zone=${ZONE}\r\n\r\n# This is a hack from github.com/ContainerEngine/accelerators/tree/master/cos-nvidia-gpu-installer\r\n# - the only possible way to expose GPUs of GKE nodes for now.\r\nsudo kubectl create -f https://raw.githubusercontent.com/ContainerEngine/accelerators/master/cos-nvidia-gpu-installer/daemonset.yaml\r\n\r\nsudo gcloud alpha container node-pools create ${NP_NAME} \\\r\n    --num-nodes=${NP_NUM_NODES} \\\r\n    --zone=${ZONE} \\\r\n    --disk-size=${DISK_SIZE} \\\r\n    --image-type=${IMAGE_TYPE} \\\r\n    --machine-type=${NP_MACHINE_TYPE} \\\r\n    --scopes storage-full \\\r\n    --cluster=${CLUSTER_NAME}\r\n```\r\nI have 1 chief, 8 training workers and 1 evaluation worker.\r\nEverything is being controlled by `MonitoredTrainingSession` in a data-parallel way.\r\n\r\n@ispirmustafa As I mentioned before, I tried this fix https://github.com/tensorflow/tensorflow/issues/12859#issuecomment-327783827, instead of `\r\nmon_sess._coordinated_creator.tf_sess.run()` and still got the same error. The latter is just for multi-task iterator initialization convenience. I haven't tried it again after @mrry's fix with `shared_name`, but I assume it's not the cause, as they both are due to session restarts. Though, I am going to add a proper `SessionRunHook` as you suggested, to eliminate the possibility of an internal leak.\r\n\r\nMy biggest confusion is that there is no INFO on to why `MonitoredTrainingSession` restarts without warnings.", "@isaprykin do you have some idea on what's going on?", "@ispirmustafa I can confirm now that using [InitializerHook](https://github.com/tensorflow/tensorflow/issues/14613#issuecomment-347719771) does not fix the behaviour.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@MtDersvan is this still an issue with more recent releases?  Are there log messages from coordinator?  From saver that it is \"Restoring parameters\"?", "@isaprykin I am at the stage of deploying these models rn, but I'll get back to training a new batch in the following 1-2 weeks. I will then try an r1.5 and a new k8s version, and give a comprehensive feedback if something goes wrong. \r\nSo you can either close this issue and I will reopen/open it if it's reproducible, or wait till I have the feedback.", "Thanks, @MtDersvan. We'll leave at \"awaiting response\"; let us know what you find.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@cy89 @isaprykin \r\nI think it's not an Iterator error.\r\nI substituted `from_string_handle` for `make_initializable_iterator`, but the error (silent session restarts without any failures) persisted during `MonitoredTrainingSession`, so I kept debugging a bit deeper.\r\n\r\nI explicitly started to check for exceptions in session run calls [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/session.py#L912), and got an uncaught `OS Error`, that was causing silent restarts:\r\n\r\n~~~\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1392, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1371, in _run_fn\r\n    target_list, status, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnavailableError: OS Error\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 910, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1165, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1386, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1405, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnavailableError: OS Error\r\n~~~\r\n\r\nI could only trace it till `pywrap_tensorflow.TF_PRun()`. It's not clear what is wrong and what to do about it.\r\nIs this an intended behaviour?  :man_shrugging:\r\nIs there any way to avoid this it?\r\n\r\n\r\nEnv Info:\r\n~~~\r\nTF DOCKER: tensorflow/tensorflow:latest-gpu-py3\r\nK8S CLUSTER_VERSION=1.9.4-gke.1\r\n~~~\r\n", "/CC @isaprykin can you take a look?", "Ah, it's not an OSError, it's an UnavailableError with the \"OS Error\" error message, right?\r\nUnavailableError is part of MonitoredSession's _PREEMPTION_ERRORS -- the list of exceptions that it tries to recover from.  But every time it happens an loggign message should be generated.\r\n\r\nCould you please include a more complete set of logs around the point where the session is restarted? ", "Yes, it's an UnavailableError with the \"OS Error\" message. (sorry for a late response)\r\nActually, that's the problem the `run()` in `_RecoverableSession` doesn't log this:\r\n~~~python\r\n    logging.info('An error was raised. This may be due to a preemption in '\r\n                 'a connected worker or parameter server. The current '\r\n                 'session will be closed and a new session will be '\r\n                 'created. Error: %s', e)\r\n~~~\r\nThat's exactly what I did to debug the problem - explicitly checked for any exceptions narrowing down the scope.\r\nAll the tensorflow logs boil down to:\r\n~~~console\r\n...\r\nI tensorflow/core/distributed_runtime/master_session.cc:1024] Start master session 6d9f53ba9c6c58c9 with config: allow_soft_placement: true\r\n...\r\n~~~\r\n\r\nAnd I'm pretty sure that all the distributed workers were online. Maybe a connection issue? Also, adding a `load_balancer` reduced the frequency [but I can't say 100%].\r\n\r\nFor now I just made a custom `_RecoverableSession` that handles `UnavailableError: OS Error` without restarting the whole session (just waits a second before rerunning). "]}, {"number": 14612, "title": "bazel build command failed for tensorflow serving ", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04 docker container\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:1.3.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:0.5.4\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:NA\r\n- **GPU model and memory**:NA\r\n- **Exact command to reproduce**:\r\nbazel build -c opt --jobs 1 --local_resources 5,2.0,2.0 --verbose_failures  tensorflow_serving/...\r\n\r\n\r\n== cat /etc/issue ===============================================\r\nLinux 25b038d948eb 4.9.49-moby #1 SMP Wed Sep 27 23:17:17 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 25b038d948eb 4.9.49-moby #1 SMP Wed Sep 27 23:17:17 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.8)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nenv_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== cat /etc/issue ===============================================\r\nLinux 25b038d948eb 4.9.49-moby #1 SMP Wed Sep 27 23:17:17 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 25b038d948eb 4.9.49-moby #1 SMP Wed Sep 27 23:17:17 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.8)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nbash: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI am facing issues while running bazel build command in order to install tensorflow serving in the docker container.\r\nCOMMAND RAN:-\r\nbazel build -c opt --jobs 1 --local_resources 5,2.0,2.0 --verbose_failures  tensorflow_serving/...\r\n\r\nERROR:-\r\n/root/.cache/bazel/_bazel_root/f8d1071c69ea316497c31e40fe01608c/external/inception_model/inception/slim/BUILD:55:1: Converting to Python 3: external/inception_model/inception/slim/ops.py failed (Exit 1): 2to3 failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/f8d1071c69ea316497c31e40fe01608c/execroot/tf_serving && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/local-py3-opt/genfiles/python3/external/inception_model/inception/slim --write-unchanged-files external/inception_model/inception/slim/ops.py).\r\n\r\nI am trying to create a tensorflow serving container with python3 and tensorflow 1.3. I have successfully create tensorflow serving with python2 and tested mnist and inception models.\r\n\r\n", "comments": []}, {"number": 14611, "title": "KeyError: \"Couldn't find field google.protobuf.DescriptorProto.ExtensionRange.options\"", "body": "My tensorflow was working fine a few days ago and now I get this error:\r\n\r\n```\r\nLast login: Mon Nov 13 17:56:32 on ttys001\r\npWed Nov 15 22:38:01 :~$ python\r\nPython 3.6.2 |Anaconda custom (x86_64)| (default, Jul 20 2017, 13:14:59)\r\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/mona/anaconda/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Users/mona/anaconda/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/Users/mona/anaconda/lib/python3.6/site-packages/tensorflow/core/framework/graph_pb2.py\", line 10, in <module>\r\n    from google.protobuf import descriptor_pb2\r\n  File \"/Users/mona/anaconda/lib/python3.6/site-packages/google/protobuf/descriptor_pb2.py\", line 409, in <module>\r\n    options=None),\r\n  File \"/Users/mona/anaconda/lib/python3.6/site-packages/google/protobuf/descriptor.py\", line 501, in __new__\r\n    return _message.default_pool.FindFieldByName(full_name)\r\nKeyError: \"Couldn't find field google.protobuf.DescriptorProto.ExtensionRange.options\"\r\n>>>\r\n```\r\n\r\nAnd my sys info is:\r\n```\r\nWed Nov 15 23:41:58 :~$ uname -a\r\nDarwin Monas-MacBook-Pro.local 17.0.0 Darwin Kernel Version 17.0.0: Thu Aug 24 21:48:19 PDT 2017; root:xnu-4570.1.46~2/RELEASE_X86_64 x86_64\r\nWed Nov 15 23:42:05 :~$ conda list | grep tensorflow\r\ntensorflow                1.1.0               np112py36_0\r\ntensorflow                1.4.0                     <pip>\r\ntensorflow-tensorboard    0.1.8                     <pip>\r\n```", "comments": ["I uninstalled all the packages and installed again using pip and still getting error:\r\n```\r\nWed Nov 15 23:55:20 :~$ /Users/mona/anaconda/bin/pip install tensorflow tensorflow-tensorboard\r\nCollecting tensorflow\r\n  Using cached tensorflow-1.4.0-cp36-cp36m-macosx_10_11_x86_64.whl\r\nCollecting tensorflow-tensorboard\r\n  Using cached tensorflow_tensorboard-0.1.8-py3-none-any.whl\r\nRequirement already satisfied: protobuf>=3.3.0 in ./anaconda/lib/python3.6/site-packages (from tensorflow)\r\nRequirement already satisfied: numpy>=1.12.1 in ./anaconda/lib/python3.6/site-packages (from tensorflow)\r\nRequirement already satisfied: wheel>=0.26 in ./anaconda/lib/python3.6/site-packages (from tensorflow)\r\nRequirement already satisfied: six>=1.10.0 in ./anaconda/lib/python3.6/site-packages (from tensorflow)\r\nRequirement already satisfied: enum34>=1.1.6 in ./anaconda/lib/python3.6/site-packages (from tensorflow)\r\nRequirement already satisfied: html5lib==0.9999999 in ./anaconda/lib/python3.6/site-packages (from tensorflow-tensorboard)\r\nRequirement already satisfied: markdown>=2.6.8 in ./anaconda/lib/python3.6/site-packages (from tensorflow-tensorboard)\r\nRequirement already satisfied: werkzeug>=0.11.10 in ./anaconda/lib/python3.6/site-packages (from tensorflow-tensorboard)\r\nRequirement already satisfied: bleach==1.5.0 in ./anaconda/lib/python3.6/site-packages (from tensorflow-tensorboard)\r\nRequirement already satisfied: setuptools in ./anaconda/lib/python3.6/site-packages (from protobuf>=3.3.0->tensorflow)\r\nInstalling collected packages: tensorflow-tensorboard, tensorflow\r\nSuccessfully installed tensorflow-1.4.0 tensorflow-tensorboard-0.1.8\r\nWed Nov 15 23:56:36 :~$ python\r\nPython 3.6.2 |Anaconda custom (x86_64)| (default, Jul 20 2017, 13:14:59)\r\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/mona/anaconda/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Users/mona/anaconda/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/Users/mona/anaconda/lib/python3.6/site-packages/tensorflow/core/framework/graph_pb2.py\", line 10, in <module>\r\n    from google.protobuf import descriptor_pb2\r\n  File \"/Users/mona/anaconda/lib/python3.6/site-packages/google/protobuf/descriptor_pb2.py\", line 409, in <module>\r\n    options=None),\r\n  File \"/Users/mona/anaconda/lib/python3.6/site-packages/google/protobuf/descriptor.py\", line 501, in __new__\r\n    return _message.default_pool.FindFieldByName(full_name)\r\nKeyError: \"Couldn't find field google.protobuf.DescriptorProto.ExtensionRange.options\"\r\n>>> quit()\r\nWed Nov 15 23:57:51 :~$ conda list | grep tensorflow\r\ntensorflow                1.4.0                     <pip>\r\ntensorflow-tensorboard    0.1.8                     <pip>\r\n```\r\n\r\nI need both tensorflow and tensor board that's why I used pip. Please suggest a solution.", "I had the same error today (on manjaro linux, however). Somehow there was a system wide tensorflow installation that was updated (`python-tensorflow-cuda`) which interfered with the pip install of `tensorflow-gpu`. After I uninstalled `python-tensorflow-cuda` everything was back to normal. Hope this helps in your case too!", "@monajalal can you please try @abieler 's suggestion, and let us know if that doesn't work for you?", "Edit: after initially solving the problem, I updated TF back to `1.4.0` which broke the installation again. It was then resolved by downgrading `protobuf` as I commented here:\r\n\r\nhttps://github.com/google/protobuf/issues/3631", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 180 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 14610, "title": "Error when bazel building after pull down newest master branch", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **CUDA/cuDNN version**:  8.0.61\r\n- **GPU model and memory**: NVIDIA Corporation Device 1b06\r\n- **Exact command to reproduce**: \r\n\r\n```\r\nbazel build tensorflow/python/tools:freeze_graph\r\n```\r\n\r\n### Describe the problem\r\nHere is the warning and error message I got :\r\n\r\n```\r\nWARNING: tensorflow/tensorflow/core/BUILD:1801:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1108:30\r\nWARNING: tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/simonlee/Work/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Analysed target //tensorflow/python/tools:freeze_graph (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: tensorflow/tensorflow/contrib/lite/toco/BUILD:158:1: C++ compilation of rule '//tensorflow/contrib/lite/toco:graph_transformations' failed (Exit 1)\r\nIn file included from external/gemmlowp/public/../internal/dispatch_gemm_shape.h:20:0,\r\n                 from external/gemmlowp/public/gemmlowp.h:19,\r\n                 from ./tensorflow/contrib/lite/kernels/internal/common.h:48,\r\n                 from ./tensorflow/contrib/lite/toco/runtime/types.h:18,\r\n                 from ./tensorflow/contrib/lite/toco/model.h:25,\r\n                 from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23,\r\n                 from tensorflow/contrib/lite/toco/graph_transformations/identify_l2_pool.cc:20:\r\nexternal/gemmlowp/public/../internal/../internal/kernel_default.h:88:2: error: #error \"SIMD not enabled, you'd be getting a slow software fallback. Consider enabling SIMD extensions (for example using -msse4 if you're on modern x86). If that's not an option, and you would like to continue with the slow fallback, define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK.\"\r\n #error \\\r\n  ^\r\nTarget //tensorflow/python/tools:freeze_graph failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1.120s, Critical Path: 0.85s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["**I get the same error when bazel building,do you know how to deal with it,thks!**\r\nERROR: /home/deeplearn/.cache/bazel/_bazel_deeplearn/bf1e87bfe06d5731809039dca55f14ae/external/org_tensorflow/tensorflow/contrib/lite/toco/BUILD:174:1: C++ compilation of rule '@org_tensorflow//tensorflow/contrib/lite/toco:graph_transformations' failed (Exit 1).\r\nIn file included from external/gemmlowp/public/../internal/dispatch_gemm_shape.h:20:0,\r\n                 from external/gemmlowp/public/gemmlowp.h:19,\r\n                 from external/org_tensorflow/tensorflow/contrib/lite/kernels/internal/common.h:48,\r\n                 from external/org_tensorflow/tensorflow/contrib/lite/toco/runtime/types.h:18,\r\n                 from external/org_tensorflow/tensorflow/contrib/lite/toco/model.h:25,\r\n                 from external/org_tensorflow/tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23,\r\n                 from external/org_tensorflow/tensorflow/contrib/lite/toco/graph_transformations/remove_tensorflow_assert.cc:19:\r\nexternal/gemmlowp/public/../internal/../internal/kernel_default.h:88:2: error: #error \"SIMD not enabled, you'd be getting a slow software fallback. Consider enabling SIMD extensions (for example using -msse4 if you're on modern x86). If that's not an option, and you would like to continue with the slow fallback, define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK.\"\r\n #error \\\r\n  ^\r\n____Building complete.\r\n____Elapsed time: 135.971s, Critical Path: 37.49s", "Sorry , I didn't solve it , does it still remain on newest branch ?", "i got the same error", "I fellow  the part of  \u201cOptimized build\u201d  on the page of https://www.tensorflow.org/serving/setup,\r\nand added msse4 options(Although mine is on modern x86_64) when building and testing,it seems work,but maybe not the best solution(maybe define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK in the source code when  x86_64).\r\n1.> nohup.out && nohup  bazel build -c opt **--copt=-msse4.1 --copt=-msse4.2** tensorflow_serving/... &\r\n2.bazel test -c opt **--copt=-msse4.1 --copt=-msse4.2**  tensorflow_serving/...", "I have try below command , and it passed!\r\n\r\nbazel build   -c opt --copt=-msse4.1 --copt=-msse4.2 tensorflow/python/tools:freeze_graph", "Sorry for my late reply .\r\nI try the latest master branch and do not modify anything , all bazel build is going very well .\r\nDo you use the latest branch ?", "Are you building on a Intel machine or are you building on say a Jetson host device or an ARM host device?", "I get the same error when bazel building........\r\ni changed the python environment from python2 to python3, it shows this error. is there any help", "@aselle I am using intel machine , and I do nothing special when bazel building , everything is going well now.\r\n\r\n@XiaoSX \r\nDo you try command provided by @offbye \uff1f\r\nAnd what machine do you use ?", "@Sixigma ,hi, i am using intel machine, too. I have tried the command above, but there is still something wrong. It looks like some run-time cpp files, showed  lines \"from StringIO import StringIO\" , py3 has no that module, but could not be edited.", "Getting the same error while building tensorflow servings on docker.....\r\nAny help Please.....\r\nThanks in advance\r\n\r\n\r\nERROR: '@org_tensorflow//tensorflow/contrib/lite/toco:graph_transformations' failed (Exit 1).\r\nIn file included from external/gemmlowp/public/../internal/dispatch_gemm_shape.h:20:0,\r\n                 from external/gemmlowp/public/gemmlowp.h:19,\r\n                 from external/org_tensorflow/tensorflow/contrib/lite/kernels/internal/common.h:48,\r\n                 from external/org_tensorflow/tensorflow/contrib/lite/toco/runtime/types.h:18,\r\n                 from external/org_tensorflow/tensorflow/contrib/lite/toco/model.h:25,\r\n                 from external/org_tensorflow/tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23,\r\n                 from external/org_tensorflow/tensorflow/contrib/lite/toco/graph_transformations/create_im2col_arrays.cc:21:\r\nexternal/gemmlowp/public/../internal/../internal/kernel_default.h:88:2: error: #error \"SIMD not enabled, you'd be getting a slow software fallback. Consider enabling SIMD extensions (for example using -msse4 if you're on modern x86). If that's not an option, and you would like to continue with the slow fallback, define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK.\"\r\n #error \\\r\n  ^\r\nINFO: Elapsed time: 14.932s, Critical Path: 9.05s\r\n", "@visshvesh , you can try compiling it by using the following cmd\r\n\r\n`bazel build -c opt --copt=-msse4.1 --copt=-msse4.2 tensorflow_serving/... `\r\n\r\nI have also made a docker image compiled with tf-serving (CPU) and tf-serving(GPU). You can use them by pulling the image from the docker hub, \r\n\r\nCPU:\r\n`docker pull gauravkaila/tf_serving_cpu`\r\n\r\nGPU:\r\n`docker pull gauravkaila/tf_serving_gpu`", "**@gauravkaila hey, I tried compiling with that but I always get the same error when trying to test the running model, in fact I get the error trying to run anything in the container. I had to export the model myself by adding savedmodelbuilder code into the retrain script for inception. I pulled your image and am again running into the same error:**\r\n\r\n```\r\nroot@42fd2c27af9d:/serving# bazel-bin/tensorflow_serving/example/inception_client --server=localhost:9000 --image=./Xiang_Xiang_panda.jpg\r\nTraceback (most recent call last):\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py\", line 56, in <module>\r\n    tf.app.run()\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 129, in run\r\n    _sys.exit(main(argv))\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py\", line 50, in main\r\n    tf.contrib.util.make_tensor_proto(data, shape=[1]))\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n    module = self._load()\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module\r\n    __import__(name)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/contrib/__init__.py\", line 81, in <module>\r\n    from tensorflow.contrib.eager.python import tfe as eager\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/contrib/eager/python/tfe.py\", line 75, in <module>\r\n    from tensorflow.contrib.eager.python.datasets import Iterator\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/contrib/eager/python/datasets.py\", line 23, in <module>\r\n    from tensorflow.contrib.data.python.ops import prefetching_ops\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/contrib/data/python/ops/prefetching_ops.py\", line 25, in <module>\r\n    resource_loader.get_path_to_datafile(\"../../_prefetching_ops.so\"))\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/contrib/util/loader.py\", line 55, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/python/framework/load_library.py\", line 56, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: /serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/contrib/data/python/ops/../../_prefetching_ops.so: undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringB5cxx11E\r\n```\r\n\r\n\r\n**When I check the running log I can see that my model is running:**\r\n```\r\n\r\n2017-12-09 01:24:00.397485: I tensorflow_serving/model_servers/main.cc:147] Building single TensorFlow model file config:  model_name: inception model_base_path: /tmp/new5\r\n2017-12-09 01:24:00.397670: I tensorflow_serving/model_servers/server_core.cc:439] Adding/updating models.\r\n2017-12-09 01:24:00.397696: I tensorflow_serving/model_servers/server_core.cc:490]  (Re-)adding model: inception\r\n2017-12-09 01:24:00.498119: I tensorflow_serving/core/basic_manager.cc:705] Successfully reserved resources to load servable {name: inception version: 1}\r\n2017-12-09 01:24:00.498154: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: inception version: 1}\r\n2017-12-09 01:24:00.498169: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: inception version: 1}\r\n2017-12-09 01:24:00.498189: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:360] Attempting to load native SavedModelBundle in bundle-shim from: /tmp/new5/1\r\n2017-12-09 01:24:00.498203: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:236] Loading SavedModel from: /tmp/new5/1\r\n2017-12-09 01:24:00.623487: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 FMA\r\n2017-12-09 01:24:00.743901: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:155] Restoring SavedModel bundle.\r\n2017-12-09 01:24:00.798587: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running LegacyInitOp on SavedModel bundle.\r\n2017-12-09 01:24:00.805405: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:284] Loading SavedModel: success. Took 307196 microseconds.\r\n2017-12-09 01:24:00.805517: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: inception version: 1}\r\n2017-12-09 01:24:00.810840: I tensorflow_serving/model_servers/main.cc:288] Running ModelServer at 0.0.0.0:9000 ...\r\n```", "@jda91 i have same problem,have u solve that?", "@yokiqust nope :/ it's driving me crazy, I can't find any information online about it either ", "@gauravkaila \r\nThanks but still getting the same issue...\r\nI ll try pulling the image and using it, Thanks", "My error message contained the following line:\r\n`external/gemmlowp/public/../internal/../internal/kernel_default.h:88:2: error: #error \"SIMD not enabled, you'd be getting a slow software fallback. Consider enabling SIMD extensions (for example using -msse4 if you're on modern x86). If that's not an option, and you would like to continue with the slow fallback, define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK.\"`\r\n\r\n\r\nUsing the following options in the command have worked for me:\r\n`--copt=-msse4.1 --copt=-msse4.2`", "I have met the same problem.\r\nAlso I have tried to use\r\nbazel build -c opt --copt=-msse4.1 --copt=-msse4.2 tensorflow/python/tools:freeze_graph\r\nit works fine~,", "As I said, I tried that, multiple times on multiple computers on various operating systems, I get the same exact error every time. I can't export or run any python script, even if I natively install pip tensorflow serving, the same error occurs. I tried in  the docker container, I tried without it, same error every time. ", "I was having this same issue in a new container I created to build tensorflow and run some tests. Error went away after I remembered to run the ./configure script at the root of the project. ", "Close this issue due to lack of activities , try compile commands above in advance , reopen this issue if needed."]}, {"number": 14609, "title": "Why cannot use tf.Print() ?", "body": "I just want to print out the executing data like this:\r\n![t](https://user-images.githubusercontent.com/14851411/32872558-47be9c0c-ca7f-11e7-8e5d-6fa6796cb043.jpg)\r\nBut I got :\"**Cannot assign a device for operation 'Print': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available**\"\r\nI'm sure this piece of code can work in python console with tf.device('/gpu:0') assigned:\r\nAnyone help ?\r\n\r\n\r\n", "comments": ["@MaeThird Are you using TF eager mode by any chance?", "@caisq no. This is just a test with my tedious codes.I thought there must be something wrong since it can print as excepted in console:\r\n![t2](https://user-images.githubusercontent.com/14851411/32873268-feb1290e-ca82-11e7-8c91-187c4555a150.jpg)\r\n\r\n\r\n", "Putting `tf.Session()` under 'tf.device' has no effect at all. Putting `tf.Print` will probably give you the same error.\r\nEnabling \"allow_soft_placement\" session option should avoid this error.", "@ppwwyyxx  `allow_soft_placement=True` is ok! Thank U."]}, {"number": 14608, "title": "save restore from a incomplete model", "body": "# problem\r\nEach time the trainning programe begining to run will restore model from where the last model was saved\u3002but sometimes the last model is incompleted.the model restore fail. In order to make programe restore from a normal model\uff0cI hava tomodify the checkpoint file manually\u3002 \r\n\r\n# the checkpoint file as follow:\r\n```\r\nmodel_checkpoint_path: \"model.ckpt-88663487\"\r\nall_model_checkpoint_paths: \"model.ckpt-88598144\"\r\nall_model_checkpoint_paths: \"model.ckpt-88631248\"\r\nall_model_checkpoint_paths: \"model.ckpt-88631251\"\r\nall_model_checkpoint_paths: \"model.ckpt-88631617\"\r\nall_model_checkpoint_paths: \"model.ckpt-88663473\"\r\nall_model_checkpoint_paths: \"model.ckpt-88663487\"\r\n```\r\n# model.ckpt-88663487 is incomplete\r\n```\r\nmodel.ckpt-88663487.data-00000-of-00040\r\nmodel.ckpt-88663487.data-00002-of-00040\r\nmodel.ckpt-88663487.data-00003-of-00040\r\nmodel.ckpt-88663487.data-00004-of-00040\r\nmodel.ckpt-88663487.data-00005-of-00040\r\nmodel.ckpt-88663487.data-00006-of-00040\r\nmodel.ckpt-88663487.data-00007-of-00040\r\nmodel.ckpt-88663487.data-00008-of-00040\r\nmodel.ckpt-88663487.data-00009-of-00040\r\nmodel.ckpt-88663487.data-00010-of-00040\r\nmodel.ckpt-88663487.data-00011-of-00040\r\nmodel.ckpt-88663487.data-00012-of-00040\r\nmodel.ckpt-88663487.data-00013-of-00040\r\nmodel.ckpt-88663487.data-00014-of-00040\r\nmodel.ckpt-88663487.data-00015-of-00040\r\nmodel.ckpt-88663487.data-00016-of-00040\r\nmodel.ckpt-88663487.data-00017-of-00040\r\nmodel.ckpt-88663487.data-00018-of-00040\r\nmodel.ckpt-88663487.data-00019-of-00040\r\nmodel.ckpt-88663487.data-00020-of-00040\r\nmodel.ckpt-88663487.data-00021-of-00040\r\nmodel.ckpt-88663487.data-00022-of-00040\r\nmodel.ckpt-88663487.data-00023-of-00040\r\nmodel.ckpt-88663487.data-00024-of-00040\r\nmodel.ckpt-88663487.data-00025-of-00040\r\nmodel.ckpt-88663487.data-00026-of-00040\r\nmodel.ckpt-88663487.data-00027-of-00040\r\nmodel.ckpt-88663487.data-00028-of-00040\r\nmodel.ckpt-88663487.data-00029-of-00040\r\nmodel.ckpt-88663487.data-00030-of-00040\r\nmodel.ckpt-88663487.data-00031-of-00040\r\nmodel.ckpt-88663487.data-00032-of-00040\r\nmodel.ckpt-88663487.data-00033-of-00040\r\nmodel.ckpt-88663487.data-00034-of-00040\r\nmodel.ckpt-88663487.data-00035-of-00040\r\nmodel.ckpt-88663487.data-00036-of-00040\r\nmodel.ckpt-88663487.data-00037-of-00040\r\nmodel.ckpt-88663487.data-00038-of-00040\r\nmodel.ckpt-88663487.data-00039-of-00040\r\nmodel.ckpt-88663487.index\r\nmodel.ckpt-88663487_temp_c555bad9d4a149fca7fa899d665d3ea3\r\n```\r\n# confuse\r\nThe model will restort from model.ckpt-88663487,but model.ckpt-88663487 is incompleted.because during the programe save model, the programe is stoped. why change the checkpoint's model_checkpoint_path,when the model is not saved completely\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 14607, "title": "Tensorflow Lite Support for Windows", "body": "Are you planning to support Tensorflow Lite on Windows? Specifically Windows 32bit.", "comments": ["What kind of support are you interested in? Inference or tooling for preparing models? Why do you need 32 bit windows? Likely we will first support model preparation tooling first on 64 bit windows.", "I need inference on the 32bit windows with pretraind model,, because there are a lot of 32bit windows devices yet and our product need support all windows devices.\r\nHow can I run the pretraind tensorflow model on the 32bit windows?\r\nThanks.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "See also\r\n#16374\r\n\r\n", "@apollo-time, you can probably make a very simple build that supports windows. There is also a pending PR that adds support \r\n#16490\r\nbut it has not been merged into mainline yet.\r\n\r\n\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@apollo-time you can see https://github.com/freeors/aismart/tree/master/aismart-src/aismart/external/tensorflow ,this preject had build tflite on windows , docs in https://zhuanlan.zhihu.com/p/32190069", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Hey guys, any word on if/when this is going to happen?  This is something we are really looking for (not the part about 32 bit specifically, just a TF-lite for Windows build in general).\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "This is an issue. Lite should can support UWP/WPF/Form apps and potentially MacOS native apps.", "It's still an issue here as well. We'd really like to have a Windows build of TF-Lite.  It's not a hard blocker right this minute, but it's something we could definitely use.", "+1 Still an issue, want to use the same code package for both Windows and Android", "Having to include the full tensorflow package to preform inference in a windows app is not ideal for me, so I'm looking forward to this issue being resolved. "]}]