[{"number": 32738, "title": "[XLA:GPU][ROCm] Adding no_rocm tag to ROCm XLA CI failed test cases", "body": "Disable the failing test cases to fix ROCm CI regressions:\r\n\r\n- buffer_comparator_test: This component should not be used in ROCm now, so disable its test case\r\n- Test cases under mlir: needs more investigation on root cause of failures\r\n- `depthwise_conv_op_test`: needs more investigation on root cause of failures", "comments": ["What tests are failing in MLIR? Are they all failing? (the majority should not have anything to do with ROCm). Can you looked into being more \"scoped\" in what is disabled here?\r\n\r\nThanks.", "@joker-eph Thanks for reminding. No they are not all failing, but we do not have enough resources to look at the failures now. Adding `no_rocm` tag is only a temporary resort and we do keep a record & plan to take a closer look later. I can @ you when coming up with fixes for the unit tests."]}, {"number": 32737, "title": "tf.keras.layers.Input has undefined shape when setting sparse=True, making it impossible to use in a Model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15.0rc1\r\n- Python version:3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Nvidia GeForce GTX 1050, 4GB\r\n\r\n**Describe the current behavior**\r\nThe tensor created by `tensorflow.keras.layers.Input(specified_shape, sparse=True)` has shape `(None,) + (None, ) * len(specified_shape)` and cannot be used as input to e.g., `Dense` layers. \r\n\r\n**Describe the expected behavior**\r\nThe tensor created by `tensorflow.keras.layers.Input(specified_shape, sparse=True)` has shape `(None,) + specified_shape` and can be used as input to e.g., `Dense` layers. This is the default behaviour in Keras. \r\n\r\n**Code to reproduce the issue**\r\n```python\r\nfrom tensorflow.keras.layers import Input, Dense\r\n\r\ni = Input(shape=(1, ), sparse=True)\r\no = Dense(1)(i)\r\n```\r\n\r\n**Other info / logs**\r\nStack trace: \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 7, in <module>\r\n    o = Dense(1)(i)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 824, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2146, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/layers/core.py\", line 1009, in build\r\n    raise ValueError('The last dimension of the inputs to `Dense` '\r\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n\r\n```\r\n", "comments": ["Could reproduce the issue on colab with TF 1.15.1.rc1.Please take a look at gist [here](https://colab.sandbox.google.com/gist/gadagashwini/8ed685bf1786cd6fe8f392a285fdd2c6/untitled162.ipynb). Thanks!", "@danielegrattarola This is due to sparse tensors. Please read [this](https://github.com/tensorflow/tensorflow/issues/23748#issuecomment-443890338) for more details on the source of the issue.\r\n\r\nThere is not an easy fix for this. A workaround is \r\n`i = Input(shape=(1,), sparse=True, batch_size=10)`\r\n\r\nThanks!", "@jvishnuvardhan I see, but why did it work in previous versions of Keras and TensorFlow then? \r\nAlso defining a batch size is only a workaround for very simple use cases, not a stable solution at all. \r\n\r\nIt feels like this makes using sparse tensors practically impossible in most situation.\r\nIs there anything that I or anybody else can do to restore this functionality in Tensorflow?", "@danielegrattarola I think @omalleyt12 provided couple of other ideas (creating subclass layer or Lambda layer to convert your sparse inputs to dense) in response to the other [similar issue](https://github.com/tensorflow/tensorflow/issues/23748#issuecomment-443890338). Please feel free to follow that link and start contribute through PRs. Thanks!", "I've fixed this in 74195b50fb5e1f22eb95ffd1646b4b0ceca2ea9b, #23748\r\n\r\n", "Closing this issue as it was resolved already. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32737\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32737\">No</a>\n", "I am on `tf.__version__` `'2.5.0-rc3'` and while `Model.predict` now works fine with sparse data, `Model.fit` does not:\r\n```python\r\nTypeError: 'SparseTensor' object is not subscriptable\r\n```\r\n\r\nIf I switch my imports from tf.keras to just import from keras, Dense models work just fine.\r\n\r\nWhy does the keras package support training with sparse input, but tf.keras does not?", "@zachmayer\r\n\r\nShoot, when I fixed it for predict, I added tests for predict assuming that there were tests for fit __somewhere__...\r\n\r\nI'll take a  look, it may b e too late to patch for tf2.5...", "Can you post the fill trace-back and/or the some simple code to reproduce this?", "I tried to reproduce this but did not get that error (model.fit worked fine for me). I need more details to make any progress.\r\n\r\nIs it possible you're passing a sparse tensor to a layer that doesn't support it?", "Here is a reproducible example.  Some notes:\r\n1. If you are using sparse data, you must call `x.sort_indices()` before fitting a keras model.  It sort of feels like maybe keras should do that for you?\r\n2. My specific error was caused by calling model.fit on a pandas.Series (but the error message did not make this at all clear)!  I suggest that maybe keras either: a. raise an exception on pandas series input or b. convert the pandas series to a numpy array.  I fixed my error by converting the pandas series to a numpy array.\r\n\r\nHere is the code:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\n\r\ntext = ['sdsadfasd', 'wdfsdfg', 'sDFQERG', 'ASDFASDFG']\r\nx = TfidfVectorizer(analyzer='char').fit_transform(text)\r\nx.sort_indices()  # This is required or the keras model fails\r\ny = pd.Series([1, 0, 1, 0])\r\n\r\ninputs = tf.keras.layers.Input(shape=(x.shape[1],), name='dense_inputs')\r\noutputs = tf.keras.layers.Dense(1)(inputs)\r\nmodel = tf.keras.models.Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(optimizer=\"Adam\", loss=\"mse\")\r\n\r\n# Fails\r\nmodel.fit(x, y)\r\nmodel.predict(x)\r\n```\r\n\r\nAnd then you simply change `model.fit(x, y)` to `model.fit(x, y.values)` and the code works.\r\n\r\n```python\r\n# Works\r\nmodel.fit({'dense_inputs': x}, y.values)\r\nmodel.predict(x)\r\n```\r\n\r\nI am on tensorflow version `'2.5.0-rc3'`", "The traceback when the fit fails is below.  What's interesting is that the pandas series isn't part of the traceback.  Converting numpy to pandas somehow changes how the sparse matrix is indexed:\r\n\r\n```python\r\nWARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\r\n2021-05-14 22:09:35.779336: W tensorflow/core/framework/op_kernel.cc:1755] Invalid argument: TypeError: 'SparseTensor' object is not subscriptable\r\nTraceback (most recent call last):\r\n\r\n  File \"/Users/zachary/python/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 247, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"/Users/zachary/python/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 135, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"/Users/zachary/python/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 645, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"/Users/zachary/python/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 488, in py_method\r\n    return [slice_array(inp) for inp in flat_inputs]\r\n\r\n  File \"/Users/zachary/python/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 488, in <listcomp>\r\n    return [slice_array(inp) for inp in flat_inputs]\r\n\r\n  File \"/Users/zachary/python/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 486, in slice_array\r\n    return training_utils.slice_arrays(data, ind.numpy(),\r\n\r\n  File \"/Users/zachary/python/lib/python3.9/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 50, in slice_arrays\r\n    entries = [[x[i:i + 1] for i in indices] for x in arrays]\r\n\r\n  File \"/Users/zachary/python/lib/python3.9/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 50, in <listcomp>\r\n    entries = [[x[i:i + 1] for i in indices] for x in arrays]\r\n\r\n  File \"/Users/zachary/python/lib/python3.9/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 50, in <listcomp>\r\n    entries = [[x[i:i + 1] for i in indices] for x in arrays]\r\n\r\nTypeError: 'SparseTensor' object is not subscriptable\r\n\r\n\r\nInvalidArgumentError:  TypeError: 'SparseTensor' object is not subscriptable\r\nTraceback (most recent call last):\r\n\r\n  File \"/Users/zachary/Kaggle/CommonLit/python/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 247, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"/Users/zachary/Kaggle/CommonLit/python/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 135, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"/Users/zachary/Kaggle/CommonLit/python/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 645, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"/Users/zachary/Kaggle/CommonLit/python/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 488, in py_method\r\n    return [slice_array(inp) for inp in flat_inputs]\r\n\r\n  File \"/Users/zachary/Kaggle/CommonLit/python/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 488, in <listcomp>\r\n    return [slice_array(inp) for inp in flat_inputs]\r\n\r\n  File \"/Users/zachary/Kaggle/CommonLit/python/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 486, in slice_array\r\n    return training_utils.slice_arrays(data, ind.numpy(),\r\n\r\n  File \"/Users/zachary/Kaggle/CommonLit/python/lib/python3.9/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 50, in slice_arrays\r\n    entries = [[x[i:i + 1] for i in indices] for x in arrays]\r\n\r\n  File \"/Users/zachary/Kaggle/CommonLit/python/lib/python3.9/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 50, in <listcomp>\r\n    entries = [[x[i:i + 1] for i in indices] for x in arrays]\r\n\r\n  File \"/Users/zachary/Kaggle/CommonLit/python/lib/python3.9/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 50, in <listcomp>\r\n    entries = [[x[i:i + 1] for i in indices] for x in arrays]\r\n\r\nTypeError: 'SparseTensor' object is not subscriptable\r\n\r\n\r\n\t [[{{node EagerPyFunc}}]]\r\n\t [[IteratorGetNext]] [Op:__inference_train_function_4965]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```", "@MarkDaoust given my code snippet, are you able to reproduce?", "@MarkDaoust ping", "Thanks for the info (and the ping). \r\n\r\nI haven't worked out the details, but it looks like the difference in behavior stems from this line in the data adapter code:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r2.5/tensorflow/python/keras/engine/data_adapter.py#L988\r\n\r\nWhen you pass the `pd.Series` the result is `GenericArrayLikeDataAdapter` (fails), but when you pass the values as a np.array the result is a `CompositeTensorDataAdapter` (passes).\r\n\r\nThat decision seems to be driven by the [`can_handle` method here](https://github.com/tensorflow/tensorflow/blob/15d5b930d7e6e4463e275cfcff22042105148e3f/tensorflow/python/keras/engine/data_adapter.py#L569).\r\n\r\nMaybe that line should be checking the [`get_tensor_types`](https://github.com/tensorflow/tensorflow/blob/15d5b930d7e6e4463e275cfcff22042105148e3f/tensorflow/python/keras/engine/data_adapter.py#L569) list of classes.\r\n\r\nBut IDK. "]}, {"number": 32736, "title": "1.15.0 release candidates missing from Docker Hub", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version: 1.15.0-rc1\r\n- Python version: Python 2 or 3\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nThere does not appear to be a tagged container which matches the `1.15.0-rc1` (or _any_ `1.15`) release at https://hub.docker.com/r/tensorflow/tensorflow.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI navigated to https://hub.docker.com/r/tensorflow/tensorflow/tags and entered 1.15 into the \"Filter Tags\" search box as illustrated [here](https://hub.docker.com/r/tensorflow/tensorflow/tags?page=1&name=1.15). I understand that _currently_ the `nightly-` builds are based on `1.15.0-devX`, but I would have expected there to be a specific tag for the release candidate (like there is for 2.0.0).\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Thanks for reporting this! We have automation to build our new images automatically; since TF 2.0 is the \"newest\", these images weren't being triggered properly. I deployed the latest 1.15rc1 images, and they're now available from Docker Hub.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32736\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32736\">No</a>\n", "Thank you, @angerson!", "@angerson: quick ping regarding the latest 1.15 release. Was this also skipped due to latest pointing to 2.0?", "@patzm Thanks for the ping, that was indeed the case. I've deployed our 1.15.0 images now and verified that they're present on Docker Hub."]}, {"number": 32735, "title": "Cherrypicks r5 nmq", "body": "Rollup of bug fixes and performance improvements in Grappler and Eigen.", "comments": ["It is probably too late for this large PR. Deferring to 2.1."]}, {"number": 32734, "title": "JSON Serializable checks for array and structs", "body": "Checks for np.ndarray and np.generic for logs.item to fix JSON Serializable issues in tenorflow.keras.RemoteMonitor class #32192 ", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32734) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "@googlebot I fixed it.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32734) for more info**.\n\n<!-- ok -->", "@ashahab thank you , can you please add some test cases around this ?", "@rthadur  thanks for looking at this, yes I'm working on the test-cases.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32734) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32734) for more info**.\n\n<!-- ok -->", "@googlebot I fixed it.", "@rthadur I have added a test.", "@rchao ", "@rchao @rthadur Can I get a review?", "@rchao  Did you have other questions on this?", "@rchao Who can merge this?", "@ashahab can you please check the build failures ?", "@rthadur Checked, there was a python style issue.", "Thank you re-ran tests again .", "@rthadur not sure why the Android CI builds are failing.", "@rthadur Not sure what else is needed for merging the code.", "@ashahab thanks for the changes , @rchao gentle ping to review the changes.", "@rchao sorry need another review(because of the latest change in comment)", "@rthadur @rchao , is there anything else pending on this?", "@rchao can you please help merging this PR.Thank you", "@rchao @rthadur Let me know what needs to be done to merge this.", "@rchao @rthadur Can this be merged? Also are there steps in the contributor guideline that I've missed? \r\nBTW if you guys are in tensorflow world summit, I'd love to meet. I'll be speaking at the summit tomorrow.", "@rthadur Can you check what else needs to be done for this to be merged? Thanks.", "@ashahab sorry about that , you PR has been stuck internally and developers are working on it to get merged.Thanks for your patience.\r\ncc @mihaimaruseac ", "There is need for more infrastructure changes. I'll tyr to come back to this early next week, sorry for the delay", "Any updates on this?\n\nOn Thu, Dec 5, 2019 at 2:46 AM gbaned <notifications@github.com> wrote:\n\n> *@gbaned* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/keras/callbacks_test.py\n> <https://github.com/tensorflow/tensorflow/pull/32734#discussion_r354233620>\n> :\n>\n> > @@ -23,6 +23,7 @@\n>  import json\n>  import os\n>  import re\n> +import requests\n>\n> @mihaimaruseac <https://github.com/mihaimaruseac> friendly reminder\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/32734?email_source=notifications&email_token=AAFISPKNDAWDJAATBKS4METQXDLYFA5CNFSM4IZODBKKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCOCJXYI#discussion_r354233620>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAFISPKQNLCDCVPVRUSINHLQXDLYFANCNFSM4IZODBKA>\n> .\n>\n", "Apologies. I didn't get a time to look into this. Will dedicate a day this week to handle this and some other similar issues. Apologies again", "Hi Mihai,\nWas there any progress with this? Is there a way I can help set this up?\nAbin\n\nOn Mon, Dec 16, 2019 at 3:15 PM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> Apologies. I didn't get a time to look into this. Will dedicate a day this\n> week to handle this and some other similar issues. Apologies again\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/32734?email_source=notifications&email_token=AAFISPNGFIDSSYMSJ7OQHDDQZAD2TA5CNFSM4IZODBKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHAO33Y#issuecomment-566291951>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAFISPPDGC6V4VD36SX3ERDQZAD2TANCNFSM4IZODBKA>\n> .\n>\n", "Happy new year.\nIs it possible to merge this PR?\n\nOn Thu, Dec 19, 2019 at 9:38 AM Abin Shahab <ashahab@gmail.com> wrote:\n\n> Hi Mihai,\n> Was there any progress with this? Is there a way I can help set this up?\n> Abin\n>\n> On Mon, Dec 16, 2019 at 3:15 PM Mihai Maruseac <notifications@github.com>\n> wrote:\n>\n>> Apologies. I didn't get a time to look into this. Will dedicate a day\n>> this week to handle this and some other similar issues. Apologies again\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/32734?email_source=notifications&email_token=AAFISPNGFIDSSYMSJ7OQHDDQZAD2TA5CNFSM4IZODBKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHAO33Y#issuecomment-566291951>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/AAFISPPDGC6V4VD36SX3ERDQZAD2TANCNFSM4IZODBKA>\n>> .\n>>\n>\n", "Hi. sorry for the delay. Got swamped with high priority stuff and forgot to check this.", "Hi Mihai,\nAnything we can do to help proceed with this task?\nAbin\n\nOn Mon, Jan 6, 2020 at 3:20 PM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> Hi. sorry for the delay. Got swamped with high priority stuff and forgot\n> to check this.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/32734?email_source=notifications&email_token=AAFISPLCBCQZ2QRCSYKCDVLQ4O4CDA5CNFSM4IZODBKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEIHD3YY#issuecomment-571358691>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAFISPOWR3ACQODQH6BUB2LQ4O4CDANCNFSM4IZODBKA>\n> .\n>\n", "I think this is good to import now. Thank you and sorry for the delay", "@mihaimaruseac  many thanks!\r\n@rthadur what's the next step to get this merged?", "This still fails in google with \r\n```\r\n    import requests\r\nModuleNotFoundError: No module named 'requests'`\r\n```\r\n\r\nI think this needs some more manual work internally :(", "@mihaimaruseac What does \"manual work internally\" mean? Do you want me to change the PR? If I make the requests optional, then this may pass but that'd mean skipping the tests always.", "I'll patch the internally imported change. I'll have an update shortly", "One thing why the tests looked OK here but not internally is because of:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8a4e2041f0e1f1710f9a4c70101906d7c39266a4/tensorflow/python/keras/BUILD#L1211\r\n\r\nThat tag tells bazel to not run the test if in open source mode (i.e., outside of Google).", "Can you please remove that tag and try running the tests again?\r\n\r\nI would assume they would fail because `requests` is not added to `workspace.bzl`. Can you add it there, similar to `six_archive` (for `six`)? Or at least tell me which version of `requests` should we import", "I'm patching this internally. No need to do anything", "Hi.\r\n\r\nWe cannot take a dependency on `urllib` internally as it depends on `chardet` which is licensed under LGPL. As such, please surround the import with a `try` block instead.", "@mihaimaruseac I resolved the issues. Please take a look.", "@rthadur @mihaimaruseac  Can we get this merged?", "Sanity test needs fixing\r\n\r\n```\r\nFAIL: Found 4 non-whitelisted pylint errors:\r\ntensorflow/python/keras/callbacks_test.py:1354: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\n\r\ntensorflow/python/keras/callbacks_test.py:1355: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\n\r\ntensorflow/python/keras/callbacks_test.py:1367: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\n\r\ntensorflow/python/keras/callbacks_test.py:1368: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\n```", "That's odd. I don't know why the CI tests on my laptop passed.\n\nOn Sun, Jan 12, 2020 at 4:21 PM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> Sanity test needs fixing\n>\n> FAIL: Found 4 non-whitelisted pylint errors:\n> tensorflow/python/keras/callbacks_test.py:1354: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\n>\n> tensorflow/python/keras/callbacks_test.py:1355: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\n>\n> tensorflow/python/keras/callbacks_test.py:1367: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\n>\n> tensorflow/python/keras/callbacks_test.py:1368: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/32734?email_source=notifications&email_token=AAFISPLJT5I6KRFUTOIEFHTQ5OXW3A5CNFSM4IZODBKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEIXIQWY#issuecomment-573474907>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAFISPJNZDGMBWXLFAGMOLLQ5OXW3ANCNFSM4IZODBKA>\n> .\n>\n", "Apologies. I'll look at this tonight.\n\nOn Mon, Jan 13, 2020 at 1:16 PM Rajeshwar Reddy T <notifications@github.com>\nwrote:\n\n> *@rthadur* requested changes on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/keras/callbacks_test.py\n> <https://github.com/tensorflow/tensorflow/pull/32734#discussion_r366032269>\n> :\n>\n> > @@ -1350,6 +1350,33 @@ def target():\n>        t.join()\n>        assert not t.is_alive()\n>\n> +  @unittest.skipIf(\n> +      requests == None,\n>\n> @ashahab <https://github.com/ashahab> here is the internal error , can\n> you please take a look\n> Traceback (most recent call last): File \"<embedded module '_launcher'>\",\n> line 165, in run_filename_as_main File \"<embedded module '_launcher'>\",\n> line 39, in _run_code_in_main File\n> \"/build/work/google3/runfiles/google3/third_party/tensorflow/python/keras/callbacks_test.py\",\n> line 26, in <module> import requests ModuleNotFoundError: No module named\n> 'requests' -- Forge runner: Test failed with exit code 1\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/32734?email_source=notifications&email_token=AAFISPMCPIXY3TAD4NXC6XDQ5TK4LA5CNFSM4IZODBKKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCRSLJ5A#pullrequestreview-342144244>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAFISPJMMET6NQT62WOZQNTQ5TK4LANCNFSM4IZODBKA>\n> .\n>\n", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32734) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32734) for more info**.\n\n<!-- ok -->", "Nope, no changes in the email address I use to commit, very odd.", "Let me know if there's anything remaining, I don't see any failures in the internal CI builds", "Can you provide an example?\n\nOn Sat, Jan 18, 2020 at 10:24 AM Shanqing Cai <notifications@github.com>\nwrote:\n\n> *@caisq* requested changes on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/keras/callbacks.py\n> <https://github.com/tensorflow/tensorflow/pull/32734#discussion_r368241034>\n> :\n>\n> > @@ -1370,7 +1370,13 @@ def on_epoch_end(self, epoch, logs=None):\n>      send = {}\n>      send['epoch'] = epoch\n>      for k, v in logs.items():\n> -      send[k] = v\n> +      # np.ndarray and np.generic are not scalar types\n> +      # therefore we must unwrap their scalar values and\n> +      # pass to the json-serializable dict 'send'\n> +      if isinstance(v, (np.ndarray, np.generic)):\n> +        send[k] = v.item()\n>\n> Do we know for sure that we'll get only scalar-shaped numpy.ndarrays? I\n> understand that in many cases, this is just a numpy scalar. But I don't\n> know if it's possible that the value can be a non-scalar shape (e.g., (1,\n> 2)) now or in the future.\n>\n> cc @karmel <https://github.com/karmel> @fchollet\n> <https://github.com/fchollet>\n>\n> There are two options:\n>\n>    1. You may want to add a check of len(v.shape) == 0 above.\n>    2. Call x.tolist() instead.\n>\n> I think 2 is the better option, as it works for scalar and non-scalar\n> shapes alike.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/32734?email_source=notifications&email_token=AAFISPL7IGVXLTRQJ6RFIW3Q6NCPRA5CNFSM4IZODBKKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCSHZY3Q#pullrequestreview-344956014>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAFISPM2ENXLUBLROTVCX73Q6NCPRANCNFSM4IZODBKA>\n> .\n>\n", "@ashahab I can't provide any concrete example for non-scalar numpy array OTOH. But it's not inconceivable that metrics such as PR curve, confusion matrix will emerge in the future, and those are not scalars. That's why I said it's just advocating for covering possible edge cases. What's the downside of using `v.tolist()`? It's functionally equivalent when the shape is `()`: it returns a python scalar; when it's not shaped as `()`, it doesn't error out with an error like `ValueError: can only convert an array of size 1 to a Python scalar`.\r\n\r\nThank you.", "@caisq Thanks for your suggestion! \r\nIf I understand correctly, the example of current working structure is this:\r\n```\r\n logs = {'loss': 0., \"val\": np.array(1)}\r\n```\r\nwhich would get flattened to: \r\n```\r\nsend = {'loss': 0., \"epoch\": 0, \"val\": 1}\r\n```\r\nYou are suggesting we support a struct like this:\r\n```\r\n logs = {'loss': 0., \"val\": np.array([[1, 2], [3, 4]])}\r\n```\r\nThis would get flattened to:\r\n```\r\nsend = {'loss': 0., \"epoch\": 0, \"val\": [[1, 2], [3, 4]]}\r\n```\r\nI have a few questions on this approach:\r\n1. What is the cost of doing a recursive `v.tolist()` call on any shaped ndarray? \r\n2. Why isn't this supported in Keras: https://github.com/keras-team/keras/blob/7a39b6c62d43c25472b2c2476bd2a8983ae4f682/keras/callbacks/callbacks.py#L895\r\n", "@ashahab Ah - I wasn't aware that keras-team / keras uses `.item()`. This is indication that at least on the keras-team/keras side, the contract is that metrics and losses will always be a scalar (at least for now), in which case your change is fine. But tf.keras might be different. But I still don't see any argument against `tolist()`. The cost of `tolist()` compared to `item()` on a scalar ndarray should be negligible, and the function is more general. I'll approve the PR now. Feel free to make the `tolist()` change if you agree with me.", "@caisq  Thank you.\r\n@rthadur Let me know if you want me to rebase or can it be pulled.", "It is always better to rebase before pulling.", "@mihaimaruseac  rebased", "I have rebased this. I see 3 jobs got stuck in Expected \u2014 Waiting for status to be reported state. Can someone help?", "@mihaimaruseac Seems like tests have passed after your approval. What is the next step for merging this?", "It needs importing inside Google source control, running more tests there and another approval internally.", "Should be merged soon", "Thanks for the merge. I will work on some of the followup tasks in another PR."]}, {"number": 32733, "title": "Pin Estimator version to newly released 2.0.0", "body": "", "comments": []}, {"number": 32732, "title": "Finetune with additional layers", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu16.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.10\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: 1070TI\r\n\r\nI wonder how to finetune a model with several additional layers\r\nI have a set of checkpoint, including .ckpt and .meta\r\n\r\nI add two deconvolution layers to a previous model\r\n```\r\nflow_pred_x = tf.layers.conv2d_transpose(flow_x, filters=1, kernel_size=8, strides=4, padding='same', use_bias=False, name='slice_deconv_x')\r\nflow_pred_y = tf.layers.conv2d_transpose(flow_y, filters=1, kernel_size=8, strides=4, padding='same', use_bias=False, name='slice_deconv_y')\r\n```\r\n<br>\r\nI tried to use this at first<br>\r\n\r\n'''\r\nsaver = tf.train.Saver()\r\nsaver.restore(session, './model.ckpt')\r\n```\r\nHowever, the error is unmatch<br>\r\nThen I tried to save as .pb\r\n```\r\nconstant_graph = graph_util.convert_variables_to_constants(self.sess, input_graph_def=self.sess.graph_def, output_node_names=['pwcnet/ctxt/refined_flow2', 'pwcnet/flow_x', 'pwcnet/flow_y'])\r\nwith tf.gfile.FastGFile('./modelflowxy.pb', mode='wb') as f:\r\n       f.write(constant_graph.SerializeToString())\r\n```\r\nAnd load the .pb and add layers<br>\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\r\nwith tf.gfile.FastGFile(r'./modelflowxy.pb', 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    _ = tf.import_graph_def(graph_def, name='')\r\nwith tf.Session() as session:\r\n   flow_x = tf.get_default_graph().get_tensor_by_name('pwcnet/flow_x:0')\r\n    flow_y = tf.get_default_graph().get_tensor_by_name('pwcnet/flow_y:0')\r\n    init = np.load('./slice_conv_x_0_w.npy')\r\n    init = tf.constant_initializer(init)\r\n    flow_pred_x = tf.layers.conv2d_transpose(flow_x, filters=1, kernel_size=8, strides=4, padding='same', use_bias=False, name='slice_deconv_x')\r\n    flow_pred_y = tf.layers.conv2d_transpose(flow_y, filters=1, kernel_size=8, strides=4, padding='same', use_bias=False, name='slice_deconv_y')\r\n    flow_pred = tf.concat([flow_pred_x, flow_pred_y], axis=3, name='flow_pred')\r\n    init = tf.global_variables_initializer()\r\n    session.run(init)\r\n    saver = tf.train.Saver(var_list=tf.trainable_variables())\r\n    saver.save(session, './ckpt_test2/all.ckpt')\r\n```\r\n\r\nIt doesn't work too, and the layers in the previous model unmatch\r\nAnd I check the new checkpoint, and find that only two newly added layers are recorded.<br>\r\nSo, I sincerely hope if anyone could tell me how to finetune model with additional layers ", "comments": ["See https://www.tensorflow.org/tutorials/images/transfer_learning for your use case.\r\nIn particular see https://www.tensorflow.org/tutorials/images/transfer_learning#fine_tuning"]}, {"number": 32731, "title": "Tensorflow-gpu 2.0.0.rc1 or rc2 on Ubuntu 19.04 does not find the cuda libraries.", "body": "I am trying to run Tensorflow-gpu 2.0.0.rc1 on ubunto 19.04. I have already installed the stable version (1.14.0) via conda and I have been using it without problem. Meaning all the libraries and drivers are fine however for new tensorflow I get the following errors which is odd because all the libraries are installed in the \"/usr/lib/x86_64-linux-gnu/\".\r\n\r\nI have used \r\n`source ./myenv/bin/activate`\r\n`pip install tensorflow=2.0.0.rc1`\r\n`pip install tensorflow-gpu=2.0.0.rc1`\r\n\r\n```\r\npython -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n2019-09-23 16:20:27.250506: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-09-23 16:20:27.267348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8095\r\npciBusID: 0000:05:00.0\r\n2019-09-23 16:20:27.267424: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/\r\n2019-09-23 16:20:27.267468: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/\r\n2019-09-23 16:20:27.267507: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/\r\n2019-09-23 16:20:27.267545: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/\r\n2019-09-23 16:20:27.267583: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/\r\n2019-09-23 16:20:27.267620: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/\r\n2019-09-23 16:20:27.270227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-09-23 16:20:27.270242: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2019-09-23 16:20:27.270441: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-23 16:20:27.294668: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600075000 Hz\r\n2019-09-23 16:20:27.295454: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4683530 executing computations on platform Host. Devices:\r\n2019-09-23 16:20:27.295488: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-09-23 16:20:27.401522: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x46b6430 executing computations on platform CUDA. Devices:\r\n2019-09-23 16:20:27.401568: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1\r\n2019-09-23 16:20:27.401692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-09-23 16:20:27.401713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]\r\n```", "comments": ["There is an rc2 released this morning, can you try with it please?", "Thank you for the suggestion but still I get the same error. I also confirmed that it has been installed via pip.\r\n\r\n`python -c \"import tensorflow as tf;print(tf.__version__)\"`, \r\n**2.0.0-rc2**\r\n\r\nand \r\n`python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"`\r\n\r\nshows the same error and I noticed that it shows also an output:\r\n**tf.Tensor(1728.6349, shape=(), dtype=float32)**  # which is wrong to me! \r\n\r\nand compare it to the stable version (1.14.0) outputs which I installed via conda:\r\n**Tensor(\"Sum:0\", shape=(), dtype=float32)**\r\n\r\nany idea?", "```tf.Tensor(1728.6349, shape=(), dtype=float32)``` is correct since eager execution is enabled by default in TF 2.X \r\nWhat versions of cuda, cudnn are you using?", "Thanks. I am using cuda 10.1.105, and cudnn 7.6.0.  Do I need to update to latest versions?", "Please switch to cuda 10.0 and update cuda environment paths.\r\nSee https://www.tensorflow.org/install/gpu#software_requirements", "it seems that installing Cuda 10.0 on ubuntu 19.04 with kernel version 5 is very complicated. Because it shows many conflicts and also many packages must be downgraded such as gcc, tcc, g++.\r\n\r\nThanks to comments of @selvamshan on issue #26289, It seems that the exact library names are missing in the library folder. after creating them with symbolic connection to original library it seems the problem is solved. \r\n```sudo ln -s /usr/lib/x86_64-linux-gnu/libcurand.so.10.1.105 /usr/lib/x86_64-linux-gnu/libcurand.so.10.0```\r\n\r\nand do it for the rest of the files.\r\nlibcudart.so.10.0  \r\nlibcublas.so.10.0\r\nlibcufft.so.10.0\r\nlibcurand.so.10.0\r\nlibcusolver.so.10.0\r\nlibcusparse.so.10.0\r\n\r\nI have tested with the \"mnist\" script and it works and outputs similar results. \r\nhowever I don't know why I still get all this verbose messages!! any idea why?\r\n```\r\npython -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n2019-09-25 01:52:56.565869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-09-25 01:52:56.580626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8095\r\npciBusID: 0000:05:00.0\r\n2019-09-25 01:52:56.580787: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-09-25 01:52:56.581898: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-09-25 01:52:56.582680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-09-25 01:52:56.582859: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-09-25 01:52:56.583991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-09-25 01:52:56.584830: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-09-25 01:52:56.587371: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-09-25 01:52:56.588192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-09-25 01:52:56.588398: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-25 01:52:56.617384: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600070000 Hz\r\n2019-09-25 01:52:56.618332: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29d04a0 executing computations on platform Host. Devices:\r\n2019-09-25 01:52:56.618368: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-09-25 01:52:56.717833: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4501870 executing computations on platform CUDA. Devices:\r\n2019-09-25 01:52:56.717872: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1\r\n2019-09-25 01:52:56.719213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8095\r\npciBusID: 0000:05:00.0\r\n2019-09-25 01:52:56.719266: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-09-25 01:52:56.719385: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-09-25 01:52:56.719411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-09-25 01:52:56.719430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-09-25 01:52:56.719449: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-09-25 01:52:56.719539: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-09-25 01:52:56.719565: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-09-25 01:52:56.721928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-09-25 01:52:56.722023: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-09-25 01:52:56.723695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-09-25 01:52:56.723722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-09-25 01:52:56.723737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-09-25 01:52:56.726162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7493 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:05:00.0, compute capability: 6.1)\r\ntf.Tensor(738.868, shape=(), dtype=float32)```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32731\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32731\">No</a>\n", "The messages there are just information log messages. They can be hidden.\r\n"]}, {"number": 32730, "title": "tf.nn.relu on nan inputs returns zeros on GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see minimal example code section below.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1 and 1.14.0\r\n- Python version: 3.6.5\r\n- CUDA/cuDNN version: 10.0, 7.6.3\r\n- GPU model and memory: GeForce GTX 1080 (8GB)\r\n\r\n**Describe the current behavior**\r\nThe behavior of `tf.nn.relu` when fed with `nan`-valued inputs is inconsistent:\r\n- if the input is a constant tensor, `relu` returns `nan`.\r\n- if the input is a variable tensor (like `nan` wrapped into `tf.Variable` or multiplied by a random tensor), it returns zeros.\r\n\r\nThis behavior can only be observed on the GPU. On the CPU, `relu` consistently returns `nan`. The behavior on the CPU is also consistent with other activation functions.\r\n\r\n\r\n**Describe the expected behavior**\r\n`tf.nn.relu` should handle `nan` inputs from all sources consistently. To keep consistency with other activation functions, it should return `nan` in all cases.\r\n\r\n\r\n**Code to reproduce the issue**\r\nThe assertions below all pass. The second and third assertion (the `not` assertions) would be expected to fail.\r\n```python\r\nx1 = tf.nn.relu(np.nan)\r\nx2 = tf.nn.relu(np.nan * tf.random_normal(shape=[]))\r\nx3 = tf.nn.relu(tf.Variable(np.nan))\r\n\r\nwith tf.device(\"/cpu:0\"):\r\n    x1_cpu = tf.nn.relu(np.nan)\r\n    x2_cpu = tf.nn.relu(np.nan * tf.random_normal(shape=[]))\r\n    x3_cpu = tf.nn.relu(tf.Variable(np.nan))\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    assert np.all(np.isnan(sess.run(x1)))\r\n    assert not np.any(np.isnan(sess.run(x2)))  # should fail but does not\r\n    assert not np.any(np.isnan(sess.run(x3)))  # should fail but does not\r\n\r\n    assert np.all(np.allclose(sess.run(x2), 0.0))\r\n    assert np.all(np.allclose(sess.run(x3), 0.0))\r\n\r\n    assert np.all(np.isnan(sess.run(x1_cpu)))\r\n    assert np.all(np.isnan(sess.run(x2_cpu)))\r\n    assert np.all(np.isnan(sess.run(x3_cpu)))\r\n```\r\n\r\n", "comments": ["Thanks for reporting the issue, its replicating for TF-1.14. Kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/062f044c9b794f5ca05dd17070873d5a/32730.ipynb) of colab.", "@msoelch Sorry for the delay in my response. This was resolved in recent `TF1.x` version `TF1.15.3`. I ran your code and cannot reproduce anymore. Please take a look at the [gist](https://colab.research.google.com/gist/jvishnuvardhan/ff32c27d1f56b18d59cafeb02777a1b4/32730.ipynb). Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "@jvishnuvardhan Thank you. The issue indeed seems fixed in `TF1.15.3`. \r\n\r\nHowever, the bug shows in the latest `TF2.2`, with a similar snippet:\r\n```python\r\nx1 = tf.nn.relu(np.nan)\r\nx2 = tf.nn.relu(np.nan * tf.random.normal(shape=[]))\r\nx3 = tf.nn.relu(tf.Variable(np.nan))\r\nwith tf.device(\"/cpu:0\"):\r\n    x1_cpu = tf.nn.relu(np.nan)\r\n    x2_cpu = tf.nn.relu(np.nan * tf.random.normal(shape=[]))\r\n    x3_cpu = tf.nn.relu(tf.Variable(np.nan))\r\n\r\nassert np.all(np.isnan(x1_cpu.numpy()))\r\nassert np.all(np.isnan(x2_cpu.numpy()))\r\nassert np.all(np.isnan(x3_cpu.numpy()))\r\n\r\nassert np.all(np.isnan(x1.numpy()))\r\nassert np.all(np.isnan(x2.numpy()))\r\nassert np.all(np.isnan(x3.numpy()))\r\n```\r\nThe first group of assertions on CPU passes, the second group on GPU fails.", "@msoelch Thanks for the confirmation that it is working as expected with `TF1.15.3`. Please update the question or create a new issue as the original issue was resolved. It will be easy for users to follow the issue. Thanks!", "I've created #40072 as a follow-up for `TF2.2`. That should close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32730\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32730\">No</a>\n"]}, {"number": 32729, "title": "ValueError: Variable <tf.Variable 'basic_nstepTD_1/noisy_dense/bias:0' shape=(3,) dtype=float32> has `None` for gradient.", "body": "I am trying to implement a self-defined noisy-layer, when I put the `add` operator in the `build` function\r\n```python \r\nself.kernel = self.weight_mu + self.weight_sigma * self.weights_eps\r\nself.bias = self.bias_mu + self.bias_sigma * self.bias_eps\r\n```\r\n, it will raise the error that  ` ValueError: Variable <tf.Variable 'basic_nstepTD_1/noisy_dense/bias:0' shape=(3,) dtype=float32> has 'None' for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.`  But when I put that two-line code into the `call` function, the error is disappeared.  \r\nMy code:\r\n```python\r\nclass NoisyDense(kl.Layer):\r\n    def __init__(self, units, std_init=0.5):\r\n        super().__init__()\r\n        self.units = units\r\n        self.std_init = std_init\r\n\r\n    def build(self, input_shape):\r\n        self.reset_noise(input_shape[-1])\r\n        mu_range = 1 / np.sqrt(input_shape[-1])\r\n        mu_initializer = tf.random_uniform_initializer(-mu_range, mu_range)\r\n        sigma_initializer = tf.constant_initializer(self.std_init / np.sqrt(self.units))\r\n\r\n        self.weight_mu = tf.Variable(initial_value=mu_initializer(shape=(input_shape[-1], self.units), dtype='float32'),\r\n                                     trainable=True)\r\n\r\n        self.weight_sigma = tf.Variable(initial_value=sigma_initializer(shape=(input_shape[-1], self.units), dtype='float32'),\r\n                                        trainable=True)\r\n\r\n        self.bias_mu = tf.Variable(initial_value=mu_initializer(shape=(self.units,), dtype='float32'),\r\n                                     trainable=True)\r\n\r\n        self.bias_sigma = tf.Variable(initial_value=sigma_initializer(shape=(self.units,), dtype='float32'),\r\n                                        trainable=True)\r\n\r\n        self.kernel = self.weight_mu + self.weight_sigma * self.weights_eps\r\n        self.bias = self.bias_mu + self.bias_sigma * self.bias_eps\r\n\r\n    def call(self, inputs):\r\n        # output = tf.tensordot(inputs, self.kernel, 1)\r\n        # tf.nn.bias_add(output, self.bias)\r\n        # return output\r\n        # self.kernel = self.weight_mu + self.weight_sigma * self.weights_eps\r\n        # self.bias = self.bias_mu + self.bias_sigma * self.bias_eps\r\n        return tf.matmul(inputs, self.kernel) + self.bias\r\n\r\n    def _scale_noise(self, dim):\r\n        noise = tf.random.normal([dim])\r\n        return tf.sign(noise) * tf.sqrt(tf.abs(noise))\r\n\r\n    def reset_noise(self, input_shape):\r\n        eps_in = self._scale_noise(input_shape)\r\n        eps_out = self._scale_noise(self.units)\r\n        self.weights_eps = tf.multiply(tf.expand_dims(eps_in, 1), eps_out)\r\n        self.bias_eps = eps_out\r\n```\r\n", "comments": ["So, how can I define mu own `initializer` in tensorflow 2.0?", "@Huixxi \r\nLooks like code is incomplete. In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@Huixxi \r\nCan you help us with minimal standalone code to reproduce the issue reported here. Thanks!", "> @Huixxi\r\n> Looks like code is incomplete. In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!\r\n\r\nSorry for late reply, It still an issue, I've the reason but I don't know. The details are updated in the mainbody of that issue.", "@Huixxi \r\nI tried to reproduce the issue in colab using TF 2.0 .But i feel code is incomplete. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/a1a3199a71c49429923e563b5452eada/untitled277.ipynb). Please, help us in reproducing the issue, then it is easy for localizing the issue faster. Thanks again.!", "I've update the code, here\n<https://colab.research.google.com/gist/Huixxi/5dc9af1e4d3d4f4253de9e6312ee194d/untitled277.ipynb>\n\n\nOn Thu, Oct 17, 2019 at 12:14 PM ravikyram <notifications@github.com> wrote:\n\n> @Huixxi <https://github.com/Huixxi>\n> I tried to reproduce the issue in colab using TF 2.0 .But i feel code is\n> incomplete. Please, find the gist here\n> <https://colab.sandbox.google.com/gist/ravikyram/a1a3199a71c49429923e563b5452eada/untitled277.ipynb>.\n> Please, help us in reproducing the issue, then it is easy for localizing\n> the issue faster. Thanks again.!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32729?email_source=notifications&email_token=AFCYYNPXNJXSN63ABWLOR53QO7RAZA5CNFSM4IZLNDR2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBOV6SA#issuecomment-542990152>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFCYYNKWEFDOPKCE5AWHTX3QO7RAZANCNFSM4IZLNDRQ>\n> .\n>\n", "@Huixxi,\r\nYou have commented saying, \r\n\r\n> But when I put that two-line code into the call function, the error is disappeared. \r\n\r\nBut in the [Gist](https://colab.sandbox.google.com/gist/Huixxi/5dc9af1e4d3d4f4253de9e6312ee194d/untitled277.ipynb#scrollTo=nJIgyw1Tt3Xy) which you have provided, there is no error even when the `Two Line Code` is commented in the `Call` function. \r\n\r\nCan you please help us reproduce the error. Thanks!", "Sorry, maybe I made a mistake in my former statement, it's not *Error*, it\nis *Warning*, which means the is no gradient pass through my *weights*, so\nthat I can not train or update the weight parameters, and that is what I\nunderstand, when you put the two line in the build function, you can see\nthat right? It will raise abundant of Warnings, you will see the output\njust run the code in the Gist I provided, *However*, the error will appear.\nwhen I remove the two-line code in the *build function, *line 47, 48, and\nadd them to* call function*, those warnings will disappear.\nThanks. Regards\n\nOn Fri, Oct 18, 2019 at 8:12 PM rmothukuru <notifications@github.com> wrote:\n\n> @Huixxi <https://github.com/Huixxi>,\n> You have commented saying,\n>\n> But when I put that two-line code into the call function, the error is\n> disappeared.\n>\n> But in the Gist\n> <https://colab.sandbox.google.com/gist/Huixxi/5dc9af1e4d3d4f4253de9e6312ee194d/untitled277.ipynb#scrollTo=nJIgyw1Tt3Xy>\n> which you have provided, there is no error even when the Two Line Code is\n> commented in the Call function.\n>\n> Can you please help us reproduce the error. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32729?email_source=notifications&email_token=AFCYYNKXNGHHOFLFQ2ZTQJDQPGR2VA5CNFSM4IZLNDR2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBUDZBY#issuecomment-543702151>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFCYYNNQC3OPUZQOQHUQZY3QPGR2VANCNFSM4IZLNDRQ>\n> .\n>\n", "@Huixxi \r\n \r\nThe build method is called when the model containing the layer is built. This is where you set up the weights of the layer. The input_shape is accepted as an argument to the function.\r\n\r\nThe call method defines the computations performed on the input. The function accepts the input tensor as its argument and returns the output tensor after applying the required operations.\r\n\r\nSo, this is the reason why there are no gradients passing through the weights.\r\n\r\nFor more indepth expanation please read the following [doc](https://www.saama.com/blog/deep-learning-diaries-building-custom-layers-in-keras/)", "@Huixxi Closing this issue as it has been answered. Please add additional comments and we can open this issue again. Thanks!"]}, {"number": 32728, "title": "Memory leak when loading and unloading multiple graphs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not applicable\r\n- TensorFlow installed from (source or binary): I used tensorflow/tensorflow:1.14.0-gpu-py3 docker image (tested also on different version both with and without docker)\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: Python 3.6.8\r\n- Bazel version (if compiling from source): Not applicable\r\n- GCC/Compiler version (if compiling from source): Not applicable\r\n- CUDA/cuDNN version: Cuda 10.0, cuDNN 7\r\n- GPU model and memory: GeForce GTX TITAN X (tested also on GeForce GTX 1080)\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I load multiple Inception V3 graphs into memory and afterwards unload all of them I get a memory leak. With 20 Inception graphs loaded and unloaded, the RAM usage goes up to around 2GB. With 100 Inception graphs, the RAM usage goes up to around 10GB. If I load and unload Inception V3 graphs one by one, there is no memory leak, RAM usage stays below 1GB, does not matter how many Inception V3 graphs I load.\r\n\r\nI tried to load this Inception V3 graph multiple times for purposes of testing: http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz . In reality, I need to load different Inception V3 graphs into memory.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe expected behaviour is that I am able to load multiple Inception V3 graphs and afterwards unload all of them without any memory leak.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport sys \r\nimport os\r\nimport tensorflow as tf\r\nimport gc\r\nimport time\r\n\r\nclass InceptionV3Graph:\r\n    def __init__(self, graph_path):\r\n        with tf.gfile.FastGFile(graph_path, 'rb') as f:\r\n            graph_def = tf.GraphDef()\r\n            graph_def.ParseFromString(f.read())\r\n            _ = tf.import_graph_def(graph_def, name='')\r\n        self.sess = tf.Session(graph=tf.get_default_graph())\r\n\r\n    def close(self):\r\n        tf.reset_default_graph()\r\n        gc.collect()\r\n        self.sess.close()\r\n\r\nif __name__ == \"__main__\":\r\n    graph_path = \"/path/to/classify_image_graph_def.pb\" # you can get classify_image_graph_def.pb from http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\r\n    N_GRAPHS = 100 \r\n    graphs = dict()\r\n    for i in range(N_GRAPHS):\r\n        print(\"Loading graph {}\".format(i+1))\r\n        graphs[i] = InceptionV3Graph(graph_path)\r\n        # If you uncomment these two lines below there won't be any the memory leak\r\n        #graphs[i].close()\r\n        #del graphs[i]\r\n    for i in range(N_GRAPHS):\r\n        print(\"Unloading graph {}\".format(i+1))\r\n        if i in graphs:\r\n            graphs[i].close()\r\n            del graphs[i]\r\n    print(graphs)\r\n    gc.collect()\r\n    print(\"All graphs unloaded\")\r\n    time.sleep(120)\r\n    print(\"Quitting...\")\r\n```\r\n\r\n\r\n**Other info / logs**\r\nI successfully reproduced the problem on two different Linux machines (Ubuntu 16.04 and 18.04) with and without docker. However, the memory leak does not seem to reproduce on Windows 10.\r\n", "comments": ["@uziela,\r\nAs this is more related to TF models, please post this in TF [model repo](https://github.com/tensorflow/models/issues). Thanks!", "Thanks for suggestion, now I have posted the problem to TF model repo:\r\nhttps://github.com/tensorflow/models/issues/7600", "@uziela, Thanks.\r\nWill close issue here. We can track the resolution in that thread. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32728\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32728\">No</a>\n"]}, {"number": 32726, "title": "How to avoid tensorflow@gpu copy outputs to cpu?", "body": "I use tensorflow 1.7.0 at gpu. I call Session::Run function, the inputs Tensor is already gpu Tensor, so input and forward calculate is well. But before output, I see that every outputs has been copied from gpu to cpu, and this is not what I hope. I just want the outputs are all gpu Tensor. How can I do?\r\n\r\n```\r\n// tensorflow/core/public/session.h\r\nclass Session {\r\n...\r\n  virtual Status Run(const std::vector<std::pair<string, Tensor> >& inputs,\r\n                     const std::vector<string>& output_tensor_names,\r\n                     const std::vector<string>& target_node_names,\r\n                     std::vector<Tensor>* outputs) = 0;\r\n...\r\n};\r\n```", "comments": ["the outputs, include target and gradients. MEMCPYDtoH in tensorflow is slow very much, so we hope outputs is gpu Tensors. But I don't know how to ...", "I try MakeCallable/RunCallable in tensorflow 1.14, but the sad story is:\r\n\r\n```\r\ntf session MakeCallable failed, errormsg:Cannot feed or fetch tensor '4_d/fc/bias:0' from device /job:localhost/replica:0/task:0/device:GPU:0 as feeding/fetching from GPU devices is not yet supported for float_ref tensors\r\n```\r\n\r\nThen I see https://github.com/tensorflow/tensorflow/issues/5902\r\n\r\nCan anyone help me ?", "> I use tensorflow 1.7.0 at gpu. I call Session::Run function, the inputs Tensor is already gpu Tensor, so input and forward calculate is well. But before output, I see that every outputs has been copied from gpu to cpu, and this is not what I hope. I just want the outputs are all gpu Tensor. How can I do?\r\n> \r\n> ```\r\n> // tensorflow/core/public/session.h\r\n> class Session {\r\n> ...\r\n>   virtual Status Run(const std::vector<std::pair<string, Tensor> >& inputs,\r\n>                      const std::vector<string>& output_tensor_names,\r\n>                      const std::vector<string>& target_node_names,\r\n>                      std::vector<Tensor>* outputs) = 0;\r\n> ...\r\n> };\r\n> ```\r\n\r\nBy the way, in tensorflow 1.7.0, inputting gpu tensors using Session::Run can work well, avoid MEMCPYHToD. But in 1.14, inputting gpu tensors using Session::Run seems to be forbidden ...", "> I try MakeCallable/RunCallable in tensorflow 1.14, but the sad story is:\r\n> \r\n> ```\r\n> tf session MakeCallable failed, errormsg:Cannot feed or fetch tensor '4_d/fc/bias:0' from device /job:localhost/replica:0/task:0/device:GPU:0 as feeding/fetching from GPU devices is not yet supported for float_ref tensors\r\n> ```\r\n> \r\n> Then I see #5902\r\n> \r\n> Can anyone help me ?\r\n\r\nAnd, intput tensors data type is tensorflow::DT_FLOAT, not tensorflow::DT_FLOAT_REF", "> DT_FLOAT_REF\r\n\r\nThen I change output, the result changed to \r\n\r\n```\r\nCannot feed or fetch tensor 'Placeholder:0' from device /job:localhost/replica:0/task:0/device:GPU:0 as feeding/fetching from GPU devices is not yet supported for int32 tensors\r\n```", "> > I try MakeCallable/RunCallable in tensorflow 1.14, but the sad story is:\r\n> > ```\r\n> > tf session MakeCallable failed, errormsg:Cannot feed or fetch tensor '4_d/fc/bias:0' from device /job:localhost/replica:0/task:0/device:GPU:0 as feeding/fetching from GPU devices is not yet supported for float_ref tensors\r\n> > ```\r\n> > \r\n> > \r\n> > Then I see #5902\r\n> > Can anyone help me ?\r\n> \r\n> And, intput tensors data type is tensorflow::DT_FLOAT, not tensorflow::DT_FLOAT_REF\r\n\r\nI found that _REF is because variable definition (fc/bias, fc/kernel, etc), even thought I don't put them into output...", "The issue you're seeing is because of ref variables. Instead of fetching ref variables, fetch the result of tf.identity(ref_variable) (or use tf.enable_resource_variables() when building your graph to stop using the deprecated ref variables).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32726\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32726\">No</a>\n", "When I call session MakeCallable, same message comes out. Seems like TensorFlow disallow feeding Variable from gpu device?"]}, {"number": 32725, "title": "run interpreter.invoke() just show Aborted (core dumped)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n-(yes) Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n-(Linux Ubuntu 16.04) OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- (conda) TensorFlow installed from (source or binary):\r\n- (1.13.1 and 1.14) TensorFlow version (use command below):\r\n- (3.6.8) Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen I use tf version 1.13.1 to convert pb to tflite, it shows dim not matched error, but when I use 1.14 to convert, it succeeds to save the tflite file. But when I use the test code in the tf doc, it just shows core dumped.\r\n**Describe the expected behavior**\r\ni want to use the tflite file in android, however, i even can't use it in python\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\ncode generated the tflite file\r\n```\r\nimport tensorflow as tf\r\n\r\ngraph_def_file = \"../model/resnet18/weights-89-0.952.pb\"\r\ninput_arrays = [\"input\"]\r\noutput_arrays = [\"lambda_1/l2_normalize\"]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n  graph_def_file, input_arrays, output_arrays, input_shapes={\"input\" : [1, 257, 400,1]})\r\ntflite_model = converter.convert()\r\nopen(\"../model/resnet18/converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\ncode to test in python(from doc)\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"/home/dm/Desktop/VGG-Speaker-Recognition/model/resnet18/converted_model.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\nprint(input_details)\r\nprint(output_details)\r\n# Test model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\nprint(input_data.shape)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nconverted with 1.13.1\r\n\r\n     ture_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA  \r\n    2019-09-23 14:33:07.069746: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz  \r\n    2019-09-23 14:33:07.072588: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x564ecd24a790 executing computations on platform Host. Devices:  \r\n    2019-09-23 14:33:07.072658: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>  \r\n    Ignore 'tcmalloc: large alloc' warnings.  \r\n    Traceback (most recent call last):  \r\n      File \"pb-to-tflite.py\", line 9, in <module>  \r\n        tflite_model = converter.convert()  \r\n      File \"/home/dm/anaconda3/envs/s-t3/lib/python3.6/site- \r\n     packages/tensorflow/lite/python/lite.py\", line 455, in convert  \r\n        **converter_kwargs)  \r\n      File \"/home/dm/anaconda3/envs/s-t3/lib/python3.6/site- \r\n     packages/tensorflow/lite/python/convert.py\", line 442, in toco_convert_impl  \r\n        input_data.SerializeToString())  \r\n      File \"/home/dm/anaconda3/envs/s-t3/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos  \r\n        \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.  \r\n    2019-09-23 14:33:21.269591: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 529 operators, 854 arrays (0 quantized)  \r\n    2019-09-23 14:33:21.275653: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 527 operators, 849 arrays (0 quantized)  \r\n    2019-09-23 14:33:21.283483: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 527 operators, 849 arrays (0 quantized)  \r\n    2019-09-23 14:33:21.330461: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:117] Check failed: dim_x == dim_y (512 vs. 10)Dimensions must match  \r\n    Aborted (core dumped) \r\n\r\n \r\n\r\n\r\nconverted by 1.14\r\n \r\n\r\n    2019-09-23 14:38:45.285133: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize  \r\n    2019-09-23 14:38:45.285519: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 440 nodes (-112), 631 edges (-112), time = 213.324ms.  \r\n    2019-09-23 14:38:45.285580: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 440 nodes (0), 631 edges (0), time = 54.184ms.  \r\n\r\n\r\nand when I use the code above to test ,it just shows\r\n\r\n     [{'name': 'input', 'index': 95, 'shape': array([  1, 257, 400,   1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]  \r\n    [{'name': 'lambda_1/l2_normalize', 'index': 96, 'shape': array([  1, 512], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]  \r\n    (1, 257, 400, 1)  \r\n    Aborted (core dumped)  \r\n\r\nand I could use netron to open the generated tflite file\r\n[the network structure it shows][1]\r\n\r\nhere is the code of this model\r\nhttps://github.com/WeidiXie/VGG-Speaker-Recognition\r\n\r\ncould somebody give me some help?\r\n\r\n  [1]: https://i.stack.imgur.com/zlSAn.png\r\n", "comments": ["by selecting different graph node as the output , i found maybe is the \"sub\" node making this error, i defined my own trainable variable in keras, and sub them to calculate the residual, is there any way to fixed this problem?\r\n![image](https://user-images.githubusercontent.com/31615877/65477728-c9f36b80-deb9-11e9-949c-849de9df7cd3.png)\r\n", "i try to avoid to use broadcast, but still not work, and i also try to use tf.get_variable() to replace keras add_weight() in self-define layer, but not work again", "I also have the same issue", "i found the reason of my problem is because when two tensors subtract using broadcast. but if the tensor dim is smaller than 5, this problem will not occur. Once the tensor dim is larger than 4, it can't use broadcast or will be broken. But when i try to use tf.tile to avoid using broadcast, the tflite will move this tile node from the graph, may be it's where the reason from.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32725\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32725\">No</a>\n", "the same issue, is there any solution?"]}, {"number": 32724, "title": "Tensorflow Lite for Android on x86_64", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nubuntu 16.0\r\n- TensorFlow installed from (source or binary):\r\nYes\r\n- TensorFlow version (or github SHA if from source):\r\nmaster\r\n\r\n\r\nHi  , I like to build Tensorflow Lite to run on x86_64 with Android.\r\nI see there is package downloaded  NEON_2_SSE and there is a scripts in tools/make build_XXX for Android/Ios/RPI\r\nCan anyone suggest how to combine both SSE and Android .\r\nThanks.", "comments": ["We publish .aar's that have x86_64 binaries. If you look at [our examples](https://www.tensorflow.org/lite/examples), each of them should run on x86_64 when built with Android Studio and/or gradle.\r\n\r\nOtherwise, if you want to build manually, you can follow the instructions @ https://www.tensorflow.org/lite/guide/android. The x86 build should automatically use NEON_2_SSE for our CPU kernels.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32724\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32724\">No</a>\n"]}, {"number": 32723, "title": "model.ckpt-180000.data-00000-of-00001.tempstate5599057054002993368 appeared", "body": "I am running a image resnet152 model in imagenet dataset, However, when 18W step, a strange file named model.ckpt-180000.data-00000-of-00001.tempstate5599057054002993368 appeared, then my validation is wrong like  \r\n\r\nOP_REQUIRES failed at save_restore_v2_ops.cc:134 : Unknown: output/checkpoint/model.ckpt-180000.data-00000-of-00001.tempstate5599057054002993368; Input/output error .\r\n\r\nIt troubled me many days , is there anyone meet the same problem? hope someone can help me. thank you!!\r\n\r\n\r\n", "comments": ["Apologies for the delay in response.\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32723\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32723\">No</a>\n"]}, {"number": 32722, "title": "AttributeError: module 'tensorflow.python.ops.gen_logging_ops' has no attribute '_scalar_summary'v", "body": "Hello \r\nI getting this error while train my data i  already install latest version of tensor flow  \r\nerrors\r\n\r\nAttributeError: module 'tensorflow.python.ops.gen_logging_ops' has no attribute '_scalar_summary'", "comments": ["Can you provide a sample code? Please use [proper makrdown syntax to highlight the code](https://guides.github.com/features/mastering-markdown/#)", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@rahimullah11 ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 32721, "title": "how to feed mere than one enter in tensorlow c++", "body": "hi I just new some help with this graph \r\n![image](https://user-images.githubusercontent.com/30030792/65396297-9eae3680-dd6a-11e9-9c23-62a9c25ceac8.png)\r\n as you can see in the graph, I have more than one enter, are 3 images, I made in the past some feeding in c++ tensorfow but only with one enter I mean one std::string and one tensor, how can I feed the 3 images at the same time 3 std::strings and the 3 tensors calling just one session thanks \r\n", "comments": ["_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Overflow._\r\n\r\nThis isn't a Tensorflow issue. [See if this S/O answers your question.](https://stackoverflow.com/questions/40318457/how-to-build-a-multiple-input-graph-with-tensor-flow) When you post on S/O please give a bit more detail about how you are constructing your graph. Cheers.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32721\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32721\">No</a>\n"]}, {"number": 32720, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value with BatchNorm and Map_fn", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\nAny help would be very greatly appreciated! Thanks!\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.06 & OSX 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14 on both os's, tf-gpu on ubuntu\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: (ubuntu only) 10.1\r\n- GPU model and memory: Titan X Pascal\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nApplying batch normalization on top of batchwise tf.map_fn results in retval[0] error.  Note that this fails even when tf.map_fn operates over convolution filters. In this latter case, we have a different filter for each batch element, and so cannot apply the normal tf.conv functions without a map.\r\n\r\n**Describe the expected behavior**\r\nApplying batch normalization on top of batchwise tf.map_fn should pass without issue, as in this case it should emulate batchwise matmul.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nBelow is the code. Alternatively, it can be found in this txt file:\r\n[run_dummy.txt](https://github.com/tensorflow/tensorflow/files/3643105/run_dummy.txt)\r\n\r\n\r\n\r\n```python3\r\nimport tensorflow as tf\r\n\r\n\r\nclass DummyModelRunner(object):\r\n    def __init__(self):\r\n        self.optimizer = tf.train.AdamOptimizer()\r\n\r\n        with tf.variable_scope('Dummy_Vars', use_resource=True):\r\n            self.variables = self.create_variables()\r\n\r\n        self.is_train = tf.placeholder_with_default(False, shape=[], name='is_train')\r\n\r\n        self.batch_inputs = tf.ones(shape=[256, 10], dtype=tf.float32, name='e1_embs')\r\n        actual_answers = tf.ones(shape=[256, 10, 5], dtype=tf.float32, name='actual_answers')\r\n        targets = tf.ones(shape=[256, 5], dtype=tf.float32, name='targets')\r\n\r\n        self.batch_outputs = self.create_predictions(input_embs=self.batch_inputs)\r\n\r\n        self.predictions = self.compute_likelihoods(self.batch_outputs, actual_answers)\r\n\r\n        self.loss = self.create_loss(self.predictions, targets)\r\n\r\n        # The following control dependency is needed in order for batch\r\n        # normalization to work correctly.\r\n        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        with tf.control_dependencies(self.update_ops):\r\n            self.train_op = self.optimizer.minimize(self.loss)\r\n            gradients, variables = zip(*self.optimizer.compute_gradients(self.loss))\r\n            gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\r\n            self.train_op = self.optimizer.apply_gradients(zip(gradients, variables))\r\n\r\n    def create_variables(self):\r\n        \"\"\"\r\n        create all network variables\r\n        :return: all variable dictionary\r\n        \"\"\"\r\n        weight = tf.get_variable(name='DummyWeight',\r\n                                 shape=[10, 10],\r\n                                 dtype=tf.float32,\r\n                                 initializer=tf.contrib.layers.xavier_initializer())\r\n        bias = tf.get_variable(name='DummyBiasRel',\r\n                               shape=[1, 10],\r\n                               dtype=tf.float32,\r\n                               initializer=tf.zeros_initializer())\r\n\r\n        variables = {'weights': weight,\r\n                     'biases': bias}\r\n\r\n        return variables\r\n\r\n\r\n    def create_predictions(self, input_embs):\r\n        weight, bias = self.variables['weights'], self.variables['biases']\r\n        is_train_batch_norm = self.is_train\r\n\r\n        def matmul(pair):\r\n            input_tensor = pair[0][None]\r\n            projection_tensor = pair[1]\r\n            projection = tf.matmul(input_tensor, projection_tensor)[0]\r\n            return (projection, tf.zeros([]))\r\n\r\n        output = tf.reshape(input_embs, [tf.shape(input_embs)[0], -1])\r\n        # Next two lines cause problem\r\n        weight = tf.broadcast_to(weight, [256, 10, 10])\r\n        output = tf.map_fn(fn=matmul, elems=(output, weight))[0] + bias\r\n\r\n        output = tf.layers.batch_normalization(output, momentum=.1, reuse=tf.AUTO_REUSE,\r\n                                               training=is_train_batch_norm, fused=True,\r\n                                               name='DummyBatchNorm')\r\n\r\n        return output\r\n\r\n    def compute_likelihoods(self, input_vectors, actual_answers):\r\n        with tf.name_scope('output_layer'):\r\n\r\n            predictions = tf.matmul(input_vectors[:, None, :], actual_answers)[:, 0, :]\r\n\r\n        return predictions\r\n\r\n    def create_loss(self, predictions, targets):\r\n        with tf.name_scope('loss'):\r\n            loss = tf.reduce_sum(\r\n                tf.compat.v1.losses.sigmoid_cross_entropy(targets, predictions),\r\n                name='loss')\r\n        return loss\r\n\r\n\r\nif __name__ == '__main__':\r\n    # Create the model.\r\n    # Can be /GPU:0 for ubuntu\r\n    with tf.device('/CPU:0'):\r\n        # We are using resource variables because due to some implementation details, this allows us to\r\n        # better utilize GPUs while training.\r\n        with tf.variable_scope('variables', use_resource=True):\r\n\r\n            model = DummyModelRunner()\r\n\r\n    # Create a TensorFlow session and start training.\r\n    config = tf.ConfigProto(allow_soft_placement=True)\r\n    config.gpu_options.allow_growth = True\r\n    session = tf.Session(config=config)\r\n    saver = tf.train.Saver()\r\n\r\n    # Initialize the values of all variables and the train dataset iterator.\r\n    session.run(tf.global_variables_initializer())\r\n\r\n    for step in range(100):\r\n        # print('Hi!')\r\n        feed_dict = {model.is_train: True}\r\n\r\n        loss, _ = session.run((model.loss, model.train_op), feed_dict)\r\n\r\n        print(loss)\r\n        exit()\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nTraceback below (deprecation warning's excluded. Please let me know if I should include those as well):\r\nTraceback (most recent call last):\r\n  File \"/Users/georgestoica/Desktop/Research/venvs/rl_3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/Users/georgestoica/Desktop/Research/venvs/rl_3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/Users/georgestoica/Desktop/Research/venvs/rl_3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/georgestoica/Desktop/Research/new_exploration/src/qa_cpg/run_cpg_minimal.py\", line 111, in <module>\r\n    loss, _ = session.run((model.loss, model.train_op), feed_dict)\r\n  File \"/Users/georgestoica/Desktop/Research/venvs/rl_3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/georgestoica/Desktop/Research/venvs/rl_3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/Users/georgestoica/Desktop/Research/venvs/rl_3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/Users/georgestoica/Desktop/Research/venvs/rl_3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value\r\n\r\n", "comments": ["@gstoica27 ,\r\nThank you for reporting the issue.\r\nCan you please provide code in any text-file with right indentation ?Thanks!", "Please use [proper markdown](https://guides.github.com/features/mastering-markdown/#) to format the code so that it is easier to read. Thanks.", "Hi, thanks for your responses and apologies for the code malformatting! I've just formatted the code according to @mihaimaruseac's suggestions and added a txt file as per @oanush request.\r\n\r\nHope this helps! Please let me know if there is anything I else I can/should provide. Thanks!", "Issue replicating for TF-[1.15rc1](https://colab.sandbox.google.com/gist/oanush/f481f4351f8ab1b74a2ceaeba90b05b3/32720.ipynb) and also TF-nightly-1.15,kindly find the gist of colab.Thanks!", "Following warning log can help understand the behaviour:\r\n```python\r\nWARNING:tensorflow:From <ipython-input-1-227adad630b4>:69: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\r\n```\r\nThe script executes successfully if control dependencies are eliminated.", "I saw that warning at the time, but I understood that warning as stating \"you shouldn't use tf.control_dependencies(...) with keras.layers.BatchNormalization\", especially because the [documentation](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/layers/batch_normalization) on tf.layers.batch_normalization contains the starred section:\r\n\r\n> Note: when training, the moving_mean and moving_variance need to be updated. By default the update ops are placed in tf.GraphKeys.UPDATE_OPS, so they need to be executed alongside the train_op. Also, be sure to add any batch_normalization ops before getting the update_ops collection. Otherwise, update_ops will be empty, and training/inference will not work properly. \r\n\r\nIs that warning overwriting the documentation, and instead stating to not use control dependencies for this tf.layers.batchnorm anymore?\r\n\r\nThanks for the help!", "TensorFlow's default way of updating BatchNorm moving statistics by a collection, is known to cause various kind of bugs.\r\nBy replacing your BatchNorm layer with [tensorpack's BatchNorm layer](https://tensorpack.readthedocs.io/modules/models.html#tensorpack.models.BatchNorm), your code can then run correctly. The diff is:\r\n```diff\r\n70,72c70,71\r\n<         output = tf.layers.batch_normalization(output, momentum=.1, reuse=tf.AUTO_REUSE,\r\n<                                                training=is_train_batch_norm, fused=True,\r\n<                                                name='DummyBatchNorm')\r\n---\r\n>         from tensorpack.models import BatchNorm\r\n>         output = BatchNorm('DummyBatchNorm', output, momentum=.1, ema_update='internal')\r\n93a93\r\n>     from tensorpack.tfutils import TowerContext\r\n97c97\r\n<         with tf.variable_scope('variables', use_resource=True):\r\n---\r\n>         with TowerContext('', is_training=True), tf.variable_scope('variables', use_resource=True):\r\n```", "Is this still an issue? Can you please test with latest version of TensorFlow 2.X and check if the issue exists.\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32720\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32720\">No</a>\n"]}, {"number": 32719, "title": "bundle_reader read tensor larger than 2GB will lead index overflow", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nin tensor_bundle.cc method GetValue \r\nif the tensor has entry.size() larger than 2GB the underlying hdfspread method accept a int32 type of size but entry.size() has 64 bit \r\nthe static_cast will trigger a error of invalid param since it will cast the 64bit to a negative integer\r\n\r\n**Describe the expected behavior**\r\n\r\nif the entry size larger than 2GB, we should read it from parts not in a whole\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@churvey,\r\nPlease provide details about your TensorFlow version. Also, did you compile from source or install a binary?\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "it is fixed in master branch "]}, {"number": 32718, "title": "[TF2.0] TensorFlow nightly build for Python 3.7", "body": "**System information**\r\n- OS Platform and Distribution: **macOS**\r\n- TensorFlow version: **tf-nightly-2.0-preview**\r\n- Python version: **3.7**\r\n- Installed using virtualenv? pip? conda?: **conda**\r\n\r\nHello everyone,\r\n\r\nI can't install tensorflow 2.0 nighly build for `py3.7`:\r\n\r\n```bash\r\n(py3.7) artemav:~/code/GPflow (awav/gpflow-2.0)\r\n\u2192 pip install tf-nightly-2.0-preview\r\nCollecting tf-nightly-2.0-preview\r\n  ERROR: Could not find a version that satisfies the requirement tf-nightly-2.0-preview (from versions: none)\r\nERROR: No matching distribution found for tf-nightly-2.0-preview\r\n```\r\n\r\n", "comments": ["@awav \r\n\r\nCan you please downgrade to Python 3.6.8 and see if the issue still persists. Thanks!", "@ravikyram , I didn't have problem with installing TF2.0-preview for _py3.6.8_. According to the tensorflow page it supports 3.7 and it has a pip package for that python version, whereas TF2.0-preview doesn't have a build for _py3.7_.", "@awav I could reproduce the issue. However, you could install most recent release candidate (rc2) as `pip3 install tensorflow==2.0.0rc2`. Thanks\r\n\r\n@mihaimaruseac Are we not releasing any pips for 2.0 nightly on python 3.7.? You have commented earlier for [similar issue](https://github.com/tensorflow/tensorflow/issues/27470#issuecomment-530108258) on Wiondows. Any thoughts on this for Mac? Thanks!\r\n", "@awav Based on @mihaimaruseac, apparently we're not releasing nightlies for py3.7 except for Linux. However, this will be fixed when 2.0 is released. Till then, please use `pip3 install tensorflow==2.0.0rc2`. Thanks!", "Mac python3.7 2.0 nightly builds should now be available. \r\nhttps://pypi.org/project/tf-nightly-2.0-preview/2.0.0.dev20190926/#files\r\n\r\nsame for windows python 3.7 version\r\ncpu - https://pypi.org/project/tf-nightly-2.0-preview/2.0.0.dev20190926/#files\r\ngpu - https://pypi.org/project/tf-nightly-gpu-2.0-preview/2.0.0.dev20190926/#files", "Thanks for reporting the issue, am closing this now based on the above comment. Feel free to open if you are not satisfied with this resolution. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32718\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32718\">No</a>\n"]}, {"number": 32717, "title": " Did you open MPI when compiling the WHL package for this URL?https://pypi.org/project/tensorflow/1.14.0/#files", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nDid you open MPI when compiling the WHL package for this URL?https://pypi.org/project/tensorflow/1.14.0/#files\r\n![image](https://user-images.githubusercontent.com/25795827/65386223-27c56d80-dd6b-11e9-9d2b-949b5b71688d.png)\r\n\r\nthanks\uff01", "comments": ["I'm sorry, I don't understand what the problem here is. Please file another issue and be sure to fully explain your bug or installation issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32717\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32717\">No</a>\n"]}, {"number": 32716, "title": "Tensorflow Serving Bad Docker Example", "body": "Please provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/tfx/serving/docker\r\n\r\n## Description of issue (what needs changing):\r\nThe first example does not work when you get to this command:\r\n\r\ndocker run -t --rm -p 8501:8501 \\\r\n    -v \"$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two\" \\\r\n    -e MODEL_NAME=half_plus_two \\\r\n    tensorflow/serving &\r\n\r\nThe command will repeatedly return the error:\r\n\"No versions of servable half_plus_two found under base path /models/half_plus_two\"\r\n\r\nThis happens when you go through all of the steps sequentially, as they are listed in that example. \r\n\r\nAdditionally, the example has the headline \"One of the easiest ways to get started using TensorFlow Serving is with Docker.\"\r\n\r\nThis is contradicted by the difficulties experienced with that example. \r\n\r\n### Submit a pull request?\r\nI'm afraid I don't know what the correct command is, so I currently will not be submitting a pull request. ", "comments": ["@DonaldM164 ,\r\nPlease check the [link](https://github.com/tensorflow/serving/issues/1419) and also [Stack-overflow](https://stackoverflow.com/questions/45544928/tensorflow-serving-no-versions-of-servable-model-found-under-base-path) link.Thanks!", "Thanks for the reply oanush. I actually did see this stack overflow problem resolution before, however using the example given currently, all models are assigned the number 0000123, so it would seem that a version number is assigned . Despite it not working (for myself, this would be a support issue), I thought it would also be beneficial to bring this up as a documentation problem as well. If the example doesn't work without modification on the user end, then perhaps the example should be revisited and updated accordingly. \r\n\r\nFollowing the [link](https://github.com/tensorflow/serving/issues/1419) I found that the original poster did not have this problem on ubuntu but did have this problem on Windows. I too am using Windows 10 (Home) with docker, utilizing Virtual Box instead of Hypervisor. \r\n", "@DonaldM164 ,\r\nThis is a TF-serving issue, please open the issue in  [TFServing](https://github.com/tensorflow/serving/issues) repo.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32716\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32716\">No</a>\n"]}, {"number": 32715, "title": "About FGSM implementation in the tutorial", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/beta/tutorials/generative/adversarial_fgsm\r\n## Description of issue (what needs changing):\r\n\r\nThe FGSM  implementaiton in the documentation seems to be incorrect.\r\n\r\n### Clear description\r\n\r\nIn the doc, `image_probs` which is equal to the value `model.predict(image)`,  is used to calculate the perturbation.\r\n```python\r\nperturbations = create_adversarial_pattern(image, image_probs)\r\n```\r\n\r\nThe `create_adversarial_pattern` function takes `input_image` and `input_label`. So the above code is the same as the blow code.\r\n```python\r\nperturbations = create_adversarial_pattern(input_image=image, input_label=model.predict(image))\r\n```\r\n\r\nHowever, `input_label` must be not the predicted _probability_ of the model, but the (one hot encoded) correct _label_ of input_image, I think. In fact, it is calculated that\r\n```python\r\nprediction = pretrained_model(input_image)\r\nloss = loss_object(input_label, prediction)\r\n```\r\nin the `create_adversarial_pattern` function.\r\n\r\nSorry if I have misunderstood.\r\n\r\nref. \"EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES,\" https://arxiv.org/pdf/1412.6572.pdf, p3.\r\n\r\n### ### Submit a pull request?\r\nIf my understanding is correct, I will submit a PR. But I do not have confidence that my understanding is correct yet.\r\n\r\n\r\n", "comments": ["You're right about the error.\r\n\r\n> Submit a pull request?\r\n\r\nYes please."]}, {"number": 32714, "title": "Fix _CumprodGrad for 0. inputs", "body": "The gradient of cumprod returned nan/inf, if it is given 0 inputs. A simple solution to this problem is to use math_ops.div_no_nan to compute the gradient instead of a normal div. This has the exact behaviour we need, it returns 0 for positions where x is 0, which is exactly the gradient at these positions anyways.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32714) for more info**.\n\n<!-- need_sender_cla -->", "This should be opened on master and then (if needed) cherry-picked into release branches", "Is this pr a correct solution? Why hasn't it been merged...", "Because development happens only on master branch and features are cherrypicked on release branches. This PR was created against a release branch directly.", "Thanks for your reply. So I'll make a pull request on the master branch.\r\nAnd maybe someone else can check about it."]}, {"number": 32713, "title": "Ussage via JNI (libtensorflow-1.14.0.jar+libtensorflow-src.jar+tensorflow_jni.dll)  JDK=1.8", "body": "Hello! Append lib (libtensorflow-1.14.0.jar) as User library on Eclipse, include libtensorflow-src.jar as source for this user lib, try to build examle and java take me:\r\n`Exception in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86_64.`\r\n\r\nwhy resoure name is `org/tensorflow/native/windows-x86_64/tensorflow_jni.dll`?\r\n\r\nNote: \r\nlibtensorflow-1.14.0.jar+libtensorflow-src.jar+tensorflow_jni.dll  in project folder.\r\n\r\nWTD?\r\n", "comments": ["@Yrij-Zhavoronkov,\r\nProvide the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "1) Go to: https://www.tensorflow.org/install/lang_java and download libtensorflow.jar and Java Native Interface (JNI) file, save it to project root;\r\n2) Go to: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/README.md and download soure jar for libtensorflow, save to project root;\r\n2) In Eclipse:  Menu of project -> build Path -> Configure -> Libraries;\r\n3) Create new user lib \r\n- set libtensorflow-1.14.0.jar as jar for lib, \r\n- libtensorflow-src.jar as soures, \r\n- native library location set to project root.\r\n- Apply, Finish;\r\n3) Copy Example code from https://www.tensorflow.org/install/lang_java and try to run/debug\r\n\r\nExample: \r\n`public static void main(String[] args) {\r\n\t\tObjectDetector detector = new ObjectDetector();\r\n\t}\r\n\r\n\tpublic ObjectDetector() {\r\n\t\ttry (Graph g = new Graph()) {\r\n\t\t      final String value = \"Hello from \" + TensorFlow.version();\r\n\t\t\r\n\t\ttry (Tensor t = Tensor.create(value.getBytes(\"UTF-8\"))) {\r\n\t        // The Java API doesn't yet include convenience functions for adding operations.\r\n\t        g.opBuilder(\"Const\", \"MyConst\").setAttr(\"dtype\", t.dataType()).setAttr(\"value\", t).build();\r\n\t    } catch (UnsupportedEncodingException e) {\r\n\t\t\t// TODO Auto-generated catch block\r\n\t\t\te.printStackTrace();\r\n\t\t}\r\n\t\t\r\n\t\t// Execute the \"MyConst\" operation in a Session.\r\n\t      try (Session s = new Session(g);\r\n\t          // Generally, there may be multiple output tensors,\r\n\t          // all of them must be closed to prevent resource leaks.\r\n\t          Tensor output = s.runner().fetch(\"MyConst\").run().get(0)) {\r\n\t        System.out.println(new String(output.bytesValue(), \"UTF-8\"));\r\n\t      } catch (UnsupportedEncodingException e) {\r\n\t\t\t// TODO Auto-generated catch block\r\n\t\t\te.printStackTrace();\r\n\t\t}\r\n\t\t}\r\n\t}`", "Any ideas?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32713\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32713\">No</a>\n"]}, {"number": 32712, "title": "TPUStrategy.make_dataset_iterator does not work for a Dataset created by tf.data.Dataset.from_generator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab default environment\r\n- TensorFlow installed from (source or binary): Colab  default\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: Python 3\r\n\r\n**Describe the current behavior**\r\nI first create a Dataset with an iterator, then create a TPU iterator with the Dataset. After that, the iterator is used as the second input of `TPUStrategy.experimental_run`. As I run the output of `experimental_run`,  an error occurs:\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1340       return self._call_tf_sessionrun(\r\n-> 1341           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1342 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1428         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1429         run_metadata)\r\n   1430 \r\n\r\nAbortedError: Session 5c40387f44056164 is not found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAbortedError                              Traceback (most recent call last)\r\n<ipython-input-7-b0bded0c272b> in <module>()\r\n     18     # Custom training loop\r\n     19     session.run(train_iterator_init)\r\n---> 20     session.run(dist_train)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    948     try:\r\n    949       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 950                          run_metadata_ptr)\r\n    951       if run_metadata:\r\n    952         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1171     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1172       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1173                              feed_dict_tensor, options, run_metadata)\r\n   1174     else:\r\n   1175       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1348     if handle is None:\r\n   1349       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1350                            run_metadata)\r\n   1351     else:\r\n   1352       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1368           pass\r\n   1369       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1370       raise type(e)(node_def, op, message)\r\n   1371 \r\n   1372   def _extend_graph(self):\r\n\r\nAbortedError: Session 5c40387f44056164 is not found.\r\n```\r\n**Describe the expected behavior**\r\nRun without error.\r\n**Code to reproduce the issue**\r\n```\r\nimport os\r\nimport pprint\r\nimport tensorflow as tf\r\nimport numpy as np\r\nresolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.contrib.distribute.initialize_tpu_system(resolver)\r\nstrategy = tf.contrib.distribute.TPUStrategy(resolver)\r\n\r\ndef my_generator():\r\n    for i in range(100):\r\n      x = np.random.rand(28,28).astype('float32')\r\n      y = np.zeros([], dtype='int32')\r\n      yield x,y\r\n\r\ndef train_fn(inputs):\r\n    return inputs[0]\r\n\r\nwith strategy.scope():\r\n  config = tf.ConfigProto()\r\n  config.allow_soft_placement = True\r\n  cluster_spec = resolver.cluster_spec()\r\n  if cluster_spec:\r\n    config.cluster_def.CopyFrom(cluster_spec.as_cluster_def())\r\n  print('Starting training...')\r\n  # Do all the computations inside a Session (as opposed to doing eager mode)\r\n  with tf.Session(target=resolver.master(), config=config) as session:  \r\n    #train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(8, drop_remainder=True)\r\n    train_dataset = tf.data.Dataset.from_generator(my_generator, ('float32', 'int32'), ([28,28], [])).batch(8, drop_remainder=True)\r\n    train_iterator = strategy.make_dataset_iterator(train_dataset)\r\n    train_iterator_init = train_iterator.initialize()\r\n    dist_train = strategy.experimental_run(train_fn, train_iterator).values\r\n\r\n    # Custom training loop\r\n    session.run(train_iterator_init)\r\n    session.run(dist_train)\r\n```\r\nThis snippet of code is a simplified version of  [this Colab notebook](https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/tutorials/distribute/tpu_custom_training.ipynb).  The original `tf.data.Dataset.from_tensor_slices` works fine but  `tf.data.Dataset.from_generator` fails.\r\n\r\nAt the same time, I provide a [Colab notebook](https://colab.research.google.com/drive/1SNCAPKRcF_9XCSunRYvb6rGWkOcrweuN) for reproducing this issue.\r\n\r\n", "comments": ["I have tried on colab with TF version 1.14 and was able to reproduce the issue.However i tried reproducing the issue with TF 1.15.0-rc1 and seeing the below error. \r\n`NotFoundError: Op type not registered 'RebatchDataset' in binary running on n-f4303a18-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/11adb2b7180fb825bfa006b303f0cfcd/untitled210.ipynb).Thanks!", "@rxsang does from_generator work with TPUStrategy at all? \r\n\r\nAlso, from the error in https://github.com/tensorflow/tensorflow/issues/32712#issuecomment-533975396, it seems that TPU binary for 1.15 is not yet updated, is that the case? ", "I'm encountering this same issue with TF 1.15 on TPU v3. Are there any workarounds?", "tf.data.Dataset.from_generator is not supported in TPU because TPU is a 2 VM setup, where the TPU worker is a C++ binary which cannot execute python functions. @jsimsa who raised the discussion about this topic recently. ", "@lindong28 was your CL to fix the issue merged?", "@jsimsa Are you referring to patch that places all PyFunc op on the coordinator's address space? That fix has been committed in https://github.com/tensorflow/tensorflow/commit/7bfb8380a7c09603259f49027374a4faf4199ad2. However this patch will only be included in the next TF release 2.2.0 which is scheduled to have branch cut on 2/26.\r\n ", "@tsc2017 would you be able to try if your program work with a nightly release of TF which should include the above fix?", "@jsimsa \r\nIn the following, I provide two notebooks for testing:\r\n[custom code](https://colab.research.google.com/drive/1Jf43Aq8LtGxUD5QSMGp4KJJnoUdYGiF-)\r\n[keras](https://colab.research.google.com/drive/1kOb4jzlpYsSf-0hv1Rs7_4SGTNini0LK)\r\nHowever, I am not able to test them as I have difficulties in importing tf-nightly in Colab.", "Hey @tsc2017, I verified that by adding \"!pip install tf-nightly==2.2.0.dev20200131\", the error changed from \"No registered 'PyFunc' OpKernel for 'CPU' devices compatible with node\" (see [this](https://colab.sandbox.google.com/drive/1Jf43Aq8LtGxUD5QSMGp4KJJnoUdYGiF-#scrollTo=KtHGmqEtvMZa)) to \"End of sequence\" (see [this](https://colab.sandbox.google.com/drive/1WikNeKoOxq09DaYJMXI1W1hd8f06wnzc#scrollTo=dp9njklCaBG8)).\r\n", "This bug still exists on 2.2.0-rc2(see [this notebook](https://colab.research.google.com/drive/1Jf43Aq8LtGxUD5QSMGp4KJJnoUdYGiF-)):\r\n\r\nNotFoundError: From /job:worker/replica:0/task:0:\r\nNo registered 'PyFunc' OpKernel for 'CPU' devices compatible with node {{node PyFunc}}\r\n", "I'm struggling with the same issue using tf 2.2.0. My dataset is too large to use tf.data.Dataset.from_tensor_slices, so, any workaround to this problem with TPU and generators would be very helpful.", "@nicosacco The recommended way here is to preprocess your data separately and write them out into TFRecord files, which you can then store on GCS. TPUs can parallelize GCS reads very quickly.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32712\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32712\">No</a>\n"]}, {"number": 32711, "title": "tf.pow throwing negative or zero value for large value of exponent.", "body": "I am using tensorflow 1.14.0 version with Python 3 Google compute engine backend in Google Colab having a RAM 12.72 GB, Disk 48.97 GB and tf.GIT_VERSION v1.14.0-0-g87989f6959\r\n\r\nWhile performing tf.pow operation I have observed that tf.pow returns negative or zero value for large value of exponent.\r\n\r\n```\r\na = tf.constant(50)\r\nb = tf.constant(9)\r\nc = tf.constant(10)\r\nd = tf.constant(15)\r\ne = tf.constant(100)\r\n\r\nwith tf.Session() as sess:\r\n  a_b, a_c, a_d, a_e = sess.run([tf.pow(a,b),\r\n                                 tf.pow(a, c),\r\n                                 tf.pow(a, d),\r\n                                 tf.pow(a, e)])\r\n  print('a_b is: ', a_b)\r\n  print('a_c is: ', a_c)\r\n  print('a_d is: ',a_d)\r\n  print('a_e is:', a_e)\r\n\r\n```\r\nPrinted values are:\r\n```\r\n a_b is:  1507045888\r\n a_c is:  -1957116928\r\n a_d is:  -606830592\r\n a_e is: 0\r\n\r\n```\r\n**I also tried this with tensorflow 2.0.0-alpha0 version and getting the same error.**\r\n```\r\nimport tensorflow as tf\r\na = tf.constant(50)\r\nb = tf.constant(9)\r\nc = tf.constant(10)\r\nd = tf.constant(15)\r\ne = tf.constant(100)\r\n\r\nprint('a_b is: ', tf.pow(a, b))\r\nprint('a_c is: ', tf.pow(a, c))\r\nprint('a_d is: ', tf.pow(a, d))\r\nprint('a_e is:',  tf.pow(a, e))\r\n\r\n```\r\nprinted values are \r\n```\r\na_b is:  tf.Tensor(1507045888, shape=(), dtype=int32)\r\na_c is:  tf.Tensor(-1957116928, shape=(), dtype=int32)\r\na_d is:  tf.Tensor(-606830592, shape=(), dtype=int32)\r\na_e is: tf.Tensor(0, shape=(), dtype=int32)\r\n```", "comments": ["I have resolved this issue. The issue was this dtype. By default, tf.constant inferred the value of dtype is not specified. This inference is causing the error when dealing with larger multiplication.\r\n\r\n> In above case, specifying dtype='tf.float64' resolved the issue. \r\n\r\n\r\n Shouldn't tensorflow notify this to the user instead of giving the incorrect values ? Wouldn't it be a good idea to raise memory or dtype error instead of throwing the wrong value, as it is harder to detect these error in complex setting.\r\n\r\n"]}, {"number": 32710, "title": "Android Examples", "body": "i already install and run tensorflow examples but it consist of tf_classify,tf_detect,tf_speech and tf_style. so can anyone tell me how to install only tf_detect ?", "comments": ["@ferdinan97 ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\nMake sure you also include the exact command if possible to produce the output included in your test case. \r\nIf you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "**System information**\r\n- OS Platform :Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Asus Zenfone 4 max\r\n- TensorFlow installed from (source or binary):pip install tensorflow-gpu==1.13.1\r\n- TensorFlow version:1.13.1\r\n- Python version:3.6.9\r\n- Installed using :miniconda\r\n- Bazel version (if compiling from source): no bazel\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.0\r\n- GPU model and memory:Nvdia Geforce 970MX\r\n\r\n\r\n\r\n**Describe the problem**\r\nHow can i only install TF_Detect from tensorflow (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) that consist of tf_style,tf_detect,tf_classify and tf_speech ? cause i only need tf_detect", "Please look at the TensorFlow Lite examples [here](https://github.com/tensorflow/examples/tree/master/lite). They are each packaged separately.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32710\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32710\">No</a>\n", "Actually i know about that, but the model i work didnt support the tensorflow lite thats why i ask this question. Can you reopen the issue or give me solution ? @jdduke", "We won't be updating the TF Mobile example any further, so there's no way to decouple the tf_detect portion from the other examples. You'd have to manually pull out just the necessary code.", "Ok then , thank you"]}, {"number": 32709, "title": "out of memory flood on the simplest op", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win7 x64\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0rc1\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: 10.0, 3.7.5\r\n- GPU model and memory: GTX 1060, 6GB\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nimport tensorflow as tf\r\nx = [[2.]]\r\nm = tf.matmul(x, x)\r\n```\r\n\r\nresults infinite flood:\r\n```\r\n2019-09-21 14:55:58.537800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830\r\n] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou\r\nt of memory\r\n2019-09-21 14:55:58.540800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830\r\n] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou\r\nt of memory\r\n2019-09-21 14:55:58.543800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830\r\n] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou\r\nt of memory\r\n2019-09-21 14:55:58.546800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830\r\n] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou\r\nt of memory\r\n2019-09-21 14:55:58.549800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830\r\n] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou\r\nt of memory\r\n2019-09-21 14:55:58.552800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830\r\n] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou\r\nt of memory\r\n2019-09-21 14:55:58.555800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830\r\n] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou\r\nt of memory\r\n2019-09-21 14:55:58.566800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830\r\n] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou\r\nt of memory\r\n2019-09-21 14:55:58.569800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830\r\n] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou\r\nt of memory\r\n2019-09-21 14:55:58.572800: I tensorflow/stream_executor/cuda/cuda_driver.cc:830\r\n] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: ou\r\nt of memory\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nexpected to work properly\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nx = [[2.]]\r\nm = tf.matmul(x, x)\r\n```\r\n\r\n**Other logs**\r\n\r\n```\r\n2019-09-21 14:58:51.205800: I tensorflow/stream_executor/platform/default/dso_lo\r\nader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n>>> tf.constant(1)\r\n2019-09-21 14:58:54.148800: I tensorflow/stream_executor/platform/default/dso_lo\r\nader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-09-21 14:58:54.220800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1\r\n618] Found device 0 with properties:\r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\r\npciBusID: 0000:01:00.0\r\n2019-09-21 14:58:54.226800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1\r\n618] Found device 1 with properties:\r\nname: GeForce GT 730 major: 3 minor: 5 memoryClockRate(GHz): 0.9015\r\npciBusID: 0000:03:00.0\r\n2019-09-21 14:58:54.229800: I tensorflow/stream_executor/platform/default/dlopen\r\n_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-09-21 14:58:54.239800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1\r\n731] Ignoring visible gpu device (device: 1, name: GeForce GT 730, pci bus id: 0\r\n000:03:00.0, compute capability: 3.5) with core count: 2. The minimum required c\r\nount is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROC\r\nESSOR_COUNT.\r\n2019-09-21 14:58:54.245800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1\r\n746] Adding visible gpu devices: 0\r\n2019-09-21 14:58:54.248800: I tensorflow/core/platform/cpu_feature_guard.cc:142]\r\n Your CPU supports instructions that this TensorFlow binary was not compiled to\r\nuse: AVX2\r\n2019-09-21 14:58:54.308800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1\r\n618] Found device 0 with properties:\r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\r\npciBusID: 0000:01:00.0\r\n2019-09-21 14:58:54.312800: I tensorflow/stream_executor/platform/default/dlopen\r\n_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-09-21 14:58:54.319800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1\r\n746] Adding visible gpu devices: 0\r\n2019-09-21 14:58:55.057800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1\r\n159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-09-21 14:58:55.061800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1\r\n165]      0\r\n2019-09-21 14:58:55.064800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1\r\n178] 0:   N\r\n2019-09-21 14:58:55.071800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1\r\n304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 wit\r\nh 4675 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bu\r\ns id: 0000:01:00.0, compute capability: 6.1)\r\n```\r\n", "comments": ["@iperov ,\r\nTried running the code in colab for tf-2.0rc1 did not face any error, please take a look at the [gist](https://colab.sandbox.google.com/gist/oanush/f92701b1542fca43d2d1f66b0f262077/32709.ipynb) of the colab. kindly share us the gist if the issue is replicating.Thanks!", "@oanush  did you try it with cudnn 7.5.0 ?\r\n\r\nI installed 7.4.1 and all is fine.", "also problem in windows, not in linux colab.\r\n\r\nRead System information.\r\n", "Are you using the GPU device only for this compute task?\r\n\r\nSeems like an issue in your GPU, not in TensorFlow", "@mihaimaruseac are you troll?\r\n\r\nissue in my GPU?? srsly?? I thought developers of tensorflow are a bit smarter.\r\n\r\nthis is simple task to reproduce the bug.\r\n\r\nThis bug is reproduced only with cudnn 7.5.0 on windows, but 7.4.1 works fine.\r\n", "No, not a troll. Please always assume good intent as I was basing my question on https://stackoverflow.com/questions/34514324/error-using-tensorflow-with-gpu/34514932#34514932\r\n\r\nThat being said, can you also post output of `nvidia-smi`, on both 7.5.0 and 7.4.1, before and after running your script?", "> nvidia-smi, on both 7.5.0 and 7.4.1\r\n\r\nnvidia-smi does not depend on cudnn.\r\ncudnn is only the one dll that loaded by python environment.\r\nI can pass to it any version I want. So 7.5.0 has bug, but 7.4.1 - has not.", "It doesn't depend on cudnn but it also shows the state of the GPU so we can detect if there is a memory leak.", "I don't know why, but bug is no more reproducable :(", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32709\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32709\">No</a>\n", "So it turns out it was the state of your system.", "or the hidden bug of tf", "Doubtful to be a hidden bug of TF (as it seems to always happen on your issues) but if it reproduces again please post full details and debug how much memory your GPU still has available."]}, {"number": 32708, "title": "module 'tensorflow' has no attribute 'enable_eager_execution'", "body": "I installed *tensorflow-gpu==2.0.0rc1* from pip\r\n\r\n```\r\n>>> tf.enable_eager_execution()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'\r\n```", "comments": ["Eager execution is enabled by default in 2.0 and onwards, hence the symbol to enable it is no longer needed nor provided.", "You use enable_eager_execution with earlier versions of tensorflow ", "Put these lines at the starting of the program\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\ntf.executing_eagerly() ", "I am working with deep learning frameworks for 2 years.\r\nAnd I am saying \r\n\r\nGRAPH SYSTEM IS THE BEST.\r\n\r\npytorch sucks and only good for research.\r\nSo I cannot understand why to copy pytorch and enable eager by default."]}]