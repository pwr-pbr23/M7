[{"number": 36098, "title": "TFLu: Fixed convolution layer in the CMSIS-NN glue", "body": "Missing min/max activation for the reference kernel", "comments": []}, {"number": 36097, "title": "tridiagonal_matmul: Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ArchLinux\r\n- TensorFlow installed from (source or binary): https://www.archlinux.org/packages/community/x86_64/tensorflow-opt/\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.81\r\n\r\n```\r\n== check python ===================================================\r\npython version: 3.8.1\r\npython branch: \r\npython build version: ('default', 'Jan  8 2020 23:09:20')\r\npython compiler version: GCC 9.2.0\r\npython implementation: CPython\r\n\r\n== check os platform ===============================================\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 9.2.0\r\nCopyright (C) 2019 Free Software Foundation, Inc.\r\n\r\n== check pips ===================================================\r\nnumpy                      1.18.1             \r\nnumpy-quaternion           2019.12.11.22.25.52\r\nprotobuf                   3.11.2             \r\ntensorflow                 2.1.0              \r\ntensorflow-estimator       2.1.0              \r\ntensorflow-serving-api     2.0.0              \r\ntensorflow-serving-api-gpu 2.0.0              \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.1.0\r\ntf.version.GIT_VERSION = unknown\r\ntf.version.COMPILER_VERSION = 9.2.0\r\n```\r\n\r\n**Describe the current behavior**\r\nSegfault (see log) or sigserv\r\n[dump.txt](https://github.com/tensorflow/tensorflow/files/4091136/dump.txt)\r\n \r\n```\r\n2020-01-21 13:56:11.320166: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1800000000 Hz\r\n2020-01-21 13:56:11.320500: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557f9e6e4c30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-21 13:56:11.320521: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n\r\nProcess finished with exit code 139 (interrupted by signal 11: SIGSEGV)\r\n```\r\n\r\n**Describe the expected behavior**\r\nPropagate layer\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\nnp.set_printoptions(precision=4)\r\n\r\n\r\nclass Tridiagonal(layers.Layer):\r\n    def __init__(self):\r\n        super(Tridiagonal, self).__init__()\r\n\r\n    def build(self, input_shape):\r\n        self.super = self.add_weight(shape=(input_shape[-1], 1),\r\n                                     initializer='random_normal',\r\n                                     trainable=True)\r\n        self.main = self.add_weight(shape=(input_shape[-1], 1),\r\n                                    initializer='random_normal',\r\n                                    trainable=True)\r\n        self.sub = self.add_weight(shape=(input_shape[-1], 1),\r\n                                   initializer='random_normal',\r\n                                   trainable=True)\r\n\r\n    def call(self, inputs):\r\n        return tf.linalg.tridiagonal_matmul(\r\n            (self.super, self.main, self.sub), inputs,\r\n            diagonals_format='sequence')\r\n\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(Tridiagonal())\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.01),\r\n              loss='mse',  # mean squared error\r\n              metrics=['mae'])  # mean absolute error\r\n\r\ndata = tf.convert_to_tensor(np.random.random((10, 2)), dtype=tf.float32)\r\nlabels = tf.convert_to_tensor(np.random.random((10, 2)), dtype=tf.float32)\r\n\r\ntd = Tridiagonal()\r\ntd(data)\r\n```\r\n\r\n**Other info / logs**\r\nSee attachment\r\n[dump.txt](https://github.com/tensorflow/tensorflow/files/4091139/dump.txt)\r\n\r\n", "comments": ["Your code snippet executes successfully in google colab. Can you create virtual env and use any Python version 3.4\u20133.7 to see if the issue persists? Thanks! ", "I'm afraid I cannot:\r\n```\r\npipenv install tensorflow\r\nInstalling tensorflow\u2026\r\nAdding tensorflow to Pipfile's [packages]\u2026\r\n\u2714 Installation Succeeded \r\nPipfile.lock not found, creating\u2026\r\nLocking [dev-packages] dependencies\u2026\r\nLocking [packages] dependencies\u2026\r\n\u2714 Success! \r\nUpdated Pipfile.lock (66d06e)!\r\nInstalling dependencies from Pipfile.lock (66d06e)\u2026\r\nAn error occurred while installing tensorflow==2.1.0 --hash=sha256:1cf129ccda0aea616b122f34b0c4bc39da959d34c4a4d8c23ed944555c5e47ab --hash=sha256:2e8fc9764b7ea87687a4c80c2fbde69aeeb459a536eb5a591938d7931ab004c2 --hash=sha256:33e4b16e8f8905ee088bf8f413dcce2820b777fdf7f799009b3a47f354ebb23f --hash=sha256:513d48dd751e0076d1b1e5e498e3522891305bedd2840f3cb4b1c57ffcb7d97d --hash=sha256:5cfa729fc71f6f2dca0ea77ebe768ea293e723e22ecb086a0b3ab26cc1776e37 --hash=sha256:7bad8ea686a1f33d9dac13eb578c4597346789d4f826980c8bbcfbd08e7dc921 --hash=sha256:8c0fae0f9f772ed7e3370f1b286f88c27debbcf09468e5036670ea2c67e239ec --hash=sha256:92c4f1c939de438fbe484d011e5eebe059fc8e5244cfe32a81c6891b3357d109 --hash=sha256:c420e70d4127c2ac00054aece54cf04a1a43d5d4f25de90267f247873f1bd5a8 --hash=sha256:e631f55cf30054fee3230c89a7f998fd08748aa3045651a5a760cec2c5b9f9d6 --hash=sha256:e877fbf373d5be42fb118269df1670b8d3c0df9be223904a2584a8f8ed23b082! Will try again.\r\n  \ud83d\udc0d   \u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589 34/34 \u2014 00:00:17\r\nInstalling initially failed dependencies\u2026\r\n[pipenv.exceptions.InstallError]:   File \"/usr/lib/python3.8/site-packages/pipenv/core.py\", line 1983, in do_install\r\n[pipenv.exceptions.InstallError]:       do_init(\r\n[pipenv.exceptions.InstallError]:   File \"/usr/lib/python3.8/site-packages/pipenv/core.py\", line 1246, in do_init\r\n[pipenv.exceptions.InstallError]:       do_install_dependencies(\r\n[pipenv.exceptions.InstallError]:   File \"/usr/lib/python3.8/site-packages/pipenv/core.py\", line 858, in do_install_dependencies\r\n[pipenv.exceptions.InstallError]:       batch_install(\r\n[pipenv.exceptions.InstallError]:   File \"/usr/lib/python3.8/site-packages/pipenv/core.py\", line 763, in batch_install\r\n[pipenv.exceptions.InstallError]:       _cleanup_procs(procs, not blocking, failed_deps_queue, retry=retry)\r\n[pipenv.exceptions.InstallError]:   File \"/usr/lib/python3.8/site-packages/pipenv/core.py\", line 681, in _cleanup_procs\r\n[pipenv.exceptions.InstallError]:       raise exceptions.InstallError(c.dep.name, extra=err_lines)\r\n[pipenv.exceptions.InstallError]: []\r\n[pipenv.exceptions.InstallError]: ['ERROR: Could not find a version that satisfies the requirement tensorflow==2.1.0 (from -r /tmp/pipenv-cqik3ea3-requirements/pipenv-k1yz1z36-requirement.txt (line 1)) (from versions: none)', 'ERROR: No matching distribution found for tensorflow==2.1.0 (from -r /tmp/pipenv-cqik3ea3-requirements/pipenv-k1yz1z36-requirement.txt (line 1))']\r\nERROR: ERROR: Package installation failed...\r\n  \u2624  \u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589 0/1 \u2014 00:00:01\r\n```\r\n```console\r\n$ pip -V\r\npip 20.0.2 from /home/user/tf/lib/python3.8/site-packages/pip (python 3.8)\r\n$ pip install --upgrade tensorflow\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n```", "We don't have TF binaries with python 3.8 support as of now.\r\nTo know more see https://github.com/tensorflow/tensorflow/issues/33374#issuecomment-571074915", "You need to switch back to Python version 3.4\u20133.7.\r\nhttps://www.tensorflow.org/install/pip?lang=python3#system-requirements\r\nWe will update this [thread](https://github.com/tensorflow/tensorflow/issues/33374) when python 3.8 is up. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36097\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36097\">No</a>\n"]}, {"number": 36096, "title": "Performance problems with tf.nn.embedding_lookup in eager mode", "body": "**System information**\r\n- Have I written custom code: Yes - I condensed it to a self-contained example and attach it to the issue.\r\n- OS Platform and Distribution: Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from: binary (pip)\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: CPU only\r\n- GPU model and memory: CPU only\r\n\r\n**Describe the current behavior**\r\ntf.nn.embedding_lookup with eager mode enabled is so slow for me that it is practically unusable. Falling back to Numpy is faster by an order of magnitude.\r\n\r\n**Describe the expected behavior**\r\nThe embedding lookup should perform well enough to be used. Or the information on how to use it properly should be improved.\r\n\r\n**Code to reproduce the issue**\r\nBelow is my benchmarking code. It simulates a static word2vec (3M, 300d) lookup with artificial data under different configurations. I use a static document length of 500 tokens which I found to be the average of my dataset (OpenWebText sample).\r\n[tf-embedding-benchmark.zip](https://github.com/tensorflow/tensorflow/files/4090909/tf-embedding-benchmark.zip)\r\n\r\n**Other info / logs**\r\nBelow is a plot of my benchmarks. I did the benchmarks with little data and therefore there's a bit of variance in the charts. I'm currently running them with some more data, but it takes time because it is so slow. As far I can tell already, the results do not look significantly different though. `embedding_type` refers to the embedding lookup: `numpy` means Numpy indexing using `tf.py_func`. The TensorFlow variants refer to the underlying embedding data structure: `tf.constant` and `tf.Variable` with and without the `trainable` flag set.\r\n\r\n![embedding_lookup_bm](https://user-images.githubusercontent.com/779877/72803346-2be89580-3c4e-11ea-9fd7-c0a4a5525eeb.png)\r\n", "comments": ["Update: Benchmarks with more data:\r\n\r\n![embedding_lookup_bm](https://user-images.githubusercontent.com/779877/72892866-ea242180-3d17-11ea-8394-f4873f2f7a14.png)", "I would also be interested where the strong peak around a batch size of 50 comes from.", "@jjedele,\r\nCould you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue. Thanks!", "@amahendrakar , sorry, in the meantime I do not have access to the system anymore. We worked around the problem back then. We can close the issue since apparently it's not something other people ran into.", "@jjedele,\r\nThank you for the update. Closing this issue, please feel free to re-open if necessary."]}, {"number": 36095, "title": "Riscv 32 mcu", "body": "Update for this pull request(reviewed by @nkreeger before):\r\nhttps://github.com/tensorflow/tensorflow/pull/33680\r\n\r\nrelated issues:\r\nhttps://github.com/tensorflow/tensorflow/issues/32041\r\nhttps://github.com/tensorflow/tensorflow/issues/33677\r\n\r\nIf possible, pls review as soon as possible, the tensorflow repo is very easy to get conflict because update so quickly.  Thanks,\r\n\r\nAbout the three patches for review:\r\n1) lite:micro:tools:make: Move the default target(all) up before any others.\r\n    \r\n The default Makefile target `all` is compiled if there's no command-line arguments.\r\n\r\n2) lite:micro:riscv32_mcu:Fix build failure of undefined references.\r\n    \r\n    Fix the ld error:\r\n    undefined references to `__wrap_puts` for build commands like `make -f tensorflow/lite/micro/tools/make/Makefile TARGET=riscv32_mcu hello_world_bin`\r\n    The related issue is tensorflow#32041\r\n\r\n    Refactoring, suggested by Nick Kreeger(nick.kreeger@gmail.com):\r\n    The targets/mcu_riscv_makefile.inc should only include the bare-bones parts for building for this platform.\r\n    So move platform specific items from targets/mcu_riscv_makefile.inc to the actual example folder.\r\n    Create a riscv32_mcu folder in each example directory. In those directories, create a new Makefile.inc that adds these rules moved out.    \r\n\r\n    The bug's original reasons:\r\n    The Makefile variables XXX_TEST_SRCS/XXX_SRCS in targets/mcu_riscv_makefile.inc are overridden by the the examples's respective makefile.inc (eg. hello_world/Makefile.inc), which leads to the   architecture special __wrap__funs are not included correctly.\r\n\r\n3) lite:micro:riscv32_mcu: Fix hidden symbol `__dso_handle' isn't defined.\r\n For arduino sketch in riscv_mcu examples, this patch fix by declare the global variable `void* __dso_handle;`.\r\n\r\n", "comments": ["> None of the changes to compiler/ seems motivated or appropriate.\r\n\r\nHi @jpienaar \r\nSorry to make a mistake to Revert one step more of  \"Conversion rule for Conv2dBackpropInput\".\r\nI will try revert and update.\r\n\r\n", "Hi @gbaned @jpienaar  @nkreeger \r\nI delete the old fork(too dirty ) and create a new fork of tensorflow.\r\nSo I create a new clean PR https://github.com/tensorflow/tensorflow/pull/36118 , and closed this one.\r\n"]}, {"number": 36094, "title": "SlurmClusterResolver should use env variables ob job step and return correct num_accelerators", "body": "The [SlurmClusterResolver](https://github.com/tensorflow/tensorflow/blob/96d0f42d1b236d21157d32805d4aa87e136083b3/tensorflow/python/distribute/cluster_resolver/slurm_cluster_resolver.py) has a few issues:\r\n\r\n- `num_accelerators` returns the GPUs per **node**  although they should be per ** task** according to usage\r\n- It uses scontrol which may not return the correct values (see next point)\r\n- Slurm job steps might use different configurations than its job. E.g. after allocating 1 task with 10 cpus and gpus one might run a job step with 10 tasks and 1 cpu and gpu each. This is currently ignored leading to wrong behavior\r\n\r\nExample: From inside an allocation with 1 task and 1 node the following will print a single hostname each, and not 2 times the same hostname each: `srun --ntasks 2 --cpus-per-task=5 scontrol show hostname`\r\n\r\nProposed fix:\r\n- Use environment variables `SLURM_*` instead of `scontrol`\r\n- When exists, use `SLURM_STEP_*` variables instead of its corresponding job variable. It might even be reasonable to NOT use the job variables at all as it doesn't make sense to run TF in the context of a job and not a job step (difference: after `salloc` or `sbatch` you'll run in the context of a job in a single instance, `srun` from inside such an allocation then starts a step and distributes execution. `srun` can also be used to implicitly call `salloc`)\r\n\r\nI have developed a version based on the current cluster resolver which implements the above and uses https://pypi.org/project/python-hostlist to parse the tasks per node (it is possible that slurm allocates 3 tasks on the first node and 2 tasks on the second when allocating 5 tasks) and expanding the hostlist (the format used is something like `[n1-n3, n5]`.   \r\n\r\nIn the current version it makes heavy use of default values so it is possible to simply create an instance of `SlurmClusterResolver` with **no** arguments and it will fill out everything from the environment of the current job step or job. The goal was to relieve users from any fiddling with information available already and just use e.g. `MultiWorkerMirroredStrategy`\r\n\r\nIs there interest in reviewing and accepting a PR with my improved version?", "comments": ["@Flamefire, thanks for the bug report. I am happy to review your PR, so just create one and assign it to me. Thanks!", "Ok I opened a PR with the class itself. I'll change the tests once the general approach is accepted. I don't see why it won't but I don't want to waste time if something significant needs changing.", "Dear @Flamefire and @frankchn \r\n\r\nI can login to a Slurm cluster with several nodes and 4GPUs per node. I want to do inference using distribute training (MirroredStrategy), but I do not how to see two or more modes resources. Can you share some example on using tf.distribute.cluster_resolver.SlurmClusterResolver? \r\n\r\nBest regards", "Issue resolved by https://github.com/tensorflow/tensorflow/pull/36159\r\n\r\n@AngelBerihuete \r\n\r\nYou need a Strategy that accepts a `cluster_resolver` argument and when you want MultiNode you want `MultiWorkerMirroredStrategy`. For Slurm you can copy the SlurmClusterResolver from #36159 when you are using TF < 2.2 (or wait for its release)\r\n\r\nFor TF < 2.2 there is also bug #36550 which requires you to use:\r\n\r\n```\r\ndef set_tf_config(resolver, environment=None):\r\n    \"\"\"Set the TF_CONFIG env variable from the given cluster resolver\"\"\"\r\n    cfg = {\r\n        'cluster': resolver.cluster_spec().as_dict(),\r\n        'task': {\r\n            'type': resolver.get_task_info()[0],\r\n            'index': resolver.get_task_info()[1],\r\n        },\r\n        'rpc_layer': resolver.rpc_layer,\r\n    }\r\n    if environment:\r\n        cfg['environment'] = environment\r\n    os.environ['TF_CONFIG'] = json.dumps(cfg)\r\n```\r\n\r\nSo full example of that part:\r\n\r\n```\r\n    resolver = SlurmClusterResolver()\r\n    set_tf_config(resolver)\r\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(cluster_resolver=resolver)\r\n    with strategy.scope():\r\n        # Load and compile model and data\r\n```\r\n\r\nSee https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras for more. Most importantly you need the `steps_per_epoch` parameter if you want to use `model.fit`", "Hi @Flamefire! Many thanks.\r\n\r\nThe example you share yesterday did not have errors, but the behavior was not as I expected:\r\n\r\n```bash\r\n== Starting run at Tue Apr  7 12:35:29 CEST 2020\r\n== Job ID: 3965827\r\n== Job NPROCS: 2\r\n== Job NNODES: 2\r\n== Node list: p9r2n[01,09]\r\n\r\nTF version:  2.1.0\r\nTFP version:  0.9.0\r\nNumber of devices to do Multi Worker Mirrored Strategy: 8\r\nTF version:  2.1.0\r\nTFP version:  0.9.0\r\nNumber of devices to do Multi Worker Mirrored Strategy: 8\r\n\r\n```\r\nAnd I expect only one execution, not two. \r\n\r\n```bash\r\nTF version:  2.1.0\r\nTFP version:  0.9.0\r\nNumber of devices to do Multi Worker Mirrored Strategy: 8\r\n\r\n```\r\nThis is the slurm file \r\n\r\n```bash \r\n#!/bin/bash\r\n\r\n#------- job description -------\r\n\r\n#SBATCH --job-name='MultiWorker_test'\r\n#SBATCH --qos=debug\r\n\r\n#------- setup -------\r\n# In order to use GPUs devices on BSC it\r\n# is mandatory to allocate 40 CPUs per GPU\r\n# requested.\r\n# You specify the gres configuration PER-NODE\r\n# for a job with the --gres flag and a number\r\n# of GPUs. For instance, --gres=gpu:4, we will\r\n# request four GPUs per node (maximun GPUs per\r\n# node in this machine (CTE-POWER at BSC).\r\n# Then, if also you set --node=2, you'll request\r\n# 2*4 GPUs, and --ntasks=2*4*40=320\r\n\r\n\r\n#SBATCH --nodes=2\r\n#SBATCH --ntasks=2\r\n#SBATCH --cpus-per-task=160\r\n#SBATCH --gres=gpu:4\r\n#SBATCH --time=00:05:00\r\n\r\n#------- I/O -------\r\n\r\n#SBATCH -D .\r\n#SBATCH --output=TF2_%j.out\r\n#SBATCH --error=TF2_%j.err\r\n\r\n#------- load modules  -------\r\n\r\nmodule purge\r\n\r\nmodule load gcc/8.3.0 cuda/10.1 cudnn/7.6.4 nccl/2.4.8 tensorrt/6.0.1\r\nopenmpi/4.0.1 atlas/3.10.3 scalapack/2.0.2 fftw/3.3.8 szip/2.1.1\r\nffmpeg/4.2.1 opencv/4.1.1 python/3.7.4_ML\r\n\r\necho \"== Starting run at $(date)\"\r\necho \"== Job ID: ${SLURM_JOBID}\"\r\necho \"== Job NPROCS: ${SLURM_NPROCS}\"\r\necho \"== Job NNODES: ${SLURM_NNODES}\"\r\necho \"== Node list: ${SLURM_NODELIST}\"\r\necho \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\r\n\r\n#------- Comando ------\r\nsrun python test_setup_slurm_multiple_node.py\r\n\r\n```\r\nand this is test_setup_slurm_multiple_node.py\r\n\r\n```python\r\nfrom __future__ import absolute_import, division, print_function,\r\nunicode_literals\r\n\r\nfrom slurm_cluster_resolver import SlurmClusterResolver\r\nimport os\r\nimport json\r\nimport sys\r\n\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\nprint(\"TF version: \", tf.__version__)\r\nprint(\"TFP version: \", tfp.__version__)\r\n\r\ndef set_tf_config(resolver, environment=None):\r\n    \"\"\"Set the TF_CONFIG env variable from the given cluster resolver\"\"\"\r\n    cfg = {\r\n        'cluster': resolver.cluster_spec().as_dict(),\r\n        'task': {\r\n            'type': resolver.get_task_info()[0],\r\n            'index': resolver.get_task_info()[1],\r\n        },\r\n        'rpc_layer': resolver.rpc_layer,\r\n    }\r\n    if environment:\r\n        cfg['environment'] = environment\r\n    os.environ['TF_CONFIG'] = json.dumps(cfg)\r\n\r\n\r\nresolver = SlurmClusterResolver()\r\nset_tf_config(resolver)\r\n\r\nstrategy =\r\ntf.distribute.experimental.MultiWorkerMirroredStrategy(cluster_resolver=resolver)\r\n\r\n\r\nprint ('Number of devices to do Multi Worker Mirrored Strategy:\r\n{}'.format(strategy.num_replicas_in_sync))\r\n```\r\nOne of the solutions given by the cluster support team is to do a ```salloc``` and then ```python  test_setup_slurm_multiple_node.py```. Are you agree?\r\n", "No. Obviously you run 2 applications as is usual with slurm/MPI in general. That is the whole idea of \"MultiWorker\". The strategy will make those communicate with each other.", "Got it! Many thanks @Flamefire !"]}, {"number": 36093, "title": "Saving tf.keras.Sequential model fails with RNN containing more than one GRUCell", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- Mobile device if the issue happens on mobile device: -\r\n- TensorFlow installed from: binary (tf-nightly-2.0-preview)\r\n- TensorFlow version: GIT_VERSION = v1.12.1-7529-g3e0ad8a004, VERSION = 2.0.0-dev20190731\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: CPU only\r\n- GPU model and memory: CPU only\r\n\r\n**Describe the current behavior**\r\nSaving a `tf.keras.Sequential` model with `tf.keras.layers.RNN` fails when containing more than one `tf.keras.layers.GRUCell` with error message `RuntimeError: Unable to create link (name already exists)`.\r\n\r\n**Describe the expected behavior**\r\nSaving should succeed, not only when having one cell.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n# saving succeeds for number_of_cells = 1, but fails for number_of_cells > 1\r\nnumber_of_cells = 2\r\n\r\nmodel = tf.keras.Sequential()\r\n\r\nmodel.add(tf.keras.layers.Input(shape=(1, 1,)))\r\n\r\ncells = []\r\n\r\nfor i in range(number_of_cells):\r\n    cells.append(tf.keras.layers.GRUCell(10))\r\n\r\nmodel.add(tf.keras.layers.RNN(cells))\r\n\r\nmodel.save(\"rnn.h5\")\r\n```\r\n\r\n**Other info / logs**\r\nBehavior is the same when using `SimpleRNNCell` instead of `GRUCell`.\r\n\r\nTraceback in case of failure:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"test_rnn_gru.py\", line 17, in <module>\r\n    model.save(\"rnn.h5\")\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1157, in save\r\n    saving.save_model(self, filepath, overwrite, include_optimizer, save_format)\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py\", line 105, in save_model\r\n    model, filepath, overwrite, include_optimizer)\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 103, in save_model_to_hdf5\r\n    save_weights_to_hdf5_group(model_weights_group, model_layers)\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 625, in save_weights_to_hdf5_group\r\n    param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/h5py/_hl/group.py\", line 139, in create_dataset\r\n    self[name] = dset\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/h5py/_hl/group.py\", line 373, in __setitem__\r\n    h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)\r\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/h5o.pyx\", line 202, in h5py.h5o.link\r\nRuntimeError: Unable to create link (name already exists)\r\n```\r\n", "comments": ["I tried the workaround `model.save(\"rnn.h5\", include_optimizer=False)` according to #27688, but this didn't help. I also tried to manually set different names for the cells with `GRUCell(10, name=\"cell\" + str(i))`, which didn't help either.", "@padoremu,\r\nCan you try saving the model in tf format? using `model.save('rnn.tf',save_format='tf')`. It worked fine for me without any issue's. Kindly find the [gist of colab ](https://colab.sandbox.google.com/gist/oanush/84a2c6ef04084c18bbc699bc556c5acb/36093.ipynb#scrollTo=wJL_s5pKUetX)for your reference.Thanks!", "@oanush,\r\nThank you for your quick response. I tried with my 2.0.0-dev20190731 installation. It seems to work, but I am getting warnings:\r\n\r\n```\r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc8b0cd21e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc871a55048> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc8701499d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc871af60d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc84c67e1e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc84c401158> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc8146d6ea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n2020-01-22 08:16:48.048918: W tensorflow/python/util/util.cc:288] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:Entity <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fc8b0b65bf8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc8144619d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:From /home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1784: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\n```\r\n\r\nApparently I need a newer tensorflow version, as in [gist of colab](https://colab.sandbox.google.com/gist/oanush/84a2c6ef04084c18bbc699bc556c5acb/36093.ipynb#scrollTo=wJL_s5pKUetX) with tf_nightly-2.2.0.dev20200121, most of the warnings do not occur. I would love to install a current snapshot, but whatever I tried has failed so far:\r\n\r\n1. Installing tf-nightly with pip results in installing tf-nightly-1.15.0.dev20190730, which is not as desired.\r\n\r\n2. Installing a wheel package with pip fails with error `tf_nightly-2.2.0.dev20200121-cp36-cp36m-manylinux2010_x86_64.whl is not a supported wheel on this platform.`.\r\n\r\n3. Source build fails eventually with error `Server terminated abruptly (error code: 14, error message: 'Socket closed'` (see https://github.com/tensorflow/models/issues/3647#issuecomment-577033825).\r\n\r\nIs there any way to install e.g. a tf_nightly-2.2.0.dev20200121 (as seen on the gist of colab) on an Ubuntu 18.04, ideally without building from source?", "@oanush \r\nHave you tried loading the saved model? With my 2.0.0-dev20190731 installation, when adding the line\r\n\r\n`model2 = tf.keras.models.load_model('rnn.tf')`\r\n\r\nat the end, loading fails:\r\n\r\n```\r\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\r\n\r\nTwo checkpoint references resolved to different objects (ListWrapper([<tensorflow.python.keras.saving.saved_model.load.GRUCell object at 0x7f2c344becf8>, <tensorflow.python.keras.saving.saved_model.load.GRUCell object at 0x7f2c344bef28>]) and ListWrapper([ListWrapper([<tensorflow.python.keras.saving.saved_model.load.GRUCell object at 0x7f2c344becf8>, <tensorflow.python.keras.saving.saved_model.load.GRUCell object at 0x7f2c344bef28>]), ListWrapper([])])).\r\nTraceback (most recent call last):\r\n  File \"test_rnn_gru.py\", line 22, in <module>\r\n    model2 = tf.keras.models.load_model('rnn.tf')\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py\", line 142, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 86, in load\r\n    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/load.py\", line 541, in load_internal\r\n    export_dir)\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 102, in __init__\r\n    super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/load.py\", line 128, in __init__\r\n    self._restore_checkpoint()\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/load.py\", line 281, in _restore_checkpoint\r\n    load_status.assert_existing_objects_matched()\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py\", line 752, in assert_existing_objects_matched\r\n    for trackable_object in self._graph_view.list_objects():\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py\", line 430, in list_objects\r\n    trackable_objects, _, _ = self.objects_ids_and_slot_variables()\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py\", line 415, in objects_ids_and_slot_variables\r\n    trackable_objects, path_to_root = self._breadth_first_traversal()\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py\", line 199, in _breadth_first_traversal\r\n    for name, dependency in self.list_dependencies(current_trackable):\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py\", line 159, in list_dependencies\r\n    return obj._checkpoint_dependencies\r\n  File \"/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/data_structures.py\", line 458, in _checkpoint_dependencies\r\n    \"automatically un-wrapped and subsequently ignored.\" % (self,)))\r\nValueError: Unable to save the object ListWrapper([ListWrapper([<tensorflow.python.keras.saving.saved_model.load.GRUCell object at 0x7f2c344becf8>, <tensorflow.python.keras.saving.saved_model.load.GRUCell object at 0x7f2c344bef28>]), ListWrapper([])]) (a list wrapper constructed to track trackable TensorFlow objects). A list element was replaced (__setitem__, __setslice__), deleted (__delitem__, __delslice__), or moved (sort). In order to support restoration on object creation, tracking is exclusively for append-only data structures.\r\n\r\nIf you don't need this list checkpointed, wrap it in a tf.contrib.checkpoint.NoDependency object; it will be automatically un-wrapped and subsequently ignored.\r\n```\r\n", "> @oanush,\r\n> Thank you for your quick response. I tried with my 2.0.0-dev20190731 installation. It seems to work, but I am getting warnings:\r\n> \r\n> ```\r\n> WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc8b0cd21e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc871a55048> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc8701499d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc871af60d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc84c67e1e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc84c401158> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc8146d6ea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> 2020-01-22 08:16:48.048918: W tensorflow/python/util/util.cc:288] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n> WARNING:tensorflow:Entity <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fc8b0b65bf8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\n> WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fc8144619d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n> WARNING:tensorflow:From /home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1784: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\n> ```\r\n> \r\n> Apparently I need a newer tensorflow version, as in [gist of colab](https://colab.sandbox.google.com/gist/oanush/84a2c6ef04084c18bbc699bc556c5acb/36093.ipynb#scrollTo=wJL_s5pKUetX) with tf_nightly-2.2.0.dev20200121, most of the warnings do not occur. I would love to install a current snapshot, but whatever I tried has failed so far:\r\n> \r\n> 1. Installing tf-nightly with pip results in installing tf-nightly-1.15.0.dev20190730, which is not as desired.\r\n> 2. Installing a wheel package with pip fails with error `tf_nightly-2.2.0.dev20200121-cp36-cp36m-manylinux2010_x86_64.whl is not a supported wheel on this platform.`.\r\n> 3. Source build fails eventually with error `Server terminated abruptly (error code: 14, error message: 'Socket closed'` (see [tensorflow/models#3647 (comment)](https://github.com/tensorflow/models/issues/3647#issuecomment-577033825)).\r\n> \r\n> Is there any way to install e.g. a tf_nightly-2.2.0.dev20200121 (as seen on the gist of colab) on an Ubuntu 18.04, ideally without building from source?\r\n\r\n@padoremu ,\r\n\r\nHello you can try installing [nightly version](https://pypi.org/project/tf-nightly/) with `!pip install tf-nightly`, it should work fine! ", "@oanush \r\nUnfortunately, that doesn't work for me at all, as in bash, the exclamation mark will substitute the command name with the last command in my history starting by that name - and this is somewhat arbitrary. What was your purpose of the exclamation mark?\r\n\r\nIf I create a new virtual environment and do `pip install tf-nightly`, it downloads `tf_nightly-1.15.0.dev20190730-cp36-cp36m-manylinux1_x86_64.whl`, which is why I end up with 1.15.0.dev20190730 being installed. By the way, if I download that wheel package manually and install it with pip, it works fine. However, installation of e.g. tf_nightly-2.2.0.dev20200121-cp36-cp36m-manylinux2010_x86_64.whl fails.\r\n\r\nApparently the wheel package downloaded by pip depends on the platform, and tf-nightly seems to be frozen for Ubuntu 18.04 at 1.15.0.dev20190730, is that right?", "@oanush Do you have any news? I tried to load the saved tf model in the colab, and it fails. So unfortunately, the workaround does not work, not even with the newest tensorflow version.", "@padoremu In order to install `tf-nightly`, you can do this. At the command prompt, enter `pip3 install tf-nightly==2.2.0dev20200129`, then it will install `tf-nightly` that was released on 01/29 (today). I tried it on my Linux desktop and it works. Hope it works for you too. \r\n\r\nRegarding saving the model in \"h5\" format, we have another [similar bug](https://github.com/tensorflow/tensorflow/issues/35731). We will follow the progress there. \r\n\r\nIf this was resolved, please close the issue. Thanks!", "@jvishnuvardhan Thank you for your reply.\r\n\r\nRegarding the model: For me, neither \"h5\" nor \"tf\" format work in the end. Saving in \"tf\" format works, but loading it doesn't  - can't see this issue being mentioned in the other ticket.\r\n\r\nRegarding tf-nightly. Unfortunately, this doesn't work for me, I get the error:\r\n\r\n```\r\nCould not find a version that satisfies the requirement tf-nightly==2.2.0dev20200129 (from versions: 1.13.0.dev20190101, 1.13.0.dev20190102, 1.13.0.dev20190103, 1.13.0.dev20190104, 1.13.0.dev20190108, 1.13.0.dev20190109, 1.13.0.dev20190110, 1.13.0.dev20190111, 1.13.0.dev20190112, 1.13.0.dev20190113, 1.13.0.dev20190114, 1.13.0.dev20190115, 1.13.0.dev20190116, 1.13.0.dev20190117, 1.13.0.dev20190118, 1.13.0.dev20190119, 1.13.0.dev20190120, 1.13.0.dev20190121, 1.13.0.dev20190122, 1.13.0.dev20190123, 1.13.0.dev20190124, 1.13.0.dev20190125, 1.13.0.dev20190203, 1.13.0.dev20190204, 1.13.0.dev20190205, 1.13.0.dev20190206, 1.13.0.dev20190207, 1.13.0.dev20190208, 1.13.0.dev20190211, 1.13.0.dev20190212, 1.13.0.dev20190213, 1.13.0.dev20190214, 1.13.0.dev20190215, 1.13.0.dev20190216, 1.13.0.dev20190217, 1.13.0.dev20190218, 1.13.0.dev20190219, 1.13.0.dev20190220, 1.13.0.dev20190221, 1.13.0.dev20190222, 1.13.0.dev20190223, 1.13.0.dev20190224, 1.13.0.dev20190225, 1.13.0.dev20190226, 1.13.0.dev20190227, 1.13.0.dev20190228, 1.14.1.dev20190301, 1.14.1.dev20190302, 1.14.1.dev20190303, 1.14.1.dev20190304, 1.14.1.dev20190305, 1.14.1.dev20190306, 1.14.1.dev20190307, 1.14.1.dev20190308, 1.14.1.dev20190309, 1.14.1.dev20190310, 1.14.1.dev20190311, 1.14.1.dev20190312, 1.14.1.dev20190313, 1.14.1.dev20190314, 1.14.1.dev20190315, 1.14.1.dev20190316, 1.14.1.dev20190317, 1.14.1.dev20190318, 1.14.1.dev20190319, 1.14.1.dev20190320, 1.14.1.dev20190321, 1.14.1.dev20190322, 1.14.1.dev20190323, 1.14.1.dev20190324, 1.14.1.dev20190325, 1.14.1.dev20190326, 1.14.1.dev20190327, 1.14.1.dev20190329, 1.14.1.dev20190330, 1.14.1.dev20190331, 1.14.1.dev20190401, 1.14.1.dev20190402, 1.14.1.dev20190403, 1.14.1.dev20190404, 1.14.1.dev20190405, 1.14.1.dev20190406, 1.14.1.dev20190407, 1.14.1.dev20190408, 1.14.1.dev20190409, 1.14.1.dev20190410, 1.14.1.dev20190412, 1.14.1.dev20190413, 1.14.1.dev20190414, 1.14.1.dev20190415, 1.14.1.dev20190416, 1.14.1.dev20190417, 1.14.1.dev20190419, 1.14.1.dev20190420, 1.14.1.dev20190421, 1.14.1.dev20190422, 1.14.1.dev20190423, 1.14.1.dev20190424, 1.14.1.dev20190425, 1.14.1.dev20190426, 1.14.1.dev20190427, 1.14.1.dev20190428, 1.14.1.dev20190429, 1.14.1.dev20190430, 1.14.1.dev20190501, 1.14.1.dev20190502, 1.14.1.dev20190503, 1.14.1.dev20190504, 1.14.1.dev20190505, 1.14.1.dev20190506, 1.14.1.dev20190507, 1.14.1.dev20190508, 1.14.1.dev20190509, 1.14.1.dev20190510, 1.14.1.dev20190511, 1.14.1.dev20190512, 1.14.1.dev20190513, 1.14.1.dev20190514, 1.14.1.dev20190515, 1.14.1.dev20190516, 1.14.1.dev20190517, 1.14.1.dev20190518, 1.14.1.dev20190519, 1.14.1.dev20190520, 1.14.1.dev20190521, 1.14.1.dev20190522, 1.14.1.dev20190523, 1.14.1.dev20190524, 1.14.1.dev20190525, 1.14.1.dev20190526, 1.14.1.dev20190527, 1.14.1.dev20190528, 1.14.1.dev20190529, 1.14.1.dev20190530, 1.14.1.dev20190531, 1.14.1.dev20190601, 1.14.1.dev20190602, 1.14.1.dev20190603, 1.14.1.dev20190604, 1.14.1.dev20190605, 1.14.1.dev20190606, 1.14.1.dev20190607, 1.14.1.dev20190608, 1.14.1.dev20190611, 1.14.1.dev20190612, 1.14.1.dev20190613, 1.14.1.dev20190614, 1.14.1.dev20190615, 1.14.1.dev20190616, 1.14.1.dev20190617, 1.14.1.dev20190618, 1.14.1.dev20190619, 1.14.1.dev20190620, 1.14.1.dev20190621, 1.14.1.dev20190622, 1.14.1.dev20190623, 1.14.1.dev20190624, 1.14.1.dev20190625, 1.15.0.dev20190626, 1.15.0.dev20190627, 1.15.0.dev20190628, 1.15.0.dev20190703, 1.15.0.dev20190704, 1.15.0.dev20190705, 1.15.0.dev20190706, 1.15.0.dev20190707, 1.15.0.dev20190708, 1.15.0.dev20190709, 1.15.0.dev20190710, 1.15.0.dev20190711, 1.15.0.dev20190712, 1.15.0.dev20190713, 1.15.0.dev20190714, 1.15.0.dev20190715, 1.15.0.dev20190716, 1.15.0.dev20190717, 1.15.0.dev20190718, 1.15.0.dev20190723, 1.15.0.dev20190724, 1.15.0.dev20190725, 1.15.0.dev20190726, 1.15.0.dev20190727, 1.15.0.dev20190728, 1.15.0.dev20190729, 1.15.0.dev20190730)\r\nNo matching distribution found for tf-nightly==2.2.0dev20200129\r\n```\r\nWhat is your Linux and Python version? (for mine, see first posting here)", "@padoremu Regarding the installation of `tf-nightly`, it is strange. You should be able to install with the command I provided. `pip3 install tf-nightly==2.2.0dev20200129`.\r\n\r\nCan you please share the command you used? some time unknowingly we use pip instead of `pip3`.  \r\nI have Python 3.6.8 and Linux 5.2.17-1rodete3-amd64 x86_64. \r\n\r\nI will check the saving issue and respond to you soon. Thanks!", "@jvishnuvardhan I had copy&pasted. Now also created a fresh venv:\r\n```\r\ntest@test:~/dev/tensorflow$ python3 -m venv tf0\r\ntest@test:~/dev/tensorflow$ source tf0/bin/activate\r\n(tf0) test@test:~/dev/tensorflow$ pip3 install tf-nightly==2.2.0dev20200129\r\n```\r\nSame result as already posted here. I have Python 3.6.9 and Linux 5.3.0-28-generic x86_64.", "@padoremu Can you please follow the following steps to make virtual env.\r\n\r\n```\r\nStep 1: $:virtualenv --system-site-packages -p python3 ./venv_tfnightly\r\n\r\nStep 2: source ./venv_tfnightly/bin/activate  \r\n\r\nStep 3: pip3 install tf-nightly==2.2.0dev20200129\r\n\r\nStep 4: $ python3\r\n\r\nStep 5: >>> import tensorflow as tf\r\n>>> tf.__version__  # it should print '2.2.0-dev20200129'\r\n\r\n```\r\nThanks. Hope this helps.\r\n\r\n", "@jvishnuvardhan Thank you, that worked more or less, but after trying a full cleanup of my pip3 packages, because I got a warning due to some old tf packages (not in the virtualenv), something seems to be broken. So in the end I decided to use the nightly docker image, which works like a charm.\r\n\r\nWith the current nightly (2.2.0-dev20200202), loading the tf model works fine, which solves my problem, so I am closing the ticket. Thanks for the support!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36093\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36093\">No</a>\n", "Apparently, the bug has been fixed in the meanwhile. Saving the model as h5 with 2.2.0-dev20200228 works."]}, {"number": 36092, "title": "creating virtual environment and installing tensorflow package", "body": "can any one tell me how to  create virtual environment  and also install the tensor flow in python 3.7.2", "comments": ["@ravikanth076 \r\n\r\nCan you please go through the [link](https://www.tensorflow.org/install/pip) and see if it helps you.Thanks!\r\n", "thank u so much for the link.\r\nin 2nd point(creating virtual environment ) i do not understand the statement. the statement is given below:\r\nvirtualenv --system-site-packages -p python3 ./venv \r\n can u please explain me in detail?", "This is not a question for this venue as it is not related to Tensorflow code issues. Please consult relevant Python documentation, do Google searches, ask on Stack Overflow, etc."]}, {"number": 36091, "title": "Add usage example to tf.keras.utils.to_categorical", "body": "", "comments": []}, {"number": 36090, "title": "tf.GradientTape.gradients()", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 36089, "title": "Frozen Graph generation warning lead to error in running the model", "body": "**System information**\r\n- OS Platform and Distribution:Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: tensorflow-gpu       1.15.0\r\n- Python version: 3.6.10\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4\r\n- CUDA/cuDNN version:10.2\r\n- GPU model and memory: GEFORCE GTX 960M - 16GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nAim is to convert .ckpt and .config files to .tflite for ssd_mobilenet_v2_quantized_coco model.\r\nFollowing error is seen when converting from .ckpt and .config files to .pb\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nFollowing is the sequence of commands\r\n\r\n`(py36_tf_gpu) ridlr@ridlr107:~/TensorFlow/models-master/research/object_detection$ python export_tflite_ssd_graph.py --pipeline_config_path /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/pipeline.config --trained_checkpoint_prefix /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt --output_directory /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/tflite_graph_export_tflite/`\r\n\r\nThe above command generated a tflite_graph.pb file\r\n\r\n`tflite_convert --output_file tflite_graph.tflite --graph_def_file tflite_graph.pb --input_arrays normalized_input_image_tensor --output_arrays TFLite_Detection_PostProcess --input_shapes 1,300,300,3 --inference_type QUANTIZED_UINT8 --std_dev_values 0 --mean_values 1 --default_ranges_min 0 --default_ranges_max 6 --allow_custom_ops\r\n`\r\nThe above command generated a tflite_graph.tflite file\r\n\r\n```\r\n(py36_tf_gpu) ridlr@ridlr107:~/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/tflite_graph_export_tflite$ edgetpu_compiler tflite_graph.tflite \r\nEdge TPU Compiler version 2.0.267685300\r\nERROR: :106 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.\r\nERROR: Node number 0 (CONV_2D) failed to prepare.\r\n\r\n\r\nModel compiled successfully in 11 ms.\r\n\r\nInput model: tflite_graph.tflite\r\nInput size: 5.89MiB\r\nOutput model: tflite_graph_edgetpu.tflite\r\nOutput size: 5.88MiB\r\nOn-chip memory available for caching model parameters: 0.00B\r\nOn-chip memory used for caching model parameters: 0.00B\r\nOff-chip memory used for streaming uncached model parameters: 0.00B\r\nNumber of Edge TPU subgraphs: 0\r\nTotal number of operations: 0\r\nOperation log: tflite_graph_edgetpu.log\r\nSee the operation log file for individual operation details.\r\n```\r\n\r\n\r\nThe above command generated a tflite_grapf_edgetpu.tflite file inspite of the error. When I run this model on the Coral I get the following error\r\n\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nTraceback (most recent call last):\r\n  File \"detect_image.py\", line 124, in <module>\r\n    main()\r\n  File \"detect_image.py\", line 91, in main\r\n    interpreter.allocate_tensors()\r\n  File \"/home/ankit/anaconda3/envs/py35/lib/python3.5/site-packages/tflite_runtime/interpreter.py\", line 244, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/home/ankit/anaconda3/envs/py35/lib/python3.5/site-packages/tflite_runtime/interpreter_wrapper.py\", line 114, in AllocateTensors\r\n    return _interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/kernel_util.cc:119 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.Node number 0 (CONV_2D) failed to prepare.\r\n\r\n```\r\n\r\nI suspect that there is warning/Info displayed when generating the tflite_graph.pb file which is leading to the above error. The build log for the same is below. The line from the beolow log that concerns me is \r\n`INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold\r\n`\r\nHow Can I resolve this warning?\r\n```\r\n(py36_tf_gpu) ridlr@ridlr107:~/TensorFlow/models-master/research/object_detection$ python export_tflite_ssd_graph.py --pipeline_config_path /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/pipeline.config --trained_checkpoint_prefix /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt --output_directory /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/tflite_graph_export_tflite/\r\nWARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nWARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/slim/nets/inception_resnet_v2.py:374: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\r\n\r\nWARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\r\n\r\nWARNING:tensorflow:From export_tflite_ssd_graph.py:143: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\r\n\r\nWARNING:tensorflow:From export_tflite_ssd_graph.py:133: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n\r\nW0121 12:09:58.011585 140284674922304 module_wrapper.py:139] From export_tflite_ssd_graph.py:133: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n\r\nWARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:193: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\r\n\r\nW0121 12:09:58.016252 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:193: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\r\n\r\nWARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:237: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nW0121 12:09:58.016631 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:237: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nWARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n\r\nW0121 12:09:58.020018 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n\r\nWARNING:tensorflow:From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.__call__` method instead.\r\nW0121 12:09:58.022555 140284674922304 deprecation.py:323] From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.__call__` method instead.\r\nWARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\r\n\r\nW0121 12:09:59.660506 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\r\n\r\nWARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\r\n\r\nW0121 12:09:59.668454 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\r\n\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0121 12:09:59.668638 140284674922304 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0121 12:09:59.696758 140284674922304 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0121 12:09:59.722609 140284674922304 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0121 12:09:59.747994 140284674922304 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0121 12:09:59.774057 140284674922304 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0121 12:09:59.801787 140284674922304 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\r\nWARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\nW0121 12:09:59.837386 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2020-01-21 12:09:59.838240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-01-21 12:09:59.847343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-21 12:09:59.847611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176\r\npciBusID: 0000:01:00.0\r\n2020-01-21 12:09:59.847750: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\r\n2020-01-21 12:09:59.847852: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n2020-01-21 12:09:59.847962: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\r\n2020-01-21 12:09:59.848044: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\r\n2020-01-21 12:09:59.848109: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\r\n2020-01-21 12:09:59.848188: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\r\n2020-01-21 12:09:59.851251: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-21 12:09:59.851275: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-01-21 12:09:59.851543: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-21 12:09:59.875285: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2020-01-21 12:09:59.876004: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559d0e122aa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-21 12:09:59.876041: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-01-21 12:09:59.903778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-21 12:09:59.904104: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559d0e124900 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-01-21 12:09:59.904122: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0\r\n2020-01-21 12:09:59.904227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-21 12:09:59.904235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \r\nWARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:267: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n\r\nW0121 12:10:00.032385 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:267: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n\r\nWARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/builders/graph_rewriter_builder.py:41: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\r\n\r\nW0121 12:10:00.034862 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/builders/graph_rewriter_builder.py:41: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\r\n\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold\r\nI0121 12:10:01.108431 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv/depthwise/add_fold\r\nI0121 12:10:01.108743 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/expand/add_fold\r\nI0121 12:10:01.108995 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/add_fold\r\nI0121 12:10:01.109181 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/expand/add_fold\r\nI0121 12:10:01.109415 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/add_fold\r\nI0121 12:10:01.109586 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/expand/add_fold\r\nI0121 12:10:01.109818 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/add_fold\r\nI0121 12:10:01.109977 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/expand/add_fold\r\nI0121 12:10:01.110195 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/add_fold\r\nI0121 12:10:01.110363 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/expand/add_fold\r\nI0121 12:10:01.110591 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/add_fold\r\nI0121 12:10:01.110761 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/expand/add_fold\r\nI0121 12:10:01.111005 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/add_fold\r\nI0121 12:10:01.111187 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/expand/add_fold\r\nI0121 12:10:01.111420 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/add_fold\r\nI0121 12:10:01.111594 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/expand/add_fold\r\nI0121 12:10:01.111815 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/add_fold\r\nI0121 12:10:01.112004 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/expand/add_fold\r\nI0121 12:10:01.112233 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/add_fold\r\nI0121 12:10:01.112395 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/expand/add_fold\r\nI0121 12:10:01.112610 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/add_fold\r\nI0121 12:10:01.112753 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/expand/add_fold\r\nI0121 12:10:01.112978 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/add_fold\r\nI0121 12:10:01.113132 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/expand/add_fold\r\nI0121 12:10:01.113349 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/add_fold\r\nI0121 12:10:01.113510 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/expand/add_fold\r\nI0121 12:10:01.113732 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/add_fold\r\nI0121 12:10:01.113906 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/expand/add_fold\r\nI0121 12:10:01.114142 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/add_fold\r\nI0121 12:10:01.114297 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/expand/add_fold\r\nI0121 12:10:01.114525 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/add_fold\r\nI0121 12:10:01.114687 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/expand/add_fold\r\nI0121 12:10:01.114912 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/expand/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/add_fold\r\nI0121 12:10:01.115073 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv_1/add_fold\r\nI0121 12:10:01.115322 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/Conv_1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/add_fold\r\nI0121 12:10:01.115458 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/add_fold\r\nI0121 12:10:01.115600 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/add_fold\r\nI0121 12:10:01.115738 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/add_fold\r\nI0121 12:10:01.115870 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/add_fold\r\nI0121 12:10:01.116004 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/add_fold\r\nI0121 12:10:01.116144 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/add_fold\r\nI0121 12:10:01.116278 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/add_fold\r\nI0121 12:10:01.116429 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/add_fold\r\n2020-01-21 12:10:01.121862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-21 12:10:01.121904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \r\nWARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/exporter.py:111: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\r\n\r\nW0121 12:10:01.122092 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/exporter.py:111: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\r\n\r\nINFO:tensorflow:Restoring parameters from /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt\r\nI0121 12:10:01.465342 140284674922304 saver.py:1284] Restoring parameters from /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt\r\nWARNING:tensorflow:From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nW0121 12:10:03.793860 140284674922304 deprecation.py:323] From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\n2020-01-21 12:10:04.520189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-21 12:10:04.520234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \r\nINFO:tensorflow:Restoring parameters from /tmp/tmpkmw_fvex\r\nI0121 12:10:04.521256 140284674922304 saver.py:1284] Restoring parameters from /tmp/tmpkmw_fvex\r\nWARNING:tensorflow:From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nW0121 12:10:05.583929 140284674922304 deprecation.py:323] From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nWARNING:tensorflow:From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nW0121 12:10:05.584106 140284674922304 deprecation.py:323] From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nINFO:tensorflow:Froze 632 variables.\r\nI0121 12:10:06.121339 140284674922304 graph_util_impl.py:334] Froze 632 variables.\r\nINFO:tensorflow:Converted 632 variables to const ops.\r\nI0121 12:10:06.188232 140284674922304 graph_util_impl.py:394] Converted 632 variables to const ops.\r\n2020-01-21 12:10:06.305956: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes\r\n\r\n```", "comments": ["Using a docker resolved this issue. The Docker is provided by Coral and has all the necessary environment set.\r\nDocker Link : \r\nhttps://coral.ai/docs/edgetpu/retrain-detection/#set-up-the-docker-container", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36089\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36089\">No</a>\n"]}, {"number": 36088, "title": "tensorflow nightly still has a problem reported at #35029", "body": "I am using 2.2.0-dev20200119 with CUDA 10.1 on 1080Ti. I still meet the problem #35029  when I run the following code.\r\n\r\n```python\r\n#!/usr/bin/python3\r\nimport numpy as np;\r\nimport tensorflow as tf;\r\ngpus = tf.config.experimental.list_physical_devices('GPU');\r\ntf.config.experimental.set_memory_growth(gpus[0], True);\r\na=tf.constant(np.random.normal(size=(8,100)), dtype = tf.float32);\r\nb=tf.keras.layers.Dense(units=200)(a);\r\n```", "comments": ["@breadbread1984 I don't see any issue with your code. May be there is some cuda initialization issue. Can you please restart your system and run your code. Also, paste the full trace of the error. Thanks!", "i keep getting the following error\r\n\r\n>2020-01-23 07:49:01.813088: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/xieyi/.mujoco/mjpro150/bin:/home/xieyi/opt/nccl2/lib:/home/xieyi/opt/dlib/lib:/usr/local/cuda/lib64:\r\n2020-01-23 07:49:01.813219: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/xieyi/.mujoco/mjpro150/bin:/home/xieyi/opt/nccl2/lib:/home/xieyi/opt/dlib/lib:/usr/local/cuda/lib64:\r\n2020-01-23 07:49:01.813239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2020-01-23 07:49:02.771859: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-01-23 07:49:02.802388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-23 07:49:02.803359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.645GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-01-23 07:49:02.803587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-23 07:49:02.805465: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-23 07:49:02.807003: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-01-23 07:49:02.807307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-01-23 07:49:02.809246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-23 07:49:02.810396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-23 07:49:02.814600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-23 07:49:02.814733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-23 07:49:02.815611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-23 07:49:02.816410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-01-23 07:49:02.842086: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1995490000 Hz\r\n2020-01-23 07:49:02.843554: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x608ff40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-23 07:49:02.843593: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-01-23 07:49:02.949456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-23 07:49:02.950412: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x60f5700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-01-23 07:49:02.950479: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-01-23 07:49:02.950816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-23 07:49:02.952118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.645GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-01-23 07:49:02.952206: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-23 07:49:02.952249: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-23 07:49:02.952306: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-01-23 07:49:02.952348: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-01-23 07:49:02.952389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-23 07:49:02.952442: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-23 07:49:02.952497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-23 07:49:02.952632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-23 07:49:02.954021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-23 07:49:02.961920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-01-23 07:49:02.962037: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-23 07:49:02.964481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-23 07:49:02.964534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-01-23 07:49:02.964555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-01-23 07:49:02.964815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-23 07:49:02.966321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-23 07:49:02.967706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10390 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0123 07:49:03.801364 139757452814144 base_layer.py:1790] Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\r\n\r\n>If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\n>To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n\r\n>2020-01-23 07:49:03.802421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-23 07:49:03.802942: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-01-23 07:49:03.802996: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-01-23 07:49:03.803014: W tensorflow/stream_executor/stream.cc:2041] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 822, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py\", line 1142, in call\r\n    outputs = gen_math_ops.mat_mul(inputs, self.kernel)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\", line 5616, in mat_mul\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 6606, in raise_from_not_ok_status\r\n    >six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(8, 100), b.shape=(100, 200), m=8, n=200, k=100 [Op:MatMul]\r\n", "I reinstalled cuda 10.1 and solved the problem. sorry for the false alarm", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36088\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36088\">No</a>\n"]}, {"number": 36087, "title": "master branch build failed ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master branch commit-ID: \r\n4ab86d026ff419a5d35ee41493e29611f29a555d\r\n\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 1.2.1\r\n- GCC/Compiler version (if compiling from source): 6.3\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nBuild steps:\r\n```\r\nyes  \"\" | python configure.py \r\nbazel build --config=mkl  -c opt  //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nCrash with the error:\r\n```\r\nERROR: /home/lesliefang/tensorflow_master/tensorflow/tensorflow/core/common_runtime/eager/BUILD:352:1: C++ compilation of rule '//tensorflow/core/common_runtime/eager:mkl_eager_op_rewrite' failed (Exit 1)\r\ntensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc: In static member function 'static tensorflow::Status tensorflow::MklEagerOpRewrite::SetupNewOp(tensorflow::EagerOperation*, std::string, std::unique_ptr<tensorflow::EagerOperation>*)':\r\ntensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc:119:45: error: cannot convert 'tensorflow::EagerContext' to 'tensorflow::EagerContext*' in initialization\r\n   EagerContext* ctx = orig_op->EagerContext();\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Build failed at this line:https://github.com/tensorflow/tensorflow/blob/4ab86d026ff419a5d35ee41493e29611f29a555d/tensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc#L119\r\nwhich needs a EagerContext point but return a object.\r\nSeems the API changes by this commit:\r\n77aaa1ef2d85aa2766afb7c5ba069c4df0af5cb6\r\n", "It seems a fix is coming soon", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36087\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36087\">No</a>\n"]}, {"number": 36086, "title": "Attempt 4 at fixing release builds", "body": "Remove testing targets that were not tested during final release.", "comments": []}, {"number": 36085, "title": "Attempt 4 at fixing release builds", "body": "Remove testing targets that were not tested during final release.", "comments": []}, {"number": 36084, "title": "\"TypeError: Not JSON Serializable: Tensor\" when save model!", "body": "Hi,\r\nI just meet a problem when add a Variable to model as a changable weight for loss. But I have error when I save model. I don't find any way work for this problem.\r\n\r\nThis is my code. if I don't use changable weights, it works good.\r\n`model.alpha = tf.Variable(args.alpha, trainable=False, name='alpha', dtype=tf.float32)\r\nmodel.compile(optimizer=opt, loss=['mae', TVdist], loss_weights=[1.0 / (1 + model.alpha), model.alpha / (1 + model.alpha)])`\r\n\r\nThis is my error info:\r\nTraceback (most recent call last):\r\n  File \"train_sgdnet_change.py\", line 321, in <module>\r\n    save_best_only=False), reduce_lr, AlphaChange])\r\n  File \"/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/engine/training.py\", line 1418, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File \"/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/engine/training_generator.py\", line 251, in fit_generator\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/callbacks.py\", line 79, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/callbacks.py\", line 457, in on_epoch_end\r\n    self.model.save(filepath, overwrite=True)\r\n  File \"/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/engine/network.py\", line 1090, in save\r\n    save_model(self, filepath, overwrite, include_optimizer)\r\n  File \"/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/engine/saving.py\", line 382, in save_model\r\n    _serialize_model(model, f, include_optimizer)\r\n  File \"/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/engine/saving.py\", line 138, in _serialize_model\r\n    }, default=get_json_type).encode('utf8')\r\n  File \"/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/json/__init__.py\", line 251, in dumps\r\n    sort_keys=sort_keys, **kw).encode(obj)\r\n  File \"/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/json/encoder.py\", line 207, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/json/encoder.py\", line 270, in iterencode\r\n    return _iterencode(o, 0)\r\n  File \"/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/engine/saving.py\", line 74, in get_json_type\r\n    raise TypeError('Not JSON Serializable: %s' % (obj,))\r\nTypeError: Not JSON Serializable: Tensor(\"truediv:0\", shape=(), dtype=float32)\r\n\r\nThis is my environment:\r\nkeras 2.2.4\r\ntensorflow 1.10.1\r\n\r\nLooking forward for a solution!\r\nAaron", "comments": ["@Aaron9477 \r\n\r\nLooks like code is incomplete. Please, help us with the minimal standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@Aaron9477 \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36083, "title": "TensorBoard showing nothing after upgrade ", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 1809\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: cuda_10.1.243_426.00_win10 / cudnn-10.1-windows10-x64-v7.6.5.32\r\n- GPU model and memory: gtx 1070 / 6g\r\n\r\n**Describe the current behavior**\r\nI log some data by tf.summary.scalar, every things is ok UNTILL i upgrade tf from 2.0.0 to 2.1.0\r\nthrough pip install -upgrade tensorflow.\r\nThen I use TensorBoard but both chrome and edge show nothing.\r\nI can get correct result by TensorBoard in tf 2.0.0 which is installed on other mechine.\r\nThen i try to reinstall tensorboard by pip uninstall tensorboard then pip install tensorboard,\r\nbut still not working.\r\n\r\n**Describe the expected behavior**\r\nWeb page show nothing on both chrome and edge.\r\n\r\n**Code to reproduce the issue**\r\nI run tensorboard by CMD: tensorboard --logdir=log\r\n\r\n**Other info / logs**\r\nCMD show\r\n2020-01-21 12:26:21.211807: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\r\nTensorBoard 2.1.0 at http://localhost:6006/ (Press CTRL+C to quit)", "comments": ["@yomcoding \r\n\r\nRequest you to provide the simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "I log the loss value by \r\n```\r\nfile_writer = tf.summary.create_file_writer(self.logs_path)\r\nfile_writer.set_as_default()\r\n# ......\r\nloss_value = loss(y_train[step], y_pred)\r\ntf.summary.scalar(\"Loss\",loss_value,step=cur_stp + step)\r\n# ......\r\n```\r\n\r\nThen i got log file, and run tb by CMD: tensorboard --logdir=log\r\nbut when I open webpage, it shows nothing:\r\n![result](https://fn.storage.img.ztzl.moe/2020/01/22/efa4ed8e1826bc98e2070273c7884a12.jpg)\r\n\r\nHow can I get log info of tensorboard?", "Yes, I tried running on colab notebook, I ran into the error too. Here is my [gist](https://colab.research.google.com/gist/gowthamkpr/85159b9cdabb3190d1825d6991af04e6/tensorboard_in_notebooks.ipynb)", "Sorry you're having trouble. \r\n\r\nCan you please run the diagnose_tensorboard.py script (link below) in the same\r\nenvironment from which you normally run TensorFlow/TensorBoard, and\r\npaste the output here?\r\n\r\nhttps://github.com/tensorflow/tensorboard/blob/master/tensorboard/tools/diagnose_tensorboard.py\r\n\r\n\r\n\r\n\r\n\r\n", "### Diagnostics\r\n\r\n<details>\r\n<summary>Diagnostics output</summary>\r\n\r\n``````\r\n--- check: autoidentify\r\nINFO: diagnose_tensorboard.py version 5ee40aab64f17918c57bcfbddac7618d5f85956f\r\n\r\n--- check: general\r\nINFO: sys.version_info: sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\r\nINFO: os.name: nt\r\nINFO: os.uname(): N/A\r\nINFO: sys.getwindowsversion(): sys.getwindowsversion(major=10, minor=0, build=17763, platform=2, service_pack='')\r\n\r\n--- check: package_management\r\nINFO: has conda-meta: False\r\nINFO: $VIRTUAL_ENV: None\r\n\r\n--- check: installed_packages\r\nINFO: installed: tb-nightly==1.14.0a20190603\r\nINFO: installed: tensorboard==2.1.0\r\nWARNING: conflicting installations: ['tb-nightly', 'tensorboard']\r\nINFO: installed: tensorflow==2.1.0\r\nINFO: installed: tf-estimator-nightly==1.14.0.dev2019060501\r\nINFO: installed: tensorflow-estimator==2.1.0\r\nWARNING: conflicting installations: ['tensorflow-estimator', 'tf-estimator-nightly']\r\n\r\n--- check: tensorboard_python_version\r\nINFO: tensorboard.version.VERSION: '2.1.0'\r\n\r\n--- check: tensorflow_python_version\r\n2020-01-24 08:37:23.950783: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nINFO: tensorflow.__version__: '2.1.0'\r\nINFO: tensorflow.__git_version__: 'v2.1.0-rc2-17-ge5bf8de410'\r\n\r\n--- check: tensorboard_binary_path\r\nINFO: which tensorboard: b'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Python37_64\\\\Scripts\\\\tensorboard.exe\\r\\n'\r\n\r\n--- check: addrinfos\r\nsocket.has_ipv6 = True\r\nsocket.AF_UNSPEC = <AddressFamily.AF_UNSPEC: 0>\r\nsocket.SOCK_STREAM = <SocketKind.SOCK_STREAM: 1>\r\nsocket.AI_ADDRCONFIG = <AddressInfo.AI_ADDRCONFIG: 1024>\r\nsocket.AI_PASSIVE = <AddressInfo.AI_PASSIVE: 1>\r\nLoopback flags: <AddressInfo.AI_ADDRCONFIG: 1024>\r\nLoopback infos: [(<AddressFamily.AF_INET6: 23>, <SocketKind.SOCK_STREAM: 1>, 0, '', ('::1', 0, 0, 0)), (<AddressFamily.AF_INET: 2>, <SocketKind.SOCK_STREAM: 1>, 0, '', ('127.0.0.1', 0))]\r\nWildcard flags: <AddressInfo.AI_PASSIVE: 1>\r\nWildcard infos: [(<AddressFamily.AF_INET6: 23>, <SocketKind.SOCK_STREAM: 1>, 0, '', ('::', 0, 0, 0)), (<AddressFamily.AF_INET: 2>, <SocketKind.SOCK_STREAM: 1>, 0,\r\n'', ('0.0.0.0', 0))]\r\n\r\n--- check: readable_fqdn\r\nINFO: socket.getfqdn(): 'YOM-PC.lan'\r\n\r\n--- check: stat_tensorboardinfo\r\nINFO: directory: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\.tensorboard-info\r\nINFO: os.stat(...): os.stat_result(st_mode=16895, st_ino=20547673299973826, st_dev=1780807057, st_nlink=1, st_uid=0, st_gid=0, st_size=0, st_atime=1579663853, st_mtime=1579663853, st_ctime=1579243708)\r\nINFO: mode: 0o40777\r\n\r\n--- check: source_trees_without_genfiles\r\nINFO: tensorboard_roots (1): ['C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Python37_64\\\\lib\\\\site-packages']; bad_roots (0): []\r\n\r\n--- check: full_pip_freeze\r\nINFO: pip freeze --all:\r\nabsl-py==0.8.0\r\nastor==0.8.0\r\nastroid==2.2.5\r\nautopep8==1.4.4\r\ncachetools==4.0.0\r\ncertifi==2019.11.28\r\ncffi==1.13.2\r\nchardet==3.0.4\r\ncolorama==0.4.1\r\ncupy==6.3.0\r\ncycler==0.10.0\r\nCython==0.29.13\r\neasydict==1.9\r\nfastrlock==0.4\r\ngast==0.2.2\r\ngoogle-auth==1.10.0\r\ngoogle-auth-oauthlib==0.4.1\r\ngoogle-pasta==0.1.7\r\ngrpcio==1.26.0\r\nh5py==2.10.0\r\nhanziconv==0.3.2\r\nidna==2.8\r\nisort==4.3.21\r\njsonlines==1.2.0\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.0\r\nkiwisolver==1.1.0\r\nlazy-object-proxy==1.4.2\r\nlxml==4.4.1\r\nMarkdown==3.1.1\r\nmatplotlib==3.1.1\r\nmccabe==0.6.1\r\nmsgpack==0.6.2\r\nnumpy==1.17.2\r\noauthlib==3.1.0\r\nopencv-contrib-python==4.1.1.26\r\nopencv-python==4.1.2.30\r\nopt-einsum==3.0.1\r\nPillow==6.2.1\r\npip==19.3.1\r\nplotly==4.3.0\r\nprotobuf==3.9.1\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\npycodestyle==2.5.0\r\npycparser==2.19\r\npylint==2.3.1\r\npyparsing==2.4.2\r\npython-dateutil==2.8.0\r\nPyYAML==5.2\r\nrequests==2.22.0\r\nrequests-oauthlib==1.3.0\r\nretrying==1.3.3\r\nrsa==4.0\r\nscipy==1.4.1\r\nsetuptools==41.2.0\r\nsix==1.12.0\r\ntb-nightly==1.14.0a20190603\r\ntensorboard==2.1.0\r\ntensorboardX==1.9\r\ntensorflow==2.1.0\r\ntensorflow-estimator==2.1.0\r\ntermcolor==1.1.0\r\ntf-estimator-nightly==1.14.0.dev2019060501\r\ntorch==1.3.1\r\ntorchvision==0.4.2\r\ntyped-ast==1.4.0\r\nurllib3==1.25.7\r\nWerkzeug==0.15.6\r\nwheel==0.33.6\r\nwrapt==1.11.2\r\n\r\n``````\r\n\r\n</details>\r\n\r\n### Suggestion: Fix conflicting installations\r\n\r\nConflicting package installations found. Depending on the order of\r\ninstallations and uninstallations, behavior may be undefined. Please\r\nuninstall ALL versions of TensorFlow and TensorBoard, then reinstall\r\nONLY the desired version of TensorFlow, which will transitively pull\r\nin the proper version of TensorBoard. (If you use TensorBoard without\r\nTensorFlow, just reinstall the appropriate version of TensorBoard\r\ndirectly.)\r\n\r\nNamely:\r\n\r\n        pip uninstall tb-nightly tensorboard tensorflow tensorflow-estimator tf-estimator-nightly\r\n        pip install tensorflow  # or `tensorflow-gpu`, or `tf-nightly`, ...\r\n\r\n### Next steps\r\n\r\nPlease try each suggestion enumerated above to determine whether it\r\nsolves your problem. If none of these suggestions works, please copy\r\nALL of the above output, including the lines containing only\r\nbackticks, into your GitHub issue or comment. Be sure to redact any\r\nsensitive information.", "Thanks. It looks like you have a polluted Python environment with conflicting tensorboard versions. See your diagnostic output above.\r\n\r\nCan you please try with a clean Python virtualenv? I'm going to close this because this is an installation issue and not an underlying tensorboard issue.\r\n\r\nPlease reopen if you find an issue with tensorboard.", "@manivaradarajan I have experienced the same problem and I tried to debug it by running the code provided by you. However, it asked me to copy all the output to the comment (as provided below). Could you please tell me what is the problem in my case? Thanks for your help in advance.\r\n\r\n> \r\n### Diagnostics\r\n\r\n<details>\r\n<summary>Diagnostics output</summary>\r\n\r\n``````\r\n--- check: autoidentify\r\nINFO: diagnose_tensorboard.py version d515ab103e2b1cfcea2b096187741a0eeb8822ef\r\n\r\n--- check: general\r\nINFO: sys.version_info: sys.version_info(major=3, minor=7, micro=6, releaselevel='final', serial=0)\r\nINFO: os.name: nt\r\nINFO: os.uname(): N/A\r\nINFO: sys.getwindowsversion(): sys.getwindowsversion(major=10, minor=0, build=17134, platform=2, service_pack='')\r\n\r\n--- check: package_management\r\nINFO: has conda-meta: True\r\nINFO: $VIRTUAL_ENV: None\r\n\r\n--- check: installed_packages\r\nINFO: installed: tensorboard==2.1.0\r\nINFO: installed: tensorflow-gpu==2.1.0\r\nWARNING: no installation among: ['tensorflow-estimator', 'tensorflow-estimator-2.0-preview', 'tf-estimator-nightly']\r\n\r\n--- check: tensorboard_python_version\r\nINFO: tensorboard.version.VERSION: '2.1.0'\r\n\r\n--- check: tensorflow_python_version\r\n2020-04-06 12:44:52.236991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nINFO: tensorflow.__version__: '2.1.0'\r\nINFO: tensorflow.__git_version__: 'v2.1.0-rc2-17-ge5bf8de410'\r\n\r\n--- check: tensorboard_binary_path\r\nINFO: which tensorboard: b'C:\\\\Users\\\\bobgao\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\venv\\\\Scripts\\\\tensorboard.exe\\r\\nC:\\\\Users\\\\bobgao\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36\\\\Scripts\\\\tensorboard.exe\\r\\n'\r\n\r\n--- check: addrinfos\r\nsocket.has_ipv6 = True\r\nsocket.AF_UNSPEC = <AddressFamily.AF_UNSPEC: 0>\r\nsocket.SOCK_STREAM = <SocketKind.SOCK_STREAM: 1>\r\nsocket.AI_ADDRCONFIG = <AddressInfo.AI_ADDRCONFIG: 1024>\r\nsocket.AI_PASSIVE = <AddressInfo.AI_PASSIVE: 1>\r\nLoopback flags: <AddressInfo.AI_ADDRCONFIG: 1024>\r\nLoopback infos: [(<AddressFamily.AF_INET6: 23>, <SocketKind.SOCK_STREAM: 1>, 0, '', ('::1', 0, 0, 0)), (<AddressFamily.AF_INET: 2>, <SocketKind.SOCK_STREAM: 1>, 0, '', ('127.0.0.1', 0))]\r\nWildcard flags: <AddressInfo.AI_PASSIVE: 1>\r\nWildcard infos: [(<AddressFamily.AF_INET6: 23>, <SocketKind.SOCK_STREAM: 1>, 0, '', ('::', 0, 0, 0)), (<AddressFamily.AF_INET: 2>, <SocketKind.SOCK_STREAM: 1>, 0, '', ('0.0.0.0', 0))]\r\n\r\n--- check: readable_fqdn\r\nINFO: socket.getfqdn(): 'paladium.UGCT.UGENT.BE'\r\n\r\n--- check: stat_tensorboardinfo\r\nINFO: directory: C:\\Users\\bobgao\\AppData\\Local\\Temp\\.tensorboard-info\r\nINFO: os.stat(...): os.stat_result(st_mode=16895, st_ino=7318349396135381, st_dev=3774082503, st_nlink=1, st_uid=0, st_gid=0, st_size=4096, st_atime=1586169846, st_mtime=1586169846, st_ctime=1585913527)\r\nINFO: mode: 0o40777\r\n\r\n--- check: source_trees_without_genfiles\r\nINFO: tensorboard_roots (1): ['C:\\\\Users\\\\bobgao\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\venv\\\\lib\\\\site-packages']; bad_roots (0): []\r\n\r\n--- check: full_pip_freeze\r\nINFO: pip freeze --all:\r\nabsl-py==0.9.0\r\nalabaster==0.7.12\r\nargh==0.26.2\r\nasn1crypto==1.3.0\r\nastor==0.8.1\r\nastroid==2.3.3\r\natomicwrites==1.3.0\r\nattrs==19.3.0\r\nautopep8==1.4.4\r\nBabel==2.8.0\r\nbackcall==0.1.0\r\nbcrypt==3.1.7\r\nbleach==3.1.0\r\ncachetools==4.0.0\r\ncertifi==2019.11.28\r\ncffi==1.14.0\r\nchardet==3.0.4\r\ncloudpickle==1.2.2\r\ncolorama==0.4.3\r\ncryptography==2.8\r\ncycler==0.10.0\r\ndecorator==4.4.1\r\ndefusedxml==0.6.0\r\ndiff-match-patch==20181111\r\ndill==0.3.1.1\r\ndocutils==0.16\r\nentrypoints==0.3\r\nflake8==3.7.9\r\nfuture==0.18.2\r\ngast==0.2.2\r\ngoogle-auth==1.11.2\r\ngoogle-auth-oauthlib==0.4.1\r\ngoogle-pasta==0.1.8\r\ngoogleapis-common-protos==1.51.0\r\ngrpcio==1.27.2\r\ngym==0.16.0\r\nh5py==2.10.0\r\nidna==2.8\r\nimageio==2.8.0\r\nimagesize==1.2.0\r\nimportlib-metadata==1.5.0\r\nintervaltree==3.0.2\r\nipykernel==5.1.4\r\nipython==7.12.0\r\nipython-genutils==0.2.0\r\nipywidgets==7.5.1\r\nisort==4.3.21\r\njedi==0.15.2\r\nJinja2==2.11.1\r\njsonschema==3.2.0\r\njupyter==1.0.0\r\njupyter-client==5.3.4\r\njupyter-console==6.1.0\r\njupyter-core==4.6.1\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.0\r\nkeyring==21.1.0\r\nkiwisolver==1.1.0\r\nlazy-object-proxy==1.4.3\r\nlibtiff==0.4.2\r\nMarkdown==3.2.1\r\nMarkupSafe==1.1.1\r\nmatplotlib==3.1.3\r\nmccabe==0.6.1\r\nmistune==0.8.4\r\nnbconvert==5.6.1\r\nnbformat==5.0.4\r\nnetworkx==2.4\r\nnotebook==6.0.3\r\nnumpy==1.18.1\r\nnumpydoc==0.9.2\r\noauthlib==3.1.0\r\nodl==0.7.0\r\nopt-einsum==3.1.0\r\npackaging==20.1\r\npandocfilters==1.4.2\r\nparamiko==2.6.0\r\nparso==0.5.2\r\npathtools==0.1.2\r\npexpect==4.8.0\r\npickleshare==0.7.5\r\nPillow==7.0.0\r\npip==20.0.2\r\npluggy==0.13.1\r\nprometheus-client==0.7.1\r\npromise==2.3\r\nprompt-toolkit==3.0.3\r\nprotobuf==3.11.3\r\npsutil==5.6.7\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\npycodestyle==2.5.0\r\npycparser==2.19\r\npydocstyle==4.0.1\r\npydot-ng==2.0.0\r\npyflakes==2.1.1\r\npyglet==1.5.0\r\nPygments==2.5.2\r\npylint==2.4.4\r\nPyNaCl==1.3.0\r\npyOpenSSL==19.1.0\r\npyparsing==2.4.6\r\npyrsistent==0.15.7\r\nPySocks==1.7.1\r\npython-dateutil==2.8.1\r\npython-jsonrpc-server==0.3.4\r\npython-language-server==0.31.9\r\npytz==2019.3\r\nPyWavelets==1.1.1\r\npywin32==227\r\npywin32-ctypes==0.2.0\r\npywinpty==0.5.7\r\nPyYAML==5.3\r\npyzmq==18.1.1\r\nQDarkStyle==2.8\r\nQtAwesome==0.6.1\r\nqtconsole==4.6.0\r\nQtPy==1.9.0\r\nrequests==2.22.0\r\nrequests-oauthlib==1.3.0\r\nrope==0.16.0\r\nrsa==4.0\r\nRtree==0.9.3\r\nscikit-image==0.16.2\r\nscipy==1.4.1\r\nSend2Trash==1.5.0\r\nsetuptools==45.2.0.post20200210\r\nsix==1.14.0\r\nsnowballstemmer==2.0.0\r\nsortedcontainers==2.1.0\r\nSphinx==2.4.0\r\nsphinxcontrib-applehelp==1.0.1\r\nsphinxcontrib-devhelp==1.0.1\r\nsphinxcontrib-htmlhelp==1.0.2\r\nsphinxcontrib-jsmath==1.0.1\r\nsphinxcontrib-qthelp==1.0.2\r\nsphinxcontrib-serializinghtml==1.1.3\r\nspyder==4.1.1\r\nspyder-kernels==1.9.0\r\ntensorboard==2.1.0\r\ntensorflow-datasets==2.0.0\r\ntensorflow-examples===b9e79fa20b42e04efbf2b76e3dd984fc195fab8d-\r\ntensorflow-gpu==2.1.0\r\ntensorflow-gpu-estimator==2.1.0\r\ntensorflow-metadata==0.21.1\r\ntermcolor==1.1.0\r\nterminado==0.8.3\r\ntestpath==0.4.4\r\ntornado==6.0.3\r\ntqdm==4.42.1\r\ntraitlets==4.3.3\r\nujson==1.35\r\nurllib3==1.25.8\r\nwatchdog==0.10.2\r\nwcwidth==0.1.8\r\nwebencodings==0.5.1\r\nWerkzeug==1.0.0\r\nwheel==0.34.2\r\nwidgetsnbextension==3.5.1\r\nwin-inet-pton==1.1.0\r\nwincertstore==0.2\r\nwrapt==1.12.0\r\nyapf==0.28.0\r\nzipp==2.2.0\r\n\r\n``````\r\n\r\n</details>\r\n\r\n### Next steps\r\n\r\nNo action items identified. Please copy ALL of the above output,\r\nincluding the lines containing only backticks, into your GitHub issue\r\nor comment. Be sure to redact any sensitive information.\r\n\r\n", "hi, bro. I've got the same issue today, my original tensorboard version is 2.1.0, but my tensorflow version is 1.15. I downgraded the tensorboard version as 2.0.0, got an error like \" ERROR: tensorflow-gpu 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.0 which is incompatible.\" ,so I install a new version tensorboard  fowllowing the cmd: pip install tensorboard==1.15, then, problem solved! Hope helpful for you."]}, {"number": 36082, "title": "[s3_file_system] set sync_needed_ as false after Sync()", "body": "In file systems like s3 and gcs, `sync_needed` is meant to track whether an upload is needed. GCS file system correctly implemented it, but it's not actually in effect in s3_file_system.\r\n\r\nThis looks like a bug in s3, and this fix should correct it.\r\n\r\n\r\nI don't have the historic context of whether this is intentional. Please let me know if I am wrong.\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36082) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36082) for more info**.\n\n<!-- ok -->", "@mrry may have some context, please redirect if I am wrong!", "I have no context, but the change looks sensible to me, so I'm happy to approve it."]}, {"number": 36081, "title": "Which distributed strategy should I use when running training without GPU?", "body": "There are many distributed strategy in Tensorflow,\r\n\r\n- MirroredStrategy\r\n- TPUStrategy\r\n- MultiWorkerMirroredStrategy\r\n- CentralStorageStrategy\r\n- ParameterServerStrategy\r\n- OneDeviceStrategy\r\n\r\nSome of them run on one machine and broadcast model to different GPUs, and some of them use different GPUs on different machines.\r\n\r\nMy question if I don't have GPU in my server, which strategy should I use, so I can get the benefit when running distributed training?\r\n\r\nI try to run MirroredStrategy, ParameterServerStrategy on four machines, but it seems that it's slower than running on a single machine.", "comments": ["Sorry for the delay. Since you want to train on multiple machines then you should look at [`MultiWorkerMirroredStrategy`](https://www.tensorflow.org/guide/distributed_training#multiworkermirroredstrategy) or [`ParameterServerStrategy`](https://www.tensorflow.org/guide/distributed_training#parameterserverstrategy). \r\nThanks!", "I will give it a try", "Scaling vertically might lead to better results compared to using multiple machines if you don't have GPUs. The overhead of synchronizing the gradients across your machines with CPUs could explain why scaling to four machines is not reducing your training time. ", "As mentioned by @ymodak, both MultiWorkerMirroredStrategy or ParameterServerStrategy support training across multiple machines. Closing this issue now since it is not a bug, but feel free to create a new issue if you run into performance issues with these strategies in the future.", "@nikitamaia Thanks for the reply and help"]}, {"number": 36080, "title": "Enable explicit batch mode with optimization profiles", "body": "- This PR adds a new experimental feature to TF-TRT.\r\n- This features enables the explicit batch mode in TF-TRT which comes with two main advantages: 1) Batch dimension is not a special dimension in this mode, which means a single TRT network can have layers with different batch sizes and more importantly reshape layers can change the batch dimension dynamically, 2) TRT networks can be created with dynamic shapes (-1 dims), while optimization profiles are generated automatically out of the users data to hint TRT for optimizing the built engines.\r\n- Currently this feature does not support building TRT engines during runtime, which means the only single engine that is built during `build()` can be used during runtime. This makes it important for users to feed data to `build()` with all interesting shapes so that the single engine can execute those shapes.\r\n- Currently this feature does not support INT8 calibration.\r\n- This feature is not supposed to change any of the existing behavior of TF-TRT in any of the various modes.\r\n- It is expected to use this feature for elementwise ops. We are soon going to enable all the existing converters to work with this feature.\r\n\r\nAn example on how to use this feature:\r\n\r\n```\r\n    root = _model()\r\n    save.save(root, input_saved_model_dir,\r\n              {signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: root.run})\r\n\r\n    rewriter_config_with_trt = rewriter_config_pb2.RewriterConfig()\r\n    rewriter_config_with_trt.optimizers.extend(\r\n        [\"constfold\", \"layout\", \"constfold\"])\r\n    rewriter_config_with_trt.meta_optimizer_iterations = (\r\n        rewriter_config_pb2.RewriterConfig.ONE)\r\n    optimizer = rewriter_config_with_trt.custom_optimizers.add()\r\n    rewriter_config_with_trt.custom_optimizers.add().name = \"constfold\"\r\n    optimizer.name = \"TensorRTOptimizer\"\r\n    optimizer.parameter_map[\"minimum_segment_size\"].i = 1\r\n    optimizer.parameter_map[\"is_dynamic_op\"].b = True\r\n    optimizer.parameter_map[\"maximum_cached_engines\"].i = 1\r\n    optimizer.parameter_map[\"use_implicit_batch\"].b = False\r\n\r\n    conversion_params = trt_convert.DEFAULT_TRT_CONVERSION_PARAMS._replace(\r\n        rewriter_config_template=rewriter_config_with_trt)\r\n    converter = trt_convert.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir,\r\n                                    conversion_params=conversion_params)\r\n    converter.convert()\r\n\r\n    input_shapes = [(2, 4), (3, 9), (64, 128)]\r\n    def my_input_fn():\r\n      for x in input_shapes:\r\n        yield (np.random.normal(size=x).astype(np.float32),)\r\n\r\n    converter.build(input_fn=my_input_fn)\r\n\r\n    output_saved_model_dir = self.mkdtemp()\r\n    converter.save(output_saved_model_dir=output_saved_model_dir)\r\n```\r\n\r\n", "comments": ["I have started to break this PR into smaller steps. The first one is PR #36379, and more is on the way. Please review these first.", "Here is a list of PRs related to the current one. They incrementally add changes that shall make the current PR smaller. Each PR is based on the previous one in the list, please review them in the given order:\r\n1  #36379\r\n2. #36434\r\n3. #36435\r\n4. #36439\r\n5. Add optimizaton profiles #36660\r\n6. Handle multiple contexts #36664\r\n7. ~Test deserialization of opt profiles~ (will be a separate independent PR)\r\n\r\nAfter these the current PR will be reduced to the following: Enable build mode to generate optimization profiles for dynamic shapes. ", "@pooyadavoodi Can you please resolve conflicts? Thanks!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36080) for more info**.\n\n<!-- need_sender_cla -->", "Thanks @gbaned for the message, I have updated the PR. Please note that PRs #36660 and #36664 have to merged before this one.", "@pooyadavoodi, @tfeher Can you please sign cla ? Thank you!", "@googlebot I consent", "The CLA is failing because the original PR author has left the company and is not on the corporate CLA anymore. Since this PR is anyways broken into smaller pieces, I have taken the last piece still contained here and moved into a separate PR #36729.  \r\n\r\nThis PR can be closed.\r\n\r\nFor completeness, here is a list of PRs that were created out of this one:\r\n1 Enable explicit batch mode #36379\r\n2. Improve binding index query #36434\r\n3. Add binding size specification #36435\r\n4. Define networks with dynamic shapes #36439\r\n5. Add optimizaton profiles #36660\r\n6. Execution context managment #36664\r\n7. ~Test deserialization of opt profiles~ (will be added independently of the rest in a separate PR).\r\n8.  TensorRT profile generation mode #36729\r\n", "Thanks **@tfeher** \r\nClosing this PR, as its change has been moved to [#36729](https://github.com/tensorflow/tensorflow/pull/36729)"]}, {"number": 36079, "title": "TimeDistributed compute_mask: Handling case when input uid isn't in _input_map", "body": "Fixes #33261\r\n\r\nWhen `input_uid` isn't a key in `self._input_map`, the wrong `inner_inputs` tensor is passed to `self.layer.compute_mask` (it still has the sequence dimension).", "comments": ["Btw. the input_map has being removed to avoid memory leak. Could u check your original issue and see if its resolved by that?\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/33441", "@qlzh727 Yes, it looks like 9f2aa61811b29e700b8325bb57b1f4b0093c1d4d resolves the issue. Thanks!"]}, {"number": 36078, "title": "[Speech Commands] export function playRawAudio() no bit depth is set.", "body": "**System information**\r\n-  stock example script provided in TensorFlow `tfjs/SpeechCommands`\r\n- Windows 10\r\n- Mobile device N/A\r\n- TensorFlow installed from (source):\r\n- TensorFlow version (use command below): Latest\r\n- Python version: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: sli gtx\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nCurrent playback of captured speech command produces audible degradation. Setting breakpoints on `export function playRawAudio()` indicates a sample rate of `44100` which is good but I am not seeing the bit depth set anywhere. The playback seems to be using a very low bit depth 8bit? \r\n\r\n**Describe the expected behavior**\r\nThe expected behavior should play back recorded audio sample with clarity, changing the bit depth to 16, 24 or 32 bit float would be ideal.\r\n\r\n**Code to reproduce the issue**\r\n**browser_fft_utils.ts**\r\n\r\n```\r\nexport function playRawAudio(\r\n    rawAudio: RawAudioData, onEnded: () => void|Promise<void>): void {\r\n  const audioContextConstructor =\r\n      // tslint:disable-next-line:no-any\r\n      (window as any).AudioContext || (window as any).webkitAudioContext;\r\n  const audioContext: AudioContext = new audioContextConstructor();\r\n  const arrayBuffer =\r\n      audioContext.createBuffer(1, rawAudio.data.length, rawAudio.sampleRateHz);\r\n  const nowBuffering = arrayBuffer.getChannelData(0);\r\n  nowBuffering.set(rawAudio.data);\r\n  const source = audioContext.createBufferSource();\r\n  source.buffer = arrayBuffer;\r\n  source.connect(audioContext.destination);\r\n  source.start();\r\n  source.onended = () => {\r\n    if (onEnded != null) {\r\n      onEnded();\r\n    }\r\n  };\r\n\r\n```\r\n\r\n**Other info / logs**\r\nhttps://en.wikipedia.org/wiki/Audio_bit_depth\r\n", "comments": ["@mcwebdev \r\n\r\nHi, This is a TF.js issue . Kindly raise a new issue in TF.js repo using this [link](https://github.com/tensorflow/tfjs/issues/new).Thanks!\r\n\r\n", "@mcwebdev \r\n\r\nHope you have raised the issue in TF.js repo. Can we track the issue in TF.js repo and close here. Please, confirm. Thanks!", "This issue can be closed, further analysis revealed the bit depth being set correctly. It seems the duration multiplier was the culprit. I think I will change that label to reflect `playback speed` in my app.", "@mcwebdev \r\n\r\nGlad to know that you have found workaround for this issue. I am closing the issue .Thanks for the confirmation."]}, {"number": 36077, "title": "Migration issue make_one_shot_iterator", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n**ValueError: For performance reasons Keras `fit`, `evaluate` and`predict` accept tf.data `Datasets` as input but not iterators that have been manually generated from Datasets by users. Please directly pass in the original `Dataset` object instead of passing in `iter(dataset)`.**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n\r\nTrying to migrate the code from tensorflow 1.10 to tensorflow 2.0. using  \r\n**import tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()**\r\n\r\nif train_obj.if_validate:\r\n            tr_history = model.fit(tf.data.make_one_shot_iterator(tr_dataset), epochs=self.epochs, callbacks=callbacks, steps_per_epoch=self.steps_per_epoch, validation_data=tf.data.make_one_shot_iterator(val_dataset), validation_steps=self.validation_steps)\r\n        else:\r\n            tr_history = model.fit(tr_dataset.make_one_shot_iterator(), epochs=self.epochs, callbacks=callbacks, steps_per_epoch=self.steps_per_epoch)\r\n        \r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\nSince I'm importing tensorflow.compat.v1 as tf and disabling v2 behaviour. Why am I still not able to use make_one_shot_iterator?\r\n\r\n", "comments": ["@shuklakirti, Please follow the instructions mentioned in this [document](https://www.tensorflow.org/guide/migrate) to migrate from Tf1 to Tf2. Thanks", "@gadagashwini Thank you for replying.\r\nThe document says \r\n**It is still possible to run 1.X code, unmodified (except for contrib), in TensorFlow 2.0 import tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n\r\nIf your code works in TensorFlow 2.0 using tf.compat.v1.disable_v2_behavior(), there are still global behavioral changes you may need to address. The major changes are:\r\n\r\nEager execution, v1.enable_eager_execution()t.\r\nResource variables, v1.enable_resource_variables():\r\nTensor shapes, v1.enable_v2_tensorshape()\r\nControl flow, v1.enable_control_flow_v2()**\r\n\r\nI'm not able to use make_one_shot_iterator since Keras accept tf.data Datasets as input but not iterators. How can I make it work?\r\nPassing in the original Dataset object is also not working.\r\n", "Hi\r\n**Update on the issue: AttributeError: 'DatasetV1Adapter' object has no attribute shape**\r\nWhen I changed the code to\r\n        if train_obj.if_validate:\r\n            tr_history = model.fit(tr_dataset, epochs=self.epochs, callbacks=callbacks, steps_per_epoch=self.steps_per_epoch, validation_data=val_dataset, validation_steps=self.validation_steps)\r\n\r\nTraceback (most recent call last):\r\n\r\n  File , line 711, in <module>\r\n    run_fn(model_name, model_path, data_path + '/model/params.json', data_path + '/experiment/params.json')\r\n\r\n  File, line 613, in run_fn\r\n    acc = run_obj.train_and_test(model_name,multi_gpu=True)\r\n\r\n  File , line 236, in train_and_test\r\n    train_obj.multi_gpu_train(train_data_obj, parallel_model, compile_train_model)\r\n\r\n  File, line 313, in multi_gpu_train\r\n    tr_history = model.fit(tr_dataset, epochs=self.epochs, callbacks=callbacks, steps_per_epoch=self.steps_per_epoch, validation_data=val_dataset, validation_steps=self.validation_steps)\r\n\r\n  File, line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n\r\n  File, line 792, in fit\r\n    steps_name='steps_per_epoch')\r\n\r\n  File, line 144, in model_iteration\r\n    shuffle=shuffle)\r\n\r\n  File , line 477, in convert_to_generator_like\r\n    num_samples = int(nest.flatten(data)[0].shape[0])\r\n\r\nAttributeError: 'DatasetV1Adapter' object has no attribute 'shape'\r\n        ", "@shuklakirti, Could you provide the minimal code snippet to replicate the reported issue. Thanks!", "@shuklakirti. Provide the minimal code snippet to analyze the issue. Thanks", "Hey. Thanks, closing the issue."]}, {"number": 36076, "title": "Migration issue from tensorflow 1.10 to tensorflow 2.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 36075, "title": "Question about GLES delegate 3.1 spec support", "body": "Hello,\r\nNot sure if this is right issue category, but I am interested in learning details about the GLES delegate.\r\nIn particular, I would like to know what part of the 3.1 spec is needed for the delegate, as I am looking into creating a new delegate for a system with incomplete support for 3.1. It does support SSBO and compute shaders.\r\nThanks!\r\n", "comments": ["@boxerab, Please take look at the [document](https://www.tensorflow.org/lite/performance/gpu#android_with_android_studio) for openGl support. Thanks!", "@gadagashwini thanks. I am looking for specifics on what part of the 3.1 spec is needed. The document you linked to doesn't cover this, unfortunately. ", "Yeah, the major reason for OpenGL ES 3.1 was for compute shaders & SSBO.   I would just comment out the version check and see whether things work :P", "Thanks, @impjdi "]}, {"number": 36074, "title": "sklearn requires KerasRegressor/KerasClassifier to have _estimator_type set", "body": "Various sklearn functions validate _estimator_type. This in the respective constructors fixes that:\r\n\r\nKerasRegressor:\r\n`self._estimator_type = 'regressor'`\r\n\r\nKerasClassifier:\r\n`self._estimator_type = 'classifier'`\r\n", "comments": ["@oxqfsyef, \r\ncan you elaborate the issue with the code snippet and error log.  \r\nAlso provide the tensorflow version. Thanks!", "Tensorflow version is 1.14.0\r\n`from tensorflow.keras.wrappers.scikit_learn import KerasRegressor`\r\n`from sklearn.ensemble import VotingRegressor`\r\n`a = VotingRegressor([('x', KerasRegressor(lambda: None))])`\r\n`a.fit([[1,2]], [3])`\r\nTraceback (most recent call last):  \r\n  File \"<stdin>\", line 1, in <module>  \r\n  File \".../.conda/envs/tensorflow-gpu_1.14.0/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\", line 406, in fit  \r\n    return super().fit(X, y, sample_weight)  \r\n  File \".../.conda/envs/tensorflow-gpu_1.14.0/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\", line 57, in fit  \r\n    names, clfs = self._validate_estimators()  \r\n  File \".../.conda/envs/tensorflow-gpu_1.14.0/lib/python3.7/site-packages/sklearn/ensemble/_base.py\", line 251, in _validate_estimators  \r\n    est.__class__.__name__, is_estimator_type.__name__[3:]  \r\nValueError: The estimator KerasRegressor should be a regressor.  ", "Was able to reproduce the issue with Tf 1.14. and Tf 2.1 on colab.\r\nPlease find the gist [here](https://colab.research.google.com/gist/gadagashwini/1d556dddd9622aaa29c829bbf4b09ae0/untitled353.ipynb). Tha", "The `build_fn` in `tf.keras.wrappers.scikit_learn.KerasRegressor` should return a `keras model` which can be used on `fit/predict` method downstream.\r\nSee https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/wrappers/scikit_learn.py#L43", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36074\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36074\">No</a>\n", "@oxqfsyef Please check out this wrapper package, I think it should fix your issue: https://pypi.org/project/scikeras/\r\nhttps://github.com/adriangb/scikeras"]}, {"number": 36073, "title": "Subclassed tf.keras.Model always in training mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: P100 16GB\r\n\r\n**Describe the current behavior**\r\n\r\nAs per the [docs](https://github.com/tensorflow/docs/blob/r1.14/site/en/api_docs/python/tf/keras/models/Model.md)\r\n\r\n> If you subclass `Model`, you can optionally have a `training` argument (boolean) in `call`, which you can use to specify a different behavior in training and inference\r\n\r\nSo I expect the dropout in MyModel to affect the result when training.\r\n\r\n\r\nBut this script outputs \r\n```\r\nPredicting :\r\n[[1.]]\r\nTraining :\r\n1.0\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should output\r\n```\r\nPredicting :\r\n[[1.]]\r\nTraining :\r\n0.5\r\n```\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n  def __init__(self):\r\n\r\n    super(MyModel, self).__init__()\r\n\r\n    self.dense = tf.keras.layers.Dense(100, kernel_initializer=\"ones\", trainable=False)\r\n    self.dropout = tf.keras.layers.Dropout(0.5)\r\n\r\n\r\n  def call(self, inputs, training=False):\r\n\r\n    x = self.dense(inputs)\r\n\r\n    if training:\r\n      x = self.dropout(x, training=training)\r\n\r\n    x = tf.reshape(tf.reduce_sum(x)/100., [1, 1]) # If we dont reshape, we get RuntimeError: Attempted to aggregate unsupported object 1.0.\r\n    \r\n    return x\r\n\r\nmodel = MyModel()\r\n\r\ndef loss(y_true, y_pred):\r\n    return y_pred\r\n\r\nmodel.compile(optimizer=\"sgd\", loss=loss)\r\n\r\nx = np.ones((1, 1), dtype=np.float32)\r\n\r\nprint(\"Predicting :\")\r\nprint(model.predict(x)) # No dropout, output is 1 as expected\r\n\r\nprint(\"Training :\")\r\nprint(model.train_on_batch(x)) # dropout should put half of the activations of model.dense to 0, so I expect this value to be 0.5\r\n```", "comments": ["I have tried on colab with TF version 1.14, 1.15 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/26a02592151fb81a7fdf9b8625377a97/untitled578.ipynb). Thanks!", "The code actually runs correctly, and I think you have a misunderstanding of dropout.\r\n\r\nIf you change your model body to do:\r\n```\r\n  def call(self, inputs, training=False):\r\n    if training:\r\n      return x + 1\r\n    else:\r\n      return x + 2\r\n\r\nx = np.ones((1, 1), dtype=np.float32)\r\nmodel.predict(x)\r\nmodel.train_on_batch(x, x)\r\n```\r\nThe model will correctly output 3 for predict, and 2 for train.\r\n\r\nYour original expectation for dropout is not correct since dropout will scale up the unmasked value to maintain the numerical mean value. See https://stats.stackexchange.com/questions/205932/dropout-scaling-the-activation-versus-inverting-the-dropout for more details about the math.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36073\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36073\">No</a>\n", "I did not know that, thank you ! I think it would be nice to mention that in the docs though."]}, {"number": 36072, "title": "hangs on model.fit", "body": "Although I can't extract the code to reproduce the problem, I think that documenting it here will help improve this project and anyone who encounters the same problem.\r\n\r\nSystem: Windows 10\r\nVersion: tf-nightly-gpu 2020.1.19\r\n\r\nI use `tf.data.Dataset` to provide samples\r\nWhen I use `GPU + eager + batch_size > 16`, it will hang on `model.fit` and continue to occupy a core CPU\r\nWhen I use cpu or turn off eager mode or set batch <= 16, he will run normally.\r\n\r\nIt will continue like this\r\n\r\n> 2020-01-21 01:20:24.249995: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-21 01:20:28.403903: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-21 01:20:29.708097: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\nRelying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n   9/1406 [..............................] - ETA: 42:30 - loss: 30.3274 - accuracy: 0.0000e+00", "comments": ["@liuxingbaoyu \r\n\r\nRequest you to provide colab link or simple standalone code to reproduce the issue in our environment.It helps in localizing the issue faster. Thanks!", "> @liuxingbaoyu\r\n> \r\n> Request you to provide colab link or simple standalone code to reproduce the issue in our environment.It helps in localizing the issue faster. Thanks!\r\n\r\nThank you for your attention!\r\nI successfully extracted the example, and I found more clues!\r\nIt didn't cause problems with fashion_mnist 10 classes, but when I expanded to 2000 classes it went wrong.\r\nIn the case I described above, only `keras.optimizers.Adam (amsgrad = True)` will have this problem.\r\n\r\nToday I discovered another related issue.\r\n`gpu + no-eager + Adam` has a similar problem,\r\nAnd I found that it may be related to https://github.com/tensorflow/tensorflow/issues/35107 and https://github.com/tensorflow/tensorflow/issues/32477.\r\nWhen I follow the above solution (`keras.layers.BatchNormalization._USE_V2_BEHAVIOR = False`), it works fine.\r\n\r\ngpu + eager + amsgrad + batch=64: \u00d7\r\ngpu + eager + amsgrad + batch=16: \u221a\r\ngpu + no-eager + amsgrad + batch=64: \u221a\r\n\r\ngpu + no-eager + Adam + _USE_V2_BEHAVIOR = true: \u00d7\r\ngpu + no-eager + Adam + _USE_V2_BEHAVIOR = false: \u221a\r\n\r\nThe following code seems to only reproduce `amsgrad`.\r\n\r\n```2020-01-22 02:21:38.048632: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2.2.0-dev20200119 True True\r\n2020-01-22 02:21:41.843421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-01-22 02:21:41.880930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:41:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-01-22 02:21:41.881171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-22 02:21:41.886384: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-22 02:21:41.891444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-22 02:21:41.893798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-22 02:21:41.899172: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-22 02:21:41.903390: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-22 02:21:41.913975: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-22 02:21:41.914617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n(60000, 28, 28)   (60000,)\r\n(10000, 28, 28)   (10000,)\r\n(60000, 28, 28, 1)   (60000, 10)\r\n(10000, 28, 28, 1)   (10000, 10)\r\n2020-01-22 02:21:42.336968: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-01-22 02:21:42.338761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:41:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-01-22 02:21:42.338994: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-22 02:21:42.339104: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-22 02:21:42.339224: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-22 02:21:42.339345: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-22 02:21:42.339451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-22 02:21:42.339587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-22 02:21:42.339702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-22 02:21:42.340251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-01-22 02:21:43.053204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-22 02:21:43.053337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 \r\n2020-01-22 02:21:43.053408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N \r\n2020-01-22 02:21:43.054284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9526 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5)\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_2 (InputLayer)         [(None, 160, 160, 3)]     0         \r\n_________________________________________________________________\r\nmobilenetv2_1.00_160 (Model) (None, 5, 5, 1280)        2257984   \r\n_________________________________________________________________\r\nglobal_average_pooling2d (Gl (None, 1280)              0         \r\n_________________________________________________________________\r\nbatch_normalization (BatchNo (None, 1280)              5120      \r\n_________________________________________________________________\r\ndense (Dense)                (None, 2000)              2562000   \r\n_________________________________________________________________\r\ntf_op_layer_Softmax (TensorF [(None, 2000)]            0         \r\n=================================================================\r\nTotal params: 4,825,104\r\nTrainable params: 4,788,432\r\nNon-trainable params: 36,672\r\n_________________________________________________________________\r\nTrain for 10000 steps\r\nEpoch 1/5\r\n2020-01-22 02:21:57.958703: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-22 02:21:58.362411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-22 02:22:00.005238: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\nRelying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n    2/10000 [..............................] - ETA: 17:08:53 - loss: 7.9542 - accuracy: 0.0000e+00\r\n```\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\nprint(tf.version.VERSION, tf.executing_eagerly(), keras.layers.BatchNormalization._USE_V2_BEHAVIOR)\r\n\r\n# keras.layers.BatchNormalization._USE_V2_BEHAVIOR = False\r\n\r\n# tf.compat.v1.disable_eager_execution() # Uncomment this sentence works normally\r\n\r\n# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\r\ntf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)\r\n\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\r\nprint(x_train.shape, ' ', y_train.shape)\r\nprint(x_test.shape, ' ', y_test.shape)\r\n\r\nx_train = x_train.reshape((-1, 28, 28, 1))\r\nx_test = x_test.reshape((-1, 28, 28, 1))\r\n\r\ny_train = keras.utils.to_categorical(y_train, 10)\r\ny_test = keras.utils.to_categorical(y_test, 10)\r\n\r\nprint(x_train.shape, ' ', y_train.shape)\r\nprint(x_test.shape, ' ', y_test.shape)\r\n\r\ninput_shape = (160, 160, 3)\r\nclasses = 2000\r\n\r\ndef preprocess(x, y):\r\n    x = tf.image.resize(x, input_shape[:2])\r\n    if x.shape[2] == 1:\r\n        x = tf.image.grayscale_to_rgb(x)\r\n    x = x / 255\r\n\r\n    y = tf.one_hot(0, classes)\r\n\r\n    return (x, y), y\r\n\r\n\r\nbase_model = keras.applications.mobilenet_v2.MobileNetV2(include_top=False, input_shape=input_shape, classes=classes)\r\ninputs = tf.keras.layers.Input(shape=input_shape)\r\nx = base_model(inputs)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\r\nx = keras.layers.BatchNormalization()(x)\r\nx = keras.layers.Dense(classes)(x)\r\nx = keras.activations.softmax(x)\r\nmodel = tf.keras.models.Model(inputs=inputs, outputs=x)\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(amsgrad=True),loss=\"categorical_crossentropy\",metrics=['accuracy'])\r\n\r\nmodel.summary()\r\n\r\ndata_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(preprocess).batch(64).repeat()\r\n\r\nmodel.fit(data_train, steps_per_epoch=10000, epochs=5)\r\n```", "@liuxingbaoyu \r\n\r\nI have tried on colab with TF version 2.2.0-dev20200121 and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/8e3a23bef4ec032c7bd0bdc3e16cb3f0/untitled581.ipynb). Thanks!\r\n", "@ravikyram \r\nSorry, I don't know why, but it does happen on my computer, is there any good way? Thank you.", "@liuxingbaoyu It could be related to GPU memory. Can you share more details on GPU card? Can you try with smaller model and see whether the issue persists? Thanks!", "@jvishnuvardhan I am using RTX 2080TI.\r\n\r\nI updated the latest version of tf-nightly-gpu and the problem persists.\r\nWhen it runs, the GPU memory usage is as follows.\r\nAnd I guess the problem may be in the BN layer, when I comment out the BN layer, it works normally.\r\nThanks!\r\n\r\nnvcc -V\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Wed_Apr_24_19:11:20_Pacific_Daylight_Time_2019\r\nCuda compilation tools, release 10.1, V10.1.168\r\n```\r\nnvidia-smi.exe\r\n```Thu Jan 30 17:26:32 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 441.41       Driver Version: 441.41       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208... WDDM  | 00000000:41:00.0  On |                  N/A |\r\n|  0%   42C    P0    67W / 300W |   6741MiB / 11264MiB |      6%      Default |\r\n+-------------------------------+----------------------+----------------------+```", "@liuxingbaoyu May be there are some ops running in `cpu` and some Ops are running in `GPU`. You could check with any profiling tools to see which part of the code is taking more time and you can run them in graph model using @tf.function. For example, `def preprocess(x, y):` is taking more time then you can insert @tf.function above that function definition to run it in graph mode. You could run computationally expesive lines  with `with tf.device(\"/device:GPU:0\"):` so that they run on GPU.\r\n\r\n```\r\nwith tf.device(\"/device:GPU:0\"):\r\n   model.fit(data_train, steps_per_epoch=10000, epochs=5)\r\n```\r\n\r\nHope this helps. Thanks!\r\n\r\n", "@jvishnuvardhan \r\nI tried `@tf.function` + with `tf.device (\"/ device: GPU: 0\"):` still doesn't work.\r\nWhen I run it, it takes 1 ~ 30 steps, then starts to freeze, and then only uses the cpu, not the gpu.\r\nAnd I found that it is not occupying one CPU core, but it is occupying two CPU cores.\r\nI think it might endlessly loop somewhere, but I can't find where.\r\nThank you!", "@jvishnuvardhan Hello, are there any updates? Thank you!", "@liuxingbaoyu Can you please provide system information? I am not sure about the root-cause. Can you please check with other models and data. Some time when there is missing data, TF tries to look for the data. Please check with other datasets also. Thanks!\r\n```\r\nSystem information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n```", "@jvishnuvardhan \r\n```\r\nSystem information\r\nOS Platform and Distribution: WIN10\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version: 2.2.0-dev20200121\r\nPython version: Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)] on win32\r\nInstalled using virtualenv? pip? conda?: pip\r\nCUDA/cuDNN version: Cuda compilation tools, release 10.1, V10.1.168 / cudnn 7.6.0\r\nGPU model and memory: RTX 2080TI 11G\r\n```\r\n\r\n\r\n\r\n> @liuxingbaoyu\r\n> \r\n> I have tried on colab with TF version 2.2.0-dev20200121 and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/8e3a23bef4ec032c7bd0bdc3e16cb3f0/untitled581.ipynb). Thanks!\r\n\r\nIt is normal on colab.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36072#issuecomment-576813991\r\nAnd modifying these settings will work.\r\n\r\n\r\nThanks!", "Will still hang on the latest nightly and `tf-gpu 2.1 cudn 10.2`", "@liuxingbaoyu Can you please follow the suggestion [here](https://github.com/tensorflow/tensorflow/issues/35107#issuecomment-592120998) and let us know whether it resolved for you or not. Thanks!", "@liuxingbaoyu \r\nI get the solution for my problem at this https://github.com/tensorflow/tensorflow/issues/37216#issue-573656418  \r\n\r\n for this issue maybe it helps, found that my **GPU** and **SSD** get too hot (something around 80), so decide to change the system from `AirCoo`l to `WaterCoo`l and changing the `thermal paste`  \r\n\r\n previously on that issue, I report changing OS on the same hardware helps, but to add another SSD drive I moved my PC to another place which was much cooler than where it was  \r\n\r\nI recommend that using any tools you are comfortable with  to check system component temperature", "@jvishnuvardhan \r\nThank you for your attention! But this doesn't seem to solve my problem. I think this should be a bug, 0% gpu usage, and the endless loop occupies two cpu cores.", "@pykeras \r\nThank you very much!", "2.2.0-dev20200314\r\n\r\n_pywrap_tensorflow_internal.pyd\r\nbaseaddr \uff1a00007FFC392D0000\r\n\r\nnvcuda.dll\r\nversion\uff1a26.21.14.4141\r\nMD5     \uff1a86A2011161FC43EF70E374DD9B6F90D7\r\nSHA1    \uff1a13F1603528F2AA4A8D9ECDC3D3AA4D9C09DDDE88\r\nbaseaddr \uff1a00007FFC27500000\r\n\r\n```\r\n\u5730\u5740               \u8fd4\u56de\u5230              \u8fd4\u56de\u81ea              \u5927\u5c0f  \u6ce8\u91ca                                           \u65b9   \r\n0000007105CFF1E8 00007FFC27848836 00007FFCEBB94BE4 B0  win32u.NtGdiDdDDIEscape+14                   \u7cfb\u7edf\u6a21\u5757\r\n0000007105CFF298 00007FFC2754BA9E 00007FFC2784890C 30  nvcuda.00007FFC2784890C                      \u7cfb\u7edf\u6a21\u5757\r\n0000007105CFF2C8 00007FFC2760C28F 00007FFC2754BA9E 40  nvcuda.sub_7FFC2754BA60+3E                   \u7cfb\u7edf\u6a21\u5757\r\n0000007105CFF308 00007FFC277333B2 00007FFC2760C22D 60  nvcuda.00007FFC2760C22D                      \u7cfb\u7edf\u6a21\u5757\r\n0000007105CFF368 00007FFC27516D0A 00007FFC277333B2 40  nvcuda.sub_7FFC27733190+222                  \u7cfb\u7edf\u6a21\u5757\r\n0000007105CFF3A8 00007FFC276ABD19 00007FFC27516D0A D0  nvcuda.00007FFC27516D0A                      \u7cfb\u7edf\u6a21\u5757\r\n0000007105CFF478 00007FFC3FD74303 00007FFC276ABD19 E0  nvcuda.sub_7FFC276ABB50+1C9                  \u7528\u6237\u6a21\u5757\r\n0000007105CFF558 00007FFC3FD6A4DF 00007FFC3FD744A3 150 _pywrap_tensorflow_internal.00007FFC3FD744A3 \u7528\u6237\u6a21\u5757\r\n0000007105CFF6A8 00007FFC3FCF16C3 00007FFC3FD6A4DF 160 _pywrap_tensorflow_internal.00007FFC3FD6A4DF \u7528\u6237\u6a21\u5757\r\n0000007105CFF808 00007FFC3FCF192A 00007FFC3FCF16C3 1D0 _pywrap_tensorflow_internal.00007FFC3FCF16C3 \u7528\u6237\u6a21\u5757\r\n0000007105CFF9D8 00007FFC40007B69 00007FFC3FCF192A D0  _pywrap_tensorflow_internal.00007FFC3FCF192A \u7528\u6237\u6a21\u5757\r\n0000007105CFFAA8 00007FFC40008001 00007FFC40007B69 30  _pywrap_tensorflow_internal.00007FFC40007B69 \u7528\u6237\u6a21\u5757\r\n0000007105CFFAD8 00007FFC40016AB8 00007FFC40008001 30  _pywrap_tensorflow_internal.00007FFC40008001 \u7528\u6237\u6a21\u5757\r\n0000007105CFFB08 00007FFCEBE51542 00007FFC40016AB8 30  _pywrap_tensorflow_internal.00007FFC40016AB8 \u7cfb\u7edf\u6a21\u5757\r\n0000007105CFFB38 00007FFCED796FD4 00007FFCEBE51542 30  ucrtbase.00007FFCEBE51542                    \u7cfb\u7edf\u6a21\u5757\r\n0000007105CFFB68 00007FFCEE49CF31 00007FFCED796FD4 80  kernel32.00007FFCED796FD4                    \u7cfb\u7edf\u6a21\u5757\r\n0000007105CFFBE8 0000000000000000 00007FFCEE49CF31     ntdll.00007FFCEE49CF31                       \u7528\u6237\u6a21\u5757\r\n\r\n```\r\n```\r\n\u5730\u5740               \u8fd4\u56de\u5230              \u8fd4\u56de\u81ea              \u5927\u5c0f \u6ce8\u91ca                          \u65b9   \r\n000000710553F728 00007FFCEC105A84 00007FFCEE4EB694 30 ntdll.00007FFCEE4EB694      \u7cfb\u7edf\u6a21\u5757\r\n000000710553F758 00007FFC27857CF6 00007FFCEC105A84 50 kernelbase.00007FFCEC105A84 \u7cfb\u7edf\u6a21\u5757\r\n000000710553F7A8 00007FFC27678D63 00007FFC27857CF6 30 nvcuda.00007FFC27857CF6     \u7cfb\u7edf\u6a21\u5757\r\n000000710553F7D8 00007FFC2793F6C0 00007FFC27678D63 30 nvcuda.00007FFC27678D63     \u7cfb\u7edf\u6a21\u5757\r\n000000710553F808 00007FFCED796FD4 00007FFC2793F6C0 30 nvcuda.00007FFC2793F6C0     \u7cfb\u7edf\u6a21\u5757\r\n000000710553F838 00007FFCEE49CF31 00007FFCED796FD4 80 kernel32.00007FFCED796FD4   \u7cfb\u7edf\u6a21\u5757\r\n000000710553F8B8 0000000000000000 00007FFCEE49CF31    ntdll.00007FFCEE49CF31      \u7528\u6237\u6a21\u5757\r\n```\r\n\r\nTwo threads are stuck in an endless loop, because I have no symbols, so I can only provide offsets.", "I have the same problem", "@liuxingbaoyu \r\nI got the same message. I think the problem is because your CPU is processing your data every batch (or every epoch) before sending them to the model, where GPU trains the model. The thing is your CPU is processing the data much much slower than GPU training your model so most of your training time is actually spent on processing the data on CPU. \r\nI was able to solve the problem (the warning message disappeared) by preprocessing the data and store them before the training begins. When starting to train the model, CPU only needs to read from the stored data and the rest is handled by GPU. \r\n", "@Jiaming-Jin This should be a bug, hardware or software.", "After I repaired the graphics card, the problem seemed to disappear. I don't know what the cause is. People who encounter the same problem can also try it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36072\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36072\">No</a>\n", "Just listing the steps that helped me fix this issue.\r\n\r\nI have faced the same problem while using TF 2.1 on a windows laptop. Same code works on a kaggle kernel and colab but hangs on my laptop. So I have followed below steps to isolate the issue.\r\n\r\n1. Upgrade to tensorflow 2.2 - Didn't fix\r\n2. create a new virtual environment and install all necessary libraries - Didn't fix\r\n\r\nAfter above two steps, I believe it has to do with my GPU drivers or CUDA version. It's annoying that there was no error message or anything. Model.fit does not progress except for some CPU usage by the python process.\r\n\r\n3.  So I have updated my CUDA Tool kit, cuDNN and my graphics card driver (just to be on the safe side) - Problem fixed. Seems in line with @liuxingbaoyu  's experience above.", "> Just listing the steps that helped me fix this issue.\r\n> \r\n> I have faced the same problem while using TF 2.1 on a windows laptop. Same code works on a kaggle kernel and colab but hangs on my laptop. So I have followed below steps to isolate the issue.\r\n> \r\n> 1. Upgrade to tensorflow 2.2 - Didn't fix\r\n> 2. create a new virtual environment and install all necessary libraries - Didn't fix\r\n> \r\n> After above two steps, I believe it has to do with my GPU drivers or CUDA version. It's annoying that there was no error message or anything. Model.fit does not progress except for some CPU usage by the python process.\r\n> \r\n> 1. So I have updated my CUDA Tool kit, cuDNN and my graphics card driver (just to be on the safe side) - Problem fixed. Seems in line with @liuxingbaoyu  's experience above.\r\n\r\nHi, my model training also comes with this problem, could you please provide your detailed version of tensorflow, cuda and cudnn? Thanks a lot.", "Hey @KaimingZhu \r\n\r\nI have updated to the latest available versions - tensorflow to 2.2 stable and GPU specific libraries according to \r\nhttps://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html\r\n\r\ncuDNN - 9 , CUDA - 11.0.2 and Graphics driver 451.67 for windows 10.  \r\n\r\nA word of caution, getting tensorflow working was my first priority, so I didn't hesitate with the upgrades. Please assess based on your priorities. I remember having troubles updating to the latest cuDNN when working with openCV and Dlib.", "> Hey @KaimingZhu\r\n> \r\n> I have updated to the latest available versions - tensorflow to 2.2 stable and GPU specific libraries according to\r\n> https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html\r\n> \r\n> cuDNN - 9 , CUDA - 11.0.2 and Graphics driver 451.67 for windows 10.\r\n> \r\n> A word of caution, getting tensorflow working was my first priority, so I didn't hesitate with the upgrades. Please assess based on your priorities. I remember having troubles updating to the latest cuDNN when working with openCV and Dlib.\r\n\r\nThis CUDA and CUDNN version with tensorflow 2.2 works for me, thanks for your kindness!", "Hi @AswaniArisetty \r\n\r\nPlease can you confirm that it was cuDNN 9 you are using? I can't seem to find it.\r\n\r\nThank you :)", "Hey @kateob  sorry, typo. Please try cuDNN v7.6 or may be even the latest ones.", "Cool, I think that should work. Thanks again @AswaniArisetty.", "Cool, I solved it by updating graphics card driver. Thanks!", "I also solved this issue by only updating my nvidia graphic card driver from version 431.70 to version 451.48.\r\n[This new driver version is compatible with CUDA 10 and 11](https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility)."]}, {"number": 36071, "title": "Can't set None on TextVectorization layer's split parameter.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `no`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 18.04`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `no`\r\n- TensorFlow installed from (source or binary): `no`\r\n- TensorFlow version (use command below): `2.0.0`\r\n- Python version: `3.7.3`\r\n- Bazel version (if compiling from source): `-`\r\n- GCC/Compiler version (if compiling from source): `-`\r\n- CUDA/cuDNN version: `-`\r\n- GPU model and memory: `-`\r\n\r\n**Describe the current behavior**\r\nThe TextVectorization layer `split` parameter expects `None` as a possible value but can't handle it.\r\n\r\n**Describe the expected behavior**\r\nThe layer should work properly when `None` is passed as a `split` parameter, or documentation should be updated.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\nfrom pprint import pprint\r\nassert tf.__version__ == '2.1.0-rc1'\r\n\r\ndummy_data = [\"Foo\", \"bar\", \"foo foo\", \"foo bar\", \"foobar.\"]\r\npredict_data = [\"foo\", \"bar\", \"foobar\", \"foo foo\", \"OOV\"]\r\ninputs = tf.keras.layers.Input(shape=(1, ), dtype=tf.string, name=\"text\")\r\nvectorize_layer = TextVectorization(output_mode=\"binary\", max_tokens=5, split=None)\r\nvectorize_layer.adapt(np.asarray(dummy_data))\r\nprint(f\"Vocabulary:\\t\\t{vectorize_layer.get_vocabulary()}\")\r\noutputs = vectorize_layer(inputs)\r\nmodel = tf.keras.Model(inputs, outputs)\r\nprint(f\"Prediction data:\\t{predict_data}\")\r\npredictions = model.predict(predict_data)\r\nprint(f\"Predictions:\")\r\npprint(predictions)\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-f1a03cb1e414> in <module>()\r\n      9 inputs = tf.keras.layers.Input(shape=(1, ), dtype=tf.string, name=\"text\")\r\n     10 vectorize_layer = TextVectorization(output_mode=\"binary\", max_tokens=5, split=None)\r\n---> 11 vectorize_layer.adapt(np.asarray(dummy_data))\r\n     12 print(f\"Vocabulary:\\t\\t{vectorize_layer.get_vocabulary()}\")\r\n     13 outputs = vectorize_layer(inputs)\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/layers/preprocessing/text_vectorization.py in _to_numpy(self, preprocessed_data)\r\n    334     if isinstance(preprocessed_data, np.ndarray):\r\n    335       return preprocessed_data\r\n    336     return np.array(preprocessed_data.to_list())\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'to_list'\r\n```", "comments": ["I have tried on colab with TF version 2.1.0-rc2, 2.2.0-dev20200121  and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/c006c43d35583da0a38f75fce1062854/untitled577.ipynb). Thanks!", "HI @stefanondisponibile and @ravikyram: we have fixed this at HEAD internally (we changed how the layer performs adapt() by deferring to internal sublayers) and I have verified your code snippet functions as expected. I believe these changes should be in the nightly, if not now then very shortly.", "@stefanondisponibile This was resolved in recent `tf-nightly`. PTAL at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/444f44c8ce3d736ae470d994626ce1e5/untitled577.ipynb).\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "Thank you @jvishnuvardhan, looks good to me!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36071\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36071\">No</a>\n"]}, {"number": 36070, "title": "Add 16 bit activations support to kernel operator STRIDED_SLICE", "body": "This PR is one of steps to extend TFLite to support symmetric 16-bit activations.\r\n\r\nIn this PR we enable implementation and test for STRIDED_SLICE kernel function.", "comments": ["Hi @suharshs @jdduke\r\nCould you please review this?\r\nAs discussed, this is one of the INT16 reference kernel reference function.\r\nThanks!"]}, {"number": 36069, "title": "setting allow_nudging_weights_to_use_fast_gemm_kernel in the python API does not work", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, 18.04 Ubuntu\r\n- TensorFlow installed from (source or binary): pip3\r\n- TensorFlow version (or github SHA if from source): 1.x\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\n...\r\nfrom tensorflow.lite.toco import toco_flags_pb2 as toco_flags\r\ntoco = toco_flags.TocoFlags()\r\ntoco.allow_nudging_weights_to_use_fast_gemm_kernel = True\r\nconverter.convert()\r\n...\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nBad value for conv/weights at index 187, previous bad value at index 175, distance=12, kMinDistanceBetweenBadValues=16. Consider passing --allow_nudging_weights_to_use_fast_gemm_kernel if you don't care about accuracy.\r\n```\r\nIn particular\r\n```Consider passing --allow_nudging_weights_to_use_fast_gemm_kernel if you don't care about accuracy.```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nN/A - see below\r\n```\r\n\r\n**Failure details**\r\n- expected behaviour - the erroring code continues past this step and either fails elsewhere, or continues to pass. It should not suggest the same fix.\r\n\r\n**Any other info / logs**\r\n\r\nI am restricted in the details I can post of my application.\r\nIt looks like the source does not have a check in there, so probably still in 2.x", "comments": ["@ianfhunter Can you please provide simple standalone code for faster resolution? You could use any public data. Thanks!", "@ianfhunter Can you please provide simple standalone code for faster resolution? You could use any public data. Thanks!", "I'll try to do so in the coming week", "@ianfhunter Are you still working on providing simple standalone code? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36069\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36069\">No</a>\n"]}]