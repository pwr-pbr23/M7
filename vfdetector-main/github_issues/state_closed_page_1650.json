[{"number": 3407, "title": "//util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'", "body": "I am installing TensorFlow on **Unbutu 12.04** with source code. I can build `bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu` successfully, but where i create pip package, i get the warnning:\n\nWARNING: /root/.cache/bazel/_bazel_root/f23957347b508b087abaf4db43031df3/external/protobuf/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/f23957347b508b087abaf4db43031df3/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\nWARNING: /root/.cache/bazel/_bazel_root/f23957347b508b087abaf4db43031df3/external/re2/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/f23957347b508b087abaf4db43031df3/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\nWARNING: /root/.cache/bazel/_bazel_root/f23957347b508b087abaf4db43031df3/external/highwayhash/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/f23957347b508b087abaf4db43031df3/external/highwayhash/WORKSPACE (@__main__) does not match the name given in the repository's definition (@highwayhash); this will cause a build error in future versions.\nWARNING: /data1/web/soft/tensorflow/tensorflow/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\n\nI build failed with error:\n\n/tensorflow/tensorflow/tensorflow/python/BUILD:87:1: C++ compilation of rule '//tensorflow/python:py_func_lib' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 121 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\ntensorflow/python/lib/core/py_func.cc: In function 'tensorflow::Status tensorflow::{anonymous}::ConvertNdarrayToTensor(PyObject_, tensorflow::Tensor_)':\ntensorflow/python/lib/core/py_func.cc:164:37: error: 'PyArray_SHAPE' was not declared in this scope\n     shape.AddDim(PyArray_SHAPE(input)[i]);\n                                     ^\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n\nI have run **./configure** like this:\n\nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc nvcc should use as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc-4.9\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.0\nPlease specify the location where CUDA 7.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 7.0\nPlease specify the location where cuDNN 7.0 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: \nSetting up Cuda include\nSetting up Cuda lib64\nSetting up Cuda bin\nSetting up Cuda nvvm\nSetting up CUPTI include\nSetting up CUPTI lib64\nConfiguration finished\n\nI'm not familiar with bazel, is there any one who can help?\n", "comments": ["I update numpy to 1.11, then the error disappear.\n"]}, {"number": 3406, "title": "How to debug tensorflow swig shared library", "body": "I tired PyDev + CDT, and GDB, so Python file is easy to debugging . But swig generations *.so file can't debug into *.cc file side . \n\nCould anybody tell me HOW TO debug swig?\n", "comments": ["This is best suited for StackOverflow, as it is not a bug nor feature request.\n"]}, {"number": 3405, "title": "Fix minor typo", "body": "Fix indentation in tensorflow/workspace.bzl\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 3404, "title": "Can't create placeholder with partially defined shape e.g. (-1, 10)", "body": "I get `ValueError: Dimension -1 must be >= 0`\n", "comments": []}, {"number": 3403, "title": "A link in tensorflow's website is ineffective", "body": "The link in **Download [the tutorial code](https://github.com/tensorflow/tensorflow/tree/r0.9/tensorflow/examples/learn/wide_n_deep_tutorial.py).**  from [TensorFlow Linear Model Tutorial](https://www.tensorflow.org/versions/master/tutorials/wide/index.html) failed.\n\nIs the source code of the website not open? I didn't find it. \n", "comments": ["https://github.com/tensorflow/tensorflow/tree/master/tensorflow/g3doc/tutorials/wide Feel free to send a PR to update the docs for us!\n", "This fixed by a recent commit ac482df8f3522656d9bab96f665405aa2ba5f98b, and the official site doesn't update now. \n\n``` shell\ncommit ac482df8f3522656d9bab96f665405aa2ba5f98b\nAuthor: A. Unique TensorFlower <gardener@tensorflow.org>\nDate:   Thu Jul 7 15:07:26 2016 -0800\n\n    Points links to the wide_n_deep.py example to master, and fixes a heading problem in the tf.contrib.learn quickstart.\n    Change: 126857823\n\ndiff --git a/tensorflow/g3doc/tutorials/wide_and_deep/index.md b/tensorflow/g3doc/tutorials/wide_and_deep/index.md\nindex 910e91e..d238b72 100644\n--- a/tensorflow/g3doc/tutorials/wide_and_deep/index.md\n+++ b/tensorflow/g3doc/tutorials/wide_and_deep/index.md\n@@ -42,7 +42,7 @@ To try the code for this tutorial:\n already.\n\n 2.  Download [the tutorial code](\n-https://www.tensorflow.org/code/tensorflow/examples/learn/wide_n_deep_tutorial.py).\n+https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py).\n\n 3.  Install the pandas data analysis library. tf.learn doesn't require pandas, but it does support it, and this tutorial u\n     1. Get `pip`:\n\n```\n", "@vrv The g3doc needs to be regenerated. I saw some other places with this issue as well. \n"]}, {"number": 3402, "title": "Given one input tensor, why tf.nn.max_pool generate two tensors?", "body": "  with tf.name_scope('layer3'):\n    conv3_weights = tf.Variable(tf.truncated_normal(\n      [3, 3, 128, 256], stddev=0.1))\n    variable_summaries(conv3_weights, 'layer3' + '/weights')\n    conv3_biases = tf.Variable(tf.constant(0.1, shape=[256]))\n    variable_summaries(conv3_biases, 'layer3' + '/biases')\n    conv3 = tf.nn.conv2d(pool2,\n      conv3_weights,\n      strides=[1, 1, 1, 1],\n      padding='SAME')\n    tf.histogram_summary('layer3'+'/pre_activations', conv3)\n    relu3 = tf.nn.relu(tf.nn.bias_add(conv3, conv3_biases))\n    tf.histogram_summary('layer3'+'/activations', relu3)\n    print relu3.get_shape()\n    pool3 = tf.nn.max_pool(relu3,\n      ksize=[1, 2, 2, 1],\n      strides=[1, 2, 2, 1],\n      padding='SAME')\n    tf.histogram_summary('layer3'+'/pool', pool3)\n    print pool3.get_shape()\n\nWhen using TensorBoard to visualize the graph, I find the output of the layer3 are two tensors. I am puzzled why does it happen? Thank you very much! \n![1](https://cloud.githubusercontent.com/assets/20062851/16974093/b96aee78-4e6b-11e6-97c5-c266b4dc28d8.jpg)\n", "comments": ["Can you click on `MaxPool` and see the `Outputs`?\n\nI suspect the extra tensor might just be CheckNumerics or a control dependency?\n", "OK, the screenshot is as follow:\n![2](https://cloud.githubusercontent.com/assets/20062851/16979506/fb42e134-4e93-11e6-9b40-3062fc421f27.jpg)\n", "Got it! I implement unpooling function in the layer4.\n\n``````\ndef UnPooling2x2ZeroFilled(x):\n  out = tf.concat(3, [x, tf.zeros_like(x)])\n  out = tf.concat(2, [out, tf.zeros_like(out)])\n\n  sh = x.get_shape().as_list()\n  if None not in sh[1:]:\n    out_size = [-1, sh[1] * 2, sh[2] * 2, sh[3]]\n    return tf.reshape(out, out_size)\n  else:\n    sh = tf.shape(x)\n    ret = tf.reshape(out, tf.pack([-1, sh[1] * 2, sh[2] * 2, sh[3]]))\n    ret.set_shape([None, None, None, sh[3]])\n    return ret\n```def UnPooling2x2ZeroFilled(x):\n  out = tf.concat(3, [x, tf.zeros_like(x)])\n  out = tf.concat(2, [out, tf.zeros_like(out)])\n\n  sh = x.get_shape().as_list()\n  if None not in sh[1:]:\n    out_size = [-1, sh[1] * 2, sh[2] * 2, sh[3]]\n    return tf.reshape(out, out_size)\n  else:\n    sh = tf.shape(x)\n    ret = tf.reshape(out, tf.pack([-1, sh[1] * 2, sh[2] * 2, sh[3]]))\n    ret.set_shape([None, None, None, sh[3]])\n    return ret\n\nwhere x and tf.zeros_like(x) are two tensors, is right? In addition, when the gradient is computed , the error raises as follow:\n\n![3](https://cloud.githubusercontent.com/assets/20062851/16981952/d60e861e-4e9f-11e6-9400-e830989b885c.jpg)\n\nCould you tell me what's wrong? Thank you very much!\n``````\n", "![3](https://cloud.githubusercontent.com/assets/20062851/16981976/ecfd6d36-4e9f-11e6-904c-05e804864f67.jpg)\n", "This does not sound like a bug ticket.  Please redirect to StackOverflow or tensorflow-discuss@.\n"]}, {"number": 3401, "title": "Run a TensorFlow demo model : IOError: CRC check failed", "body": "### Environment info\n\nOperating System:\n(tensorflow) paddlescoot@paddlescoot-Satellite-P870:~$ cat /proc/version\nLinux version 4.4.0-31-generic (buildd@lgw01-16) (gcc version 5.3.1 20160413 (Ubuntu 5.3.1-14ubuntu2.1) ) #50-Ubuntu SMP Wed Jul 13 00:07:12 UTC 2016\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   Follow python 2.7 virtualenv installtion notes - https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#virtualenv-installation\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.9.0\n### Steps to reproduce\n1. Follow Test notes here - https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#test-the-tensorflow-installation\n2. When you get to the last step, the following error appears : \n   (tensorflow) paddlescoot@paddlescoot-Satellite-P870:~$ python /home/paddlescoot/tensorflow/local/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\n   Extracting data/train-images-idx3-ubyte.gz\n   Traceback (most recent call last):\n   File \"/home/paddlescoot/tensorflow/local/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\", line 316, in <module>\n     tf.app.run()\n   File \"/home/paddlescoot/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n     sys.exit(main(sys.argv))\n   File \"/home/paddlescoot/tensorflow/local/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\", line 128, in main\n     train_data = extract_data(train_data_filename, 60000)\n   File \"/home/paddlescoot/tensorflow/local/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\", line 75, in extract_data\n     buf = bytestream.read(IMAGE_SIZE \\* IMAGE_SIZE \\* num_images)\n   File \"/usr/lib/python2.7/gzip.py\", line 268, in read\n     self._read(readsize)\n   File \"/usr/lib/python2.7/gzip.py\", line 315, in _read\n     self._read_eof()\n   File \"/usr/lib/python2.7/gzip.py\", line 354, in _read_eof\n     hex(self.crc)))\n   IOError: CRC check failed 0xb3e5d2f6 != 0x255cc2beL\n### What have you tried?\n1. Searching similar issues.\n", "comments": ["I cannot reproduce, and this sounds like the download of the MNIST data zip files failed.\n\nCould you try to manually download the MNIST zip files, and replace the corrupted versions with the fresh ones?  \n", "Thanks @concretevitamin . Yes something happened to the file on original download.\n\nWas ...\n9609216 train-images-idx3-ubyte.gz\n\nShould be ...\n9912422 train-images-idx3-ubyte.gz\n\nManually downloaded, re-ran convolutional.py and it worked.\n", "I meet the same problem when I run /models/tutorials/image/imagenet/classify_image.py. \r\nI delete /tmp/imagenet/inception-2015-12-05.tgz  and re-download the model. The problem was solved.", "I met the same problem deleting the data dir in the same folder as the script worked for me ", "any solution for this?"]}, {"number": 3400, "title": "new stacks", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["I want to start\n"]}, {"number": 3399, "title": "Fix typo in rnn_cell.py", "body": "Fix typo.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks!\n"]}, {"number": 3398, "title": "Added debugging information for Pi errors", "body": "", "comments": []}, {"number": 3397, "title": "Can't place tf.nn.moments() on GPU when first dimension is None", "body": "At first this sounds like #2508, but this was closed and apparently fixed and the error message is also different. So it might be more related to #139, and possibly to the comment of @mrry on `tf.mean` on GPUs. The example below works with cpu device context (or no explicit device placement at all). Is this a missing feature or should one not expect that this is fixable?\n### Environment info\n\nOperating System:\nUbuntu 15.10\n\nInstalled version of CUDA and cuDNN: \nCUDA7.5/cuDNN4.0.4\n\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n-rw-r--r-- 1 root root 189170 Jan  1 23:25 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jan  1 23:25 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jan  1 23:25 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Jan  1 23:25 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Jan  1 23:25 /usr/local/cuda/lib/libcudart_static.a\n```\n\nIf installed from sources, provide the commit hash: [fc91629](https://github.com/tensorflow/tensorflow/commit/fc9162975e52978d3af38549b570cc3cc5f0ab66)\n### Steps to reproduce\n\n```\nimport tensorflow as tf\nimport numpy as np\n\nwith tf.device(\"/gpu:0\"):\n    x = tf.placeholder(tf.float32, shape=[None, 5])\n    mean, var = tf.nn.moments(x, axes=[0])\nsess = tf.Session()\nsess.run([mean, var], feed_dict={x:np.random.randn(10, 5)})\n```\n\nresults in\n\n```\nE tensorflow/core/client/tensor_c_api.cc:485] Cannot assign a device to node 'moments/sufficient_statistics/SparseToDense': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n     [[Node: moments/sufficient_statistics/SparseToDense = SparseToDense[T=DT_BOOL, Tindices=DT_INT32, validate_indices=true, _device=\"/device:GPU:0\"](moments/sufficient_statistics/SparseToDense/sparse_indices, moments/sufficient_statistics/Shape_1, moments/sufficient_statistics/SparseToDense/sparse_values, moments/sufficient_statistics/SparseToDense/default_value)]]\n```\n### What have you tried?\n1. Setting `shape=[10, 5]` fixes the above problem (to be expected).\n2. Setting device context to `with tf.device(\"/cpu:0\"):` works with `None` shape info, too.\n", "comments": ["The call stack is as such: `tf.nn.moments()` calls `sufficient_statistics()`, which contains [this part](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn.py#L695):\n\n``` python\n    if x_shape.is_fully_defined():\n        <some code>\n    else:  # shape needs to be inferred at runtime.\n      x_shape = array_ops.shape(x)\n      select_axes = sparse_ops.sparse_to_dense(axes, array_ops.shape(x_shape),\n                                               True, False)\n```\n\nSince the shape contains `None` the else branch is taken.  Lastly, the `SparseToDense` op only has a CPU kernel, not a GPU one.  Hence the failure.\n\nPossible fixes:\n- Somehow change the `tf.nn.moments()` code path, so that `sparse_to_dense()` is avoided.\n- Or, implement a SparseToDense GPU kernel.\n\nMarking as contribution welcome, since we're not likely to work on this in the near future.\n", "/cc @vincentvanhoucke \n", "Yes, it's a known wrinkle, that would only be properly solved with a dedicated op or a GPU implementation of `sparse_to_dense`. In practice, there should be no good reason to want to enforce a hard placement of all the ops on GPU though. With soft placement, you may let the graph mostly run on GPU and back off to CPU only for this one op without a GPU equivalent.\n", "Thank you for the replies. Out of curiosity, would the performance hit of going down to cpu and up again be negligible? The `tf.nn.moments` is used in a BatchNorm layer in this case, so with ResNets, many of this transfers would happen. Or is using `None` in the batchsize itself having already such a negative effect (e.g. because some optimizations can't be realized offline) that this copying does not add that much overhead?\n", "This op operates on four floats. There should be no impact at all.\n", "@osdf: for now, I think a workaround is to use\n\n```\n    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n```\n\nwhich should attempt to place non-GPU-available ops on CPU instead.\n", "Thank you @concretevitamin and @vincentvanhoucke. Do you think this issue should be closed? If it turns out that somebody needs this to be placed on GPU it could be reopened. (I briefly thought of taking a stab at it but a bit time-limited right now). Apart from that it serves as a reference if anybody else encounters it.\n", "Please leave it open. It's likely others will hit the same problem.\n", "Pending https://github.com/tensorflow/tensorflow/pull/6031 should fully address this.", "I think the issue is unresolved. I thought it would be better to post here, continuing the discussion.\r\nTake this toy example:\r\n\r\n```python\r\nwith tf.device('/gpu:2'):\r\n    x = tf.placeholder(tf.float32, shape=[None, 100])\r\n    weight_dense_1 = tf.Variable(tf.zeros([100, 10]))\r\n    dense_1_out = tf.matmul(x, weight_dense_1)\r\n    mean, var = tf.nn.moments(dense_1_out, [0], shift=None, name=None, keep_dims=False)\r\n    coeff = tf.rsqrt(var + 1e-8)\r\n    y = dense_1_out * coeff - mean * coeff\r\n    grad = tf.gradients(y, [weight_dense_1], colocate_gradients_with_ops=True)\r\n```\r\n\r\nThis immediately shows up some warnings:\r\n```\r\nWARNING:tensorflow:Tried to colocate gradients/moments/sufficient_statistics/count_grad/Rank with an op moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\r\nWARNING:tensorflow:Tried to colocate gradients/moments/sufficient_statistics/count_grad/range_1/start with an op moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\r\nWARNING:tensorflow:Tried to colocate gradients/moments/sufficient_statistics/count_grad/range_1/delta with an op moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\r\nWARNING:tensorflow:Tried to colocate gradients/moments/sufficient_statistics/count_grad/range_1 with an op moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\r\nWARNING:tensorflow:Tried to colocate gradients/moments/sufficient_statistics/count_grad/ListDiff with an op moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\r\nWARNING:tensorflow:Tried to colocate gradients/moments/sufficient_statistics/count_grad/concat/axis with an op moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\r\nWARNING:tensorflow:Tried to colocate gradients/moments/sufficient_statistics/count_grad/concat with an op moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\r\nWARNING:tensorflow:Tried to colocate gradients/moments/sufficient_statistics/count_grad/Gather with an op moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\r\nWARNING:tensorflow:Tried to colocate gradients/moments/sufficient_statistics/count_grad/Const with an op moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\r\nWARNING:tensorflow:Tried to colocate gradients/moments/sufficient_statistics/count_grad/Prod with an op moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\r\nWARNING:tensorflow:Tried to colocate gradients/moments/sufficient_statistics/count_grad/Gather_1 with an op moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\r\nWARNING:tensorflow:Tried to colocate gradients/moments/sufficient_statistics/count_grad/Const_1 with an op moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\r\nWARNING:tensorflow:Tried to colocate gradients/moments/sufficient_statistics/count_grad/Prod_1 with an op moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\r\n```\r\n\r\nAnd if I force it to run(by setting `allow_soft_placement=False`), an error is occured:\r\n```\r\nInvalidArgumentError: Cannot colocate nodes 'gradients/moments/sufficient_statistics/count_grad/Const_1' and 'gradients/moments/sufficient_statistics/count_grad/Cumprod_1/axis: Cannot merge devices with incompatible types: '/GPU:2' and '/CPU:0'\r\n\t [[Node: gradients/moments/sufficient_statistics/count_grad/Const_1 = Const[_class=[\"loc:@moments/sufficient_statistics/count\"], dtype=DT_INT32, value=Tensor<type: int32 shape: [1] values: 0>, _device=\"/device:CPU:0\"]()]]\r\n```\r\n\r\nWhen setting the placeholder's batch size explicitly, or using CPU, the warning is gone.\r\n\r\nI'm using pip3-installed `tensorflow-gpu==1.0.1`, with Python 3.5.2.\r\nCUDA 8.0.61, cuDNN 5.1.10 is installed on Ubuntu 16.04.", "As the problem seems to be about a whole different things, I'm opening a new issue.", "I am experiencing issues with this as well. I would LOVE a tutorial or working example of training and testing a model by using batch normalisation. "]}, {"number": 3396, "title": "Numpy reshape and tf.reshape give different results in MNIST classification", "body": "### Environment info\n\nOperating System:\n\n```\nLinux anjum-Studio-XPS-1340 4.4.0-31-generic #50-Ubuntu SMP Wed Jul 13 00:07:12 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n```\n\nInstalled version of CUDA and cuDNN: \nUsing CPU only version\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp35-cp35m-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.: \n   `0.9.0`\n### Steps to reproduce\n1. Use the example from http://terrytangyuan.github.io/2016/06/09/scikit-flow-v09/#customized-model but edit to use the MNIST dataset instead.\n2. Flatten the MNIST images from 8x8 to 1x64 using `numpy.reshape` before passing the features to the model. This gives around 95% accuracy\n3. Flatten the MNIST images from 8x8 to 1x64 using `tf.reshape` within the model. This gives around 93% accuracy\n### What have you tried?\n\nThe feature tensors within the two models are the same shape:\nNumpy.reshape: `Tensor(\"input:0\", shape=(?, 64), dtype=float32)`\ntf.reshape: `Tensor(\"Reshape:0\", shape=(?, 64), dtype=float32)`\n### Logs or other output that would be helpful\n\nGist recreating the issue is here: https://gist.github.com/Anjum48/ca0a1f597a70b6b50b37fed2032a91d5\n", "comments": ["Did you try isolating the issue, by looking at the two reshaped `features` and compare differences?  That might uncover bugs/issues better than comparing the end-to-end accuracy.\n\n(I expect them to be equal.  And the differences in accuracy are just...machine learning? =D Did you train the `tf.reshape` version to full?)\n", "Thanks for the reply! What's the best way of comparing the differences between the reshaped features? I'm afraid my tensor debugging doesn't extend beyond print (I posted the output above).\n\nI initially thought it was ML accuracy, but I would have thought that 2% was pretty significant using MNIST? It also seems to be giving the same result each time which makes me think something else is up. Is there an easy way to fix the random seed between models?\n", "`tf.set_random_seed(1618)` or your favorite number should do it.\n\nTo compare, can't you `print(sess.run(tf_features))` and `print(np_features)?`\n", "Setting the random seed as above still gives the same results - so it's not a stochastic issue.\n\nI'm using skflow so i'm not sure how to get `print(sess.run(tf_features))` to work\n", "Maybe @terrytangyuan can take a look?  Marking as community support.  \n", "@Anjum48 Try printing features in your custom model definition. It's hard to dig into this without isolating the problem since there could be a lot underlying variations that lead to this inconsistency. \n", "Hi @terrytangyuan, here's where I've put my prints\n\n```\ndef my_model(features, target):\n    tf.set_random_seed(1618)\n    target = tf.one_hot(target, 10, 1, 0)  # Create 10x1 one-hot vectors\n    print(features)\n    features = layers.stack(features, layers.fully_connected, [10, 20, 10])\n    prediction, loss = (tf.contrib.learn.models.logistic_regression_zero_init(features, target))\n    train_op = tf.contrib.layers.optimize_loss(\n        loss, tf.contrib.framework.get_global_step(), optimizer='Adagrad',\n        learning_rate=0.1)\n    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op\n\n\ndef my_model2(features, target):\n    tf.set_random_seed(1618)\n    target = tf.one_hot(target, 10, 1, 0)  # Create 10x1 one-hot vectors\n    features = tf.reshape(features, [-1, 8*8])  # Flatten 8x8 MNIST digits\n    print(features)\n    features = layers.stack(features, layers.fully_connected, [10, 20, 10])\n    prediction, loss = (tf.contrib.learn.models.logistic_regression_zero_init(features, target))\n    train_op = tf.contrib.layers.optimize_loss(\n        loss, tf.contrib.framework.get_global_step(), optimizer='Adagrad',\n        learning_rate=0.1)\n    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op\n```\n\nAnd here's the output (i'm using yesterday's build):\n\n```\n/usr/bin/python3.5 \"/home/anjum/Python/numpy vs tf reshape.py\"\n(1149, 8, 8) (288, 8, 8) (360, 8, 8)\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp68y2znql\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpc8uirsil\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(64)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)\nTensor(\"input:0\", shape=(?, 64), dtype=float32)\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(8), Dimension(8)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)\nTensor(\"Reshape:0\", shape=(?, 64), dtype=float32)\nTensor(\"input:0\", shape=(?, 64), dtype=float32)\nTensor(\"Reshape:0\", shape=(?, 64), dtype=float32)\n             precision    recall  f1-score   support\n\n          0       0.97      0.91      0.94        33\n          1       0.97      1.00      0.98        28\n          2       0.94      0.97      0.96        33\n          3       1.00      0.91      0.95        34\n          4       0.96      0.98      0.97        46\n          5       0.94      0.96      0.95        47\n          6       1.00      0.97      0.99        35\n          7       0.92      0.97      0.94        34\n          8       0.97      0.93      0.95        30\n          9       0.93      0.95      0.94        40\n\navg / total       0.96      0.96      0.96       360\n\n             precision    recall  f1-score   support\n\n          0       0.97      0.94      0.95        33\n          1       0.96      0.96      0.96        28\n          2       0.80      0.85      0.82        33\n          3       0.93      0.82      0.87        34\n          4       0.96      1.00      0.98        46\n          5       0.94      0.94      0.94        47\n          6       0.97      0.97      0.97        35\n          7       1.00      0.97      0.99        34\n          8       0.87      0.90      0.89        30\n          9       0.88      0.90      0.89        40\n\navg / total       0.93      0.93      0.93       360\n\n\nProcess finished with exit code 0\n```\n", "I don't see you using numpy to flatten though. \n", "Sorry - check the Gist. It's flattened before it's passed to the fit of the first model\n", "I see. I think we need some results using two different reshape functions in order to see what the problem is, without getting into estimators first. \n", "Closing this issue as stale. Feel free to reopen if there is any concrete action to be taken."]}, {"number": 3395, "title": "standardize BUILD files format using buildifier", "body": "I just run `buildifier -mode=fix`find . -iname BUILD -type f`. \n\n`go get github.com/bazelbuild/buildifier` to install the tool.\n\nI like to have one format for the build files. \nWhat is the general feeling about this ?\n", "comments": ["Can one of the admins verify this patch?\n", "This is awesome. @gunan @caisq can we make this part of the ci-sanity? It causes all kinds of conflicts.\n", "Maybe without the autofix option, but I agree that we should have this.\n", "Jenkins, test this please.\n", "Actually, I was thinking about adding a test to cl-sanity that simply runs\nbuildifier and fails if any changes are suggested. The submitter can then\nrun it themselves to make sure it passes.\n\nOn Tue, Jul 19, 2016 at 3:55 PM gunan notifications@github.com wrote:\n\n> Jenkins, test this please.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/3395#issuecomment-233791059,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_TuqNVKFG0JfE2IsPSzUmzoWcZP5ks5qXVXUgaJpZM4JQAfu\n> .\n", "I think a CI check would be sufficient, you can run the command with `-mode=check` to only check if the files are well formatted. \n\nbuildifier -mode=check `find . -iname BUILD -type f` \n"]}, {"number": 3394, "title": "logdir not recognized as a keyword argument when fitting a DNNRegressor", "body": "Hi,\nI got the following error when trying to fit a DNNRegressor with this code:\n\n```\nreg=learn.DNNRegressor(hidden_units=[200,20],activation_fn=tf.tanh)\nreg.fit(X_train,Y_train,steps=1000,logdir='/tmp/')\n```\n\nTypeError: fit() got an unexpected keyword argument 'logdir'\n\nWas the option to monitor a regressor not implemented yet?\nThanks,\nDavid\n", "comments": ["Please use model_dir in the constructor\n", "Using model_dir in the constructor still gives me the same error\n\n```\nreg=learn.DNNRegressor(hidden_units=[100,100],activation_fn=tf.tanh,model_dir='/DNN/')\nreg.fit(X_train,Y_train,steps=100,monitors=[val_monitor],logdir='/DNN/')\n\n> TypeError: fit() got an unexpected keyword argument 'logdir'\n```\n\nany help is welcome.\n\nBy the way I'm running tensorflow for Ubuntu/Linux 64-bit, GPU enabled, Python 2.7 \nFound here: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl\n\nThanks!\n", "Pass it to DNNRegressor (constructor), not fit.\n\nOn Wednesday, July 20, 2016, David Bikard notifications@github.com wrote:\n\n> Using model_dir in the constructor still gives me the same error\n> \n> reg=learn.DNNRegressor(hidden_units=[100,100],activation_fn=tf.tanh,model_dir='/DNN/')\n> reg.fit(X_train,Y_train,steps=100,monitors=[val_monitor],logdir='/DNN/')\n> \n> > TypeError: fit() got an unexpected keyword argument 'logdir'\n> \n> any help is welcome.\n> Thanks!\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3394#issuecomment-233876171,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AEEnSj0VfESlfQE8KD8M2dos3uraJKs6ks5qXdc0gaJpZM4JP-LJ\n> .\n", "Sounds like a usage issue.  Nonetheless, StackOverflow is a better venue.\n", "Thanks, removing logdir from fit() worked, and I was able to launch tensorboard. I was mislead by the usage reported here: http://terrytangyuan.github.io/2016/03/14/scikit-flow-intro/\n"]}, {"number": 3393, "title": "build_pip_package failed to build without GPU option", "body": "### Environment info\n\nOperating System: Ubuntu 16.04 LTS\nGCC Version: 5.4.0\npython 3.5.2\nInstalled version of CUDA and cuDNN:  None\n\nIf installed from sources, provide the commit hash:\nfc9162975e52978d3af38549b570cc3cc5f0ab66\n### Steps to reproduce\n1. build bazel from sources\n2. git clone --recurse-submodules https://github.com/tensorflow/tensorflow\n3. ./configure\n4. bazel build -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\n### What have you tried?\n1. rebuild bazel and rebuild tensorflow\n### Logs or other output that would be helpful\n\n> INFO: Waiting for response from Bazel server (pid 17346)...\n> WARNING: /home/jxypoi/Documents/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\n> WARNING: /home/jxypoi/.cache/bazel/_bazel_jxypoi/d296672d8345626b0b5383fe0f5b61e2/external/boringssl_git/WORKSPACE:1: Workspace name in /home/jxypoi/.cache/bazel/_bazel_jxypoi/d296672d8345626b0b5383fe0f5b61e2/external/boringssl_git/WORKSPACE (@boringssl) does not match the name given in the repository's definition (@boringssl_git); this will cause a build error in future versions.\n> INFO: Found 1 target...\n> ERROR: /home/jxypoi/Documents/tensorflow/tensorflow/core/kernels/BUILD:1080:1: C++ compilation of rule '//tensorflow/core/kernels:sparse_matmul_op' failed: gcc failed: error executing command \n>   (cd /home/jxypoi/.cache/bazel/_bazel_jxypoi/d296672d8345626b0b5383fe0f5b61e2/execroot/tensorflow && \\\n>   exec env - \\\n>     PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\n>   /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/local-py3-opt/bin/tensorflow/core/kernels/_objs/sparse_matmul_op/tensorflow/core/kernels/sparse_matmul_op.pic.d '-frandom-seed=bazel-out/local-py3-opt/bin/tensorflow/core/kernels/_objs/sparse_matmul_op/tensorflow/core/kernels/sparse_matmul_op.pic.o' -fPIC -DHAVE_CONFIG_H -iquote . -iquote bazel-out/local-py3-opt/genfiles -iquote external/protobuf -iquote bazel-out/local-py3-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/local-py3-opt/genfiles/external/bazel_tools -iquote external/farmhash_archive -iquote bazel-out/local-py3-opt/genfiles/external/farmhash_archive -iquote external/jpeg_archive -iquote bazel-out/local-py3-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local-py3-opt/genfiles/external/png_archive -iquote external/gif_archive -iquote bazel-out/local-py3-opt/genfiles/external/gif_archive -iquote external/highwayhash -iquote bazel-out/local-py3-opt/genfiles/external/highwayhash -iquote external/re2 -iquote bazel-out/local-py3-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local-py3-opt/genfiles/external/eigen_archive -iquote external/zlib_archive -iquote bazel-out/local-py3-opt/genfiles/external/zlib_archive -iquote external/boringssl_git -iquote bazel-out/local-py3-opt/genfiles/external/boringssl_git -iquote external/jsoncpp_git -iquote bazel-out/local-py3-opt/genfiles/external/jsoncpp_git -isystem external/protobuf/src -isystem bazel-out/local-py3-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem bazel-out/local-py3-opt/genfiles/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local-py3-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/gif_archive/giflib-5.1.4/lib -isystem bazel-out/local-py3-opt/genfiles/external/gif_archive/giflib-5.1.4/lib -isystem external/highwayhash -isystem bazel-out/local-py3-opt/genfiles/external/highwayhash -isystem external/re2 -isystem bazel-out/local-py3-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local-py3-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-b4fa9622b809 -isystem bazel-out/local-py3-opt/genfiles/external/eigen_archive/eigen-eigen-b4fa9622b809 -isystem external/zlib_archive/zlib-1.2.8 -isystem bazel-out/local-py3-opt/genfiles/external/zlib_archive/zlib-1.2.8 -isystem external/boringssl_git/src/include -isystem bazel-out/local-py3-opt/genfiles/external/boringssl_git/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/local-py3-opt/genfiles/external/jsoncpp_git/include -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/kernels/sparse_matmul_op.cc -o bazel-out/local-py3-opt/bin/tensorflow/core/kernels/_objs/sparse_matmul_op/tensorflow/core/kernels/sparse_matmul_op.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n> In file included from tensorflow/core/kernels/sparse_matmul_op.cc:20:0:\n> ./tensorflow/core/kernels/sparse_matmul_op.h:46:26: error: 'Packet4f' does not name a type\n>  EIGEN_DEVICE_FUNC inline Packet4f pexpand_bf16_l(const Packet4f& from) {\n>                           ^\n> ./tensorflow/core/kernels/sparse_matmul_op.h:59:26: error: 'Packet4f' does not name a type\n>  EIGEN_DEVICE_FUNC inline Packet4f pexpand_bf16_u(const Packet4f& from) {\n>                           ^\n> ./tensorflow/core/kernels/sparse_matmul_op.h:117:21: error: 'Packet4f' does not name a type\n>  EIGEN_STRONG_INLINE Packet4f pload4bf16<Packet4f>(const float\\* from) {\n>                      ^\n> ./tensorflow/core/kernels/sparse_matmul_op.h:128:21: error: 'Packet4f' does not name a type\n>  EIGEN_STRONG_INLINE Packet4f pload2bf16<Packet4f>(const float\\* from) {\n>                      ^\n> ./tensorflow/core/kernels/sparse_matmul_op.h: In instantiation of 'Packet Eigen::internal::pexpand_bf16_l(const Packet&) [with Packet = float]':\n> tensorflow/core/kernels/sparse_matmul_op.cc:375:3:   required from here\n> ./tensorflow/core/kernels/sparse_matmul_op.h:31:44: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n>    return reinterpret_cast<const float&>(tmp);\n>                                             ^\n> ./tensorflow/core/kernels/sparse_matmul_op.h: In instantiation of 'Packet Eigen::internal::pexpand_bf16_u(const Packet&) [with Packet = float]':\n> tensorflow/core/kernels/sparse_matmul_op.cc:376:3:   required from here\n> ./tensorflow/core/kernels/sparse_matmul_op.h:40:44: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n>    return reinterpret_cast<const float&>(tmp);\n>                                             ^\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\n\n**any problems with the src?**\n", "comments": ["OK, I think it may result from my building this on a 32bit virtual machine... Can anyone confirm this?\n", "I encountered this same compilation failure on Raspberry Pi 2 (Raspbian Jessie, GCC 4.9.2) with tensorflow commit fc91629.\n\nI was running this command and got the same error messages:\n\n```\n$ bazel build :libtensorflow.so --jobs 3 --ram_utilization_factor 25 --genrule_strategy=standalone   --spawn_strategy=standalone\n...\nERROR: /home/pi/tf/tensorflow/tensorflow/core/kernels/BUILD:1080:1: C++ compilation of rule '//tensorflow/core/kernels:sparse_matmul_op' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer '-std=c++0x' -DHAVE_CONFIG_H -iquote . -iquote ... (remaining 101 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nIn file included from tensorflow/core/kernels/sparse_matmul_op.cc:20:0:\n./tensorflow/core/kernels/sparse_matmul_op.h:46:26: error: 'Packet4f' does not name a type\n EIGEN_DEVICE_FUNC inline Packet4f pexpand_bf16_l(const Packet4f& from) {\n                          ^\n./tensorflow/core/kernels/sparse_matmul_op.h:59:26: error: 'Packet4f' does not name a type\n EIGEN_DEVICE_FUNC inline Packet4f pexpand_bf16_u(const Packet4f& from) {\n                          ^\n./tensorflow/core/kernels/sparse_matmul_op.h:117:21: error: 'Packet4f' does not name a type\n EIGEN_STRONG_INLINE Packet4f pload4bf16<Packet4f>(const float* from) {\n                     ^\n./tensorflow/core/kernels/sparse_matmul_op.h:128:21: error: 'Packet4f' does not name a type\n EIGEN_STRONG_INLINE Packet4f pload2bf16<Packet4f>(const float* from) {\n...\nTarget //tensorflow:libtensorflow.so failed to build\n```\n", "~~I tried to checkout tensorflow v0.9.0, but the same error occurs also there.~~\nEDIT: I need to try this again. I think I didn't really checkout v0.9.0 correctly.\n\nEDIT2: This doesn't occur in tensorflow v0.9.0.\n", "For the original environment: can you confirm that the issue is still there in a non-virtual machine?\n", "Hello there, I have a try on another virtual machine today and make it this time (a little slow though...).\n\nCentOS7(x86_64)\ngcc-4.8.5\npython 3.5.2\nno GPU option\n\n> Target //tensorflow/tools/pip_package:build_pip_package up-to-date:\n>     bazel-bin/tensorflow/tools/pip_package/build_pip_package\n> INFO: Elapsed time: 4577.867s, Critical_Path: 4385.53s\n\nSo I guess it's an issue related to 32bit arch......\n"]}, {"number": 3392, "title": "Remove unnecessary input-size requirement for convolutions with padding='SAME'", "body": "Right now there is conflicting behavior when `padding='SAME'`: If inputs have a defined height and width, then convolutions require that filters be no larger than input images (spatially). If inputs do not have a defined height and width, then it's okay for filters to be larger than images.\n\nI think this conflicting behavior should be removed, especially since `padding='SAME'` is used for convenience and with the intention of allowing some border effects, and because this way we can continue to use this convenience even when filter size > input size.\n\nTensorFlow 0.9 example with defined height and width:\n\n```\ninputs = tf.placeholder(tf.float32, shape=[None, 2, 2, 3])\nweights = tf.get_variable('weights', [3, 3, 3, 10], tf.float32,\n                          initializer=tf.random_normal_initializer())\nt = tf.nn.conv2d(inputs, weights, [1, 1, 1, 1], 'SAME')\n\n# ValueError: Filter must not be larger than the input: Filter: (3, 3) Input: (2, 2)\n```\n\nTensorFlow 0.9 example without defined height and width:\n\n```\ninputs = tf.placeholder(tf.float32, shape=[None, None, None, 3])\nweights = tf.get_variable('weights', [3, 3, 3, 10], tf.float32,\n                          initializer=tf.random_normal_initializer())\nt = tf.nn.conv2d(inputs, weights, [1, 1, 1, 1], 'SAME')\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(t, feed_dict={inputs: np.random.rand(1, 2, 2, 3)}).shape)\n\n# Shape is what we expect: (1, 2, 2, 10)\n```\n", "comments": ["@zheng-xq: are you the right person to ping?  Thanks.\n", "I agree that this check is not completely necessary at this point. However, both the forward/ backward and CPU/GPU in a number of use cases are affected by potential relaxing of this restriction. Some code paths may not work. We need to check them extensively before relaxing it. \n", "We probably won't be working on this in the short term.  Marking as contributions welcome.\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 3391, "title": "cudnn5  so slow", "body": "I updated cudnn from 4 to 5, and reinstall tensorflow from source.\nBut I found that my code run almost 6 time slower than before.\n", "comments": ["Please provide more environment details and a minimum repro (if you can isolate the performance degradation to a few ops, or a minimum model).\n", "(For example, please copy the issue report template and fill in your specific details from #3366.) \n"]}, {"number": 3390, "title": "Index error in decoding", "body": "When I tried  --decode  and gave a sentence, I got this error\nTraceback (most recent call last):\n  File \"translate.py\", line 279, in <module>\n    tf.app.run()\n  File \"/home/devadath/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"translate.py\", line 274, in main\n    decode()\n  File \"translate.py\", line 244, in decode\n    print(\" \".join([tf.compat.as_str(rev_hi_vocab[output]) for output in outputs]))\n  File \"translate.py\", line 244, in <listcomp>\n    print(\" \".join([tf.compat.as_str(rev_hi_vocab[output]) for output in outputs]))\nIndexError: list index out of range\n\nAny help is appreciated\n", "comments": ["We will need more info.  See #3366 as an example.\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n", "Check that your rev_hi_vocab array is crossed.", "I am also getting the exact same error. Waiting for help.\r\nTraceback (most recent call last):\r\n  File \"translate.py\", line 432, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"translate.py\", line 425, in main\r\n    decode()\r\n  File \"translate.py\", line 395, in decode\r\n    print(\" \".join([tf.compat.as_str(rev_en_vocab[output]) for output in outputs]))\r\nIndexError: list index out of range\r\n", "Facing this issue too. \r\nI was following the [seq2seq tutorial](https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r0.7/tensorflow/g3doc/tutorials/seq2seq/index.md) for a similar task. Since the tutorial is dated, and the code now nowhere to be found on the official repos, I used this [repo](https://github.com/petewarden/tensorflow_makefile/) as source. \r\nTensorflow 0.12.1\r\nPython 3.6.2 \r\nOS - Amazon Linux\r\n\r\n**Data and Configuration**\r\nTrained a small model (2 layers, 256 units) using 50k rows as the training set, and another 25k for eval. Note that the task isn't translation, it is similar to Named Entity Recognition. Hence, the length of the target is very small (<5 tokens), and the length of the source isn't that large either. \r\nI set the target_vocab_size to 4000, and kept the source_vocab_size at its default of 40000.   \r\n**Steps**\r\nI trained the model for 5k steps, at the end of which the perplexity dropped to ~1. I expected the model to give good results, but when I decoded a sentence, the output tokens ids were beyond 4000, which is what resulted in this error. \r\nHere is the log\r\n\r\n```\r\nInput Token IDs :[159, 10, 5, 6, 3387, 2827, 1363, 315, 156, 70, 1194, 4, 13, 59, 15, 7, 71, 16, 4, 17, 20, 76, 118] \r\nOutputs [11229, 11229, 11229, 11229, 11229, 11229, 11229, 11229, 24412, 24412, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050, 5050]\r\n\r\nTraceback (most recent call last):\r\n  File \"translate.py\", line 287, in <module>\r\n    tf.app.run()\r\n  File \"/home/ec2-user/anaconda3/envs/tfseq2seq/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"translate.py\", line 282, in main\r\n    decode()\r\n  File \"translate.py\", line 252, in decode\r\n    print(\" \".join([tf.compat.as_str(rev_fr_vocab[output]) for output in outputs]))\r\n  File \"translate.py\", line 252, in <listcomp>\r\n    print(\" \".join([tf.compat.as_str(rev_fr_vocab[output]) for output in outputs]))\r\nIndexError: list index out of range\r\n```\r\n\r\nAny pointers as to what could be the cause of this?"]}, {"number": 3389, "title": "Why doesn't tensorflow support tensor as the feed_dict?", "body": "**What I'm trying to do**\n\nI am trying to extract CNN features for my own images with residual-net based on https://github.com/ry/tensorflow-resnet. I plan to input image data from JPG files before exploring how to convert the images into a single file.\n\n**What I have done**\n\nI have read https://www.tensorflow.org/versions/r0.9/how_tos/reading_data/index.html and some related materials about how to input data like feeding and placeholder. Here is my code:\n\n```\nimport tensorflow as tf\nfrom convert import print_prob, checkpoint_fn, meta_fn\nfrom image_processing import image_preprocessing\ntf.app.flags.DEFINE_integer('batch_size', 1, \"batch size\")\ntf.app.flags.DEFINE_integer('input_size', 224, \"input image size\")\ntf.app.flags.DEFINE_integer('min_after_dequeue', 224, \"min after dequeue\")\ntf.app.flags.DEFINE_integer('layers', 152, \"The number of layers in the net\")\ntf.app.flags.DEFINE_integer('image_number', 6951, \"number of images\")\nFLAGS = tf.app.flags.FLAGS\n\n\ndef placeholder_inputs():\n    images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, FLAGS.input_size, FLAGS.input_size, 3))\n    label_placeholder = tf.placeholder(tf.int32, shape=FLAGS.batch_size)\n    return images_placeholder, label_placeholder\n\n\ndef fill_feed_dict(image_ba, label_ba, images_pl, labels_pl):\n    feed_dict = {\n        images_pl: image_ba,\n    }\n    return feed_dict\n\nmin_fraction_of_examples_in_queue = 0.4\nmin_queue_examples = int(FLAGS.image_number *\n                     min_fraction_of_examples_in_queue)\ndataset = tf.train.string_input_producer([\"hollywood_test.txt\"])\nreader = tf.TextLineReader()\n_, file_content = reader.read(dataset)\nimage_name, label, _ = tf.decode_csv(file_content, [[\"\"], [\"\"], [\"\"]], \" \")\nlabel = tf.string_to_number(label)\nnum_preprocess_threads = 10\nimages_and_labels = []\nwith tf.Session() as sess:\n    for thread_id in range(num_preprocess_threads):\n        image_buffer = tf.read_file(image_name)\n        bbox = []\n        train = False\n        image = image_preprocessing(image_buffer, bbox, train, thread_id)\n        image = image_buffer\n        images_and_labels.append([image, label])\n    image_batch, label_batch = tf.train.batch_join(images_and_labels,\n                                            batch_size=FLAGS.batch_size,\n                                            capacity=min_queue_examples + 3 * FLAGS.batch_size)\n    images_placeholder, labels_placeholder = placeholder_inputs()\n    new_saver = tf.train.import_meta_graph(meta_fn(FLAGS.layers))\n    new_saver.restore(sess, checkpoint_fn(FLAGS.layers))\n    graph = tf.get_default_graph()\n    prob_tensor = graph.get_tensor_by_name(\"prob:0\")\n    images = graph.get_tensor_by_name(\"images:0\")\n    feed_dict = fill_feed_dict(image_batch, label_batch, images, labels_placeholder)\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(coord=coord)\n    sess.run(tf.initialize_all_variables())\n    prob = sess.run(prob_tensor, feed_dict=feed_dict)\n    print_prob(prob[0])\n    coord.request_stop()\n    coord.join(threads)\n```\n\n**What my question is**\n\nThe code above got the error `TypeError: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, or numpy ndarrays.` You can see I am trying to do all the work in the context of tensorflow. As far as I know, tensorflow is a framework where all the inputs and outputs of the nodes are tensors. However, I am quite confused why feed_dict doesn't support tensor as an input. But the batch_join function return a tensor and so do other ops. I found that in the mnist example of tensorflow there is even another function to produce a batch when tensorflow is providing methods for batching. So I wonder if there is an elegant way to do these things. If this is because of my lack of searching and careful reading I really apologize.\n", "comments": ["I'm new to tensorflow, according to my experience, you really don't need to feed a tf.tensor to feed_dict. Once you have a tensor with value, you can get this value with your_tensor.eval() or sess.run(your_tensor) and then you can feed the output to your feed_dict\n", "I have also searched about how to get the value of a tensor to solve this problem and seen the solutions like above. However when I try to get image_batch.eval() or even image_buffer.eval(), the program just keep working without stopping or giving any output. I have tried to get a constant value from a tensor and succeeded. But this is just useless with tensor gotten from tf.read_file() and etc.\n", "If you have your data in tensors anyway, you don't need to use placeholders and `feed_dict`. You can just directly use `image_batch` / `label_batch` instead of `images_placeholder` / `label_placeholder` when setting up your model / loss.\n", "Yes, I can see from the examples of training procedures that I can just use tensors as input of any ops in tensorflow. However in this case I am using a predefined network so I am confused whether I can change the network definition. That is to say, I have no idea how to change the input of loss function(the definition of node \"images\" and \"prob\" is `images = tf.placeholder(\"float32\", [None, 224, 224, 3], name=\"images\")\n        logits = resnet.inference(images,\n                                  is_training=False,\n                                  num_blocks=num_blocks,\n                                  preprocess=True,\n                                  bottleneck=True)\n        prob = tf.nn.softmax(logits, name='prob')` in the predefined model).\n", "StackOverflow is a better venue.  One key thing to note is that `Tensor` is simply a **symbolic** object.  The values of your `feed_dict` are the **actual** values, e.g. a Numpy ndarry.\n", "I have the same questions with @wishforgood .\n\"I have no idea how to change the input of loss function\"...\n\nHow?\n", "Why is this closed?\r\n\r\nAuthor raises legitimate concerns.\r\n\r\n1. Why data already on GPU, cannot be fed into GPU model.\r\n\r\n2. Why redefine model to use data already on GPU. This makes code more complicated.\r\n\r\n3. What if I have 5 variables with data, do I need to define now 5 models with shared trainable variables?\r\n\r\nYes it is confusing, make you write complicated (or slow) code -- I think you guys should reopen and address this.\r\n", "I have the same opinion as yours. These days, i use the cnn to do some text work. For the input's dimension  is too large, i have to change it to other type in case of the memory error. But , what confused me is the same as yous. Why tensor cannot be a feed_dict value!!!", "Looks like you could use a [session_handle](https://www.tensorflow.org/api_docs/python/tf/get_session_handle) to do this:\r\n\r\nSimple case:\r\n```\r\np = tf.placeholder(tf.float32, shape=()) \r\nc = tf.constant(1.0, dtype=tf.float32)\r\nh = tf.get_session_handle(c)\r\nh = sess.run(h)        # gives you a handle\r\nsess.run(p, feed_dict={p:h})\r\n```\r\n\r\nThe below is an example from the get_session_handle doc:\r\n```\r\nc = tf.multiply(v1, v2)\r\nh = tf.get_session_handle(c)\r\nh = sess.run(h)        # gives you a handle\r\n\r\np, a = tf.get_session_tensor(h.handle, tf.float32)\r\nb = tf.multiply(a, 10)\r\nc = sess.run(b, feed_dict={p: h.handle})\r\n```", "This works for a scalar value but when you try to get handle for a sequence of inputs, it is failing.\r\nBasically, i am also getting the same error **\"TypeError: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles.\"**\r\n\r\noperation I performed is feeding the sequence of number from -10 to 10 for a function.\r\n\r\npart of code:\r\nf = tf.square(x) + 2 * x + 5\r\n\r\nval_x = tf.range(-10, 10, 0.1)\r\n\r\nval_f = sess.run(f, feed_dict = {x: val_x})\r\n\r\nBut if i replace val_x = tf.range(-10, 10, 0.1)  --> val_x = np.arange(-10, 10, 0.1), it works fine without any errors.\r\n\r\nwhy is sequence generated using tf.range() is not treated as tensor object?\r\n\r\nThanks in Advance!\r\n", "@AnilKumarES For the feed_dict can't accept the tensor object. As for the f = tf.square(x) + 2 * x + 5, it can be fed with scalars generated by numpy", "You can do something like this in a loop to make batches without using any TensorFlow backend. This will ensure that the output of it remains non-tensor.\r\n\r\n```\r\nwith tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n            for index, offset in enumerate(range(0, train, batch_size)):\r\n                x_epoch, y_epoch = np.array(train_x[offset: offset + batch_size,:]), np.array(train_y[offset: offset +  batch_size])\r\n```\r\n", "The short answer to the question is because of Tensorflow make\u044b simple things complicated", "I may have a different question but similar with yours: if my input data is already in tensor type, how can i feed them into the network? this is very simple and straightforward in pytorch, but as we know, tensorflow uses static computational graph. we have to pre-define the graph and then change the input in a cycle. if the inputs of network and loss function are defined when creating the graph, how can we change it during training?", "One workaround is to construct a graph def and load it in with [input_mapping](https://www.tensorflow.org/api_docs/python/tf/graph_util/import_graph_def), through this the [tf.data.Iterator](https://www.tensorflow.org/api_docs/python/tf/data/Iterator) can be hooked up to any arbitrary graph which previously used placeholders for input. This is especially beneficial for inference, since weights can be frozen.", "For anyone who lands on this question, Lakrish provided a very usable solution (only a few lines of code). I have posted an example using this suggestion at this related SO question: https://stackoverflow.com/questions/38618960/tensorflow-how-to-insert-custom-input-to-existing-graph/57015133#57015133"]}, {"number": 3388, "title": "Bug: Exception ignored in BaseSession.__del__", "body": "Hello Everyone,\n\nThe following code produces an error in TensorFlow:\n\n```\nimport tensorflow as tf\n\na = tf.constant(123)\nb = tf.constant(456)\nc = a * b\n\nsession = tf.Session()\n\n# A slightly different error is produced if this is removed.\nsession.run(tf.initialize_all_variables())\n\nresult = session.run(c)\n\nprint(result)\n\nsession.close()    # The error is produced regardless of this.\n\n#quit()            # This produces the error.\n\nimport sys\nsys.exit()         # This also produces the error.\n```\n\nThis is the contents of sandbox3.py which I run in PyCharm.\n\nThe output is:\n\n```\n/home/magnus/anaconda3/envs/tensorflow/bin/python /home/magnus/development/TensorFlow-Tutorials/sandbox3.py\n56088\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7fb4ecd301d0>>\nTraceback (most recent call last):\n  File \"/home/magnus/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 171, in __del__\n  File \"/home/magnus/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 167, in close\nAttributeError: 'NoneType' object has no attribute 'raise_exception_on_not_ok_status'\n\nProcess finished with exit code 0\n```\n\nI'm using the following versions:\n- TensorFlow 0.9.0 CPU installed from binary.\n- Python 3.5.1\n- Anaconda 2.5.0\n- PyCharm 5.0.4\n- Linux Mint 17.3\n", "comments": ["I cannot reproduce this on Ubuntu or my local Mac laptop -- both run fine, with or without the `quit()` line.  FYI, Linux Mint is not officially supported, so I'm marking this as community support.\n", "Same bug with me, anyway, it didn't happen every time, just occasionally.\n", "Also happens to me occassionally on TensorFlow 0.10.0, Python 3.5, Ubuntu 15.10.\n", "Same problem on elementary OS 0.4 Loki\n- conda version : 4.2.9\n- conda-env version : 4.2.9\n- conda-build version : 2.0.2\n- python version : 3.5.2.final.0\n- tensorflow                0.10.0\n\nNot everytime too.\n", "That should be fixed in 0.11rc0 by\nhttps://github.com/tensorflow/tensorflow/commit/b21c69a266b9058133827d12f8915829ae07cd9a\n\nOn Tue, Oct 4, 2016 at 12:14 PM, Ben Hur Bahia do Nascimento <\nnotifications@github.com> wrote:\n\n> Same problem on elementary OS 0.4 Loki\n> - conda version : 4.2.9\n> - conda-env version : 4.2.9\n> - conda-build version : 2.0.2\n> - python version : 3.5.2.final.0\n> - tensorflow 0.10.0\n> \n> Not everytime too.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3388#issuecomment-251484561,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHDIrWMPYN10kkzoh-vxeXyauvTj5ks5qwqWIgaJpZM4JPnE0\n> .\n", "Unfortunately, after [b21c69a](https://github.com/tensorflow/tensorflow/commit/b21c69a266b9058133827d12f8915829ae07cd9a) fix, this exception will come up oftenly (not every time.):\n\n``` code\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7f78b6702a58>>\nTraceback (most recent call last):\n  File \"/home/liusida/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 532, in __del__\nAttributeError: 'NoneType' object has no attribute 'TF_DeleteStatus'\n```\n\nand sometimes:\n\n``` code\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7f276ef81cf8>>\nTraceback (most recent call last):\n  File \"/home/liusida/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 532, in __del__\nUnboundLocalError: local variable 'status' referenced before assignment\n```\n", "Hi @liusida - can the bug be fixed . I am somehow getting this error - though not often but I am getting it. My grades in class is somehow dependent on it. \n", "@motiur If you cannot wait, and if you just want to mute this error message temporarily, I think you can just comment this line out:\n.../site-packages/tensorflow/python/client/session.py, line 532, in **del**\n`tf_session.TF_DeleteStatus(status)`\n\nBut I suggest you to wait for the official fix.\n", "@liusida Hi I have commented out `try` and`finally` block  of code:\n\n```\ntry:\n  status = tf_seesion.TF_NewStatus( )\n  tf_session.TF_DeleteSession(self._session, status)\nfinally:\n  tf_session_TF_DeleteStatus(status)\n\n```\n\nHope it wouldn't have drastic impact down the road. \n", "@yaroslavvb Looks like it wasn't: (Python 3.5.2, Ubuntu 16.10, GPU)\n\n```\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7f86c267fe80>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 532, in __del__\nUnboundLocalError: local variable 'status' referenced before assignment\n```\n", "@atemerev that's a different error, which seems to be there at head -- status is a local variable which doesn't get initialized in some cases https://github.com/tensorflow/tensorflow/blob/7e94b1ee27cb4e009f8bbee1f230c7ca3adfccf3/tensorflow/python/client/session.py#L578\n\n```\n def __del__(self):\n    # cleanly ignore all exceptions\n    try:\n      self.close()\n    except Exception:  # pylint: disable=broad-except\n      pass\n    if self._session is not None:\n      try:\n        status = tf_session.TF_NewStatus()\n        tf_session.TF_DeleteDeprecatedSession(self._session, status)\n      finally:\n        tf_session.TF_DeleteStatus(status)\n      self._session = None\n```\n", "it induced by different gc sequence, if python collect session first , the program will exit successfully, if python collect swig memory(tf_session) first, the program exit with failure.\r\n\r\nyou can force python to del session by:\r\n\r\n`del session`\r\n\r\nor if you are using keras, you cant get the session instance, you can run folloing code at end of your code:\r\n`import gc;\r\ngc.collect()`", "@biolee It works for me. Thanks a lot! I'm using keras. ", "I'm using Keras but this fix doesn't work for me.  Still fails intermittently.", "@ksrinivs64  Can you post your error message and the problematic code here? ", "@ksrinivs64 try to delete session from keras backend:\r\n```python\r\nfrom keras import backend as K\r\n\r\n# ... code\r\nK.clear_session()\r\n```", "Here's code that reproduces the Keras problem for me from a recent build of my test system. Ubuntu 14.04, Python 3.4 (default OS version), TensorFlow 0.12.1, Keras 1.2.0. pima.csv is a copy of [this file](http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data):\r\n\r\n```\r\n\"\"\"Verify that Keras works correctly.\r\n\r\nExample comes from: http://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\r\n\"\"\"\r\nfrom keras import models\r\nfrom keras import layers\r\nimport numpy\r\nimport gc\r\n\r\n# fix random seed for reproducibility\r\nnumpy.random.seed(7)\r\n\r\ndataset = numpy.loadtxt('pima.csv', delimiter=',')\r\n# First 8 columns are the input variables\r\nX = dataset[:, 0:8]\r\n# Last column is the output variable\r\nY = dataset[:, 8]\r\n\r\n# Create fully-connected network with three layers.\r\n# The last layer has one neuron to predict the class.\r\nmodel = models.Sequential()\r\nmodel.add(layers.Dense(12, input_dim=8, init='uniform', activation='relu'))\r\nmodel.add(layers.Dense(8, init='uniform', activation='relu'))\r\nmodel.add(layers.Dense(1, init='uniform', activation='sigmoid'))\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\nmodel.fit(X, Y, nb_epoch=150, batch_size=10, verbose=False)\r\n\r\nscores = model.evaluate(X, Y, verbose=False)\r\nprint('{}: {}%'.format(model.metrics_names[1], scores[1] * 100))\r\n\r\n# Explicitly reap session to avoid an AttributeError sometimes thrown by\r\n# TensorFlow on shutdown. See:\r\n# https://github.com/tensorflow/tensorflow/issues/3388\r\ngc.collect()\r\n```\r\n\r\n_Update:_ Calling `backend.clear_session()` instead of `gc.collect()` does indeed seem to fix the problem. Yay for not relying on GC magic! But this still really ought to be fixed. :-)", "Another update: ITOT the `gc.collect()` solution I attempted in my straight-up TensorFlow test ended up not working reliably either. So, I went from doing this:\r\n\r\n```\r\nimport gc\r\nwith tf.Session() as session:\r\n    session.run(...)\r\ngc.collect()\r\n```\r\n\r\nTo doing this:\r\n\r\n```\r\nwith tf.Session() as session:\r\n    session.run(...)\r\ndel session\r\n```\r\n\r\nIt's a nondeterminstic error, so we'll see if that takes.", "Just hit this error too. Another system spec data point: Python 3.5.0, TF 0.12.1, Macbook Pro, Nvidia 980 Ti GPU (external)", "+1", "\r\n\r\ni think is some obj reference bug that Swig induced , so i propose by `del sess` or `gc.collect()`, if it not reliable as report before, further debug TBD \r\n\r\n\r\n@kengz clear_session is not delete\uff0coriginal code is\r\n```\r\ndef clear_session():\r\n    \"\"\"Destroys the current TF graph and creates a new one.\r\n    Useful to avoid clutter from old models / layers.\r\n    \"\"\"\r\n    global _SESSION\r\n    global _GRAPH_LEARNING_PHASES\r\n    tf.reset_default_graph()\r\n    reset_uids()\r\n    _SESSION = None\r\n    phase = tf.placeholder(dtype='bool', name='keras_learning_phase')\r\n    _GRAPH_LEARNING_PHASES[tf.get_default_graph()] = phase\r\n```", "@biolee you can add your last comment (on how to solve it) there on the source code as a comment (I think the github offers that).\r\n", "Another: TF1.0.0, Win10\r\n\r\n[Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x000001D3B79EF358>>\r\nTraceback (most recent call last):\r\n  File \"lib\\tensorflow_gpu\\tensorflow\\python\\client\\session.py\", line 582, in __del__\r\nUnboundLocalError: local variable 'status' referenced before assignment](url)\r\n\r\n", "Seems like the module-level functions are cleared before `Session.__del__()` is called.\r\nWhich is standard cpython behaviour on shutdown (i.e. `tf_session = None` is called by python modules cleanup before `tf_session.TF_NewStatus()`).\r\nShould not be a problem in `python > 3.4` but having python 3.5 bug reports here, I'm not sure.\r\n", "I am on TF 1.1 rc2 freshly built from source on macOS Sierra and keras 2.0.2. Facing the same problem. Commented out the try-finally block at line 595 in session.py to make it work. Don't know if the hack will have any effect on the model.", "I'm also using TF1.1.0, Python 3.5, Ubuntu 16.04.2 and am still getting this error. I edited __ del __() in session.py as follows (just added the second exception). Is there any reason to worry that this might cause other problems?\r\n```\r\n      except (AttributeError, TypeError) as e:\r\n        # 'NoneType' object has no attribute 'TF_NewStatus' or\r\n        # 'NoneType' object is not callable\r\n        pass\r\n```", "Getting the same error with TF (GPU) 1.12.0, Python 3.5.3, 1080ti (external) on my MBP running Win10. The code was basically the sample code from this MNIST Keras-Tutorial: https://github.com/wxs/keras-mnist-tutorial/blob/master/MNIST%20in%20Keras.ipynb (until [10]) .", "Same issue happened occasionally.\r\n\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7f88e5b52908>>\r\nTraceback (most recent call last):\r\n  File \"/home/wcai/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 701, in __del__\r\nTypeError: 'NoneType' object is not callable\r\n\r\nPython 3.5.2\r\nTensorFlow 1.3.0 GPU version installed by virtualenv\r\nLinux Mint 18 64-bit", "@ChristopherLucas I agree with you, something inside `status = c_api_util.ScopedTFStatus()` must call `None()`\r\n\r\nBecause during GC there is guaranteed almost nothing, the best may be even `except Exception`", "So can someone tell me what the final fix is? python 3.5, keras with tf backend. same problem.\r\n```\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7ffad5365eb8>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 696, in __del__\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/c_api_util.py\", line 30, in __init__\r\nTypeError: 'NoneType' object is not callable\r\n```", "I was having the same problem and it was caused because when loading the weights of a previous model it also loaded the number of epochs, based on that previous session. So when you try to load your model from epoch = 50 and train it for epochs = 25 it will give you a Nonetype object because you already pass those 25 epochs. \r\n\r\nSumming up, set `n_epochs_new_model>n_epochs_loaded_model`.   ", "@barathbheeman \r\n\r\nCalling `K.clear_session()` at the end got rid of this issue for me.\r\n", "@JaviFuentes94 sorry for the question but when you mention setting `n_epochs_new_model>n_epochs_loaded_model` where should this be set?", "Hello everyone,\r\n\r\nSame issue still happening to me occasionally even though I cleared the TF session with `K.clear_session()` as @hosford42 suggested after the code was done. @Nimi42 also suggested that in #8652. I also tried clearing the session before each forward pass to no avail.\r\n\r\nAdding `import tensorflow as tf` before importing Keras like @Ares513 suggested in #8652 didn't work either.\r\n\r\nI'm making inferences on multiple images, which are kicked off from separate bash scripts that are executing sequentially so I'm wondering if clearing the session would actually help since every forward pass of my net is invoked from a different process.\r\n\r\nI'm using python 3.4, keras 2.1.6, and tensorflow 1.8.0 (no GPU support). \r\n\r\nThis is the error:\r\n\r\n`\r\nException ignored in: <bound method Session.__del__ of <tensorflow.python.client.session.Session object at 0x7f037640b198>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 707, in __del__\r\nTypeError: 'NoneType' object is not callable\r\n`\r\n\r\nDon't know if it may be related but this is also a warning I get each time I run my inference script:\r\n\r\n`\r\n/usr/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  return f(*args, **kwds)\r\n`\r\n\r\n#8652  is related and some also solved it by placing `K.clear_session()` **_after_** all the code was done. Note that contrary to what the author of #8652 states this issue is not closed and @naatje80 mentions it's a Keras issue but [this](https://github.com/fizyr/keras-retinanet/issues/192) is a related Keras issue that was closed stating it's a TF issue. [This](https://github.com/tensorflow/cleverhans/issues/17) issue is also related and was also closed stating it's a tensorflow issue.\r\n\r\nThis is a nagging issue which is very important for me to sort out due to an upcoming checkpoint for the [xView dataset challenge](http://xviewdataset.org/) (June 15). If anyone can shed some light on this I will greatly appreciate it.\r\n\r\nThanks in advance.", "The code from @ChristopherLucas should work.  Let's see the reason:\r\nThe  original code:\r\nif self._session is not None:\r\n try:\r\n   if self._created_with_new_api:\r\n     tf_session.TF_DeleteSession(self._session)\r\n   else:\r\n     tf_session.TF_DeleteDeprecatedSession(self._session)\r\n except AttributeError:\r\n   # At shutdown, `c_api_util` or `tf_session` may have been garbage\r\n   # collected, causing the above method calls to fail. In this case,\r\n   # silently leak since the program is about to terminate anyway.\r\n   pass\r\n self._session = None\r\n\r\nLook at the comments. What the comment said should also be suitable for the NoneType Error, I think. So the code from @ChristopherLucas should work perfectly.\r\n\r\nI did a similar change to the code and it works fine:\r\n      except (AttributeError, TypeError) as e:\r\n        # At shutdown, `c_api_util` or `tf_session` may have been garbage\r\n        # collected, causing the above method calls to fail. In this case,\r\n        # silently leak since the program is about to terminate anyway.\r\n        print('Error passed, Do not worry. `c_api_util` or `tf_session` may have been garbage collected. The error is ',e.args[-1])\r\n        pass", " i run this code \r\nimport tensorflow as tf    \r\n# read and decode the image\r\nimage_contents = tf.read_file(\"~/Desktop/test.jpg\")\r\nimage = tf.image.decode_jpeg(image_contents, channels=3)\r\n\r\nwith tf.Session() as sess:   \r\n    img = sess.run(image)\r\n    print(img)\r\nand give this flowing error how to fix it\r\nNotFoundError                             Traceback (most recent call last)\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1321     try:\r\n-> 1322       return fn(*args)\r\n   1323     except errors.OpError as e:\r\n\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1306       return self._call_tf_sessionrun(\r\n-> 1307           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1308 \r\n\r\n ", "For those using tensorflow as keras backend, you may add the following function to your code. It is also useful in case you need to manage your memory.\r\n```\r\nimport os\r\nimport importlib\r\nfrom keras import backend as K\r\n\r\ndef set_keras_backend(backend):\r\n    if K.backend() != backend:\r\n        os.environ['KERAS_BACKEND'] = backend\r\n        importlib.reload(K)\r\n        assert K.backend() == backend\r\n    if backend == \"tensorflow\":\r\n        K.get_session().close()\r\n        cfg = K.tf.ConfigProto()\r\n        cfg.gpu_options.allow_growth = True\r\n        K.set_session(K.tf.Session(config=cfg))\r\n        K.clear_session()\r\n\r\nset_keras_backend(\"tensorflow\")  \r\n```      ", "Thank you @rfgm6, I tried your suggestion.  The frequency of this bug has dropped from \"always\" to \"sometimes\", but unfortunately, for us, isn't a complete solution yet.\r\n", "I am getting the same error when loading two models in two different threads (one thread is for training and the other for predicting):\r\n\r\n> Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x00000000220AD7B8>>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\a703572\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\", line 1455, in __del__\r\n    self._session._session, self._handle, status)\r\n  File \"C:\\Users\\a703572\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\r\nException ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x00000000226AD390>>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\a703572\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\", line 1455, in __del__\r\n    self._session._session, self._handle, status)\r\n  File \"C:\\Users\\a703572\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\r\n\r\nHowever, there is no \"Process finished with exit code 0\" so i can keep up with the usual workflow, but I am afraid that in production this might actually crash at any time. I am still not sure if this error might come from using different graphs and sessions for predicting and training (this is the solution i found, together with some filelocks, for multithread in keras).\r\nI use the following code when doing any tf operations:\r\n\r\n                with graph_pred.as_default():\r\n                    with tf.Session(graph=graph):\r\n\r\nIs there any advance in this issue?", "Have you solved your problem? @adr-arroyo", "Actually no :(\r\nI guess it is because I force the use of several models. But I do not know how to avoid this exception yet", "i encounter this issue too. please see the log file as below\r\nException ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x0000018D3D47D0F0>>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Gakki\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\", line 1455, in __del__\r\n    self._session._session, self._handle, status)\r\n  File \"C:\\Users\\Gakki\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\r\nException ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x0000018D3D492CC0>>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Gakki\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\", line 1455, in __del__\r\n    self._session._session, self._handle, status)\r\n  File \"C:\\Users\\Gakki\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\r\nException ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x0000018D3E670048>>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Gakki\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\", line 1455, in __del__\r\n    self._session._session, self._handle, status)\r\n  File \"C:\\Users\\Gakki\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\r\nTrain on 60 samples, validate on 60 samples\r\nEpoch 1/10\r\n2019-04-01 14:50:46.963109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-04-01 14:50:46.963378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-04-01 14:50:46.963695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-04-01 14:50:46.963872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-04-01 14:50:46.964132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3857 MB memory) -> physical GPU (device: 0, name: Quadro P2000, pci bus id: 0000:01:00.0, compute capability: 6.1)", "I avoided some of the errors by removing this redundant code:\r\n\r\n    with tf.get_default_graph():\r\n        with tf.Session as sess:\r\n\r\nBut ocassionally the sesion keeps closing again.", "Getting the following error once after running my code seems to make it no longer runable\r\n\r\n`Exception ignored in: <function _CheckpointRestoreCoordinatorDeleter.__del__ at 0x0000027B7FEA34C0>\r\nTraceback (most recent call last):\r\n  File \"D:\\Users\\user\\anaconda3\\envs\\Prophet\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\", line 158, in __del__\r\nTypeError: 'NoneType' object is not callable`\r\n\r\nI've noticed that a 'Untitled Project' file gets created in my project directory after running my code, and deleting this file seems to make the code work again.\r\n\r\n![image_2021-08-02_004857](https://user-images.githubusercontent.com/87274844/127786276-35ff20c9-aa6b-47c4-bc5e-7736c1549468.png)"]}, {"number": 3387, "title": "mobile tensorflow", "body": "About mobile tensorflow ( https://www.tensorflow.org/mobile.html ), I want to download the inception\n(http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz), but it has been \ndamaged.\n\nThank you!\n", "comments": ["The download/extraction works for me; are you sure it's not a problem with your network connection or something like that?\n", "Same: works for me.  Closing for now.\n", "Thank you very much.\n"]}, {"number": 3386, "title": "Ubuntu 16.04", "body": "On Ubuntu 16.04 (gcc default is 5.3), \napt-get install cuda-toolkit installs the library files in usr/lib/x86_64-linux-gnu\ntensorflow prefers them in usr/local/cuda\n.configure allows you to change the directory\n\ncuDNN installs in /usr/local/cuda (more or less by default)\ntelling configure that cuDNN is in usr/local/cuda makes tf look for cuda in usr/local/cuda\n\nBefore you know it, you have cuda versions, nvidia versions, cuda folders etc. all over the place but tf not knowing any GPU!!\n", "comments": ["I have not had success getting TensorFlow GPU to work through apt-get install cuda-toolkit, hence I always install cuda through tarbar from NVidia website. That puts CUDA in /usr/local/cuda directory\n", "Yeah, I understand, but in 16.04, it is possible to set the default gcc to 4.9 to be able to build the .run file,  however, I ran into complaints about the 5.3 compiler again when building tf from source after that.  With the cuda .deb, tf doesn't know the gpu:0 the .deb possibly overwrites the nvidia 367 driver (partially) (or it lets you select whether to do so, it comes with 361 long term supported version I think).\n\nAt least, please update the TF website, the ubuntu install instruction is outdated, especially wrt 16.04 but also wrt cuDNN, because cuDNN 5 is no longer a  release candidate. It just gets spectacularly confusing, just following those instructions to the letter, they seem outdated for 16.04. You'll see SO full with questions again soon, just because versions are progressing and not thoroughly tested or explained.\n", "I get the gpu:0 now through the nvidia-docker solution\nPlease leave it open until the website is updated.\n", "@vrv: who is the point of contact of `os_setup.md`?\n", "there is no point of contact per-se, anyone can send a PR to add information there if it is suitably generic for an important class of systems that we support\n", "Related issue #2897 \n", "I just installed Cuda using apt-get and copy and pasted all the CudNN files into the usr folder and using the pip GPU installation everything worked dandy.  Just need to modify chmod and bash updates.\n\nI walked through the whole process here.\nhttps://www.youtube.com/watch?v=HM57GyqwGxA\n", "Looks like this issue was resolved.\r\nAlso there has been many updates to our scripts and documentation, so I think this issue got resolved, but it was not closed.\r\nPlease reopen if you still run into this problem."]}, {"number": 3385, "title": "Update probabilities for stratified_sampling", "body": "The function `tf.contrib.framework.sampling_ops.stratified_sample` doesn't accept probability update. Is it a bug or a normal behaviour ?\nBellow is a sample code that produce the error. The idea is to resample a label sub set from large dataset with large number of label (and unbalanced) and update the target prob after each iteration : \n\n```\ntarget_num_labels = 10\nnum_labels = 1000\n\np = [float(target_num_labels)/float(num_labels)]*num_labels\nlabel_sampler = tf.contrib.distributions.Bernoulli(p=p, dtype=tf.float32)\nlabel_samples = tf.squeeze(label_sampler.sample(1))\ntarget_prob = tf.div(label_samples, tf.reduce_sum(label_samples))\n\nimages, labels = ....\n\ninit_prob = ... # unbalanced label distribution from the data set\n[images], labels = tf.contrib.framework.stratified_sample([images], \n     labels, init_prob, target_prob, 256, enqueue_many=True)\n```\n", "comments": ["A few comments:\n1) `tf.contrib.distributions.Bernoulli` is not a Tensor. If it has a registered conversion to a Tensor then your code will be fine in that respect, but if not it should throw a graph-build error.\n2) The tensor `target_prob` represents \"Target class proportions in batch\". It should be a decimal number representing the target fraction per class. For example, if you have 3 classes, a 1-D Tensor with 3 elements would be acceptable. Its values could change each run of the graph, and it could take values like [.3, .4, .3], [.2, .7, .1], etc (note that these are all valid probability distributions).\n\nDoes that answer your question?\n", "Yes  this anwer my question. Finally, I use a non trainable variable to store the target_prob. Then with the tf.assign function I update the variable every run.\n", "Hi all, \nDid some of you use the stratified sampling function for a problem where we have a large number of classes. The current implementation is relatively slow in this situation and I'm trying to figure out a way to improve the speed in this situation.\n", "I have been able to use this Op with thousands of classes at roughly the same speed as without this Op, in certain cases. Can you copy an example of your usage?\n\nThere are 3 general sources of slowness in this op:\n1) Fixed cost of setting up extra Ops and Queues (small)\n2) Cost of rejecting examples (depends on the source and target distributions)\n3) Queueing delays ie enqueue Ops waiting for resources, queues being full, etc \n\n2) is controlled the value of the tensors you are passing the Op. The relevant information are the # of classes, source distribution, and target distribution. Can you share these with me to help debug?\n3) Is controlled by the parameters you pass at initialization (queue_capacity and threads_per_queue). Can you share these with me as well?\n", "@joel-shor  Here is what exactly I'm trying to do : I have about 2000 classes that are heavily unbalanced. There can be a factor of 30 between the least and the most populated classes. I need to create a batch with a given number of classes and a given number of data per class e.g 10 classes with 20 samples per class for a batch of 200 samples. In this case after each run, I begin by sampling 10 class labels and update the target probability accordingly. In the following you have a snippets of my codes : \n\n```\n# sample labels and update target prob\n# target_labels : number of label to select\n# num_labels : total number of labels\np = [float(target_labels)/float(num_labels)]\nlabel_sampler = tf.contrib.distributions.Bernoulli(p=p, dtype=tf.float32)\nlabel_samples = tf.squeeze(label_sampler.sample_n(num_labels))\nlabel_samples = tf.while_loop(lambda l: tf.less(tf.reduce_sum(l), 3),\n                                      lambda l: l,\n                                      [label_samples])\ntarget_label_prob = tf.div(label_samples, tf.reduce_sum(label_samples))\n\n\ninit_prob = np.random.binomial(1, target_labels/float(num_labels), num_labels)\ninit_prob = init_prob/np.sum(init_prob)\n\n# HACK : put the target prob in a non trainable variable to make update possible\ntarget_prob = tf.get_variable('target_prob',\n                                      trainable=False, shape=[num_labels],\n                                      initializer=tf.constant_initializer(init_prob, dtype=tf.float32))\nupdate_target_prob = tf.assign(target_prob, target_label_prob)\nupdate_target_prob = tf.group(update_target_prob,\n                                      tf.Print(target_prob, [target_prob, tf.reduce_sum(label_samples)],\n                                               message=\"Update probabilty \", first_n=50))\ntf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_target_prob)\n\n# data_prob is computed from the data set\n[imgs], lbls = tf.contrib.training.stratified_sample([images], labels, target_prob, batch_size,\n                                                                  init_probs=data_prob, enqueue_many=True,\n                                                                  threads_per_queue=num_threads)\n```\n", "If you are not careful, your target distributions can be very far from the original distribution, and you can spend a lot of time rejecting samples. This can take a lot of time even if the overhead of the Op is basically 0. Can you print out some examples of your initial and target distributions so we know it isn't that?\n"]}, {"number": 3384, "title": "Branch 127784684", "body": "", "comments": []}, {"number": 3383, "title": "R0.9", "body": "Thank all authors of these tensorflow tutorials!\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 3382, "title": "Updated makefile to use a more robust way of generating file lists", "body": "This should help reduce the frequency of breakages, since it uses a wildcard and exclusion list approach modeled on the Bazel rule, rather than a plain list of files.\n", "comments": []}, {"number": 3381, "title": "iOS Example Make Build Error", "body": "I am following the iOS static TF build on the \"Building By Hand\" section [found here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#building-by-hand).  I am running the build command after downloading dependencies and compiling successfully.\n\nWhen running `make -f tensorflow/contrib/makefile/Makefile  TARGET=IOS  IOS_ARCH=ARM64`, there is a generated error that stops the build.\n\n```\nerror:statement expression not allowed at file scope\n```\n\n```\nIn file included from tensorflow/core/kernels/xent_op.cc:20:\nIn file included from ./tensorflow/core/kernels/xent_op.h:20:\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\nIn file included from /Users/michaellee/Code/ios/tensorflow/tensorflow/contrib/makefile/downloads/eigen-eigen-b4fa9622b809/unsupported/Eigen/CXX11/Tensor:14:\nIn file included from /Users/michaellee/Code/ios/tensorflow/tensorflow/contrib/makefile/downloads/eigen-eigen-b4fa9622b809/unsupported/Eigen/CXX11/../../../Eigen/Core:354:\n/Users/michaellee/Code/ios/tensorflow/tensorflow/contrib/makefile/downloads/eigen-eigen-b4fa9622b809/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/NEON/Complex.h:286:35: error:\n      statement expression not allowed at file scope\nstatic uint64x2_t p2ul_CONJ_XOR = vld1q_u64( p2ul_conj_XOR_DATA );\n                                  ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../lib/clang/7.3.0/include/arm_neon.h:7624:39: note:\n      expanded from macro 'vld1q_u64'\n#define vld1q_u64(__p0) __extension__ ({ \\\n                                      ^\n1 error generated.\nmake: *** [/Users/michaellee/Code/ios/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/xent_op.o] Error 1\n```\n\nI have recreated this problem on two different computers, and on a freshly pulled version\n\nXcode 7.3.1, OS X 10.11.5\n", "comments": ["Issue resolved by the latest commit.\n"]}, {"number": 3380, "title": "Multi-GPU Example only using a single GPU", "body": "When running the CIFAR-10 example, it appears like TensorFlow is only utilizing one card - despite the fact that I have 4 cards installed in this machine (it is an NVIDIA DIGITS box). The script runs fine otherwise (can complete computation on a single card), but simply refuses to use more than one card. When attempting to use the script here: [https://github.com/aymericdamien/TensorFlow-Examples/blob/master/multigpu_basics.py](multigpu_basics.py) it appears as if Tensorflow allocates memory to one card, runs the computation, then allocates memory to a second card (in reverse numerical order, no less), and then runs the computation again, on that single card. Oddly, TensorFlow seems to see my cards in reverse order - when using \"/gpu:0\", it allocates work to the 4th card in the box. This is probably unrelated, but I mention it just in case. \n\nOperating System: Ubuntu 14.04\nuname -a output: `Linux nvidia-1 3.16.0-34-generic #47~14.04.1-Ubuntu SMP Fri Apr 10 17:49:16 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux`\nCIFAR example invocation: `cifar10_multi_gpu_train.py --num-gpus=2` - results are the same if I set that number between 2 and 4, inclusive.\n\nnvidia-smi output, before running the CIFAR example:\n\n``` +------------------------------------------------------+\n| NVIDIA-SMI 352.93     Driver Version: 352.93         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX TIT...  Off  | 0000:05:00.0      On |                  N/A |\n| 22%   36C    P8    17W / 250W |    256MiB / 12284MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX TIT...  Off  | 0000:06:00.0     Off |                  N/A |\n| 22%   38C    P8    16W / 250W |     23MiB / 12287MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |\n| 22%   37C    P8    17W / 250W |     23MiB / 12287MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  GeForce GTX TIT...  Off  | 0000:0A:00.0     Off |                  N/A |\n| 22%   35C    P8    15W / 250W |     23MiB / 12287MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1697    G   /usr/bin/X                                     174MiB |\n|    0      2983    G   compiz                                          55MiB |\n+-----------------------------------------------------------------------------+\n```\n\nNVIDIA-SMI under load:\n\n```\n+------------------------------------------------------+\n| NVIDIA-SMI 352.93     Driver Version: 352.93         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX TIT...  Off  | 0000:05:00.0      On |                  N/A |\n| 22%   38C    P8    16W / 250W |    364MiB / 12284MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX TIT...  Off  | 0000:06:00.0     Off |                  N/A |\n| 22%   41C    P8    16W / 250W |    138MiB / 12287MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |\n| 22%   39C    P8    17W / 250W |    138MiB / 12287MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  GeForce GTX TIT...  Off  | 0000:0A:00.0     Off |                  N/A |\n| 22%   45C    P2    88W / 250W |  11767MiB / 12287MiB |     90%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1697    G   /usr/bin/X                                     174MiB |\n|    0      2983    G   compiz                                          49MiB |\n|    0     11514    C   python                                         111MiB |\n|    1     11514    C   python                                         111MiB |\n|    2     11514    C   python                                         111MiB |\n|    3     11514    C   python                                       11740MiB |\n+-----------------------------------------------------------------------------+\n```\n\nI installed TensorFlow from source.\n`\npython -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\n0.9.0\n`\n\nThis is the Git commit hash that I got when pulling down TensorFlow:\n`a3f61c1d5c76339e6c9655dac426bb3822659772`\n", "comments": ["In the event that it's helpful, I installed the latest version of Hashcat (as I knew it's benchmark capability could detect and use multiple GPU's). Hashcat was able to detect and use all 4 GPU's within the system at the same time (as observed by nvidia-smi), leading me to think that this is a TensorFlow issue, either due to TensorFlow itself, or due to some error I introduced during installation.\n", "--num-gpus=2 won't set the flag to 2, just change the number in the code and test again.\n", "This worked. Thank you for the help.\n", "@zhangcx93 @zhanif3  thanks. but isn't this a bug?\n"]}, {"number": 3379, "title": "Tensorflow 0.9 HDF5", "body": "The Tensorflow 0.9 changelog mentions HDF5 support but I can't find any further details. What is actually implemented, is it possible to stream HDF5 files using the Tensorflow pipelines?\n", "comments": ["I did a search in the repo, perhaps the only thing is an usage example?  See `tensorflow/examples/skflow/hdf5_classification.py`.\n", "That example is probably broken as we are switching to learn.dataframe soon instead of data feeder. For now, you can probably load it into numpy. Stay tuned!\n", "Ok, closing for now. \n", "Loading into numpy doesn't work since I need out-of-core support, it seems the only way to achieve this now without IO/CPU blocking training is to convert to a TFRecord, is that correct?\n", "@tetmin: you can use QueueRunners, see: http://stackoverflow.com/questions/34594198/how-to-prefetch-data-using-a-custom-python-function-in-tensorflow\n", "@tomrunia  You suggestion just works for small datasets, which out-of-core support is not needed. For big dataset, it is not possible to load the entire dataset once, right? If there's is any mistake from my opinion, please let me know.\r\n\r\n@tetmin Hi, I have the same issue. And from my search, change hdf5 file to tfrecord or binary file will be the only feasible way. Did you find another way?"]}, {"number": 3378, "title": "Why not feed placeholders by name?", "body": "I couldn't find anything [in the documentation](https://www.tensorflow.org/versions/r0.9/how_tos/reading_data/index.html#feeding) on this. Is it possible to feed placeholders by name? If not, why? I think this would be useful to feed graphs after loading them from disk.\n\n``` python\nimport tensorflow as tf\nx = tf.placeholder(tf.float32, (None,), 'x')\ny = tf.reduce_sum(x)\nsess = tf.Session()\n\nsess.run(y, {x: [1, 2, 3]}\n# > 6.0\n\nsess.run(y, {'x': [1, 2, 3]}\n# > Cannot interpret feed_dict key as Tensor: The name 'x' refers to an operation,\n# > not a Tensor. Tensor names must be of the form \"<op_name>:<output_index>\".\n\nsess.run(y, {tf.get_default_graph().get_operation_by_name('x').outputs[0]: [1, 2, 3]})\n# > 6.0\n```\n", "comments": ["You need to add \":0\"\nprint sess.run(y, {'x:0': [1, 2, 3]})\n\nHere's some background on why the \":0\" is needed:\nhttp://stackoverflow.com/a/37870634/419116\n\nOn Tue, Jul 19, 2016 at 3:06 AM, Danijar Hafner notifications@github.com\nwrote:\n\n> I couldn't find anything in the documentation\n> https://www.tensorflow.org/versions/r0.9/how_tos/reading_data/index.html#feeding\n> on this. Is it possible to feed placeholders by name? If not, why?\n> \n> import tensorflow as tf\n> x = tf.placeholder(tf.float32, (None,), 'x')\n> y = tf.reduce_sum(x)\n> sess = tf.Session()\n> \n> sess.run(y, {x: [1, 2, 3]}# > 6.0\n> \n> sess.run(y, {'x': [1, 2, 3]}# > Cannot interpret feed_dict key as Tensor: The name 'x' refers to an operation,# > not a Tensor. Tensor names must be of the form \"<op_name>:<output_index>\".\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3378, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHC4Zw3XpE8IsALgIyYkLxdfAKhJrks5qXAbmgaJpZM4JPOjg\n> .\n", "Thanks.\n", "didn't work for me, but maybe try {x:1} instead of {\"x\":1} since it takes a dict of tensor as keys and values as values.", "@dataf3l  Try to figure out that your placeholder has the same name 'x'. First I try the method @yaroslavvb mentioned it didn't work, and I find out that the placeholder's default name is not 'x'."]}]