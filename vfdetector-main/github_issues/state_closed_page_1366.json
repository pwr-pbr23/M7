[{"number": 12093, "title": "Feature Request - Return final loss from tf.estimator.Estimator.train along with self.", "body": "### Describe the problem\r\nThings like hyper opt need to know the loss so that it can effectively pick the best hyper parameters for the model. Right now self is just returned, it is a one line change to the code since the loss is set the exact line before. I would be happy to initiate the PR myself assuming its wanted.\r\n\r\n### Source code / logs\r\nFrom:\r\n```\r\nloss = self._train_model(input_fn=input_fn, hooks=hooks)\r\nlogging.info('Loss for final step: %s.', loss)\r\nreturn self\r\n```\r\n\r\nTo:\r\n```\r\nloss = self._train_model(input_fn=input_fn, hooks=hooks)\r\nlogging.info('Loss for final step: %s.', loss)\r\nreturn self, loss\r\n```\r\n", "comments": ["I had a PR that might address your need as well: https://github.com/tensorflow/tensorflow/pull/12026", "Just checked it out, I suppose I could pull it from the written summary after the training was done, definitely a viable solution. It would still be easier to just grab it from the return of the function, but I understand that changing the signature of the function may not be what 's best for the project in terms of breaking peoples already written code. I'd like to see some more discussion on this. If it turns out that most of the people who chime in would prefer not to alter the signature then by all means I'd wait for yours to be merged in or even help out if anything needs doing. If people do like it in conjunction we could add it onto that pr. ", "So my work around for this at the moment is that I wrote a function that monkeypatches the function in the estimator at execution and returns the loss. Not pretty but it works for my purposes. I'd still like to see people's opinions on this. It seems completely reasonable to me for this function to return the final loss. ", "@jhseu Can you comment?  Thanks!", "You can add a custom session run hook with something like following:\r\n\r\n```\r\ndef after_run(self, run_context, run_values):\r\n  losses = run_context.session.graph.get_collection(\"losses\")\r\n  print(run_context.session.run(losses))\r\n```\r\nand then attach this to your estimator. ", "Again not disputing that works and I'm largely aware of methods like that, its just largely round about which seems like a bad api to me.  \r\n\r\nEdit: I'm also not insinuating that my work around that I'm using at the moment is not largely roundabout. My main point is that any work around that exists for this is wonky and indirect which to me is not great given there is what I think a clear path to a satisfactory resolution.", "Yeah this will be a huge compatibility break. @ispirmustafa Any thoughts on this since this is related to the PR I created/closed? ", "final training loss shows the loss on final mini batch. We do hyper parameter tuning by using result of evaluate not train. ", "Yup I was aware of that. Assuming loss has been steadily decreasing with no abnormality the final minibatch loss should still be just as useful for tuning assuming either overfitting is not an issue or you don't care about over fitting, which in my case is the latter. What I'm hearing though is that there is an easier way through an alternate method. I'd honestly forgotten I opened this suggestion till I just checked my alerts on here for another lib, my life hasn't been deeply halted by this and for that reason I'm going to close this since I just expect it to collect dust.", "@GrandathePanda \r\nPlease I am having the same issue. I'm quite surprised at how its so difficult to get my loss value out of the TF Estimator object. I have asked the question on stack overflow also, still awaiting answers (https://stackoverflow.com/questions/47757772/tensorflow-how-to-get-my-loss-value-from-tf-estimaor)\r\n\r\nPlease can you let me know the work around you used?", "Hey so while I monkey patched the original method to suit my purposes in that instance. I later found it was simpler (and more correct) to just grab loss on my evaluation passes. The .evaluate() method on the estimator returns a dictionary of metrics you can specify in your model function, in the estimator spec, https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec eval_metric_ops\r\n\r\n", "Thanks, @GrandathePanda , it worked!", "What function did you use with eval_metric_ops to get the loss?", "@GrandathePanda \r\nEstimatorSpec will return validation loss. Any idea how I can extract the training loss?", "> ### Describe the problem\r\n> Things like hyper opt need to know the loss so that it can effectively pick the best hyper parameters for the model. Right now self is just returned, it is a one line change to the code since the loss is set the exact line before. I would be happy to initiate the PR myself assuming its wanted.\r\n> \r\n> ### Source code / logs\r\n> From:\r\n> \r\n> ```\r\n> loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n> logging.info('Loss for final step: %s.', loss)\r\n> return self\r\n> ```\r\n> To:\r\n> \r\n> ```\r\n> loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n> logging.info('Loss for final step: %s.', loss)\r\n> return self, loss\r\n> ```\r\n\r\nIf there is appetite for this change a hacky solution would be to add a parameter ``return_loss = False`` then use\r\n```\r\nif return_loss:\r\n  return self,loss\r\nreturn self\r\n```\r\n\r\nbut that might be equally hacky"]}, {"number": 12092, "title": "Updating release notes known failures regarding Bazel 0.5.3", "body": "", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @vrv and @benoitsteiner to be potential reviewers.", "@tensorflow-jenkins test this please", "Linux GPU has changed for 1.3 we can merge this readily. Release tests duplicate this functionality.", "Can the actual fix for this be merged to 1.3 before release? \r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/11949\r\n\r\n0.5.3 is the default version installed by google's apt package (see below) so it would be very unfortunate for this to be a longer term issue!\r\n\r\nRight now this is the Bazel install:\r\n```\r\necho \"deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list\r\ncurl https://storage.googleapis.com/bazel-apt/doc/apt-key.pub.gpg | sudo apt-key add -\r\nsudo apt-get update\r\nsudo apt-get install -y bazel\r\nsudo apt-get upgrade bazel\r\n```\r\n\r\nWhich gives you this version:\r\n```\r\n) bazel version\r\nBuild label: 0.5.3\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Jul 28 08:34:59 2017 (1501230899)\r\nBuild timestamp: 1501230899\r\nBuild timestamp as int: 1501230899\r\n\r\n```", "The actual fix is cherry picked to r1.3 in https://github.com/tensorflow/tensorflow/pull/12248"]}, {"number": 12091, "title": "Compiling from source", "body": "I am trying to compile tensorflow from source as the pip version is compiled on older version of cuDNN \r\n\r\n### System information\r\n- **OS Platform and Distribution ** : Linux Ubuntu 16.04\r\n- **TensorFlow installed from** source:\r\n- **TensorFlow version (use command below)**: \r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:  Build label: 0.5.3\r\n- **CUDA/cuDNN version**: CUDA 8, cuDNN 7.0.1\r\n- **GPU model and memory**: GTX 1080 Ti and VRAM 11GB, RAM 32 GB\r\n- **Exact command to reproduce**:\r\nTrying to compile with   :  https://www.tensorflow.org/install/install_sources\r\n\r\n### Describe the problem\r\nUnable to compile from source. I tried the binary but it uses an old version of cuDNN I am using 7.0.1\r\n\r\n### Source code / logs\r\nERROR: /home/ntweat/tensorflow/tensorflow/stream_executor/BUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python2.7 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n    TF_CUDA_VERSION=8.0 \\\r\n    TF_CUDNN_VERSION=7.0.1 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n", "comments": ["@Ntweat Please include the full error log", "Hi\r\nIt was issue with cuDNN, I think Tensorflow does not work with cuDNN 7. \r\nI downgraded cuDNN to v6 and it compiled without any issues. ", "Ah yes, please see #12052 for the cuDNN 7 status", "Thank you \ud83d\udc4d  I will look out for updates on cuDNN 7 "]}, {"number": 12090, "title": "imagenet_distributed_train using inception v3 stuck on saving check points forever.", "body": "System information\r\n```bash\r\n== cat /etc/issue ===============================================\r\nLinux ip-172-30-4-87 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7 (Core)\"\r\nVERSION_ID=\"7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux ip-172-30-4-87 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.0)\r\nprotobuf (3.3.0)\r\ntensorflow-gpu (1.2.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.0\r\ntf.GIT_VERSION = v1.2.0-rc2-21-g12f033d\r\ntf.COMPILER_VERSION = v1.2.0-rc2-21-g12f033d\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nMon Aug  7 21:00:40 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.51                 Driver Version: 375.51                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           On   | 0000:00:1E.0     Off |                    0 |\r\n| N/A   51C    P0    72W / 149W |  10944MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      2886    C   /bin/python                                  10938MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n```\r\nI am running the imagenet_distributed_train.py of inception: https://github.com/tensorflow/models/tree/master/inception, with 16 AWS p2x2 machines. I didn't change any code of inception and follow the guidance to run imagenet_distributed_train using parallel-ssh.\r\n\r\nThe script I use to run parallel-ssh:\r\n```python\r\nfrom pssh.pssh_client import ParallelSSHClient\r\nimport datetime\r\nfrom pprint import pprint\r\nfrom pssh.utils import load_private_key\r\n\r\noutput_ps = []\r\noutput_worker = []\r\nsome host ip here\r\nps = [host1,host2,host3]\r\nworker = [host0,host1,host2,host3,host4,host5,host6,host7,host8,host9,host10,host11,host12,host13,host14,host15]\r\nclient_ps = ParallelSSHClient(ps, user='centos')\r\nclient_worker = ParallelSSHClient(worker, user='centos')\r\n\r\noutput_ps = client_ps.run_command('%s', host_args=(\r\n    ('/imagenet/run_ps.sh --job_name ps --task_id 0 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_ps.sh --job_name ps --task_id 1 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_ps.sh --job_name ps --task_id 2 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ))\r\n\r\noutput_worker = client_worker.run_command( '%s', host_args=(\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 0 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 1 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 2 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 3 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 4 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 5 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 6 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 7 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 8 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 9 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 10 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 11 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 12 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 13 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 14 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 15 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n       ))\r\n\r\nclient_ps.join(output_ps)\r\n#client_worker.join(output_worker)\r\npprint(output_ps.values()[0].exit_code)\r\n#pprint(output_worker.values()[0].exit_code)\r\n\r\nfor host, host_output in output_ps.items():\r\n    for line in host_output.stdout:\r\n        print(\"Host [%s] - %s\" % (host, line))\r\n```\r\nI think this script worked fine because I logged in every machine and checked with ps command and ensured the program was running with correct parameters. Then the program just worked fine but to some point, it started to save checkpoints forever(here is the output of worker0):\r\n\r\n```bash\r\nINFO:tensorflow:Worker 0: 2017-08-04 06:46:08.510727: step 2340, loss = 11.22(2.0 examples/sec; 15.788  sec/batch)\r\nINFO:tensorflow:Running Summary operation on the chief.\r\nINFO:tensorflow:Finished running Summary operation.\r\nINFO:tensorflow:Running Summary operation on the chief.\r\nINFO:tensorflow:Finished running Summary operation.\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Running Summary operation on the chief.\r\nINFO:tensorflow:Finished running Summary operation.\r\nINFO:tensorflow:Worker 0: 2017-08-04 06:53:55.553703: step 2370, loss = 10.30(2.1 examples/sec; 15.573  sec/batch)\r\nINFO:tensorflow:Running Summary operation on the chief.\r\nINFO:tensorflow:Finished running Summary operation.\r\nINFO:tensorflow:Running Summary operation on the chief.\r\nINFO:tensorflow:Finished running Summary operation.\r\nINFO:tensorflow:Worker 0: 2017-08-04 07:01:44.226068: step 2400, loss = 10.84(2.1 examples/sec; 15.421  sec/batch)\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Running Summary operation on the chief.\r\nINFO:tensorflow:Finished running Summary operation.\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\n(same saving checkpoint output forever)\r\n```\r\nI ran nvidia-smi and found the GPU wasn't working and same with other nodes. The output of worker 1-15 just stucked on step 2400 and didn't do any progress. I tried this several time on new set of 16 machines but it all stucked on saving checkpoint forever problem at some time. I guess it might be a bug in tensorflow? Or does this caused network failure? but it didn't retrun any network failure error.", "comments": ["From the looks of it, the saving checkpoint output thing is caused by this loop https://github.com/tensorflow/tensorflow/blob/2cfbd3347b943a389bb688125c2a90095b7735b5/tensorflow/python/training/supervisor.py#L1051 which should I think run every 10 minutes as the default for save_model_secs is 600. Is that what you're seeing? If so, then that isn't a concern.\r\n\r\nThe problem I guess is that your training is stuck and you're not seeing any steps beyond 2400. I would check the logs of the PS's and the workers to see if there is something fishy there... ", "@rohan100jain  Hi! Thank you for replying.  The logs you mean is what printed out when we run the program? Then it is super weird. Because all worker stop printing out logs after step 2400 and the worker0 just print out INFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt. \r\n\r\nThere is no error message in any of those worker's logs and no error message in PS's logs. \r\nI also try to run imagenet-distributed_train.py on 10 AWS instances (each instance has one GPU) with parameters: 3 PSs and 10 workers, it starts to have this check point forever problem after 900 steps and also no error messages.\r\nI try to: ping instance's IP. The network is ok.\r\n\r\nI try to look into the data save in check point directory but it is almost unreadable.  \r\nDo you have any ideas what the problem might be?", "The forever stop and make no progress problem also happens in benchmark code\r\n<img width=\"1440\" alt=\"screen shot 2017-08-10 at 10 45 14 pm\" src=\"https://user-images.githubusercontent.com/24256365/29202515-cbf7b034-7e1d-11e7-8e16-09e1b2336729.png\">\r\n\r\nThe command I use to run is: python tf_cnn_benchmarks.py --model=inception3 --eval=False --batch_size=64 --data_dir=/imagenet/data --trace_file=/home/centos/trace.json --summary_verbosity=0 --ps_hosts=ec2-54-209-254-203.compute-1.amazonaws.com:2221 --worker_hosts=ec2-174-129-166-55.compute-1.amazonaws.com:2222,ec2-54-144-65-162.compute-1.amazonaws.com:2222,ec2-54-209-254-203.compute-1.amazonaws.com:2222,ec2-54-161-80-220.compute-1.amazonaws.com:2222 --job_name=worker --task_index=0\r\n\r\nand change the job_name and task_index correspondingly.  But it stucks.....also....at 80 step of each machine, again no error messages..", "@ispirmustafa , are you able to take a look? Perhaps recommend a move to MonitoredSession?", "We're using MonitoredSession instead of Supervisor. Could you please give it a shot? it may provide you a better error message.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "closing due to inactivity."]}, {"number": 12089, "title": "Revert \"Fix segfault when recording raw allocation returns nullptr\"", "body": "Reverts tensorflow/tensorflow#12074, in favour of a complete fix in 9bb2c8e.", "comments": ["Can one of the admins verify this patch?", "@byronyi, thanks for your PR! By analyzing the history of the files in this pull request, we identified @asimshankar, @keveman and @tensorflower-gardener to be potential reviewers.", "@tensorflow-jenkins test this please", "@byronyi Why do you revert instead of just sending the updated version?", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (25/25 (100%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from github-smtp2-ext8.iad.github.net ([192.30.252.199]:40522 helo=github-smtp2a-ext-cp1-prd.iad.github.net)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES256-GCM-SHA384:256)\n\t(Exim 4.89)\n\t(envelope-from <noreply@github.com>)\n\tid 1deqxj-0001Yt-V0\n\tfor mazecreator@mazecreator.com; Mon, 07 Aug 2017 17:57:58 -0500\nDate: Mon, 07 Aug 2017 16:08:21 -0700\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=github.com;\n\ts=pf2014; t=1502147301;\n\tbh=MkNa3dU7IUf8eY3AoIRE53uhMZ7DOm2t7aiev6kpJFQ=;\n\th=From:Reply-To:To:Cc:In-Reply-To:References:Subject:List-ID:\n\t List-Archive:List-Post:List-Unsubscribe:From;\n\tb=hx7lEcfD49MtvZHOpzOKcaoCnyPkDGZBVKG1QoklTh6zW6Qd02gYvL6BNWeBJbapI\n\t 6d3+/+g/mSCAS8/Z8orMR/rCT0jG9MdID+oBcvxYG+RtrMq7JcfCjsgAGu9ZvHb6IR\n\t xh/1XqRAng1wZB3IlTQBf68hKCr+bGyCP26BVtgA=\nFrom: Rasmus Munk Larsen <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/pull/12089/c320804507@github.com>\nIn-Reply-To: <tensorflow/tensorflow/pull/12089@github.com>\nReferences: <tensorflow/tensorflow/pull/12089@github.com>\nSubject: Re: [tensorflow/tensorflow] Revert \"Fix segfault when recording raw\n allocation returns nullptr\" (#12089)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5988f2e57ab16_2c6583fa55da15c383053d\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: rmlarsen\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a031fdefe3fee2efdf73bd32e5989c218d7ab721f292cf0000000115a0b4e592a169ce0ed08a70@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoItgyVzaA19wrxiYjBx9tBl18R3Cks5sV5jlgaJpZM4OwBPW>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n\n----==_mimepart_5988f2e57ab16_2c6583fa55da15c383053d\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n@byronyi Why do you revert instead of just sending the updated version?\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/pull/12089#issuecomment-320804507\n----==_mimepart_5988f2e57ab16_2c6583fa55da15c383053d\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p><a href=\"https://github.com/byronyi\" class=\"user-mention\">@byronyi</a> Why do you revert instead of just sending the updated version?</p>\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/pull/12089#issuecomment-320804507\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoPJyxcUBIv1TVYvcRihG29ex3Inmks5sV5jlgaJpZM4OwBPW\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoESvTI87FMJ5dt4VxnRow3nDtQRZks5sV5jlgaJpZM4OwBPW.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/pull/12089#issuecomment-320804507\"></link>\n  <meta itemprop=\"name\" content=\"View Pull Request\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Pull Request on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@rmlarsen in #12089: @byronyi Why do you revert instead of just sending the updated version?\"}],\"action\":{\"name\":\"View Pull Request\",\"url\":\"https://github.com/tensorflow/tensorflow/pull/12089#issuecomment-320804507\"}}}</script>\n----==_mimepart_5988f2e57ab16_2c6583fa55da15c383053d--\n", "Since the current master is not broken with this bug (it does not track CPU allocations by default), and only broken with GDR enabled in #11392 (I registered a tracking CPU allocator to be the default one), I think it would be more suitable to put this fix into the patch.", "Ok I will close this and merge everything in the other PR. Sorry again."]}, {"number": 12088, "title": "TENSORFLOW CUDNN_HOME NOT USED", "body": "When I build with cmake on windows10, there is always a warning:\r\n\r\nCMake Warning:\r\n  Manually-specified variables were not used by the project:\r\n\r\n    CUDNN_HOME\r\n\r\nIf I just ignore it, there will be fatal errors about missing pb.h files. Can anyone help?", "comments": []}, {"number": 12087, "title": "Update variables.py", "body": "Added deprecated message", "comments": ["I have been looking at the core training framework. It seems that the other functions such as `get_global_step` are also deprecated. Should I add the deprecated comments for them too?", "@martinwicke should we add deprecation statements for all these function?", "Yes. For every function for which a direct equivalent exists outside of contrib, we should deprecate the contrib version.\r\n\r\nPlease (also in this PR) set the date to `None`. Timelines confuse people, especially if they're in the past.\r\n\r\nThis may lead to spurious warnings, which we'll have to fight with the `with deprecation.silence` decorator, but we can do that in a follow-up.", "@tensorflow-jenkins test this please"]}, {"number": 12086, "title": "Cannot learn initial_states with batch_sequences_with_states", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18\r\n- **TensorFlow installed from (source or binary)**: Binary (pip)\r\n- **TensorFlow version (use command below)**: v1.3.0.0rc0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nUsing `tf.contrib.training.batch_sequences_with_states`, it seems impossible to learn the `initial_states` passed to it. I've tried using Variables with `trainable=True` with several initializers and while my optimizer picks them up through `tf.trainable_variables()`, their values do not get learned.\r\n\r\nI assume this has something to do with TensorFlow continuing to work with the Tensors, not the Variables? Additionally, the initializer is called each run and in TensorBoard it shows that it does not feed into the optimizer, whereas network weights for example would.\r\n\r\nAre my presumptions correct, and if not, how do I fix this? If this is a TensorFlow limitation, how could I work around it?\r\n\r\n### Source code / logs\r\n```\r\n            initializers = {\r\n                    'lstm_c': train_init,\r\n                    'lstm_h': train_init,\r\n                    'encode_lstm_c': train_init,\r\n                    'encode_lstm_h': train_init,\r\n                    'last_out': train_init,\r\n                    }\r\n\r\n            with tf.variable_scope(tf.get_variable_scope(), reuse=not is_training):\r\n\r\n                initial_states = { k: tf.get_variable('initial_{}'.format(k), v.shape,\r\n                        dtype=v.dtype,\r\n                        initializer=initializers[k],\r\n                        trainable=is_training and self.learn_initial_states, \\\r\n                        collections=[attend.GraphKeys.INITIAL_STATES]) \\\r\n                        for k, v in initial_constants.items() }\r\n\r\n\r\n                # This makes sure we're not dealing with the reference but learned values\r\n                initial_states = { k: v.initialized_value() for k, v in initial_states.items() }\r\n```\r\n", "comments": ["I'm not sure I completely understand your question... The documentation for NextQueuedSequenceBatch (object returned from tf.contrib.training.batch_sequences_with_states) seems to suggest that the state() and save_state() methods could be used to see / store the values of the states. \r\n\r\nAlso, I think this question might be more suitable for StackOverflow. ", "I agree I worded it as a SO question but that's only because of my frustration to get this to work and an inability to understand the inner workings of variables in this scenario. In all other places I would just create a variable and multiple it for example and the optimizer would know what to do. In this scenario it's a lot less clear and I can't use variables inside `initial_states` passed to `batch_sequences_with_state`,  I have to get their `initialized_value`. \r\n\r\nI would also like to discuss this use case if it turns out it's currently not supported and request support for it, or discuss the workaround that should commonly be adopted. Learning initial values for for example recurrent layers is a fairly common practice I've gathered. ", "You are right the initial_states cannot be learned with the SequenceQueueingStateSaver that is constructed in batch_sequences_with_states.\r\n\r\nNo gradients can flow through the queue. This is a limitation. The documentation could be a bit clearer but it mentions \"e.g. constants or tensors\" for the initial state.\r\n\r\nThe only way around this is not using the SequenceQueueingStateSaver. For instance, how about training the initial state with the dynamic_rnn on small enough (pre-segmented) sequences? Then you can switch to the state_saving_rnn with the SequenceQueueingStateSaver with the pre-trained initial state.", "I have also tried something with `tf.cond` and `tf.map_fn` to conditionally use variables instead of the tensor states provided by the state saver when it notices a certain batch is the first one in its sequence, but to no avail.\r\n\r\nPre-training indeed sounds like the only way to go about this. Not a bad idea at all, thanks. I'd still hoped to make this work end-to-end.", "Never mind, my last attempt didn't work because I messed up variable initialization.\r\n\r\nThe way I worked around this is as follows. I keep a `first` boolean in my state saver which is initialized to `True` and gets set to `False` after the first time a batch is encountered.\r\nI keep a separate set of to-be-learned initial states. The trick is to only use them during the first batch for a sequence. I do this by creating a mask from `first`, expanding the initial state vector to the correct batch size, masking it, then adding it to the state saver's initial states, effectively replacing all initial zero rows and only those. This will include the initial state variables in the computation graph making them optimizable.\r\n\r\n```\r\n             c = state_saver.state('lstm_c')\r\n             first = state_saver.state('first')\r\n\r\n             only_first_mask = tf.expand_dims(first, 1) # Mask rows\r\n             not_first_mask = tf.logical_not(only_first_mask)\r\n             only_first_mask = tf.cast(only_first_mask, tf.float32)\r\n             not_first_mask = tf.cast(not_first_mask, tf.float32)\r\n\r\n             # learnable initial variable\r\n             init_c = provider.initial_variables['lstm_c']\r\n             # B x 1 * 1 x 512 -> 512 x 512 (repeats row)\r\n             # Repeats the initial variable `batch_size` times\r\n             init_c = tf.einsum('ij,jk->ik', tf.ones([batch_size,1]), tf.expand_dims(init_c,0))\r\n             # Mask the initial variable\r\n             init_c = only_first_mask * init_c\r\n             c = not_first_mask * c # Has first=true set to zeros\r\n             # This now contains entries from `init_c` in the rows corresponding to `first`\r\n             c = c + init_c\r\n```\r\n\r\n\r\n"]}, {"number": 12085, "title": "Branch 164481383", "body": "", "comments": ["@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 12084, "title": "InvalidArgumentError mobilenet", "body": "im getting the following error while training my own dataset with mobilenet\r\n\r\n```\r\nCaused by op 'MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise', defined at:\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 1326, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 984, in main\r\n    create_model_graph(model_info))\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 282, in create_model_graph\r\n    model_info['resized_input_tensor_name'],\r\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 287, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[\"SAME\", \"VALID\"]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)\r\n\t [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)]]\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 1326, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 1025, in main\r\n    bottleneck_tensor, FLAGS.architecture)\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 476, in cache_bottlenecks\r\n    resized_input_tensor, bottleneck_tensor, architecture)\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 418, in get_or_create_bottleneck\r\n    bottleneck_tensor)\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 373, in create_bottleneck_file\r\n    str(e)))\r\nRuntimeError: Error during processing file tensorflow/examples/image_retraining/dataset/female/2Q== (1).jpg (NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[\"SAME\", \"VALID\"]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)\r\n\t [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)]]\r\n\r\nCaused by op 'MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise', defined at:\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 1326, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 984, in main\r\n    create_model_graph(model_info))\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 282, in create_model_graph\r\n    model_info['resized_input_tensor_name'],\r\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 287, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[\"SAME\", \"VALID\"]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)\r\n\t [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)]]\r\n)\r\n\r\n```", "comments": ["This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (29/25 (115%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\n------ The body of the message is 14155 characters long; only the first\n------ 5000 or so are included here.\nReceived: from o7.sgmail.github.com ([167.89.101.198]:58440)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES128-GCM-SHA256:128)\n\t(Exim 4.89)\n\t(envelope-from <bounces+848413-e6d9-mazecreator=mazecreator.com@sgmail.github.com>)\n\tid 1demEA-0003IL-6V\n\tfor mazecreator@mazecreator.com; Mon, 07 Aug 2017 12:54:36 -0500\nDKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=github.com; \n\th=from:reply-to:to:cc:subject:mime-version:content-type:content-transfer-encoding:list-id:list-archive:list-post:list-unsubscribe; \n\ts=s20150108; bh=0AINoU+s8aDu8A0Euo5ZQRyHbE0=; b=EzM3/8YjKMtskB3A\n\tUuz9VclXz+GKFnD8nBjACKldiL5cNmTzlbOSeybFUJaW5JOl/tMEvQJrt598AU6o\n\triweKno2pROgbeh7llr6cghvCEvO+lDdMKGSVYQZaU/luSXg6cbedfvG+GQHNbQ2\n\taVm0SGy0Ybxr6ZSp2PgFaK2C40Y=\nReceived: by filter0560p1mdw1.sendgrid.net with SMTP id filter0560p1mdw1-32731-5988ABC7-53\n        2017-08-07 18:04:55.744833811 +0000 UTC\nReceived: from github-smtp2a-ext-cp1-prd.iad.github.net (github-smtp2a-ext-cp1-prd.iad.github.net [192.30.253.16])\n\tby ismtpd0026p1mdw1.sendgrid.net (SG) with ESMTP id CGugROtmTEae8rmw3IRN4w\n\tfor <mazecreator@mazecreator.com>; Mon, 07 Aug 2017 18:04:55.716 +0000 (UTC)\nDate: Mon, 07 Aug 2017 18:04:55 +0000 (UTC)\nFrom: Zumbalamambo <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/issues/12084@github.com>\nSubject: [tensorflow/tensorflow] InvalidArgumentError mobilenet (#12084)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5988abc75869b_56c33f960b269c343214fc\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: Zumbalamambo\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a0d811eedc9184dc8b30985248a1b01c674f1c00a192cf0000000115a06dc792a169ce0ecf9974@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoHnYeWCJjjrK72r5VmuPhqH-sc8mks5sV1HHgaJpZM4OvwW0>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-SG-EID: BJ4qYjf5a3yL0lCrdDNghY4YYR+k1a+cluU6wEX1JqzNZWXWEFaH8hGjU0O8hKhtFDv+vvw/R5lWtx\n asQrB9bBlEf85TCRJuH9mke3Kn0+BtCPeGnMxpG1oa6GKrDAqzrcFF2pe08VCE5pieaMFnNCxr7har\n jYTLf3SjDoOZWGSv64y1N+prV+9duqx9GDsZu33Ds4SsArtqzFg7mnFwPZcdK5GrV+DQ0DliBtfusL\n E=\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n----==_mimepart_5988abc75869b_56c33f960b269c343214fc\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\nim getting the following error while training my own dataset with mobilenet\n\n```\nCaused by op 'MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise', defined at:\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 1326, in <module>\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 44, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 984, in main\n    create_model_graph(model_info))\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 282, in create_model_graph\n    model_info['resized_input_tensor_name'],\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 287, in import_graph_def\n    op_def=op_def)\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[\"SAME\", \"VALID\"]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)\n\t [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)]]\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 1326, in <module>\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 44, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 1025, in main\n    bottleneck_tensor, FLAGS.architecture)\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 476, in cache_bottlenecks\n    resized_input_tensor, bottleneck_tensor, architecture)\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 418, in get_or_create_bottleneck\n    bottleneck_tensor)\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 373, in create_bottleneck_file\n    str(e)))\nRuntimeError: Error during processing file tensorflow/examples/image_retraining/dataset/female/2Q== (1).jpg (NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[\"SAME\", \"VALID\"]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)\n\t [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read)]]\n\nCaused by op 'MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise', defined at:\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 1326, in <module>\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 44, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 984, in main\n    create_model_graph(model_info))\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 282, in create_model_graph\n    model_info['resized_input_tensor_name'],\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 287, in import_graph_def\n    op_def=op_def)\n  File \"/Users/zumbala/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_origi\n", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!\r\n\r\nPlease also provide the exact command you issued for execution.", "Hi @rohan100jain\r\nI have the same problem.\r\nI was running this code on GCP:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py\r\n\r\nthe version of tensorflow I used: \r\n1.2.1\r\n\r\nThanks a lot.", "@rohan100jain  tensorflow 1.2.1\r\nwindows 10\r\n8 gb ram, cpu\r\n\r\nthats the spec", "Please follow the [new issue template](https://github.com/tensorflow/tensorflow/issues/new). We need more information in order to be able to help.", "Hi I am having a similar issue in which prediction on GCP ML-ENGINE gives an error related to the \"data_format\" attr using mobilenet_v1 from the tutorials:\r\n\r\nTried asking in StackOverflow but nothing so far\r\nhttps://stackoverflow.com/questions/45953344/getting-error-on-ml-engine-predict-but-local-predict-works-fine", "I am trying to run mobilenet model trained over imagenet on Android Tensorflow for object recognition and facing the issue as described below.\r\n\r\n```\r\n TensorFlowInferenceInterface: Failed to load model \r\n from 'file:///android_asset/mobilenet_imagenet.pb': java.io.IOException: \r\n Not a valid TensorFlow Graph serialization: \r\n NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative;\r\n signature=input:T, filter:T -> output:T; \r\n attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); \r\n attr=padding:string,allowed=[\"SAME\", \"VALID\"]>; \r\n NodeDef: conv_dw_1/depthwise = \r\n DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1]]\r\n (conv1_relu/clip_by_value, conv_dw_1/depthwise_kernel/read)\r\n\r\n```\r\nI followed the tutorial as given in the [link](https://blog.mindorks.com/android-tensorflow-machine-learning-example-ff0e9b2654cc) to integrate Tensorflow on Android. I could run basic CNN classifier custom trained (using python 2.7.12, Tensorflow 1.2 on Ubuntu 16.04) and it is working fine.\r\n\r\nI could successfully run Mobilenet trained over imagenet on python 2.7.12 on Ubuntu 16.04 using Tensorflow 1.2 and 1.3. Now when I try to run the same '.pb' model on Android it gives me the error as mentioned above.\r\n \r\nThe code for initializing the model is as given below :\r\n\r\n   ```Java\r\nTensorFlowImageClassifier c = new TensorFlowImageClassifier();\r\n   c.inferenceInterface = new TensorFlowInferenceInterface();\r\n   if (c.inferenceInterface.initializeTensorFlow(assetManager, modelFilename) != 0) {\r\n            throw new RuntimeException(\"TF initialization failed\");\r\n   }\r\n\r\n```\r\nPlease suggest me solution or work around. Thank you", "The above issue has been resolved by updating the \"libandroid_tensorflow_inference_java.jar\" and \"libtensorflow_inference.so\" to the latest version i.e. Tensorflow - 1.4. The latest \".jar\" and \".so\" files can be found [here](https://ci.tensorflow.org/view/Nightly/job/nightly-android/).\r\n\r\nThe issue was due to mismatch of Tensorflow versions. Mobilenet classifier trained over ImageNet was built on Tensorflow - 1.3 and was being used for inference on Android with Tensorflow - 1.1.\r\n\r\nThe stackoverflow answer can be found [here](https://stackoverflow.com/a/47493624/7665986)", "Assigning to @petewarden for further triage.", "Looks like this is resolved, so closing."]}, {"number": 12083, "title": "tf.contrib.data.Iterator.make_initializer operation lacks name", "body": "I'm using tf.contrib.data for my input pipeline. Since it feeds the model without placeholders, I need to be able to reload both the model and input pipeline in order to resume training later. However, the make_initializer method of tf.contrib.data.Iterator does not have a name argument like most (all?) other Tensorflow operations, making it impossible (or at best, error-prone) to find these ops in the reloaded graph. Unless I'm mistaken, this seems like a standard use-case and the missing name argument is inconsistent with the rest of Tensorflow.", "comments": ["This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (27/25 (108%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from o7.sgmail.github.com ([167.89.101.198]:21528)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES128-GCM-SHA256:128)\n\t(Exim 4.89)\n\t(envelope-from <bounces+848413-e6d9-mazecreator=mazecreator.com@sgmail.github.com>)\n\tid 1dem6p-00010w-TV\n\tfor mazecreator@mazecreator.com; Mon, 07 Aug 2017 12:47:01 -0500\nDKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=github.com; \n\th=from:reply-to:to:cc:subject:mime-version:content-type:content-transfer-encoding:list-id:list-archive:list-post:list-unsubscribe; \n\ts=s20150108; bh=vIpSQZRSGLJvoZmWvy+whtLSUTc=; b=i+ggCBd9VC6vFnfc\n\tqldH1BzUgaRq4OkldPzk3EOwj/R/SpUVnoIKeqoSVqAiiqDTym0k6D8NRqQls7hd\n\tnr4DpbkGDj8zxvANEruFDyCfX2cLpUUO4f2mqxF6r+23W/lhAa/AC7BRu9qVekOj\n\tMILauzAc3ZzgjH6A1g/Niv7Io54=\nReceived: by filter0932p1mdw1.sendgrid.net with SMTP id filter0932p1mdw1-25425-5988AA04-31\n        2017-08-07 17:57:24.726128496 +0000 UTC\nReceived: from github-smtp2a-ext-cp1-prd.iad.github.net (github-smtp2a-ext-cp1-prd.iad.github.net [192.30.253.16])\n\tby ismtpd0040p1mdw1.sendgrid.net (SG) with ESMTP id E9ME3ay9QwyT__Q6MmKBNw\n\tfor <mazecreator@mazecreator.com>; Mon, 07 Aug 2017 17:57:24.677 +0000 (UTC)\nDate: Mon, 07 Aug 2017 17:57:24 +0000 (UTC)\nFrom: Jason Taylor <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/issues/12083@github.com>\nSubject: [tensorflow/tensorflow] tf.contrib.data.Iterator.make_initializer\n operation lacks name (#12083)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5988aa02f1656_43c73fc0fd359c303321b3\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: jrbtaylor\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a063414c5212a6078372bf55886f5b2cd72f9175e692cf0000000115a06c0292a169ce0ecf945f@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoEmFZkGuMX4w59qZPYgNfWjUxNSMks5sV1ACgaJpZM4OvwAD>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-SG-EID: BJ4qYjf5a3yL0lCrdDNghY4YYR+k1a+cluU6wEX1JqyrUTrsHGEsw3btqicPDe7W58SJ1M1LpZOhvc\n s399Ilj8f4ZNxb5kuo+3HpHYGxW6XbRpzhOQxtN80WqIpeSpvND2ZXxBrl/neQE3pfHCkqlhFL7GFL\n mdyrnn6EmFeFiOIbA6d1OcyLqdrJwQTm2ogU88gf9GefOEPwSyf4zibZYhIubyMaEiCSsm6U0VnDu9\n A=\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n----==_mimepart_5988aa02f1656_43c73fc0fd359c303321b3\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\nI'm using tf.contrib.data for my input pipeline. Since it feeds the model without placeholders, I need to be able to reload both the model and input pipeline in order to resume training later. However, the make_initializer method of tf.contrib.data.Iterator does not have a name argument like most (all?) other Tensorflow operations, making it impossible (or at best, error-prone) to find these ops in the reloaded graph. Unless I'm mistaken, this seems like a standard use-case and the missing name argument is inconsistent with the rest of Tensorflow.\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/issues/12083\n----==_mimepart_5988aa02f1656_43c73fc0fd359c303321b3\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p>I'm using tf.contrib.data for my input pipeline. Since it feeds the model without placeholders, I need to be able to reload both the model and input pipeline in order to resume training later. However, the make_initializer method of tf.contrib.data.Iterator does not have a name argument like most (all?) other Tensorflow operations, making it impossible (or at best, error-prone) to find these ops in the reloaded graph. Unless I'm mistaken, this seems like a standard use-case and the missing name argument is inconsistent with the rest of Tensorflow.</p>\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/issues/12083\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoGyU1Z0R5mBKSwbbmSIjVc8hxmhwks5sV1ACgaJpZM4OvwAD\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoCvYsW95XN5aYu1_tVPIK8l8pPGxks5sV1ACgaJpZM4OvwAD.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/issues/12083\"></link>\n  <meta itemprop=\"name\" content=\"View Issue\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Issue on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"DESCRIPTION\",\"message\":\"tf.contrib.data.Iterator.make_initializer operation lacks name (#12083)\"}],\"action\":{\"name\":\"View Issue\",\"url\":\"https://github.com/tensorflow/tensorflow/issues/12083\"}}}</script>\n----==_mimepart_5988aa02f1656_43c73fc0fd359c303321b3--\n", "A general solution would be something similar to tf.identity(..., name) but for operations.", "Could this be added along the lines of https://github.com/tensorflow/tensorflow/pull/11510 ?", "Added PR #12122.", "lol, that was fast"]}, {"number": 12082, "title": "AttributeError", "body": "Im getting the following error while training mobile net model as\r\n\r\n```\r\n python retrain.py \\\r\n    --image_dir ~/dataset --architecture mobilenet_1.0_224\r\n```\r\n\r\n\r\nFollowing is the error..\r\n\r\n```\r\n _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"retrain.py\", line 989, in main\r\n    class_count = len(image_lists.keys())\r\nAttributeError: 'NoneType' object has no attribute 'keys'\r\n```", "comments": ["This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (28/25 (112%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from o7.sgmail.github.com ([167.89.101.198]:19977)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES128-GCM-SHA256:128)\n\t(Exim 4.89)\n\t(envelope-from <bounces+848413-e6d9-mazecreator=mazecreator.com@sgmail.github.com>)\n\tid 1dem78-000137-6n\n\tfor mazecreator@mazecreator.com; Mon, 07 Aug 2017 12:47:20 -0500\nDKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=github.com; \n\th=from:reply-to:to:cc:subject:mime-version:content-type:content-transfer-encoding:list-id:list-archive:list-post:list-unsubscribe; \n\ts=s20150108; bh=sxb50LBJRvieO1wieg58SOe0iLs=; b=p3IntJbwfhhPGeJ0\n\tJTR26vfjtl+1ACjgAwIiuBTFZXFZE/p6uv3UMWRhXV42PXMTFOw3u5soQaQgeG+5\n\togGFUmkSKk/ztxHYQDqnIB4FUsYXq/V5DiFxCviU1q7+1/qKe8sfIQUGS2PX4VeS\n\tYA1nkyAe9zTmzzATzTAsLPUP7Jc=\nReceived: by filter0944p1mdw1.sendgrid.net with SMTP id filter0944p1mdw1-28386-5988AA08-31\n        2017-08-07 17:57:28.3777779 +0000 UTC\nReceived: from github-smtp2b-ext-cp1-prd.iad.github.net (github-smtp2b-ext-cp1-prd.iad.github.net [192.30.253.17])\n\tby ismtpd0027p1mdw1.sendgrid.net (SG) with ESMTP id If3CCDphSxaBaCCZQ3rS2Q\n\tfor <mazecreator@mazecreator.com>; Mon, 07 Aug 2017 17:57:28.334 +0000 (UTC)\nDate: Mon, 07 Aug 2017 17:57:28 +0000 (UTC)\nFrom: Zumbalamambo <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/issues/12082@github.com>\nSubject: [tensorflow/tensorflow] AttributeError (#12082)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5988aa07e705_71733fd13eba3c3c3874cc\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: Zumbalamambo\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a06afccaced202c3ee8361ee738ebf55094922fd1b92cf0000000115a06c0792a169ce0ecf9449@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoILaycVoankcWQoCB2NrvMdVRT8dks5sV1AHgaJpZM4Ovv_s>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-SG-EID: BJ4qYjf5a3yL0lCrdDNghY4YYR+k1a+cluU6wEX1JqxWO5mQzBFahGbzCH/mse9M+wxsWmYN5VfZDv\n 6EcAtlQYs7S9u6owsaX6xIbB8oEXAxyoCqnsa1VU+RCzFKRJIhP//tP4zSlxIBp5ew52+9GqsZXKHG\n TrSHzRM1wm4WeBd0WpDEZfIRp72dAI5ihcGBLsP+POmezLjlRUzWG+PSLPmSs/3fjyDSJyrY01nP00\n g=\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n----==_mimepart_5988aa07e705_71733fd13eba3c3c3874cc\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\nIm getting the following error while training mobile net model as\n\n```\n python retrain.py \\\n    --image_dir ~/dataset --architecture mobilenet_1.0_224\n```\n\n\nFollowing is the error..\n\n```\n _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"retrain.py\", line 989, in main\n    class_count = len(image_lists.keys())\nAttributeError: 'NoneType' object has no attribute 'keys'\n```\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/issues/12082\n----==_mimepart_5988aa07e705_71733fd13eba3c3c3874cc\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p>Im getting the following error while training mobile net model as</p>\n<pre><code> python retrain.py \\\n    --image_dir ~/dataset --architecture mobilenet_1.0_224\n</code></pre>\n<p>Following is the error..</p>\n<pre><code> _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"retrain.py\", line 989, in main\n    class_count = len(image_lists.keys())\nAttributeError: 'NoneType' object has no attribute 'keys'\n</code></pre>\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/issues/12082\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoHOULoQtS309vRJDjCyyxBUAMcuvks5sV1AHgaJpZM4Ovv_s\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoFHSnEgHgxHaf22Q_I7w33B0Ukkgks5sV1AHgaJpZM4Ovv_s.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/issues/12082\"></link>\n  <meta itemprop=\"name\" content=\"View Issue\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Issue on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"DESCRIPTION\",\"message\":\"AttributeError (#12082)\"}],\"action\":{\"name\":\"View Issue\",\"url\":\"https://github.com/tensorflow/tensorflow/issues/12082\"}}}</script>\n----==_mimepart_5988aa07e705_71733fd13eba3c3c3874cc--\n", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!\r\n\r\nAlso in this particular case, would be helpful to know which retrain.py were you using."]}, {"number": 12081, "title": "Tensorflow bezel command line c options does not pass down to gcc if static object is compiled", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nCentOS 7.1\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.3 rc1\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.5.1\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n- **GPU model and memory**:\r\nn/a\r\n- **Exact command to reproduce**:\r\nbazel build --copt=\"-DEIGEN_USE_MKL_VML\" -c opt //tensorflow/tools/pip_package:build_pip_package -s\r\n\r\n### Describe the problem\r\n\r\n\r\nWhen I compile TensorFlow with bazel, I found the c option I put in the bazel command line only passed to the object file for a dynamic library (with -fPIC option), not the object file for a static library (without -fPIC).\r\n\r\nFor example, I ran the following command:\r\n\r\nbazel build --copt=\"-DEIGEN_USE_MKL_VML\" -c opt //tensorflow/tools/pip_package:build_pip_package -s\r\nI expect -DEIGEN_USE_MKL_VML passed down to gcc, but it does not for *.o. For example, the gcc command line for external/nasm/labels.c is the following:\r\n\r\n(cd /nfs/pdx/home/sfu2/.cache/bazel/_bazel_sfu2/fec016c4b4f3097e22950dbc1f4b848d/execroot/private-tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/nfs/pdx/home/sfu2/gcc/install/lib64:/usr/lib64:/usr/local/lib \\\r\n    PATH=/nfs/pdx/home/sfu2/bin:/usr/bin:/usr/local/bin/:/usr/lib64/qt-3.3/bin:/nfs/pdx/home/sfu2/perl5/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -g0 -MD -MF bazel-out/host/bin/external/nasm/_objs/nasm/external/nasm/labels.d -DHAVE_SNPRINTF -iquote external/nasm -iquote bazel-out/host/genfiles/external/nasm -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -w '-std=c99' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/nasm/labels.c -o bazel-out/host/bin/external/nasm/_objs/nasm/external/nasm/labels.o)\r\n\r\nYou can see the c option \"-DEIGEN_USE_MKL_VML\" is not on the command line.\r\n\r\nIs it a bug in TensorFlow build script?", "comments": ["The `nasm` assembler that `bazel` is building is part of the host toolchain, not a regular build target.\r\n\r\nI doubt you really want to do this, but if you really want to set a -D on host tools, you can use `--host_copt`:\r\nhttps://docs.bazel.build/versions/master/bazel-user-manual.html#flag--host_copt"]}, {"number": 12080, "title": "distribute tensorflow programe failed when increase worker number", "body": "I run a distribute tensorflow programe with 5 ps and 20 workers. It work normal\u3002\r\nwhen I increase the worker number to 50, the programe fail. the error log as follow. \r\nI don't know how to debug this problem\r\n\r\n2017-08-07 03:51:42.191472: E tensorflow/core/distributed_runtime/master.cc:251] Master init: Unavailable: {\"created\":\"@1502077902.191300911\",\"description\":\"OS Error\",\"errno\":104,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":229,\"grpc_status\":14,\"os_error\":\"Connection reset by peer\",\"syscall\":\"recvmsg\"}\r\n2017-08-07 03:51:42.735190: E tensorflow/core/distributed_runtime/master.cc:251] Master init: Unavailable: {\"created\":\"@1502077902.735079403\",\"description\":\"OS Error\",\"errno\":104,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":229,\"grpc_status\":14,\"os_error\":\"Connection reset by peer\",\"syscall\":\"recvmsg\"}\r\n2017-08-07 03:51:43.224557: E tensorflow/core/distributed_runtime/master.cc:251] Master init: Unavailable: {\"created\":\"@1502077903.224418940\",\"description\":\"OS Error\",\"errno\":104,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":229,\"grpc_status\":14,\"os_error\":\"Connection reset by peer\",\"syscall\":\"recvmsg\"}\r\n2017-08-07 03:51:43.630253: I tensorflow/core/distributed_runtime/master.cc:203] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1\r\n2017-08-07 03:51:43.630305: I tensorflow/core/distributed_runtime/master.cc:203] CreateSession still waiting for response from worker: /job:ps/replica:0/task:4\r\n2017-08-07 03:51:45.901058: E tensorflow/core/distributed_runtime/master.cc:251] Master init: Unavailable: {\"created\":\"@1502077905.900893611\",\"description\":\"OS Error\",\"errno\":104,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":229,\"grpc_status\":14,\"os_error\":\"Connection reset by peer\",\"syscall\":\"recvmsg\"}\r\n2017-08-07 03:51:49.926346: E tensorflow/core/distributed_runtime/master.cc:251] Master init: Unavailable: {\"created\":\"@1502077909.926169663\",\"description\":\"OS Error\",\"errno\":104,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":229,\"grpc_status\":14,\"os_error\":\"Connection reset by peer\",\"syscall\":\"recvmsg\"}\r\nTraceback (most recent call last):\r\nFile \"/home/xiangqin.oxq/tensorflow/train_ps.py\", line 117, in <module>\r\ntf.app.run()\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n_sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\nFile \"/home/xiangqin.oxq/tensorflow/train_ps.py\", line 106, in main\r\nwith sv.prepare_or_wait_for_session(server.target, config=sess_config, max_wait_secs=600) as sess:\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 714, in prepare_or_wait_for_session\r\nmax_wait_secs=max_wait_secs)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 384, in wait_for_session\r\nsess)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 467, in _try_run_local_init_op\r\nsess.run(self._local_init_op)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 786, in run\r\nrun_metadata_ptr)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 994, in _run\r\nfeed_dict_string, options, run_metadata)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1044, in _do_run\r\ntarget_list, options, run_metadata)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1064, in _do_call\r\nraise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnavailableError: {\"created\":\"@1502077902.191300911\",\"description\":\"OS Error\",\"errno\":104,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":229,\"grpc_status\":14,\"os_error\":\"Connection reset by peer\",\"syscall\":\"recvmsg\"}\r\n", "comments": ["1. This normal behavior of TensorFlow when there's connection error on operating system level. You need to catch exception and recreate the session. Higher-level interfaces like Estimator do this automatically.\r\n2. It would be useful to figure out why your system fails to connect. This is external to TensorFlow. For instance, I've dealt with this error caused by Azure SNAT which reset connections after 4mins: https://blogs.msdn.microsoft.com/mast/2015/07/13/azure-snat/", "@yaroslavvb  thank you for your help, I try to catch the excetption and recreate the session.", "@chengdianxuezi have you solved the problem ?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 12079, "title": "My TensorBoard isn't showing any data", "body": "System:\r\nUbuntu 16.04, Tensorflow built from source for python3.\r\n\r\ntensorboard --inspect --logdir=Logdirpath\r\ncorrectly outputs my logs with e.g.\r\nsessionlog:checkpoint\r\n   first_step           0\r\n   last_step            25302\r\n   max_step             25302\r\n\r\n and \r\n\r\nscalars\r\n   Validation_Accuracy\r\n   batch/fraction_of_240_full\r\n   global_step/sec\r\n   parallel_read/filenames/fraction_of_32_full\r\n   parallel_read/fraction_of_204_full\r\n\r\ntensorboard --logdir=Logdirpath outputs \r\nStarting TensorBoard b'55' at http://*****:6006\r\n(Press CTRL+C to quit)\r\n\r\nbut opening the link in firefox just shows a blank page. As does the command with --debug\r\n", "comments": ["_Warning: As you've not filled in the required info above you may not get the support you're looking for from the devs. Perhaps this is an issue for Stack Overflow?_\r\nEven http://localhost/:6006 ? What TF version? Have you filed on the tensorboard issue tracker?", "Please file tensorboard-related issues under the tensorboard repository:\r\nhttps://github.com/tensorflow/tensorboard/issues"]}, {"number": 12078, "title": "Assert randomly fails when training with multiple threads", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04.2 LTS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3.0-rc2\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: no\r\n- **GPU model and memory**: no\r\n- **Exact command to reproduce**: `for ((n=0;n<100;n++)); do python mnist_softmax_parallel_issue.py; done`\r\n\r\n\r\n### Describe the problem\r\nThe following script randomly crashes (i.e., sometimes crashes and produces this traceback, most of the times it does not). The script trains the MNIST softmax model in parallel leveraging several threads. \r\n\r\n### Source code / logs\r\n\r\n_mnist_softmax_device_issue.py_\r\n\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport sys\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nimport tensorflow as tf\r\nimport threading\r\nimport numpy as np\r\nimport json\r\nimport os\r\nimport time\r\n\r\nFLAGS = None\r\n\r\nINTER_OP_PARALLELISM = 76\r\nINTRA_OP_PARALLELISM = 1\r\nBATCH_SIZE = 100\r\nITERATIONS = 1000\r\nTRAINING_THREADS = 46\r\n\r\nthreads = [None] * TRAINING_THREADS\r\n\r\ndef train_function(thread_idx, mnist, sess, train_step, x, y_, y):\r\n  iterations = int(ITERATIONS/TRAINING_THREADS)\r\n  for i in range(iterations):\r\n    batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\r\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\r\n\r\ndef main(_):\r\n  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\r\n\r\n  x = tf.placeholder(tf.float32, [None, 784])\r\n  W = tf.Variable(tf.zeros([784, 10]))\r\n  b = tf.Variable(tf.zeros([10]))\r\n  y = tf.matmul(x, W) + b\r\n\r\n  y_ = tf.placeholder(tf.float32, [None, 10])\r\n\r\n  cross_entropy = tf.reduce_mean(\r\n      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\r\n  train_step = tf.train.GradientDescentOptimizer(0.5, use_locking=True).minimize(cross_entropy)\r\n\r\n  sess = tf.InteractiveSession(config=tf.ConfigProto(intra_op_parallelism_threads = INTRA_OP_PARALLELISM, inter_op_parallelism_threads= INTER_OP_PARALLELISM))\r\n  sess.run(tf.global_variables_initializer())\r\n\r\n  for i in range(TRAINING_THREADS):\r\n      threads[i] = threading.Thread(target=train_function, args=[i, mnist, sess, train_step, x, y_, y])\r\n\r\n  for thread in threads:\r\n      thread.start()\r\n  for thread in threads:\r\n      thread.join()\r\n\r\n\r\nif __name__ == '__main__':\r\n  parser = argparse.ArgumentParser()\r\n  parser.add_argument('--data_dir', type=str, default='mnist-data',\r\n                      help='Directory for storing input data')\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n\r\n```\r\n\r\n_Traceback_\r\n\r\n`\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:125: Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::T\r\nensorEvaluator(const XprType&, const Device&) [with Broadcast = const Eigen::IndexList<Eigen::type2index<1l>, int>; ArgType = const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long\r\n int>, 16, Eigen::MakePointer>; Device = Eigen::ThreadPoolDevice; Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::XprType = Eigen::TensorBroadcastingOp<const Eigen::IndexList<Eigen::type2index<1l>, int>, const Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer> >]: Assertion input_dims[i] > $' failed.\r\n`\r\n", "comments": ["I found the cause. \r\nI was not using `mnist.train.next_batch()` correctly. I was assuming it was thread safe.\r\n\r\nMore details here: https://stackoverflow.com/questions/45574025/random-crashes-when-training-with-multiple-threads-on-tensorflow/45594648#45594648\r\n\r\nI'll close this issue. "]}, {"number": 12077, "title": "Occur error when compile tf_core_gpu_kernels/generated_adjust_hue_op_gpu.cu.cc file in VS2015", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nwindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.3\r\n- **IDE version**: \r\nvs2015 Debug mode\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nCUDA8.0 cuDNN5.1\r\n- **GPU model and memory**:\r\n4GB\r\n- **Exact command to reproduce**:\r\nD:/tensorflow-r1.3\\tensorflow/stream_executor/device_description.h(85): warning : type qualifier on return type is meaningless\r\n35>D:/tensorflow-r1.3\\tensorflow/stream_executor/device_description.h(144): warning : type qualifier on return type is meaningless\r\n35>E:/vs2015/VC/bin/../../VC/INCLUDE\\xutility(911): **error : calling a __host__ function(\"std::_Debug_message\") from a __device__ function(\"std::_Debug_lt<const int &, const int &> \") is not allowed**\r\n35>  1 error detected in the compilation of \"C:/Users/hh/AppData/Local/Temp/tmpxft_0000425c_00000000-15_adjust_hue_op_gpu.cu.compute_52.cpp1.ii\".\r\n35>  adjust_hue_op_gpu.cu.cc\r\n35>  CMake Error at tf_core_gpu_kernels_generated_adjust_hue_op_gpu.cu.cc.obj.Debug.cmake:282 (message):\r\n35>    Error generating file\r\n35>    D:/tensorflow-r1.3/CMAKE-GPU/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/Debug/tf_core_gpu_kernels_generated_adjust_hue_op_gpu.cu.cc.obj\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI built tensorflow project for GPU version in vs2015, and get CUDA error. It seems like call host function in the device function, but I can't find the place where error occur\r\n", "comments": ["@mrry Do you have any tips here?", "Unfortunately not... the VS Debug configuration isn't well supported on Windows. You could try a `RelWithDebInfo` build instead.\r\n\r\n/cc @guschmue ", "I think I have seen similar issues during the initial port which was some stl thing being called from the device. \r\nI can take a look.", "Is there any solution to fix this, I met also now. ", "Hi! @HannH .We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.5  version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 12076, "title": "[OpenCL] Fixes core_rnn_cell_tests", "body": "SYCL uses the same implementation as the CPU one for BiasAdd", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins ", "@tensorflow-jenkins test this please", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (25/25 (100%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from github-smtp2-ext4.iad.github.net ([192.30.252.195]:58752 helo=github-smtp2a-ext-cp1-prd.iad.github.net)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES256-GCM-SHA384:256)\n\t(Exim 4.89)\n\t(envelope-from <noreply@github.com>)\n\tid 1demv0-0005fw-1m\n\tfor mazecreator@mazecreator.com; Mon, 07 Aug 2017 13:38:52 -0500\nDate: Mon, 07 Aug 2017 11:49:15 -0700\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=github.com;\n\ts=pf2014; t=1502131755;\n\tbh=P/h8w7tbR/8xwMQxV3PmreV0jc3cU8w7K0WqUjoKlSc=;\n\th=From:Reply-To:To:Cc:In-Reply-To:References:Subject:List-ID:\n\t List-Archive:List-Post:List-Unsubscribe:From;\n\tb=kaPWFBj6xoS9neuDkKe90PE+lv6WRtDjBkXm4di+aKN/ivZJiOWIzaF4/hcCxOzps\n\t MNrm6aB+d7ItmAeeFFcLmclrvpQBkfZeBbq6XC0CEOvZpvSGsoDzXMPQrquseLjsgR\n\t q10YGjvMEGKFtKVCAVyRuGIjj962mcGK2fvZ1UZg=\nFrom: Rasmus Munk Larsen <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/pull/12076/c320747587@github.com>\nIn-Reply-To: <tensorflow/tensorflow/pull/12076@github.com>\nReferences: <tensorflow/tensorflow/pull/12076@github.com>\nSubject: Re: [tensorflow/tensorflow] [OpenCL] Fixes core_rnn_cell_tests\n (#12076)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5988b62b2db28_59723ff869421c3c389047\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: rmlarsen\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a0f7d5de42bd34d6fe36242b4f4fcf0a01c2f30e8092cf0000000115a0782b92a169ce0ece4d4c@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoH6G0S2-5wLQtu_otI_oEVbPbMFBks5sV1wrgaJpZM4OvZH_>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n\n----==_mimepart_5988b62b2db28_59723ff869421c3c389047\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n@tensorflow-jenkins test this please\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/pull/12076#issuecomment-320747587\n----==_mimepart_5988b62b2db28_59723ff869421c3c389047\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p><a href=\"https://github.com/tensorflow-jenkins\" class=\"user-mention\">@tensorflow-jenkins</a> test this please</p>\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/pull/12076#issuecomment-320747587\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoCiHWplGLyYtTL_5jUoCJbUehM6nks5sV1wrgaJpZM4OvZH_\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoFlXat_TBDO0FJI8UH-IHarRKILKks5sV1wrgaJpZM4OvZH_.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/pull/12076#issuecomment-320747587\"></link>\n  <meta itemprop=\"name\" content=\"View Pull Request\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Pull Request on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@rmlarsen in #12076: @tensorflow-jenkins test this please\"}],\"action\":{\"name\":\"View Pull Request\",\"url\":\"https://github.com/tensorflow/tensorflow/pull/12076#issuecomment-320747587\"}}}</script>\n----==_mimepart_5988b62b2db28_59723ff869421c3c389047--\n", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (26/25 (104%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from o4.sgmail.github.com ([192.254.112.99]:4115)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES128-GCM-SHA256:128)\n\t(Exim 4.89)\n\t(envelope-from <bounces+848413-e6d9-mazecreator=mazecreator.com@sgmail.github.com>)\n\tid 1demv6-0005gJ-NO\n\tfor mazecreator@mazecreator.com; Mon, 07 Aug 2017 13:38:58 -0500\nDKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=github.com; \n\th=from:reply-to:to:cc:in-reply-to:references:subject:mime-version:content-type:content-transfer-encoding:list-id:list-archive:list-post:list-unsubscribe; \n\ts=s20150108; bh=VDGbuoU2FrZAPnRZNFZlApIA8nM=; b=Nwg24bEWzAUJGGHf\n\tT3eTFneGfGwHOtn3thOI9bSs7s3gNGBS6Sd6JawhYneVxm9nSb83DDLCiIG8Bj47\n\t6VyU5GrZU0kNVdcgdn4dljPD4uvUYVBimyjRm35/T3orxhpfiSEmOMKguga1NMom\n\t1eCTcwtIkzPPTPMHe7hNrKwbMlU=\nReceived: by filter0470p1mdw1.sendgrid.net with SMTP id filter0470p1mdw1-31435-5988B630-7C\n        2017-08-07 18:49:20.806939127 +0000 UTC\nReceived: from github-smtp2a-ext-cp1-prd.iad.github.net (github-smtp2a-ext-cp1-prd.iad.github.net [192.30.253.16])\n\tby ismtpd0003p1iad1.sendgrid.net (SG) with ESMTP id 9aYVFB0gQ1KlKBhANoMEBg\n\tfor <mazecreator@mazecreator.com>; Mon, 07 Aug 2017 18:49:20.795 +0000 (UTC)\nDate: Mon, 07 Aug 2017 18:49:21 +0000 (UTC)\nFrom: Rasmus Munk Larsen <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/pull/12076/review/54741484@github.com>\nIn-Reply-To: <tensorflow/tensorflow/pull/12076@github.com>\nReferences: <tensorflow/tensorflow/pull/12076@github.com>\nSubject: Re: [tensorflow/tensorflow] [OpenCL] Fixes core_rnn_cell_tests\n (#12076)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5988b62f5a9ff_35823fa5f02adc3c361725\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: rmlarsen\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a0117c6eca25eaf16e48de9b01317787b0cf580f0c92cf0000000115a0782f92a169ce0ece4d4c@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoCsZ6ai7IKf9fkbg16ho9tT4CRZ9ks5sV1wvgaJpZM4OvZH_>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-SG-EID: BJ4qYjf5a3yL0lCrdDNghY4YYR+k1a+cluU6wEX1Jqyi3+A4eFw7bGiD1y4dFUFyo3ddPxei6p4C8l\n i0S4clX0s0YPOJ+gxsJ/C/6+EtXSaW1IyS0NYBaHsGwmSzUedYJBfZba58eiJ7MO/fO/pR/kaAZsJQ\n 3tS8JcRrlsbQv3ZB1pdN5kMw7o/7RppTHFlxMVJYLAS7n5R6fAFbrCU84PUglWmWkODhJnZ+rwZNm0\n vs9bdOlz/rLH2H2iSKaxuI\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n----==_mimepart_5988b62f5a9ff_35823fa5f02adc3c361725\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\nrmlarsen approved this pull request.\n\n\n\n\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/pull/12076#pullrequestreview-54741484\n----==_mimepart_5988b62f5a9ff_35823fa5f02adc3c361725\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p><b>@rmlarsen</b> approved this pull request.</p>\n\n\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/pull/12076#pullrequestreview-54741484\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoExkxLh0R-v9ggMYyr0Lf4V_Cskuks5sV1wvgaJpZM4OvZH_\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoPPsJvpv42U0zitnchjOZuniPD_tks5sV1wvgaJpZM4OvZH_.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/pull/12076#pullrequestreview-54741484\"></link>\n  <meta itemprop=\"name\" content=\"View Pull Request\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Pull Request on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@rmlarsen approved #12076\"}],\"action\":{\"name\":\"View Pull Request\",\"url\":\"https://github.com/tensorflow/tensorflow/pull/12076#pullrequestreview-54741484\"}}}</script>\n----==_mimepart_5988b62f5a9ff_35823fa5f02adc3c361725--\n", "@lukeiwanski FYI: I approved this prematurely. We don't need to add the DeviceName template function. the proper way to get the device name is through the context->device_type() method. I'll revert that part.", "@lukeiwanski I should say context->device_type().type_string() or context->device()->attributes().name()  :-)", "@rmlarsen Would you like me to create PR for that fix ?"]}, {"number": 12075, "title": "add args of TimeReversedFusedRNN", "body": "I want to use TimeReversedFusedRNN together with LSTMBlockFusedCell , but the batch_axis of LSTMBlockFusedCell's input is 1 , but not 0 . But the reverse function of TimeReversedFusedRNN can't decide which is batch_axis , just do array_ops.reverse_sequence(t, lengths, 0, 1) . So , I think it's useful that users can tell it which is batch_axis and seq_axis .", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@ebrevdo feel free to delegate", "Isn't seq_axis just 1-batch_axis?", "yes, that's true.So should I recommit?", "I think user can decide which is seq_axis and batch_axis is useful.", "Please remove these two arguments and instead provide the single bool\nargument time_major, just like tf.nn.dynamic_rnn does.  The default value\nshould be such that the existing behavior does not change.\n\nOn Aug 7, 2017 4:09 PM, \"Jun Wang\" <notifications@github.com> wrote:\n\n> I think user can decide which is seq_axis and batch_axis is useful.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12075#issuecomment-320829918>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim52i2L5WcqEfX6mVTQcGOgOOnm20ks5sV8NagaJpZM4OvSlP>\n> .\n>\n", "Thanks \uff0c I've recommited."]}, {"number": 12074, "title": "Fix segfault when recording raw allocation returns nullptr", "body": "See [here](https://github.com/tensorflow/tensorflow/commit/ec1403e7dc2b919531e527d36d28659f60621c9e#commitcomment-23432313) for discussion.", "comments": ["Can one of the admins verify this patch?", "@byronyi, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @benoitsteiner to be potential reviewers.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@michaelisard @reedwm Could you take a look here? Thanks!", "Thank you for the fix!", "I am sorry, but could you revert this? Seems this is only a partial fix that does for allocation but not de-allocation. I will try to address it in 9bb2c8eda."]}, {"number": 12073, "title": "tf.py_func errors in freeze_graph", "body": "I use `tf.py_func` in my networks, and it works fine in training and test mode.  Then I run `bazel-bin/tensorflow/python/tools/freeze_graph` to freeze the graph and generate xxx.pb file.\r\nI try to restore the xxx.pb and run the model again, error happens.\r\n\r\n### Erorr logs\r\n```\r\n2017-08-07 16:04:15.565535: W tensorflow/core/framework/op_kernel.cc:1158] Unknown: exceptions.KeyError: 'pyfunc_0'\r\nTraceback (most recent call last):\r\n  File \"object_detector_pb.py\", line 159, in <module>\r\n    objects = detector.detect(img)\r\n  File \"object_detector_pb.py\", line 118, in detect\r\n    scores, boxes = self.im_detect(image)\r\n  File \"object_detector_pb.py\", line 101, in im_detect\r\n    scores, bbox_pred, rois = self.sess.run([self._cls_prob, self._bbox_pred, self._rois], feed)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 997, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnknownError: exceptions.KeyError: 'pyfunc_0'\r\n\t [[Node: detector/MobilenetV1_2/ANCHOR_default/generate_anchors = PyFunc[Tin=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_FLOAT], Tout=[DT_FLOAT, DT_INT32], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](detector/MobilenetV1_2/ANCHOR_default/ToInt32, detector/MobilenetV1_2/ANCHOR_default/ToInt32_1, detector/MobilenetV1_2/ANCHOR_default/generate_anchors/input_2, detector/MobilenetV1_2/ANCHOR_default/generate_anchors/input_3, detector/MobilenetV1_2/ANCHOR_default/generate_anchors/input_4)]]\r\n\r\nCaused by op u'detector/MobilenetV1_2/ANCHOR_default/generate_anchors', defined at:\r\n  File \"object_detector_pb.py\", line 143, in <module>\r\n    detector = ObjectDetector(model_path, config_file)\r\n  File \"object_detector_pb.py\", line 76, in __init__\r\n    tf.import_graph_def(graph_def, name=\"detector\")\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 311, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nUnknownError (see above for traceback): exceptions.KeyError: 'pyfunc_0'\r\n\t [[Node: detector/MobilenetV1_2/ANCHOR_default/generate_anchors = PyFunc[Tin=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_FLOAT], Tout=[DT_FLOAT, DT_INT32], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](detector/MobilenetV1_2/ANCHOR_default/ToInt32, detector/MobilenetV1_2/ANCHOR_default/ToInt32_1, detector/MobilenetV1_2/ANCHOR_default/generate_anchors/input_2, detector/MobilenetV1_2/ANCHOR_default/generate_anchors/input_3, detector/MobilenetV1_2/ANCHOR_default/generate_anchors/input_4)]]\r\n```\r\n\r\nI found the `tf.py_func` is not in the graph. How can I define and use the `tf.py_func` in freezed graph?", "comments": ["@petewarden would know if `tf.py_func` is meant to be supported for freeze graph", "I also encountered this problem, how to solve?", "I didn't find a method to solve this problem gracefully. However you can try: split the graph into two sub-grapy, one before the py_func the other after the py_func.", "I  can't know how to do it? I have no idea for this problems.", "Got the same problem.  Can you be more specific? @rolai ", "@huangwenwenlili @JacquelineWang \r\nTake `faster-rcnn` for example,  `py_func` is used to count the anchors and rois. The graph is splited into two sub-graphs by the `py_func`:  `rpn` and `rcnn`. \r\n1. restore the faster-rcnn checkpoint model file into two checkpoint files (for `rpn` and `rcnn`)\r\n2. freeze each checkpoint files  into one pb file\r\n3. assemble the two pb files\r\n   3.1 run rpn network\r\n   3.2 run the py_func with the outputs of rpn network\r\n   3.3 run the rcnn network with the outputs of py_func\r\n", "Hi, @rolai . In my case,  `resnet_v1_50_3/rois/PyFunc` is in the freeze graph. However, I'm still suffering from the `pyfunc_0 is not found` error. Do you have any solutions for this, and please let me know if I misinterpret something. Thanks in advance. ", "@rolai Hi,I have met the same problem, when I use py_func in my network, I can do inference and get the correct result,but after freeze network and save the graph_def to \"model.pb\", I cannot do test with error \"pyfunc_0 is not found\" .\r\ndo you have the solution of this problem? "]}, {"number": 12072, "title": "Recurrent Reinforcement Learning Issue", "body": "Hi guys,\r\n\r\nI am working on a structure with recurrent neural network as the deep part of deep reinforcement learning.\r\n\r\nI have been searching all along but I haven't found a way to build this structure by using tensorflow. The recurrent feature makes the objective function iterative. I am wondering if TF can calculate the gradient by itself.\r\n\r\nIt would be great if I can get to know if this structure can be done or not.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12070, "title": "[XLA] Should we disable `REGISTER_XLA_BACKEND(GPU)` if there is no GPU installed on the machine? ", "body": "I install TensorFlow from source with `XLA` and `TF_CPP_MIN_VLOG_LEVEL` enabled (without `GPU`), after trying `XLA` example, I got these logs:\r\n\r\n```shell\r\n...\r\nXLA op registration: device: XLA_CPU_JIT op: AssignAddVariableOp\r\nXLA op registration: device: XLA_GPU_JIT op: AssignAddVariableOp\r\nXLA op registration: device: XLA_CPU_JIT op: _UnsafeReadVariable\r\nXLA op registration: device: XLA_GPU_JIT op: _UnsafeReadVariable\r\nXLA op registration: device: XLA_CPU_JIT op: Unpack\r\nXLA op registration: device: XLA_GPU_JIT op: Unpack\r\n...\r\n```\r\n\r\nBut there is no `GPU` installed on my machine, so I don't enable `CUDA` option, but it still do\r\n\r\n```cpp\r\nREGISTER_XLA_BACKEND(DEVICE_GPU_XLA_JIT, kGpuAllTypes, GpuOpFilter);\r\n```\r\n\r\nShould we disable `REGISTER_XLA_BACKEND(GPU)` if there is no `GPU` installed on the machine?\r\n\r\n@tatatodd @hawkinsp WDYT?\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n  No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\n  Source\r\n- **TensorFlow version (use command below)**:\r\n  v1.2.1\r\n- **Python version**: \r\n  2.7\r\n- **Bazel version (if compiling from source)**:\r\n  0.4.5\r\n- **CUDA/cuDNN version**:\r\n  N/A\r\n- **GPU model and memory**:\r\n  N/A\r\n- **Exact command to reproduce**:", "comments": ["ping @tatatodd @hawkinsp ", "Thanks for the bug report!\r\n\r\nThese messages are benign \u2014 I would just ignore them. They won't hurt anything and they only show up if you turn on verbose logging; at worst they could slow down startup a very tiny amount.\r\n\r\nWe could maybe try to avoid creating op registrations if not building with CUDA, but if it isn't causing a problem I don't think it's a high priority.\r\n\r\nHowever if you want to send a PR to fix it I wouldn't object :-)", "@hawkinsp Thanks for your reply!\r\n\r\nI will send a PR to fix it later :)"]}, {"number": 12069, "title": "Fix typo", "body": "`values` is not defined.", "comments": ["Can one of the admins verify this patch?", "@ppwwyyxx, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @ebrevdo to be potential reviewers.", "@tensorflow-jenkins test this please"]}, {"number": 12068, "title": "what's the purpose of class StatsPublisherInterface?", "body": "### Describe the problem\r\nTensorflow Open source version contains the class StatsPublisherInterface and a dummy implementation, NoOpStatsPublisher. I was wondering the design purpose of this class. From the class name it seems that there is some other system that can subscribe the statistics of tensorflow step and display the information in realtime.  Can someone help ex\r\n\r\n### Source code / logs\r\nhttps://github.com/tensorflow/tensorflow/blob/0a4f5b6bda405814af59803829216762e030728d/tensorflow/core/common_runtime/stats_publisher_interface.h\r\n", "comments": ["it looks like it's part of refactoring of an existing system that lets you display tensorflow timeline, cc @suharshs ", "Thanks @yaroslavvb . Yes that's correct. Eventually if we can implement a proper StatsPublisherInterface to have stats more/better visualizations. ", "thanks. "]}, {"number": 12067, "title": "fix a typo in tf.nn.separable_conv2d's doc", "body": "It's obvious that the right bracket in \"sum_{di, dj, q, r]\" should\r\nbe a right brace.", "comments": ["@freedomtan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @drpngx, @tensorflower-gardener and @vrv to be potential reviewers.", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 12066, "title": "Encounter Fatal signal 11 (SIGSEGV) problem doing training on mobile device", "body": "I am stuck on the SIGSEGV problem while trying to do training on a mobile device (Nexus 7), can you help me out? \r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes. I added a couple of lines to the TensorFlowInferenceInterface.java\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.2.1\r\n- **Python version**: \r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n0.52\r\n- **CUDA/cuDNN version**:\r\n-\r\n- **GPU model and memory**:\r\n-\r\n- **Exact command to reproduce**:\r\n-\r\n### Describe the problem\r\nI am trying to find out the performance of doing training on mobile devices. \r\nI got the Fatal signal 11 (SIGSEGV) when I am trying to work with a graph including a couple of convolutional layers, FC layers, and cross entropy operation. \r\n\r\nHere is what I did:\r\nI modified the TensorFlowInferenceInterface.java by adding a function for initialization all nodes; then I was able to reproduce the example in the following link in Android.\r\nhttps://tebesu.github.io/posts/Training-a-TensorFlow-graph-in-C++-API\r\n\r\nI try to do the training with a more complicated graph (including a couple of convolutional layers, FC layers and cross entropy operation). I can do the training with this graph using the C++ API.\r\n\r\nI first solved the No OpKernel problem. The error message is as follows. \r\nNo OpKernel was registered to support Op 'SparseSoftmaxCrossEntropyWithLogits' with these attrs.\r\n\r\nI add the sparse_xent_op.h and the sparse_xent_op.cc files to the BUILD file locates at /tensorflow/tensorflow/core/kernels/. \r\nI modify the list_op_files.txt locally, adding the sparse_xent_op operator and, rebuild the Tensorflow source. \r\n\r\nThe No OpKernel error disappears, but the program stops at runner.runAndFetchMetadata() and throws me an error:\r\nA/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x0 in tid 15173 (inference)\r\n\r\n### Source code / logs\r\nTensorFlowInferenceInterface.java:\r\n\r\n  public void runTarget(String[] outputNames){\r\n    Log.d(TAG, \"start of the runTarget\");\r\n    for (String t : outputNames) {\r\n      runner.addTarget(t);\r\n   \r\n    }\r\n    Log.d(TAG, \"finished adding target\");\r\n\r\n      runner.runAndFetchMetadata();\r\n    Log.d(TAG,\"runAndFetchMetadata\");\r\n       runner = sess.runner();\r\n    Log.d(TAG,\"sess.runner\");\r\n\r\n  }\r\n\r\nCorresponding Logcat messages:\r\n08-06 16:25:01.900 15099-15173/org.tensorflow.demo D/TensorFlowInferenceInterface: start of the runTarget\r\n08-06 16:25:01.900 15099-15173/org.tensorflow.demo D/TensorFlowInferenceInterface: finished adding target\r\n08-06 16:25:04.576 15099-15173/org.tensorflow.demo A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x0 in tid 15173 (inference)\r\n\r\nCrash dump result:\r\n\r\n********** Crash dump: **********\r\nBuild fingerprint: 'google/razor/flo:6.0/MRA58K/2256973:user/release-keys'\r\npid: 5414, tid: 5478, name: inference  >>> org.tensorflow.demo <<<\r\nsignal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0\r\nStack frame #00 pc 00017664  /system/lib/libc.so (__memcpy_base+91)\r\nStack frame #01 pc 0072a203  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so\r\nStack frame #02 pc 006eea49  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so\r\nStack frame #03 pc 006d59d3  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so\r\nStack frame #04 pc 006ed9c3  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so\r\nStack frame #05 pc 006df991  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so\r\nStack frame #06 pc 006dffb5  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so\r\nStack frame #07 pc 0009eff9  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so\r\nStack frame #08 pc 0009f403  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so\r\nStack frame #09 pc 00099085  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_inference.so (Java_org_tensorflow_Session_run+920)\r\nStack frame #10 pc 000eaa29  /system/lib/libart.so (art_quick_generic_jni_trampoline+40)\r\nStack frame #11 pc 000e6331  /system/lib/libart.so (art_quick_invoke_stub_internal+64)\r\nStack frame #12 pc 00402663  /system/lib/libart.so (art_quick_invoke_static_stub+170)\r\nStack frame #13 pc 001009f4  [stack:5478]\r\n", "comments": ["Could any one help me with this? ", "Sorry, but training is not supported on mobile TensorFlow, only inference. The gradients for backpropagation aren't even compiled into the library."]}, {"number": 12065, "title": "Feature Request - Seq2Seq Inference Helper w/o Embeddings", "body": "`tf.contrib.seq2seq` has two `Helper` classes to use during inference, `SampleEmbeddingHelper` and `GreedyEmbeddingHelper`. However, both make use of embeddings, which is unhelpful when building sequence-to-sequence models that operate on non-embedded target sequences (my target sequence already consists of meaningful vectors).\r\n\r\nI'd like a new `Helper` class that pipes the output of the decoder RNN at one time step into the decoder RNN at the following time step. It should permit the `start_tokens` to be vectors (tensors?) and the `end_token` to be a vector (tensor?) as well. Right now, I'm attempting to use `ScheduledOutputTrainingHelper` with `sampling_probability` set equal to 1.0, but I'm struggling to get it to work. Something like a simple `OutputInferenceHelper` would be very nice :)\r\n\r\nIf there already exists an easy way to do what I'm suggesting, please let me know!\r\n\r\n", "comments": ["I'm trying to implement my suggestion, but I'm running into an error that I'm unsure of how to debug.\r\n\r\nMy code:\r\n\r\n```\r\nclass OutputInferenceHelper(Helper):\r\n\r\n    def __init__(self, start_tensors, end_tensor, name=None):\r\n        \"\"\"Initializer.\r\n        \r\n        Args:\r\n          start_tensors: `float32` tensor shaped `[batch_size, ...]`, the start\r\n           tensors.\r\n          end_tensor: `float32` tensor shaped `[...]`, the tensor that marks \r\n           end of decoding.\r\n        \"\"\"\r\n        with ops.name_scope(name, \"OutputInferenceHelper\"):\r\n            self._start_tensors = ops.convert_to_tensor(\r\n                start_tensors, dtype=dtypes.float32, name=\"start_tensors\")\r\n            self._end_tensor = ops.convert_to_tensor(\r\n                end_tensor, dtype=dtypes.float32, name=\"end_tensor\")\r\n            self._batch_size = array_ops.shape(start_tensors)[0]\r\n\r\n    @property\r\n    def batch_size(self):\r\n        return self._batch_size\r\n\r\n    def initialize(self, name=None):\r\n        with ops.name_scope(name, \"OutputInferenceHelperInitialize\"):\r\n            finished = array_ops.tile([False], [self._batch_size])\r\n            return (finished, self._start_tensors)\r\n\r\n    def sample(self, time, outputs, state, name=None):\r\n        with ops.name_scope(name, \"OutputInferenceHelperSample\"):\r\n            del time, state\r\n            if not isinstance(outputs, ops.Tensor):\r\n                raise TypeError(\"Expected outputs to be a single Tensor, got: %s\" %\r\n                                type(outputs))\r\n            return outputs\r\n\r\n    def next_inputs(self, time, outputs, state, sample_ids, name=None):\r\n        with ops.name_scope(name, \"OutputInferenceHelperNextInputs\"):\r\n            del time, sample_ids\r\n            finished = math_ops.equal(outputs, self._end_tensor)\r\n            all_finished = math_ops.reduce_all(finished)\r\n            next_inputs = control_flow_ops.cond(\r\n                all_finished,\r\n                # If we're finished, the next_inputs value doesn't matter\r\n                lambda: self._start_tensors,\r\n                lambda: outputs)\r\n\r\n            return (finished, next_inputs, state)\r\n```\r\n\r\nI'm receiving the following error: `tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes must be equal rank, but are 1 and 2 for 'add_inference/add_decoder/decoder/while/Select' (op: 'Select') with input shapes: [128,4], [?], [?].`\r\n\r\nMy formal parameter for `start_tensors` when instantiating a `OutputInferenceHelper` object has shape [128, 4], but beyond that, I'm lost.\r\n\r\nMy Traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\", line 671, in _call_cpp_shape_fn_impl\r\n    input_tensors_as_shapes, status)\r\n  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  <my function calls>\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 286, in dynamic_decode\r\n    swap_memory=swap_memory)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2770, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2599, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2549, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 242, in body\r\n    sequence_lengths)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2328, in where\r\n    return gen_math_ops._select(condition=condition, t=x, e=y, name=name)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2145, in _select\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2508, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1873, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1823, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\", line 676, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: Shapes must be equal rank, but are 1 and 2 for 'add_inference/add_decoder/decoder/while/Select' (op: 'Select') with input shapes: [128,4], [?], [?].\r\n\r\n```", "@ebrevdo @lukaszkaiser Can you comment on this?  Thanks!", "I'm not too familiar with this code, will wait for @ebrevdo or @lmthang to comment.", "One mistake I'm making (I think) is that `finished = math_ops.equal(outputs, self._end_tensor)` operates element-wise, which means `finished` will be a tensor of shape `[batch size, ...]` and should instead be a tensor of shape `[batch size]`. I believe a statement is `finished = math_ops.reduce_all(math_ops.equal(outputs, self._end_tensor), axis=1)`, but I'm not sure if this generalizes to tensors of higher dimensions.", "Adam, iirc you wrote the training version, right?  Ptal?\n\nOn Aug 9, 2017 9:37 PM, \"Rylan Schaeffer\" <notifications@github.com> wrote:\n\n> One mistake I'm making (I think) is that finished =\n> math_ops.equal(outputs, self._end_tensor) operates element-wise, which\n> means finished will be a tensor of shape [batch size, ...] and should\n> instead be a tensor of shape [batch size]. I believe a statement is finished\n> = math_ops.reduce_all(math_ops.equal(outputs, self._end_tensor), axis=1),\n> but I'm not sure if this generalizes to tensors of higher dimensions.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12065#issuecomment-321448809>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0RVcPD-bKi8I97y970QWUsY4uNvks5sWokjgaJpZM4Ou28p>\n> .\n>\n", "This is actually a more specific case of what I am building, which is a general sampling helper that takes a layer like the ScheduledOutputTrainingHelper. The problem with using the training helper is that it doesn't output the actual samples so it can't be used during inference.\r\n\r\nRylan, your solution solution will almost work, but you are going to hit the issue that sample_ids is currently required to be a scalar integer, something which I am also solving with my change. If you can wait until next week, I can have something that will work for you. In the meantime, I'd recommend hacking up the CustomHelper to work for your use case. You'll also need to adjust the BasicDecoder output_size and output_dtype to match the shape of your sample tensor.", "@adarob , thanks for responding!  I need to meet a deadline this Sunday for my internship project, but I have another month after that to continue working on the project. Could I email you directly for help with hacking a solution in the short term? (I'm happy to talk here, but I know TensorFlow developers like to keep conversations focused on the original issue).", "Sure @RylanSchaeffer ", "@adarob , thanks! Sent!", "@adarob @RylanSchaeffer Hi guys, I've also been looking for an output helper for this purpose. Would it be possible to share your solution with me?\r\n\r\nThanks in advance. \r\n", "@ppyht2 Adam's solution for my problem might not work generally. In my case, my decoder's output is a (4,) tensor of binary values, so he suggested using a `CustomHelper` that maps the decoder's outputs to sample_ids by treating the output as an integer represented in binary i.e. [0, 0, 0, 0,]'s sample_id is 0, [0, 1, 0, 0]'s sample_id is 4, etc.\r\n\r\nLet me know if that makes sense.", "I added a new InferenceHelper to the Google codebase. I'm not sure how\noften that is synced with GitHub but I'd expect it to happen in the next\ncouple of days.\n\nOn Aug 13, 2017 9:35 AM, \"Rylan Schaeffer\" <notifications@github.com> wrote:\n\n@ppyht2 <https://github.com/ppyht2> Adam's solution for my problem might\nnot work generally. In my case, my decoder's output is a (4,) tensor of\nbinary values, so he suggested using a CustomHelper that maps the decoder's\noutput to sample_ids by treating the output as an integer represented in\nbinary i.e. [0, 0, 0, 0,]'s sample_id is 0, [0, 1, 0, 0]'s sample_id is 4,\netc.\n\nLet me know if that makes sense.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/12065#issuecomment-322052181>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/ABCa6CdmtfB-wN4ypySoYD9zuVXjlqiXks5sXyXegaJpZM4Ou28p>\n.\n", "@adarob , would it be possible to also get a corresponding modified version of `sequence_loss` that doesn't require `targets` to be shaped `[batch_size x sequence_length]`?", "@RylanSchaeffer I'm currently working on a problem where my decoder's output has float values, in which case the solution will no longer work, is that correct?\r\n\r\n@adarob will the InferenceHelper resolve this issue?\r\n\r\nThanks for you help guys :) \r\n\r\n", "@ppyht2 That's correct. However, I think there's an easier solution if all you want is the decoder's output to be passed as input at the next time step and you don't care about sample_ids.\r\n\r\nIf you look at `dynamic_decode`, you'll see that the function [calls its decoder's step method](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/seq2seq/python/ops/decoder.py#L234). Assuming you're using a `BasicDecoder`, the [step method does three things](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py#L126):\r\n\r\n1) Runs the decoder's cell for 1 step ([139](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py#L139))\r\n2) Feeds the cell's output to the `Helper`'s `sample_fn` to generate `sample_ids` ([142](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py#L142))\r\n3) Feeds the cell's output and the sample_ids (and a few other parameters) to the `Helper`'s `next_input_fn` ([144](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py#L144))\r\n\r\nIf all you want is the cell's outputs to be the decoder's inputs at the next time step, you can write a `CustomHelper` with a `sample_fn` that returns arbitrary integers (to pass a type and shape later check) and then write a `next_inputs_fn` that returns `outputs` as `next_inputs` if the decoder isn't done.\r\n\r\nHope that makes sense!\r\n\r\nDisclaimer: My `CustomHelper` isn't working, so I'm not sure if this is a viable solution.", "@RylanSchaeffer This solution sounds like it could work, I will give it a crack. Thanks for your help. \r\n\r\nDid you had any explanation as to why the ```CustomHelper``` is not working? ", "@ppyht2 I suspect that I'm incorrectly using the TrainingHelper. I've been receiving help from people on another issue at the NMT tutorial (https://github.com/tensorflow/nmt/issues/3), but I haven't been able to fix my issue yet.", "The new InferenceHelper added in e9a8d75 resolves this issue.", "@RylanSchaeffer does @adarob's PR solve your issue?", "@ebrevdo Let's presume yes, and I'll reopen the issue with additional details if it doesn't.", "@adarob , thank you!", "@RylanSchaeffer I am facing a similar issue, just not working on vectors but on regular good old floating point numbers for time series forecasting. Would you mind me contacting you via mail with a few short questions regarding your InferenceHelper implementation?", "@fritzfitzpatrick , I'd rather help you here in case anyone else runs into a similar issue.\r\n\r\nHere's the code I used. In my case, I had a (4,)-shaped tensor of zeros and ones as outputs, hence 16 possible outcomes, hence my sampling function. However, the sampling function was just for helping me debug whereas you can get by with a function that does nothing.\r\n\r\n# inference\r\n            elif self.mode == 'inference':\r\n\r\n                def initialize_fn():\r\n                    finished = tf.tile([False], [FLAGS.batch_size])\r\n                    start_inputs = tf.fill([FLAGS.batch_size, 4], -1.)\r\n                    return (finished, start_inputs)\r\n\r\n                def sample_fn(time, outputs, state):\r\n                    del time, state\r\n                    outputs = tf.cast(tf.round(tf.nn.sigmoid(outputs)),\r\n                                      dtype=tf.int32)\r\n                    sample_ids = outputs[:, 0] + 2 * outputs[:, 1] + \\\r\n                                 4 * outputs[:, 2] + 8 * outputs[:, 3]\r\n                    return sample_ids\r\n\r\n                def next_inputs_fn(time, outputs, state, sample_ids):\r\n                    del time, sample_ids\r\n                    squashed_logits = tf.nn.sigmoid(outputs)\r\n                    binary_decoder_outputs = tf.round(squashed_logits)\r\n                    finished = tf.equal(\r\n                        0.,\r\n                        tf.reduce_sum(binary_decoder_outputs, axis=1))\r\n                    all_finished = tf.reduce_all(finished)\r\n                    next_inputs = tf.cond(\r\n                        all_finished,\r\n                        # If we're finished, the next_inputs value doesn't matter\r\n                        lambda: tf.zeros_like(outputs),\r\n                        lambda: squashed_logits)\r\n                    return (finished, next_inputs, state)\r\n\r\n                helper = CustomHelper(initialize_fn=initialize_fn,\r\n                                      sample_fn=sample_fn,\r\n                                      next_inputs_fn=next_inputs_fn)", "@adarob @ebrevdo Just my personal opinion, but even though the issue is technically solved by giving the programmer the ability to implement their own helper through `InferenceHelper`, I don't feel like this practically solved the issue since (in my opinion) one should be able to pipe the output at one time step to the next time step without needing to write custom code.", "@RylanSchaeffer are you suggesting having a CategoricalInferenceHelper that provides categorical sampling, as is most often used? I think it's reasonable to add that.", "@adarob that would be helpful, but I was referring to something slightly different. I haven't looked at this in 5+ months, so maybe the module has changed, but my understanding is that the sample_fn is used to collapse the rnn cell's output tensor into a single number (either deterministically or stochastically), which is then reconstituted as an input tensor for the next step. This is useful if I want the input tensor to represent a discrete input e.g. a word, but in the context that I was using the library, I wanted the output tensor to be passed to the next step unchanged.\r\n\r\nFor concreteness, my understanding of the current implementation is like this:\r\noutput at time T is [0.5, 0.1, 0.1, 0.1] => 0th element is sampled => [1., 0., 0., 0.] is passed to time step T+1\r\n\r\nMy desired behavior:\r\noutput at time T is  [0.5, 0.1, 0.1, 0.1]  =>  [0.5, 0.1, 0.1, 0.1]  is passed to time step T+1\r\n\r\n", "@RylanSchaeffer Thanks for sharing. My code is a bit more bare bones, but I should be able to wire something together based on your snippet.\r\n\r\n@adarob Passing the decoder output at time step T as the decoder input at time step T+1 is exactly what I am after, and I have been struggling a bit with implementing this through other helpers (mostly because of my lack of understanding re: sampling in TF and/or how to deal with function arguments like time, that I now see Rylan just deletes from the function scope).\r\n\r\nOverall I am really happy with TensorFlow and it's a great entry point for ML beginners like me, so thanks a lot!", "What you're requesting is possible by setting sample_fn=tf.identity.\r\nHowever it is redundant to have the unmodified outputs be the inputs in an RNN since this is already passed through as the state.", "@adarob Thanks, I'll try tf.identity. \r\n\r\nI will pass the modified decoder output (through a dense layer) of time step T as inputs in time step T+1 to the decoder, I was unclear in my above post, sorry for that.", "I'm facing a similar issue. I've described what I'm trying to do at stackoverflow.  \r\nhttps://stackoverflow.com/questions/48216786/seq2seq-in-tensorflow-without-embeddings\r\n\r\nHas anyone managed to use a CustomHelper for this? ", "@nishaskinner I think Rylan managed to get it to work, but I am getting stuck. \r\n\r\n@RylanSchaeffer can I pick your brain about this in a mail? I want to understand and share my knowledge, as apparently there's more people out there that need help implementing the CustomHelper outside the nmt domain.\r\n\r\nI am creating a generic example for regression using a sequence to sequence architecture that will work with any input sequence length and number of features, as well as any output sequence length and number of features.\r\n\r\nThe work in progress notebook can be found [here](https://github.com/fritzfitzpatrick/tensorflow-seq2seq-generic-example/blob/master/tensorflow_seq2seq_generic_example_for_github.ipynb), and I want to turn this into a beginner friendly blog post as tutorials on non-language applications are few and far between.\r\n\r\nThanks a lot in advance!", "@adarob , I agree that while you're correct, the lack of documentation and examples (and a general explanation of what a Helper even does) makes writing a CustomHelper difficult for people who aren't familiar with the library, as these comments demonstrate.\r\n\r\n@nishaskinner Yes, my CustomHelper worked as I intended. I posted the code above.\r\n\r\n@fritzfitzpatrick , what exactly would you like to talk about? My email address is rylanschaeffer@gmail.com, but like I said above, I'd prefer to keep my conversations public in case others have similar questions.", "Sorry to respond late.  Is anyone here interested in adding a comprehensive\ndocstring to the module explaining helpers and how to create your own?  You\ncould base it on existing unit tests.\n\nOn Sat, Feb 10, 2018, 9:28 AM Rylan Schaeffer <notifications@github.com>\nwrote:\n\n> @adarob <https://github.com/adarob> , I agree that while you're correct,\n> the lack of documentation and examples (and a general explanation of what a\n> Helper even does) makes writing a CustomHelper difficult for people who\n> aren't familiar with the library, as these comments demonstrate.\n>\n> @nishaskinner <https://github.com/nishaskinner> Yes, my CustomHelper\n> worked as I intended. I posted the code above.\n>\n> @fritzfitzpatrick <https://github.com/fritzfitzpatrick> , what exactly\n> would you like to talk about? My email address is rylanschaeffer@gmail.com,\n> but like I said above, I'd prefer to keep my conversations public in case\n> others have similar questions.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12065#issuecomment-364673968>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1ApLs7vSeSXl6VHjH2PNOuUO3v0ks5tTdHAgaJpZM4Ou28p>\n> .\n>\n", "@RylanSchaeffer I have tried using a go token as well as the last value of the encoder input sequence as my start_inputs in the initializer_fn, and I have tried using the projection layer outputs as well as the original signal as the next_inputs in the next_inputs_fn. \r\n\r\nI am training my model on a linear time series, it is seeing a sequence [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6] and should learn to predict [0.7, 0.8]. During training and validation the l2 regulated loss goes down to 0.010 and the validation prediction using the training helper looks somewhat like [0.69, 0.79], so close enough.\r\n\r\nWhen I save that model, restore the variables to an identical inference model and run it, the output looks more like this [-0.14, 0.19] etc. I have no idea why it is off like that. Attached is the model in its current form, but as I said above, I have been through a few configurations. I can't seem to find the reason as to why the inference prediction is so vastly off the mark.\r\n`\r\n\r\n    # define training encoder\r\n    inf_enc_out, inf_enc_state = tf.nn.dynamic_rnn(\r\n        enc_cell, \r\n        inf_enc_inp,\r\n        dtype = tf.float32,\r\n        sequence_length = seq_length_inp,\r\n        time_major = time_major)\r\n    \r\n    with tf.variable_scope('projection_layer', reuse = tf.AUTO_REUSE):\r\n        from tensorflow.python.layers.core import Dense\r\n        projection_layer = Dense(features_dec_exp_out)\r\n\r\n    # define inference custom helper\r\n    def initialize_fn():\r\n        finished = tf.tile([False], [batch_size])\r\n        enc_inp_end = inf_enc_inp[0, observation_length - 1, 0]\r\n        start_inputs = tf.reshape(enc_inp_end, shape=[1, 1]) \r\n        return (finished, start_inputs)\r\n    \r\n    def sample_fn(time, outputs, state):\r\n        return tf.constant([0])\r\n\r\n    def next_inputs_fn(time, outputs, state, sample_ids):\r\n        finished = time >= prediction_length\r\n        next_inputs = outputs\r\n        return (finished, next_inputs, state)\r\n\r\n    inf_custom_helper = tf.contrib.seq2seq.CustomHelper(\r\n        initialize_fn = initialize_fn,\r\n        sample_fn = sample_fn,                      \r\n        next_inputs_fn = next_inputs_fn)\r\n\r\n    # create inference decoder\r\n    inf_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n        dec_cell, \r\n        inf_custom_helper, \r\n        inf_enc_state,\r\n        projection_layer)\r\n\r\n    # create inference dynamic decoding\r\n    inf_dec_out, inf_dec_state, inf_dec_out_seq_length = tf.contrib.seq2seq.dynamic_decode(\r\n        inf_decoder, \r\n        output_time_major = time_major)\r\n\r\n    # extract prediction from decoder output\r\n    inf_output_dense = inf_dec_out.rnn_output`\r\n\r\n", "@fritzfitzpatrick did you have any luck with this? I have been struggling with this for days and training is fine with the traininghelper, however I am still completely lost with what should happen during inference.", "@fritzfitzpatrick I'm also struggling with same issue. Could you please share the solution or guide me, if you have managed to fix this?", "@MrfksIv @VinoJose \r\n\r\nI indeed have made some progress using a seq2seq architecture, but it is not very accurate and tends to generalise towards a perfectly linear horizontal line after just a few iterations. I have however tested another architecture that works quite nicely. I am currently pretty tapped out with business travels, but I will try to upload a python notebook for you to peruse.\r\n\r\nIf anyone else finally got multistep numerical time signal predictions going with the inference or custom helper using a seq2seq architecture, please let us know. Can't be that hard, can it.", "@VinoJose @fritzfitzpatrick \r\nBy following [this code](https://stackoverflow.com/questions/44639647/tensorflow-1-2-how-to-setup-time-series-prediction-at-inference-time-using-seq2s) I have successfully trained the network with a RMSE of 0.02. \r\nThe problem with the code is that the training-helper expects and requires the true +1 timesteps which are of course unknown during inference. My naive solution to this was to give  to the network the predicted sequence as the correct one. \r\nThe `predict_sequence_end` below is test data neither trained, nor validated on. \r\n\r\n`for i in range(steps_ahead):\r\n    predict_sequence_end = sess.run(h, feed_dict={enc_inp: \r\n    predict_sequence_end.reshape((1,batch_steps,1)), \r\n                                                                 expect:predict_sequence_end.reshape((1,batch_steps,1)), \r\n                                                                 expect_length: [n_steps]* predict_sequence_end.shape[0],\r\n                                                                 keep_prob: keepprob})` \r\n\r\nFrom my limited understanding, this bypasses the problem of the training helper, does it not? By not running the train_op, I assume that the network weights remain constant. This gives quite good results, although I am not sure if I am 'cheating' in any way. Do you have any ideas on this?  \r\n![image](https://user-images.githubusercontent.com/10939319/42036454-3f5157f6-7aee-11e8-9e69-c5b63d140cec.png)\r\n", "lacking a `CategoricalInferenceHelper` I used the `GreedyEmbeddingHelper` with\r\n` embedding  = tf.eye(VOCAB_SIZE, dtype=tf.float32, name='embedding')`\r\n\r\nHope this helps anyone who reaches this thread", "I got it to work for no embedding in a much simpler way, using a very rudimentary `InferenceHelper`:\r\n\r\n```\r\ninference_helper = tf.contrib.seq2seq.InferenceHelper(\r\n            sample_fn=lambda outputs: outputs,\r\n            sample_shape=[dim],\r\n            sample_dtype=dtypes.float32,\r\n            start_inputs=start_tokens,\r\n            end_fn=lambda sample_ids: False)\r\n```\r\n\r\nMy inputs are floats with the shape `[batch_size, time, dim]`. For the example above with @MrfksIv 's plot, `dim` would be 1, but this can easily be extended to more dimensions.  Here's the relevant chunk of the code:\r\n\r\n```\r\n# Dense layer to translate the decoder's output at each time\r\n# step.\r\nprojection_layer = tf.layers.Dense(\r\n    units=1,  # = dim\r\n    kernel_initializer=tf.truncated_normal_initializer(\r\n        mean=0.0, stddev=0.1))\r\n\r\n# Training Decoder\r\ntraining_decoder_output = None\r\nwith tf.variable_scope(\"decode\"):\r\n    # output_data doesn't exist during prediction phase.\r\n    if output_data is not None:\r\n        # Prepend the \"go\" token\r\n        go_tokens = tf.constant(go_token, shape=[batch_size, 1, 1])\r\n        dec_input = tf.concat([go_tokens, target_data], axis=1)\r\n\r\n        # Helper for the training process.\r\n        training_helper = tf.contrib.seq2seq.TrainingHelper(\r\n            inputs=dec_input,\r\n            sequence_length=[output_size] * batch_size)\r\n\r\n        # Basic decoder\r\n        training_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n            dec_cell,  training_helper, enc_state, projection_layer)\r\n\r\n        # Perform dynamic decoding using the decoder\r\n        training_decoder_output = tf.contrib.seq2seq.dynamic_decode(\r\n            training_decoder, impute_finished=True,\r\n            maximum_iterations=output_size)[0]\r\n\r\n# Inference Decoder\r\n# Reuses the same parameters trained by the training process.\r\nwith tf.variable_scope(\"decode\", reuse=tf.AUTO_REUSE):\r\n    start_tokens = tf.constant(\r\n        go_token, shape=[batch_size, 1])\r\n\r\n    # The sample_ids are the actual output in this case (not dealing with any logits here).\r\n    # My end_fn is always False because I'm working with a generator that will stop giving \r\n    # more data. You may extend the end_fn as you wish. E.g. you can append end_tokens \r\n    # and make end_fn be true when the sample_id is the end token.\r\n    inference_helper = tf.contrib.seq2seq.InferenceHelper(\r\n        sample_fn=lambda outputs: outputs,\r\n        sample_shape=[1],  # again because dim=1\r\n        sample_dtype=dtypes.float32,\r\n        start_inputs=start_tokens,\r\n        end_fn=lambda sample_ids: False)\r\n\r\n    # Basic decoder\r\n    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\r\n        inference_helper, enc_state, projection_layer)\r\n\r\n    # Perform dynamic decoding using the decoder\r\n    inference_decoder_output = tf.contrib.seq2seq.dynamic_decode(\r\n        inference_decoder, impute_finished=True,\r\n        maximum_iterations=output_size)[0]\r\n```", "@Andreea-G could you please elaborate on this flag `end_fn=lambda sample_ids: False` in your `InferenceHelper`? Wouldn't this be equal to setting `impute_finished=False`?\r\n\r\nI use the following lambda function `end_fn=lambda outputs: tf.greater_equal(tf.shape(outputs)[1], self.outputs_len)` where `self.outputs_len` are the true lengths of the target sequences. In this setting, during training, the validation loss doesn't decrease. If I set `impute_finished=False`, validation loss does decrease. I'm having difficulties in understanding this behavior.\r\n", "@Andreea-G Could you elaborate the `CustomHelper` for `ConvLSTMCell`. That means input is of shape `[batch_size, time_step, row_number, column_number, channel_number]`?", "> Sorry to respond late. Is anyone here interested in adding a comprehensive docstring to the module explaining helpers and how to create your own? You could base it on existing unit tests.\r\n> [\u2026](#)\r\n> On Sat, Feb 10, 2018, 9:28 AM Rylan Schaeffer ***@***.***> wrote: @adarob <https://github.com/adarob> , I agree that while you're correct, the lack of documentation and examples (and a general explanation of what a Helper even does) makes writing a CustomHelper difficult for people who aren't familiar with the library, as these comments demonstrate. @nishaskinner <https://github.com/nishaskinner> Yes, my CustomHelper worked as I intended. I posted the code above. @fritzfitzpatrick <https://github.com/fritzfitzpatrick> , what exactly would you like to talk about? My email address is ***@***.***, but like I said above, I'd prefer to keep my conversations public in case others have similar questions. \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#12065 (comment)](https://github.com/tensorflow/tensorflow/issues/12065#issuecomment-364673968)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABtim1ApLs7vSeSXl6VHjH2PNOuUO3v0ks5tTdHAgaJpZM4Ou28p> .\r\n\r\n@ebrevdo I would love to do that, if it's still required?", "Sure\n\nOn Mon, Dec 31, 2018, 1:10 AM SHIVAM PRASAD <notifications@github.com wrote:\n\n> Sorry to respond late. Is anyone here interested in adding a comprehensive\n> docstring to the module explaining helpers and how to create your own? You\n> could base it on existing unit tests.\n> \u2026 <#m_4343391276730480940_>\n> On Sat, Feb 10, 2018, 9:28 AM Rylan Schaeffer ***@***.***> wrote: @adarob\n> <https://github.com/adarob> https://github.com/adarob , I agree that\n> while you're correct, the lack of documentation and examples (and a general\n> explanation of what a Helper even does) makes writing a CustomHelper\n> difficult for people who aren't familiar with the library, as these\n> comments demonstrate. @nishaskinner <https://github.com/nishaskinner>\n> https://github.com/nishaskinner Yes, my CustomHelper worked as I\n> intended. I posted the code above. @fritzfitzpatrick\n> <https://github.com/fritzfitzpatrick> https://github.com/fritzfitzpatrick\n> , what exactly would you like to talk about? My email address is\n> ***@***.***, but like I said above, I'd prefer to keep my conversations\n> public in case others have similar questions. \u2014 You are receiving this\n> because you were mentioned. Reply to this email directly, view it on GitHub\n> <#12065 (comment)\n> <https://github.com/tensorflow/tensorflow/issues/12065#issuecomment-364673968>>,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim1ApLs7vSeSXl6VHjH2PNOuUO3v0ks5tTdHAgaJpZM4Ou28p\n> .\n>\n> @ebrevdo <https://github.com/ebrevdo> I would love to do that, if it's\n> still required?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12065#issuecomment-450622991>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim2VegMe-9XpPHhLloJU4YJ7O9Tfyks5u-dSfgaJpZM4Ou28p>\n> .\n>\n"]}, {"number": 12064, "title": "Fix typo in RELEASE.md", "body": "", "comments": ["Can one of the admins verify this patch?", "@standy66, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @vrv and @av8ramit to be potential reviewers.", "@tensorflow-jenkins test this please"]}, {"number": 12063, "title": "The name 'final_result:0' refers to a Tensor which does not exist. The operation, 'final_result', does not exist in the graph.", "body": "### System information\r\nLinux Ubuntu 16.04\r\n\r\nTensorflow 1.2.1\r\n\r\nHaving done a clean install of Tensorflow, I have rebuilt the model using training data which went fine. On classifying I receive:\r\n\r\n```\r\n  File \"/home/tass/OpenTass/Tensorflow/TassClassifier.py\", line 235, in classify\r\n    softmaxTensor = sess.graph.get_tensor_by_name('final_result:0')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2733, in get_tensor_by_name\r\n    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2584, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2626, in _as_graph_element_locked\r\n    \"graph.\" % (repr(name), repr(op_name)))\r\nKeyError: \"The name 'final_result:0' refers to a Tensor which does not exist. The operation, 'final_result', does not exist in the graph.\"\r\n```\r\nAny ideas? Thanks in advance.", "comments": ["Sorry my bad, I forgot to create graph before starting classification.", "Hey, I got the same error, what's that solution?", "I got same issue what 's the soln"]}]