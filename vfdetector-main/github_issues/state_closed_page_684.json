[{"number": 33074, "title": "tf.keras accepts incorrect CNN input shapes from tf.data.Dataset when eager execution is disabled", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.7\r\n- Device: CPU\r\n\r\n**Describe the current behavior**\r\nI am using a `tf.data.Dataset` to train a `tf.keras` model. My CNN input layer size is (72, 96, 1). I find that if I input a tensor with size (96, 72, 1) (note the reversed dimensions), training completes successfully. However, if I enable eager execution, a `ValueError` is produced as expected.\r\n\r\n**Describe the expected behavior**\r\nA `ValueError` should be produced in response to the incorrect input shape, whether or not eager execution is enabled.\r\n\r\n**Code to reproduce the issue**\r\nNOTE: With the code below, there is no error message. However, uncomment the `tf.enable_eager_execution()` line and you will get a `ValueError`.\r\n\r\n```\r\n#!/usr/bin/env python3\r\n  \r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\n# UNCOMMENT THIS LINE AND YOU WILL GET AN ERROR\r\n# tf.enable_eager_execution()\r\n\r\ndef tf_dataset():\r\n\r\n    def data_gen():\r\n        while True:\r\n            yield np.random.rand(72, 96, 1), np.random.rand(18)\r\n\r\n    types = (tf.float64, tf.float64)\r\n    shapes = (tf.TensorShape([None, None, None]), tf.TensorShape([None]))\r\n    dataset = tf.data.Dataset.from_generator(data_gen, types, output_shapes=shapes)\r\n    dataset = dataset.batch(16)\r\n    return dataset\r\n\r\ndataset = tf_dataset()\r\n\r\nmodel = tf.keras.Sequential()\r\n# Notice how the input dimensions are mismatched\r\nmodel.add(layers.InputLayer((96, 72, 1)))\r\nmodel.add(layers.Conv2D(filters=32, kernel_size=4, strides=4, activation='relu'))\r\nmodel.add(layers.Conv2D(filters=32, kernel_size=4, strides=4, activation='relu'))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(18, activation='softmax'))\r\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')\r\nprint(model.summary())\r\nmodel.fit(dataset, steps_per_epoch=1, epochs=1)\r\n```", "comments": ["I could reproduce the issue with TF1.14.0 on colab. Please see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/47a1e1726b762a876c0fc29c00e440b9/untitled182.ipynb).", "@bfmat, I tried with TF 1.15.0rc1 but i didn't see ValueError with or without Eager execution. Please take a look at colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/ebbbb7296a5830c3ee0b852c4ec4db6e/untitled183.ipynb). Thanks!", "@gadagashwini, thanks for creating the gist! I re-ran it with TensorFlow 2.0 as well, and no error is raised.\r\n\r\nIt is worth noting that if I change the data shape to (72, 128) instead of (72, 96), an error is raised as expected. However, if I use (72, 97), no error is raised, which is worrying, considering that the tensor size is incorrect as well as the shape.", "@bfmat,\r\n I could execute with shape (72,128) as well, successfully. Please find the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/591ca795c780d0fcd8ddc88247b5c699/untitled183.ipynb).", "@bfmat,\r\nCan you please refer the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/591ca795c780d0fcd8ddc88247b5c699/untitled183.ipynb) and please let us know if your issue still persists. Thanks! ", "@rmothukuru The Gist exhibits the issue. What I meant is that if you leave the input layer at (96, 72), and set the data shape to (72, 128), it raises an error. However, if you set the data shape to (72, 97) while still leaving the input layer at (96, 72), no error is raised.", "@bfmat,\r\nThe error, ` ValueError: Error when checking input: expected input_1 to have shape (96, 72, 1) but got array with shape (72, 97, 1)` is being triggered when `data shape to (72, 97) while still leaving the input layer at (96, 72)`.\r\n\r\nThe error disappears if the Shape of the Data and the Shape of the Input Layer is the same. For example, Shapes of Input Layer and that of Data is `(72, 128, 1)`, the Code runs without any Error, both for Eager Execution and Graph Mode execution. \r\n\r\nPlease find the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/bd539ba720fd6d38cc48bf84cc68e075/untitled183.ipynb). \r\n\r\nAs per my understanding, it is the intended behavior to expect same shape for Input Layer and that for the Data. Please let me know your opinion regarding the same. Thanks!", "@rmothukuru @tensorflowbutler Apologies for the delay. Yes, it is still an issue. In the gist posted most recently, the input and data shapes are both `(72, 128, 1)`. No error is expected in this case. The problem is that, when eager execution is disabled, _no_ error is raised when, for instance, the data shape is `(128, 72, 1)` while the input shape is `(72, 128, 1)`.", "To add to this issue from keras perspective, maybe it helps finding out the error.\r\n\r\nTF version: 1.14.0\r\nKeras version 2.2.4\r\n\r\nI have a model with input shape as:\r\n\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 1024, 1024, 3)     0   \r\n\r\nI'm feeding this model those three inputs of different sizes :\r\n\r\n        ri_teacher1 = self.model1([ri_larger])\r\n        ri_teacher2 = self.model1([ri_small])\r\n        ri_teacher3 = self.model1([ri])\r\n\r\nprint out from the inputs:\r\n\r\nri_larger Tensor(\"lambda_11/Tile:0\", shape=(?, 1024, 512, 4), dtype=float32)\r\nri_small Tensor(\"lambda_10/AvgPool:0\", shape=(?, 256, 256, 3), dtype=float32)\r\nri Tensor(\"conv2d_45/Relu:0\", shape=(?, 512, 512, 4), dtype=float32)\r\n\r\nNone of those raise an error and training proceeds...while it is expected it should raise an error since the input shape is (?,1024,1024,3), but I am feeding it (?,1024,512,4), (?,256,256,3) and (?,512,512,4) respectively.\r\n\r\nThis is using Keras with TF backend\r\n", "I think we should error out. The fact that:\r\n1. it doesn't put any warning in `model.fit`\r\n2. it puts warning in `model.call`\r\nis a little bit troubling. Here's where it swallows the error:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/network.py#L958-L964\r\nReassigning to author of that function.", "@bfmat   Closing this issue since we're only making critical bug fixes to graph-mode code. Please feel free to reopen the issue if necessary.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33074\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33074\">No</a>\n"]}, {"number": 33073, "title": "Update mel_ops.py", "body": "This update explains the mel scale formula that is used in Tensorflow implementation. This can resolve the reproducability/compatibility issues signal processing people would face when they are working with other signal processing library such as librosa. ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33073) for more info**.\n\n<!-- need_sender_cla -->", "(cc' @rryan :)", "Hi. Please make the PR against master as the release branches are only updated during releases."]}, {"number": 33072, "title": "Distributed training Keras ConvLSTM2D layer with stateful=True failing", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0-dev20191004\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0.130/7.6\r\n- GPU model and memory: 2x GeForce GTX 1080 Ti, 10481 MB\r\n\r\n**Describe the current behavior**\r\nTraining Keras model with ConvLSTM2D stateful layer fails to train under distributed scope.\r\nIf I set stateful to False it distributed learning works.\r\nError message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:\\work\\aicomp\\challenges\\fatigue\\ws\\stateful_mirrored\\src\\test.py\", line 17, in <module>\r\n    model.fit(dataset, steps_per_epoch=1, epochs=1)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 766, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 333, in fit\r\n    total_epochs=epochs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 554, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 600, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 493, in _initialize\r\n    *args, **kwds))\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2320, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2628, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2517, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 943, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 435, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 73, in distributed_function\r\n    per_replica_function, args=(x, y, sample_weights))\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\", line 764, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\", line 1820, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\distribute\\mirrored_strategy.py\", line 688, in _call_for_each_replica\r\n    fn, args, kwargs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\distribute\\mirrored_strategy.py\", line 200, in _call_for_each_replica\r\n    coord.join(threads)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\training\\coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\six.py\", line 693, in reraise\r\n    raise value\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\training\\coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\distribute\\mirrored_strategy.py\", line 909, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 264, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\", line 311, in train_on_batch\r\n    output_loss_metrics=output_loss_metrics))\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\", line 252, in _process_single_batch\r\n    training=training))\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\", line 127, in _model_loss\r\n    outs = model(inputs, **kwargs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 759, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 712, in call\r\n    convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 868, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\convolutional_recurrent.py\", line 299, in __call__\r\n    return super(ConvRNN2D, self).__call__(inputs, **kwargs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\", line 631, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 759, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\convolutional_recurrent.py\", line 938, in call\r\n    initial_state=initial_state)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\convolutional_recurrent.py\", line 397, in call\r\n    updates.append(K.update(self.states[i], states[i]))\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 1557, in update\r\n    return state_ops.assign(x, new_x)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\state_ops.py\", line 228, in assign\r\n    return ref.assign(value, name=name)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\distribute\\values.py\", line 1040, in assign\r\n    return self._assign_func(f=assign_fn, *args, **kwargs)\r\n  File \"D:\\work\\aicomp\\tf2_nightly_venv\\lib\\site-packages\\tensorflow_core\\python\\distribute\\values.py\", line 1020, in _assign_func\r\n    variable_type=\"MirroredVariable\"))\r\nValueError: You must specify an aggregation method to update a MirroredVariable in Replica Context. You can do so by passing an explicit value for argument `aggregation` to tf.Variable(..).e.g. `tf.Variable(..., aggregation=tf.VariableAggregation.SUM)``tf.VariableAggregation` lists the possible aggregation methods.This is required because MirroredVariable should always be kept in sync. When updating them or assigning to them in a replica context, we automatically try to aggregate the values before updating the variable. For this aggregation, we need to know the aggregation method. Another alternative is to not try to update such MirroredVariable in replica context, but in cross replica context. You can enter cross replica context by calling `tf.distribute.get_replica_context().merge_call(merge_fn, ..)`.Inside `merge_fn`, you can then update the MirroredVariable using `tf.distribute.StrategyExtended.update()`.\r\n```\r\n\r\n**Describe the expected behavior**\r\nTrain model distributed without error.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.layers import Input, ConvLSTM2D\r\nfrom tensorflow.keras.models import Model\r\n\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\n\r\nwith mirrored_strategy.scope():\r\n    model_input = Input(shape=(None, 3, 3, 1), batch_size=1)\r\n    model = ConvLSTM2D(1, kernel_size=3, stateful=True)(model_input)    \r\n    model = Model(inputs=[model_input], outputs=[model])\r\n    model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adadelta())    \r\n    input_dataset = tf.data.Dataset.from_tensors(tf.random.uniform([1, 3, 3, 1])).batch(1)\r\n    output_dataset = tf.data.Dataset.from_tensors(tf.random.uniform([1, 3, 3, 1])).batch(1)\r\n    dataset = tf.data.Dataset.zip((input_dataset, output_dataset))\r\n    model.fit(dataset, steps_per_epoch=1, epochs=1)\r\n```\r\n", "comments": ["Could reproduce the issue with TF Version 2.0. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/1b2a8bfdef1007a0c70b78f327a8dbd2/33072.ipynb).", "We currently [don't support](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent.py#L434) tf.distribute strategy with stateful RNNs.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33072\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33072\">No</a>\n", "By which release is the support to be extended? Any plans?\r\n\r\n"]}, {"number": 33071, "title": "Explicitly Deleted Function Results in Failed Build with Master Branch on Windows", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.0 Nightly\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source): VS2017\r\n- CUDA/cuDNN version: 10.0\r\n\r\nWhen attempting to build TF 2.0 from the master branch off Github, the build fails with the following error.\r\n\r\n```\r\nERROR: D:/sdks/tensorflow/tensorflow/core/profiler/internal/gpu/BUILD:98:1: C++ compilation of rule '//tensorflow/core/profiler/internal/gpu:cupti_tracer' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/adam/_bazel_adam/fcwavxxi/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.13.26128\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.13.26128\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.13.26128\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.13.26128\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.13.26128\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\MSBuild\\15.0\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\\\MSBuild\\15.0\\bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\Tools\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Adam/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Adam/Anaconda3/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\Adam\\AppData\\Local\\Temp\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=C:\\Users\\Adam\\AppData\\Local\\Temp\r\n  C:/Users/Adam/Anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cupti_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Iexternal/local_config_cuda/cuda/cuda/extras/CUPTI/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/extras/CUPTI/include /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX -std=c++14 -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -DGOOGLE_CUDA=1 /Fobazel-out/x64_windows-opt/bin/tensorflow/core/profiler/internal/gpu/_objs/cupti_tracer/cupti_tracer.o /c tensorflow/core/profiler/internal/gpu/cupti_tracer.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ncl : Command line warning D9002 : ignoring unknown option '-std=c++14'\r\nexternal/com_google_absl\\absl/meta/type_traits.h(141): error C2280: 'void tensorflow::profiler::AnnotationMap::operator =(const tensorflow::profiler::AnnotationMap &)': attempting to reference a deleted function\r\n.\\tensorflow/core/profiler/internal/gpu/cupti_tracer.h(198): note: see declaration of 'tensorflow::profiler::AnnotationMap::operator ='\r\nexternal/com_google_absl\\absl/meta/type_traits.h(121): note: see reference to alias template instantiation 'IsCopyAssignableImpl<tensorflow::profiler::AnnotationMap>' being compiled\r\nexternal/com_google_absl\\absl/meta/type_traits.h(150): note: see reference to class template instantiation 'absl::type_traits_internal::is_detected<absl::type_traits_internal::IsCopyAssignableImpl,T>' being compiled\r\n        with\r\n        [\r\n            T=tensorflow::profiler::AnnotationMap\r\n        ]\r\nexternal/com_google_absl\\absl/meta/type_traits.h(432): note: see reference to class template instantiation 'absl::is_copy_assignable<T>' being compiled\r\n        with\r\n        [\r\n            T=tensorflow::profiler::AnnotationMap\r\n        ]\r\nexternal/com_google_absl\\absl/types/internal/optional.h(173): note: see reference to class template instantiation 'absl::is_trivially_copy_assignable<tensorflow::profiler::AnnotationMap>' being compiled\r\n.\\tensorflow/core/profiler/internal/gpu/cupti_tracer.h(262): note: see reference to class template instantiation 'absl::optional<tensorflow::profiler::AnnotationMap>' being compiled\r\n.\\tensorflow/core/profiler/internal/gpu/cupti_tracer.h(198): note: 'void tensorflow::profiler::AnnotationMap::operator =(const tensorflow::profiler::AnnotationMap &)': function was explicitly deleted\r\nexternal/com_google_absl\\absl/meta/type_traits.h(121): error C2248: 'tensorflow::profiler::AnnotationMap::operator =': cannot access private member declared in class 'tensorflow::profiler::AnnotationMap'\r\n.\\tensorflow/core/profiler/internal/gpu/cupti_tracer.h(198): note: see declaration of 'tensorflow::profiler::AnnotationMap::operator ='\r\n.\\tensorflow/core/profiler/internal/gpu/cupti_tracer.h(177): note: see declaration of 'tensorflow::profiler::AnnotationMap'\r\nexternal/com_google_absl\\absl/meta/type_traits.h(144): error C2280: 'void tensorflow::profiler::AnnotationMap::operator =(const tensorflow::profiler::AnnotationMap &)': attempting to reference a deleted function\r\n.\\tensorflow/core/profiler/internal/gpu/cupti_tracer.h(198): note: see declaration of 'tensorflow::profiler::AnnotationMap::operator ='\r\nexternal/com_google_absl\\absl/meta/type_traits.h(121): note: see reference to alias template instantiation 'IsMoveAssignableImpl<tensorflow::profiler::AnnotationMap>' being compiled\r\nexternal/com_google_absl\\absl/meta/type_traits.h(155): note: see reference to class template instantiation 'absl::type_traits_internal::is_detected<absl::type_traits_internal::IsMoveAssignableImpl,T>' being compiled\r\n        with\r\n        [\r\n            T=tensorflow::profiler::AnnotationMap\r\n        ]\r\nexternal/com_google_absl\\absl/types/internal/optional.h(328): note: see reference to class template instantiation 'absl::is_move_assignable<T>' being compiled\r\n        with\r\n        [\r\n            T=tensorflow::profiler::AnnotationMap\r\n        ]\r\nexternal/com_google_absl\\absl/types/optional.h(120): note: see reference to class template instantiation 'absl::optional_internal::assign_copy_traits<T>' being compiled\r\n        with\r\n        [\r\n            T=tensorflow::profiler::AnnotationMap\r\n        ]\r\n.\\tensorflow/core/profiler/internal/gpu/cupti_tracer.h(198): note: 'void tensorflow::profiler::AnnotationMap::operator =(const tensorflow::profiler::AnnotationMap &)': function was explicitly deleted\r\n```", "comments": ["Please Close - Fixed by updating VS2017 to latest release and downgrading Bazel."]}, {"number": 33070, "title": "Add files via upload", "body": "Better readme file", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33070) for more info**.\n\n<!-- need_sender_cla -->", "> @googlebot I signed it!\r\n\r\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33070) for more info**.\n\n<!-- ok -->", "Thanks, but this copy has been worked over and agreed upon. I think we're good as-is \ud83d\ude00"]}, {"number": 33069, "title": "TF2.0.0: tensorflow.python.framework.errors_impl.UnknownError: CUDNN_STATUS_BAD_PARAM", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows Server 2019**\r\n- TensorFlow installed from (source or binary): **pip install tensorflow-gpu**\r\n- TensorFlow version (use command below): **2.0.0**\r\n- Python version:**3.6.9**\r\n- CUDA/cuDNN version:**10.1/7.6.4**\r\n- GPU model and memory: **GTX1080Ti  x4**\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I succeeded in debugging on my own host, I occured an error on windows server 2019 for debugging.The error message is below:\r\n\r\n```cmd\r\n2019-10-05 21:54:41.858023: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\nTraceback (most recent call last):\r\n  File \"E:/PythonProjects/Sequence-Labeling/bi-lstm_tf2.py\", line 139, in <module>\r\n    loss_value, grads = gradient_descent(model, train_x, train_y)\r\n  File \"E:/PythonProjects/Sequence-Labeling/bi-lstm_tf2.py\", line 50, in gradient_descent\r\n    y_predict = model(inputs)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py36-tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 891, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py36-tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\", line 256, in call\r\n    return super(Sequential, self).call(inputs, training=training, mask=mask)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py36-tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 708, in call\r\n    convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py36-tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 860, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py36-tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\wrappers.py\", line 528, in __call__\r\n    return super(Bidirectional, self).__call__(inputs, **kwargs)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py36-tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 891, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py36-tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\wrappers.py\", line 642, in call\r\n    initial_state=forward_state, **kwargs)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py36-tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\", line 623, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py36-tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 891, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py36-tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent_v2.py\", line 961, in call\r\n    **cudnn_lstm_kwargs)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py36-tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent_v2.py\", line 1160, in cudnn_lstm\r\n    rnn_mode='lstm', sequence_lengths=sequence_length)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py36-tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_cudnn_rnn_ops.py\", line 2008, in cudnn_rnnv3\r\n    ctx=_ctx)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py36-tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_cudnn_rnn_ops.py\", line 2114, in cudnn_rnnv3_eager_fallback\r\n    attrs=_attrs, ctx=_ctx, name=name)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py36-tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\n**tensorflow.python.framework.errors_impl.UnknownError: CUDNN_STATUS_BAD_PARAM**\r\n**in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)' [Op:CudnnRNNV3]**\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nCode works fine\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n\t# Currently, memory growth needs to be the same across GPUs\r\n\ttry:\r\n\t\tfor gpu in gpus:\r\n\t\t\ttf.config.experimental.set_memory_growth(gpu, True)\r\n\texcept RuntimeError as e:\r\n\t\tprint(e)\r\n\r\ndef gradient_descent(model, inputs, targets):\r\n\twith tf.GradientTape() as tape:\r\n\t\t# compute loss value\r\n\t\ty_predict = model(inputs)\r\n\t\tloss_value = tf.keras.losses.categorical_crossentropy(y_true = targets,\r\n\t\t                                                      y_pred = y_predict)\r\n\treturn loss_value, tape.gradient(loss, model.trainable_variables)\r\n\r\nwith tf.device('/device:gpu:0'):\r\n    model = tf.keras.Sequential([\r\n\t\t    tf.keras.Input(shape = (sequence_len, num_feature),\r\n\t\t                   name = 'InputLayer'),\r\n\t\t    tf.keras.layers.Masking(mask_value = 0.,\r\n\t\t                            input_shape = (sequence_len, num_feature)),\r\n\t\t    tf.keras.layers.Bidirectional(\r\n\t\t\t    tf.keras.layers.LSTM(units = 50, return_sequences = True),\r\n\t\t\t    name = 'BiLSTM-1'),\r\n\t\t    tf.keras.layers.Dense(units = 3, activation = 'softmax',\r\n\t\t                          name = 'Softmax')])\r\n\r\n    train_x, train_y = train_dataset.next_batch()\r\n    loss_value, grads = gradient_descent(model, train_x, train_y)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@HsinJhao, Please downgrade CUDA to 10.0 version and cuDNN version to 7.4 and check. And also provide the complete code to reproduce the issue. Thanks! ", "Thanks your  reply! @gadagashwini \r\nI am trying to downgrade CUDA to  10.0 version and cuDNN version to 7.4.2.\r\nThe code and some data files is provided to you! Hope it can help! \r\nThanks again!\r\n\r\n[codeAndData.zip](https://github.com/tensorflow/tensorflow/files/3697284/codeAndData.zip)\r\n", "@gadagashwini\r\nThanks your response!\r\nI think i find what is wrong when i read this sentence:\r\n\r\n> The requirements to use the cuDNN implementation are: \r\n> 6. Inputs are not masked or strictly right padded.\r\n\r\nin [tf2.0api: tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\r\n\r\nThe problem is that my input data existing all-zeros sequences. It will cause the input data will be all masked by Masking Layer.\r\n\r\nAnyway, Thanks a lot! please close this issue right now! ", "Closing per user request", "> The problem is that my input data existing all-zeros sequences. It will cause the input data will be all masked by Masking Layer.\r\n\r\nThanks a lot.\r\n"]}, {"number": 33068, "title": "Linspace for multi-dimensional tensors", "body": "Issue [32093](https://github.com/tensorflow/tensorflow/issues/32093)\r\n\r\nPrevious PR was this one: [32141](https://github.com/tensorflow/tensorflow/pull/32141)\r\n\r\nThere were comments in the previous PR by @nfelt @yongtang and @alextp , which I believe have been addressed: currently, `linspace_nd` can handle correctly a fully unknown shape, also addressed linter problems and added tests for the requested changes. \r\n\r\nThe newly added function `linspace_nd` behaves like [`np.linspace`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html) and thus is slightly different than `tf.linspace` in two ways:\r\n\r\n* It can accept N-D tensors\r\n* When `num == 0`, the function adheres to Numpy's behaviour (returning an empty tensor), while `tf.linspace` raises an `InvallidArgumentError`\r\n\r\nFor this reason, I added it as a different function.", "comments": ["So I see that some builds are failing due to an old Numpy version (I use `np.linspace` in tests). I can just put the expected result there (without calling the new `np.linspace`), I think this should fix the build.", "@hristo-vrigazov Can you please check build failures? Thanks!", "@gbaned Sure thing, I will try to have a closer look soon, I see that there are some documentation tests failing, and some API problems. I will try to resolve these and update here. Sorry for not checking Windows, Mac, Android and so on locally - I am using only the `nvidia-docker` image for `py3` on a Ubuntu. ", "The reasons for the build failures were as follows:\r\n\r\n* Ubuntu sanity - pylint error that I had missed for a line too long\r\n* Ubuntu CPU - a doctest was not correctly formatted (this test target now runs successfully inside my container), also API compatibility error because of missing API review.\r\n* MacOS - API compatibility error\r\n* Android - some unrelated error, that also happens on the master branch, I couldn't understand the cause\r\n\r\nI have fixed the errors that were actionable on my side I believe. How can I request an API review? Should we first trigger a build and only request the API review when this is the only error left, or can it be started now?", "Removing myself from `R` since I don't have much context here.", "@hristo-vrigazov Could you please check the reviewer comments and keep us posted. Thanks!", "Hi, sure, I will expose it under `linspace` and make sure I don't introduce new issues. I will update when I have progress.", "Two of the builds (MacOS and Ubuntu CPU) are failing because of the API compatibility (adding a new parameter to `linspace`) - still not sure what should I do there, as it requires changes in the golden API. @alextp @yongtang any ideas? \r\n\r\nI am looking into the third one - this test got cached for me - it does seem that it is caused by the difference when `num == 0` - in this PR `linspace` behaves like `numpy` and returns an empty tensor, while the scalar implementation throws `ValueError`. I will change this to adhere to the previous behaviour for backward compatibility and update here when I am done.", "You need to regenerate the API goldens to pass the API compatibility tests.\n\nOn Mon, Dec 9, 2019 at 10:44 AM Hristo Vrigazov <notifications@github.com>\nwrote:\n\n> Two of the builds (MacOS and Ubuntu CPU) are failing because of the API\n> compatibility (adding a new parameter to linspace) - still not sure what\n> should I do there, as it requires changes in the golden API. @alextp\n> <https://github.com/alextp> @yongtang <https://github.com/yongtang> any\n> ideas?\n>\n> I am looking into the third one - this test got cached for me - it does\n> seem that it is caused by the difference when num == 0 - in this PR\n> linspace behaves like numpy and returns an empty tensor, while the scalar\n> implementation throws ValueError. I will change this to adhere to the\n> previous behaviour for backward compatibility and update here when I am\n> done.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/33068?email_source=notifications&email_token=AAABHROMBR7XII7U5UHO5XLQX2GYRA5CNFSM4I5X3TNKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGKGXSI#issuecomment-563375049>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPV6S7HC5XMY2JRYVDQX2GYRANCNFSM4I5X3TNA>\n> .\n>\n\n\n-- \n - Alex\n", "> You need to regenerate the API goldens to pass the API compatibility tests.\r\n> [\u2026](#)\r\n> On Mon, Dec 9, 2019 at 10:44 AM Hristo Vrigazov ***@***.***> wrote: Two of the builds (MacOS and Ubuntu CPU) are failing because of the API compatibility (adding a new parameter to linspace) - still not sure what should I do there, as it requires changes in the golden API. @alextp <https://github.com/alextp> @yongtang <https://github.com/yongtang> any ideas? I am looking into the third one - this test got cached for me - it does seem that it is caused by the difference when num == 0 - in this PR linspace behaves like numpy and returns an empty tensor, while the scalar implementation throws ValueError. I will change this to adhere to the previous behaviour for backward compatibility and update here when I am done. \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#33068?email_source=notifications&email_token=AAABHROMBR7XII7U5UHO5XLQX2GYRA5CNFSM4I5X3TNKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGKGXSI#issuecomment-563375049>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAABHRPV6S7HC5XMY2JRYVDQX2GYRANCNFSM4I5X3TNA> .\r\n> -- - Alex\r\n\r\nThanks, did not realize this is something I can do. Sorry if it's a stupid question, but which target should I run to achieve this? I tried `bazel build //tensorflow/tools/api/golden:api_golden_v1` and then running this target says that there are no executable targets.", "bazel run tensorflow/tools/api/tests:api_compatibility_test --\n--update_goldens=True\n\nOn Mon, Dec 9, 2019 at 11:27 AM Hristo Vrigazov <notifications@github.com>\nwrote:\n\n> You need to regenerate the API goldens to pass the API compatibility tests.\n> \u2026 <#m_-4384847604403523743_>\n> On Mon, Dec 9, 2019 at 10:44 AM Hristo Vrigazov *@*.***> wrote: Two of\n> the builds (MacOS and Ubuntu CPU) are failing because of the API\n> compatibility (adding a new parameter to linspace) - still not sure what\n> should I do there, as it requires changes in the golden API. @alextp\n> <https://github.com/alextp> https://github.com/alextp @yongtang\n> <https://github.com/yongtang> https://github.com/yongtang any ideas? I am\n> looking into the third one - this test got cached for me - it does seem\n> that it is caused by the difference when num == 0 - in this PR linspace\n> behaves like numpy and returns an empty tensor, while the scalar\n> implementation throws ValueError. I will change this to adhere to the\n> previous behaviour for backward compatibility and update here when I am\n> done. \u2014 You are receiving this because you were mentioned. Reply to this\n> email directly, view it on GitHub <#33068\n> <https://github.com/tensorflow/tensorflow/pull/33068>?email_source=notifications&email_token=AAABHROMBR7XII7U5UHO5XLQX2GYRA5CNFSM4I5X3TNKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGKGXSI#issuecomment-563375049>,\n> or unsubscribe\n> https://github.com/notifications/unsubscribe-auth/AAABHRPV6S7HC5XMY2JRYVDQX2GYRANCNFSM4I5X3TNA\n> .\n> -- - Alex\n>\n> Thanks, did not realize this is something I can do. Sorry if it's a stupid\n> question, but which target should I run to achieve this? I tried bazel\n> build //tensorflow/tools/api/golden:api_golden_v1 and then running this\n> target says that there are no executable targets.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/33068?email_source=notifications&email_token=AAABHRKU4TM2NTDIUV3CDXDQX2L2RA5CNFSM4I5X3TNKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGKLRSI#issuecomment-563394761>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHROS64KYSDJBD3L3Y2DQX2L2RANCNFSM4I5X3TNA>\n> .\n>\n\n\n-- \n - Alex\n", "Awesome! Thanks, I will fix the other test and then push", "I have updated the golden APIs and have fixed the failing test.", "I see that the check `import/copybara` is still \"expected\",  is this testing code migration? How can we trigger it?", "@alextp Sorry to bother you again with questions about the golden API, but I found that depending on the directory from which I run the generator, the `pywrap` gets removed from golden API, which does not seem right: https://github.com/tensorflow/tensorflow/pull/33068/files#diff-766e65e30dc1ef20726b91c146e59359L551\r\n\r\nI am not sure what the purpose of `pywrap_tensorflow`, but that does not seem right, should I remove the deletion of the `pywrap_tensorflow` entry?", "Yes, remove that deletion, thanks.", "I think it is correct now.", "Looks like this PR was merged successfully. I want to say a big THANK YOU to @yongtang and especially @alextp  for their tremendous help, I learned a lot during the review :heart: "]}, {"number": 33067, "title": "tf.gather can't perform real values but op while using sess.run()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Custom Code \r\n- OS Platform and Distribution : Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):tensorflow-gpu 1.12.0\r\n- Python version:3.6\r\n- CUDA/cuDNN version: CUDA 9.0\r\n\r\n**Describe the current behavior**\r\nI 'm a beginner of TF.In my code ,I have a `myinput['point']`  matrix(shape:500x3  500 for the number of points,3 for 3-D) which stores the whole point set. I also have a `myinput['neigh']`  matirx(500x10  10 indicates each point have 10 neighbors ) which stores the **idx** of each point's neighs. When I want to get the neighs of each point , I use `tf.gather(myinput['point'],myinput['neigh'])`  .However, when I want to get the neighs of neighs,\r\nI use the code as follow:\r\n```\r\nneigh_neigh_idx=tf.gather(myinput['neigh'],myinput['neigh'])\r\nneigh_neigh=tf.gather(myinput['point'],neigh_neigh_idx)\r\nwith tf.Session() as sess:\r\n    n_n_idx=sess.run(neigh_neigh_idx)\r\n```\r\nafter evaluate this code, I only get `neigh_neigh_idx` and  `neigh_neigh` in console and I can't find any information but this about n_n_idx.I wander how to get ther real value of  `n_n_idx` instead of some ops.\r\n```\r\n[[node PointNetwork/layer_2/conv2/GatherV2_22 (defined at <string>:1)  = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](PointNetwork/layer_2/conv2/GatherV2_22/params, PointNetwork/layer_2/conv2/GatherV2_22/params, PointNetwork/layer_2/conv2/GatherV2_22/axis)]]\r\n```\r\nTHX~\r\n\r\n", "comments": ["@LuciusPennyworth, Please provide the complete code to reproduce the issue. Thanks!", "@LuciusPennyworth, Any update on code. Thanks!", "The code I post above is a brief description of my question. And it 's difficult for me to provide the complete code because they scattered in various files.However, I have figured out a way to advoid this problem and I will close this issue .Thanks for your concern."]}, {"number": 33066, "title": "Fix tf.assert_equal issue when one tenor is empty and another is non-empty", "body": "This fix tries to address the issue raised in #32082 where tf.assert_equal([], [1.0]) doesn't raise error.\r\nThe reason was that in assert_equal `[1.0]` was broadcasted as `[]` and equal was in place in that situation.\r\n\r\nThis PR updates the _binary_asesert so that it will check if x, y are both empty or both non-empty. If one is empty and another is non-empty, then assertion throws exception. This change is to not impact other ops that depends on the broadcast behavior.\r\n\r\nThis fix fixes #32082.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@alextp @gbaned \r\n\r\nI take a look at the internal testing error:\r\n```\r\nInvalidArgumentError: assertion failed: [Tensor(\"Placeholder:0\", dtype=int32)elements must be either positive integers or-1`.] [Condition x >= y did not hold element-wise:] [x (Placeholder:0) = ] [] [y (reshape/assert_greater_equal/y:0) = ] [-1]\r\n[[node reshape/assert_greater_equal/Assert/Assert (defined at /tensorflow/contrib/distributions/python/kernel_tests/bijectors/reshape_test.py:111) ]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node reshape/assert_greater_equal/Assert/Assert:\r\n```\r\n\r\nHowever, It is pointing to `tensorflow/contrib/distributions/python/kernel_tests/bijectors/reshape_test.py:111`\r\n\r\nwhich does not exist anymore in master branch, because  tf.contrib has been deleted.\r\n\r\nI am wondering if any additional information is available about how this internal test is being performed?\r\n\r\nIf you can point me to the location of the source file that I could see, I think I could quickly fix it."]}, {"number": 33065, "title": "fix C2993 cwise_op_clip_gpu.cu.cc", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.0.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): msvc 16.3.0\r\n- CUDA/cuDNN version: 10.1.243 / 7.6.4.38\r\n- GPU model and memory: NVIDIA GeForce RTX 2080 TI 11GB / driver 426.00\r\n\r\n**Any other info / logs**\r\n```\r\nC:\\users\\user\\_bazel_ching\\uoh5m3ct\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src\\Core\\../plugins/ArrayCwiseBinaryOps.h(207): error C2993: 'Derived': \u975e\u985e\u578b\u6a23\u677f\u53c3\u6578 '__formal' \u7684\u4e0d\u5408\u6cd5\u985e\u578b\r\nC:\\users\\user\\_bazel_ching\\uoh5m3ct\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src\\Core\\../plugins/ArrayCwiseBinaryOps.h(207): error C2993: 'Derived': \u975e\u985e\u578b\u6a23\u677f\u53c3\u6578 '__formal' \u7684\u4e0d\u5408\u6cd5\u985e\u578b\r\n\r\n```", "comments": []}, {"number": 33064, "title": "[Intel Mkl] Upgrade curl to fix CVE-2019-5481 and CVE-2019-5482", "body": "# [CVE-2019-5482](https://nvd.nist.gov/vuln/detail/CVE-2019-5482)\r\n\r\n*NVD:* 2019/09/16 - CVSS v2.0 Base Score: 7.5 - CVSS v3.0 Base Score: 0.0\r\nHeap buffer overflow in the TFTP protocol handler in cURL 7.19.4 to 7.65.3.\r\n## References to Advisories, Solutions, and Tools\r\n\r\nSource | Link | Type\r\n---- | ---- | ----\r\nSUSE | [lists.opensuse.org](http://lists.opensuse.org/opensuse-security-announce/2019-09/msg00055.html) | \u00a0\r\nFEDORA | [lists.fedoraproject.org](https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/UA7KDM2WPM5CJDDGOEGFV6SSGD2J7RNT/) | \u00a0\r\nSUSE | [lists.opensuse.org](http://lists.opensuse.org/opensuse-security-announce/2019-09/msg00048.html) | \u00a0\r\nFEDORA | [lists.fedoraproject.org](https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/RGDVKSLY5JUNJRLYRUA6CXGQ2LM63XC3/) | \u00a0\r\nCONFIRM | [curl.haxx.se](https://curl.haxx.se/docs/CVE-2019-5482.html) | Vendor Advisory\r\n\r\n# CVE-2019-5481\r\n*NVD:* 2019/09/16 - CVSS v2.0 Base Score: 7.5 - CVSS v3.0 Base Score: 0.0\r\nDouble-free vulnerability in the FTP-kerberos code in cURL 7.52.0 to 7.65.3.\r\n\r\n## References to Advisories, Solutions, and Tools\r\n\r\nSource | Link | Type\r\n---- | ---- | ----\r\nSUSE | [lists.opensuse.org](http://lists.opensuse.org/opensuse-security-announce/2019-09/msg00055.html) | \u00a0\r\nFEDORA | [lists.fedoraproject.org](https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/UA7KDM2WPM5CJDDGOEGFV6SSGD2J7RNT/) | \u00a0\r\nSUSE | [lists.opensuse.org](http://lists.opensuse.org/opensuse-security-announce/2019-09/msg00048.html) | \u00a0\r\nFEDORA | [lists.fedoraproject.org](https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/RGDVKSLY5JUNJRLYRUA6CXGQ2LM63XC3/) | \u00a0\r\nCONFIRM | [curl.haxx.se](https://curl.haxx.se/docs/CVE-2019-5481.html) | Vendor Advisory\r\n\r\n", "comments": ["Might want to consider cherry-picking..."]}, {"number": 33063, "title": "Add XLA-only merge that can merge all types.", "body": "This prevents insertion of H2D and D2H copies when XLA-GPU clusters\r\nhave int32 outputs. This merge is only used the merge the outputs\r\nfrom the XlaRun and the the PartitionedCall node.", "comments": ["LGTM from me, but I want @rmlarsen to also take a look.", "Update: I ran the internal benchmarks with this and found some minor regressions, checking if they are noise or not.", "> Update: I ran the internal benchmarks with this and found some minor regressions, checking if they are noise or not.\r\n\r\nChecking in too see if any real issues were discovered.", "> Checking in too see if any real issues were discovered.\r\n\r\nNo issues, the only \"regression\" was a noisy benchmark.  I've kicked off the submission internally."]}, {"number": 33062, "title": "Jupyter notebook from `tensorflow/tensorflow:nightly-py3-jupyter` won't authenticate its own token", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- TensorFlow installed from (source or binary): ` docker pull tensorflow/tensorflow:nightly-py3-jupyter`\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:n/a\r\n- GPU model and memory: n/a\r\n\r\n* Tensorflow 1.14 installed on system via `conda install tensorflow`; using docker container because needed a higher version without disturbing the system installation*\r\n\r\n**Describe the current behavior**\r\n![Screenshot from 2019-10-05 08-08-27](https://user-images.githubusercontent.com/31768189/66248857-65ea6600-e749-11e9-8d75-22dca6cf6129.png)\r\n\r\nThe jupyter notebook that comes bundled with `tensorflow/tensorflow:nightly-py3-jupyter` docker image won't authenticate its own token\r\n\r\n**Describe the expected behavior**\r\n\r\nJupyter notebook should authenticate its own token.\r\n\r\n**Code to reproduce the issue**\r\n\r\n``` bash\r\n$ docker pull tensorflow/tensorflow:nightly-py3-jupyter\r\n$ docker run -u $(id -u):$(id -u) -it -p 10001:10001 tensorflow/tensorflow:nightly-py3-jupyter\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\n________                               _______________                \r\n___  __/__________________________________  ____/__  /________      __\r\n__  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | /| / /\r\n_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / \r\n/_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  \\____/____/|__/\r\n\r\n\r\nYou are running this container as user with ID 1000 and group 1000,\r\nwhich should map to the ID and group for your user on the Docker host. Great!\r\n\r\n[I 02:35:18.648 NotebookApp] Writing notebook server cookie secret to /.local/share/jupyter/runtime/notebook_cookie_secret\r\n/usr/local/lib/python3.6/dist-packages/IPython/paths.py:68: UserWarning: IPython parent '/' is not a writable location, using a temp directory.\r\n  \" using a temp directory.\".format(parent))\r\n[I 02:35:18.817 NotebookApp] Serving notebooks from local directory: /tf\r\n[I 02:35:18.818 NotebookApp] The Jupyter Notebook is running at:\r\n[I 02:35:18.818 NotebookApp] http://82fe909436a2:8888/?token=508ff9f2b673ebcc134e67f6c21f539659cc74b659fceeb1\r\n[I 02:35:18.818 NotebookApp]  or http://127.0.0.1:8888/?token=508ff9f2b673ebcc134e67f6c21f539659cc74b659fceeb1\r\n[I 02:35:18.818 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\n[C 02:35:18.822 NotebookApp] \r\n    \r\n    To access the notebook, open this file in a browser:\r\n        file:///.local/share/jupyter/runtime/nbserver-1-open.html\r\n    Or copy and paste one of these URLs:\r\n        http://82fe909436a2:8888/?token=508ff9f2b673ebcc134e67f6c21f539659cc74b659fceeb1\r\n     or http://127.0.0.1:8888/?token=508ff9f2b673ebcc134e67f6c21f539659cc74b659fceeb1\r\n```\r\n", "comments": ["@rakshitraj, Did you try this \r\n`docker run -it -p 8888:8888 tensorflow/tensorflow:nightly-py3-jupyter`. Let us know if that helps. Thanks!", "Initially I got this\r\n\r\n```bash\r\nraxit@mBox:~$ docker run -it -p 8888:8888 tensorflow/tensorflow:nightly-py3-jupyter\r\n\r\ndocker: Error response from daemon: driver failed programming external connectivity on endpoint confident_keller (4f284434a00f2fbdf9d7fec344acf2b33f333c2f41e5e097f254b81c4560f32b): Error starting userland proxy: listen tcp 0.0.0.0:8888: bind: address already in use.\r\n\r\n```\r\nThen I went ahead and tried this\r\n\r\n```bash\r\nsudo kill `sudo lsof -t -i:8888`  # to free up the port\r\n```\r\nand viola\r\n![Screenshot from 2019-10-07 16-20-10](https://user-images.githubusercontent.com/31768189/66306069-5ec88100-e91e-11e9-9f48-fd909174abbb.png)\r\nIt works!\r\n\r\nBTW could you tell me why it didn't work the first time. I was using it on a free port, I made sure of that using `netstat -an`\r\n\r\nThanks a lot!\r\n", "I have exactly the same issue.", "@rakshitraj,\r\nGlad it worked.\r\nInitial error might be because your jupyter notebook is already in use or you tried to connect jupyter notebook on different terminal. You can restart the system or kill the existing port to free the port. Thanks!", "@mbenezra, Can you please post a new issue by providing the information asked by the template?\r\nThe reason for this is we can focus on your specific configuration and problem since the root cause can be unrelated even though the error messages are similar. Thanks!", "Closing this issue since its resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33062\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33062\">No</a>\n", "> @mbenezra, Can you please post a new issue by providing the information asked by the template?\r\n> The reason for this is we can focus on your specific configuration and problem since the root cause can be unrelated even though the error messages are similar. Thanks!\r\n\r\ndocker run -it -p 8888:8888 tensorflow/tensorflow:nightly-py3-jupyter works for me now."]}, {"number": 33061, "title": "image_gradients: 'Tensor' object has no attribute 'get_shape'", "body": "Hi all,\r\n\r\nWhile I'm trying to feed my tensor to `image_gradients`, this error arises:\r\n\r\n`AttributeError: 'Tensor' object has no attribute 'get_shape'`\r\n\r\nThe type of this tensor is: `torch.cuda.floattensor`\r\n\r\nAnd its size: `torch.Size([2, 1, 128, 128])`\r\n\r\nAny help, please.", "comments": ["Is this pytorch or TensorFlow?", "@SamuelMarks It's PyTorch ", "This is the TensorFlow GitHub. Anyway, checking it out:\r\nhttps://pytorch.org/docs/stable/tensors.html\r\n\r\nI do not see a `get_shape` available.\r\n\r\nMaybe you want `.size()`?\r\n https://pytorch.org/docs/stable/tensors.html?highlight=size#torch.Tensor.size", "@SamuelMarks Yes, that's what I notice. \r\n\r\nIt seems that `image_gradients` function implementation call `get_shape`, in which it's not available for `tensor`. And yes you're right tensor has `.size` instead (but the problem is within `image_gradients` function), I cannot call `.size` within `image_gradients` function!", "@SaraAlthubaiti, Looks like the issue is not related to Tensorflow. Can please post this issue in relevant Repo. Thanks!", "@gadagashwini I think so, thanks for your help!"]}, {"number": 33060, "title": "Possible corruption in Load or freeze in TF2.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary 2.0.0\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 7.6.4\r\n- GPU model and memory: Volta/Turing\r\n\r\n**Describe the current behavior**\r\nWhen I run load and freeze in two different python functions, I get a crash that says: \r\n\r\n```AssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.```\r\n\r\nBut when I run both of load and freeze in the same python function, then it works as expected.\r\n\r\n**Describe the expected behavior**\r\nCalling load and freeze in different python functions should work unless there is some hidden assumption in TF 2.0. My guess is that there is a leak or some dependency between the two APIs.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport argparse\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.saved_model import signature_constants\r\nfrom tensorflow.python.saved_model import tag_constants\r\nfrom tensorflow.python.framework import convert_to_constants\r\n\r\ndef get_dataset(batch_size, input_size):\r\n  features = np.random.normal(\r\n      loc=112, scale=70,\r\n      size=(batch_size, input_size, input_size, 3)).astype(np.float32)\r\n  features = np.clip(features, 0.0, 255.0)\r\n  features = tf.convert_to_tensor(value=tf.compat.v1.get_variable(\r\n      \"features\", dtype=tf.float32, initializer=tf.constant(features)))\r\n  dataset = tf.data.Dataset.from_tensor_slices([features])\r\n  dataset = dataset.repeat()\r\n  return dataset\r\n\r\n\r\ndef run_func(saved_model_dir):\r\n\r\n  def load_model():\r\n    saved_model_loaded = tf.saved_model.load(\r\n        saved_model_dir, tags=[tag_constants.SERVING])\r\n    graph_func = saved_model_loaded.signatures[\r\n        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n    return graph_func\r\n  graph_func = load_model()\r\n\r\n# Replace load_model function and its call with the following to make it work\r\n#  saved_model_loaded = tf.saved_model.load(\r\n#      saved_model_dir, tags=[tag_constants.SERVING])\r\n#  graph_func = saved_model_loaded.signatures[\r\n#      signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n\r\n  frozen_func = convert_to_constants.convert_variables_to_constants_v2(graph_func)\r\n  def wrap_func(*args, **kwargs):\r\n    return frozen_func(*args, **kwargs)[0]\r\n  inference_graph_func = wrap_func\r\n  dataset = get_dataset(batch_size=8, input_size=224)\r\n  for i, (batch_feats) in enumerate(dataset):\r\n    batch_preds = inference_graph_func(batch_feats).numpy()\r\n    print(batch_preds)\r\n\r\n\r\nif __name__ == '__main__':\r\n  parser = argparse.ArgumentParser(description='Evaluate model')\r\n  parser.add_argument('--saved_model_dir', type=str, default=None,\r\n                      help='Directory containing a particular saved model.')\r\n  args = parser.parse_args()\r\n\r\n  run_func(saved_model_dir=args.saved_model_dir)\r\n```\r\n\r\n**Other info / logs**\r\nCommand line to run: `python tfv2_load_issue.py --saved_model_dir path_to_saved_model`\r\n\r\n```\r\n2019-10-04 20:35:48.913525: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:734] Optimization results for grappler item: graph_to_optimize\r\n2019-10-04 20:35:48.913553: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:736]   function_optimizer: Graph size after: 1314 nodes (1044), 2670 edges (2400), time = 23.171ms.\r\n2019-10-04 20:35:48.913557: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:736]   function_optimizer: function_optimizer did nothing. time = 0.371ms.\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 51, in <module>\r\n    run_func(saved_model_dir=args.saved_model_dir)\r\n  File \"run.py\", line 35, in run_func\r\n    frozen_func = convert_to_constants.convert_variables_to_constants_v2(graph_func)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/convert_to_constants.py\", line 411, in convert_variables_to_constants_v2\r\n    tensor_data = _get_tensor_data(func)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/convert_to_constants.py\", line 182, in _get_tensor_data\r\n    for var in func.graph.variables:\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\", line 435, in variables\r\n    \"Called a function referencing variables which have been deleted. \"\r\nAssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.\r\n```\r\n", "comments": ["@pooyadavoodi,\r\nI have executed the code you provided, with the Exported Model of Iris Data and I have executed the command line argument provided by you but I didn't get any error. It printed the values. Can you please help me reproduce the issue. Thanks!", "@pooyadavoodi,\r\nCan you please let us know if your issue is resolved. Else, can you please help us reproduce the issue. Thanks! ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33060\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33060\">No</a>\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I am facing the same issue . When returning a ```tf.saved_model.load``` object inside a function and then try to use it, it is not working. \r\n\r\nI am having a file ```sample.py``` \r\n```\r\n#### sample.py\r\n\r\nimport tensorflow as tf\r\ndef load_model(model_dir):\r\n\r\n    # Load Model\r\n    loaded = tf.saved_model.load(model_dir)\r\n    model = loaded.signatures['serving_default']\r\n    print(\"Model Loaded\")\r\n    return model\r\n\r\n```\r\n\r\nWhen I am executing ```main.py```\r\n\r\n```\r\nfrom sample import load_model\r\n\r\nmodel_dir = 'som_path of a saved model'\r\nmodel1 = load_model(model_dir)\r\n```\r\n\r\nIf I print model.variables I am getting following error\r\n\r\n```\r\nAssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.\r\n\r\n```\r\n\r\nBut. If load the model with same code inside the function, but not using the function it works fine\r\n\r\n```\r\n#### main.py\r\nloaded = tf.saved_model.load(model_dir)\r\nmodel = loaded.signatures['serving_default']\r\n```\r\nIf I print model.variables, its working as expected. ", "@s4sarath encountered the same issue, it seems that that the function loading the model must return `tf.saved_model.load(path)` not just an inference function obtained by referencing `signature` property `tf.saved_model.load(path).signatures['serving_default']`, otherwise the inference fails", "Glad someone mentioned it. It's very critical right.\n\nOn Wed, Jul 1, 2020, 9:30 PM etsygankov <notifications@github.com> wrote:\n\n> @s4sarath <https://github.com/s4sarath> encountered the same issue, it\n> seems that that the function loading the model must return\n> tf.saved_model.load(path) not just an inference function obtained by\n> referencing signature property\n> tf.saved_model.load(path).signatures['serving_default'].\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33060#issuecomment-652505966>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACRE6KGYUUJIODAOYL2P2U3RZNMTBANCNFSM4I5VKZCA>\n> .\n>\n"]}, {"number": 33059, "title": "Size and CombinedNMS Op support request for tflite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (or github SHA if from source): 2.0.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n2019-10-04 16:17:28.794407: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-10-04 16:17:28.798220: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2019-10-04 16:17:28.798242: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: WD-AI-lab\r\n2019-10-04 16:17:28.798246: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: WD-AI-lab\r\n2019-10-04 16:17:28.798282: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0\r\n2019-10-04 16:17:28.798299: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 396.51.0\r\n2019-10-04 16:17:28.798303: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 396.51.0 does not match DSO version 430.26.0 -- cannot find working devices in this configuration\r\n2019-10-04 16:17:28.798415: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-04 16:17:28.821484: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4008000000 Hz\r\n2019-10-04 16:17:28.822257: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5563ee4031d0 executing computations on platform Host. Devices:\r\n2019-10-04 16:17:28.822268: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-10-04 16:17:32.842974: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2019-10-04 16:17:32.843029: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-10-04 16:17:32.858437: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-10-04 16:17:32.858454: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n2019-10-04 16:17:32.858458: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-10-04 16:17:35.240170: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2019-10-04 16:17:35.240264: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-10-04 16:17:35.995629: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-10-04 16:17:35.995652: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 1001 nodes (-358), 2553 edges (-358), time = 346.923ms.\r\n2019-10-04 16:17:35.995656: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 1001 nodes (0), 2553 edges (0), time = 114.777ms.\r\nTraceback (most recent call last):\r\n  File \"/home/wd_ai/git-pkgs/yolov3-tf2/convert_tflite.py\", line 29, in <module>\r\n    app.run(main)\r\n  File \"/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/wd_ai/git-pkgs/yolov3-tf2/convert_tflite.py\", line 24, in main\r\n    tflite_model = converter.convert()\r\n  File \"/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 446, in convert\r\n    **converter_kwargs)\r\n  File \"/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 449, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 200, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2019-10-04 16:17:37.744297: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2019-10-04 16:17:37.744348: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2019-10-04 16:17:37.744519: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2019-10-04 16:17:37.744542: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2019-10-04 16:17:37.744676: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2019-10-04 16:17:37.744683: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2019-10-04 16:17:37.744759: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: CombinedNonMaxSuppression\r\n2019-10-04 16:17:37.757406: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 743 operators, 1376 arrays (0 quantized)\r\n2019-10-04 16:17:37.767372: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 743 operators, 1376 arrays (0 quantized)\r\n2019-10-04 16:17:38.148616: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 284 operators, 533 arrays (0 quantized)\r\n2019-10-04 16:17:38.152237: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 284 operators, 533 arrays (0 quantized)\r\n2019-10-04 16:17:38.155833: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 284 operators, 533 arrays (0 quantized)\r\n2019-10-04 16:17:38.158553: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 284 operators, 533 arrays (0 quantized)\r\n2019-10-04 16:17:38.164271: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 66560128 bytes, theoretical optimal value: 44408960 bytes.\r\n2019-10-04 16:17:38.165392: E tensorflow/lite/toco/toco_tooling.cc:466] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FILL, LEAKY_RELU, LOGISTIC, MUL, PACK, PAD, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: CombinedNonMaxSuppression, Size.\r\nTraceback (most recent call last):\r\n  File \"/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/bin/toco_from_protos\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FILL, LEAKY_RELU, LOGISTIC, MUL, PACK, PAD, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: CombinedNonMaxSuppression, Size.\r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@wuhy08 , `Size` as an op is supported by our [Select TF ops](https://www.tensorflow.org/lite/guide/ops_select) feature - assuming you are okay with a few MBs of increase in binary size. However, CombinedNonMaxSuppression is not.\r\n\r\nAs part of some upcoming TFLite converter changes, we are planning to support the usage of NonMaxSuppressionV4 & NonMaxSuppressionV5 in TFLite graphs - is there any way you could modify your network to get these instead? Are you using TF's object detection API?", "Hi @srjoglekar246 \r\n\r\nI am using `tf.image.combined_non_max_suppression`, I think it uses `tensorflow::ops::CombinedNonMaxSuppression` on the backend, correct? \r\n\r\nIf I switch to `tf.image.non_max_suppression`, will it call `tensorflow::ops::NonMaxSuppressionV3`?\r\n\r\nWhich front end API calls V4 or V5?\r\n\r\nThank you!", "One of the following should yield the required ops in the graph:\r\n\r\n1. `tf.image.non_max_suppression_padded`\r\n2. `tf.image.non_max_suppression_with_scores`\r\n\r\nThe first is just a slight modification to `tf.image.non_max_suppression`, while the latter one generalizes it to 'soft' NMS. Which ops are generated by each function based on the params can be seen in [this file](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/image_ops_impl.py).\r\n\r\nOnce you have these ops in, using [Select TF ops](https://www.tensorflow.org/lite/guide/ops_select) should correctly convert your model. Later this year, we will be making some converter updates that make NMS ops even easier to support natively in TFLite :-)", "Thank you, @srjoglekar246 !", "Closing this for now. Feel free to re-open if you are still stuck. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33059\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33059\">No</a>\n"]}, {"number": 33058, "title": "[Intel MKL] Fix missing libiomp5 issue due to missing deps.", "body": "", "comments": []}, {"number": 33057, "title": "No stat cache for s3 file system", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.14\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nIn gcs file system there's a cache for object `stat` where you don't have to repeatedly send `stat` call to remote server to get stat of the same object. This is a nice performance gain but not available in s3 file system.\r\n\r\nAs s3 share the same nature as gcs, the same cache can be applied to s3 file system for the performance gain.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nUser of s3 file system, whose use pattern send rapid api calls.\r\n\r\n**Any Other info.**\r\n", "comments": ["Thanks for looking into this.\r\n\r\nI'm not familiar with the lifecycle of a tensorflow issue ticket. Will there be an ETA or anything I can expect next?\r\n\r\nI'm happy to help too.\r\n\r\nIt looks like there's already a lru_cache implemented and used by gcs, it'll be a straightforward change to apply to s3 too. I've internally patched it this way for our systems.", "Can you submit a PR?\r\n\r\nI won't be able to get into this until after https://github.com/tensorflow/community/pull/101 work is fully done, when I'll be converting the s3 filesystem to that API.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 33056, "title": "[r1.15-CherryPIck]:Update version to 1.15.0-rc3", "body": "", "comments": []}, {"number": 33055, "title": "Using more precise log ", "body": "Using log(1+x) to compute when x is close to zero", "comments": ["Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 33054, "title": "Build failed, 2.0 for CPU without avx support. ", "body": "tensorflow/compiler/xla/client/lib/prng.cc(438): error C2065: 'M_PI': undeclared identifier                             tensorflow/compiler/xla/client/lib/prng.cc(438): error C2672: 'ScalarLike': no matching overloaded function found       tensorflow/compiler/xla/client/lib/prng.cc(438): error C2780: 'xla::XlaOp xla::ScalarLike(xla::XlaOp,T)': expects 2 arguments - 1 provided\r\n.\\tensorflow/compiler/xla/client/lib/constants.h(91): note: see declaration of 'xla::ScalarLike'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build                                                 INFO: Elapsed time: 4480.154s, Critical Path: 158.59s                                                                   INFO: 800 processes: 800 local.                                                                                         FAILED: Build did NOT complete successfully                 \r\n", "comments": ["I've solved the problem by disable XLA/Jit on running `./configure` in first step.\r\n"]}, {"number": 33053, "title": "Expose tf.keras.CallbackList API", "body": "Proposing this in reference to https://github.com/tensorflow/tensorflow/pull/23880#issuecomment-514821825. Will this change add \"from tensorflow.python.keras.callbacks import CallbackList\" to the machine-generated code in \"tensorflow/_api/v1/keras/callbacks/init.py\" upon building so that it can be imported as in the regular Keras? If not, please let me know what to do. Thanks!", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33053) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33053) for more info**.\n\n<!-- ok -->", "@rchao Can you please take a look on this PR? Thanks!", "Hello @QRabbani, thanks for the PR. I wonder what's the rationale for exposing this? CallbackList is supposed to be an internal-used only class.", "Hi @rchao, the rationale was simply to be consistent with Keras for backwards compatibility, as Keras does expose it. As far as why I was using it, I was building my own custom fitting function, which was only possible by hooking into some of the normally \u201cinternal-only\u201d API including CallbackList and low-level batch and epoch hooks for the callbacks.", "Hello @QRabbani, thanks for the information. Actually as I checked the code at head it appears to be exported already. This should be available in 2.3. Thanks!", "@rchao thank you Rick for clarification , @QRabbani will be closing this PR , thanks for your contribution. "]}, {"number": 33052, "title": "Keras RNN training speed significantly slower with eager execution/control flow v2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.0/7.6\r\n- GPU model and memory: GTX 980 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nEnabling eager execution or control flow v2 causes RNN training speed to decrease significantly.\r\n\r\n**Describe the expected behavior**\r\n\r\nEnabling eager mode or control flow v2 should not affect the training time (or improve it, ideally).\r\n\r\n**Code to reproduce the issue**\r\n\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport timeit\r\n\r\nuse_eager = False\r\nuse_v2 = False\r\n\r\nif not use_eager:\r\n    tf.compat.v1.disable_eager_execution()\r\nif not use_v2:\r\n    tf.compat.v1.disable_control_flow_v2()\r\n\r\n\r\nn_steps = 1000\r\nn_input = 100\r\nn_hidden = 1000\r\nbatch_size = 64\r\n\r\ninputs = tf.keras.Input((n_steps, n_input))\r\noutputs = tf.keras.layers.SimpleRNN(units=n_hidden, return_sequences=True)(inputs)\r\noutputs = tf.keras.layers.Dense(units=n_input)(outputs)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\nmodel.compile(optimizer=tf.optimizers.SGD(0.1), loss=\"mse\")\r\n\r\nx = np.ones((batch_size, n_steps, n_input))\r\ny = np.ones((batch_size, n_steps, n_input))\r\n\r\n# warmup\r\nmodel.fit(x, y, epochs=1)\r\n\r\nstart = timeit.default_timer()\r\nmodel.fit(x, y, epochs=10)\r\nprint(\"Execution time:\", timeit.default_timer() - start)\r\n\r\n```\r\n\r\n**Other info / logs**\r\nOn my machine the results look like:\r\n- use_eager=False, use_v2=False: 5.90s\r\n- use_eager=False, use_v2=True: 8.08s\r\n- use_eager=True, use_v2=False: 9.81s\r\n- use_eager=True, use_v2=True: 10.10s\r\n\r\nSo, overall a >60% increase in training time comparing no eager and no v2 to the current defaults.\r\n", "comments": ["@drasmuss, I tried replicating the code on colab with Tf 2.0.0.rc2 but i didn't see increase in the training time. Please take a look at the colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/63e56c1793622ad377705725f555482e/untitled179.ipynb). Let me know the expected result. Thanks!", "A couple problems with the gist. One is that the `use_v2`/`use_eager` logic won't work properly when it's copy-pasted multiple times like that. It only runs the disable command when flag=False, so e.g. if you run once with `use_v2=False` and then again with `use_v2=True`, it's actually disabled for both of those runs.  I don't think it's possible to re-enable those things after disabling them (you'll get an error like \"`tf.enable_eager_execution must be called at program startup.`\"). So you need to just edit the values and then restart the notebook to collect the different timing values. \r\n\r\nIf you fix that, you should see a difference (although not as significant). But to see the significant differences you'll need to be running things on GPU, I'm not sure if it's possible to do that through the colab gists or not.\r\n\r\nThis is also on 2.0.0 release, not rc2. I haven't checked if that makes a difference or not though.", "Here's a script you can use if you'd like to see the overall effect within one script (this works because it runs first with the defaults, and then again after disabling eager mode and control flow v2).\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport timeit\r\n\r\n\r\nn_steps = 1000\r\nn_input = 100\r\nn_hidden = 1000\r\nbatch_size = 64\r\nwith tf.device(\"/gpu:0\"):\r\n    inputs = tf.keras.Input((n_steps, n_input))\r\n    outputs = tf.keras.layers.SimpleRNN(units=n_hidden, return_sequences=True)(inputs)\r\n    outputs = tf.keras.layers.Dense(units=n_input)(outputs)\r\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n    model.compile(optimizer=tf.optimizers.SGD(0.1), loss=\"mse\")\r\n\r\n    x = np.ones((batch_size, n_steps, n_input))\r\n    y = np.ones((batch_size, n_steps, n_input))\r\n\r\n    # warmup\r\n    model.fit(x, y, epochs=1)\r\n\r\n    start = timeit.default_timer()\r\n    model.fit(x, y, epochs=10)\r\n    print(\"Execution time (defaults):\", timeit.default_timer() - start)\r\n\r\n\r\ntf.compat.v1.disable_eager_execution()\r\ntf.compat.v1.disable_control_flow_v2()\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    inputs = tf.keras.Input((n_steps, n_input))\r\n    outputs = tf.keras.layers.SimpleRNN(units=n_hidden, return_sequences=True)(inputs)\r\n    outputs = tf.keras.layers.Dense(units=n_input)(outputs)\r\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n    model.compile(optimizer=tf.optimizers.SGD(0.1), loss=\"mse\")\r\n\r\n    x = np.ones((batch_size, n_steps, n_input))\r\n    y = np.ones((batch_size, n_steps, n_input))\r\n\r\n    # warmup\r\n    model.fit(x, y, epochs=1)\r\n\r\n    start = timeit.default_timer()\r\n    model.fit(x, y, epochs=10)\r\n    print(\"Execution time (no eager, no v2):\", timeit.default_timer() - start)\r\n```", "I could reproduce the issue. Please take a look at the [gist here](https://colab.sandbox.google.com/gist/gadagashwini/54c7b0895c0c116e2019512e64bd7f1d/untitled188.ipynb). Thanks!", "Seems to be a generic performance issue for eager runtime. Adding performance expert @robieta here.", "Also note that I have tried with compile(experimental_run_tf_function=False) and still produce the same result, which indicates that somehow the slowness is caused by runtime.", "With all the recent update for performance, the v1/v2 and eager/graph training time is almost identical to each other.\r\n\r\nPlease see the results in https://colab.research.google.com/gist/qlzh727/5bf1e16285bfcf2b3c2aea506fffd2e8/untitled188.ipynb\r\n\r\nI update the original test script to fit on 10 batches instead of 1 batch in each epoch, which should simulate more standard use case."]}, {"number": 33051, "title": "Build Tensorflow  from sources  for Android x86_64", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbunutu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nMaster, 1.14\r\n- TensorFlow version:\r\n- Python version:\r\n2.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n25\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\nNDK 17,18\r\n\r\n**Describe the problem**\r\nHi , I am trying to build tensorflow shared library from sources for our device CPU Appolo Lake with Android OS .\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbuild command \r\nbazel build  //tensorflow:tensorflow  --cpu=x86_64 --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\"\r\n\r\noutput\r\nFrom Compiling tensorflow/core/kernels/string_split_op.cc:\r\ntensorflow/core/kernels/string_split_op.cc:129:15: warning: 'RemoveLeadingWhitespace' is deprecated: Use absl::StripLeadingAsciiWhitespace instead. [-Wdeprecated-declarations]\r\n    str_util::RemoveLeadingWhitespace(&text);\r\n              ^\r\n./tensorflow/core/lib/strings/str_util.h:55:1: note: 'RemoveLeadingWhitespace' has been explicitly marked deprecated here\r\nABSL_DEPRECATED(\"Use absl::StripLeadingAsciiWhitespace instead.\")\r\n^\r\nexternal/com_google_absl/absl/base/macros.h:148:49: note: expanded from macro 'ABSL_DEPRECATED'\r\n#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))\r\n                                                ^\r\ntensorflow/core/kernels/string_split_op.cc:133:17: warning: 'RemoveLeadingWhitespace' is deprecated: Use absl::StripLeadingAsciiWhitespace instead. [-Wdeprecated-declarations]\r\n      str_util::RemoveLeadingWhitespace(&text);\r\n                ^\r\n./tensorflow/core/lib/strings/str_util.h:55:1: note: 'RemoveLeadingWhitespace' has been explicitly marked deprecated here\r\nABSL_DEPRECATED(\"Use absl::StripLeadingAsciiWhitespace instead.\")\r\n^\r\nexternal/com_google_absl/absl/base/macros.h:148:49: note: expanded from macro 'ABSL_DEPRECATED'\r\n#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))\r\n                                                ^\r\n2 warnings generated.\r\nERROR: /home/dnozik/.cache/bazel/_bazel_dnozik/0d8e3d5d91fa64e88329b83a7165efc6/external/lmdb/BUILD.bazel:8:1: C++ compilation of rule '@lmdb//:lmdb' failed (Exit 1)\r\nexternal/lmdb/mdb.c:4859:53: error: use of undeclared identifier 'PTHREAD_MUTEX_ROBUST'\r\n                if (!rc) rc = pthread_mutexattr_setrobust(&mattr, PTHREAD_MUTEX_ROBUST);\r\n                                                                  ^\r\n1 error generated.\r\nTarget //tensorflow:tensorflow failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1894.656s, Critical Path: 288.31s\r\nINFO: 628 processes: 628 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\nPlease help\r\n\r\n", "comments": ["@dimitryn, You would like to try Tensorflow lite on Android. Please take a look at the instructions mentioned in this [link](https://www.tensorflow.org/lite/guide/android).Thanks!", "@dimitryn, Is this still an issue? ", "no, thanks"]}, {"number": 33050, "title": "[r1.15-CherryPick]: Expose session_creation_timeout_secs in estimator RunConfig", "body": "PiperOrigin-RevId: 267290523", "comments": []}, {"number": 33049, "title": "Input pipeline guide refactoring", "body": "## URL(s) with the issue:\r\n\r\nBetter performance with tf.data: https://www.tensorflow.org/guide/data_performance\r\n\r\n## Description of issue (what needs changing):\r\n\r\nCurrently, this guide seems to be the main documentation source for `tf.data` usage.\r\nHowever, the differents steps shown do not seems to be optimal. For example:\r\n* TFRecordDataset usage do not match the actual API (see #33048)\r\n* Usage of `interleave` with TFRecordDataset is redoundant (see [SO post](https://stackoverflow.com/questions/58014123/how-to-improve-data-input-pipeline-performance)).\r\n\r\n### Submit a pull request?\r\n\r\nNo PR intented for the moment as I think this requires some discussion if I missed something or whatever.", "comments": ["Hi @AlexisBRENON,\r\n\r\nWe don't have anyone currently working on that doc, so if you want to make some updates, you're unlikely to run into any conflicts. It sounds like you're identified some areas for improvement. I would welcome a PR to tensorflow_docs with any fixes.\r\n\r\nIdeally the first step would be to convert the markdown file to a notebook:\r\n\r\n- I can setup testing for to avoid errors like #33048.\r\n- A notebook can demonstrate concrete improvements.\r\n\r\n\r\n+ @rohan100jain, @jsimsa FYI.\r\n", "Ok, no problem, I'm on it!\r\n\r\nI am sorry, I do not understand your first point. What do you mean by \"I can setup testing for to avoid errors like #33048\"?\r\n\r\nI will convert the current file to a notebook, just some questions:\r\n\r\n* Is there some guidelines on coding style? As it is a guide/tutorial notebook I\u00a0would write many comments, but maybe it infringes some coding style rules\r\n* Is there some public repo of TFRecords that I can use for the example (letting the notebook run seemlessly) ?\r\n* Is there some common cells to add at the top/bottom of the notebook (copyright, links to GH, Colab, etc.)", "Thanks Alexis,\r\n\r\n> What do you mean by \"I can setup testing for to avoid errors like #33048\"?\r\n\r\nI just mean that since notebooks are executable we run them as tests, and get warnings when they break. That's something I will take care of, If this file gets converted to a notebook. \r\n\r\n> Is there some guidelines on coding style? As it is a guide/tutorial notebook I would write many comments, but maybe it infringes some coding style rules\r\n\r\n> Is there some common cells to add at the top/bottom of the notebook (copyright, links to GH, Colab, etc.)\r\n\r\nThere's some advice in our notebook template:\r\n\r\nhttps://github.com/tensorflow/docs/blob/master/tools/templates/notebook.ipynb\r\nhttps://colab.sandbox.google.com/github/tensorflow/docs/blob/master/tools/templates/notebook.ipynb\r\n\r\n> I would write many comments\r\n\r\nTypically if it's more than a line or two of __code comments__ I encourage people to pull the text out into a markdown cell.\r\n\r\n>Is there some public repo of TFRecords that I can use for the example (letting the notebook run seemlessly) ?\r\n\r\nOne simple option here might be to download a few files from the \"French Street Name Signs\" dataset:\r\n\r\nhttps://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/train/train-00001-of-00512\r\nhttps://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/validation/validation-00000-of-00064\r\nhttps://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/test/test-00000-of-00064\r\n\r\nI give an example of loading this data in [this section of the tf.data guide](https://www.tensorflow.org/guide/data#consuming_tfrecord_data)", "Thanks for all the advices.\r\n\r\nFor the moment I mainly copied the base document to a notebook, just updating the code blocks to make them run.\r\n\r\nWhile presenting an example with TFRecord is interesting to showcase a real use case it is hard to use it to effectively show improvements with `prefetch`, `interleave`, `cache` and co. So I tried to build some artificial example which can be crafted to exhibits the performance gains. Take a look at it at the end of [the notebook](https://colab.research.google.com/github/tensorflow/docs/blob/89563f70f1a7306f2d06f8f4e905c2265459a759/site/en/guide/data_performance.ipynb#scrollTo=-XhmoSv9yTiZ), in the `Some ideas` section. It needs some more polish but allow to plot nice charts to show the improvement brang by each operation. What do you think of this kind of artificial example show cases?\r\n\r\nMoreover, I just stumbled upon the `data.ipynb` file which already presents \"real use case\" of the `tf.data` API, with TFRecords, CSV, Text, and co. Maybe the `data_performance` one can reffer the the former as a pre-requisite, and I\u00a0can concentrate on performance improvements showcases.", "It looks like you making good progress on that notebook, please send it as a PR and \"@MarkDaoust\" me on the thread if I'm not auto assigned as a reviewer. Let's continue the conversation there.\r\n\r\n> So I tried to build some artificial example which can be crafted to exhibits the performance gains.\r\n\r\nThe one caution with artificial examples is that they may not reflect use cases that matter. So we should be sure to make sure that if we use artificial data for this we set it up to reflect real world usage.\r\n\r\n> Maybe the data_performance one can reffer the the former as a pre-requisite, and I can concentrate on performance improvements showcases.\r\n\r\nYes.\r\n\r\n> plot nice charts\r\n\r\nCool. "]}, {"number": 33048, "title": "TFRecordDataset constructor does not support globbing pattern", "body": "## URL(s) with the issue:\r\n\r\nBetter performance with data: https://www.tensorflow.org/guide/data_performance#structure_of_an_input_pipeline\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe code example provided instantiate a `tf.data.TFRecordDataset` passing a globbing pattern: `\"/path/to/dataset/train-*.tfrecord\"` while it does not support it.\r\n\r\nThe example should be updated, relying on [`tf.data.Dataset.list_files`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#list_files).\r\n\r\n```diff\r\n- dataset = tf.data.TFRecordDataset(\"/path/to/dataset/train-*.tfrecord\")\r\n+ dataset = tf.data.TFRecordDataset(tf.data.Dataset.list_files(\"/path/to/dataset/train-*.tfrecord\"))\r\n```\r\n\r\n### Submit a pull request?\r\n\r\nI will submit a pull request soon.", "comments": ["This has been fixed with tensorflow/docs#1111."]}, {"number": 33047, "title": "Kernel restating while importing tensorflow on Old Macs, works OK on new one.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hello, \r\nI am installing Tensorflow according to the Jeff Heaton jupyter notebook available here : https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_01_1_overview.ipynb. \r\nOn an iMac mid-2010 with 2,93 GHz Core i7, 12GB RAM, the installation of the whole packages is OK, I can start jupyter. Than, when I want to import tensorflow as tf, the kernel restarts... I have tried it on the same host in a Parallels Virtual Machine configured with MacOS 10.13.6, same behavior. Than I have installed the same software stack in a clean VM (VirtualBox) running Ubuntu 18.04, same behavior.\r\nOn a MacBook Pro Retina 2012, everything works. On an older MacBook Pro 17\" mid-2010, Core i5, the problem is present too.\r\nAny clue ?\r\nThanks for you help,\r\nThierry\r\n", "@Tj85710, Please follow the instructions mentioned in the [Tensorflow](https://www.tensorflow.org/install/pip) website to install Tensorflow,select the mac os.Thanks! ", "I have found references to this behaviour : My processors are too old, they don't support the AVX instruction set. I am building a VM with Ubuntu, 8 Xeon cores, 40 GB RAM and try to build a custom version with the correct --march flag.\r\nCross fingers !\r\nThanks for your answer,\r\n\r\nThierry"]}, {"number": 33046, "title": "Converting Tensor to numpy array", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.14.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI already trained a model and I need to load the model in another script (model1) and use it for prediction (df: new data).\r\n\r\nI load the model to use it for prediction as follow:\r\nmodel1 = tf.keras.models.load_model(model_path, custom_objects=None, compile=False)\r\nresult = model1(df)\r\n\r\nThen how can I extract the numpy array from result Tensor?\r\n\r\nif I use \"result.numpy()\" I will get \"AttributeError: 'Normal' object has no attribute 'numpy'\"\r\n\r\nI need to return numbers to the frontend application.\r\n\r\n**Will this change the current api? How?**\r\nI don't know\r\n\r\n**Who will benefit with this feature?**\r\nAnyone using TensorFlow for production applications.\r\n\r\n**Any Other info.**", "comments": ["@mnozary You need to run a session in `TF1.x` in order to access the graph tensor. However, in TF2.0, everything runs eagerly and you can access a numpy array from a tensor as you mentioned above (`result.numpy()`)\r\n\r\nSo in TF1.x, you need to add the following lines to access `result` numpy array.\r\n```\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  result_output=sess.run(result)\r\n```\r\n\r\nPlease check the gist [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/2d0e36917906fec79ee4643ae29ee3ba/tf33046_1x.ipynb) with TF1.x. You can also check [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/2b0ef0d3842bb9e2cfaf064673f95bea/tf33046_keras_loadmodel.ipynb) with TF2.0. Thanks!\r\n\r\nI am closing this issue as it was resolved. Please feel free to open the issue if it persists again. Thanks!"]}, {"number": 33045, "title": "TPU support in tensorflow 2.0 release", "body": "From the following link https://www.tensorflow.org/guide/distributed_training I understand that TPU training is supported in tensorflow 2.0.\r\n\r\nI followed the snippet code provided in the same page:\r\n\r\ncluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\r\n    tpu=tpu_address)\r\ntf.config.experimental_connect_to_cluster(cluster_resolver)\r\ntf.tpu.experimental.initialize_tpu_system(cluster_resolver)\r\ntpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\r\n\r\nAnd I got the following error:\r\nInvalidArgumentError: Unable to find a context_id matching the specified one (-7989870214237460624). Perhaps the worker was restarted, or the context was GC'd?\r\nAdditional GRPC error information:\r\n{\"created\":\"@1570180842.964900283\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Unable to find a context_id matching the specified one (-7989870214237460624). Perhaps the worker was restarted, or the context was GC'd?\",\"grpc_status\":3}\r\n\r\nSame error was thrown when I didn't provide the tpu_address as it is stated in the above link.\r\n\r\n> The TPUClusterResolver instance helps locate the TPUs. In Colab, you don't need to specify any arguments to it.\r\n\r\nThe test was done in google colab and I selected the TPU accelerator. I installed tensorflow-gpu with !pip install tensorflow-gpu.\r\n\r\nIf TPU is not yet supported in tf 2.0 when it is planed to be added?\r\n", "comments": ["@georgealexandruvlad, Please provide the complete code to reproduce the issue reported issue here. Thanks!", "@gadagashwini  The code can be found at the following link: https://colab.research.google.com/drive/1CfY-bW-sRjZbvkQDjjl1CMiIzN6bJhQn\r\n\r\nBasically I just tried to initialize the TPU.\r\n```\r\n!pip install tensorflow-gpu\r\nimport tensorflow as tf\r\ntf.__version__\r\n# tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\n\r\ncluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.config.experimental_connect_to_cluster(cluster_resolver)\r\ntf.tpu.experimental.initialize_tpu_system(cluster_resolver)\r\ntpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\r\n```\r\n\r\nFull error message:\r\n\r\n```\r\nINFO:tensorflow:Initializing the TPU system: 10.20.247.218:8470\r\nINFO:tensorflow:Initializing the TPU system: 10.20.247.218:8470\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-9d9ec86e8e0b> in <module>()\r\n      5 cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n      6 tf.config.experimental_connect_to_cluster(cluster_resolver)\r\n----> 7 tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\r\n      8 tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\r\n\r\n8 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/context.py in add_function(self, fn)\r\n    987     \"\"\"\r\n    988     self.ensure_initialized()\r\n--> 989     pywrap_tensorflow.TFE_ContextAddFunction(self._handle, fn)\r\n    990 \r\n    991   def add_function_def(self, fdef):\r\n\r\nInvalidArgumentError: Unable to find a context_id matching the specified one (477754036002299890). Perhaps the worker was restarted, or the context was GC'd?\r\nAdditional GRPC error information:\r\n{\"created\":\"@1570430673.362553023\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Unable to find a context_id matching the specified one (477754036002299890). Perhaps the worker was restarted, or the context was GC'd?\",\"grpc_status\":3}\r\n```", "@georgealexandruvlad Thanks for testing TF2.0 TPUs. This seems to be a known issue as https://github.com/huan/tensorflow-handbook-tpu/issues/1 which the colab kernel on tpu worker is out dated, Team is working on the fix. Thanks!", "@jvishnuvardhan Thank you. Is there any way to follow the progress on the matter or do you happen to know when the issue might be solved. Thanks!", "@jvishnuvardhan I checked it today and it looks like the issue has been solved. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33045\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33045\">No</a>\n", "Uhh I don't it's fixed yet. Initializing works, but not fitting.\r\n\r\nFirst off, you installed '1.15.0-rc3', not tensorflow 2.0.\r\n\r\nSecond, you need to fit under tpu_strategy.scope() to train it under the TPU\r\n\r\nDo that, and you get an error.\r\n\r\nBottom line: TPU is not ready for Tensorflow 2.0 yet", "I am wondering if there's still issues with TPU support in tf 2.0. I have a model that runs just fine in GPU, but I am getting an \r\n\r\n```\r\n    AttributeError: Tensor.name is meaningless when eager execution is enabled.\r\n```\r\n\r\nError at `self.optimizer.apply_gradients(zip(gradients, trainable_vars))`.\r\n\r\nLink\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/34635#issuecomment-650667602\r\n\r\n"]}]