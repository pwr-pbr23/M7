[{"number": 26225, "title": "record_summaries_every_n_global_steps() should not execute code unless global_step % n == 0", "body": "* Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n  No\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  Ubuntu 18.04\r\n* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n  No\r\n* TensorFlow installed from (source or binary):\r\n  Source\r\n* TensorFlow version (use command below):\r\n  b'v1.12.0-5845-g764109a352' 1.12.0\r\n* Python version:\r\n  3.6.7\r\n* Bazel version (if compiling from source):\r\n  Invocation ID: 42251854-036f-415c-8a52-76aac8520ea0\r\n  Build label: 0.21.0\r\n  Build time: Wed Dec 19 12:58:44 2018 (1545224324)\r\n* GCC/Compiler version (if compiling from source):\r\n  gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n* CUDA/cuDNN version:\r\n  Cuda compilation tools, release 10.0, V10.0.130\r\n* GPU model and memory:\r\n  GTX 1060 Max Q, 6gb VRAM\r\n\r\n**Describe the current behavior**\r\n`tf.contrib.summary.record_summaries_every_n_global_steps(n, global_step)` may not be recording summaries unless `global_step % n == 0`, HOWEVER, it is running the code within its block anyways. In my opinion this is a huge waste of resources, especially when image logging is being done and transformations are required. Furthermore, there is no information of this behaviour on the documentation. Finally, it defeats the purpose of the function.\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/summary/record_summaries_every_n_global_steps\r\n**Describe the expected behavior**\r\nIt should not run the code within its \"with\" block when `global_step % n != 0`, or documentation should be added that describes the current behaviour. It defeats the purpose of it when I can combine an if statement with `tf.contrib.summary.always_record_summaries():` to produce better results.\r\n**Code to reproduce the issue**\r\n```\r\nglobal_step = tf.train.get_or_create_global_step()\r\nwriter = tf.contrib.summary.create_file_writer('./test')\r\nwriter.set_as_default()\r\n\r\nimages = []\r\nfor i in range(100):\r\n        images.append(np.random.uniform(0, 255, (32, 32, 3)).astype(np.float32))\r\nimages = np.array(images)\r\ndata = tf.data.Dataset.from_tensor_slices((images))\r\n\r\nfor i in range(10000):\r\n        batch = data.batch(len(images))\r\n        for image in batch:\r\n                with tf.contrib.summary.record_summaries_every_n_global_steps(100, global_step):\r\n                        print(global_step.numpy())\r\n                        tf.contrib.summary.image('image', image)\r\n        global_step.assign_add(1)\r\n```\r\n**Other info / logs**\r\n```\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n....\r\n9999\r\n```\r\nThe images were still logged every global_step % n == 0 iterations, yet as can be seen here, the code within the block still executed every global_step. I shouldn't need to add an `if global_step % n == 0:` outside of the `with tf.contrib.summary.record_summaries_every_n_global_steps(n, global_step):` function. It literally defeats the purpose of it, as I might as well just use `with tf.contrib.summary.always_record_summaries():` inside of an `if global_step % n == 0:`.", "comments": ["@jpatts This is more related to tensorboard Repo. Please post the issue [here](https://github.com/tensorflow/tensorboard/issues) and the Tboard Team will take care of that issue. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 26224, "title": "change to .shape instead of method .get_shape().", "body": "This is the recommended way of getting the shape.", "comments": []}, {"number": 26223, "title": "Updated quantize_model.cc", "body": "Fixed warning in the file", "comments": ["@rthadur , i have checked the failure is because of some other reason and nor because of the PR, kindly check and merge."]}, {"number": 26222, "title": "Estimator training hangs in multiple gpu if dataset doesn't have enough element to feed both gpus last batches", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nDistributed training (one node, Multiple GPUs)\r\n- TensorFlow installed from (source or binary):\r\nPIP\r\n- TensorFlow version (use command below):\r\nTF 1.12\r\n- Python version:\r\n3.6.8\r\n- CUDA/cuDNN version:\r\n9.0\r\n- GPU model and memory:\r\n2 GTX1080 8Go\r\n\r\n**Describe the current behavior**\r\nBasically, if the dataset doesn't have enough elements to feed both gpus last batches the training hangs.\r\n- If you doesn't have enough to feed the first gpu last batch and don't want to drop the last batch then the training hangs.\r\n- If you doesn't have enough to feed the first gpu last batch and want to drop the last batch then you're fine.\r\n- If you have enough to feed the first gpu last batch but not the second gpu last batch and don't want to drop the last batch then the training hangs\r\n- If you have enough to feed the first gpu last batch but not the second gpu last batch and want to drop the last batch then the training hangs\r\n\r\n\r\n**Describe the expected behavior**\r\n- If you doesn't have enough to feed the first gpu last batch and don't want to drop the last batch then run the first gpu partial batch and do nothing with the second gpu\r\n- If you doesn't have enough to feed the first gpu last batch and want to drop the last batch then drop the last batch for both gpus.\r\n- If you have enough to feed the first gpu last batch but not the second gpu last batch and don't want to drop the last batch then run the first gpu entire batch and run the second gpu partial batch.\r\n- If you have enough to feed the first gpu last batch but not the second gpu last batch and want to drop the last batch then run the first gpu entire batch and do nothing with the second gpu\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\n# Play with sample count (5, 6, 7) and drop_remainder (True, False) to reproduce the issue\r\nsample_count = 5\r\ndrop_remainder = False\r\n\r\n\r\ndef run():\r\n    # Config\r\n    run_config = tf.estimator.RunConfig(\r\n        session_config=tf.ConfigProto(allow_soft_placement=True),\r\n        train_distribute=tf.contrib.distribute.MirroredStrategy(num_gpus=2))\r\n\r\n    # Estimator\r\n    estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)\r\n    estimator.train(train_input_fn)\r\n\r\n\r\n# Times two dataset\r\ndef train_input_fn():\r\n    return tf.data.Dataset \\\r\n        .range(sample_count) \\\r\n        .repeat(1) \\\r\n        .map(lambda x: (x, x * 2)) \\\r\n        .batch(2, drop_remainder)\r\n\r\n\r\n# Times two model\r\ndef model_fn(features, labels, mode):\r\n    input_layer = tf.cast(tf.reshape(features, [-1, 1]), tf.float32)\r\n    expected_output = tf.cast(tf.reshape(labels, [-1, 1]), tf.float32)\r\n\r\n    logit = tf.layers.dense(input_layer, 1, None, False)\r\n    loss = tf.losses.mean_squared_error(expected_output, logit)\r\n\r\n    logging_hook = tf.train.LoggingTensorHook(tensors={\"feature_value\": features.name}, every_n_iter=1)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = tf.train.AdamOptimizer(0.001)\r\n        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          train_op=train_op,\r\n                                          training_hooks=[logging_hook])\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.logging.set_verbosity(tf.logging.DEBUG)\r\n    run()\r\n\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\nINFO:tensorflow:Not using Distribute Coordinator.\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp0ofz0qx1\r\nINFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_device_fn': None, '_experimental_distribute': None, '_task_type': 'worker', '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_distribute_coordinator_mode': None, '_service': None, '_save_summary_steps': 100, '_model_dir': '/tmp/tmp0ofz0qx1', '_master': '', '_keep_checkpoint_max': 5, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7feedf9f5dd8>, '_protocol': None, '_task_id': 0, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\r\n, '_is_chief': True, '_num_worker_replicas': 1, '_global_id_in_cluster': 0, '_evaluation_master': '', '_log_step_count_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7feedf9f5e48>, '_eval_distribute': None, '_num_ps_replicas': 0}\r\n2019-02-28 11:18:22.645478: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-02-28 11:18:22.818624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-02-28 11:18:22.820057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.847\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.90GiB freeMemory: 7.11GiB\r\n2019-02-28 11:18:22.954140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-02-28 11:18:22.955822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.847\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.81GiB\r\n2019-02-28 11:18:22.957142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1\r\n2019-02-28 11:18:23.349095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-28 11:18:23.349133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 \r\n2019-02-28 11:18:23.349139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y \r\n2019-02-28 11:18:23.349143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N \r\n2019-02-28 11:18:23.349775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 6853 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-02-28 11:18:23.350097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:1 with 7535 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\nINFO:tensorflow:Configured nccl all-reduce.\r\n2019-02-28 11:18:23.372783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1\r\n2019-02-28 11:18:23.373002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-28 11:18:23.373032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 \r\n2019-02-28 11:18:23.373038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y \r\n2019-02-28 11:18:23.373043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N \r\n2019-02-28 11:18:23.373272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6853 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-02-28 11:18:23.373346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7535 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n2019-02-28 11:18:23.707824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1\r\n2019-02-28 11:18:23.707940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-28 11:18:23.707963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 \r\n2019-02-28 11:18:23.707967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y \r\n2019-02-28 11:18:23.707988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N \r\n2019-02-28 11:18:23.708250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6853 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-02-28 11:18:23.708475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7535 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp0ofz0qx1/model.ckpt.\r\nINFO:tensorflow:loss = 47.902126, step = 0\r\nINFO:tensorflow:feature_value = [2 3]\r\n```\r\n", "comments": ["@Silb78dg Could you check whether the bug persists with newer TF versions? Thanks!", "I encounter the same problem using tf1.12 and MirroredStratety. This can be solved by use batch after shuffle, not after repeat. However, I think this bug should be solved, because batch should be after repeat for data efficiency.\r\n\r\nSuccessful code:\r\n\r\n```python\r\ndef input_fn(image_file, labels_file, batch_size, num_epoch):\r\n    image_ds = tf.data.FixedLengthRecordDataset(\r\n        image_file, 28*28, header_bytes=16)#.map(decode_image, num_parallel_calls=40)\r\n    labels_ds = tf.data.FixedLengthRecordDataset(\r\n        labels_file, 1, header_bytes=8)#.map(decode_label, num_parallel_calls=40)\r\n    dataset = tf.data.Dataset.zip((image_ds, labels_ds))\r\n    dataset = dataset.shuffle(buffer_size=10000)\r\n    dataset = dataset.batch(batch_size, drop_remainder=True)\r\n    dataset = dataset.repeat(num_epoch)\r\n    dataset = dataset.prefetch(buffer_size=1)\r\n    # for distributed strategy, it must return a tf.data.Dataset\r\n    return dataset\r\n````\r\n\r\n\r\nFailed code\r\n\r\n```\r\ndef input_fn(image_file, labels_file, batch_size, num_epoch):\r\n    image_ds = tf.data.FixedLengthRecordDataset(\r\n        image_file, 28*28, header_bytes=16)#.map(decode_image, num_parallel_calls=40)\r\n    labels_ds = tf.data.FixedLengthRecordDataset(\r\n        labels_file, 1, header_bytes=8)#.map(decode_label, num_parallel_calls=40)\r\n    dataset = tf.data.Dataset.zip((image_ds, labels_ds))\r\n    dataset = dataset.shuffle(buffer_size=10000)\r\n    dataset = dataset.repeat(num_epoch)\r\n    dataset = dataset.batch(batch_size, drop_remainder=True)\r\n    dataset = dataset.prefetch(buffer_size=1)\r\n    # for distributed strategy, it must return a tf.data.Dataset\r\n    return dataset\r\n```\r\n", "Hey @jsimsa, that's the issue we were discussing during tf dev summit.\r\n\r\nThanks for your help.\r\nDenis.\r\n", "@jsimsa - will you look into this issue?", "I believe that @rxsang is actively working on fixing this issue.", "@rxsang mentioned that this is fixed in TF 2 so closing the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26222\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26222\">No</a>\n"]}, {"number": 26221, "title": "Update feature_column_v2.py", "body": "Replaced the deprecated `to_int64` and `to_float` function with cast.", "comments": []}, {"number": 26220, "title": "Fix hwloc build for non x86 platforms", "body": "Commit a6bf9c8 yesterday triggered the building of hwloc in TensorFlow.\r\nWhile many platforms (windows, mac, android) were excluded from building\r\nhwloc, others like ppc64le and raspberry pi still tried to build hwloc.\r\n\r\nThis fails because hwloc references x86 specific files. Instead of excluding\r\nthose platforms from building with hwloc, lets add conditions to only include\r\nthe x86 files during a build on the x86 platform. This might enable the\r\nother platforms to still take advantage of hwloc function.", "comments": ["@gunan , please review. Thanks!", "Thanks @wdirons !"]}, {"number": 26219, "title": "patching PR 18797", "body": "Swtich to use axis instead of squeeze_dims in tf.squeeze", "comments": ["[patch changes here ](https://github.com/tensorflow/tensorflow/pull/18797)"]}, {"number": 26218, "title": "tf-gpu==1.13.1  : 35% less batch size before OOM vs tf-gpu==1.11.0", "body": "**System information**\r\n- Windows 7\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.11.0 , 1.13.1\r\n- Python version: 3.6.5\r\n- CUDA/cuDNN version: 9/7.1.4 , 10/7.4.1\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n**Describe the current behavior**\r\n\r\nI have standard AE network with pixel shuffler layer.\r\n\r\non `tf.1.11.0-cuda 9` maximum batch size for my GTX 1060 6GB is `132`\r\n\r\nbut after upgrade to `tf.1.13.1-cuda 10` tf cannot handle same batch size it produces OOM error\r\nand maximum now `90` for my card.\r\n\r\n**Describe the expected behavior**\r\n\r\nexpected not to downgrade performance when upgrading tensorflow\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nkeras = tf.keras\r\nKL = keras.layers\r\nK = keras.backend\r\n\r\nbgr_shape = (128, 128, 3)\r\n#batch_size = 132 #max -tf.1.11.0-cuda 9\r\nbatch_size = 86 #max -tf.1.13.1-cuda 10\r\n \r\nclass PixelShuffler(keras.layers.Layer):\r\n    def __init__(self, size=(2, 2), data_format=None, **kwargs):\r\n        super(PixelShuffler, self).__init__(**kwargs)\r\n        self.size = size\r\n\r\n    def call(self, inputs):\r\n\r\n        input_shape = K.int_shape(inputs)\r\n        if len(input_shape) != 4:\r\n            raise ValueError('Inputs should have rank ' +\r\n                             str(4) +\r\n                             '; Received input shape:', str(input_shape))\r\n\r\n\r\n        batch_size, h, w, c = input_shape\r\n        if batch_size is None:\r\n            batch_size = -1\r\n        rh, rw = self.size\r\n        oh, ow = h * rh, w * rw\r\n        oc = c // (rh * rw)\r\n\r\n        out = K.reshape(inputs, (batch_size, h, w, rh, rw, oc))\r\n        out = K.permute_dimensions(out, (0, 1, 3, 2, 4, 5))\r\n        out = K.reshape(out, (batch_size, oh, ow, oc))\r\n        return out\r\n\r\n    def compute_output_shape(self, input_shape):\r\n\r\n        if len(input_shape) != 4:\r\n            raise ValueError('Inputs should have rank ' +\r\n                             str(4) +\r\n                             '; Received input shape:', str(input_shape))\r\n\r\n\r\n        height = input_shape[1] * self.size[0] if input_shape[1] is not None else None\r\n        width = input_shape[2] * self.size[1] if input_shape[2] is not None else None\r\n        channels = input_shape[3] // self.size[0] // self.size[1]\r\n\r\n        if channels * self.size[0] * self.size[1] != input_shape[3]:\r\n            raise ValueError('channels of input and size are incompatible')\r\n\r\n        return (input_shape[0],\r\n                height,\r\n                width,\r\n                channels)\r\n\r\n    def get_config(self):\r\n        config = {'size': self.size}\r\n        base_config = super(PixelShuffler, self).get_config()\r\n\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n        \r\ndef upscale (dim):\r\n    def func(x):\r\n        return PixelShuffler()((KL.Conv2D(dim * 4, kernel_size=3, strides=1, padding='same')(x)))\r\n    return func \r\n            \r\ninp = KL.Input(bgr_shape)\r\nx = inp\r\nx = KL.Conv2D(128, 5, strides=2, padding='same')(x)\r\nx = KL.Conv2D(256, 5, strides=2, padding='same')(x)\r\nx = KL.Conv2D(512, 5, strides=2, padding='same')(x)\r\nx = KL.Conv2D(1024, 5, strides=2, padding='same')(x)\r\nx = KL.Dense(1024)(KL.Flatten()(x))\r\nx = KL.Dense(8 * 8 * 1024)(x)\r\nx = KL.Reshape((8, 8, 1024))(x)\r\nx = upscale(512)(x)\r\nx = upscale(256)(x)\r\nx = upscale(128)(x)\r\nx = upscale(64)(x)\r\nx = KL.Conv2D(3, 5, strides=1, padding='same')(x)\r\n\r\nmodel = keras.models.Model ([inp], [x])\r\nmodel.compile(optimizer=keras.optimizers.Adam(lr=5e-5, beta_1=0.5, beta_2=0.999), loss='mae')\r\n\r\ntraining_data = np.zeros ( (batch_size,128,128,3) )\r\nloss = model.train_on_batch( [training_data], [training_data] )\r\nprint (\"FINE\")\r\n```\r\n\r\n\r\n\r\n\r\n**Other info / logs**\r\n\r\n```\r\n1] 1 Chunks of size 12032 totalling 11.8KiB\r\n2019-02-28 19:45:23.516100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 4 Chunks of size 19200 totalling 75.0KiB\r\n2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 4 Chunks of size 38400 totalling 150.0KiB\r\n2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 4 Chunks of size 262144 totalling 1.00MiB\r\n2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 1 Chunks of size 368640 totalling 360.0KiB\r\n2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 4 Chunks of size 1179648 totalling 4.50MiB\r\n2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 5 Chunks of size 3276800 totalling 15.63MiB\r\n2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 4 Chunks of size 4718592 totalling 18.00MiB\r\n2019-02-28 19:45:23.520100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 3 Chunks of size 13107200 totalling 37.50MiB\r\n2019-02-28 19:45:23.520100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 1 Chunks of size 17028352 totalling 16.24MiB\r\n2019-02-28 19:45:23.521100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 1 Chunks of size 17694720 totalling 16.88MiB\r\n2019-02-28 19:45:23.521100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 1 Chunks of size 17694976 totalling 16.88MiB\r\n2019-02-28 19:45:23.521100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 3 Chunks of size 18874368 totalling 54.00MiB\r\n2019-02-28 19:45:23.521100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 1 Chunks of size 23592960 totalling 22.50MiB\r\n2019-02-28 19:45:23.521100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 5 Chunks of size 52428800 totalling 250.00MiB\r\n2019-02-28 19:45:23.529100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 5 Chunks of size 75497472 totalling 360.00MiB\r\n2019-02-28 19:45:23.529100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 1 Chunks of size 94371840 totalling 90.00MiB\r\n2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 1 Chunks of size 100362240 totalling 95.71MiB\r\n2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 2 Chunks of size 188743680 totalling 360.00MiB\r\n2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 1 Chunks of size 194688000 totalling 185.67MiB\r\n2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 12 Chunks of size 268435456 totalling 3.00GiB\r\n2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n1] 1 Chunks of size 552317184 totalling 526.73MiB\r\n2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n5] Sum Total of in-use chunks: 5.02GiB\r\n2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64\r\n7] Stats:\r\nLimit:                  5838622720\r\nInUse:                  5393793792\r\nMaxInUse:               5708028928\r\nNumAllocs:                     434\r\nMaxAllocSize:           1363673088\r\n\r\n2019-02-28 19:45:23.531100: W tensorflow/core/common_runtime/bfc_allocator.cc:27\r\n1] *****************************************************__**********_***********\r\n**********************x\r\n2019-02-28 19:45:23.531100: W tensorflow/core/framework/op_kernel.cc:1401] OP_RE\r\nQUIRES failed at conv_grad_input_ops.cc:1054 : Resource exhausted: OOM when allo\r\ncating tensor with shape[90,128,64,64] and type float on /job:localhost/replica:\r\n0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nTraceback (most recent call last):\r\n  File \"D:\\DeepFaceLab\\_internal\\bin\\DeepFaceLab\\test.py\", line 87, in <module>\r\n    loss = model.train_on_batch( [training_data], [training_data] )\r\n  File \"D:\\DeepFaceLab\\_internal\\bin\\lib\\site-packages\\tensorflow\\python\\keras\\e\r\nngine\\training.py\", line 1188, in train_on_batch\r\n    outputs = self.train_function(ins)  # pylint: disable=not-callable\r\n  File \"D:\\DeepFaceLab\\_internal\\bin\\lib\\site-packages\\tensorflow\\python\\keras\\b\r\nackend.py\", line 3076, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"D:\\DeepFaceLab\\_internal\\bin\\lib\\site-packages\\tensorflow\\python\\client\\\r\nsession.py\", line 1439, in __call__\r\n    run_metadata_ptr)\r\n  File \"D:\\DeepFaceLab\\_internal\\bin\\lib\\site-packages\\tensorflow\\python\\framewo\r\nrk\\errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocat\r\ning tensor with shape[90,128,64,64] and type float on /job:localhost/replica:0/t\r\nask:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[{{node training/Adam/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropInp\r\nut}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add repor\r\nt_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n```\r\n\r\n", "comments": ["still no solution?\r\nI guess it is not fixed in tf 2.0\r\n", "@iperov Is this still an issue? ", "@tatianashp are you kidding?\r\nreproduced significantly performance downgrade\r\nno fix or official comment about it", "@smit-hinsu Can you look into what's happening here?", "@iperov Sorry it fell through the cracks.", "Compared to 1.12, I'm finding that the exact same code uses about 10% extra GPU memory as per tf.profiler. Specifically, I get about 6400MB usage total (i.e., for _TFProfRoot) on 1.12.0 but about 7100MB for 1.13.1. With a smaller version of the same model, the proportional difference is about the same---about 3450MB for 1.13.1 and about 3100MB for 1.12.0.", "@rightaditya \r\nthx, you reproduced the bug too.\r\nWhat are next actions?", "Sorry for the long delay for the update.\r\n\r\nThis looks like a bug introduced between cuDNN v7.2 and v7.4. We will report this to NVIDIA and update this issue after that.\r\n\r\nThanks for providing with a small example to reproduce the issue.", "The difference seems to be coming from a change in CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING convolution algorithm implementation causing it to use more memory with cuDNN v7.4. I don't see any mention of this in the release notes but this might be an intended change.\r\n\r\nI was able to work around this issue by setting TF_CUDNN_WORKSPACE_LIMIT_IN_MB environment variable to \"1024\".\r\n\r\nIf for some reason that does not work out for you, then also give it a try setting TF_CUDNN_WORKSPACE_LIMIT_IN_MB to \"0\" and then TF_CUDNN_USE_AUTOTUNE to \"0\".\r\n\r\nLet us know if either of the above helps resolve the issue.", "@smit-hinsu tried both solutions, but oom still happens", "last cudnn 7.5.1 also fails", "I don't have the exact code that I tested earlier, but it didn't use convolutions at all. It was all LSTMs, the CUDNN ops if I'm remembering correctly.\r\nHowever, I should point out that some tests I ran yesterday found substantial variation in the memory usage reported by tfprof for the same code and data, so what I saw before may have been within the bounds of that variation. I did try it a few times with each version to get an idea of the distributions though and they didn't seem to overlap.", "We are experiencing the same issue even with tf-2.0. The model we use consumes approx 1GB of  more GPU memory than the pytorch implementation of the same model, though tf model is way faster. Looking forward to the solution.", "> \r\n> \r\n> last cudnn 7.5.1 also fails\r\n\r\nDid cudnn 7.6.0 solve this issue?", "cudnn 7.6.0 same problem", "Hi, any updates on this issue?", "@timshen91 Can you please take a look at this?  I think the next steps are:\r\n\r\n1. Verify that this is a problem still.\r\n2. If it is still a problem, file a bug with NVIDIA (CC @nluehr )", "With tf2.0.0 and batch=132, I cannot reproduce the OOM with the garbage collector on. With GC off, I can still see the OOM.\r\n\r\nI'll close the bug as GC seems to deallocate dead memory. Please re-open it if the problem still remains.\r\n\r\n```\r\n2020-01-03 14:18:31.905093: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-01-03 14:18:31.913239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: TITAN V major: 7 minor: 0 memoryClockRate(GHz): 1.455\r\npciBusID: 0000:d8:00.0\r\n2020-01-03 14:18:31.913417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-01-03 14:18:31.914810: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-01-03 14:18:31.915919: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-01-03 14:18:31.916171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-01-03 14:18:31.917729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-01-03 14:18:31.918760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-01-03 14:18:31.922199: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-03 14:18:31.922733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-01-03 14:18:31.922922: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2020-01-03 14:18:31.964210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz\r\n2020-01-03 14:18:31.976496: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x52556b0 executing computations on platform Host. Devices:\r\n2020-01-03 14:18:31.976542: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2020-01-03 14:18:32.117365: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x52b8130 executing computations on platform CUDA. Devices:\r\n2020-01-03 14:18:32.117430: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): TITAN V, Compute Capability 7.0\r\n2020-01-03 14:18:32.118844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: TITAN V major: 7 minor: 0 memoryClockRate(GHz): 1.455\r\npciBusID: 0000:d8:00.0\r\n2020-01-03 14:18:32.118913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-01-03 14:18:32.118941: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-01-03 14:18:32.118963: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-01-03 14:18:32.118985: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-01-03 14:18:32.119006: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-01-03 14:18:32.119027: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-01-03 14:18:32.119049: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-03 14:18:32.120211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-01-03 14:18:32.120271: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-01-03 14:18:32.122202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-03 14:18:32.122230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2020-01-03 14:18:32.122244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2020-01-03 14:18:32.124553: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n2020-01-03 14:18:32.124591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5537 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:d8:00.0, compute capability: 7.0)\r\n2020-01-03 14:18:32.911533: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-03 14:18:33.885741: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found\r\nRelying on driver to perform ptx compilation. This message will be only logged once.\r\n2020-01-03 14:18:34.006646: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.63GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-01-03 14:18:34.006681: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.63GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-01-03 14:18:34.385341: W tensorflow/core/common_runtime/bfc_allocator.cc:305] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\r\n2020-01-03 14:18:34.461297: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 3.43G (3678928896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-01-03 14:18:34.557533: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-01-03 14:18:35.221099: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.96GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-01-03 14:18:35.221135: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.96GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-01-03 14:18:35.641610: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.34GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-01-03 14:18:35.641645: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.34GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-01-03 14:18:35.708188: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-01-03 14:18:35.708215: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-01-03 14:18:35.834415: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.38GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-01-03 14:18:35.834441: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.38GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\nFINE\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26218\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26218\">No</a>\n"]}, {"number": 26217, "title": "Simplify python loops", "body": "This PR replaces loops over `range(len(...))` with `enumerate()` or `zip()`. This makes the code (arguably) easier to read and in some cases even slightly faster.", "comments": ["Sorry, I messed up the rebase from #25586 and included a change that broke the bazel build. Fixed it in the latest commit \ud83d\ude07 ", "@caisq can you PTAL , thank you", "@lgeiger can you please resolve conflicts ?", "> @lgeiger can you please resolve conflicts ?\r\n\r\n@rthadur Rebased it :+1:"]}, {"number": 26216, "title": "Keras: Simplify recurrent state updates", "body": "This PR replaces the collection of state updates with list comprehensions to improve readability.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26216) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26216) for more info**.\n\n<!-- ok -->", "For some reason GitHub didn't accept my rebase the first time. Sorry for the noise and reviews I have requested due to my first push. I don't have permissions to remove them \ud83d\ude07 ", "@lgeiger can you please resolve conflicts ? ", "> @lgeiger can you please resolve conflicts ?\r\n\r\n@rthadur I rebased this a month ago (right after you asked me to), and now again because it accumulated merge conflicts in the mean time. If you are interested in merging this please go ahead or close it otherwise.", "sure , @tanzhenyu could you please review ", "Reassigning to someone more familiar with recurrent layers.", "Can one of the admins verify this patch?", "@lgeiger can you please check build failures ?", "> @lgeiger can you please check build failures ?\r\n\r\n@rthadur The windows build failures seams unrelated and the `pylint` task fails with `tensorflow/python/keras/layers/serialization.py:32: [W0622(redefined-builtin), ] Redefining built-in 'zip'` in a file that isn't touched by this PR.\r\n\r\nI can rebase, if the failures are fixed on master, but that would require an additional approval to re-run CI.", "@lgeiger sure please make changes i can start CI.", "> @lgeiger sure please make changes i can start CI.\r\n\r\n\ud83d\udc4dRebased it", "@rthadur Looks like there are still failures, though they seam unrelated and I cannot view the logs to investigate a fix.", "@lgeiger here are latest  sanity builds failures https://source.cloud.google.com/results/invocations/a242630c-9f53-4fb6-be89-f7e4bcd83a38/log , please check", "@rthadur thanks for the link. I had to disable the pylint rule in 1dfd5fb17cdb88edb39a408b6093e80b5b7c25c1"]}, {"number": 26215, "title": "option for returning *state* sequences in tf.keras.layers.rnn", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\nCurrently tf.keras.layers.RNN has an option for returning the sequnces of outputs `return_sequences`, and an option for returning the final state `return_state`.  Sometimes it is useful to analyze the trajectory of c-states for a given sequence of inputs. Therefore I'd like to recommend including an option to `return_state_sequences` where just like `return_sequences` returns a sequence of outputs, `return_state_sequences` returns the sequence of internal states. \r\n", "comments": ["Is it still not possible to return the states as a sequence?\r\nI'm trying to implement an easy buffer for a subsequent RNN but I need the state as sequence too, not only the last one.\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras import backend as K\r\n\r\ninputs = np.random.random([2, 7, 4]).astype(np.float32)\r\n\r\nclass kRNNCell(tf.keras.layers.AbstractRNNCell):\r\n    def __init__(self, units, **kwargs):\r\n        self.units = units\r\n        super().__init__(**kwargs)\r\n    @property\r\n    def output_size(self):\r\n    \treturn self.units\r\n    @property\r\n    def state_size(self):\r\n        return self.units\r\n    def build(self, input_shape):\r\n        self.built = True\r\n    def call(self, inputs, states):\r\n        prev_output = states[0]\r\n        return prev_output, inputs\r\n\r\nrnn_cells = [kRNNCell(4) for _ in range(6)]\r\n\r\nstacked_rnn = tf.keras.layers.StackedRNNCells(rnn_cells)\r\n\r\nstacked_rnn_layer = tf.keras.layers.RNN(stacked_rnn, return_sequences=False,\r\n      return_state=True)\r\n\r\noutput = stacked_rnn_layer(inputs)\r\n\r\nprint(inputs)\r\nprint(tf.stack(output, axis=1)[:,:]) # <= this should be a sequence for the next layer\r\n```", "Hi @andrebeu !We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "hi @mohantym NB this was two years ago, I think i was on the current version.   \r\nNB all resolved ", "Hi @andrebeu ,Could you close this issue then?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 26214, "title": "No registered 'Placeholder' OpKernel for XLA_TPU_JIT devices compatible with node {{node tpu_139872799811176/input_2}}", "body": "Please have a look at my original post here: https://github.com/tensorflow/tensorflow/issues/26081\r\n It is no longer a tf.keras problem - now I get \r\n\r\n> RuntimeError: Compilation failed: Compilation failure: Detected unsupported operations when trying to compile graph cluster_15366487156777984482[] on XLA_TPU_JIT: Placeholder (No registered 'Placeholder' OpKernel for XLA_TPU_JIT devices compatible with node {{node tpu_139872799811176/input_2}}\r\n> \t.  Registered:  device='TPU'\r\n>   device='CPU'\r\n>   device='GPU'\r\n>   device='XLA_CPU'\r\n> ){{node tpu_139872799811176/input_2}}", "comments": ["two weeks ago, I opened this thread, now I have new results:\r\ni reinvented the code and got my training working on TPUs. For this I used a CNN. The CNN trains on TPU without any problem. Changing only the model to mobile net from tf.keras: `from tensorflow.keras.applications import MobileNet` brings us back the error.\r\n\r\n**### CNN:**\r\n![cnn_model_plot](https://user-images.githubusercontent.com/27775323/54422150-1e997b00-470e-11e9-91ac-cc0975681b78.png)\r\n\r\n**### Mobilenet:**\r\n![mobnet_model_plot](https://user-images.githubusercontent.com/27775323/54422155-21946b80-470e-11e9-9606-3652a394de65.png)\r\n\r\nYou can see the difference in the plot I attached cnn (working) and (mobnet) not working. Please excuse the large number instead of input layer in con-plot, sometimes plot_model has a problem.\r\n\r\nSo I guess the mobilenet pertained model of tf.keras is not compatible to TPUs.\r\n\r\n", "Thanks for this report. We're deprecating the tf.contrib.tpu.keras_to_tpu_model functionality in favor of DistributionStrategy, which should be available in the next TF release.\r\n\r\nWe will confirm that keras.applications work with DistributionStrategy TPU API.", "@DHOFM \r\n\r\nPlease, let us know if the issue still persists.Thanks!", "I have similar problem with 2.1 using DistributionStrategy. Shall I create new ticket?", "Thanks, please do create a new ticket (ideally with a reproduction)!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26214\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26214\">No</a>\n"]}, {"number": 26212, "title": "Lite: Conv operator optimized", "body": "1:> Memory foot print decreased in certain scenarios, by removing unnecessary temporary Tensor allocation for im2col.\r\n2:> Small code refactoring.\r\n3:> Error message improvised.", "comments": ["How did you measure performance? Can you quantify exactly what kind of improvement this made, on which device, against a particular model?", "> How did you measure performance? Can you quantify exactly what kind of improvement this made, on which device, against a particular model?\r\n\r\n@jdduke : Actually i found this problem, while i was examining Conv implementation in TFLite, it is not related to any model execution or device. But it is certainly a definite memory reduction though not very high.\r\n   To explain scenario, currently only \"kMultithreadOptimized\" Kernel is supported for non-CBLAS devices. So if it is not a hybrid case, then even though im2col is not used in kernel, im2col is allocated a chunk of memory. HWCN case also same, so i felt its better to keep it dependent on kernels, as some kernels dont use it.\r\n\r\nPlease provide your valuable opinion on it.", "@jdduke: All your comments are handled, please let me know, if anything needs to change, TIA!", "@jdduke: Your last comment is handled now, please check, Thanks!", "@jdduke : gentle reminder for conclusion, TIA!", "@jdduke : I have replied to your comment, need discuss with you, please find inline and reply, Thanks!", "> @jdduke : I have replied to your comment, need discuss with you, please find inline and reply, Thanks!\r\n\r\n@jdduke : Gentle Reminder!", "> I think this PR needs to be rebased. The version at head is different.\r\n\r\n@jdduke : Sorry for the confusion, actually i have re-based to the recent changes long back, but i was waiting for your reply to the concern i raised inline with your last comment. \r\nI am re-posting it below, please have a look, i think this might be a problem.\r\n\r\n> I have gone through latest merged changes. I think with the new changes kReference block in EvalFloat() method becomes dead code. As it always hits the else case in Eval() which hardcode kernel to kGenericOptimized. Please confirm whether this behavior is intended, Thanks!", "Hmm, I'm not seeing that, it should only fall back to kGenericOptimized for certain cases when it was already kMultithreadOptimized, not kReference.", "> Hmm, I'm not seeing that, it should only fall back to kGenericOptimized for certain cases when it was already kMultithreadOptimized, not kReference.\r\n\r\n@jdduke : Let us consider the case when kReference kernel is registered to execute, in that case it will never execute kReference kernel, it will always fall to kGenericOptimized. I think this should not happen. Please let me know your opinion on this, Thanks!", "Ahh, now I see what you're saying, and you're absolutely right. I'll upload a fix.", "> Ahh, now I see what you're saying, and you're absolutely right. I'll upload a fix.\r\n\r\n@jdduke : Thanks for quick fix. I have now re-based with latest changes, would you please help conclude on this PR, Thanks!", "@jdduke : Your comment is addressed now, please check, thanks!", "@jdduke : Gentle Reminder!", "@jdduke : Gentle Reminder!", "> Is the CL description still accurate?\r\n\r\nUpdated.\r\n\r\n> Reading the code now, I'm not sure it's actually any easier to follow. What are the primary differences now in terms of im2col/hwcn usage versus previously?\r\n\r\n@jdduke : As currently the kernel registration is static, so we cant see the wider coverage of the change. However, i can state that with this change it covers 3 run-time cases hybrid, quantized & multi-threaded case, which will not be covered in base code.  Please suggest if you feel something need improvement. Thanks!", "> Memory foot print decreased in certain scenarios, by removing unnecessary temporary Tensor allocation for im2col.\r\n\r\nWhich cases now avoid using im2col? I still think I preferred the inline logic before, it's easier to follow, but maybe this is the only way to avoid generating im2col when unnecessary?", "Can one of the admins verify this patch?", "@ANSHUMAN87 Could you please address the reviewer comments and resolve conflicts? Thanks!", "@gbaned, I will handle it next week. Thanks", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@gbaned : Conflict resolved!", "> > Memory foot print decreased in certain scenarios, by removing unnecessary temporary Tensor allocation for im2col.\r\n> \r\n> Which cases now avoid using im2col? I still think I preferred the inline logic before, it's easier to follow, but maybe this is the only way to avoid generating im2col when unnecessary?\r\n\r\nYes, you are right code has more complex checks now, but we dont have any other choice i think!", "@gbaned : This is important PR for me long pending, would you please help to merge, Thanks!", "@jdduke : Its done, Thanks!"]}, {"number": 26211, "title": "[TF 2.0 API Docs] tf.keras.activations.relu ", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 2.0\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/relu\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\n* The description is minimal, not written with complete sentences, and lacks recommendations of when and when not to use the symbol.\r\n\r\n* There is no usage example.\r\n\r\n* The parameters are described only briefly and with inconsistent capitalization.\r\n\r\n* The returned object description could be more useful.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n\r\nYes.", "comments": ["I'm terribly sorry, but I can't find the relevant file to work on in the docs repository. I see in CONTRIBUTING.md that \"[s]ome projects have their repositories under github.com/tensorflow [and] are navigable from the tensorflow/docs site/en directory,\" but I can't manage to actually navigate there from the site/en directory. I've tried searching on GitHub, but I can't find it there either.\r\n\r\nIt is likely that I'm missing something obvious, but I'd be very grateful to be pointed to the repository that contains the activations.relu doc; I'm eager to help with this.", "@BlackHoundNate - No worries! Files for the docs are often hard to track down. :slightly_smiling_face: \r\n\r\nThe part of `activations.py` you'll want to modify for `tf.keras.actions.relu` is located [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/activations.py#L143). Let me know if you run into any snags or confusions when making changes to the docstring! And thanks so much for contributing - we appreciate it. :tada: ", "Thanks so much, @dynamicwebpaige!", "Suppose you want to start from your home and drive around your neighborhood and get\r\nback to exactly where you started (your home) and you want this ride to exactly take 10\r\nminutes. Assume that all roads are laid out in a perfect grid. Suppose there is an app that\r\ngives users random paths to drive throughout the city. Every time you press the button, it\r\ngives you a random set of one-letter strings representing the directions to take. For\r\nexample, [\u2018N\u2019, \u2018W\u2019, \u2018W\u2019, \u2018S\u2019, \u2018E\u2019] means that go one block to north, two blocks to west,\r\none block to south and one block to east.\r\nKnowing that it takes you one minute to traverse one city block, create a function that\r\ngiven a path from the app, will return true if the path will take you exactly ten minutes to\r\ndrive and will return you where you started from (home). Return false otherwise.\r\nNote: you will always receive a valid array containing a random assortment of direction\r\nletters ('N', 'S', 'E', or 'W' only). It will never give you an empty array. Some one help me to write program for this in python ", "please review.\r\n@dynamicwebpaige ", "Can I take this issue?", "@yagyajoshi15 we need more of those. ", "Will try my hand at this issue", "I want to take this issue.", "I made a pull request please review and give feedback", "@Williscool13 's PR has already been merged. This issue should be closed already. \r\n@BlackHoundNate @dynamicwebpaige ", "@dynamicwebpaige  Please close this issue as this seems to be already solved.", "Thank you so much. Fixed in nightly: https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu?version=nightly"]}, {"number": 26210, "title": "app_loader failure at tensorflow.org makes website unusable", "body": "**System information**\r\n- TensorFlow version: N/A\r\n- Doc Link: https://www.tensorflow.org/\r\n\r\nMy locale is set to uk_UA. When I visit https://www.tensorflow.org/, the app_loader.js tries to load https://www.gstatic.com/devrel-devsite/v138ceba75b8052825cb18f1c6025ea06485e9d36137e0fd82d2d54619f803209/tensorflow/js/devsite_app__uk.js (notice the added '__uk' suffix). The file does not exist. Since devsite_app.js provides the main functionality, none of the links, buttons, and selectors that use JavaScript are operational. This includes the top nav bar, the side nav bar, and the search. It also makes it very hard to navigate to APIs and to browse the API pages. It is not possible to change the language, because the language drop-down is also using JavaScript.\r\n\r\nThe problem persists across different browsers (Firefox, Safari, Chrome, Vivaldi) and I assume it also affects other locales for which a localised devsite_app.js is not available (for instance, Belarusian be, Croatian hr, etc.)\r\n\r\nIdeally, the app_loader should check whether the localised version exists. If not, it should load the unlocalised version instead.\r\n\r\nI'm sorry if this is not the right place for the bug. However, it is directly related to the documentation and I could not find any other places to submit.\r\n", "comments": ["Thanks, Roman.\r\n\r\nI've filed an internal bug against site infra: b/126800213", "Marked as fixed in b/116473083\r\nI think this is the same issue as https://github.com/tensorflow/tensorflow/issues/26449\r\n"]}, {"number": 26209, "title": "TensorFlow 1.13.1: ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.6 LTS\r\n- TensorFlow installed from (source or binary): Binary (pip3 install -U tensorflow-gpu\r\n- TensorFlow version: current version from pip (maybe version 1.13.1)\r\n- Python version: Python 3.5.2\r\n- Installed using virtualenv? pip? conda?: virtualenv environment with pip\r\n- CUDA/cuDNN version: Cuda 9.0 \r\n- GPU model and memory: Titan Xp\r\n\r\n\r\n\r\n**Describe the problem**\r\nI tried to install tensorflow-gpu follow the instruction at [link](https://www.tensorflow.org/install/gpu) and pip install -U tensorflow gpu. But when I check it `import tensorflow as tf` appear a bug as follow: \r\n\r\n> ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\nMeanwhile, I also tried installing pytorch and succeeded. (Figure below)\r\n![screenshot from 2019-02-28 20-31-09](https://user-images.githubusercontent.com/28798474/53569943-e0417f00-3b97-11e9-8a31-7c23a0f2b481.png)\r\n", "comments": ["how did you solve it ?", "just had the same issue on Debian", "> how did you solve it?\r\n\r\nYes, I figured out my system installed Cuda 9.0 but maybe tensorflow 1.13.1 only support Cuda 10. So I  installed tensorflow 1.12.0 and it works!\r\n\r\nThanks!", "thanks mate\r\n\r\nworked for me as well \r\n\r\nI will open an issue about it", "I have TensorFlow 1.13.1 working on Ubunutu 16.04 with Python 3.7.2.\r\nI had the error above using CUDA 10.1, but it went away after downgrading to CUDA 10.0", "> > how did you solve it?\r\n> \r\n> Yes, I figured out my system installed Cuda 9.0 but maybe tensorflow 1.13.1 only support Cuda 10. So I installed tensorflow 1.12.0 and it works!\r\n> \r\n> Thanks!\r\n\r\nThank you. It worked for me.", "> Yes, I figured out my system installed Cuda 9.0 but maybe tensorflow 1.13.1 only support Cuda 10. So I installed tensorflow 1.12.0 and it works!\r\n\r\nThanks a lot mate! This helped me .\r\n", "> I have TensorFlow 1.13.1 working on Ubunutu 16.04 with Python 3.7.2.\r\n> I had the error above using CUDA 10.1, but it went away after downgrading to CUDA 10.0\r\n\r\nSame here on CentOS 7, TF 1.13.1 would not import with latest CUDA (10.1) but worked fine after downgrade to CUDA 10.0. Thanks for the tip.", "Without downgrade any chance ?", "> I have TensorFlow 1.13.1 working on Ubunutu 16.04 with Python 3.7.2.\r\n> I had the error above using CUDA 10.1, but it went away after downgrading to CUDA 10.0\r\n\r\nHow do you go about downgrading CUDA from 10.1 -> 10.0? Running arch btw", "same problem with\r\n```\r\ntensorflow-gpu==2.0.0a0\r\nPython 3.7.3\r\n5.0.7-arch1-1-ARCH\r\nlinux 5.0.7.arch1-1\r\nDell Raven Ridge [Radeon Vega Series / Radeon Vega Mobile Series]\r\nOpenGL renderer string: AMD RAVEN (DRM 3.27.0, 5.0.7-arch1-1-ARCH, LLVM 8.0.0)\r\n#cuda not available\r\n```", "We need to wait until **tensorflow** providing support to **CUDA10.1**.", "I had same problem with\r\n\r\n`tensorflow-gpu==1.13.1`\r\n`Python 3.7.3`\r\n`OS = Ubuntu 18.04`\r\n`cuda 10.1`\r\n`cudNN = 7.5`\r\n\r\nI fixed this by downgrading python to 3.5.5 and reinstall tensorflow-gpu", "Can you explain how to easily downgrade from cuda 10.1->10.0? Do you also have to change the Cudnn libraries?", "> Can you explain how to easily downgrade from cuda 10.1->10.0? Do you also have to change the Cudnn libraries?\r\n\r\nTo my recollection, I only had to install 10.0 and it installed in the same directory and updated the symlink. I did not reinstall libcudnn because I don't need it yet, but I expect you would have to if you need it.\r\n\r\nls -l /usr/local/\r\nlrwxrwxrwx   1 root root    9 Apr  2 19:57 cuda -> cuda-10.0\r\ndrwxr-xr-x  16 root root 4096 Apr  2 19:57 cuda-10.0\r\ndrwxr-xr-x  17 root root 4096 Apr  2 18:17 cuda-10.1\r\n", "> Can you explain how to easily downgrade from cuda 10.1->10.0? Do you also have to change the Cudnn libraries?\r\n\r\nMy situation is whichever install method I tried following the official archive [here](https://developer.nvidia.com/cuda-10.0-download-archive),  apt-get always install 10.1\r\nIf you face same problem, you can reference [this](https://www.pytorials.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-ubuntu/) article", "On ubuntu 18.10 - I successfully installed CUDA 10.0 toolkit by downgrading gcc and g++ version to 7.3. \r\nNow I can see my `nvcc --version` is `10.0 `.  \r\n\r\nBut still I getting that common errors.... `libcublas.so.10.0` is missing\r\n\r\nAny IDEA ? Now what's a issue !\r\n\r\n```\r\n\r\nshivangpatel@shivangpatel:~$ python3 -c 'import tensorflow as tf; print(tf.__version__)'  # for Python 3\r\nTraceback (most recent call last):\r\n  File \"/home/shivangpatel/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/shivangpatel/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/shivangpatel/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/shivangpatel/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/shivangpatel/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/shivangpatel/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/shivangpatel/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/shivangpatel/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/shivangpatel/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n```", "using tensorflow 1.12, it is working.", "> Can you explain how to easily downgrade from cuda 10.1->10.0? Do you also have to change the Cudnn libraries?\r\n\r\nyou can do that with this command.\r\nsudo apt purge cuda\r\nsudo apt autoremove\r\nsudo apt install cuda-10-0\r\n\r\nAnd now it is working.", "@kiitosu I tried that and still had the same error\r\nWhen you install `cuda-10-0` on ubuntu 16.04 it does not install `libcudnn.so.7`\r\nTo install cudnn you need to download `cuDNN v7.5.1` deb package from https://developer.nvidia.com/rdp/cudnn-download\r\nand then install it \r\n```\r\nsudo dpkg -i libcudnn7_7.5.1.10-1+cuda10.0_amd64.deb\r\n```\r\nAfter that `import tensorflow as tf` works!", "> @kiitosu I tried that and still had the same error\r\n> When you install `cuda-10-0` on ubuntu 16.04 it does not install `libcudnn.so.7`\r\n> To install cudnn you need to download `cuDNN v7.5.1` deb package from https://developer.nvidia.com/rdp/cudnn-download\r\n> and then install it\r\n> \r\n> ```\r\n> sudo dpkg -i libcudnn7_7.5.1.10-1+cuda10.0_amd64.deb\r\n> ```\r\n> \r\n> After that `import tensorflow as tf` works!\r\n\r\nYes I installed cuDNN before reinstall cuda10-0.", "I'm having a similar issue.\r\nwhen I do nvcc --version, I get \r\nCuda compilation tools, release 9.1, V9.1.85\r\n\r\nbut when I do nvidia-smi, \r\nI see cuda version 10 (I'm using 10 in my projects, so there's obviously an issue. How do I uninstall 9.x if that is the issue?\r\n", "@tshr729 - I wonder how got it working with CUDA 10.1 by just downgrading python, doesn't make much sense to me :-), the issue is in not supporeted version by Tensorflow itself...\r\n\r\nSo on my Gentoo system I have CUDA 10.1.105 which creates following platform links:\r\n```\r\nandromeda /opt/cuda/targets/x86_64-linux/lib # ls -la|grep cublas\r\nlrwxrwxrwx 1 root root        17 May  3 11:14 libcublasLt.so -> libcublasLt.so.10\r\nlrwxrwxrwx 1 root root        25 May  3 11:14 libcublasLt.so.10 -> libcublasLt.so.10.1.0.105\r\n-rwxr-xr-x 1 root root  37058992 May  3 11:14 libcublasLt.so.10.1.0.105\r\n-rw-r--r-- 1 root root  23513690 May  3 11:14 libcublasLt_static.a\r\nlrwxrwxrwx 1 root root        15 May  3 11:14 libcublas.so -> libcublas.so.10\r\nlrwxrwxrwx 1 root root        23 May  3 11:14 libcublas.so.10 -> libcublas.so.10.1.0.105\r\n-rwxr-xr-x 1 root root  78315120 May  3 11:14 libcublas.so.10.1.0.105\r\n-rw-r--r-- 1 root root  90723762 May  3 11:14 libcublas_static.a\r\n```\r\nSo once I downgrade to 10.0.130 I get what Tensorflow looks for in following platform structure:\r\n```\r\nandromeda /opt/cuda/targets/x86_64-linux/lib # ls -la|grep cublas\r\nlrwxrwxrwx 1 root root        17 Sep 12  2018 libcublas.so -> libcublas.so.10.0\r\nlrwxrwxrwx 1 root root        21 Sep 12  2018 libcublas.so.10.0 -> libcublas.so.10.0.130\r\n-rwxr-xr-x 1 root root  70796360 Sep 12  2018 libcublas.so.10.0.130\r\n-rw-r--r-- 1 root root  88164166 May 17 17:51 libcublas_static.a\r\n```\r\n\r\nSo now 10.0 link which is correct. I cannot understand how it by default works for you... \r\n", "> > how did you solve it?\r\n> \r\n> Yes, I figured out my system installed Cuda 9.0 but maybe tensorflow 1.13.1 only support Cuda 10. So I installed tensorflow 1.12.0 and it works!\r\n> \r\n> Thanks!\r\n\r\nMaybe that's not true. I have a machine with tf 1.13.1 working well with CUDA 9.0 and another machine with tf 1.13.1 working well with CUDA 8.0", "if you are using conda\r\n\r\n```shell\r\nconda install cudatoolkit cudnn\r\n```\r\nno need to install cuda.\r\n\r\nRecently I reinstalled ubuntu then have this problem. \r\nI searched for `libcublas.so.10.0` on pc, only found it under conda floader., maybe I should install cuda by deb.\r\n\r\n> tensorflow-gpu=1.13.1\r\n> python=3.7.3\r\n> conda 4.6.14\r\n", "I am using tensorflow-gpu=2.0.  Found `libcublas.so.10.0`  in `/usr/local/xxx-cu/targets/x86_64-linux/lib` (using Linux) using the `sudo find`. Then I added it to my `LD_LIBRARY_PATH` and it seemed to solve this issue.", "![sameubuntu1904stuffhere](https://user-images.githubusercontent.com/9089/58996980-b8972e00-87fa-11e9-83a0-a18b7bcc4de3.png)\r\nas @archenroot mentioned - I do not understand that either\r\n\r\nI specially made new 19.04 UBUNTU installation from scratch to rule off my all old dependencies.\r\n\r\nIt looks like some general problem with \r\n\r\n10 ```.0``` vs ```.1``` problem\r\n\r\neven with tf-nightly-gpu\r\n\r\n`(venv) pmasior@tr2950x:~$ python -c \"from tensorflow.python.client import device_lib; print(device_lib.list_local_devices())\"\r\n2019-06-06 01:09:11.852551: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-06 01:09:11.891764: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3492930000 Hz\r\n2019-06-06 01:09:11.893733: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x27c8160 executing computations on platform Host. Devices:\r\n2019-06-06 01:09:11.893757: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-06-06 01:09:11.894730: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-06-06 01:09:11.938466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-06 01:09:11.939175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.755\r\npciBusID: 0000:42:00.0\r\n2019-06-06 01:09:11.939249: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\r\n2019-06-06 01:09:11.939284: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n2019-06-06 01:09:11.939315: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\r\n2019-06-06 01:09:11.939346: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\r\n2019-06-06 01:09:11.939376: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\r\n2019-06-06 01:09:11.939405: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\r\n2019-06-06 01:09:11.941656: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-06 01:09:11.941677: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...\r\n2019-06-06 01:09:12.009924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-06 01:09:12.009952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-06-06 01:09:12.009957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-06-06 01:09:12.011507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-06 01:09:12.012722: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4cbebf0 executing computations on platform CUDA. Devices:\r\n2019-06-06 01:09:12.012738: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 10170710468233217199\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 10626384075838738102\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n, name: \"/device:XLA_GPU:0\"\r\ndevice_type: \"XLA_GPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 11146589999346767740\r\nphysical_device_desc: \"device: XLA_GPU device\"\r\n]\r\n`", "Hello,\r\n\r\nI had the same issue. I fixed it by adding the below command to the '**.bashrc**' file.\r\n\r\n> export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.0/lib64/\r\n\r\n**System configuration:**\r\n\r\n```\r\nUbuntu 16.04 LTS\r\nCuda 10.0\r\ncuDNN 7.6.0 for Cuda 10.0\r\n```\r\n\r\nI  used conda to configure my system.", "2019-06-30 11:29:50.377478: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-06-30 11:29:50.627371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-30 11:29:50.627688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176\r\npciBusID: 0000:01:00.0\r\n2019-06-30 11:29:50.627840: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kelvin/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib::/usr/local/cuda-10.1/lib64\r\n2019-06-30 11:29:50.627943: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kelvin/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib::/usr/local/cuda-10.1/lib64\r\n2019-06-30 11:29:50.628046: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kelvin/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib::/usr/local/cuda-10.1/lib64\r\n2019-06-30 11:29:50.628156: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kelvin/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib::/usr/local/cuda-10.1/lib64\r\n2019-06-30 11:29:50.628258: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kelvin/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib::/usr/local/cuda-10.1/lib64\r\n2019-06-30 11:29:50.628347: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kelvin/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib::/usr/local/cuda-10.1/lib64\r\n2019-06-30 11:29:50.628438: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kelvin/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib::/usr/local/cuda-10.1/lib64\r\n2019-06-30 11:29:50.628451: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...\r\n2019-06-30 11:29:50.628741: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-30 11:29:50.656947: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2893390000 Hz\r\n2019-06-30 11:29:50.657609: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x51f97b0 executing computations on platform Host. Devices:\r\n2019-06-30 11:29:50.657673: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-06-30 11:29:50.657886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-30 11:29:50.657925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      \r\n2019-06-30 11:29:50.677345: W tensorflow/compiler/xla/service/platform_util.cc:256] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 2101870592\r\n2019-06-30 11:29:50.677498: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\nAborted (core dumped)\r\n(cv3) \r\n", "any update in September 2019?? \r\nI'm with Cuda 10.1", "Not for TF 1.13. Please switch to 1.15 or 2.0", "> \r\n> \r\n> using tensorflow 1.12, it is working.\r\n\r\nthanks !!! wired but works for me.\r\n/usr/local/cuda-10.1/\r\ntensorboard                   1.12.2\r\ntensorboardX                  1.7\r\ntensorflow                    1.12.0\r\ntensorflow-estimator          1.13.0\r\ntensorflow-gpu                1.13.1\r\ntensorly                      0.4.4\r\nPython 3.6.8", "I had the same problem with tf 1.14 on Ubuntu 19.10 with Cuda 10.1 \r\n\r\n`print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))`\r\n\r\nIf it reports 0 GPUs, it should list libraries that it can't find. I got it working by creating symlinks from the 10.1 libraries to the 10.0 names that TensorFlow reported was missing\r\n\r\n`sudo ln -s /usr/lib/x86_64-linux-gnu/libcusparse.so.10.1.168 /usr/lib/x86_64-linux-gnu/libcusparse.so.10.0`\r\n\r\nYou'll need to do a find to locate the library locations and there will likely be 5-10 libraries listed that you'll need to do this for. ", "I solved the problem with missing libraries in tensorflow-gpu by creating these 7 links:\r\n\r\n`sudo ln -s /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2.89 /usr/local/cuda/lib64/libcudart.so.10.0`\r\n`sudo ln -s /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudnn.so.7.6.5 /usr/local/cuda/lib64/libcudnn.so.7`\r\n`sudo ln -s /usr/lib/x86_64-linux-gnu/libcublas.so.10.2.2.89 /usr/local/cuda/lib64/libcublas.so.10.0`\r\n`sudo ln -s /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcufft.so.10.1.2.89 /usr/local/cuda/lib64/libcufft.so.10.0`\r\n`sudo ln -s /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcurand.so.10.1.2.89 /usr/local/cuda/lib64/libcurand.so.10.0`\r\n`sudo ln -s /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcusolver.so.10.3.0.89 /usr/local/cuda/lib64/libcusolver.so.10.0`\r\n`sudo ln -s /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcusparse.so.10.3.1.89 /usr/local/cuda/lib64/libcusparse.so.10.0`\r\n", "> I had the same problem with tf 1.14 on Ubuntu 19.10 with Cuda 10.1\r\n> \r\n> `print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))`\r\n> \r\n> If it reports 0 GPUs, it should list libraries that it can't find. I got it working by creating symlinks from the 10.1 libraries to the 10.0 names that TensorFlow reported was missing\r\n> \r\n> `sudo ln -s /usr/lib/x86_64-linux-gnu/libcusparse.so.10.1.168 /usr/lib/x86_64-linux-gnu/libcusparse.so.10.0`\r\n> \r\n> You'll need to do a find to locate the library locations and there will likely be 5-10 libraries listed that you'll need to do this for.\r\n\r\nThis worked better for me than putting them in the CUDA directory at first, but that was because my LD_LIBRARY_PATH was not set. In the CUDA installer guide, they say that this is only necessary for the post-install of a runfile installer. This is bad advice. do it for all installations. Then @Aquinius-Aquila answer will work.", "> if you are using conda\r\n> \r\n> ```shell\r\n> conda install cudatoolkit cudnn\r\n> ```\r\n> \r\n> no need to install cuda.\r\n> \r\n> Recently I reinstalled ubuntu then have this problem.\r\n> I searched for `libcublas.so.10.0` on pc, only found it under conda floader., maybe I should install cuda by deb.\r\n> \r\n> > tensorflow-gpu=1.13.1\r\n> > python=3.7.3\r\n> > conda 4.6.14\r\n\r\nMiraculously this worked! Though I had to specify cuda version as 10.0, otherwise it installed 10.2\r\n\r\n`conda install cudatoolkit=10.0 cudnn`", "> > how did you solve it?\r\n> \r\n> Yes, I figured out my system installed Cuda 9.0 but maybe tensorflow 1.13.1 only support Cuda 10. So I installed tensorflow 1.12.0 and it works!\r\n> \r\n> Thanks!\r\n\r\nThanks! It was really helpful to me. I have been suffering this problem for 1 month and cant figure out how to fix this until i see your comment! <3 You are so smart <3"]}, {"number": 26208, "title": "TensorFlow device GPU:0 was not registered", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Red Hat Enterprise Linux Server release 7.4\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.10.0\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: V9.2.88\r\n- GPU model and memory: Intel(R) Xeon(R) CPU E5-2690 0 @ 2.90GHz\r\n\r\n**Describe the current behavior**\r\nCurrently I am using distributed Tensorflow to connect two machines one with only a CPU(ps) and one with a CPU and GPU(worker), the worker machine runs only the cluster and server commands followed by a join command and then remains idle, \r\nthe training algorithm is run on the ps machine after defining the cluster and the server and then the training is run but I get \"E tensorflow/core/grappler/clusters/utils.cc:128] Not found: TensorFlow device GPU:0 was not registered\" between each epoch of training and it is not run on the gpu (the delay is very big)\r\n**Describe the expected behavior**\r\nthe training should be able to run on the GPU of the worker machine\r\n\r\n**Code to reproduce the issue**\r\nCode run on the worker machine :\r\nimport tensorflow as tf\r\n\r\ncluster = tf.train.ClusterSpec({\r\n    \"worker\": [\r\n        \"172.24.145.121:2222\",\r\n    ],\r\n    \"ps\": [\r\n        \"172.24.145.14:2222\"\r\n    ]})\r\nserver = tf.train.Server(cluster, job_name=\"worker\", task_index=0)\r\nserver.join()\r\n\r\nand on the ps machine:\r\nimport tensorflow as tf\r\nimport time\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\ncluster = tf.train.ClusterSpec({\r\n    \"worker\": [\r\n        \"172.24.145.121:2222\",\r\n    ],\r\n    \"ps\": [\r\n        \"172.24.145.14:2222\"\r\n    ]})\r\nserver = tf.train.Server(cluster, job_name=\"ps\", task_index=0)\r\n\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\nmnist = input_data.read_data_sets(\"/tmp/data\",one_hot=True)\r\n\r\nn_nodes_hl1=500\r\nn_nodes_hl2=500\r\nn_nodes_hl3=500\r\n\r\nn_classes = 10\r\nbatch_size = 100\r\nx = tf.placeholder('float',[None, 784],name=\"x\")\r\ny = tf.placeholder('float',name=\"y_pred\")\r\n\r\ndef neural_network_model(data):\r\n    with tf.device(\"/job:worker/task:0/gpu:0\"):\r\n    \thidden_1_layer= {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1]),name=\"layer1_w\"),\r\n                    \t'biases':tf.Variable(tf.random_normal([n_nodes_hl1]),name=\"layer1_b\")}\r\n\r\n    \thidden_2_layer= {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2]),name=\"layer2_w\"),\r\n                \t    'biases':tf.Variable(tf.random_normal([n_nodes_hl2]),name=\"layer2_b\")}\r\n\r\n    \thidden_3_layer= {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3]),name=\"layer3_w\"),\r\n            \t        'biases':tf.Variable(tf.random_normal([n_nodes_hl3]),name=\"layer3_b\")}\r\n\r\n    \toutput_layer= {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes]),name=\"output_w\"),\r\n        \t            'biases':tf.Variable(tf.random_normal([n_classes]),name=\"output_b\")}\r\n\r\n    l1= tf.add(tf.matmul(data,hidden_1_layer['weights']),hidden_1_layer['biases'],name=\"l1\")\r\n    l1= tf.nn.relu(l1)\r\n\r\n    l2= tf.add(tf.matmul(l1,hidden_2_layer['weights']),hidden_2_layer['biases'],name=\"l2\")\r\n    l2= tf.nn.relu(l2)\r\n\r\n    l3= tf.add(tf.matmul(l2,hidden_3_layer['weights']),hidden_3_layer['biases'],name=\"l3\")\r\n    l3= tf.nn.relu(l3)\r\n\r\n    output= tf.add(tf.matmul(l3,output_layer['weights']),output_layer['biases'],name=\"output\")\r\n\r\n    return output\r\n\r\ndef train_neural_network(x):\r\n    prediction=neural_network_model(x)\r\n    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=y))\r\n\r\n    optimizer = tf.train.AdamOptimizer().minimize(cost)\r\n\r\n    hm_epoch = 10\r\n   with tf.Session(server.target) as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        for epoch in range(hm_epoch):\r\n            epoch_loss =0 \r\n            start_time = time.time()\r\n            for _ in range(int(mnist.train.num_examples/batch_size)): #(X_train.size/(28*28))\r\n                epoch_x,epoch_y=mnist.train.next_batch(batch_size)  \r\n                _,c = sess.run([optimizer, cost], feed_dict = {x:epoch_x, y:epoch_y})\r\n                epoch_loss += c\r\n            duration = time.time()-start_time\r\n            print('Epoch', epoch+1,'completed out of ',hm_epoch,'loss: ',epoch_loss)\r\n            correct = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\r\n            accuracy = tf.reduce_mean(tf.cast(correct,'float'))\r\n            print('Accuracy:' ,accuracy.eval({x:mnist.test.images,y:mnist.test.labels}),' Duration: ' ,duration)\r\ntrain_neural_network(x)\r\n\r\n**Other info / logs**\r\n E tensorflow/core/grappler/clusters/utils.cc:128] Not found: TensorFlow device GPU:0 was not registered", "comments": ["Can you run the following to check whether TensorFlow is actually seeing your GPUs? If not, you might want to try reinstalling CUDA/CuDNN and TensorFlow (make sure you are using the GPU binaries for TF).\r\n\r\nAlternatively, you might also want to try our docker images with a bunch of those built-in: https://www.tensorflow.org/install/docker\r\n\r\n```\r\nimport tensorflow as tf\r\ns = tf.Session()\r\nprint(s.list_devices())\r\n```", "TensorFlow sees the GPU if I run the master session from the node that has the GPU, but if I start the master session is from the node that only has cpu I get the TensorFlow device GPU:0 was not registered, even though I can run variables on the cpu of the other node that has both the GPU and CPU", "So if you do something like\r\n\r\n```\r\nimport tensorflow as tf\r\ns = tf.Session(master=\"grpc://remote.server.ip.address:port\")\r\nprint(s.list_devices())\r\n```\r\n\r\nyou won't see GPU, but if you run that snippet locally you will? I wonder if the local version you run with Python is different somehow than the remote server you start?", "Well, When I ran this snippet it `import tensorflow as tf\r\ns = tf.Session(master=\"grpc://remote.server.ip.address:port\")\r\nprint(s.list_devices())` it shows all the devices correctly and after a bit of testing there is indeed improved performance which shows that the GPU of the remote server is being used, but \r\n> E tensorflow/core/grappler/clusters/utils.cc:128] Not found: TensorFlow device GPU:0 was not registered\r\n\r\n> \r\n\r\nKeeps showing up between each of the epoch's in training when the the master session is initiated on the node with only the cpu.  I don't get this error when I initiate the master session and run the training on the node that contains the gpu and cpu, and use the other cpu node remotely.", "Can you show what `s.list_devices()` returns? \r\n\r\nAlso, in your original script, you are assigning ops to `/job:worker/task:0/gpu:0`, however the [canonical TensorFlow device names](https://www.tensorflow.org/guide/using_gpu#using_multiple_gpus) for GPUs are in the format `/job:worker/task:0/device:GPU:0`, so that might be an issue as well.", "ok after changing from `/job:worker/task:0/gpu:0` to `/job:worker/task:0/device:GPU:0` there was no difference i still get the \r\n> `E tensorflow/core/grappler/clusters/utils.cc:128] Not found: TensorFlow device GPU:0 was not registered` \r\n\r\n> \r\n\r\nand `s.list_devices()` returns `W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:349] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\r\n[_DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 268435456), _DeviceAttributes(/job:worker/replica:0/task:0/device:GPU:0, GPU, 11281927373), _DeviceAttributes(/job:ps/replica:0/task:0/device:CPU:0, CPU, 268435456)]`\r\nThe same devices are there if i run the session from locally or remotely the only difference is if I run it remotely i dont get the\r\n> `E tensorflow/core/grappler/clusters/utils.cc:128] Not found: TensorFlow device GPU:0 was not registered` ", "> The same devices are there if i run the session from locally or remotely the only difference is if I run it remotely i dont get the\r\n\r\nDo you mean if you run it locally on the machine with the GPU you don't get the error?\r\n\r\nCan you also try passing in a cluster_def when you create the session?\r\n\r\n```\r\ncluster_spec = tf.train.ClusterSpec( ... )\r\ncluster_def = cluster_spec.as_cluster_def()\r\nwith tf.Session(target=master, config=tf.ConfigProto(cluster_def=cluster_def, log_device_placement=True)):\r\n  ...\r\n```\r\n", "Yes, if I run it locally on the machine with the GPU I don't get the error, also if I run the session remotely on the machine with the GPU using the CPU machine I don't get the error (using the ` tf.Session(\"grpc://remote.server.ip.address:port\")` ), but if I start the session on the machine with only the CPU I get the error.", "when i try to pass the cluster_def when I create the session I get this error\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: The ClusterSpec names the job and task index to be the same names that were provided when the server booted. This is currently not allowed. Job: ps, task index: 0`", "Can you create both `tf.train.Server`s without passing in the ClusterSpec? ClusterSpec propagation only works when there are no conflicting types.", "Yes", "So does it work without the ClusterSpec?", "Closing issue for now, feel free to re-open if this continues to be an issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26208\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26208\">No</a>\n", "@frankchn Hi, do you mean the clusterspec for the server.start() and server.join() are different thats why it raises \"Not found: TF GPU device with id 0 was not registered.\" ? because i got the same issue as well but i thought tf.train.Server must take clusterspec as argument.\r\n\r\nsince i am using estimator, i am not sure how to do the cluster_def tho", "@colmantse Can you open another issue with more complete details? That error happens when TensorFlow cannot detect GPUs in your system.", "hi, thanks for the prompt reply. I suspect it has to do with me passing the wrong configproto when starting the server. i will come back tmr if it is not the case. many thanks!"]}, {"number": 26207, "title": "MirroredStrategy returns AssertionError when run with custom Estimator", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version (use command below):\r\n1.12\r\n- Python version:\r\n3.6\r\n- Bazel version (if compiling from source):\r\n0.17.2\r\n- GCC/Compiler version (if compiling from source):\r\n7.3\r\n- CUDA/cuDNN version:\r\nCUDA 10.0\r\ncuDNN 7.4\r\n- GPU model and memory:\r\nnVidia Geforce GTX 1060 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nEstimator model fails if passed a tf.contrib.distribute.MirroredStrategy as a parameter to the RunConfig object. \r\n\r\n**Describe the expected behavior**\r\n\r\nI am trying to parallelize an Estimator model built from keras model via tf.keras.estimator.model_to_estimator, which works fine unless I pass the train_distribute parameter (of RunConfig) a MirroredStrategy when creating the model. See below, the working RunConfig is commented out, the non-working one not so. My goal is to run the model on the google cloud platform on a couple of GPUs at once, but first I need to get the code running locally. I want to use the mirrored strategy to parallelize the training by splitting the batches across the GPUs.\r\nThe input is a tfrecord file with greyscale images of 40x40x1 and 3-class labels. I am intending to use a CNN and RNN on this data but distributed doesn't work with this simple example of a MLP. Any help?\r\n\r\n**Code to reproduce the issue**\r\n\r\n`\r\n\r\n    import tensorflow as tf\r\n    from tensorflow import keras as K\r\n    import argparse\r\n    import random\r\n    \r\n    \r\n    def _parse_function(proto):\r\n    \r\n        # define constants for images\r\n        imsize = 40\r\n        num_channels = 1\r\n    \r\n        # define your tfrecord feature keys\r\n        keys_to_features = {'X': tf.FixedLenFeature([imsize, imsize, num_channels], tf.float32),  # image height, image width, num_channels\r\n                            'Y': tf.FixedLenFeature([], tf.int64)}\r\n        # Load one example\r\n        parsed_features = tf.parse_single_example(proto, keys_to_features)\r\n    \r\n        # extract image and label\r\n        image = parsed_features['X']\r\n        label = tf.cast( parsed_features['Y'], tf.int32 )\r\n        label = tf.add( label, tf.constant( 1, tf.int32 ) )   # add 1 to transform interval [-1,1] to categorical interval [0,2]\r\n        label = tf.one_hot( label, depth=3 )                  # one hot encoding\r\n    \r\n        return image, label\r\n    \r\n    \r\n    def create_dataset( filepath, shuffle_buffer, batch_size, n_epochs, random_seed, num_parall_calls ):\r\n    \r\n        dataset = tf.data.TFRecordDataset( filepath )\r\n        # Maps the parser on every filepath in the array. You can set the number of parallel loaders here\r\n        dataset = dataset.map( _parse_function, num_parallel_calls=num_parall_calls )\r\n        # Set the number of datapoints you want to load and shuffle\r\n        dataset = dataset.shuffle( shuffle_buffer, random_seed ).repeat( n_epochs )\r\n        # Set the batchsize\r\n        dataset = dataset.batch( batch_size )\r\n        # prefetch\r\n        dataset.prefetch( batch_size )\r\n    \r\n        return dataset\r\n    \r\n    \r\n    def SimpleModel( in_shape=(40, 40, 1), n_out=3, dropout_rate=0.3 ):\r\n    \r\n        model = K.models.Sequential()\r\n    \r\n        # fully connected layer\r\n        model.add( K.layers.Flatten( input_shape=in_shape ) )\r\n    \r\n        model.add( K.layers.Dense( 1024, activation='tanh', kernel_regularizer=K.regularizers.l2( 0.01 ) ) )\r\n        model.add( K.layers.Dropout( dropout_rate ) )\r\n    \r\n        model.add( K.layers.Dense( 128, activation='tanh', kernel_regularizer=K.regularizers.l2( 0.01 ) ) )\r\n        model.add( K.layers.Dropout( dropout_rate ) )\r\n    \r\n        # in the end add another dense layer and an output layer\r\n        model.add( K.layers.Dense( 32, activation='tanh', kernel_regularizer=K.regularizers.l2( 0.01 ) ) )\r\n        model.add( K.layers.Dropout( dropout_rate ) )\r\n        model.add( K.layers.Dense( n_out, activation='softmax' ) )\r\n    \r\n        return model\r\n    \r\n    \r\n    if __name__ == \"__main__\":\r\n    \r\n        # get parameters\r\n        parser = argparse.ArgumentParser( description='Keras GC example NN test.' )\r\n        parser.add_argument( '--job-dir', type=str, help='GCS location to write checkpoints and export models' )\r\n        parser.add_argument( '--train_file', type=str, help='GCS location of train data .tfrecord file location.' )\r\n        parser.add_argument( '--vali_file', type=str, help='GCS location validation data .tfrecord file location.' )\r\n        parser.add_argument( '--n_CPU', type=int, help='Number of processes used for reading and parsing the data for model input.' )\r\n        parser.add_argument( '--n_GPU', type=int, help='Number of GPUs used for training the model.' )\r\n        args = parser.parse_args()\r\n    \r\n        # arguments\r\n        n_gpus = args.n_GPU\r\n        num_parallel_processes = args.n_CPU\r\n    \r\n        train_file = args.train_file\r\n        vali_file = args.vali_file\r\n    \r\n        # parameters\r\n        dropout = 0.5\r\n        num_classes = 3  # -1, 0, 1\r\n        image_size = 40\r\n        num_channel = 1\r\n        learning_rate = 0.0001\r\n        epochs = 3\r\n        shuffle_buffer = 50000      # number of samples from which it will sample\r\n        batch_size = 100\r\n        input_size = (image_size, image_size, num_channel)\r\n        checkpoint_steps = 2000\r\n        rseed = int( random.random() * (2 ** 16) )\r\n    \r\n        # number of samples\r\n        num_sets = 6310\r\n        num_sets_vali = 550\r\n        set_length = 1000\r\n        num_samples = num_sets * set_length\r\n        steps_per_epoch = num_samples // batch_size\r\n        num_samples_vali = num_sets_vali * set_length\r\n        steps_per_epoch_vali = num_samples_vali // batch_size\r\n    \r\n        # assemble the model\r\n        train_model = SimpleModel( in_shape=input_size, n_out=num_classes, dropout_rate=dropout )\r\n        optim = tf.train.AdamOptimizer( learning_rate=learning_rate )\r\n        train_model.compile( optimizer=optim,\r\n                             loss='categorical_crossentropy',\r\n                             metrics=['accuracy'] )\r\n    \r\n        tf.logging.set_verbosity( tf.logging.INFO )\r\n        strategy = tf.contrib.distribute.MirroredStrategy( num_gpus=n_gpus )\r\n        runconfig = tf.estimator.RunConfig( model_dir=args.job_dir, save_checkpoints_steps=checkpoint_steps, train_distribute=strategy )\r\n        # runconfig = tf.estimator.RunConfig( model_dir=args.job_dir, save_checkpoints_steps=checkpoint_steps )\r\n    \r\n        # transform model to estimator\r\n        est_train_model = tf.keras.estimator.model_to_estimator( keras_model=train_model, config=runconfig )\r\n    \r\n        train_spec = tf.estimator.TrainSpec( input_fn=lambda: create_dataset( train_file,\r\n                                                                              shuffle_buffer,\r\n                                                                              batch_size,\r\n                                                                              epochs,\r\n                                                                              rseed,\r\n                                                                              num_parallel_processes ),\r\n                                             max_steps=epochs * steps_per_epoch )\r\n    \r\n        eval_spec = tf.estimator.EvalSpec( input_fn=lambda: create_dataset( vali_file,\r\n                                                                            shuffle_buffer,\r\n                                                                            batch_size,\r\n                                                                            epochs,\r\n                                                                            rseed,\r\n                                                                            num_parallel_processes ),\r\n                                           steps=steps_per_epoch_vali,\r\n                                           throttle_secs=10 )\r\n    \r\n        tf.estimator.train_and_evaluate( est_train_model, train_spec, eval_spec )\r\n    \r\n\r\n`\r\n\r\n**Other info / logs**\r\n\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\n  INFO:tensorflow:Not using Distribute Coordinator.\r\n  INFO:tensorflow:Using the Keras model provided.\r\n  2019-02-28 13:39:21.467595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n  2019-02-28 13:39:21.467913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\n  name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7715\r\n  pciBusID: 0000:01:00.0\r\n  totalMemory: 5.93GiB freeMemory: 5.37GiB\r\n  2019-02-28 13:39:21.467921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n  2019-02-28 13:39:21.621140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n  2019-02-28 13:39:21.621157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n  2019-02-28 13:39:21.621160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n  2019-02-28 13:39:21.621270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5139 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n  INFO:tensorflow:Using config: {'_model_dir': 'output/try1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 2000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\r\n  graph_options {\r\n    rewrite_options {\r\n      meta_optimizer_iterations: ONE\r\n    }\r\n  }\r\n  , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f970c25bba8>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f970c25bcc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\r\n  INFO:tensorflow:Not using Distribute Coordinator.\r\n  INFO:tensorflow:Running training and evaluation locally (non-distributed).\r\n  INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 2000 or save_checkpoints_secs None.\r\n  2019-02-28 13:39:21.625686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n  2019-02-28 13:39:21.625718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n  2019-02-28 13:39:21.625721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n  2019-02-28 13:39:21.625724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n  2019-02-28 13:39:21.625814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 5139 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n  INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\r\n  INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\n  INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0\r\n  INFO:tensorflow:Configured nccl all-reduce.\r\n  2019-02-28 13:39:21.651311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n  2019-02-28 13:39:21.651333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n  2019-02-28 13:39:21.651337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n  2019-02-28 13:39:21.651339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n  2019-02-28 13:39:21.651435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5139 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n  INFO:tensorflow:Calling model_fn.\r\n  INFO:tensorflow:Error reported to Coordinator: \r\n  Traceback (most recent call last):\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n      yield\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 795, in run\r\n      self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n      model_fn_results = self._model_fn(features=features, **kwargs)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/keras.py\", line 278, in model_fn\r\n      labels)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/keras.py\", line 201, in _clone_and_build_model\r\n      optimizer_iterations=global_step)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/models.py\", line 476, in clone_and_build_model\r\n      target_tensors=target_tensors)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py\", line 474, in _method_wrapper\r\n      method(self, *args, **kwargs)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 634, in compile\r\n      for loss_tensor in self.losses:\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 667, in losses\r\n      losses = self._unfiltered_losses\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 571, in _unfiltered_losses\r\n      losses += layer.losses\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 377, in losses\r\n      loss_tensor = regularizer()\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 434, in _tag_unconditional\r\n      loss = loss()\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 629, in _loss_for_variable\r\n      with ops.colocate_with(v):\r\n    File \"/usr/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n      return next(self.gen)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4094, in _colocate_with_for_gradient\r\n      with self.colocate_with(op, ignore_existing):\r\n    File \"/usr/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n      return next(self.gen)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4146, in colocate_with\r\n      op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1307, in internal_convert_to_tensor_or_indexed_slices\r\n      value, dtype=dtype, name=name, as_ref=as_ref)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1146, in internal_convert_to_tensor\r\n      ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py\", line 439, in _tensor_conversion_mirrored\r\n      assert not as_ref\r\n  AssertionError\r\n  Traceback (most recent call last):\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\r\n      exec(code_obj, self.user_global_ns, self.user_ns)\r\n    File \"<ipython-input-2-da26aa895fe6>\", line 1, in <module>\r\n      runfile('/home/aibox/Documents/ast-research-dev/R5/R5_nn_emotional_valence_liquid_data/gcloud_trainer/mirroredTest.py', args='--job-dir=output/try1 --train_file=/mnt/DATA/R5_DATA_AI/R5_liquid_tfrecord/R5_6310x1000x40x40_train_data.tfrecords --vali_file=/mnt/DATA/R5_DATA_AI/R5_liquid_tfrecord/R5_550x1000x40x40_vali_data.tfrecords --n_CPU=6 --n_GPU=1', wdir='/home/aibox/Documents/ast-research-dev/R5/R5_nn_emotional_valence_liquid_data/gcloud_trainer')\r\n    File \"/opt/pycharm-community-2018.3.1/helpers/pydev/_pydev_bundle/pydev_umd.py\", line 198, in runfile\r\n      pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n    File \"/opt/pycharm-community-2018.3.1/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n      exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n    File \"/home/aibox/Documents/ast-research-dev/R5/R5_nn_emotional_valence_liquid_data/gcloud_trainer/mirroredTest.py\", line 138, in <module>\r\n      tf.estimator.train_and_evaluate( est_train_model, train_spec, eval_spec )\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 471, in train_and_evaluate\r\n      return executor.run()\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 610, in run\r\n      return self.run_local()\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 711, in run_local\r\n      saving_listeners=saving_listeners)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n      loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1205, in _train_model\r\n      return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1316, in _train_model_distributed\r\n      self.config)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/training/distribute.py\", line 721, in call_for_each_tower\r\n      return self._call_for_each_tower(fn, *args, **kwargs)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 556, in _call_for_each_tower\r\n      return _call_for_each_tower(self, fn, *args, **kwargs)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 183, in _call_for_each_tower\r\n      coord.join(threads)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n      six.reraise(*self._exc_info_to_raise)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n      raise value\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n      yield\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 795, in run\r\n      self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n      model_fn_results = self._model_fn(features=features, **kwargs)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/keras.py\", line 278, in model_fn\r\n      labels)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/keras.py\", line 201, in _clone_and_build_model\r\n      optimizer_iterations=global_step)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/models.py\", line 476, in clone_and_build_model\r\n      target_tensors=target_tensors)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py\", line 474, in _method_wrapper\r\n      method(self, *args, **kwargs)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 634, in compile\r\n      for loss_tensor in self.losses:\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 667, in losses\r\n      losses = self._unfiltered_losses\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 571, in _unfiltered_losses\r\n      losses += layer.losses\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 377, in losses\r\n      loss_tensor = regularizer()\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 434, in _tag_unconditional\r\n      loss = loss()\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 629, in _loss_for_variable\r\n      with ops.colocate_with(v):\r\n    File \"/usr/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n      return next(self.gen)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4094, in _colocate_with_for_gradient\r\n      with self.colocate_with(op, ignore_existing):\r\n    File \"/usr/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n      return next(self.gen)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4146, in colocate_with\r\n      op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1307, in internal_convert_to_tensor_or_indexed_slices\r\n      value, dtype=dtype, name=name, as_ref=as_ref)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1146, in internal_convert_to_tensor\r\n      ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    File \"/home/aibox/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py\", line 439, in _tensor_conversion_mirrored\r\n      assert not as_ref\r\n  AssertionError\r\n", "comments": ["Is this still an issue in 1.13?  Could you please check?", "Also, could you please try https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It worked for some reason on another machine and I did not look further into why it did not work at first. I did not try it again in tf 1.13.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26207\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26207\">No</a>\n", "The tf 1.13 still has this problem..."]}, {"number": 26206, "title": "Converting a saved_model to tflite using TensorFlow(Built from source) gives error.", "body": "**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 16.04\r\n- TensorFlow installed from : source\r\n- TensorFlow version : 1.12\r\n- Python version: 3.6\r\n- Bazel version : 0.19.1\r\n- GCC/Compiler version : 5.4.0\r\n\r\n**Describe the current behavior**\r\nConverting a saved_model to tflite using tflite_convert.py gives me the following error. But when I convert the same model using Tensorflow binary I do not find the issue.\r\n\r\n**Other info / logs**\r\n` tflite_convert --output_file=./model/lite.tflite --saved_model_dir=./model/saved_model --input_shapes=1 --input_arrays=X_pred --output_arrays=Y_pred\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/vinay/venv/tensorflow_src_1.12/bin/tflite_convert\", line 6, in <module>\r\n    from tensorflow.contrib.lite.python.tflite_convert import main\r\n  File \"/home/vinay/venv/tensorflow_src_1.12/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 48, in <module>\r\n    from tensorflow.contrib import distribute\r\n  File \"/home/vinay/venv/tensorflow_src_1.12/lib/python3.6/site-packages/tensorflow/contrib/distribute/__init__.py\", line 34, in <module>\r\n    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\r\n  File \"/home/vinay/venv/tensorflow_src_1.12/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py\", line 27, in <module>\r\n    from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n  File \"/home/vinay/venv/tensorflow_src_1.12/lib/python3.6/site-packages/tensorflow/contrib/tpu/__init__.py\", line 69, in <module>\r\n    from tensorflow.contrib.tpu.python.ops.tpu_ops import *\r\n  File \"/home/vinay/venv/tensorflow_src_1.12/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/ops/tpu_ops.py\", line 39, in <module>\r\n    resource_loader.get_path_to_datafile(\"_tpu_ops.so\"))\r\n  File \"/home/vinay/venv/tensorflow_src_1.12/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/home/vinay/venv/tensorflow_src_1.12/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 60, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\n**tensorflow.python.framework.errors_impl.InvalidArgumentError: Unrecognized type '**\r\nAn op that loads optimization parameters into HBM for embedding. Must be\r\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\r\nembedding table configuration. For example, this op is used to install\r\nparameters that are loaded from a checkpoint before a training loop is\r\nexecuted.\r\n\r\nparameters: A tensor containing the initial embedding table parameters to use in embedding\r\nlookups using the Adagrad optimization algorithm.\r\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\r\nlookups using the Adagrad optimization algorithm.\r\ntable_name: Name of this table; must match a name in the\r\n  TPUEmbeddingConfiguration proto (overrides table_id).\r\nnum_shards: Number of shards into which the embedding tables are divided.\r\nshard_id: Identifier of shard for this operation.\r\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\r\n  (deprecated).\r\n' in attr '\r\nAn op that loads optimization parameters into HBM for embedding. Must be\r\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\r\nembedding table configuration. For example, this op is used to install\r\nparameters that are loaded from a checkpoint before a training loop is\r\nexecuted.\r\n\r\nparameters: A tensor containing the initial embedding table parameters to use in embedding\r\nlookups using the Adagrad optimization algorithm.\r\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\r\nlookups using the Adagrad optimization algorithm.\r\ntable_name: Name of this table; must match a name in the\r\n  TPUEmbeddingConfiguration proto (overrides table_id).\r\nnum_shards: Number of shards into which the embedding tables are divided.\r\nshard_id: Identifier of shard for this operation.\r\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\r\n  (deprecated).\r\n'; in OpDef: name: \"LoadTPUEmbeddingAdagradParameters\" input_arg { name: \"parameters\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: DT_FLOAT type_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" number_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type_list_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } input_arg { name: \"accumulators\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: DT_FLOAT type_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" number_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type_list_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" default_value { i: -1 } description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" has_minimum: true minimum: -1 } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" default_value { s: \"\" } description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } summary: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" is_stateful: true`\r\n\r\n\r\n\r\nThanks in advance.!", "comments": ["@vinayonchip Could you check whether the bug persists with new version of TF? Thanks! ", "Hi @jvishnuvardhan \r\nWhen I run the same tflite conversion using **Tensorflow1.12 (binary)** the conversion ends with an error unsupported ops(as expected). \r\nThe same version of **Tensorflow1.12 built from source** should also give me same error (unsupported ops) but instead it throws a different error. Why?", "> @vinayonchip Could you check whether the bug persists with new version of TF? Thanks!\r\n\r\nI will check", "> > @vinayonchip Could you check whether the bug persists with new version of TF? Thanks!\r\n> \r\n> I will check\r\n\r\nUsing TensorFlow 1.13 gives a different error:\r\n\r\ntflite_convert \\\r\n>   --output_file=./model/lite.tflite \\\r\n>   --saved_model_dir=./model/saved_model/1551349080 \\\r\n>   --input_shapes=1,100 \\\r\n>   --input_arrays=input_data \\\r\n>   --output_arrays=output_data\r\nWARNING:tensorflow:From /home/vinay/venv/tensorflow_src_1.13/lib/python3.6/site-packages/tensorflow/lite/python/convert_saved_model.py:61: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nWARNING:tensorflow:From /home/vinay/venv/tensorflow_src_1.13/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nWARNING:tensorflow:From /home/vinay/venv/tensorflow_src_1.13/lib/python3.6/site-packages/tensorflow/lite/python/convert_saved_model.py:275: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.convert_variables_to_constants\r\nWARNING:tensorflow:From /home/vinay/venv/tensorflow_src_1.13/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.extract_sub_graph\r\nTraceback (most recent call last):\r\nFile \"/home/vinay/venv/tensorflow_src_1.13/bin/tflite_convert\", line 10, in \r\nsys.exit(main())\r\nFile \"/home/vinay/venv/tensorflow_src_1.13/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 442, in main\r\napp.run(main=run_main, argv=sys.argv[:1])\r\nFile \"/home/vinay/venv/tensorflow_src_1.13/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n_sys.exit(main(argv))\r\nFile \"/home/vinay/venv/tensorflow_src_1.13/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 438, in run_main\r\n_convert_model(tflite_flags)\r\nFile \"/home/vinay/venv/tensorflow_src_1.13/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 191, in _convert_model\r\noutput_data = converter.convert()\r\nFile \"/home/vinay/venv/tensorflow_src_1.13/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 455, in convert\r\n**converter_kwargs)\r\nFile \"/home/vinay/venv/tensorflow_src_1.13/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 442, in toco_convert_impl\r\ninput_data.SerializeToString())\r\nFile \"/home/vinay/venv/tensorflow_src_1.13/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos\r\n\"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-03-01 14:44:33.076916: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1040 operators, 1653 arrays (0 quantized)\r\n2019-03-01 14:44:33.100006: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 1037 operators, 1644 arrays (0 quantized)\r\n2019-03-01 14:44:33.127998: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1037 operators, 1644 arrays (0 quantized)\r\n2019-03-01 14:44:33.138969: F tensorflow/lite/toco/graph_transformations/unroll_batch_matmul.cc:75] Check failed: input_array_a.shape().dimensions_count() == 3 (4 vs. 3)Input arrays must have rank 3\r\nAborted (core dumped)", "I am also getting the below errors while trying to convert to tflite using the saved_model.\r\n\r\nconverter=tf.contrib.lite.TocoConverter.from_saved_model(saved_model_dir)\r\nconverter.post_training_quantize=True\r\ntflite_quantized_model=converter.convert()\r\nopen(\u201cquantized_model.tflite\u201d, \u201cwb\u201d).write(tflite_quantized_model)\r\n\r\n\r\nRuntimeError: MetaGraphDef associated with tags {'serve'} could not be found in SavedModel. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`\r\n\r\nOSError: SavedModel file does not exist at: _models/{saved_model.pbtxt|saved_model.pb}\r\n\r\nOSError: SavedModel file does not exist at: _models/model-mobilenet_v1_075.pbtxt/{saved_model.pbtxt|saved_model.pb}\r\n\r\nPlease let me know.\r\n\r\nBest Regards\r\nSaraswathy.\r\n", "Can you attach the SavedModel you are converting or a minimal model that reproduces the same errors as you listed above?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26206\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26206\">No</a>\n"]}, {"number": 26205, "title": "How to run the examples/android in Windows Environment?", "body": "**System information**\r\nWindows 8 cpu\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:1.12.0\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):no\r\n\r\n\r\n**Describe the problem**\r\nI have searched a huge amounts of information about how to use this example but the problem is that most of people are using it on the linux OS. I don't know how to run it on my Windows OS with android studio. Need I install .jar & .so file?And how to deploy the android studio?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "@Ekinkit Hi! I have mentioned some links below. Can you please take a look at these.\r\n\r\nTFLite code Repository : https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite\r\nTFLite Website Guide : https://www.tensorflow.org/lite/overview\r\nTFLite Github Guide : https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/android.md\r\n\r\nAs stated by @jvishnuvardhan , try posting this question at Stackoverflow and close this issue. Thank you \ud83d\ude04 ", "@Ayush517 Thank you for your reply~"]}, {"number": 26204, "title": "[TF2.0] Broadcasting", "body": "Hello everyone,\r\n\r\nI would like resuscitate very old issue. Actually, it is so old that even github's autocompletion doesn't offer it after typing \"#\" - https://github.com/tensorflow/tensorflow/issues/216. This request was raised several times, but still it hasn't been resolved.\r\n\r\nIn short, *broadcasting* interface is not \"good enough\" in TensorFlow :)\r\n\r\nLets first check how broadcasting works in `numpy`:\r\n\r\n```python\r\nIn [1]: import numpy as np\r\nIn [2]: a = np.random.rand(2, 3, 4)\r\nIn [3]: b = np.random.rand(4, 5)\r\nIn [4]: a @ b\r\nOut[4]:\r\narray([[[1.42709275, 1.40630067, 0.46525725, 0.68734581, 0.65227036],\r\n        [2.01336504, 1.59980866, 0.93739699, 0.63190484, 0.92472892],\r\n        [1.82979902, 1.46193243, 0.85498406, 0.5994646 , 0.77767957]],\r\n\r\n       [[1.83010035, 1.49088728, 0.76694665, 0.65568003, 0.89110954],\r\n        [2.12214864, 1.41728107, 1.04566743, 0.60652825, 0.97115822],\r\n        [2.32478779, 2.06297214, 1.02016205, 0.81821249, 1.02604722]]])\r\n```\r\n\r\nNow, let's check what the TF is offering:\r\n\r\n```python\r\nIn [25]: a = tf.random.normal((2, 3, 4))\r\nIn [26]: b = tf.random.normal((4, 5))\r\nIn [27]: a @ b\r\n... InvalidArgumentError: In[0] is not a matrix. Instead it has shape [2,3,4] [Op:MatMul] name: matmul/\r\n```\r\n\r\nOuch! The \"correct\" way of doing it in the TF (of course there are other) is:\r\n\r\n```python\r\nIn [26]: a = tf.random.normal((2, 3, 4))\r\nIn [27]: b = tf.random.normal((4, 5))\r\nIn [28]: a @ tf.broadcast_to(b, tf.concat([a.shape[:-2], b.shape], axis=0))\r\n<tf.Tensor: id=87, shape=(2, 3, 5), dtype=float32, numpy=\r\narray([[[ 1.1977772 , -1.363074  ,  1.8021748 ,  0.1448586 , -0.6269997 ],\r\n        [ 1.2322128 , -2.1586194 ,  0.09486479,  0.02937585, 0.9694344 ],\r\n        [ 0.5580032 ,  6.11664   , -0.24535722,  0.16691092, -2.2263217 ]],\r\n       [[-0.7386743 ,  1.2142425 ,  1.1371945 , -1.2736351 , -2.971829  ],\r\n        [-1.9222848 , -0.7198772 , -0.9807504 ,  0.02805561, 1.0210879 ],\r\n        [ 1.8334148 ,  0.80895233,  1.2308785 , -0.23910654, -1.5128168 ]]], dtype=float32)>\r\n```\r\n\r\nYou can see how much effort it requires to make operations broadcastable for two distinct tensors: extract leading shape from the left tensor, extract shape from the right tensors, concatenate these shapes with correct axis, call `tf.broadcast_to`...\r\n\r\nThe same applies to cholesky, triangular solve and other operations. That is very upsetting that such a crucial feature isn't available out of the box.\r\n\r\nAnother concern is the performance of these \"solutions\". E.g. memory consumption for tiling and `broadcast_to` operations, as they simply copy the tensor to match leading dimensions. Of course, native TensorFlow broadcasting implementation would be preferable in this case.\r\n\r\nKind regards,\r\nArtem Artemev\r\n\r\n\r\n", "comments": ["@rmlarsen , @jvishnuvardhan ping", "@awav thankfully when we do add support for this this it will not be an API removal; it just takes code which currently does not work and make it work.\r\n\r\nSo I don't think we need to block the tf2.0 release on fixing this, though it'd be very nice.\r\n\r\n@rmlarsen now that we have a proper batch_matmul can we make it broadcast correctly on leading indices? I feel like this shouldn't be hard but I'm not familiar enough with eigen intricacies to make this happen.", "This should be now fixed! (pending 3 week compatibility window) \r\n[47ab68d](http://go/gh/tensorflow/tensorflow/commit/47ab68d265a96b6e7be06afd1b4b47e0114c0ee9)", "@bloops , That is fantastic news. Will `broadcasting` be fixed for all linear algebra operations or only for `matmul`?", "Only for matmul. What else do you have in mind? \r\nWe are considering adding broadcasting support for matrix solve as well.", "@bloops, ideally, all linear algebra operations, as far as I know some of them already have broadcasting support. The `triangular_solve` is one important missing bit.", "Hello @bloops , any updates on `triangular_solve`?", "Sorry, I am not working on it currently.", "@bloops , does anyone else is going to work on that? Do you know what should be changed in the code to make it work?\r\n", "@bloops , ping? :)", "One year has passed!!! Thanks to @srvasude.", "Closing since triangular solve now has broadcasting."]}, {"number": 26203, "title": "tf.graph_util.convert_variables_to_constants converts a different  pb parameters with orginal ckpt", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n**The result using 'tf.graph_util.convert_variables_to_constants' api is different with 'freeze_graph.freeze_graph'**\r\nwhen i use 'freeze_graph.freeze_graph',  the result is same as ckpt\uff0c but different with the output when i use use 'tf.graph_util.convert_variables_to_constants' api. Why???\r\n\r\n```python\r\n# using 'tf.graph_util.convert_variables_to_constants' api\r\n    with tf.get_default_graph().as_default():\r\n        input_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name = 'input_images')\r\n        \r\n\r\n        input_images = readdata.mean_image_subtraction(input_images)\r\n\r\n        with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=1e-5)):\r\n            outputs, _ = resnet_v1.resnet_v1_50(input_images,  is_training=False, scope='resnet_v1_50', num_classes=FLAGS.num_classes)\r\n            saver = tf.train.Saver()\r\n\r\n            with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\r\n                tf.train.write_graph(sess.graph.as_graph_def(), '.', 'resnet50.pbtxt', as_text=False)\r\n                saver.restore(sess, '../model.ckpt-5723')\r\n\r\n                pb_file_path = 'model_5723.pb'\r\n                constant_graph = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,[\"resnet_v1_50/predictions/Softmax\"])\r\n\r\n                with tf.gfile.FastGFile(pb_file_path, mode='wb') as f:\r\n                    f.write(constant_graph.SerializeToString())\r\n```\r\n```python\r\n#using 'freeze_graph' api\r\n    with tf.get_default_graph().as_default():\r\n        input_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name = 'input_images')\r\n        \r\n\r\n        input_images = readdata.mean_image_subtraction(input_images)\r\n\r\n        with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=1e-5)):\r\n            outputs, _ = resnet_v1.resnet_v1_50(input_images,  is_training=False, scope='resnet_v1_50', num_classes=FLAGS.num_classes)\r\n\r\n            saver = tf.train.Saver()\r\n\r\n            with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\r\n                tf.train.write_graph(sess.graph.as_graph_def(), '.', 'resnet50.pbtxt', as_text=False)\r\n\r\n                freeze_graph.freeze_graph('./resnet50.pbtxt', \"\", True, '../model.ckpt-5723', \"resnet_v1_50/predictions/Softmax\", \"save/restore_all\", \"save/Const:0\",\"model_freeze.pb\", True, \"\")\r\n                \r\n```\r\n**Describe the expected behavior**\r\nusing 'tf.graph_util.convert_variables_to_constants' api , the output parameters should be same as ckpt.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@petewarden do you know anything about graph freezing functionality?\r\n\r\nUnfortunately I think tf.graph_util.convert_variables_to_constants is deprecated, so I'm not sure if we have the bandwidth to address this.", "> @petewarden do you know anything about graph freezing functionality?\r\n> \r\n> Unfortunately I think tf.graph_util.convert_variables_to_constants is deprecated, so I'm not sure if we have the bandwidth to address this.\r\n\r\noh, I see. I found a warning in [tensorflow api docs](https://www.tensorflow.org/api_docs/python/tf/graph_util/convert_variables_to_constants). But I am using old version(1.4.0), there is no api named \"tf.compat.v1.graph_util.convert_variables_to_constants\". So which api can I use for converting ckpt to pb\uff1f\"freeze_graph.freeze_graph\" api\uff1f", "cc @gargn ", "`freeze_graph.freeze_graph` calls `tf.graph_util.convert_variables_to_constants` it is likely a problem with how you are loading your checkpoints with the Saver. I would suggest looking at how `freeze_graph.freeze_graph` restores the checkpoint files in order to see what is going wrong.\r\n\r\nLoading `pbtxt` + checkpoints file can look something more like the following:\r\n```\r\nimport os\r\nimport google3\r\nimport tensorflow as tf\r\nfrom google.protobuf import text_format\r\n\r\ndef load_graph_in_session(pbtxt_file, checkpoint_file, sess):\r\n  \"\"\"Loads the graph into the provided session.\r\n\r\n  Args:\r\n    pbtxt_file: Text based graph.\r\n    checkpoint_file: Checkpoints file.\r\n    sess: tf.Session to load the graph into.\r\n  \"\"\"\r\n  # Load pbtxt into GraphDef.\r\n  graph_def = tf.GraphDef()\r\n  with tf.gfile.FastGFile(pbtxt_file, 'r') as f:\r\n    text_format.Merge(f.read(), graph_def)\r\n  _ = tf.import_graph_def(graph_def, name='')\r\n\r\n  # Restore variables from checkpoint file.\r\n  var_list = {}\r\n  reader = tf.train.NewCheckpointReader(checkpoint_file)\r\n  var_to_shape_map = reader.get_variable_to_shape_map()\r\n\r\n  for key in var_to_shape_map:\r\n    try:\r\n      tensor = sess.graph.get_tensor_by_name(key + ':0')\r\n    except KeyError:\r\n      # This tensor doesn't exist in the graph (for example it's\r\n      # 'global_step' or a similar housekeeping element) so skip it.\r\n      continue\r\n    var_list[key] = tensor\r\n\r\n  checkpoint_version = tf.train.SaverDef.V2\r\n  saver = tf.train.Saver(var_list=var_list, write_version=checkpoint_version)\r\n  saver.restore(sess, checkpoint_file)\r\n```\r\n\r\nIf this doesn't work, can you provide the model files (pbtxt + checkpoint files that you are using), or the minimal model files required to reproduce your error as well as a minimal script to reproduce the error.", "I can't reproduce this issue now. Maybe it's my mistake. sorry for bothering you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26203\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26203\">No</a>\n"]}, {"number": 26202, "title": "Cannot run tensorflow against tensorrt", "body": "I'm trying to run tensorflow against tensorrt but looks this always runs with TF.\r\n\"TensorRT node TRTEngineOp_0 added for segment 0 consisting of 446 nodes failed: Internal: Failed to build TensorRT engine. Fallback to TF...\"\r\n\r\n**System information**\r\nI'm trying to run the one example file: models/research/tensorrt/tensorrt.py\r\n\r\n- VERSION=\"18.04.2 LTS (Bionic Beaver)\"\r\n- one x86-based target\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version (use command below):\r\nr1.13 \r\n\r\n- Python version:\r\nPython 2.7.15rc1\r\n\r\n- Bazel version (if compiling from source):\r\nBuild label: 0.21.0\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n\r\nroot@96c10be1cfd1:/# dpkg -l | grep TensorRT\r\nii  graphsurgeon-tf                                             5.0.2-1+cuda10.0                  amd64        GraphSurgeon for TensorRT package\r\nii  libnvinfer-dev                                              5.0.2-1+cuda10.0                  amd64        TensorRT development libraries and headers\r\nii  libnvinfer-samples                                          5.0.2-1+cuda10.0                  all          TensorRT samples and documentation\r\nii  libnvinfer5                                                 5.0.2-1+cuda10.0                  amd64        TensorRT runtime libraries\r\nii  tensorrt                                                    5.0.2.6-1+cuda10.0                amd64        Meta package of TensorRT\r\nii  uff-converter-tf                                            5.0.2-1+cuda10.0                  amd64        UFF converter for TensorRT package\r\n- GPU model and memory:\r\nroot@96c10be1cfd1:/# nvidia-smi     \r\nThu Feb 28 09:59:21 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.43       Driver Version: 418.43       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GT 710      Off  | 00000000:03:00.0 N/A |                  N/A |\r\n| 40%   44C    P0    N/A /  N/A |      0MiB /  2002MiB |     N/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0                    Not Supported                                       |\r\n+-----------------------------------------------------------------------------+\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nroot@f31df1cbe0b4:/home/workspace/models/research/tensorrt# python tensorrt.py --frozen_graph=resnetv2_imagenet_frozen_graph.pb --image_file=image.jpg --int8 --output_dir=./output/               \r\n2019-02-28 09:52:50.839058: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1915900000 Hz\r\n2019-02-28 09:52:50.849051: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55bed0da2f90 executing computations on platform Host. Devices:\r\n2019-02-28 09:52:50.849128: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-02-28 09:52:51.274853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-02-28 09:52:51.276663: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55bece26afb0 executing computations on platform CUDA. Devices:\r\n2019-02-28 09:52:51.276744: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GT 710, Compute Capability 3.5\r\n2019-02-28 09:52:51.277347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce GT 710 major: 3 minor: 5 memoryClockRate(GHz): 0.954\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 1.96GiB freeMemory: 1.93GiB\r\n2019-02-28 09:52:51.277411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-02-28 09:52:51.896007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-28 09:52:51.896085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-02-28 09:52:51.896118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-02-28 09:52:51.897500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1001 MB memory) -> physical GPU (device: 0, name: GeForce GT 710, pci bus id: 0000:03:00.0, compute capability: 3.5)\r\nWARNING:tensorflow:From tensorrt.py:210: __init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.gfile.GFile.\r\nRunning INT8 graph\r\nINFO:tensorflow:Running against TensorRT version 5.0.2\r\n2019-02-28 09:52:56.511405: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 0\r\n2019-02-28 09:52:56.519624: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-02-28 09:52:56.520961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-02-28 09:52:56.521047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-28 09:52:56.521081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-02-28 09:52:56.521113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-02-28 09:52:56.521594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1001 MB memory) -> physical GPU (device: 0, name: GeForce GT 710, pci bus id: 0000:03:00.0, compute capability: 3.5)\r\n2019-02-28 09:53:00.177209: I tensorflow/contrib/tensorrt/segment/segment.cc:443] There are 4 ops of 3 different types in the graph that are not converted to TensorRT: Softmax, NoOp, Placeholder, (For more information see https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html#support-ops).\r\n2019-02-28 09:53:00.312249: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:913] Number of TensorRT candidate segments: 1\r\n2019-02-28 09:53:16.314935: W tensorflow/core/common_runtime/bfc_allocator.cc:267] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.00GiB.  Current allocation summary follows.\r\n2019-02-28 09:53:16.315102: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (256):   Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.315139: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (512):   Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.315302: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1024):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.315345: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.315386: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4096):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.315421: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.315461: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16384):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.315496: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (32768):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.315531: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (65536):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.315572: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (131072):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.315647: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (262144):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.315809: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (524288):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.315962: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1048576):       Total Chunks: 1, Chunks in use: 0. 1.00MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.316172: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2097152):       Total Chunks: 1, Chunks in use: 0. 2.00MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.316368: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4194304):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.316528: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8388608):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.316677: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16777216):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.316758: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (33554432):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.316794: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (67108864):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.316883: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (134217728):     Total Chunks: 2, Chunks in use: 2. 256.00MiB allocated for chunks. 256.00MiB in use in bin. 147.00MiB client-requested in use in bin.\r\n2019-02-28 09:53:16.317046: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (268435456):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-02-28 09:53:16.317088: I tensorflow/core/common_runtime/bfc_allocator.cc:613] Bin for 2.00GiB was 256.00MiB, Chunk State: \r\n2019-02-28 09:53:16.317155: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x400de0000 of size 1048576\r\n2019-02-28 09:53:16.317186: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x400fe0000 of size 2097152\r\n2019-02-28 09:53:16.317217: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x401600000 of size 134217728\r\n2019-02-28 09:53:16.317248: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x409600000 of size 134217728\r\n2019-02-28 09:53:16.317282: I tensorflow/core/common_runtime/bfc_allocator.cc:638]      Summary of in-use Chunks by size: \r\n2019-02-28 09:53:16.317339: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 134217728 totalling 256.00MiB\r\n2019-02-28 09:53:16.317379: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Sum Total of in-use chunks: 256.00MiB\r\n2019-02-28 09:53:16.317418: I tensorflow/core/common_runtime/bfc_allocator.cc:647] Stats: \r\nLimit:                  1049657344\r\nInUse:                   268435456\r\nMaxInUse:                268435456\r\nNumAllocs:                      15\r\nMaxAllocSize:            134217728\r\n\r\n2019-02-28 09:53:16.317461: W tensorflow/core/common_runtime/bfc_allocator.cc:271] _*****************************xxxxxxxxxxxxxxxxxxxx*****************************xxxxxxxxxxxxxxxxxxxxx\r\n2019-02-28 09:53:16.317514: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger ./resources.h (129) - OutOfMemory Error in GpuMemory: 0\r\n2019-02-28 09:53:16.330037: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger GPU memory allocation failed during tactic selection for layer: (Unnamed Layer* 0) [Shuffle]\r\n2019-02-28 09:53:16.334382: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger ./resources.h (129) - OutOfMemory Error in GpuMemory: 0\r\n2019-02-28 09:53:16.335317: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:1021] TensorRT node TRTEngineOp_0 added for segment 0 consisting of 446 nodes failed: Internal: Failed to build TensorRT engine. Fallback to TF...\r\n2019-02-28 09:53:16.503925: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] Optimization results for grappler item: tf_graph\r\n2019-02-28 09:53:16.504025: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   constant folding: Graph size after: 451 nodes (-253), 466 edges (-253), time = 1914.823ms.\r\n2019-02-28 09:53:16.504057: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   layout: Graph size after: 451 nodes (0), 466 edges (0), time = 210.214ms.\r\n2019-02-28 09:53:16.504087: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   constant folding: Graph size after: 451 nodes (0), 466 edges (0), time = 304.044ms.\r\n....\r\n\r\n**Describe the expected behavior**\r\nHope this can work.\r\n\r\n**Code to reproduce the issue**\r\nhttps://github.com/tensorflow/models/tree/master/research/tensorrt and then I just run that example file:\r\npython tensorrt.py --frozen_graph=resnetv2_imagenet_frozen_graph.pb \\\r\n  --image_file=image.jpg --native --fp32 --fp16 --int8 --output_dir=./output\r\n\r\nEven I only run that with --int8\r\npython tensorrt.py --frozen_graph=resnetv2_imagenet_frozen_graph.pb \\\r\n  --image_file=image.jpg --int8 --output_dir=./output\r\n\r\n", "comments": ["@trevor-m @pooyadavoodi  Can you PTAL", "Looks like TF is running out of memory.\r\n\r\nThe GPU that's being used has only 2GB.\r\n\r\nThe default workspace size used by https://github.com/tensorflow/models/blob/master/research/tensorrt/tensorrt.py is 1GB.\r\nCould you decrease that and retry?\r\n", "@pooyadavoodi You're right. I tied out two weeks ago but forgot to close this but really appreciate your reply.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26202\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26202\">No</a>\n"]}, {"number": 26201, "title": "The procedure of building the TF r1.13 is not clear. (CUDA10.0 + cuDNN7.5.0+TensorRT5.0.2.6)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: r1.13\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip3\r\n- Bazel version (if compiling from source): 0.19.1\r\n- GCC/Compiler version (if compiling from source): 4.9.3\r\n- CUDA/cuDNN version: 10.0/7.5.0\r\n- GPU model and memory: TitanXP\r\n\r\n**Describe the problem**\r\nThe Tensorflow r1.13 source code is get from [releases](https://github.com/tensorflow/tensorflow/releases).\r\nThen I tried to build tf 1.13 with cuda10.0 and cudnn7.5.0 and TensorRT5.0.2.6.\r\nAfter the configuration, I tried command \r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\nBut get below error:\r\n```\r\n$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=176\r\nERROR: Config value opt is not defined in any .rc file\r\n\r\n```\r\n1) Is bazel 0.19.1 not compatible with TF1.13?\r\n2) Is my configuration incorrect?\r\n3) Is the build command incorrect?\r\n\r\n\r\n\r\nBelow shows my configure:\r\n```\r\n$ ./configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.19.1 installed.\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]: \r\n\r\n\r\nPlease specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /home/web_server/xiaolun/cuda-10.0\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: \r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /home/web_server/xiaolun/cuda-10.0]: \r\n\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the location where TensorRT is installed. [Default is /usr/lib/x86_64-linux-gnu]:/home/web_server/xiaolun/TensorRT-5.0.2.6-10.0\r\n\r\n\r\nPlease specify the locally installed NCCL version you want to use. [Default is to use https://github.com/nvidia/nccl]: 2.3\r\n\r\n\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /home/web_server/xiaolun/cuda-10.0]:\r\n\r\n\r\nNCCL found at /media/disk1/fordata/web_server/project/xiaolun/cuda-10.0/lib64/libnccl.so.2\r\nAssuming NCCL header path is /media/disk1/fordata/web_server/project/xiaolun/cuda-10.0/lib64/../include/nccl.h\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 7.0\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /home/web_server/gcc-4.9.3/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=noignite    \t# Disable Apacha Ignite support.\r\n\t--config=nokafka     \t# Disable Apache Kafka support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n```\r\n", "comments": ["\"opt\" config option is created when you run `./configure` script.\r\nCould you confirm that you first ran `./configure` then ran the `bazel build ` command?", "@gunan Yes, I did. And bazel clean in prior to configuration. I tried again and confirmed the problem. ", "I tried to change --config=opt --config=cuda to -c opt and -c cuda, then get this:\r\n```\r\n$ bazel build -c opt -c cuda //tensorflow/tools/pip_package:build_pip_package\r\nERROR: While parsing option -c cuda: Not a valid compilation mode: 'cuda' (should be fastbuild, dbg or opt)\r\nINFO: Invocation ID: e96eee44-e10f-4c11-bb67-773156435474\r\n```\r\n\r\nThen I removed the --config=cuda, and it started compiling.\r\nSo the new procedure is better to be updated in the releases site, I recommend.", "Then I encountered new error:\r\n```\r\n$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Invocation ID: aa840ba4-7d91-4081-b8d2-bb3b4f0cd601\r\nERROR: /media/disk1/fordata/web_server/project/xiaolun/workshop/tensorflow.gpu.1.13/tensorflow/python/keras/BUILD:18:1: no such package '@keras_applications_archive//': java.io.IOException: Error downloading [http://mirror.bazel.build/github.com/keras-team/keras-applications/archive/1.0.6.tar.gz, https://github.com/keras-team/keras-applications/archive/1.0.6.tar.gz] to /home/web_server/.cache/bazel/_bazel_web_server/91812a5e86890d0149151b2da554d0a8/external/keras_applications_archive/1.0.6.tar.gz: All mirrors are down: [Could not initialize class sun.security.ssl.SSLContextImpl$DefaultSSLContext] and referenced by '//tensorflow/python/keras:keras'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@keras_applications_archive//': java.io.IOException: Error downloading [http://mirror.bazel.build/github.com/keras-team/keras-applications/archive/1.0.6.tar.gz, https://github.com/keras-team/keras-applications/archive/1.0.6.tar.gz] to /home/web_server/.cache/bazel/_bazel_web_server/91812a5e86890d0149151b2da554d0a8/external/keras_applications_archive/1.0.6.tar.gz: All mirrors are down: [Could not initialize class sun.security.ssl.SSLContextImpl$DefaultSSLContext]\r\nINFO: Elapsed time: 2.451s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (127 packages loaded, 5366 targets configured)\r\n    Fetching @keras_applications_archive; fetching\r\n    Fetching @com_google_absl; fetching\r\n    Fetching @grpc; fetching\r\n    Fetching @com_github_googlecloudplatform_google_cloud_cpp; fetching\r\n    Fetching @com_googlesource_code_re2; fetching\r\n    Fetching @eigen_archive; fetching\r\n    Fetching @boringssl; fetching\r\n    Fetching @aws; fetching ... (18 fetches)\r\n```\r\n\r\n------------------\r\nupdate:\r\nAbove failed is encountered when I use bazel 0.20.0.\r\nAfter changing back to 0.19.1, above error disappeared\r\n\r\n-------------------\r\nupdate:\r\nWhen use bazel 0.19.1 and gcc4.9.3, the compiling encountered this error:\r\n`ERROR: /home/web_server/.cache/bazel/_bazel_web_server/91812a5e86890d0149151b2da554d0a8/external/grpc/BUILD:1515:1: C++ compilation of rule '@grpc//:grpc_resolver_dns_ares' failed (Exit 1) gcc failed: error executing command /home/web_server/gcc-4.9.3/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/home/web_server/gcc-4.9.3/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 ... (remaining 65 argument(s) skipped)`\r\n\r\nWhich gcc version shall I use?\r\ngcc5.3 has the same error.", "@oscarriddle We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26201\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26201\">No</a>\n"]}, {"number": 26200, "title": "tf.nn.ctc_beam_search_decoder is very slow on tf >=1.4", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tried 1.4, 1.5, 1.8, 1.9, 1.12\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA8 for 1.4, CUDA9 for >=1.5\r\n- GPU model and memory:P100\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nOur text recognizer is based on CRNN model  and CTC. it was developed based on tf 1.3 and performs well.\r\nrecently we tried to upgrade the tensorflow to tf 1.9 and find the recognizer inference time increases a lot.\r\nwe identified the slow down is caused by tf.nn.ctc_beam_search_decoder op using this way:\r\n\r\nfor this line of code:\r\ndecoded, prob = tf.nn.ctc_beam_search_decoder(logits, seq_length)\r\n\r\nrunning time for session.run(logits) is fine, however session.run(decoded) is very slow\r\n\r\nwe verified the problem occurrs on tf versions 1.12, 1.9, 1.8, 1.5, 1.4\r\nwe also verified the model ckpt trained with tf 1.9 is running inference fast with tf 1.3. similarly, model ckpt trained with tf 1.4/1.5 also running fast with tf 1.3\r\n\r\nit looks there is no big change for beam search decoder op between 1.3 and 1.4. googling the problem got no luck either.\r\n\r\ncan you please take a look?\r\nwe could provide the pb file and a small inference script if needed. since pb file size is big, we do not upload it at this point\r\nthanks.\r\n\r\n**Describe the expected behavior**\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["more info:\r\nthe test for tf 1.12, 1.9, 1.8, 1.5, 1.4 are all in docker env\r\nthe test for tf 1.3 covers docker env and host env", "@ebrevdo could you help take a look?\r\nthanks", "The only PR i can think that may have affected you is [this one](https://github.com/tensorflow/tensorflow/commit/4503f464628d4ba6f01e0e5b2aa9ff829763982b).  You could try building TF before and after this PR and see if it is in fact causing the regression.", "@ebrevdo thanks for response!\r\nthe PR was introduced in tf 1.5? but the problem happens on tf 1.4 already...\r\nis it possible changes outside of ctc would cause the problem?", "@tianq01 Could you check whether the bug persists with TF2.0? Could you also check whether the issue is with docker env only? Thanks!", "thanks @jvishnuvardhan , the issue is not related to docker env.\r\ntf 2.0 might be heavy to try? any other suggestions that could help investigate and narrow down the problem?  \r\n\r\n@ebrevdo @jvishnuvardhan  FYI  timeline gave the same conclusion: decoder op is the bottleneck", "how are you doing @ebrevdo ?\r\nany update? many thanks", "@tianq01 what's your size of the output space? and beam_width? ", "> thanks @jvishnuvardhan , the issue is not related to docker env.\r\n> tf 2.0 might be heavy to try? any other suggestions that could help investigate and narrow down the problem?\r\n> \r\n> @ebrevdo @jvishnuvardhan FYI timeline gave the same conclusion: decoder op is the bottleneck\r\n\r\n@tianq01,\r\nSorry for the delayed response. Can you please give a try with **`Tensorflow Version 1.15.2`**?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 26199, "title": "Tensorflow lite model requests a buffer bigger than the neccesary", "body": "Hi, I created a custom model using keras in tensorflow. The version that I used was tensorflow nightly 1.13.1. I used the official tool to build the tensorflow lite model (the method tf.lite.TFLiteConverter.from_keras_model_file ).\r\n\r\nAfter I created the model I reviewed the input shape and nothing seems is bad.\r\n\r\nThe input and output shapes in tensorflow lite model are:\r\n\r\n[{'name': 'input_1', 'index': 59, 'shape': array([  1, 240, 240,   3], dtype=int32), 'dtype': , 'quantization': (0.0, 0)}]\r\n\r\n[{'name': 'dense/Softmax', 'index': 57, 'shape': array([1, 6], dtype=int32), 'dtype': , 'quantization': (0.0, 0)}]\r\n\r\nyou can note that input shape is 1 * 240 * 240 * 3 so I expected that the buffer would have a size of 172800 units.\r\n\r\nHowever, when I try to run the model in an android device I received the next error:\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.megacode, PID: 15067\r\n    java.lang.RuntimeException: Unable to create application com.megacode.base.ApplicationBase: java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 691200 bytes and a ByteBuffer with 172800 bytes.\r\n        at android.app.ActivityThread.handleBindApplication(ActivityThread.java:5771)\r\n        at android.app.ActivityThread.-wrap2(ActivityThread.java)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1648)\r\nI don't understand the reason why the model request an input shape of 691200 units.\r\n\r\nIf someone has a suggestion I would appreciate it", "comments": ["The input type is int32, which is 4 bytes (4 x 172800 == 691200).", "Did you get any solution for above problem? I am facing same issue.", "In my case I needed to put the size in bytes of int type. So the operation was 4x240x240x3", "Thanks rockbass2560, i tried and in my case it was issue of input size. inputsize 224 works for me. I am not getting error anymore and model run successfully but not getting image mask. How to configure remaining value in the below formula?\r\n\r\n> ByteBuffer.allocateDirect(1 * imageSize * imageSize * NUM_CLASSES * 4)  "]}, {"number": 26198, "title": "Errors while building Tensorflow 1.12", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Void Linux glibc x86_64\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: r1.12\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip, no virtualenv\r\n- Bazel version (if compiling from source): 0.22\r\n- GCC/Compiler version (if compiling from source): 8.2.0\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: None\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nHello guys, I'm trying to build TF in a non Debian based distro. But what I'm getting is this error, as referrenced by [this windows build from source](https://github.com/tensorflow/tensorflow/issues/24721), especially related to eigen.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\nbazel build --incompatible_remove_native_http_archive=false --config=opt --copt=-mavx --copt=-mavx2 --config=monolithic //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Invocation ID: 851c2110-08b2-4ed8-a7b0-4c995b8388e9\r\nDEBUG: /home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/build_bazel_rules_apple/apple/repositories.bzl:35:5: \r\nWARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.\r\n\r\nINFO: Build options --compiler and --cpu have changed, discarding analysis cache.\r\nERROR: /home/neel/Programming_Workspace/Not-My-Repos/tensorflow/third_party/eigen3/BUILD:34:1: no such package '@eigen_archive//': Traceback (most recent call last):\r\n\tFile \"/home/neel/Programming_Workspace/Not-My-Repos/tensorflow/third_party/repo.bzl\", line 106\r\n\t\t_apply_patch(ctx, ctx.attr.patch_file)\r\n\tFile \"/home/neel/Programming_Workspace/Not-My-Repos/tensorflow/third_party/repo.bzl\", line 68, in _apply_patch\r\n\t\tfail(\"patch command is not found, ple...\")\r\npatch command is not found, please install it and referenced by '//third_party/eigen3:eigen3'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@eigen_archive//': Traceback (most recent call last):\r\n\tFile \"/home/neel/Programming_Workspace/Not-My-Repos/tensorflow/third_party/repo.bzl\", line 106\r\n\t\t_apply_patch(ctx, ctx.attr.patch_file)\r\n\tFile \"/home/neel/Programming_Workspace/Not-My-Repos/tensorflow/third_party/repo.bzl\", line 68, in _apply_patch\r\n\t\tfail(\"patch command is not found, ple...\")\r\npatch command is not found, please install it\r\nINFO: Elapsed time: 3.142s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (42 packages loaded, 470 targets c\\\r\nonfigured)\r\n    currently loading: tensorflow/core/kernels\r\n    Fetching @eigen_archive; fetching\r\n    Fetching @double_conversion; fetching\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nI just cloned the latest repo from git and checked out TF r1.12 last commit. \r\nAfter that I passed only CPU options to \"configure\" and also had to install pip3.\r\nI also installed eigen3 package from my distro's repos, but no help.\r\nI don't know what to do, eigen does seem to be in the TF/tools/ folder but bazel doesn't import it. Please help !!\r\n\r\nHere's the folder structure of \"third_parties\" folder:\r\n```\r\nls ./third_party/\r\nandroid                     libxsmm.BUILD\r\narm_neon_2_x86_sse.BUILD    linenoise.BUILD\r\nastor.BUILD                 llvm\r\naws                         lmdb.BUILD\r\nbackports_weakref.BUILD     mkl\r\nboringssl                   mkl_dnn\r\nBUILD                       mpi\r\nclang_toolchain             mpi_collectives\r\ncodegen.BUILD               nanopb.BUILD\r\ncom_google_absl.BUILD       nasm\r\ncommon.bzl                  nccl\r\ncub.BUILD                   ngraph\r\ncurl.BUILD                  ortools\r\ncython.BUILD                pasta\r\ndouble_conversion.BUILD     pcre.BUILD\r\neigen3                      png.BUILD\r\neigen.BUILD                 png_fix_rpi.patch\r\nenum34.BUILD                pprof.BUILD\r\nexamples                    protobuf\r\nfarmhash.BUILD              py\r\nfft2d                       python_runtime\r\nflatbuffers                 repo.bzl\r\ngast.BUILD                  six.BUILD\r\ngif.BUILD                   snappy.BUILD\r\ngit                         sqlite.BUILD\r\ngoogleapis.BUILD            swig.BUILD\r\ngpus                        sycl\r\ngrpc                        systemlibs\r\nhadoop                      tensorrt\r\nhighwayhash                 termcolor.BUILD\r\nhwloc                       tflite_mobilenet.BUILD\r\nicu                         tflite_mobilenet_float.BUILD\r\njpeg                        tflite_mobilenet_quant.BUILD\r\njsoncpp.BUILD               tflite_ovic_testdata.BUILD\r\nkafka                       tflite_smartreply.BUILD\r\nkeras_applications_archive  toolchains\r\nkissfft                     zlib.BUILD\r\n```\r\nHeres my git branches:\r\n```\r\ngit branch\r\n  master\r\n* r1.12\r\n\r\ngit show --oneline r1.12\r\n\r\na6d8ffae09 (HEAD -> r1.12, tag: v1.12.0) Fix a bug in tpu.py and xla.py that while creating an identity node for control input edges under rewrite context, the parent control flow context is lost. (#23446)\r\ndiff --git a/tensorflow/contrib/compiler/xla.py b/tensorflow/contrib/compiler/xla.py\r\nindex 873b03580d..83d9d8c54a 100644\r\n--- a/tensorflow/contrib/compiler/xla.py\r\n+++ b/tensorflow/contrib/compiler/xla.py\r\n@@ -179,14 +179,11 @@ class XLACompileContext(control_flow_ops.XLAControlFlowContext):\r\n     if external_control_inputs:\r\n       # Use an identity to pull control inputs as data inputs. Note that we\r\n       # ignore ops which don't have outputs. TODO(phawkins): fix that.\r\n-      with ops.control_dependencies(None):\r\n-        self.Enter()\r\n-        external_control_inputs = [\r\n-            array_ops.identity(x.outputs[0]).op\r\n-            for x in external_control_inputs\r\n-            if x.outputs\r\n-        ]\r\n-        self.Exit()\r\n+      external_control_inputs = [\r\n+          array_ops.identity(x.outputs[0]).op\r\n", "comments": ["Update:\r\n---------\r\nI had somehow messed up the git repo of tensorflow(I want to work on version 1.12), and after git fetching the whole repo, I still get this error:\r\n```\r\nbazel build --incompatible_remove_native_http_archive=false --config=opt --copt=-mavx --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/home/neel/Programming_Workspace/Not-My-Repos/tensorflow/tools/bazel.rc\r\nINFO: Invocation ID: 69f627fb-d42c-470a-a1dc-e81bfedaaeba\r\nERROR: /home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/protobuf_archive/BUILD:591:1: Traceback (most recent call last):\r\n\tFile \"/home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/protobuf_archive/BUILD\", line 591\r\n\t\tinternal_gen_well_known_protos_java(srcs = WELL_KNOWN_PROTOS)\r\n\tFile \"/home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/protobuf_archive/protobuf.bzl\", line 269, in internal_gen_well_known_protos_java\r\n\t\tLabel((\"%s//protobuf_java\" % REPOSITOR...))\r\n\tFile \"/home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/protobuf_archive/protobuf.bzl\", line 269, in Label\r\n\t\tREPOSITORY_NAME\r\nbuiltin variable 'REPOSITORY_NAME' is referenced before assignment.\r\nERROR: /home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/protobuf_archive/BUILD:373:1: Target '@protobuf_archive//:android' contains an error and its package is in error and referenced by '@protobuf_archive//:protoc'\r\nERROR: /home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/protobuf_archive/BUILD:373:1: Target '@protobuf_archive//:msvc' contains an error and its package is in error and referenced by '@protobuf_archive//:protoc'\r\nERROR: /home/neel/Programming_Workspace/Not-My-Repos/tensorflow/tensorflow/contrib/boosted_trees/proto/BUILD:35:1: Target '@protobuf_archive//:protobuf_python_genproto' contains an error and its package is in error and referenced by '//tensorflow/contrib/boosted_trees/proto:tree_config_proto_py_genproto'\r\nERROR: /home/neel/Programming_Workspace/Not-My-Repos/tensorflow/tensorflow/contrib/boosted_trees/proto/BUILD:35:1: Target '@protobuf_archive//:protoc' contains an error and its package is in error and referenced by '//tensorflow/contrib/boosted_trees/proto:tree_config_proto_py_genproto'\r\nERROR: /home/neel/Programming_Workspace/Not-My-Repos/tensorflow/tensorflow/contrib/boosted_trees/proto/BUILD:35:1: Target '@protobuf_archive//:protobuf_python' contains an error and its package is in error and referenced by '//tensorflow/contrib/boosted_trees/proto:tree_config_proto_py'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 24.438s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (87 packages loaded, 538 targets c\\\r\nonfigured)\r\n    currently loading: tensorflow/core/kernels\r\n    Fetching @absl_py; fetching\r\n```\r\n", "OK, so supposedly I was using the flag \"-std=c++17\" during configuration steps, and according to [this](https://github.com/bazelbuild/bazel/issues/5728) useful link, it should be C++11.\r\nBut then again it still gives another error(:weary:) and idk lol:\r\n\r\n```\r\nbazel build --incompatible_remove_native_http_archive=false --config=opt --copt=-mavx --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/home/neel/Programming_Workspace/Not-My-Repos/tensorflow/tools/bazel.rc\r\nWARNING: ignoring _JAVA_OPTIONS in environment.\r\nStarting local Bazel server and connecting to it...\r\nINFO: Invocation ID: 21869764-322c-4760-a2a6-8154b2fb6acf\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@kafka//': Traceback (most recent call last):\r\n\tFile \"/home/neel/Programming_Workspace/Not-My-Repos/tensorflow/third_party/repo.bzl\", line 106\r\n\t\t_apply_patch(ctx, ctx.attr.patch_file)\r\n\tFile \"/home/neel/Programming_Workspace/Not-My-Repos/tensorflow/third_party/repo.bzl\", line 68, in _apply_patch\r\n\t\tfail(\"patch command is not found, ple...\")\r\npatch command is not found, please install it\r\nINFO: Elapsed time: 26.139s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (225 packages loaded, 3756 targets\\\r\n configured)\r\n    Fetching @com_google_absl; fetching 9s\r\n    Fetching @double_conversion; fetching\r\n    Fetching @aws; fetching\r\n    Fetching @kafka; fetching\r\n```\r\n", "So, here it goes...\r\nFor Kafka i found [this](https://github.com/tensorflow/tensorflow/issues/23817) issue helpful, but then again I felt into another issue with the '@io_bazel_rules_closure//closure' package. \r\n\r\nHere's a log of the command:\r\n\r\n```\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/home/neel/Programming_Workspace/Not-My-Repos/tensorflow/tools/bazel.rc\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: ignoring _JAVA_OPTIONS in environment.\r\nStarting local Bazel server and connecting to it...\r\nERROR: /home/neel/Programming_Workspace/Not-My-Repos/tensorflow/WORKSPACE:3:1: name 'http_archive' is not defined\r\nERROR: Error evaluating WORKSPACE file\r\nERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package\r\nERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package\r\nINFO: Elapsed time: 5.856s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\n\r\nAccording to [this](https://github.com/tensorflow/tensorflow/issues/7497) issue, it happens either due to [not using Oracle JDK](https://github.com/tensorflow/tensorflow/issues/7497#issuecomment-355743662) or having [OpenJDK's certs being old](https://github.com/tensorflow/tensorflow/issues/7497#issuecomment-388485871). The later I tried using this command, and then copying it to the correct position for my distro(/usr/lib/jvm/openjdk-1.8.0_202/jre/lib/security/):\r\n\r\n```\r\nmkcacerts -f /etc/ssl/certs.pem -o cacerts -k $(which keytool) -s $(which openssl)\r\n```\r\n\r\nBoth of these didn't help. If this doesn't get solve then I don't think I'll be able to use C++ for Tensorflow, and be forced to use Python.\r\n\r\nNote \r\n------\r\nI was initially doing the building using bazel 0.22. Then later on while surfing the net I saw that building Tensorflow using older versions of bazel give strange issues like [this one](https://github.com/tensorflow/tensorflow/issues/22898) which i got reference from [this one](https://github.com/google/flatbuffers/pull/5174).\r\n\r\nHope this helps anyone. If someone can solve this issue then please suggest a workaround.  ", "In your initial error log,  I see this:\r\n```\r\npatch command is not found, please install it\r\n```\r\n\r\nCould you try that?", "Yeah, my distro doesn't include patch command by default.\r\nI had to install it then run them. The results above where with patch installed.", "I think many errors I see are due to configure script not being run properly, I know for sure that \"REPOSITORY_NAME\" issue is due to that.\r\n\r\nMoreover, I think your bazel version may be too new. For 1.12, we used 0.15 to build the branch.\r\nhttps://www.tensorflow.org/install/source#linux\r\nAnd I think 0.22 is not even tested with 1.13 yet.\r\n", "Ok let me try with bazel 0.15", "Ok, on the same system with same everything, and using bazel 0.15 I'm getting this error:\r\n```\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: ignoring _JAVA_OPTIONS in environment.\r\nStarting local Bazel server and connecting to it...\r\n.............................\r\nWARNING: /home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nERROR: /home/neel/Programming_Workspace/Not-My-Repos/tensorflow/tensorflow/tools/pip_package/BUILD:132:1: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to /home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/icu/release-62-1.tar.gz: Checksum was 33d08fc296d7ca8165ff1091f61ea998b61139aec353c295168b49407123099b but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761 and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to /home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/icu/release-62-1.tar.gz: Checksum was 33d08fc296d7ca8165ff1091f61ea998b61139aec353c295168b49407123099b but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761\r\nINFO: Elapsed time: 408.008s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (191 packages loaded)\r\n```\r\nHope that helps.", "```\r\nError downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz]\r\n```\r\nNetwork issues?", "No, my network is working fine. I even downloaded other >200 bazel packages plus some study videos later. Idk.\r\nMaybe I could try to wget that host. But it's almost 11pm.  Maybe tomorrow.", "Bazel reports that it had difficulty downloading those specific links, so even if your network is working fine,  bazel is having problems with your network.", "So what should I do? To resolve that issue?\r\n", "You can try running wget, and seeing if that works. if it does not, you may be behind a firewall, or may need a proxy. If it does works, it may just be a flake.\r\n", "Can you give an example of using wget along with bazel. It was hectic this 5 days and I'm unsure where to place the downloaded packages in the tensor flow directory.\r\nThanks alot", "Hi guys I have the same error with \r\nbazel build tensorflow/tools/graph_transforms:summarize_graph\r\n\r\nINFO: Invocation ID: 21b504b6-1730-45a1-b022-0a26498210e3\r\nERROR: D:/workspaces/tensorflow/third_party/eigen3/BUILD:34:1: no such package '@eigen_archive//': Traceback (most recent call last):\r\n        File \"D:/workspaces/tensorflow/third_party/repo.bzl\", line 106\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"D:/workspaces/tensorflow/third_party/repo.bzl\", line 73, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"D:/workspaces/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(127) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/kobe/_bazel_kobe/xjytsuoi/external/eigen_archive\" \"-i\" \"D:/workspaces/tensorflow/third_party/eigen3/gpu_packet_math.patch\"':\r\nStdout:\r\nStderr: /usr/bin/bash: patch: \u93c8\ue045\u58d8\u9352\u677f\u61e1\u6d60? and referenced by '//third_party/eigen3:eigen3'\r\nERROR: Analysis of target '//tensorflow/tools/graph_transforms:summarize_graph' failed; build aborted: no such package '@eigen_archive//': Traceback (most recent call last):\r\n        File \"D:/workspaces/tensorflow/third_party/repo.bzl\", line 106\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"D:/workspaces/tensorflow/third_party/repo.bzl\", line 73, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"D:/workspaces/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(127) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/kobe/_bazel_kobe/xjytsuoi/external/eigen_archive\" \"-i\" \"D:/workspaces/tensorflow/third_party/eigen3/gpu_packet_math.patch\"':\r\nStdout:\r\nStderr: /usr/bin/bash: patch: \u93c8\ue045\u58d8\u9352\u677f\u61e1\u6d60?Analyzing: target //tensorflow/tools/graph_transforms:summarize_graph (1 paINFO: Elapsed time: 4.763s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded, 2 targets configured)\r\n    Fetching @eigen_archive; fetching 4s\r\n", "> \r\n> \r\n> Hi guys I have the same error with\r\n> bazel build tensorflow/tools/graph_transforms:summarize_graph\r\n> \r\n> INFO: Invocation ID: 21b504b6-1730-45a1-b022-0a26498210e3\r\n> ERROR: D:/workspaces/tensorflow/third_party/eigen3/BUILD:34:1: no such package '@eigen_archive//': Traceback (most recent call last):\r\n> File \"D:/workspaces/tensorflow/third_party/repo.bzl\", line 106\r\n> _apply_patch(ctx, ctx.attr.patch_file)\r\n> File \"D:/workspaces/tensorflow/third_party/repo.bzl\", line 73, in _apply_patch\r\n> _execute_and_check_ret_code(ctx, cmd)\r\n> File \"D:/workspaces/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n> fail(\"Non-zero return code({1}) when ...))\r\n> Non-zero return code(127) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/kobe/_bazel_kobe/xjytsuoi/external/eigen_archive\" \"-i\" \"D:/workspaces/tensorflow/third_party/eigen3/gpu_packet_math.patch\"':\r\n> Stdout:\r\n> Stderr: /usr/bin/bash: patch: \u93c8\ue045\u58d8\u9352\u677f\u61e1\u6d60? and referenced by '//third_party/eigen3:eigen3'\r\n> ERROR: Analysis of target '//tensorflow/tools/graph_transforms:summarize_graph' failed; build aborted: no such package '@eigen_archive//': Traceback (most recent call last):\r\n> File \"D:/workspaces/tensorflow/third_party/repo.bzl\", line 106\r\n> _apply_patch(ctx, ctx.attr.patch_file)\r\n> File \"D:/workspaces/tensorflow/third_party/repo.bzl\", line 73, in _apply_patch\r\n> _execute_and_check_ret_code(ctx, cmd)\r\n> File \"D:/workspaces/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n> fail(\"Non-zero return code({1}) when ...))\r\n> Non-zero return code(127) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/kobe/_bazel_kobe/xjytsuoi/external/eigen_archive\" \"-i\" \"D:/workspaces/tensorflow/third_party/eigen3/gpu_packet_math.patch\"':\r\n> Stdout:\r\n> Stderr: /usr/bin/bash: patch: \u93c8\ue045\u58d8\u9352\u677f\u61e1\u6d60?Analyzing: target //tensorflow/tools/graph_transforms:summarize_graph (1 paINFO: Elapsed time: 4.763s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully (1 packages loaded, 2 targets configured)\r\n> Fetching @eigen_archive; fetching 4s\r\n\r\nI solved this problem just few minutes ago.\r\nhttps://github.com/tensorflow/tensorflow/issues/21880#issuecomment-476599931", "I was simply recommending running wget from your terminal with the links that are failing to download. I do not think you can run \"bazel with wget\", or vice versa.\r\n\r\nClosing the issue, as the remaining problem is a network issue on user side, and we cannot do anything from our end, as all of our servers and mirrors are live and green from our end. Maybe check your firewall, or contact your network administrator or ISP.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26198\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26198\">No</a>\n"]}, {"number": 26197, "title": "[TF 2.0 API Docs] tf.keras", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n**System information**\r\n- TensorFlow version: 2.0\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras\r\n\r\n**Describe the documentation issue**\r\n\r\n**Links** \r\n\"Defined in python/keras/api/_v2/keras/__init__.py\" is pointing to a broken link:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/api/_v2/keras/__init__.py\r\n\r\n**Description**\r\nCurrently it has \"Detailed documentation and user guides are available at keras.io.\"\r\nIt would be better to point users to https://www.tensorflow.org/guide/keras, otherwise the experience feels broken.\r\n\r\nIt would be great to list some key differences between tf.keras vs Keras as an independent project (keras.io). Or at least point out there is no 1:1 mapping and what each one has and the other one doesn't have. This has been discussed in blog posts and forums but the official documentation should at least have a high level overview with a few sentences.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nYes\r\n", "comments": ["I would be happy to do that. Soon I will update you with my work. ", "Thank you, @Praneet460 - and thanks to @margaretmz for filing the issue! Please feel free to reach out if you have any questions. :slightly_smiling_face: ", "Hello, @dynamicwebpaige  I tried to find out the respective file in `tensorflow/docs/site/en` which I need to edit but I am not able to track it down. I would be happy if you help me a little bit in finding the appropriate file.", "@dynamicwebpaige I read some other issues and found that .bzl and __init__.py files are used for generating modules but I am not sure if these are the files we need to change. Can you please help .\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/__init__.py\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/api/generator/api_init_files.bzl", "I would also love to contribute\r\n", "@dynamicwebpaige Can you please help me with this\r\n\r\n> @dynamicwebpaige I read some other issues and found that .bzl and **init**.py files are used for generating modules but I am not sure if these are the files we need to change. Can you please help .\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/__init__.py\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/api/generator/api_init_files.bzl\r\n", "I would love to contribute to this!! please assign me this", "@dynamicwebpaige I search for the respective file but didn't find it in the docs repository of tensorflow. please direct me to the desired file for this issue.", "@margaretmz @dynamicwebpaige  I wanted to give a shot at documenting the difference between tf.keras and keras.io .Can you point me to good resources (blogs etc.) which I can use for the proper documentation. What word count and line count are we looking at?", "I would love to work on this. Can I please get a chance?", "Both the things were fixed: https://github.com/tensorflow/tensorflow/pull/33443\r\n\r\nRegarding difference between keras.io vs tensorflow.org/keras, contributions welcome :)", "@margaretmz Could you please confirm if this issue is resolved as per the previous comment.", "There are a few things here:\r\n\r\n- **Broken link on page**: I no longer see any broken links. So we are good on this.\r\n- My comment on \"Detailed documentation and user guides are available at **keras.io**\". It is still pointed to keras.io. I thought keras.io is no longer really an active project any more. Why are we still point docs there?\r\n- Thanks all for volunteering to document the different between tf.keras and keras.io. Not sure if there is any point in doing so though. Keras.io is no longer having any new releases and tf.keras will be moved into the original Keras repo. So there is really just tf.Keras.\r\n", "As the issue has been addressed moving this issue to closed status.", ">  \"Detailed documentation and user guides are available at keras.io\". It is still pointed to keras.io\r\n\r\n@margaretmz Fixed in nightly: https://www.tensorflow.org/api_docs/python/tf/keras?version=nightly"]}, {"number": 26196, "title": "TF 1.13.1 on Raspberry Pi 3B+ __init__.py gives an error: No module named 'tensorflow.python'. How to use static library file?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian 9.8\r\n- Mobile device if the issue happens on mobile device: Raspberry Pi Model 3 B+\r\n- TensorFlow installed from (source or binary): Source I think\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: Native Compile\r\n- Bazel version (if compiling from source): Not Applicable\r\n- GCC/Compiler version (if compiling from source): 6.3.0 20170516\r\n- CUDA/cuDNN version: Not Applicable\r\n- GPU model and memory: Not Applicable\r\n\r\n\r\n\r\n**Describe the problem**\r\nI followed [this guide](https://www.tensorflow.org/lite/rpi) on how to compile TensorFlow Lite Natively on the Raspberry Pi. I cloned the tensorflow repo on the home folder, and then executed the scripts accordingly. I had to adjust the swapsize [as descrbied here](https://github.com/tensorflow/tensorflow/issues/26158#issuecomment-468089490), for the Compile to successfully run.\r\n\r\nBack in the home folder, when I try to run:\r\n\r\n` ~$ python3`\r\n`>>> import tensorflow as tf`\r\n\r\nI don't get any errors.\r\n\r\nMy goal is to run Object Detection on RaspberryPi using TF-Lite, [and I found this sample code](https://github.com/freedomtan/tensorflow/blob/deeplab_tflite_python/tensorflow/contrib/lite/examples/python/object_detection.py)\r\n\r\nBut when I tried to do his tensorflow import:\r\n\r\n`>>> from tensorflow.contrib.lite.python import interpreter as interpreter_wrapper`\r\n\r\nIt didn't work because of course, in TF 1.13.1, the `lite` folder is no longer under `contrib`\r\n\r\nBut when I do:\r\n\r\n`>>> from tensorflow.lite.python import interpreter as interpreter_wrapper`\r\n\r\nI get: `ImportError: No module named 'tensorflow.lite'`\r\n\r\nBut when I do:\r\n\r\n`>>> from tensorflow.tensorflow.lite.python import interpreter as interpreter_wrapper`\r\n\r\nI get:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/home/pi/tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n> ImportError: No module named 'tensorflow.python'\r\n\r\nWhen going to the `__init__.py` file (located in ~/tensorflow/tensorflow), the line in question is:\r\n> from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\nThe error then becomes:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/home/pi/tensorflow/tensorflow/__init__.py\", line 25, in <module>\r\n>     from tensorflow.tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n>   File \"/home/pi/tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n>    from tensorflow.python import pywrap_tensorflow\r\n> ImportError: No module named 'tensorflow.python'\r\n\r\nIt's clear that in python code, I have to import Tensorflow as if I'm navigating through the file system.\r\n\r\nHaving downloaded the tensorflow repo to the home folder, and executing the python3 command from there, importing the tensorflow code had me going through the file structure.\r\n\r\nAnd now that the __init__.py file is encountering problems in importing, I'm not sure how to edit the import paths, and I don't think I should. \r\n\r\nI have a feeling that I did not compile tensorflow properly.\r\n\r\nHow are we even supposed to use the libtensorflow-lite.a static library file?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Go get yourself an RPi, install Raspbian (not through NOOBS).\r\n2. Download the Tensorflow repo in the home folder. Adjust your swapsize [by doing this](https://github.com/tensorflow/tensorflow/issues/26158#issuecomment-468089490). Then follow [the instructions in this link](https://www.tensorflow.org/lite/rpi)\r\n3. After generating the static library, go back to the home folder and run python3 and the commands I tried above.\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThe complete logs are above. \r\n", "comments": ["It looks like you're trying to use TensorFlow Lite on the Raspberry Pi? The recommended way to do this is to build just the library itself, which you can find instructions for here:\r\nhttps://www.tensorflow.org/lite/guide/build_rpi", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26196\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26196\">No</a>\n"]}, {"number": 26195, "title": "Typos in tensorflow/core", "body": "", "comments": []}]