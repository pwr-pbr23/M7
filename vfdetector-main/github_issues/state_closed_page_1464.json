[{"number": 9030, "title": "Revert \"Allow SavedModelBuilder to overwrite existing folders\"", "body": "Reverts tensorflow/tensorflow#8902\r\n\r\nBreaks API compatibility.", "comments": []}, {"number": 9029, "title": "CPU kernels for FFT (WIP)", "body": "This PR adds CPU kernels for FFTs (cf #386). A few points to note:\r\n\r\n* This PR does not yet support RFFTs. These can in principle be implemented by slicing such that the negative frequency components of the FFT are removed. Maybe @benoitsteiner has some better ideas though.\r\n* Does it make sense to split up the code into multiple files to avoid having `#if GOOGLE_CUDA` half way?\r\n* @rryan, because the tensor FFT in Eigen is templated, I couldn't avoid bloating the binary a bit as discussed via e-mail.", "comments": ["Can one of the admins verify this patch?", "The (I)RFFT needs some amount of tensor manipulation. After playing around with the problem a bit, it seems like using the `FFTFunctor` is actually quite restrictive. Instead, it would make sense to build an operation tree as described [here](https://bitbucket.org/eigen/eigen/src/default/unsupported/Eigen/CXX11/src/Tensor/README.md?fileviewer=file-view-default). So, for the RFFT I was thinking of\r\n\r\n```cpp\r\nauto axes = Eigen::ArrayXi::LinSpaced(FFTRank, 1, FFTRank);\r\nauto device = ctx->eigen_device<CPUDevice>();\r\nauto input = ((Tensor) in).flat_inner_dims<float, FFTRank + 1>();\r\nauto output = out->flat_inner_dims<complex64, FFTRank + 1>();\r\nEigen::DSizes<Eigen::DenseIndex, FFTRank + 1> startIndices;\r\nauto op = (input.template fft<Eigen::BothParts, Eigen::FFT_FORWARD>(axes))\r\n  .slice(startIndices, output.dimensions());\r\noutput.device(device) = op;\r\n```\r\n\r\nand for the IRFFT\r\n\r\n```cpp\r\nauto axes = Eigen::ArrayXi::LinSpaced(FFTRank, 1, FFTRank);\r\nauto device = ctx->eigen_device<CPUDevice>();\r\nauto input = ((Tensor) in).flat_inner_dims<float, FFTRank + 1>();\r\nauto output = out->flat_inner_dims<complex64, FFTRank + 1>();\r\n// The first dimension contains the zero-frequency component which we\r\n// do not want to duplicate. So we reconstruct the complex signal by\r\n// (1) slicing from the second element, (2) reversing the order,\r\n// (3) taking the complex conjugate, (4) concatenating with the original\r\n// input. Note that for an even input length, the last element is the\r\n// Nyquist frequency which we also do not want to duplicate.\r\nEigen::DSizes<Eigen::DenseIndex, FFTRank + 1> startIndices;\r\nstartIndices[FFTRank] = 1;\r\nauto sizes = input.dimensions();\r\nif (sizes[FFTRank] % 2 == 0) {\r\n  sizes[FFTRank] -= 1;\r\n}\r\n// Compute the complex conjugate and reverse.\r\nauto cc = input.slice(startIndices, sizes).conjugate()\r\n  .reverse(FFTRank);\r\n// Reconstruct the full FFT.\r\nauto full_fft = input.concatenate(cc, FFTRank);\r\n// Take the inverse\r\noutput.device(device) = full_fft.template fft<Eigen::RealPart, Eigen:FFT_FORWARD>(axes);\r\n```\r\n\r\nUnfortunately, I can't get it to compile because of some const mismatch:\r\n\r\n```\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:4:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:125:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:203:51: error: cannot initialize return object of type 'Scalar *' (aka 'float *') with an lvalue of type 'CoeffReturnType *const' (aka 'std::__1::complex<float> *const')\r\n  EIGEN_DEVICE_FUNC Scalar* data() const { return m_data; }\r\n```\r\n\r\nHere is a minimum example reproducing the same error.\r\n\r\n```cpp\r\n#include \"unsupported/Eigen/CXX11/Tensor\"\r\n\r\nint main() {\r\n    // Create input tensor.\r\n    Eigen::Tensor<float, 2> input(5, 5);\r\n    input.setRandom();\r\n    // Create arrays for axes and slicing.\r\n    Eigen::ArrayXi axes = {0, 1};\r\n    Eigen::DSizes<Eigen::DenseIndex, 2> startIndices;\r\n    Eigen::DSizes<Eigen::DenseIndex, 2> sizes = input.dimensions();\r\n    // Construct the graph and evaluate.\r\n    auto op = input.template fft<Eigen::BothParts, Eigen::FFT_FORWARD>(axes).slice(startIndices, sizes);\r\n    Eigen::Tensor<std::complex<float>, 2> output = op;\r\n}\r\n```\r\n\r\n<s>Alright, RFFT is added but I can't quite get the IRFFT to compile. In particular, the compiler isn't happy about passing the reconstructed full FFT (with negative frequencies) to the functor. Help from someone with more Eigen-knowledge would be great. Here's the compiler message.\r\n\r\n```\r\ntensorflow/core/kernels/fft_ops.cc:172:9: error: no matching function for call to object of type 'FFTFunctor<CPUDevice, complex64, float, Eigen::RealPart, Eigen::FFT_REVERSE, 3>' (aka 'FFTFunctor<Eigen::ThreadPoolDevice, complex<float>, float, Eigen::RealPart, Eigen::FFT_REVERSE, 3>')\r\n        functor(ctx->eigen_device<CPUDevice>(), output, full_fft);\r\n        ^~~~~~~\r\ntensorflow/core/kernels/fft_ops.cc:198:25: note: in instantiation of member function 'tensorflow::FFTCPU<true, true, 3>::DoFFT' requested here\r\n                        FFTCPU<true, true, 3>);\r\n                        ^\r\ntensorflow/core/kernels/fft_ops.cc:97:8: note: candidate function not viable: no known conversion from 'Eigen::TensorConcatenationOp<const int, Eigen::TensorMap<Eigen::Tensor<std::__1::complex<float>, 4, 1, long>, 16, MakePointer>, Eigen::TensorReverseOp<const int, const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_conjugate_op<std::__1::complex<float> >, const Eigen::TensorSlicingOp<const Eigen::DSizes<long, 4>, const Eigen::DSizes<long, 4>, Eigen::TensorMap<Eigen::Tensor<std::__1::complex<float>, 4, 1, long>, 16, MakePointer> > > > >' to 'typename TTypes<complex<float>, 3 + 1>::Tensor' (aka 'TensorMap<Eigen::Tensor<complex<float>, 4, Eigen::RowMajor, long>, Eigen::Aligned>') for 3rd argument\r\n  void operator()(const Device& d,\r\n```\r\n\r\nI've tried casting to `typename TTypes<complex64, FFTRank + 1>` to force an evaluation of the intermediate state but it wasn't happy with that either.</s>", "@rryan @benoitsteiner any guidance here?", "Jenkins, test this please.", "@tillahoffmann any updates on this PR?", "@vrv, there seems to be an issue with the types declared in the FFT operator of Eigen which means that compiling the (I)RFFT fails (see @rryan's comment above). Unfortunately, I'm not sufficiently familiar with Eigen to be able to fix it/find a work around. \r\n\r\nWe could either see whether @benoitsteiner or @rmlarsen have some ideas or we could change this PR to only provide an implementation of the complex FFT.", "Okay, @benoitsteiner or @rmlarsen let us know if you have any ideas on next steps here.", "@b11094 are you suggesting a fix for the build failures?", "@rmlarsen @benoitsteiner can you provide some guidance here re: @rryan's findings?  Would love to get this unblocked and into TF.", "We could also consider having the kernels be complex-valued, and insert casting operations on the input and output as appropriate.", "Can one of the admins verify this patch?", "Reconstructing the full FFT from the RFFT coefficients in Eigen is proving to be a bit tricky. The current PR implements the complex FFT, IFFT and the real FFT. The real IFFT is not implemented. I imagine someone with better Eigen knowledge than me would be better suited to implement it.", "@tillahoffmann Thanks a lot for this contribution! I think this is a great step forward. If @rryan approves, I think we can merge this and add the IRFFT in a followup.", "@tensorflow-jenkins test this please", "@rmlarsen, do you know why the [GPU builds are failing](https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/4815/consoleFull)? It's complaining that some symbols are not declared:\r\n\r\n```\r\nIn file included from ./tensorflow/stream_executor/kernel.h:80:0,\r\n                 from ./tensorflow/stream_executor/trace_listener.h:22,\r\n                 from ./tensorflow/stream_executor/platform.h:30,\r\n                 from ./tensorflow/stream_executor/cuda/cuda_platform_id.h:19,\r\n                 from ./tensorflow/core/platform/default/stream_executor.h:23,\r\n                 from ./tensorflow/core/platform/stream_executor.h:24,\r\n                 from tensorflow/core/kernels/fft_ops.cc:176:\r\n./tensorflow/stream_executor/lib/array_slice.h:25:19: error: 'tensorflow::tensorflow::gtl' has not been declared\r\n using tensorflow::gtl::ArraySlice;\r\n```", "@rmlarsen, let's hope this compiles. Btw, is there a way to get testing privileges on PRs?", "LGTM, thanks! Though I'm still curious about how to handle the internal/external issue. I think merging this will break internal code unless we ifdef the kernel to be opensource-only somehow.", "Jenkins, test this please.", "@rryan, do you think it would be possible to make the internal code available?", "> @rryan, do you think it would be possible to make the internal code available?\r\n\r\nUnfortunately we can't, and I'm not qualified to explain why since I'm not a lawyer. :(", "@tillahoffmann thanks! Our test resources are still somewhat limited, so we keep the privilege to trigger it to people in the team.", "@tillahoffmann thanks again for the contribution!", "thanks @tillahoffmann !", "Could this get merged into r1.2? Seems like a feature that people are really excited about.", "Maybe supid question, but along which axis is the fft performed? \r\nAssume a 3D tensor with 2D-images along the 3rd dimension, now I'm doing a fft2d. How would I do it if I want to do it for each 2D slice seperatly? \r\n\r\nMost other libraries have something like `np.fft2d(image, s=None, axes=2) `or so. \r\nWould be nice to have this feature also in TensorFlow :)\r\n\r\n", "@beniroquai It's common to have to batch dimensions first and not last.", "Ahh! Nice! That's my error. So it transforms along 1st dimension? \r\nImagine you have an Image-stack of 128x128x45, where z=45; I should better permute it to 45x128x128? \r\nThank you! :)", "@beniroquai, yes the batch dimension (45 in your case) should be the first dimension. The n-dimensional fft will transform the *last* n dimension. I don't expect to see axis support in tensorflow any time soon because the GPU kernel implementation seems to rely on the transform being along the last dimensions--but I'm happy to stand corrected.", "> I don't expect to see axis support in tensorflow any time soon because the GPU kernel implementation seems to rely on the transform being along the last dimensions--but I'm happy to stand corrected.\r\n\r\nI'd love to add an axis parameter. If someone from the community would like to add it I think it'd be welcomed by the team. I doubt it's high on anybody's TODO list at the moment though.\r\n\r\nI think the code is already generic enough to support it for GPU since we already specify the \"advanced data layout\" parameters to configure it to do the last N dimensions.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fft_ops.cc#L308\r\nhttp://docs.nvidia.com/cuda/cufft/index.html#advanced-data-layout\r\n\r\nIn the worst case, we could perform multiple serial 1D FFTs."]}, {"number": 9028, "title": "XLA with tower parallelization", "body": "### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: yes\r\n- *TensorFlow installed from (source or binary)?*: source\r\n- *TensorFlow version*: 1.1rc1\r\n- *Bazel version (if compiling from source)*: 0.4.5\r\n- *CUDA/cuDNN version*: 8.0/5.1\r\n- *GPU Model and Memory*: Titan X (12 GB)\r\n- *Exact command to reproduce*: NA\r\n\r\n### Describe the problem clearly\r\n\r\nUsing the automatic XLA option`sess_config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1` causes an 8x performance improvement for my model (while training) when running with 1 GPU, amazing! I would like to parallelize this with a standard tower setup, without XLA this scales linearly (using 4 GPUs) but with XLA I see no difference. The timeline profiler (`chrome://tracing`) indicates that it just executes each tower one at the time on each GPU.\r\n\r\nThe example in [tensorflow/models/neural_gpu/neural_gpu.py](https://github.com/tensorflow/models/blob/master/neural_gpu/neural_gpu.py) indicates that one can use the manual `tf.contrib.compiler.jit.experimental_jit_scope` context creator, using this causes a x6 times performance peanality compared to not using XLA, but does scale with multiple GPUs.\r\n\r\nI would like a feature where `experimental_jit_scope` works similarly to the automatic `sess_config` XLA option or the `sess_config` works with multiple GPUs.\r\n\r\n### Source Code / Logs\r\n\r\nMy model is almost identical to the one in [https://github.com/buriburisuri/ByteNet](https://github.com/buriburisuri/ByteNet/blob/master/train.py).\r\n", "comments": ["Could you provide more information, specifically, instructions on reproducing your problem? Ideally if it can be cut down to a small snippet that doesn't depend on external libraries (like `sugartensor` used in the code you linked to)?\r\n\r\nWithout looking at how you're splitting the model across GPUs and/or the full trace, it is a bit hard to provide a useful response. Thanks!"]}, {"number": 9027, "title": "Windows: tf.gfile methods (tf.gfile.Exists, tf.gfile.IsDirectory) are not Windows friendly", "body": "(Windows 10) in saver.py  train.save() takes a sess_path but on windows it returns an empty result.\r\nI had to modify to the following (around line 1354)\r\n    if not gfile.IsDirectory(os.path.dirname(os.path.abspath(save_path))):\r\nsimilar issue found here:\r\nhttp://stackoverflow.com/questions/7783308/os-path-dirname-file-returns-empty\r\n\r\nthis is via pip install --upgrade tensorflow-gpu", "comments": ["There isn't enough detail in this issue report to help. Could you elaborate on:\r\n\r\n- What version of TensorFlow are you using?\r\n- Code that demonstrates the problem", "print(tf._ _version_ _)     \r\n1.0.1\r\n\r\ni don't have a narrowed down standalone app(i'll try to make some time) that demonstrates it but it looks like the call sequence is just:\r\nsaver = tf.train.Saver()\r\nsaver.save(sess, 'my-model')\r\n\r\n> ValueError: Parent directory of my-model doesn't exist, can't save.", "And what is the problem with that call sequence?", "It throws the ValueError (and prints out ValueError: Parent directory of my-model doesn't exist, can't save.)  -- the example on https://www.tensorflow.org/api_docs/python/tf/train/Saver  shows that style of call.\r\n\r\n", "Ah, I see. I think the problem reduces to the different behavior of `tf.gfile.IsDirectory` on Windows and other platforms. (I suppose as a workaround you can specify an absolute path to `saver.save()` for now).\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.gfile.IsDirectory('')\r\n```\r\n\r\nis `True` on Linux and OS X but `False` on Windows, which causes this trouble.\r\nI believe this comes from the implementations of `Exists` and `Stat` in [`windows_file_system.cc`](https://github.com/tensorflow/tensorflow/blob/45d6994f69ac670396671348524a3e88cd0f9656/tensorflow/core/platform/windows/windows_file_system.cc), which is invoked from [`file_system.cc`](https://github.com/tensorflow/tensorflow/blob/45d6994f69ac670396671348524a3e88cd0f9656/tensorflow/core/platform/file_system.cc#L62), which in turn probably comes from the fact that even for Windows, [`TranslateName`](https://github.com/tensorflow/tensorflow/blob/45d6994f69ac670396671348524a3e88cd0f9656/tensorflow/core/platform/file_system.cc#L58) is implemented by [`io::CleanPath`](https://github.com/tensorflow/tensorflow/blob/45d6994f69ac670396671348524a3e88cd0f9656/tensorflow/core/lib/io/path.cc#L189), which seems not-Windows friendly with it's use of `/` and converting an empty path to `.`.\r\n\r\n@mrry : What do you suggest? Making `io::CleanPath` OS independent? Or perhaps implementing `WindowsFileSystem::TranslateName`?\r\n\r\n@guschmue : Do you have any suggestions?\r\n", "let me take a look at the code tomorrow. We had some discussions around TranslateName last year but I forgot the details why we did not implement it ... if I see the code I might remember.", "I *think* `io::CleanPath()` should be safe to use on most already-clean filenames, since Windows is tolerant of a mixture of  `'/'` and  `'\\'` in paths. However it seems like it might be more useful to switch to using something like [`PathCchCanonicalizeEx()` ](https://msdn.microsoft.com/en-us/library/windows/desktop/hh707084(v=vs.85).aspx) in `WindowsFileSystem::TranslateName()`.\r\n\r\nI'm also a bit concerned about the general logic in `saver.py`. In particular, is it guaranteed to be safe to mix `tf.gfile` functions with `os.path.dirname()` and `os.path.basename()` functions? I'm thinking this might cause the code to falsely reject e.g. an HDFS path on Windows. Or the test [on L1457](https://github.com/tensorflow/tensorflow/blob/f8dce81aeaff40dc78d398741854ad8766806f91/tensorflow/python/training/saver.py#L1457) might fail even though two filenames canonicalize to the same name. Maybe we should be delegating all `os.path` use in the Python libraries down to TensorFlow's C++ `Env` and `FileSystem` classes?\r\n\r\nOne other thing we could do in `saver.py` is to detect whenever a relative path is being supplied (which IIRC wasn't supported on any platform until the latest version) and eagerly convert it to an absolute path."]}, {"number": 9026, "title": "Unable to start the Tensor Flow", "body": "Team,\r\n\r\nI have installed anaconda on Redhat 6.8. I am following the installation steps of tensorflow from (https://www.tensorflow.org/install/install_linux#InstallingAnaconda). As the process of installation I am giving the command (conda create -n tensorflow). Because our cluster is protected I am unable to download the .conda directory. Can you please help me on this.\r\n\r\n$ export PATH=/opt/app/anaconda2/python27/bin:$PATH\r\n$ conda create -n tensorflow\r\nFetching package metadata ...\r\n\r\nCondaHTTPError: HTTP None None for url https://repo.continuum.io/pkgs/free/linux-64/repodata.json.bz2\r\nElapsed: None\r\n\r\nAn HTTP error occurred when trying to retrieve this URL.\r\nHTTP errors are often intermittent, and a simple retry will get you on your way.\r\nConnectionError(MaxRetryError(\"HTTPSConnectionPool(host='repo.continuum.io', port=443): Max retries exceeded with url: /pkgs/free/linux-64/repodata.json.bz2 (Caused by ProtocolError('Connection aborted.', error(97, 'Address family not supported by protocol')))\",),)\r\n\r\n$ cd /opt/app/anaconda2/python27/pkgs/tensorflow-0.10.0rc0-np111py27_0/\r\n$ ll\r\ntotal 12\r\ndrwxrwxr-x 2 python python 4096 Mar 31 11:20 bin\r\ndrwxrwxr-x 3 python python 4096 Dec 14 22:46 info\r\ndrwxrwxr-x 3 python python 4096 Dec 14 22:45 lib\r\n$ pwd\r\n @shyamraj242\r\n     \r\nand also\r\nWhere can I find this URL in the tensor flow.\r\n\r\nhttps://repo.continuum.io", "comments": ["The command `conda create -n tensorflow` only creates a conda environment. It hasn't begun doing anything this TensorFlow at that point.\r\n\r\nLooking at your error message, this seems to be an issue with your conda installation and not TensorFlow, so I'm tempted to close this issue out. Perhaps you can look for support on why `conda create -n` is failing. `repo.continuum.io` is an Anaconda website, not a TensorFlow one.\r\n\r\nHope that helps!\r\n"]}, {"number": 9025, "title": "Slim train_image_classifier or eval_image_classifier does not use any GPU", "body": "When I am using regular tensorflow codes there are no problem of using the GPU. But when I use slim, both train_image_classifier.py and eval_image_classifier.py does not use GPU. In my computer, there are 16 CPUs and One GPU. All CPUs are used, but the GPU is not used at all.\r\n\r\nMy environment is:\r\nOS: Windows 10\r\nTensorflow 1.10rc0\r\nPython 3.5\r\nCuda 8.0\r\ncuDNN: 8.0\r\nGPU Model: Nvidia Quadro M4000, 8G Mem\r\n\r\nI am just use the default settings in train_image_classifier.py and eval_image_classifier.py. I tested different models, and GPU is always not used. Is this a bug or did I set something wrong? Thank you.", "comments": ["Update: problem solved. I updated Tensorflow to 1.1.0rc1. Then the problem disappears. "]}, {"number": 9024, "title": "Tensorboard to Latex", "body": "It would be nice if there was an export option in tensorboard that would export a latex-ready version of the visible graph - perhaps using SVG, Pgfplots or something similar. It's possible now to export CSV, and create the graphic by hand... but having it baked into tensorboard would be nice.", "comments": ["FYI @dandelionmane \r\n\r\nSounds like a good idea. You may want to prototype a standalone tool first, or perhaps come up with a strawman design and discuss here before sending a PR. Contributions welcome!", "There is not much to do by hand. If you download the csv file and name it \"data.csv\", then you can do\r\n\r\n```latex\r\n\\documentclass{article}\r\n\\usepackage[active,tightpage]{preview}\r\n\\usepackage{pgfplots}\r\n\\pgfplotsset{compat=newest}\r\n\\PreviewEnvironment{tikzpicture} \r\n\r\n\\definecolor{tb_color_1}{RGB}{245,124,0}\r\n\\definecolor{tb_color_2}{RGB}{0,167,247}\r\n\r\n\\pgfplotscreateplotcyclelist{tb}{\r\nsemithick,tb_color_1\\\\%\r\nsemithick,tb_color_2\\\\%\r\n}\r\n\r\n\\begin{document}\r\n  \\begin{tikzpicture}\r\n    \\begin{axis}[cycle list name=tb, \r\n                 grid=both,\r\n                 grid style={solid,gray!30!white},\r\n                 axis lines=middle,\r\n                 xlabel={step},\r\n                 ylabel={value},\r\n                 x label style={at={(axis description cs:0.5,-0.1)},anchor=north},\r\n                 y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south},]\r\n      \\addplot table [x=Step, y=Value, col sep=comma] {data.csv};\r\n    \\end{axis}\r\n  \\end{tikzpicture}\r\n\\end{document}\r\n```\r\nand get\r\n![tb_latex](https://cloud.githubusercontent.com/assets/6756603/24898814/6634ede0-1e9e-11e7-9d98-3964508b88fe.png)\r\n\r\nBut indeed, it would be nice to also export the smoothed values.", "Did I now read somewhere that the download links to the data were being removed? Perhaps in some release notes? @dandelionmane have I got the wrong end of the stick regarding this?", "@dandelionmane @asimshankar \r\nMay I take a shot at it? \r\nI'm trying to export the graph in SVG format.", "Nope, we're not planning to remove the download links for scalar data. Both CSV and JSON formats will continue to exist.\r\n\r\nI've migrated this to our new repository at https://github.com/tensorflow/tensorboard/issues/32."]}, {"number": 9023, "title": "how could I processing financial datas", "body": "I want to deal with financial time series, but I do not have a clue", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9022, "title": "Softmax Regressions: Please don't talk about probabilities", "body": "[This](https://www.tensorflow.org/get_started/mnist/beginners) tutorial talks about probabilities on softmax regression. This is a common mistake. A classifier talks about its confidence, but it is not a real probability of some kind of _distribution_. Please don't teach something wrong, only to make it some kind of simple.\r\n_Confidence_ is simple enough.", "comments": ["Pardon the naivet\u00e9, but isn't the [output of a softmax function used to represent a probability distribution](https://en.wikipedia.org/wiki/Softmax_function)? ", "Classifier can be fooled, that is why you can't see them as real probabilities.\r\nIn fact, something like this a doctorand told me. But to be honest, I am lacking sources right now.", "I'm going to go ahead and close this issue out for now. If you strongly disagree, please feel free to submit a PR to fix this - and describe the justification in some detail. Thanks much!\r\n\r\n(The file to change would be: [`tensorflow/tensorflow/docs_src/get_started/mnist/beginners.md`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/mnist/beginners.md))"]}, {"number": 9021, "title": "In non-devel Docker images, positively confirm --allow-root with jupyter", "body": "to avoid an error that started to happen recently:\r\n[C 10:09:34.858 NotebookApp] Running as root is not recommended. Use\r\n--allow-root to bypass.\r\n\r\nE.g., see\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-docker-cpu/TF_DOCKER_BUILD_IS_DEVEL=NO,TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON2,label=gcs-access/361/console\r\n\r\nThis may be caused by recent changes in jupyter.", "comments": []}, {"number": 9020, "title": "tf.image.crop_and_resize not working with uint8 images", "body": "### Installation\r\n- Custom code\r\n- Pip install\r\n- *TensorFlow version*: 1.0.1\r\n- Python 2.7.9\r\n- CPU based\r\n- Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux\r\n\r\n### Problem \r\n\r\nWhen reading images with opencv and then trying to use crop_and_resize on a tensor of uint8 data type, i get the following error : \r\n\r\n```\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'CropAndResize' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; T in [DT_FLOAT]\r\n\r\n\t [[Node: generate_resized_crop = CropAndResize[T=DT_UINT8, extrapolation_value=0, method=\"bilinear\"](ExpandDims, stack, zeros_like, generate_resized_crop/crop_size)]]\r\n```\r\n\r\nHowever, when casting my tensor in float32, the program works perfectly.\r\nBut the [documentation](https://www.tensorflow.org/versions/master/api_docs/python/tf/image/crop_and_resize) specifies that it should work with uint8 datatype. \r\n\r\n### Source Code\r\n```Python\r\nimport cv2\r\nimport tensorflow as tf\r\n\r\nframe=cv2.imread('path to img.jpg',-1)\r\nraw_image=tf.placeholder(dtype=tf.uint8, shape=frame.shape, name='input_image')\r\n\r\ndef img2batch(input_image):\r\n    raw_sample_tensor_4d=tf.expand_dims(input_image, 0)\r\n    patches_top=[0,0.5]\r\n    patches_bottom =[0.25,0.75]\r\n    boxes=tf.stack([patches_top, patches_top, patches_bottom, patches_bottom], axis=1)\r\n    crops=tf.image.crop_and_resize(raw_sample_tensor_4d, boxes, box_ind=tf.zeros_like(patches_top, dtype=tf.int32), crop_size=[200,200], method=\"bilinear\", extrapolation_value=None, name=\"generate_resized_crop\")\r\n    return crops\r\n\r\nwith tf.Session() as sess:\r\n    myBatchedImage=sess.run(img2batch(raw_image), feed_dict={raw_image:frame})\r\n```\r\nProblem solved with : \r\n```Python\r\ncrops=tf.image.crop_and_resize(tf.cast(raw_sample_tensor_4d, dtype=tf.float32), boxes, box_ind=tf.zeros_like(patches_top, dtype=tf.int32), crop_size=[200,200], method=\"bilinear\", extrapolation_value=None, name=\"generate_resized_crop\")\r\n```", "comments": ["This does seem to be a documentation issue.\r\n\r\nIt seems that back in June 2016,   the initial introduction of the op added in https://github.com/tensorflow/tensorflow/commit/ea9ec0a3f938dc4db86a6d8616b7a9e62d73d49f aimed to support various types, but 7 days later this was changed to support only floats in https://github.com/tensorflow/tensorflow/commit/1d92cfcbf5c157b3e4069741ae5bdbbea6666dc5\r\n\r\n@gpapan : Does the op documentation need to be updated or do we need to register kernels for more real number types? ", "Will register more types (including uint8 for CPU) for tf.image_crop_and_resize() to resolve this. Should take a day or two to become public.", "any updates?", "It looks like a few PR were merged. Do you still see the issue on `master`?", "@Lescurel,  with your code, I got an error below:\r\n**\"Tensors in list passed to 'values' of 'Pack' Op have types [int32, <NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>, int32] that don't all match\"**\r\n\r\nI'm trying this code with the tutorial of **object detection** from [tensorflowAPI](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb).\r\n\r\nAfter detecting objects, I did try to crop and resize with this code below:\r\n`cropped = sess.run(img2batch(raw_image), feed_dict={raw_image:image_np_expanded})`\r\nthen I got the error above.\r\n\r\nI would appreciate if you could answer me. thanks."]}, {"number": 9019, "title": "Merge pull request #1 from tensorflow/master", "body": "leatest source", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 9018, "title": "R1.1", "body": "Related to issue #8191 ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed the CLA.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "TensorFlow team manages the release branches, and merging them back to the master branch whenever it is needed.\r\n\r\nWe will take care of this as a part of TF  release process.\r\nClosing this PR."]}, {"number": 9017, "title": "re2 repo: no such file when running download_dependencies.sh", "body": "Hi \r\n\r\nWhen running the download_dependencies.sh script it fail at downloading the repo at re2. \r\nsed: can't read : No such file or directory\r\nI cant find any other mention of this. I've attached the buildlog. Please advise.\r\n\r\nThanks\r\n\r\nChris", "comments": ["Without more information it is hard to provide help.\r\n\r\nWhat source version are you using?\r\nWhat exact sequence of commands did you run?\r\nWhat was the output of those commands?\r\n\r\nThanks.", "im following the instructions found here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile\r\n\r\nI've cloned into the repo and tried to run the download_dependencies.sh from tensorflow root directory as per instructions. It downloads the first few repos fine but then crashes when trying to get the tar from re2 (i included buildlog in first post). \r\n\r\nso sequence of commands:\r\n\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ntensorflow/contrib/makefile/download_dependencies.sh\r\n\r\nand then it throws up the error. \r\n\r\ni've installed homebrew and then bazel as well. not sure what else to try. I'm trying get tensorflow running on an ios app. so if there is a different way to get that going please share.\r\n\r\nthanks\r\n", "I dont' see the buildlog, are you sure you attached it or did I miss something?", "Sorry, looks like I didnt put it in. Please see attached buildlog \r\n\r\n[build_log.pdf](https://github.com/tensorflow/tensorflow/files/903613/build_log.pdf)\r\n\r\n", "I was unable to reproduce the problem using the same sequence of three commands you alluded to above. Based on the error message and build log though, it seems like the downloads succeeded and it isn't necessarily re2 but some of the [patches applied in the `download_dependencies.sh`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/download_dependencies.sh#L58) that are failing.\r\n\r\nPerhaps you can instrument the script to figure out which one is failing and debug that. Or maybe use `set -ex` instead of `set -e` at the top of the script so that it prints out every command being run.\r\n\r\n", "the output with set -ex, i cant really make sense of it:\r\n[build_log.pdf](https://github.com/tensorflow/tensorflow/files/903697/build_log.pdf)\r\n", "It seems like the output of the last build_log.pdf you attached isn't complete, it is missing the initial part. Anyway, looking at the last line, does the file `tensorflow/\r\ncontrib/makefile/downloads/eigen/Eigen/src/Core/arch/NEON/Complex.h` exist?\r\n\r\nIf not, then it seems there has been some error in extracting the eigen archive.", "Complex.h exists. And its not empty. Anything I should change in Complex.h?", "It seems this command is failing:\r\n\r\n```\r\n sed -i '' \\\r\n  -e 's#static uint32x4_t p4ui_CONJ_XOR = vld1q_u32( conj_XOR_DATA );#static uint32x4_t p4ui_CONJ_XOR; // = vld1q_u32( conj_XOR_DATA ); - Removed by script#' \\\r\n  tensorflow/contrib/makefile/downloads/eigen/Eigen/src/Core/arch/NEON/Complex.h\r\n```\r\n\r\nThis command isn't failing for me, but your log suggests its failing for you.\r\nNot sure why, but perhaps you can toy around with it and figure out what's wrong in your setup?", "I think this also depend on the OS you are running.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/configure#L38\r\n\r\nWhat is the OS you are using to run the commands?", "I'm using Mac OS Sierra 10.12.3\r\n", "@chrisw85 : Did you see my previous question in https://github.com/tensorflow/tensorflow/issues/9017#issuecomment-292312009?", "Closing due to inactivity.\r\n\r\nIf you're still running into trouble, please feel free to file a new updated issue (including the response to the question raised in the previous comment).\r\n\r\nThanks!"]}, {"number": 9016, "title": "Problem compiling for android", "body": "Compiling for android example. Not sure if its a bug \r\nAfter Trying for  tensorflow/contrib/makefile/compile_android_protobuf.sh -c\r\n\r\nchecking whether to enable maintainer-specific portions of Makefiles... yes\r\nchecking build system type... x86_64-pc-linux-gnu\r\nchecking host system type... arm-unknown-linux-androideabi\r\nchecking target system type... arm-unknown-linux-androideabi\r\nchecking for a BSD-compatible install... /usr/bin/install -c\r\nchecking whether build environment is sane... yes\r\nchecking for arm-linux-androideabi-strip... arm-linux-androideabi-strip\r\nchecking for a thread-safe mkdir -p... /bin/mkdir -p\r\nchecking for gawk... gawk\r\nchecking whether make sets $(MAKE)... yes\r\nchecking whether make supports nested variables... yes\r\nchecking whether UID '1000' is supported by ustar format... yes\r\nchecking whether GID '1000' is supported by ustar format... yes\r\nchecking how to create a ustar tar archive... gnutar\r\nchecking for arm-linux-androideabi-gcc...  arm-linux-androideabi-gcc --sysroot ../android-ndk-r14b//platforms/android-21/arch-arm\r\nchecking whether the C compiler works... no\r\nconfigure: error: in `/home/vishal/Downloads/tensorflowandroid/tensorflow/tensorflow/contrib/makefile/downloads/protobuf':\r\nconfigure: error: C compiler cannot create executables\r\nSee `config.log' for more details\r\n\r\nConfig.log is attached\r\n[config.txt](https://github.com/tensorflow/tensorflow/files/902101/config.txt)\r\n\r\nCan someone look into it\r\n\r\n\r\n\r\n", "comments": ["This seems to be an error with your Android NDK setup. Looking at `config.txt`, I see things like:\r\n\r\n```\r\n/usr/lib/gcc/arm-linux-androideabi/4.7.4/../../../../arm-linux-androideabi/bin/ld: error: cannot find -llog\r\n/usr/lib/gcc/arm-linux-androideabi/4.7.4/../../../../arm-linux-androideabi/bin/ld: error: cannot find -lz\r\n/usr/lib/gcc/arm-linux-androideabi/4.7.4/../../../../arm-linux-androideabi/bin/ld: error: cannot find -lgnustl_static\r\n```\r\n\r\nsuggesting that your NDK installation is incomplete or messed up as it is missing these libraries.\r\n\r\nClosing this out since this doesn't seem to be an TensorFlow bug."]}, {"number": 9015, "title": "build tensorflow failed,may be the problem of compiler?", "body": "### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\n- *TensorFlow installed from source *:\r\n- *TensorFlow version*:1.0.1\r\n- *Bazel version :0.4.5\r\n- *CUDA/cuDNN version*:8.0/5.1\r\n- *GPU Model and Memory*:GeForce 920M total memory 2048 MB\r\n### Describe the problem clearly\r\nhi ,I am attempting to build the tensorflow ,but it can't work ,it seems like relates to compiler of jit.\r\nbut why can't i see the TF_CUDA_VERSION='' and TF_CUDNN_VERSION='' ? I did install CUDA and CUDNN and I really add path in .bashrc .what's wrong with it? what can I do ?\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n`bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\nWARNING: /home/EI/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.\r\nWARNING: /home/EI/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.\r\nINFO: Found 1 target...\r\nERROR: /home/EI/tensorflow/tensorflow/compiler/jit/BUILD:96:1: C++ compilation of rule '//tensorflow/compiler/jit:common' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/EI/.cache/bazel/_bazel_EI/2e0782785985dd9cb1b5d0cb62ad2606/execroot/tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    LD_LIBRARY_PATH='/usr/local/cuda-8.0/lib64:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:$JAVA_HOME/bin:/snap/bin:/usr/local/cuda/bin:/usr/local/cuda/lib64' \\\r\n    PATH='/usr/local/cuda-8.0/bin:/home/EI/anaconda3/bin:/home/EI/anaconda2/bin:/home/EI/bin:/home/EI/.local/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:$JAVA_HOME/bin:/snap/bin:/usr/local/cuda/bin:JAVA_HOME/bin:/usr/lib/Java/jdk1.8.0_121/jre/bin:/home/EI/anaconda3/bin:/home/EI/anaconda2/bin:/usr/local/cuda/bin:/home/EI/bin:JAVA_HOME/bin:/usr/lib/Java/jdk1.8.0_121/jre/bin' \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2 \\\r\n    TF_CUDA_VERSION='' \\\r\n    TF_CUDNN_VERSION='' \\\r\n    TF_NEED_CUDA=1 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/compiler/jit/_objs/common/tensorflow/compiler/jit/defs.pic.d '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/compiler/jit/_objs/common/tensorflow/compiler/jit/defs.pic.o' -fPIC -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-py3-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/compiler/jit/defs.cc -o bazel-out/local_linux-py3-opt/bin/tensorflow/compiler/jit/_objs/common/tensorflow/compiler/jit/defs.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ngcc: error trying to exec 'cc1plus': execvp: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 4.918s, Critical Path: 3.31s `\r\n\r\n", "comments": ["As mentioned in #8973, can you execute the failing command to determine what the error is?", "Closing this out due to inactivity. If you're still running into trouble, please feel free to file a new issue with all relevant details (including what was suggested in the previous comment).\r\n\r\nThanks.", "This usually means you compiled tensorflow with an older version of something, like in OSX for instance, the command line tools. For me this involved opening up the CROSSTOOL file and changing the entry for /Library/Developer/CommandLineTools/usr/lib/clang/8.0.0/include to /Library/Developer/CommandLineTools/usr/lib/clang/8.1.0/include. For people with CUDA issues, it's one of those lines and for you upgrading cuda support could have caused the issue."]}, {"number": 9014, "title": "Freshly built tfcompile core dumps", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: No\r\n- *TensorFlow installed from (source or binary)?*: Source\r\n- *TensorFlow version*: docker image: \"tensorflow/tensorflow:nightly-devel\" - `git describe` = 0.6.0-16017-ga9b7946 (aka: `docker-pullable://tensorflow/tensorflow@sha256:5b568f7dd9890bb0b86101bb27779c539f608724c7d4ecf4fdfbbab289bc29de`)\r\n- *Bazel version (if compiling from source)*: Build label: 0.4.5\r\n- *CUDA/cuDNN version*: - \r\n- *GPU Model and Memory*: - \r\n- *Exact command to reproduce*: cd /tensorflow; bazel build tensorflow/compiler/aot:tfcompile ; bazel-bin/tensorflow/compiler/aot/tfcompile \r\n\r\n### Describe the problem clearly\r\nI'm trying to use tfcompile, when I build it in a Pod on my kubernetes cluster, the resulting binary exits with `Aborted (core dumped)` after displaying the help message.\r\n\r\n### Source Code / Logs\r\nI create a pod like this:\r\n```\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  labels:\r\n    name: tfcompile\r\n  name: tfcompile\r\nspec:\r\n  containers:\r\n  - args:\r\n    - sh\r\n    - -c\r\n    - cd /tensorflow; bazel build tensorflow/compiler/aot:tfcompile ; sleep 864000\r\n    image: tensorflow/tensorflow:nightly-devel\r\n    name: tfcompile\r\n    resources:\r\n      limits:\r\n        cpu: \"1\"\r\n        memory: 16Gi\r\n      requests:\r\n        cpu: \"1\"\r\n        memory: 16Gi\r\n```\r\n\r\nand then when the build is finished, I execute into the pod and try the resulting binary: `bazel-bin/tensorflow/compiler/aot/tfcompile`\r\n\r\nfull output:\r\n\r\n```\r\nroot@tfcompile-1266760932-s2dht:/tensorflow# bazel-bin/tensorflow/compiler/aot/tfcompile                                         \r\n2017-04-06 08:08:02.798285: F tensorflow/compiler/aot/tfcompile_main.cc:136] Check failed: argc == 1 && !flags.config.empty() && (flags.dump_fetch_nodes || (!flags.graph.empty() && !flags.entry_point.empty())) \r\ntfcompile performs ahead-of-time compilation of a TensorFlow graph,\r\nresulting in an object file compiled for your target architecture, and a\r\nheader file that gives access to the functionality in the object file.\r\nA typical invocation looks like this:\r\n\r\n   $ tfcompile --graph=mygraph.pb --config=myfile.pbtxt\r\n\r\nusage: bazel-bin/tensorflow/compiler/aot/tfcompile\r\nFlags:\r\n\t--graph=\"\"                       \tstring\tInput GraphDef file.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\r\n\t--config=\"\"                      \tstring\tInput file containing Config proto.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\r\n\t--dump_fetch_nodes=false         \tbool\tIf set, only flags related to fetches are processed, and the resulting fetch nodes will be dumped to stdout in a comma-separated list.  Typically used to format arguments for other tools, e.g. freeze_graph.\r\n\t--debug_dir=\"\"                   \tstring\tSpecifies a directory to dump debugging information, including rewritten graphs and the XLA HLO module.\r\n\t--target_triple=\"x86_64-pc-linux\"\tstring\tTarget platform, similar to the clang -target flag.  The general format is <arch><sub>-<vendor>-<sys>-<abi>.  http://clang.llvm.org/docs/CrossCompilation.html#target-triple.\r\n\t--target_cpu=\"\"                  \tstring\tTarget cpu, similar to the clang -mcpu flag.  http://clang.llvm.org/docs/CrossCompilation.html#cpu-fpu-abi\r\n\t--target_features=\"\"             \tstring\tTarget features, e.g. +avx2, +neon, etc.\r\n\t--entry_point=\"\"                 \tstring\tName of the generated function.  If multiple generated object files will be linked into the same binary, each will need a unique entry point.\r\n\t--cpp_class=\"\"                   \tstring\tName of the generated C++ class, wrapping the generated function.  The syntax of this flag is [[<optional_namespace>::],...]<class_name>.  This mirrors the C++ syntax for referring to a class, where multiple namespaces may precede the class name, separated by double-colons.  The class will be generated in the given namespace(s), or if no namespaces are given, within the global namespace.\r\n\t--out_object=\"out.o\"             \tstring\tOutput object file name.\r\n\t--out_header=\"out.h\"             \tstring\tOutput header file name.\r\n\t--xla_debug_cpu_dump_ir=\"\"       \tstring\tDump IR, before optimizations to a path\r\n\t--xla_cpu_llvm_opt_level=2       \tint32\tThe LLVM optimization level for the CPU XLA backend. Valid range is from 0 to 3 where 0 means no optimizations.\r\n\t--xla_cpu_llvm_cl_opts=\"\"        \tstring\tComma-separated list of command line options to pass to LLVM.\r\n\t--xla_cpu_embed_ir=false         \tbool\tEmbed the LLVM IR module string in the resultant CpuExecutable.\r\n\t--xla_cpu_parallel=false         \tbool\tUse the multi-threaded CPU backend.\r\n\t--xla_cpu_use_eigen=true         \tbool\tUse Eigen for matrix multiply on the CPU platform. This is a useful hack for performance comparisons against XLA's implementation.\r\n\t--xla_cpu_multi_thread_eigen=true\tbool\tWhen generating calls to Eigen for matmul and conv, should single or multi-threaded eigen be used? Only used when --xla_cpu_use_eigen is true.\r\n\r\nAborted (core dumped)\r\nroot@tfcompile-1266760932-s2dht:/tensorflow#\r\n```\r\n\r\nI've actually been trying this sporadically with different nightly/release builds for the past few weeks, hoping that this would be resolved, but it still does not work, so here is my GH issue ;) ", "comments": ["The error message could probably be improved, but looking at the first line of output:\r\n\r\n```\r\nCheck failed: argc == 1 && !flags.config.empty() && (flags.dump_fetch_nodes || (!flags.graph.empty() && !flags.entry_point.empty())) \r\n```\r\n\r\nit means that the tool is not being invoked correctly. As per the usage message, you need to provide the `--graph` and `--config` flags.\r\n", "ok, so I do the following (per https://www.tensorflow.org/performance/xla/tfcompile):\r\n\r\n```\r\npython  tensorflow/compiler/aot/tests/make_test_graphs.py  --out_dir=.\r\ncat > myfile.pbtxt <<EOT\r\n# Each feed is a positional input argument for the generated function.  The order\r\n# of each entry matches the order of each input argument.  Here \u201cx_hold\u201d and \u201cy_hold\u201d\r\n# refer to the names of placeholder nodes defined in the graph.\r\nfeed {\r\n  id { node_name: \"x_hold\" }\r\n  shape {\r\n    dim { size: 2 }\r\n    dim { size: 3 }\r\n  }\r\n}\r\nfeed {\r\n  id { node_name: \"y_hold\" }\r\n  shape {\r\n    dim { size: 3 }\r\n    dim { size: 2 }\r\n  }\r\n}\r\n\r\n# Each fetch is a positional output argument for the generated function.  The order\r\n# of each entry matches the order of each output argument.  Here \u201cx_y_prod\u201d\r\n# refers to the name of a matmul node defined in the graph.\r\nfetch {\r\n  id { node_name: \"x_y_prod\" }\r\n}\r\nEOT\r\nbazel-bin/tensorflow/compiler/aot/tfcompile --graph=test_graph_tfmatmul.pb --config=myfile.pbtxt\r\n```\r\nand I get the same result:\r\n```\r\nroot@tfcompile-1266760932-s2dht:/tensorflow# bazel-bin/tensorflow/compiler/aot/tfcompile --graph=test_graph_tfmatmul.pb --config=myfile.pbtxt\r\n2017-04-06 08:33:28.828508: F tensorflow/compiler/aot/tfcompile_main.cc:136] Check failed: argc == 1 && !flags.config.empty() && (flags.dump_fetch_nodes || (!flags.graph.empty() && !flags.entry_point.empty())) \r\ntfcompile performs ahead-of-time compilation of a TensorFlow graph,\r\nresulting in an object file compiled for your target architecture, and a\r\nheader file that gives access to the functionality in the object file.\r\nA typical invocation looks like this:\r\n\r\n   $ tfcompile --graph=mygraph.pb --config=myfile.pbtxt\r\n\r\nusage: bazel-bin/tensorflow/compiler/aot/tfcompile\r\nFlags:\r\n\t--graph=\"\"                       \tstring\tInput GraphDef file.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\r\n\t--config=\"\"                      \tstring\tInput file containing Config proto.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\r\n\t--dump_fetch_nodes=false         \tbool\tIf set, only flags related to fetches are processed, and the resulting fetch nodes will be dumped to stdout in a comma-separated list.  Typically used to format arguments for other tools, e.g. freeze_graph.\r\n\t--debug_dir=\"\"                   \tstring\tSpecifies a directory to dump debugging information, including rewritten graphs and the XLA HLO module.\r\n\t--target_triple=\"x86_64-pc-linux\"\tstring\tTarget platform, similar to the clang -target flag.  The general format is <arch><sub>-<vendor>-<sys>-<abi>.  http://clang.llvm.org/docs/CrossCompilation.html#target-triple.\r\n\t--target_cpu=\"\"                  \tstring\tTarget cpu, similar to the clang -mcpu flag.  http://clang.llvm.org/docs/CrossCompilation.html#cpu-fpu-abi\r\n\t--target_features=\"\"             \tstring\tTarget features, e.g. +avx2, +neon, etc.\r\n\t--entry_point=\"\"                 \tstring\tName of the generated function.  If multiple generated object files will be linked into the same binary, each will need a unique entry point.\r\n\t--cpp_class=\"\"                   \tstring\tName of the generated C++ class, wrapping the generated function.  The syntax of this flag is [[<optional_namespace>::],...]<class_name>.  This mirrors the C++ syntax for referring to a class, where multiple namespaces may precede the class name, separated by double-colons.  The class will be generated in the given namespace(s), or if no namespaces are given, within the global namespace.\r\n\t--out_object=\"out.o\"             \tstring\tOutput object file name.\r\n\t--out_header=\"out.h\"             \tstring\tOutput header file name.\r\n\t--xla_debug_cpu_dump_ir=\"\"       \tstring\tDump IR, before optimizations to a path\r\n\t--xla_cpu_llvm_opt_level=2       \tint32\tThe LLVM optimization level for the CPU XLA backend. Valid range is from 0 to 3 where 0 means no optimizations.\r\n\t--xla_cpu_llvm_cl_opts=\"\"        \tstring\tComma-separated list of command line options to pass to LLVM.\r\n\t--xla_cpu_embed_ir=false         \tbool\tEmbed the LLVM IR module string in the resultant CpuExecutable.\r\n\t--xla_cpu_parallel=false         \tbool\tUse the multi-threaded CPU backend.\r\n\t--xla_cpu_use_eigen=true         \tbool\tUse Eigen for matrix multiply on the CPU platform. This is a useful hack for performance comparisons against XLA's implementation.\r\n\t--xla_cpu_multi_thread_eigen=true\tbool\tWhen generating calls to Eigen for matmul and conv, should single or multi-threaded eigen be used? Only used when --xla_cpu_use_eigen is true.\r\n\r\nAborted (core dumped)\r\nroot@tfcompile-1266760932-s2dht:/tensorflow# \r\n```", "The error messages can be improved :), but again, looking at the `Check failed` line, it seems that you also need to specify `--entry_point`. ", "Ok, got it working now with the following commandline:\r\n\r\n`bazel-bin/tensorflow/compiler/aot/tfcompile --graph=test_graph_tfmatmul.pb --config=myfile.pbtxt --entry_point=\"test_graph_tfmatmul\" --cpp_class=\"foo::bar::MatMulComp\"`\r\n\r\nThanks!"]}, {"number": 9013, "title": "Add hint to distinguish between assignment ops and non-assignment ops", "body": "This allows XLA backends to decide whether an op is doing an in-place\r\nmodification of a tensor. a backend can use this to ensure assignment\r\noperations do not move data.", "comments": ["Can one of the admins verify this patch?", "gentle ping @hawkinsp ", "There is a discussion thread about this PR on the xla-dev Google group:\r\nhttps://groups.google.com/forum/#!forum/xla-dev\r\nand that seems like a better forum for the wider design discussion.\r\n\r\n(It seems unlikely we will merge this PR as is.)", "Closing then. Feel free to reopen or push a new PR that addresses the main concerns."]}, {"number": 9012, "title": "BiasGradOp mistakenly put on CPU", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\ncustom code\r\n- *TensorFlow installed from (source or binary)?*: from binary\r\n- *TensorFlow version*: 1.0.1\r\n- *Bazel version (if compiling from source)*: N/A\r\n- *CUDA/cuDNN version*: CUDA 8.0, cuDNN v5.1\r\n- *GPU Model and Memory*: GTX 1080, 8GB\r\n- *Exact command to reproduce*:\r\nHere is a sample script in Python that can reproduce the problem:\r\n\r\n```import numpy as np\r\nimport tensorflow as tf\r\nimport numpy as np\r\nly = tf.layers\r\n\r\n\r\ndef lrelu(x, leak=0.2, name=\"lrelu\"):\r\n    with tf.variable_scope(name):\r\n        f1 = 0.5 * (1 + leak)\r\n        f2 = 0.5 * (1 - leak)\r\n        return f1 * x + f2 * tf.abs(x)\r\n        # return tf.maximum(leak*x, x)\r\n\r\nx = np.ones([16, 3, 32, 32], dtype=np.float32)\r\n\r\nwith tf.device('/gpu:0'):\r\n    input = tf.placeholder(tf.float32, shape=[16, 3, 32, 32])\r\n    output = ly.conv2d(input, 3, kernel_size=1, data_format='channels_first',\r\n                       strides=1, activation=lrelu)\r\n    loss = tf.gradients(input - output, input)[0]\r\n    optimizer = tf.train.AdamOptimizer()\r\n    gradients = optimizer.compute_gradients(loss)\r\n    grad_op = optimizer.apply_gradients(gradients)\r\n\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    out = sess.run(grad_op, feed_dict={input: x})\r\n```\r\n\r\n### Describe the problem clearly\r\nI am on Ubuntu 16.04. While running the above script, despite the device has been specified to be GPU, tensorflow still try to do the `BiasGradOp` on CPU and will cause an error because of the data format. If I change the implementation of `lrelu()` to `return tf.maximum(leak*x, x)` then the problem goes away.\r\n\r\n### Source Code / Logs\r\nHere is the output from console:\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.797\r\npciBusID 0000:01:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.21GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\r\nE tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: CPU BiasGradOp only supports NHWC.\r\n\t [[Node: gradients_1/conv2d/BiasAdd_grad/BiasAddGrad = BiasAddGrad[T=DT_FLOAT, data_format=\"NCHW\", _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients_1/gradients/conv2d/lrelu/Abs_grad/Sign_grad/zeros)]]\r\nE tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: CPU BiasGradOp only supports NHWC.\r\n\t [[Node: gradients_1/conv2d/BiasAdd_grad/BiasAddGrad = BiasAddGrad[T=DT_FLOAT, data_format=\"NCHW\", _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients_1/gradients/conv2d/lrelu/Abs_grad/Sign_grad/zeros)]]\r\n```\r\n", "comments": ["This does indeed seem like a bug, thanks for the nice reproducible example, it really helps.\r\n\r\nWill dig into it some.\r\n", "@asimshankar  hey asim, are you still working on this?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "I'm very sorry for the delayed response.\r\nI believe this is fixed in TensorFlow 1.4.0 (the same code snippet doesn't not error out).\r\nPlease re-open if I'm mistaken. Once again, apologies for the delay."]}, {"number": 9011, "title": "compilation from sources, is broken too", "body": "```\r\n# bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n.\r\nINFO: Found 1 target...\r\n[224 / 2,346] Compiling external/pcre/pcre_byte_order.c [for host]\r\n\r\nServer terminated abruptly (error code: 14, error message: '', log file: '/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/server/jvm.out')\r\n\r\n# cat /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/server/jvm.out\r\n#\r\n```\r\n\r\nUbuntu Xenial, all times.\r\n", "comments": ["There is not enough information provided in this issue report (version of bazel being used, version of TF source code, platform details etc.) to help. It seems though that for some reason the bazel server is crashing. Is the log actually empty?\r\n\r\nClosing this out. If you have more information that suggests a bug in TensorFlow, please feel free to reopen - though do add more information (ideally all that is asked for in the new issue template)."]}, {"number": 9010, "title": "I hope pre-1.1 `tf.contrib.seq2seq` is kept somewhere for legacy support", "body": "I am disappointed by the fact that pre-1.1 `seq2seq` has been completely scraped in the latest build and has been replaced by some object-oriented redesign that seems half-finished right now. We have tonnes of codes that are based on pre-1.1 `seq2seq` interface, so this change would completely break our code base. But we can't just stick to old versions because there are some improvements in other areas that we desperately need also. Why not keep the original design somewhere for legacy (`tf.contrib.legacy_seq2seq` already exists, so need to think of another name), so those who heavily rely on the module do not have to re-implement the entire codebase? Since 1.1 is still far away from being officially released, I hope a right decision is made.", "comments": ["@ebrevdo : Any comments here? \r\n\r\n@kaniblu : 1.1.0 is in release candidate and should be available shortly (or you can try the release candidates). Does that work for you?", "There was an original tf.contrib.seq2seq which was moved to\ntf.contrib.legacy_seq2seq; and there were some functions in the new\ntf.contrib.seq2seq that were problematic and we decided not to continue\nsupporting them in favor of the new OO-based API.\n\nOn Thu, Apr 6, 2017 at 4:10 AM, Asim Shankar <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> : Any comments here?\n>\n> @kaniblu <https://github.com/kaniblu> : 1.1.0 is in release candidate and\n> should be available shortly (or you can try the release candidates). Does\n> that work for you?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9010#issuecomment-292141554>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim83wLFRrodY17eL2r5HUV9FpiXBqks5rtMgLgaJpZM4M1KGi>\n> .\n>\n", "(also a side note @kaniblu : APIs under `tf.contrib` are not guaranteed to be stable between releases, unlike other APIs - see [TensorFlow Version Semantics](https://www.tensorflow.org/programmers_guide/version_semantics) for details)"]}, {"number": 9009, "title": "Wrap python code into a tf operation", "body": "\r\nI have described my issue in the keras group [issue 6163](https://github.com/fchollet/keras/issues/6163) but was re-directed here. I am trying to wrap a non-tf function (actually a class) into a tf operation to use in a custom keras layer. When using py_func it always returns  \"ValueError: None values not supported.\" Is it generally possible to wrap an arbitrary piece of python code, which takes a list of numpy arrays as input and delivers a list of numpy arrays , into a tf op?\r\n\r\n\r\n- *I have written custom code.*\r\n- *TensorFlow installed from binary.*\r\n- *TensorFlow version is 1.0.1*\r\n- *No CUDA/cuDNN used.*\r\n- *Currently no GPU.*\r\n\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nThat said, yes it seems [`tf.py_func`](https://www.tensorflow.org/api_docs/python/tf/py_func) is what you want.", "Did you find a solution to this?\r\n"]}, {"number": 9008, "title": "Instalation is broken", "body": "```\r\n# pip install tensorflow\r\nCollecting tensorflow\r\n  Downloading tensorflow-1.0.1-cp27-cp27mu-manylinux1_x86_64.whl (44.1MB)\r\n    99% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44.1MB 28.6MB/s eta 0:00:01Killed\r\n```\r\n\r\nUbuntu Xenial. all times.", "comments": ["There is not enough information in this bug report to help diagnose your issue. Closing this out (it seems like there is some issue with your machine setup that `pip install` crashes before finishing the download).\r\n\r\nClosing this out. If you have more information that suggests a bug in TensorFlow, feel free to reopen after adding all the details requested for in the new issue template."]}, {"number": 9007, "title": "Takes more than 2min to load a small model.", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: Y\r\n- *TensorFlow installed from (source or binary)?*:  Installing with native pip\r\n- *TensorFlow version*:   libcurand.so.8.0 locally 1.0.1\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*:\r\n- *GPU Model and Memory*:\r\n- *Exact command to reproduce*:\r\n\r\n### Describe the problem clearly\r\nAfter shows following, the program waiting for more than 2min to load a small model.  \r\n\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\r\n \r\nAnd at the end it shows like this:\r\n\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\r\nmodel loaded!\r\n\r\n\r\n### Source Code / Logs\r\n`\r\n# If you have already created the dataset:\r\nif os.path.exists('train_data.npy'):\r\n    train_data = np.load('train_data.npy')\r\nelse:\r\n    train_data = create_train_data()\r\n\r\nif os.path.exists('test_data.npy'):\r\n    test_data = np.load('test_data.npy')\r\nelse:\r\n    test_data = process_test_data()\r\n\r\n# Convlotion model----------------\r\nconvnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\r\n# ----------------\r\nconvnet = conv_2d(convnet, 32, 5, activation='relu')\r\nconvnet = max_pool_2d(convnet, 5)\r\n# ----------------\r\nconvnet = conv_2d(convnet, 64, 5, activation='relu')\r\nconvnet = max_pool_2d(convnet, 5)\r\n# ----------------\r\nconvnet = conv_2d(convnet, 32, 5, activation='relu')\r\nconvnet = max_pool_2d(convnet, 5)\r\n# ----------------\r\nconvnet = conv_2d(convnet, 64, 5, activation='relu')\r\nconvnet = max_pool_2d(convnet, 5)\r\n# ----------------\r\nconvnet = conv_2d(convnet, 32, 5, activation='relu')\r\nconvnet = max_pool_2d(convnet, 5)\r\n# ----------------\r\nconvnet = conv_2d(convnet, 64, 5, activation='relu')\r\nconvnet = max_pool_2d(convnet, 5)\r\n# ----------------\r\nconvnet = fully_connected(convnet, 1024, activation='relu')\r\nconvnet = dropout(convnet, 0.8)\r\n# ----------------\r\nconvnet = fully_connected(convnet, 512, activation='relu')\r\nconvnet = dropout(convnet, 0.6)\r\n# ----------------\r\nconvnet = fully_connected(convnet, 2, activation='softmax')\r\nconvnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\r\n# ----------------\r\nmodel = tflearn.DNN(convnet, tensorboard_dir='log')\r\n\r\nif os.path.exists('{}.meta'.format(MODEL_NAME)):\r\n    model.load(MODEL_NAME)\r\n    print('model loaded!')\r\nelse:\r\n    print \"Nothing loaded\"\r\n\r\ntrain = train_data[:-500]\r\ntest = train_data[-500:]\r\n# print train\r\n\r\n`\r\n", "comments": ["The code provided doesn't seem self contained, so it is hard to be able to help (for example, it isn't clear to me which module functions like `fully_connected`, `regression` etc. are from).\r\n\r\nFrom the line `model = tflearn.DNN(convnet, tensorboard_dir='log')`, it seems you're using `tflearn` from www.tflearn.org. The `tflearn` package is a separate project not maintained by us. Perhaps you need to file a detailed issue with them?\r\n\r\nI'm closing this out since it isn't clear whether this is an issue with your use of `tflearn` or with `tensorflow`. Feel free to reopen if you have information that pinpoints an issue with TensorFlow. Thanks!", "Thanks"]}, {"number": 9006, "title": "wer is different on test ", "body": "\r\nWhen I use the distributed tensorflow and keras combined training, each epoch will be saved on the worker0 model for the keras model, but on the validation set, before saving the model, its wer is 0.17, when saved as keras model, I use a separate program to load keras model, using the same proof set, why its wer becomes 0.22", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9005, "title": "fix windows gpu build by disabling nccl which currently will not compile/work on windows", "body": "Windows gpu builds got broken because nccl was include in cmake (https://github.com/tensorflow/tensorflow/commit/08a3e36c97a644377c07d39d6c707d1abfb2c394). \r\nnccl currently does not compile/work on windows (unfortunate). To make nccl compile/work is more work so I disable the nccl op from windows for now to make the gpu build happy. There is a PR (https://github.com/NVIDIA/nccl/pull/31) for nccl that makes it mostly work on windows ... watching that and if merged we can try to make this work.\r\n", "comments": ["Can one of the admins verify this patch?", "what happend, this bug should be fixed earlier", "sorry, jenkins reported it yesterday but we did not get to it until today.", "CC @cwhipkey", "Does accessing tf.contrib on windows still work with this change?\r\n\r\n(in the change that enabled nccl in cmake, it was also imported by contrib/__init__.py - and that import may fail now if the op library can't be loaded).\r\n", "yes, you can even import nccl ... doesn't throw until you call into the .so.\r\nimport tensorflow.contrib.nccl as nccl\r\ndir(nccl)\r\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'all_max', 'all_min', 'all_prod', 'all_sum', 'broadcast']\r\n\r\nAll unit tests are passing for gpu as well.\r\n\r\nThe reason why it doesn't throw is here: windows don't have all the loadable ops yet so the loader has a special case for windows to return None instead of throwing:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/util/loader.py#L52\r\n", "Thansk for the fix!", "Jenkins, test this please."]}, {"number": 9004, "title": "Use html5lib version that bleach wants", "body": "Fixes #8971\r\n\r\ncc: @dandelionmane ", "comments": []}, {"number": 9003, "title": "set_random_seed bug", "body": "python2, TF 1.0.0\r\npython3 TF 1.0.1\r\nCUDA8, cudnn 5ish\r\ngtx 980ti\r\n\r\nprogram A:\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.set_random_seed(0)\r\nbiases = tf.Variable(tf.random_normal([1], seed=0))\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nprint(sess.run(biases))\r\n\r\n```\r\nprogram B:\r\n```\r\nimport tensorflow as tf\r\n\r\nbiases = tf.Variable(tf.random_normal([1], seed=0))\r\ntf.set_random_seed(0)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nprint(sess.run(biases))\r\n```\r\n\r\n### Program B outputs the same value at every run, program A outputs different values at each run. I  hope this is not expected behaviour.\r\n\r\n### logs for python2 and TF 1.0.0\r\nprogram A logs:\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 980 Ti\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2405\r\npciBusID 0000:01:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 4.92GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\r\n[-1.33033025]\r\n```\r\n\r\nprogram B logs:\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 980 Ti\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2405\r\npciBusID 0000:01:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 4.92GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\r\n[-0.39915758]\r\n```\r\n", "comments": ["This is noted as fixed in the 1.1.0rc1 release notes:\r\n\r\n> Fixed `tf.set_random_seed(0)` to be deterministic for all ops.", "Thanks @taion for pointing that out.\r\nYes, this was fixed by https://github.com/tensorflow/tensorflow/commit/e9786df5e89f0345b2eb32d688c7be31c5259ba0 and will be fixed in 1.1.0 (and is fixed in 1.1.0-rc1)\r\n"]}, {"number": 9002, "title": "Unable to compile TF, CUBLAS_GEMM_ALGO? not declared in scope", "body": "- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: no\r\n- *TensorFlow installed from (source or binary)?*: source\r\n- *TensorFlow version*: 1.1.0rc1 (master as of April 5, 2017)\r\n- *Bazel version (if compiling from source)*: 0.4.5\r\n- *CUDA/cuDNN version*: 8/5.1\r\n- *GPU Model and Memory*: Titan X (PASCAL) 12GB\r\n- *Exact command to reproduce*: `bazel build -c opt --copt=-mavx --copt=-mfma --copt=-mfpmath=both --config=cuda -k //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nHere's the compile config:\r\n\r\n```txt\r\n$ ./configure \r\nPlease specify the location of python. [Default is /path/to/anaconda2/bin/python]: \r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] n\r\njemalloc disabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n/path/to/python/site-packages\r\nPlease input the desired Python library path to use.  Default is [/path/to/python/site-packages]\r\n/path/to/python/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N] \r\nnvcc will be used as CUDA compiler\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /opt/gcc/4.9.2/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \r\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /opt/cuda-8.0\r\nPlease specify the cuDNN version you want to use. [Leave empty to use system default]: \r\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /opt/cuda-8.0]: /opt/cudnn-8.0\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 3.5,5.2,6.1\r\nWarning: ignoring LD_PRELOAD in environment.\r\n............\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\n\r\n```\r\n\r\n### Describe the problem clearly\r\nCentOS 6.6, compiling using instructions from https://github.com/tensorflow/tensorflow/issues/110#issuecomment-274586644 gives an error as mentioned later.\r\nI have been able to compile TF upto 1.0.1 successfully on the same machine.\r\nAlso, same issue was mentioned in https://github.com/tensorflow/tensorflow/issues/8790#issuecomment-290094265, but I'm not sure if the OP opened an issue \r\n\r\n### Source Code / Logs\r\n\r\n```txt\r\nERROR: /path/to/tensorflow/stream_executor/BUILD:39:1: Couldn't build file tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_blas.pic.o: C++ compila\r\ntion of rule '//tensorflow/stream_executor:cuda_platform' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D\r\n_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 118 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In member function 'virtual bool perftools::gputools::cuda::CUDABlas::GetBlasGemmAlgorithms(std::vector<long long int>*)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:9: error: 'CUBLAS_GEMM_ALGO5' was not declared in this scope\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n         ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:28: error: 'CUBLAS_GEMM_ALGO6' was not declared in this scope\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n                            ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:47: error: 'CUBLAS_GEMM_ALGO7' was not declared in this scope\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n                                               ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:64: error: unable to deduce 'std::initializer_list<_Tp>&&' from '{CUBLAS_GEMM_DFALT, CUBLAS_GEMM_ALGO0, CUBLAS_GEMM_ALGO1, CUBLAS_GEMM_ALGO2, CUBLAS_GEMM_ALGO3, CUBLAS_GEMM_ALGO4, <express\r\nion error>, <expression error>, <expression error>}'\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n```", "comments": ["This might be a problem with your CUDA installation. All these constants should be defined (in your case) in `/opt/cuda-8.0/include/cublas_api.h` which is included by `tensorflow/stream_executor/cuda/cuda_blas.cc`.\r\n\r\nIf `grep CUBLAS_GEMM_ALGO5 /opt/cuda-8.0/include/cublas_api.h` does not work for you, then it would appear to be a problem with your CUDA library installation. Does it?", "Had a similar a problem, I think they were missing from the first CUDA 8.0 release and then included in 8.0.44+.", "Yeah, those constants are not defined. I'll try to get my CUDA updated and see if that fixes the problem.", "Closing this out since the original problem seems to have been correctly diagnosed to missing symbols in your CUDA installation.", "Just to confirm: the issue was as pointed out above and upgrading to CUDA 8.0.61 from 8.0.21 fixed it. Thanks!", "@rohitgirdhar I'm running into the same issue. Just to double-check, you fixed it by downgrading from CUDA 8.0.61  to 8.0.21?", "OK, I got it working by upgrading to 8.0.61, looks like TF version 1.0.1 was the last version that supported 8.0.21\r\n\r\nhttps://github.com/yaroslavvb/tensorflow-community-wheels/issues/21", "Yes, I upgraded to 8.0.61", "i get the same error with \r\nCuda compilation tools, release 8.0, V8.0.61\r\nERROR: /path/to//tensorflow/stream_executor/BUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 121 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In member function 'virtual bool perftools::gputools::cuda::CUDABlas::GetBlasGemmAlgorithms(std::vector<long long int>*)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:9: error: 'CUBLAS_GEMM_ALGO5' was not declared in this scope\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n         ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:28: error: 'CUBLAS_GEMM_ALGO6' was not declared in this scope\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n                            ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:47: error: 'CUBLAS_GEMM_ALGO7' was not declared in this scope\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n                                               ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:64: error: unable to deduce 'std::initializer_list<_Tp>&&' from '{CUBLAS_GEMM_DFALT, CUBLAS_GEMM_ALGO0, CUBLAS_GEMM_ALGO1, CUBLAS_GEMM_ALGO2, CUBLAS_GEMM_ALGO3, CUBLAS_GEMM_ALGO4, <expression error>, <expression error>, <expression error>}'\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n\r\nwith tensorflow r1.1 could anyone help please\r\n", "Trying to install tensorflow-gpu 1.2 from source on Ubuntu-16.04.\r\nbazel version: 0.5.3\r\nCuda compilation tools, release 8.0, V8.0.61\r\nCUDNN_MAJOR  5  CUDNN_MINOR  1  CUDNN_PATCHLEVEL  5\r\ngcc version: 4.9.3\r\nSame error as  @sounakdey .\r\n__Note:__ I have made one change to __third_party/gpus/cuda_configure.bzl__ as mentioned in #11871 .\r\nAny solutions?", "@ameya sorry i did not find a solution. If you get one can you please post\n\nOn Fri, Aug 11, 2017 at 5:39 PM, Ameya <notifications@github.com> wrote:\n\n> Trying to install tensorflow-gpu 1.2 from source on Ubuntu-16.04.\n> bazel version: 0.5.3\n> Cuda compilation tools, release 8.0, V8.0.61\n> CUDNN_MAJOR 5 CUDNN_MINOR 1 CUDNN_PATCHLEVEL 5\n> gcc version: 4.9.3\n> Same error as @sounakdey <https://github.com/sounakdey> .\n> *Note:* I have made one change to third_party/gpus/cuda_configure.bzl as\n> mentioned in #11871\n> <https://github.com/tensorflow/tensorflow/issues/11871> .\n> Any solutions?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9002#issuecomment-321846057>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIPZy51WE_Zba9HTv6mHdniUCIlaM4cbks5sXHWXgaJpZM4M06Gk>\n> .\n>\n", "On Ubuntu 16.04 with gcc5\r\n\r\n./configure \r\nYou have bazel 0.5.3 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N] \r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] \r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] \r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N]\r\nnvcc will be used as CUDA compiler\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8\r\nPlease specify the location where CUDA 8 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nInvalid path to CUDA 8 toolkit. /usr/local/cuda/lib64/libcudart.so.8 cannot be found\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8\r\nPlease specify the location where CUDA 8 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nInvalid path to CUDA 8 toolkit. /usr/local/cuda/lib64/libcudart.so.8 cannot be found\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7.0.1\r\nPlease specify the location where cuDNN 7.0.1 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/lib/x86_64-linux-gnu/\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"6.1\"]:\r\nDo you wish to build TensorFlow with MPI support? [y/N]\r\nMPI support will not be enabled for TensorFlow\r\nConfiguration finished\r\n\r\nBUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1)\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In instantiation of 'cudnnStatus_t perftools::gputools::cuda::wrap::WrapperShim__cudnnSetRNNDescriptor::operator()(perftools::gputools::cuda::CUDAExecutor*, Args ...) [with Args = {cudnnRNNStruct*, int, int, cudnnDropoutStruct*, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t}]':\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1021:50:   required from here\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:140:38: error: cannot convert 'cudnnRNNStruct*' to 'cudnnHandle_t {aka cudnnContext*}' for argument '1' to 'cudnnStatus_t cudnnSetRNNDescriptor(cudnnHandle_t, cudnnRNNDescriptor_t, int, int, cudnnDropoutDescriptor_t, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnRNNAlgo_t, cudnnDataType_t)'\r\n       cudnnStatus_t retval = ::__name(args...);                    \\\r\n                                      ^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:234:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'\r\n   __macro(cudnnSetRNNDescriptor)", "Hi @rohitgirdhar @sounakdey , \r\n\r\nI got the same error, and there is a possible solution (I read it on a Chinese blog)... which I only test it on TX1:\r\nEdit cublas_api.h ('/usr/local/cuda/targets/aarch64-linux/include/cublas_api.h' is the TX1 path) as follows\r\n\r\n```\r\n...\r\n/*For different GEMM algorithm */\r\ntypedef enum{\r\n CUBLAS_GEMM_DFALT = -1,\r\n CUBLAS_GEMM_ALGO0 = 0,\r\n CUBLAS_GEMM_ALGO1 = 1,\r\n CUBLAS_GEMM_ALGO2 = 2,\r\n CUBLAS_GEMM_ALGO3 = 3,\r\n CUBLAS_GEMM_ALGO4 = 4,\r\n /* add these */ \r\n CUBLAS_GEMM_ALGO5         =  5,\r\n CUBLAS_GEMM_ALGO6         =  6,\r\n CUBLAS_GEMM_ALGO7         =  7,\r\n/* add these */\r\n} cublasGemmAlgo_t;\r\n...\r\n```\r\nOn TX1, it works.\r\n\r\n"]}, {"number": 9000, "title": "Allow the output of `tf.argmax` as index type", "body": "This fix tries to fix the issue raised in #8951 where the following will raise a `TypeError`:\r\n```\r\na = tf.constant([1, 2, 3], dtype=tf.float32)\r\nb = tf.argmax(a)\r\ntf.Session().run(a[b])\r\n\r\nTypeError: Input 'strides' of 'StridedSlice' Op has type int32 that does not match type int64 of argument 'begin'.\r\n```\r\n\r\nThe reason for the erorr is that, `strides` is added  as `append(1)` without type while `begin` is appended with type. (See commit diff).\r\n\r\nThe mismatch of `strides` and `begin` causes the error.\r\n\r\nThis fix fixes the issue by cast the stride with the same type as `begin` when needed.\r\n\r\nThis issue was raised in #8951. It was also raised earlier in https://github.com/tensorflow/tensorflow/issues/206#issuecomment-259076860\r\n\r\nThis fix fixes #8951.", "comments": ["Can one of the admins verify this patch?", "Thanks @ebrevdo for the review. The PR has been updated. Please take a look.", "Thanks @ebrevdo. The `s` (`slice_spec`) seems to be Tensor. I updated the PR and used `ones_like` (`strides.append(ones_like(s))`). Please take a look.", "Lgtm if it passes tests.", "@tensorflow-jenkins test this please", "There are quite a few test failures that seem to be related to this PR.", "@yongtang could you check the tests?", "@caisq @drpngx Thanks for the review and sorry for the late reply. The PR has been updated and the failed tests should have been fixed. Please take a look and let me know if there are any issues.", "@tensorflow-jenkins test this please", "@ebrevdo, are your comments addressed?", "if s is not a scalar this will do the wrong thing.  use s.dtype.as_numpy_dtype(1) and similar in the numpy array case", "Thanks @ebrevdo. The PR has been updated.", "LGTM so long as tests pass.\n\nOn Apr 28, 2017 1:20 PM, \"Yong Tang\" <notifications@github.com> wrote:\n\n> Thanks @ebrevdo <https://github.com/ebrevdo>. The PR has been updated.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9000#issuecomment-298097195>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim74eX0S_bQIcFN-NPzHwQnjlPwLJks5r0komgaJpZM4M02oZ>\n> .\n>\n", "@tensorflow-jenkins test this please", "Sadly this did not work\r\n\r\n======================================================================\r\nERROR: test_test_tensordot_complex128_4_4_2_True (__main__.TensordotTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/bazel_pip/tensorflow/python/kernel_tests/tensordot_op_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/kernel_tests/tensordot_op_test.py\", line 153, in test_tensordot\r\n    c = math_ops.tensordot(a, b, axes)\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 2413, in tensordot\r\n    a_axes, b_axes = _tensordot_axes(a, axes)\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 2408, in _tensordot_axes\r\n    return axes[0], axes[1]\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 476, in _SliceHelper\r\n    strides.append(s.dtype.type(1))\r\nAttributeError: 'int' object has no attribute 'dtype\r\n\r\n:(", "Thanks @vrv for the review. The PR has been updated to address the Jenkins failures. Please take a look.", "@tensorflow-jenkins test this please", "The only remaining thing that would be nice is a test of this behavior.  Since I've already run the tests and it's passing, I'll merge this, but a follow up PR with a test of this behavior would be swell."]}]