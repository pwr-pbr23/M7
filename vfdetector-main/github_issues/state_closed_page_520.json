[{"number": 38137, "title": "[Intel MKL] Fix incorrect DNNL1.2 integration in pooling backprop", "body": "This PR removes the performance degradation caused by incorrect pooling\r\nbackprop integration in Intel-MKL CPU backend.", "comments": []}, {"number": 38136, "title": "[XLA] vectorize row reduction for even row size (new version of PR #37260)", "body": "This is a new version of #37260 that is rabased and include the fix to the DCHECK and convert it to a CHECK:\r\nhttps://github.com/tensorflow/tensorflow/pull/37260#pullrequestreview-385503035\r\nIt also include the other small comments.\r\n@cheshire \r\n@akuegel \r\n\r\n**Original description:**\r\n\r\nThis depend on LLVM commit: https://reviews.llvm.org/D74444 that is recently merged in XLA.\r\n\r\nThis PR vectorize row reduction for even row size.\r\nFor odd row size, every other row is unaligned. So would need a more complicated fix.\r\nOn a reduction of size 32x131070, I have no speed up for a max reduction, but I have 1.21x speed on the sum+pw reduction from the softmax in float16.\r\n\r\nThis was benchmarked on the row major \"Softmax\" TF operations.\r\nThe optimized graph contain 3 main operations: reduce_8 (the max reduction), fusion_1 (the exponential and sum reduction), fusion (the post normalization pw operations, ignored).\r\nThis was benchmarked with a matrix of input shape (32x131070) with float16 dtype:\r\n\r\n| Kernel     | Before PR | After PR | Speed up |\r\n| ---------- | ---------- | ---------- | ---------- |\r\n| fusion_1  | 39.827us  | 33.843us | 1.18x |\r\n| reduce_8 | 22.112us  | 22.047us | 1x |\r\n\r\nHere is the not optimized HLO:\r\n```\r\nHloModule cluster_0\r\n%max_ {\r\n  %x = f16[] parameter(0)\r\n  %y = f16[] parameter(1)\r\n  ROOT %maximum = f16[] maximum(%x, %y)\r\n}\r\n\r\n%add_ {\r\n  %x = f32[] parameter(0)\r\n  %y = f32[] parameter(1)\r\n  ROOT %add = f32[] add(%x, %y)\r\n}\r\n\r\nENTRY %main {\r\n  %arg0.1 = f16[32,131070]{1,0} parameter(0)\r\n  %constant.3 = f16[] constant(-inf)\r\n  %pad1 = f16[32,131070] pad(%arg0.1, %constant.3), padding=0_0_0x0_0_0\r\n  %reduce.8 = f16[32]{0} reduce(%pad1, %constant.3), dimensions={1}, to_apply=%max_\r\n  %broadcast.9 = f16[32,131070]{1,0} broadcast(%reduce.8), dimensions={0}\r\n  %subtract.131070 = f16[32,131070]{1,0} subtract(%arg0.1, %broadcast.9)\r\n  %exponential.11 = f16[32,131070]{1,0} exponential(%subtract.131070)\r\n  %convert.12 = f32[32,131070]{1,0} convert(%exponential.11)\r\n  %constant.13 = f32[] constant(0)\r\n  %pad2 = f32[32,131070] pad(%convert.12, %constant.13), padding=0_0_0x0_0_0\r\n  %reduce.18 = f32[32]{0} reduce(%pad2, %constant.13), dimensions={1}, to_apply=%add_\r\n  %convert.19 = f16[32]{0} convert(%reduce.18)\r\n  %broadcast.20 = f16[32,131070]{1,0} broadcast(%convert.19), dimensions={0}\r\n  ROOT %divide.21 = f16[32,131070]{1,0} divide(%exponential.11, f16[32,131070]{1,0} %broadcast.20)\r\n}\r\n```\r\n\r\n", "comments": ["I have many problems to just run the tests you gave. I have errors likes:\r\n```\r\n/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/Scrt1.o: In function `_start':\r\n(.text+0x20): undefined reference to `main'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\nor like:\r\n```\r\n2020-04-02 15:47:09.542705: F tensorflow/compiler/xla/tests/client_library_test_base.cc:48] Non-OK-status: result.status() status: Not found: no platforms found could not create local client for testing\r\nexternal/bazel_tools/tools/test/test-setup.sh: line 310: 44350 Aborted                 (core dumped) \"${TEST_PATH}\" \"$@\" 2>&1\r\n```\r\n\r\nor like: \r\n```\r\nERROR: /home/fbastien/github/tensorflow/tensorflow/compiler/xla/client/BUILD:182:1: Linking of rule '//tensorflow/compiler/xla/client:sharding_builder' failed (Exit 1)\r\ngcc: error: missing argument to '-B'\r\n```\r\n\r\n\r\nI have this with all tests. So it looks like something is broken upstream (could be specific to my environment). I have errors on the upstream commit before mine too.\r\n\r\nCan you gave me the command you use to reproduce the error you have and on which commit you tested it? It is a rebased version of this PR or this PR directly?\r\n\r\nI'll continue to try to reproduce your error, but I already spend a few hours.", "2 days ago, a build problem was merged upstream that cause the error I saw. I made an issues: https://github.com/tensorflow/tensorflow/issues/38171\r\n\r\nNow, hopefully, I can start to look at the problem you mentioned.", "After one day I'm still not able to reproduce the error. Are you able to reproduce it? Which bazel command line allow you to reproduce it?\r\n\r\nAlso, can you give me the full error message? It would be useful to know which DCHECK failed. I do not have that information in this PR or the previous one.", "> After one day I'm still not able to reproduce the error. Are you able to reproduce it? Which bazel command line allow you to reproduce it?\r\n\r\nI ran this on f7b584639f07dcc95a405c857ce4071c93b5c9ac (my fixed version of your PR). However I only ran it internally, without specifying -c opt as I normally do so that DCHECK is actually evaluated. I am not so familiar with open source to know how you can make sure that DCHECKs are triggered. The easiest option for you might be to temporarily change the DCHECK in shape_util.cc to a CHECK (see stack trace below to see which line this is).\r\n\r\n> Also, can you give me the full error message? It would be useful to know which DCHECK failed. I do not have that information in this PR or the previous one.\r\n\r\nHere is the full stack trace:\r\n\r\nF0403 00:03:26.839476    1674 shape_util.cc:426] Check failed: shape.IsArray() (s32[3], s32[3])\r\n*** Check failure stack trace: ***\r\n    @     0x7f9965176fcc  absl::logging_internal::LogMessage::DieIfFatal()\r\n    @     0x7f9965176893  absl::logging_internal::LogMessage::SendToLog()\r\n    @     0x7f9965175b3e  absl::logging_internal::LogMessage::Flush()\r\n    @     0x7f9965177e68  absl::logging_internal::LogMessageFatal::~LogMessageFatal()\r\n    @     0x7f9977a50770  xla::ShapeUtil::ElementsIn()\r\n    @     0x7f99889179bf  xla::gpu::(anonymous namespace)::ComputeMaxUnrollFactor()\r\n    @     0x7f998891ffd2  xla::gpu::IrEmitterUnnested::EmitTargetElementLoop()\r\n    @     0x7f99888e2798  xla::gpu::IrEmitter::HandleReduce()\r\n    @     0x7f998891a487  xla::gpu::IrEmitterUnnested::HandleReduce()\r\n    @     0x7f9977f81546  xla::HloInstruction::Visit<>()\r\n    @     0x7f9977f67e50  xla::PostOrderDFS<>()\r\n    @     0x7f9977f80ccc  xla::HloInstruction::Accept<>()\r\n    @     0x7f9989ed69f1  xla::HloComputation::Accept<>()\r\n    @     0x7f99890d7aa1  xla::gpu::GpuCompiler::RunBackend()\r\n    @     0x7f99819ed2ef  xla::LLVMCompiler::Compile()\r\n    @     0x7f998971611e  xla::Service::BuildExecutables()\r\n    @     0x7f9989718f56  xla::Service::ExecuteGraphParallel()\r\n    @     0x7f9989cba4d6  xla::Client::ExecuteParallel()\r\n    @     0x7f9989cb86ff  xla::Client::Execute()\r\n    @     0x7f9989cb80a9  xla::Client::ExecuteAndTransfer()\r\n    @     0x7f9989e7cd74  xla::ClientLibraryTestBase::ExecuteAndTransfer()\r\n    @     0x7f9989e7da3d  xla::ClientLibraryTestBase::ComputeAndCompareLiteralWithStatus()\r\n    @     0x7f9989e7d2a9  xla::ClientLibraryTestBase::ComputeAndCompareLiteral()\r\n    @     0x7f9989ee16fb  xla::ClientLibraryTestBase::ComputeAndCompareR1<>()\r\n    @     0x7f9989ed58fa  xla::(anonymous namespace)::ArithmeticTest_ArgMinR2Axis0_Test::TestBody()\r\n    @     0x7f99661097b4  testing::internal::HandleSehExceptionsInMethodIfSupported<>()\r\n    @     0x7f99660f62f2  testing::internal::HandleExceptionsInMethodIfSupported<>()\r\n    @     0x7f99660e6353  testing::Test::Run()\r\n    @     0x7f99660e69f7  testing::TestInfo::Run()\r\n    @     0x7f99660e704c  testing::TestSuite::Run()\r\n    @     0x7f99660ef335  testing::internal::UnitTestImpl::RunAllTests()\r\n    @     0x7f996610e7b4  testing::internal::HandleSehExceptionsInMethodIfSupported<>()", "> After one day I'm still not able to reproduce the error. Are you able to reproduce it? Which bazel command line allow you to reproduce it?\r\n\r\nSorry that you had to spend so much time on this :-(\r\nI didn't expect that this behaves so much differently in open source than internally. I should have provided the full stack trace immediately.\r\nSo here is how you can reproduce it in open source:\r\nChange line 426 in tensorflow/compiler/xla/shape_util.cc from DCHECK to CHECK (the line that checks that shape is an array).\r\nThen you should see the CHECK fail.\r\nThere might be a way to enable DCHECKs in open source, but I don't know how :-(", "I'm not able to have this XLA test works with upstream TF. So I can't try to reproduce this error. \r\nI created an issues that use Google Docker images: https://github.com/tensorflow/tensorflow/issues/38205\r\n\r\nI just pushed a few small changes too. Local tests still compiling...", "I added a commit that fix the NCHECK failure.", "@nouiz I see small but noticeable regression on this input HLO: https://gist.github.com/cheshire/a7e5e546d5feafdc3db97ef1d89baa91, could you take a look?", "Ok. Any idea on which GPU this was run?", "Titan V.", "I have been able to identify a fusion that give me a speed up of 0.85x:\r\n```\r\nHloModule cluster_0\r\n\r\n%add_f32 (x.3360: f32[], y.3361: f32[]) -> f32[] {\r\n  %x.3360 = f32[] parameter(0)\r\n  %y.3361 = f32[] parameter(1)\r\n  ROOT %add.3362 = f32[] add(f32[] %x.3360, f32[] %y.3361)\r\n}\r\n\r\n%fused_computation.1884 (param_0.8573: f32[2,384,1024], param_1.10177: f32[2,384]) -> (f32[2,384], f32[2,384,1024], f32[2,384,1024]) {\r\n  %param_0.8573 = f32[2,384,1024] parameter(0)\r\n  %param_1.10177 = f32[2,384] parameter(1)\r\n  %constant_2237_clone_1 = f32[] constant(0.0009765625)\r\n  %broadcast.3809.clone.1 = f32[2,384] broadcast(f32[] %constant_2237_clone_1), dimensions={}\r\n  %multiply.2547.clone.1 = f32[2,384] multiply(f32[2,384] %param_1.10177, f32[2,384] %broadcast.3809.clone.1)\r\n  %broadcast.1940.clone.1 = f32[2,384,1024] broadcast(f32[2,384] %multiply.2547.clone.1), dimensions={0,1}\r\n  %subtract.415 = f32[2,384,1024] subtract(f32[2,384,1024] %param_0.8573, f32[2,384,1024] %broadcast.1940.clone.1)\r\n  %multiply.2546 = f32[2,384,1024] multiply(f32[2,384,1024] %subtract.415, f32[2,384,1024] %subtract.415)\r\n  %constant_2236 = f32[] constant(0)\r\n  %reduce.1694 = f32[2,384] reduce(f32[2,384,1024] %multiply.2546, f32[] %constant_2236), dimensions={2}, to_apply=%add_f32\r\n  ROOT %tuple.744 = (f32[2,384], f32[2,384,1024], f32[2,384,1024]) tuple(f32[2,384] %reduce.1694, f32[2,384,1024] %subtract.415, f32[2,384,1024] %broadcast.1940.clone.1)\r\n}\r\n\r\nENTRY %cluster_0__XlaCompiledKernel_true__XlaHasReferenceVars_false__XlaNumConstantArgs_1083__XlaNumResourceArgs_393_.18693 {\r\n  %get-tuple-element.1047 = f32[2,384,1024] parameter(0)\r\n  %get-tuple-element.1046 =  f32[2,384] parameter(1)\r\n  ROOT %fusion.1884 = (f32[2,384], f32[2,384,1024], f32[2,384,1024]) fusion(f32[2,384,1024] %get-tuple-element.1047, f32[2,384] %get-tuple-element.1046), kind=kInput, calls=%fused_computation.1884\r\n}\r\n```\r\n\r\nThis fusion have a tuple output. The problem is that LLVM didn't vectorize the store for the non reduction output.\r\n\r\nThe softmax operation generate one fusion with 2 output: the reduction and the exponential. LLVM vectorize the store in that case...\r\n\r\nI'll continue to investigate.", "Note, we need to be executed with this flag, as we cannot have fusion with tuples in input HLO. Something in the optimization pipeline doesn't like this: `--xla_flags=--xla_disable_all_hlo_passes`", "The MOF fusion with the reduction didn't had the alignment information in the store. So LLVM can't vectorize it. I fixed it for this case. But I saw other places that call directly CreateStore. Converting the one I didn't fix could help other cases.\r\n\r\nReady to benchmark again.", "It seems the vectorization test is now failing, maybe it needs a small adjustment?", "I tried the tests here and they pass. I rebased this PR and the test speed pass.\r\n\r\nCan you re-run the test? The error message doesnt give me any hints of what the problem is.", "You probably didn't click through to the Target Log tab which shows what part didn't match:\r\n\r\nhttps://source.cloud.google.com/results/invocations/c0898e06-66ba-46c6-914a-780826ae3d62/targets/%2F%2Ftensorflow%2Fcompiler%2Fxla%2Fservice%2Fgpu%2Ftests:reduction_vectorization_test/log", "Thanks. I pushed a fix. It seems to be a problem in some combination of environment/GPUs. But it should be fixed now."]}, {"number": 38135, "title": "Keras Model Errors on Loading - 'list' object has no attribute 'items' with TF 2.1", "body": "**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): ProductName:\tMac OS X, ProductVersion:\t10.15.2, BuildVersion:\t19C57\r\nTensorFlow installed from (source or binary): pip\r\nTensorFlow version (use command below): 2.1.0\r\nPython version: 3.6.8\r\nCUDA/cuDNN version: None\r\nGPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n\r\nWhen trying to load one of my models using tf.keras.models.load_model an error is thrown at the following location:\r\n\r\n```\r\ntensorflow_core\\python\\keras\\utils\\generic_utils.py\", line 254, in class_and_config_for_serialized_keras_object\r\nfor key, item in cls_config.items():\r\n**AttributeError: 'list' object has no attribute 'items'**\r\n```\r\nThis code expects cls_config to be a dictionary, while for this model it is a list of dictionaries.\r\n\r\nI can successfully load and run this model using TensorFlow versions 2.0.0, 1.15.0 and 1.14.0.\r\n\r\nThis section of code was introduced when adding support for passive serialization in Keras\r\n\r\n**Describe the expected behavior**\r\n\r\nCan successfully load a model from a hdf5 file.\r\n\r\n**Code to reproduce the issue:**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.load_model('cnn_multichannel_dense_f0_b0.h5', compile=False)\r\n```\r\n\r\n**Other info / logs:**\r\n\r\n**_I am also attaching a dummy hdf5 model below which can be used to test._**\r\n\r\n\r\n\r\nComplete Stacktrace of the error:\r\n\r\n```\r\n  File \"/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py\", line 146, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 168, in load_model_from_hdf5\r\n    custom_objects=custom_objects)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/keras/saving/model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/keras/layers/serialization.py\", line 106, in deserialize\r\n    printable_module_name='layer')\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\", line 292, in deserialize_keras_object\r\n    config, module_objects, custom_objects, printable_module_name)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\", line 254, in class_and_config_for_serialized_keras_object\r\n    for key, item in cls_config.items():\r\nAttributeError: 'list' object has no attribute 'items'\r\n\r\n```\r\n\r\n\r\n\r\nWhen loaded with tf.keras in v2.0.0 the layers, model config, inputs, outputs, summary etc. are all parsed correctly, as well as being able to run data through the model.", "comments": ["\r\n[cnn_multichannel_dense_f0_b0.h5.zip](https://github.com/tensorflow/tensorflow/files/4416739/cnn_multichannel_dense_f0_b0.h5.zip)\r\n", "Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/8ffb93fe9c91670fb8e3e6d21038bbcd/38135-2-1.ipynb), [TF v2.2.0-rc2](https://colab.research.google.com/gist/amahendrakar/78f9cc5cc4fdcebbe9e8e62745af53aa/38135-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/3f7747e2187f252273d49872d4615c48/38135-tf-nightly.ipynb). Works fine on [TF v2.0](https://colab.research.google.com/gist/amahendrakar/e1c64f58d7da6e77b9a5dae802664dea/38135-2-0.ipynb). Please find the attached gist. Thanks!", "@tripathysa Can you please share simple standalone code to reproduce the issue? The provided `*.h5` file is not sufficient to find root-cause of the issue. If your code is proprietary code then please try to use public data to create a standalone code. Thanks!", "@jvishnuvardhan : Do you mean sharing train code? Yes its proprietary.  If some change is needed in the train code, then it will be a problem since the trained models are being supported by all TF versions except 2.1 and we don\u2019t want to retrain them again.\r\n\r\n@amahendrakar was already able to reproduce the issue with:\r\n`import tensorflow as tf\r\n\r\nmodel = tf.keras.models.load_model('cnn_multichannel_dense_f0_b0.h5', compile=False)\u2019", "It looks like `config['config']` is expected to be a dictionary here in 2.1 while its a list(https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/utils/generic_utils.py#L252) but in 2.0. no such assumed deserialization happens as I see it.", "@jvishnuvardhan @k-w-w @tripathysa #38339 has been submitted to fix this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38135\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38135\">No</a>\n"]}, {"number": 38134, "title": "TensorFlow-gpu for GV100 on SUSE 15", "body": "- **OS Platform and Distribution: SUSE 15\r\n- **TensorFlow installed from (source or binary)**: 2.1.0\r\n- **Python version**: 3.6\r\n- **GCC/Compiler version (if compiling from source)**: 7.5.0\r\n- **CUDA/cuDNN version**: 10.5\r\n- **GPU model and memory**: 100 GV \r\n\r\n![image](https://user-images.githubusercontent.com/14827177/78168197-8441cd80-741d-11ea-9308-35e8da02be49.png)\r\n\r\nWed Apr  1 10:35:13 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro GV100        Off  | 00000000:15:00.0 Off |                  Off |\r\n| 36%   47C    P0    36W / 250W |      0MiB / 32508MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Quadro GV100        Off  | 00000000:2D:00.0 Off |                  Off |\r\n| 40%   53C    P0    36W / 250W |      0MiB / 32500MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n", "comments": ["@yskale, Please Provide the exact sequence of commands / steps that you executed before running into the problem and also full error log. Thanks!", "Thanks @gadagashwini for reply. The details are\r\n\r\n![image](https://user-images.githubusercontent.com/14827177/78253107-e227f100-74c1-11ea-9e91-cf5cd42a6e0c.png)\r\n\r\n\r\n\r\nERRORS\r\nTensorFlow Version: 2.1.0\r\n\r\n/home/ysk/gputest/gputest/lib64/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: No GPU found. Please ensure you have installed TensorFlow correctly\r\n  # This is added back by InteractiveShellApp.init_path()\r\n", "@yskale, Can you share the full error log? ", "It is not able to detect the GPUs with the Tensorflow-gpu. I have shared all the log details. Can you share couple of commands ...to test ...so that I can share the log based on that....", "@yskale, To know the available GPU devices, use this command\r\n```\r\ntf.test.is_gpu_available(\r\n    cuda_only=False, min_cuda_compute_capability=None\r\n)\r\n```\r\nFor more refer [this](https://www.tensorflow.org/api_docs/python/tf/test/is_gpu_available). Thanks", "![image](https://user-images.githubusercontent.com/14827177/78786423-bbacfe80-7976-11ea-90a0-094111f907ca.png)\r\n", "@yskale,\r\nMake sure you have added CUDA and cuDNN path.Please follow the instructions mentioned [here](https://www.tensorflow.org/install/gpu#linux_setup). Thanks! ", "I followed the guidelines, but the still not able to see the GPU's in tensorflow\r\n![image](https://user-images.githubusercontent.com/14827177/79144066-54cf8100-7d8c-11ea-898e-b2800aeceda9.png)\r\n", "@yskale Can you please check the GPU compatibility [here](https://www.tensorflow.org/install/source#gpu). Did you build tensorflow from source or binary?\r\n\r\ntensorflow-2.1.0 | 2.7, 3.5-3.7 | GCC 7.3.1 | Bazel 0.27.1 | 7.6 | 10.1\r\n-- | -- | -- | -- | -- | --\r\n\r\nIn this issue, you mentioned `CUDA/cuDNN version: 10.5` but nvidia-smi showed `NVIDIA-SMI 440.33.01 Driver Version: 440.33.01 CUDA Version: 10.2 |`. Actually, if you had installed through pip binary, then Tensorflow looks for `cuda 10.1` related directories. If you had installed from source, then try to check the cuda paths are correctly referencing in the installed tensorflow. Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I have build the tensorflow from the pip binary. ", "@yskale your nvidia-smi shows that you have `CUDA Version: 10.2` where as cuda10.2 is not compatible with `TF2.1`. Can you please check the compatibility from my [previous response](https://github.com/tensorflow/tensorflow/issues/38134#issuecomment-615018196) and update your cuda version to 10.1. So, uninstall cuda10.2, restart and install cuda10.1 and related drivers. Once everything is done, install tensorflow2.1 and check it. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38134\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38134\">No</a>\n", "Hi,\r\nThese are details for the systems, we are still not able to detect the GPU's in torch or tensor flow.\r\n\r\n![image](https://user-images.githubusercontent.com/14827177/87707762-a878ae80-c76f-11ea-85a3-1df925b08435.png)\r\n"]}, {"number": 38133, "title": "Update init_mlir.cc", "body": "", "comments": []}, {"number": 38132, "title": "test", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38132\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38132\">No</a>\n"]}, {"number": 38131, "title": "[r2.2 Cherrypick] Support building XNNPACK delegate for Linux/AArch64", "body": "This cherry-picks the build fixes from @Maratyszcza onto the r2.2 branch.\r\nIt restores support for building the [tflite model benchmark tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark) on AArch64 which was possible in 2.1, but broke due to XNNPACK in the 2.2-rc.\r\n\r\n@Maratyszcza It would be great to get a review on this, maybe I am missing something here and cfc31e324c8de6b52f752a39cb161d99d853ca99 should be included as well?\r\n\r\nPiperOrigin-RevId: 302938776\r\nChange-Id: I77c10706be866bebcee1d0ac7c696c3fa55f42ea", "comments": ["LGTM. I don't think cfc31e3 is necessary to unblock Linux/AArch64, but it does affect performance of the XNNPACK delegate.", "Great! Thanks for the review."]}, {"number": 38130, "title": "Gradient Compute Error in Embedding Layers", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Ubuntu 16.04.\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: No.\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX1080 Ti / 11G\r\n\r\n**Describe the current behavior**\r\nI tried to conduct some operations between two tensors. The first one tensor is looked up from an Embedding Layer, and the other one is the weights of the embedding layer. When I try to compute the gradients on all trainable variables, I find that only parameters related to the first tensor are computed. The other parameters in the embedding layer can not be computed and updated. This problem only occurs in Eager Model and the results in static computation graph is correct.\r\n\r\n**Describe the expected behavior**\r\nThe gradient should be computed and updated on the whole weights in the embedding layer.\r\n\r\n**Standalone code to reproduce the issue** \r\n````\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.keras.callbacks import TensorBoard\r\n\r\ndef loss_func(model, x, y):\r\n    y_ = model(x)\r\n    return tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.AUTO)(y_true=y, y_pred=y_)\r\n\r\ndef grad(model, inputs, targets):\r\n    with tf.GradientTape() as tape:\r\n        loss_value = loss_func(model, inputs, targets)\r\n    return loss_value, tape.gradient(loss_value, model.trainable_variables)\r\n\r\nbatch_size = 2\r\nnb_item = 5\r\nnb_hidden = 3\r\n\r\ninputs = np.array([[1], [2]])\r\ntargets = np.random.randn(batch_size, nb_item)\r\n\r\ninput_layer = tf.keras.layers.Input((1, ), dtype=tf.int32)\r\n\r\nembd_layer = tf.keras.layers.Embedding(nb_item, nb_hidden)\r\n\r\nembd1 = tf.reshape(embd_layer(input_layer), [-1, nb_hidden])\r\nall_index = tf.range(nb_item, dtype=tf.int32)\r\nembd2 = embd_layer(all_index)\r\n\r\nsco_mat = tf.keras.layers.Lambda(lambda x: tf.matmul(x[0], x[1], transpose_b=True))([embd1, embd2])\r\n\r\nmodel = tf.keras.models.Model(inputs=input_layer, outputs=[sco_mat])\r\n\r\nloss, grads = grad(model, inputs, targets)\r\nprint(grads[0].values.numpy())\r\n````\r\nOutput:\r\n> [[ 0.00897689  0.01056899  0.01004288]\r\n>  [-0.02378326 -0.02354109 -0.01821425]]\r\nExpect output: a tensor with shape (5, 3)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n![bbbbbb](https://user-images.githubusercontent.com/23370871/78166038-cb13df80-747e-11ea-8b5f-5f29f20a00d9.png)\r\nI think the Node MatMul/b should not be treated as a constant value.", "comments": ["i am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/ec6ec90012d0717c32267da940e06267/38130.ipynb )", "By looking at the documentation of Lambda layer, doesn't seem stateful lambda layer is encouraged. Please check the `Variables` section under keras/layers/core -> class Lambda, i.e.:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/core.py#L770-L797", "> By looking at the documentation of Lambda layer, doesn't seem stateful lambda layer is encouraged. Please check the `Variables` section under keras/layers/core -> class Lambda, i.e.:\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/core.py#L770-L797\r\n\r\nI have used a subclass layer to instead of the lambda layer as follows. But it seems that the results is still incorrect.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\n\r\ndef loss_func(model, x, y):\r\n    y_ = model(x)\r\n    return tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.AUTO)(y_true=y, y_pred=y_)\r\n\r\ndef grad(model, inputs, targets):\r\n    with tf.GradientTape() as tape:\r\n        loss_value = loss_func(model, inputs, targets)\r\n    return loss_value, tape.gradient(loss_value, model.trainable_variables)\r\n\r\nclass ScoMatLayer(tf.keras.layers.Layer):\r\n    def __init__(self, embd_layer):\r\n        super(ScoMatLayer, self).__init__()\r\n        all_index = tf.range(nb_item, dtype=tf.int32)\r\n        self.embd2 = embd_layer(all_index)\r\n    def call(self, inputs):\r\n        return tf.matmul(inputs, self.embd2, transpose_b=True)\r\n\r\nbatch_size = 2\r\nnb_item = 5\r\nnb_hidden = 3\r\n\r\ninputs = np.array([[1], [2]])\r\ntargets = np.random.randn(batch_size, nb_item)\r\n\r\ninput_layer = tf.keras.layers.Input((1, ), dtype=tf.int32)\r\n\r\nembd_layer = tf.keras.layers.Embedding(nb_item, nb_hidden)\r\n\r\nembd1 = tf.reshape(embd_layer(input_layer), [-1, nb_hidden])\r\n\r\nscl = ScoMatLayer(embd_layer)\r\n\r\nsco_mat = scl(embd1)\r\n\r\nmodel = tf.keras.models.Model(inputs=input_layer, outputs=[sco_mat])\r\n\r\nloss, grads = grad(model, inputs, targets)\r\nprint(grads[0].values.numpy())\r\n```\r\n\r\n> Output:\r\n> [[-0.01078839 -0.00246409 -0.01437871]\r\n>  [-0.01240485 -0.01926923  0.00199004]]\r\n\r\nAs you can see in this code, I think the ScoMatLayer is a stateless layer.", "Use tf.identity(weights)?", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/drive/1OnKnoDXOcZJSpmViPbB4ZjWWlz77PO10?resourcekey=0-Vo6nWmCqjSLuukP5d-5VrQ#scrollTo=jIpDRl8pd3tN). Thanks!", "@ruipingyin, Sorry for late response.\r\n\r\nDid you tried as suggested by @tanzhenyu to use tf.identity(weights)? Also, Many bugs have been fixed in the latest version. Can you please execute your code using `TFv2.7` and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38130\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38130\">No</a>\n"]}, {"number": 38129, "title": "Reloading Tensorflow 2.1.0 without restarting interpreter", "body": "In the production system  preinstalled with `tensorflow-gpu==1.13.1`, if I need to run `tensorflow-gpu==2.1.0`, I can only install and reload the library, without restarting the system, but reloading the tensorflow library gives me error, how should I resolve it?\r\n\r\nTo reproduce the problem on a client computer:\r\n\r\n\r\n1.First\r\n\r\n    pip install tensorflow-gpu==1.13.1\r\n\r\n2.Then run the following code\r\n\r\n    import tensorflow\r\n    print(tensorflow.__version__) \r\n\r\nit will print\r\n\r\n    1.13.1\r\n3. Now run\r\n\r\n    pip install tensorflow-gpu==2.1.0 \r\n\r\n4.If you run the following code (what I run on production system to reload the module)\r\n\r\n    import tensorflow\r\n    from importlib import reload\r\n    tensorflow=reload(tensorflow)\r\n    print(tensorflow.__version__) \r\n\r\n\r\nYou will get error as follows throw by `reload`, how should I resolve it?\r\n\r\n    Traceback (most recent call last):\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\r\n        exec(code_obj, self.user_global_ns, self.user_ns)\r\n      File \"<ipython-input-9-453567e02c5c>\", line 3, in <module>\r\n        tensorflow=reload(tensorflow)\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/importlib/__init__.py\", line 169, in reload\r\n        _bootstrap._exec(spec, module)\r\n      File \"<frozen importlib._bootstrap>\", line 630, in _exec\r\n      File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n      File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/__init__.py\", line 101, in <module>\r\n        from tensorflow_core import *\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/__init__.py\", line 40, in <module>\r\n        from tensorflow.python.tools import module_util as _module_util\r\n    ImportError: cannot import name 'module_util' from 'tensorflow.python.tools' (/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/tools/__init__.py)\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\r\n        stb = value._render_traceback_()\r\n    AttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\r\n        return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\r\n        return f(*args, **kwargs)\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\r\n        records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/inspect.py\", line 1502, in getinnerframes\r\n        frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/inspect.py\", line 1460, in getframeinfo\r\n        filename = getsourcefile(frame) or getfile(frame)\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/inspect.py\", line 696, in getsourcefile\r\n        if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/inspect.py\", line 733, in getmodule\r\n        if ismodule(module) and hasattr(module, '__file__'):\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n        module = self._load()\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n        module = _importlib.import_module(self.__name__)\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n        return _bootstrap._gcd_import(name[level:], package, level)\r\n      File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n      File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n      File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n      File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n      File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n      File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n      File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n      File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n      File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n      File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n      File \"/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/__init__.py\", line 40, in <module>\r\n        from tensorflow.python.tools import module_util as _module_util\r\n    ImportError: cannot import name 'module_util' from 'tensorflow.python.tools' (/home/user/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/tools/__init__.py)\r\n\r\n\r\n\r\n\r\n", "comments": ["@tianhuat, you need to upgrade tensorflow to 2.x.", "@khimraj The issue is not about upgrading, it's about can't restart the production environment after upgrading, I can only do a reloading of tensorflow library", "@tianhuat, I think currently it is not possible to reload Tensorflow 2.1.0 without restarting interpreter . You have to restart the runtime to change versions.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@tianhuat \r\n\r\nAny update on this issue please. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38129\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38129\">No</a>\n"]}, {"number": 38128, "title": "[Intel MKL] Support MatMul fusion for bfloat16 type.", "body": "Let grappler support bfloat16 MatMul fusion in MKL end.\r\n\r\nSigned-off-by: Lu Teng teng.lu@intel.com", "comments": ["@Zantares Could you please resolve the conflicts? Thanks!\r\n", "> @Zantares Could you please resolve the conflicts? Thanks!\r\n\r\nDone."]}, {"number": 38127, "title": "Update init_mlir.h", "body": "", "comments": []}, {"number": 38126, "title": "Convert 0d tensor into float value", "body": "How can I convert a tensor of rank 0 and dtype float32 into a 'regular' float value? I am not running a session so tf.eval() does not work. ", "comments": ["Hy, @johannafrost  you can use \r\n``` \r\ntf.cast(\r\n    x, dtype, name=None\r\n)\r\n```\r\nfor more info, kindly refer to this documentaiton: https://www.tensorflow.org/api_docs/python/tf/cast", "Doesn't that still return a tensor? I would like to get a float value instead of a tensor, which I can append to a list (initiated by list = []) . \r\nI am using inside a tf.function, maybe that changes things? ", "Hey, @johannafrost \r\nsuppose you are having a 0d tensor and you assign to a variable x.\r\n```\r\na = x.numpy()\r\nb = list(a)\r\ntype(b)\r\n-> list\r\n```\r\nnow \"b\" is a list and you can easily access the element of the list.", "Thank a lot for the replies. Not sure how I can apply that to a 0d tensor though, since neither tensor.numpy() or list(tensor) works. ", "Hey, @johannafrost it worked for me would you please provide your colab notebook? ", "Whenever you're doing any typecasting then assign it to another variable.\r\n```\r\ntensor = tf.constant([1.0,2.0], dtype=float32) \r\na = tensor.numpy()\r\nb = list(a) \r\ntype(b) \r\n\u2014> list\r\n```", "@johannafrost,\r\nPlease take a look at [this gist](https://colab.research.google.com/gist/amahendrakar/7dd236f2b3a93f549c196794426379ae/38126.ipynb) and let us know if it resolves your query. Thanks!", "Any updates regarding this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 38125, "title": "[INTEL MKL] Enabling conv2D (NCHW format) fusion in grappler remapper", "body": "This PR enables conv2D fusion in grappler remapper for NCHW format. This is required if a model is trained and saved in NCHW format and used later for inference.", "comments": []}, {"number": 38124, "title": "Use Unified Memory in TRT opconverter tests", "body": "TRT's opconverter unit tests are changed in this PR to use unified memory when creating test tensors. This simplifies [BuildAndRun](https://github.com/tfeher/tensorflow/blob/0d502426484f5a8e5eee9c28e14b2437423a40cd/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc#L1297-L1342), since copying the data between GPU and host is done automatically by the driver. \r\n\r\nSwitching to unified memory is achieved by moving the AsTensor and ConstructTensor functions inside the OpConverterTest class and using GpuManagedAllocator in them.\r\n\r\nThis PR is based on #38118, please review that first.", "comments": []}, {"number": 38123, "title": "TFLu: fix compilation issue in cmsis-nn/conv.cc", "body": "", "comments": []}, {"number": 38122, "title": "'tensorflow._api.v2.compat.v2.compat' has no attribute 'v1'", "body": "I am working on Ubuntu 19.10.\r\nI have installed tensorflow version '2.0.0-beta1'\r\nI have installed tensorflow-dataset version '2.1.0'\r\nWhen I try to load mnist dataset, I get the following error:\r\n >>> dataset =tfds.load(name=\"mnist\")\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/api_utils.py\", line 52, in disallow_positional_args_dec\r\n    return fn(*args, **kwargs)\r\n  File \"/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/registered.py\", line 318, in load\r\n    ds = dbuilder.as_dataset(**as_dataset_kwargs)\r\n  File \"/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/api_utils.py\", line 52, in disallow_positional_args_dec\r\n    return fn(*args, **kwargs)\r\n  File \"/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py\", line 482, in as_dataset\r\n    datasets = utils.map_nested(build_single_dataset, split, map_tuple=True)\r\n  File \"/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/utils/py_utils.py\", line 145, in map_nested\r\n    for k, v in data_struct.items()\r\n  File \"/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/utils/py_utils.py\", line 145, in <dictcomp>\r\n    for k, v in data_struct.items()\r\n  File \"/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/utils/py_utils.py\", line 159, in map_nested\r\n    return function(data_struct)\r\n  File \"/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py\", line 541, in _build_single_dataset\r\n    read_config=read_config,\r\n  File \"/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py\", line 949, in _as_dataset\r\n    shuffle_files=shuffle_files,\r\n  File \"/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/tfrecords_reader.py\", line 290, in read\r\n    return tf.nest.map_structure(_read_instruction_to_ds, instructions)\r\n  File \"/home/umesh/.local/lib/python3.7/site-packages/tensorflow/python/util/nest.py\", line 515, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/home/umesh/.local/lib/python3.7/site-packages/tensorflow/python/util/nest.py\", line 515, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/tfrecords_reader.py\", line 287, in _read_instruction_to_ds\r\n    num_examples=file_instructions.num_examples,\r\n  File \"/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/tfrecords_reader.py\", line 322, in read_files\r\n    num_examples=num_examples,\r\n  File \"/home/umesh/.local/lib/python3.7/site-packages/tensorflow_datasets/core/tfrecords_reader.py\", line 201, in _read_files\r\n    instruction_ds = tf.compat.v1.data.Dataset.from_tensor_slices(tensor_inputs)\r\nAttributeError: module 'tensorflow._api.v2.compat.v2.compat' has no attribute 'v1'", "comments": ["@rambail, Can you share the standalone code to replicate the issue and also try with the latest stable Tensorflow 2.1 version. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38122\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38122\">No</a>\n", "```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nprint(\"\\u2022 Using TensorFlow Version:\", tf.__version__)\r\nprint(\"\\u2022 Using TensorFlow Dataset Version:\", tfds.__version__)\r\n```\r\n\r\n**\u2022 Using TensorFlow Version: 2.0.0-beta0\r\n\u2022 Using TensorFlow Dataset Version: 2.1.0**\r\n\r\n'---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-6940136dcaab> in <module>\r\n----> 1 dataset =tfds.load(name=\"mnist\")\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)\r\n     50     _check_no_positional(fn, args, ismethod, allowed=allowed)\r\n     51     _check_required(fn, kwargs)\r\n---> 52     return fn(*args, **kwargs)\r\n     53 \r\n     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/registered.py in load(name, split, data_dir, batch_size, in_memory, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\r\n    316   as_dataset_kwargs.setdefault(\"read_config\", read_config)\r\n    317 \r\n--> 318   ds = dbuilder.as_dataset(**as_dataset_kwargs)\r\n    319   if with_info:\r\n    320     return ds, dbuilder.info\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)\r\n     50     _check_no_positional(fn, args, ismethod, allowed=allowed)\r\n     51     _check_required(fn, kwargs)\r\n---> 52     return fn(*args, **kwargs)\r\n     53 \r\n     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py in as_dataset(self, split, batch_size, shuffle_files, decoders, read_config, as_supervised, in_memory)\r\n    480         in_memory=in_memory,\r\n    481     )\r\n--> 482     datasets = utils.map_nested(build_single_dataset, split, map_tuple=True)\r\n    483     return datasets\r\n    484 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/utils/py_utils.py in map_nested(function, data_struct, dict_only, map_tuple)\r\n    143     return {\r\n    144         k: map_nested(function, v, dict_only, map_tuple)\r\n--> 145         for k, v in data_struct.items()\r\n    146     }\r\n    147   elif not dict_only:\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/utils/py_utils.py in <dictcomp>(.0)\r\n    143     return {\r\n    144         k: map_nested(function, v, dict_only, map_tuple)\r\n--> 145         for k, v in data_struct.items()\r\n    146     }\r\n    147   elif not dict_only:\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/utils/py_utils.py in map_nested(function, data_struct, dict_only, map_tuple)\r\n    157         return tuple(mapped)\r\n    158   # Singleton\r\n--> 159   return function(data_struct)\r\n    160 \r\n    161 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py in _build_single_dataset(self, split, shuffle_files, batch_size, decoders, read_config, as_supervised, in_memory)\r\n    539           shuffle_files=shuffle_files,\r\n    540           decoders=decoders,\r\n--> 541           read_config=read_config,\r\n    542       )\r\n    543       # Auto-cache small datasets which are small enough to fit in memory.\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py in _as_dataset(self, split, decoders, read_config, shuffle_files)\r\n    947           split_infos=self.info.splits.values(),\r\n    948           read_config=read_config,\r\n--> 949           shuffle_files=shuffle_files,\r\n    950       )\r\n    951     else:\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/tfrecords_reader.py in read(self, name, instructions, split_infos, read_config, shuffle_files)\r\n    288       )\r\n    289 \r\n--> 290     return tf.nest.map_structure(_read_instruction_to_ds, instructions)\r\n    291 \r\n    292   def read_files(\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)\r\n    513 \r\n    514   return pack_sequence_as(\r\n--> 515       structure[0], [func(*x) for x in entries],\r\n    516       expand_composites=expand_composites)\r\n    517 \r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/util/nest.py in <listcomp>(.0)\r\n    513 \r\n    514   return pack_sequence_as(\r\n--> 515       structure[0], [func(*x) for x in entries],\r\n    516       expand_composites=expand_composites)\r\n    517 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/tfrecords_reader.py in _read_instruction_to_ds(instruction)\r\n    285           read_config=read_config,\r\n    286           shuffle_files=shuffle_files,\r\n--> 287           num_examples=file_instructions.num_examples,\r\n    288       )\r\n    289 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/tfrecords_reader.py in read_files(self, files, read_config, shuffle_files, num_examples)\r\n    320         parse_fn=self._parser.parse_example,\r\n    321         shuffle_files=shuffle_files,\r\n--> 322         num_examples=num_examples,\r\n    323     )\r\n    324     return dataset\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/tfrecords_reader.py in _read_files(files, parse_fn, read_config, shuffle_files, num_examples)\r\n    199   block_length = read_config.interleave_block_length\r\n    200 \r\n--> 201   instruction_ds = tf.compat.v1.data.Dataset.from_tensor_slices(tensor_inputs)\r\n    202 \r\n    203   # If shuffle is True, we shuffle the instructions/shards\r\n\r\nAttributeError: module 'tensorflow._api.v2.compat.v2.compat' has no attribute 'v1'\r\n'\r\n\r\n**I have the latest stable version of the TensorFlow dataset i.e 2.1. I tried changing the tensorflow version to 2.0.0a0 and 2.0.0b1, but getting the same result.**\r\n\u200b", "In colab too when I code:\r\n```\r\n!pip install --upgrade tensorflow==2.0.0b0\r\n```\r\nI get the same error. But when I change the version to \r\n```\r\n!pip install --upgrade tensorflow==2.0.0\r\n```\r\n`It works!!\r\n\r\nBut unfortunately, Ubuntu 19.10 has only upto 2.0.0b1 in its pip3 repository, And I am not equal to the task of compiling from source.", "`pip install --upgrade pip` and then when you do `pip install` you should see the other versions", "I was not using pip but pip3 as I didn't want python2.7 files in my system.  ```pip install --upgrade pip``` and ```pip install --upgrade tensorflow``` installs tensorflow.2.1 in the site-packages folder of python2.7. But ```pip3 install --upgrade pip``` and followed by ```pip3 install --upgrade tensorflow``` upgrades the package in python3.7 folders. Thanks for the hint", "You can always create a virtualenv where python3 tools are default `virtualenv <dir> -p python3` and then `<dir>/bin/activate`.\r\n\r\nOr you can do `python3 -m pip install` instead of `pip install`.\r\n\r\nBoth are alternatives to using `pip3`.\r\n\r\nBut more important, now that the issue seems solved, can we close it?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38122\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38122\">No</a>\n"]}, {"number": 38121, "title": "Add support for offline planned tensor allocations", "body": "Add support for offline planning in Tensorflow Lite Micro, and accompanying testcases.\r\n\r\nThe idea behind the offline memory planner can be found in the following RFC:\r\nhttps://docs.google.com/document/d/16aTSHL5wxsq99t6adVbBz1U3K8Y5tBDAvs16iroZDEU\r\n\r\nIt is implemented according to alternative 1 described in section 3.1.", "comments": ["\"MacOS Python2 and CC\" seems unrelated to this change.", "@jenselofsson Can you please check @wangtz comments and resolve conflicts?. Thanks!", "@gbaned Taking a look!", "@wangtz Addressed the review comments!", "Let us know if there's anything more we should do. Otherwise, I think it's ready to pull.", "@jenselofsson Can you please check @wangtz comments and keep us posted. Thanks!", "@gbaned Done!", "Ready to merge", "@jenselofsson Can you please resolve conflicts? Thanks!", "@petewarden Ping for review", "The Windows Bazel GPU build is failing with this error message:\r\nERROR: T:/src/github/tensorflow/tensorflow/core/kernels/BUILD:362:1: C++ compilation of rule '//tensorflow/core/kernels:extract_volume_patches_op_gpu' failed (Exit 1)\r\n\r\nwhich is unrelated to Tensorflow Lite Micro.", "@gbaned Resolved!", "Ready to pull", "@wangtz @petewarden Ping for review!", "@gbaned ready to pull", "@jenselofsson can you please check above errors ?", "Hi @rthadur ! Thanks for catching this. All valid, we will fix.\r\n\r\nWhen we ran regression using:\r\n\r\n`make -f tensorflow/lite/micro/tools/make/Makefile test` \r\n\r\nand running the script:\r\n\r\n`./tensorflow/lite/micro/tools/ci_build/test_all.sh`\r\n\r\nthese errors weren't caught. Will you let us know how you run your tests so we can avoid these issues in the future?\r\n\r\nCheers!\r\n", "@freddan80 these are whole lot of tests which run internally,which iam not aware , @petewarden could you please help with @freddan80 ask , \r\nCan you please resolve conflicts as well.", "@rthadur Conflicts solved!", "@rthadur Fixed build issues.", "Fixed merge conflicts.", "Ready to merge", "@jenselofsson sorry for the delay it got stuck internally , please check below errors \r\n\r\n`/tensorflow/lite/micro/micro_allocator.cc:102:14: error: unused function 'CheckOfflinePlannedOffsets' [-Werror,-Wunused-function]\r\nTfLiteStatus CheckOfflinePlannedOffsets(const Model* model,\r\n             ^\r\n1 error generated.\r\nBroken by missing target \r\n/tensorflow/lite/micro/memory_planner:greedy_memory_planner\r\nthird_party/tensorflow/lite/micro/memory_planner/greedy_memory_planner.cc:194:15: error: declaration shadows a local variable [-Werror,-Wshadow]\r\n    const int buffer_id = buffer_ids_sorted_[i];\r\n              ^\r\n/tensorflow/lite/micro/memory_planner/greedy_memory_planner.cc:184:7: note: previous declaration is here\r\n  int buffer_id = buffer_ids_sorted_[0];\r\n      ^\r\n1 error generated.`\r\n\r\nI guess the tests which generated above error is \r\n\r\n/tensorflow/lite/experimental/microfrontend/lib/BUILD?q=fft_test_binary", "@rthadur Fixed the compile issues", "@jenselofsson Can you please check @rthadur comments and resolve conflicts?. Thanks!", "@jenselofsson sorry for the delay. , this changes have been failing internally , here is the error log \r\n\r\n`tensorflow/lite/micro/micro_allocator_test.cc:291:26: error: no matching constructor for initialization of 'tflite::MicroAllocator'\r\n tflite::MicroAllocator allocator(&context, model, arena, arena_size,\r\n                        ^         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/tensorflow/lite/micro/micro_allocator.h:143:3: note: candidate constructor not viable: requires 4 arguments, but 5 were provided\r\n MicroAllocator(TfLiteContext* context, const Model* model,\r\n ^\r\n/tensorflow/lite/micro/micro_allocator.h:88:7: note: candidate constructor (the implicit copy constructor) not viable: requires 1 argument, but 5 were provided\r\nclass MicroAllocator {\r\n     ^\r\n/tensorflow/lite/micro/micro_allocator_test.cc:340:26: error: no matching constructor for initialization of 'tflite::MicroAllocator'\r\n tflite::MicroAllocator allocator(&context, model, arena, arena_size,\r\n                        ^         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/tensorflow/lite/micro/micro_allocator.h:143:3: note: candidate constructor not viable: requires 4 arguments, but 5 were provided\r\n MicroAllocator(TfLiteContext* context, const Model* model,\r\n ^\r\n/tensorflow/lite/micro/micro_allocator.h:88:7: note: candidate constructor (the implicit copy constructor) not viable: requires 1 argument, but 5 were provided\r\nclass MicroAllocator {\r\n     ^\r\n/tensorflow/lite/micro/micro_allocator_test.cc:385:26: error: no matching constructor for initialization of 'tflite::MicroAllocator'\r\n tflite::MicroAllocator allocator(&context, model, arena, arena_size,\r\n                        ^         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/tensorflow/lite/micro/micro_allocator.h:143:3: note: candidate constructor not viable: requires 4 arguments, but 5 were provided\r\n MicroAllocator(TfLiteContext* context, const Model* model,\r\n ^\r\n/tensorflow/lite/micro/micro_allocator.h:88:7: note: candidate constructor (the implicit copy constructor) not viable: requires 1 argument, but 5 were provided\r\nclass MicroAllocator {\r\n     ^\r\n/tensorflow/lite/micro/micro_allocator_test.cc:433:26: error: no matching constructor for initialization of 'tflite::MicroAllocator'\r\n tflite::MicroAllocator allocator(&context, model, arena, arena_size,\r\n                        ^         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/tensorflow/lite/micro/micro_allocator.h:143:3: note: candidate constructor not viable: requires 4 arguments, but 5 were provided\r\n MicroAllocator(TfLiteContext* context, const Model* model,\r\n`", "@rthadur Fixed build issues and merge conflict!", "@jenselofsson here is the internally error , can you please check \r\n\r\n`tensorflow/lite/micro/micro_allocator.cc:895:72: error: too few arguments to function call, expected 3, have 2\r\n    TF_LITE_ENSURE_STATUS(builder.AddTensors(subgraph, context->tensors));\r\n                          ~~~~~~~~~~~~~~~~~~                           ^\r\n./tensorflow/lite/c/common.h:198:29: note: expanded from macro 'TF_LITE_ENSURE_STATUS'\r\n    const TfLiteStatus s = (a);  \\\r\n                            ^\r\n/tensorflow/lite/micro/micro_allocator.cc:225:37: note: 'AddTensors' declared here\r\nTfLiteStatus AllocationInfoBuilder::AddTensors(const SubGraph* subgraph,\r\n                                    ^\r\n1 error generated`", "@rthadur Fixed build issues"]}, {"number": 38120, "title": "TF_GPU_ALLOCATOR=cuda_malloc and the GPUBFC allocator is still used.", "body": "### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen setting the TF_GPU_ALLOCATOR=cuda_malloc env variable, I expect the GPUBFC allocator to not be used. But I have a case where the GPUBFC allocator is still used.\r\n\r\nHaving it work as expect would help separate OOM that are due to memory fragmentation issue vs other OOM issues.\r\n\r\n### Source code / logs\r\nThis can be reproduced with the BERT model from https://github.com/tensorflow/models on a computer with 2 GPUs of 16G like this:\r\n\r\nStart a container:\r\n```\r\ndocker run --gpus all --name tf_gpu_allocator --privileged --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -it nvcr.io/nvidia/tensorflow:20.02-tf2-py3\r\n```\r\n\r\nThen run those command into it to prepare the environment:\r\n\r\n```\r\npip install tensorflow_hub sentencepiece gin-config\r\ngit clone https://github.com/tensorflow/models.git\r\ncd models/official/nlp/bert/\r\ngit checkout b60dc23714d97ca0218a70d912353f388d75b5ef\r\nwget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\r\nwget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\r\nwget https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py\r\nwget https://storage.googleapis.com/cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16.tar.gz\r\ntar -zxf uncased_L-24_H-1024_A-16.tar.gz\r\n\r\nexport SQUAD_VERSION=v1.1\r\nexport SQUAD_DIR=$PWD\r\nexport BERT_BASE_DIR=${PWD}/uncased_L-24_H-1024_A-16\r\nexport OUTPUT_DIR=/tmp/OUTPUT\r\nmkdir -p /tmp/OUTPUT\r\nPYTHONPATH=../../.. python ../data/create_finetuning_data.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --squad_data_file=${SQUAD_DIR}/train-${SQUAD_VERSION}.json \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --vocab_file=${BERT_BASE_DIR}/vocab.txt \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --train_data_output_path=${OUTPUT_DIR}/squad_${SQUAD_VERSION}_train.tf_record \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --meta_data_file_path=${OUTPUT_DIR}/squad_${SQUAD_VERSION}_meta_data \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --fine_tuning_task_type=squad --max_seq_length=384\r\n\r\n```\r\n\r\nThen run the script that doesn't behave like we want:\r\n\r\n```\r\nexport SQUAD_DIR=/tmp/OUTPUT\r\nTF_CPP_VMODULE=bfc_allocator=1 TF_GPU_ALLOCATOR=cuda_malloc PYTHONPATH=../../.. timeout 30m python run_squad.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 --input_meta_data_path=${SQUAD_DIR}/squad_${SQUAD_VERSION}_meta_data \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 --train_data_path=${SQUAD_DIR}/squad_${SQUAD_VERSION}_train.tf_record \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 --predict_file=${SQUAD_DIR}/dev-v1.1.json \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 --vocab_file=${BERT_BASE_DIR}/vocab.txt \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 --train_batch_size=48 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 --predict_batch_size=8 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 --learning_rate=8e-5 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 --num_train_epochs=1 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 --distribution_strategy=mirrored \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 --num_gpus=2 --dtype=fp16 --loss_scale=dynamic --learning_rate=8e-5 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 --all_reduce_alg=nccl \u00a0&> OUT\r\n\r\ngrep gpu_host_bfc OUT\r\n```\r\n\r\nI get this as output:\r\n```\r\n2020-03-31 20:57:40.022021: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  4\r\n2020-03-31 20:57:40.023480: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  16\r\n2020-03-31 20:57:40.023520: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  4\r\n2020-03-31 20:57:40.027274: I tensorflow/core/common_runtime/bfc_allocator.cc:549] DeallocateRaw gpu_host_bfc 4\r\n2020-03-31 20:57:40.027325: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  12\r\n2020-03-31 20:57:40.027359: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  4\r\n2020-03-31 20:57:40.027460: I tensorflow/core/common_runtime/bfc_allocator.cc:549] DeallocateRaw gpu_host_bfc 4\r\n2020-03-31 20:57:40.027499: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  12\r\n2020-03-31 20:57:40.027529: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  12\r\n2020-03-31 20:57:40.027554: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  8\r\n2020-03-31 20:57:40.027576: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  8\r\n2020-03-31 20:57:40.027603: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  2\r\n2020-03-31 20:57:40.027681: I tensorflow/core/common_runtime/bfc_allocator.cc:549] DeallocateRaw gpu_host_bfc 2\r\n2020-03-31 20:57:40.027718: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  2\r\n2020-03-31 20:57:40.027782: I tensorflow/core/common_runtime/bfc_allocator.cc:549] DeallocateRaw gpu_host_bfc 2\r\n2020-03-31 20:57:40.027818: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  4\r\n2020-03-31 20:57:40.027845: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  4\r\n2020-03-31 20:57:40.027869: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  16\r\n2020-03-31 20:57:40.027894: I tensorflow/core/common_runtime/bfc_allocator.cc:227] AllocateRaw gpu_host_bfc  2\r\n...\r\n```\r\n\r\nThat show the bfc allocator is still used.\r\n\r\n\r\n", "comments": ["@nouiz \r\ncould you please let us know which tensorflow version are you facing this issue ", "I just retested with the container: tensorflow/tensorflow:nightly-gpu-py3, and I still see the problem.\r\n\r\nNote, to use that TF container, I need to modify a little bit the instruction above. \r\nI needed to add `pip install tensorflow_addons`.\r\nI needed to remove the command `git checkout b60dc23714d97ca0218a70d912353f388d75b5ef`.", "The BFC allocator is being used on the CPU, not the GPU. Note the term `gpu_host_bfc` in the logs, where \"host\" refers to the CPU. The reason the word \"gpu\" also appears is that we allocate pinned memory on the CPU when a GPU is used, which makes it faster to transfer data from the CPU to the GPU."]}, {"number": 38119, "title": "Custom loss function with tf.keras", "body": "As far as I have read and researched there is no way to use a custom loss function which uses more than the standard input variables (y_true, y_pred) in a keras model.\r\nMeaning using a Keras Sequential model as it is and just using a custom loss function.\r\nWhich makes it difficult to use tf Estimator and gpu computing. \r\nPlease let me know if I am missing something or this is something which can be done or is in progress.\r\n\r\nThanks.\r\nHere is the loss function : - \r\n```\r\ndef get_loss(self,X,Y,lambda_):\r\n        X_tensor = tf.convert_to_tensor(X)\r\n        with tf.GradientTape() as inp_tape:\r\n          inp_tape.watch(X_tensor)\r\n          pred = self.run(X_tensor)\r\n          inp_grad_reg_loss = tf.losses.categorical_crossentropy(Y,pred)\r\n        inpgrad = inp_tape.gradient(inp_grad_reg_loss,X_tensor)\r\n        return inp_grad_reg_loss + lambda_*(tf.norm(inpgrad))\r\n```\r\n**System information**\r\n- TensorFlow version (you are using): TF 2.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\nI dont know\r\n**Who will benefit with this feature?**\r\nEveryone I think\r\n**Any Other info.**\r\n", "comments": ["Hey, @devpunjabi yes you can use a custom loss function in keras models. Kindly search for custom training in keras.\r\nAlso you can refer here: https://www.tensorflow.org/tutorials/customization/custom_training", "@abhinavsp0730 \r\nHey below is my loss function and by above mentioned methods, I dont think it is possible.\r\n```\r\ndef get_loss(self,X,Y,lambda_):\r\n        X_tensor = tf.convert_to_tensor(X)\r\n        with tf.GradientTape() as inp_tape:\r\n          inp_tape.watch(X_tensor)\r\n          pred = self.run(X_tensor)\r\n          inp_grad_reg_loss = tf.losses.categorical_crossentropy(Y,pred)\r\n        inpgrad = inp_tape.gradient(inp_grad_reg_loss,X_tensor)\r\n        return inp_grad_reg_loss + lambda_*(tf.norm(inpgrad))\r\n\r\n```", "@devpunjabi, you need to define loss function as:\r\n```\r\ndef get_loss(self,X,Y,lambda_):\r\n  def loss(y_true, y_pred):\r\n    X_tensor = tf.convert_to_tensor(X)\r\n    with tf.GradientTape() as inp_tape:\r\n      inp_tape.watch(X_tensor)\r\n      pred = self.run(X_tensor)\r\n      inp_grad_reg_loss = tf.losses.categorical_crossentropy(Y,pred)\r\n    inpgrad = inp_tape.gradient(inp_grad_reg_loss,X_tensor)\r\n    return inp_grad_reg_loss + lambda_*(tf.norm(inpgrad))\r\n```\r\nand in `model.compile` method you need to pass this custom loss function with additional arguments.", "@khimraj , I dont understand how your function would work.\r\n But to make you understand mine - I am not only calculating the loss w.r.t to sigmoid output but w.r.t to gradient of the loss w.r.t input. loss = loss + lambda(norm(gradient of loss))\r\n", "@devpunjabi Can you please check this [stackoverflow answer](https://stackoverflow.com/a/45963039/9936228) for defining custom loss function. Can you please share a standalone code to reproduce the issue faster? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@devpunjabi \r\n\r\nAny update on this issue please. Thanks!", "I have implemented the suggested solution , but I am getting an unusual error :  \r\nI tried setting steps = input shape OR None, but did not work\r\n```\r\ndef keras_loss(y_true,y_pred,X,lambda_, model):\r\n\r\n        X_tensor = tf.convert_to_tensor(X)\r\n        with tf.GradientTape() as inp_tape:\r\n          inp_tape.watch(X_tensor)\r\n          pred = model.predict(X_tensor,batch_size=None, steps=None)\r\n          inp_grad_reg_loss = tf.keras.losses.categorical_crossentropy(y_true,pred)\r\n        inpgrad = inp_tape.gradient(inp_grad_reg_loss,X_tensor)\r\n        return inp_grad_reg_loss + lambda_*(tf.norm(inpgrad))\r\n\r\n\r\ndef loss_wrapper(X,lambda_,model):\r\n  def custom_loss(y_true, y_pred):\r\n      return keras_loss(y_true,y_pred, X,lambda_,model)\r\n  return custom_loss\r\n\r\n\r\nmodel2 = tf.keras.Sequential([Dense(128,input_shape= (68,100,1) , activation='relu',name = \"dense\")])\r\n\r\nmodloss = loss_wrapper(model2.input, 0.1,model2)\r\n\r\nmodel2.compile(optimizer='adam', \r\n              loss=modloss,\r\n              metrics=['accuracy'])\r\n  \r\n\r\n\r\n---------------------------------------------------------------------------\r\n\r\nValueError                                Traceback (most recent call last)\r\n\r\n<ipython-input-24-9e442b918a59> in <module>()\r\n      9               loss=modloss,\r\n     10               #'categorical_crossentropy',\r\n---> 11               metrics=['accuracy'])\r\n     12 \r\n\r\n11 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    444 \r\n    445       # Creates the model loss and weighted metrics sub-graphs.\r\n--> 446       self._compile_weights_loss_and_weighted_metrics()\r\n    447 \r\n    448       # Functions for train, test and predict will\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)\r\n   1590       #                   loss_weight_2 * output_2_loss_fn(...) +\r\n   1591       #                   layer losses.\r\n-> 1592       self.total_loss = self._prepare_total_loss(masks)\r\n   1593 \r\n   1594   def _prepare_skip_target_masks(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _prepare_total_loss(self, masks)\r\n   1650 \r\n   1651           if hasattr(loss_fn, 'reduction'):\r\n-> 1652             per_sample_losses = loss_fn.call(y_true, y_pred)\r\n   1653             weighted_losses = losses_utils.compute_weighted_loss(\r\n   1654                 per_sample_losses,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/losses.py in call(self, y_true, y_pred)\r\n    219       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\r\n    220           y_pred, y_true)\r\n--> 221     return self.fn(y_true, y_pred, **self._fn_kwargs)\r\n    222 \r\n    223   def get_config(self):\r\n\r\n<ipython-input-20-3519391241e3> in cus_loss(y_true, y_pred)\r\n    143 def loss_wrapper(X,lambda_,model):\r\n    144   def cus_loss(y_true, y_pred):\r\n--> 145       return keras_loss(y_true,y_pred, X,lambda_,model)\r\n    146   return cus_loss\r\n    147 \r\n\r\n<ipython-input-20-3519391241e3> in keras_loss(y_true, y_pred, X, lambda_, model)\r\n    129         with tf.GradientTape() as inp_tape:\r\n    130           inp_tape.watch(X_tensor)\r\n--> 131           pred = model.predict(X_tensor,batch_size=None, steps=None)\r\n    132           # print(pred)\r\n    133           # inp_grad_reg_loss = tf.losses.categorical_crossentropy(Y,pred)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n   1011         max_queue_size=max_queue_size,\r\n   1012         workers=workers,\r\n-> 1013         use_multiprocessing=use_multiprocessing)\r\n   1014 \r\n   1015   def reset_metrics(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py in predict(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\r\n    719     batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)\r\n    720     x, _, _ = model._standardize_user_data(\r\n--> 721         x, check_steps=True, steps_name='steps', steps=steps)\r\n    722     return predict_loop(\r\n    723         model,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n   2340     # Validates `steps` argument based on x's type.\r\n   2341     if check_steps:\r\n-> 2342       training_utils.check_steps_argument(x, steps, steps_name)\r\n   2343 \r\n   2344     # First, we build the model on the fly if necessary.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py in check_steps_argument(input_data, steps, steps_name)\r\n   1293       raise ValueError('When using {input_type} as input to a model, you should'\r\n   1294                        ' specify the `{steps_name}` argument.'.format(\r\n-> 1295                            input_type=input_type_str, steps_name=steps_name))\r\n   1296     return True\r\n   1297 \r\n\r\nValueError: When using data tensors as input to a model, you should specify the `steps` argument.\r\n\r\n\r\n\r\n```", "@devpunjabi Can you please share a standalone code to reproduce the issue faster? Thanks!", "```\r\npip install tensorflow==2.1.0\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense\r\n\r\ndef keras_loss(y_true,y_pred,X,lambda_, model):\r\n\r\n        X_tensor = tf.convert_to_tensor(X)\r\n        with tf.GradientTape() as inp_tape:\r\n          inp_tape.watch(X_tensor)\r\n          pred = model.predict(X_tensor,steps=None)\r\n          inp_grad_reg_loss = tf.keras.losses.categorical_crossentropy(y_true,pred)\r\n        inpgrad = inp_tape.gradient(inp_grad_reg_loss,X_tensor)\r\n        return inp_grad_reg_loss + lambda_*(tf.norm(inpgrad))\r\n\r\n\r\ndef loss_wrapper(X,lambda_,model):\r\n  def custom_loss(y_true, y_pred):\r\n      return keras_loss(y_true,y_pred, X,lambda_,model)\r\n  return custom_loss\r\n\r\n\r\nmodel2 = tf.keras.Sequential([Dense(128,input_shape= (68,100,1) , activation='relu',name = \"dense\"),\r\n                              Dense(4 , activation='relu',name = \"dense2\")\r\n                              ])\r\n\r\nmodloss = loss_wrapper(model2.input, 0.1,model2)\r\n\r\nmodel2.compile(optimizer='adam', \r\n              loss=modloss,\r\n              metrics=['accuracy'])\r\n\r\n```\r\n\r\n@jvishnuvardhan is this sufficient? because it is failing at compile itself.\r\n", "Please update", "@devpunjabi I tried running your code in `tf-nightly` and don't see any error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/ac62381a69ad529aa69f2dba335d705d/untitled173.ipynb). Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38118, "title": "Refactor ExecuteTrtEngine.", "body": "This PR refactors [ExecuteTrtEngine](https://github.com/tensorflow/tensorflow/blob/34b86e53d7faaa62a9b62946a3a0a6d65c517eba/tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc#L653-L867) so that its components can be reused from [BuildAndRun](https://github.com/tensorflow/tensorflow/blob/34b86e53d7faaa62a9b62946a3a0a6d65c517eba/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc#L1287-L1353).\r\n\r\nBuildAndRun is used by the TF-TRT op converter tests to initialize TRT bindings and execute inference. Its role is roughly equivalent to ExecuteTrtEngine. Recently ExecuteTrtEngine was modified to handle inference using dynamic shapes (PRs #36379, #36434, #36435), the same was not done for the opconverter's BuildAndRun. To allow unit testing of op converters in dynamic shape mode, one needs the extend BuildAndRun with similar changes.\r\n\r\nThis PR refactors ExecuteTRTEngine, so that both BuildAndRun and ExecuteTrtEngie can use a common set of utility functions to run inference. While doing these changes, the logging is streamlined: we return a status with the log message if an error occurs.  \r\n\r\nFor easier comparison, one could do something like:\r\n```\r\ngit show master:tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc | sed -n '619,868p' > ExecuteTrtEngine_master.cc && diff ExecuteTrtEngine_master.cc tensorflow/compiler/tf2tensorrt/utils/trt_engine_utils.cc\r\n```\r\n\r\nThis PR does not add new tests, nor does it modify any existing behavior. This PR is a prerequisite of op converter tests in dynamic shape mode. \r\n\r\nAn alternative solution to this PR would be to copy the relevant code to BuildAndRun, but this would further increase the already existing code duplication between ExecuteTrtEngine and BuildAndRun. Furthermore, the exact way of specifying the input shapes is subject to change (for example currently we do not specify [input shape bindings](https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/classnvinfer1_1_1_i_execution_context.html#a306a7c94521d6a211be715ceb07c8c80), and it might be required in the future). Therefore it is preferred to reuse code between BuildAndRun and ExecuteTrtEngine.\r\n\r\nTagging @DEKHTIARJonathan and @bixia1 for review.\r\n", "comments": ["@tfeher Can you please address Ubuntu Sanity errors? Thanks!", "Fixed the sanity error.", "The merge process is blocked due to the error below, can you please fix this?\r\ntensorflow/compiler/tf2tensorrt/convert/utils.cc:169:3: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n  TensorShapeUtils::MakeShape(trt_dims.data(), trt_dims.size(), &shape);\r\n  ^~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/compiler/tf2tensorrt/convert/utils.cc:179:3: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n  TensorShapeUtils::MakeShape(trt_dims.d, trt_dims.nbDims, &shape);\r\n  ^~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n2 errors generated.", "Thanks @bixia1, I have fixed the problems with the unused results. "]}, {"number": 38117, "title": "Using tf.constant in the model causes saving to fail", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): **Windows 10**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): **binary**\r\n- TensorFlow version (use command below): v2.2.0-rc1-34-ge6e5d6df2a 2.2.0-rc2\r\n- Python version: 3.7\r\n - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX 970, 4GB memory\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhenever I try to save a model which has a tf.constant as input, it fails with \"IndexError: list index out of range\" from inside Keras.\r\nIf instead of tf.constant I use a keras Input and pass the same value, everything seems fine.\r\nNote that the model trains and infers successfully, so this is probably an export issue.\r\n\r\n**Describe the expected behavior**\r\nSave should succeed.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n        import numpy as np\r\n        import tensorflow as tf\r\n        from tensorflow.keras.layers import Input, Dense, Concatenate, TimeDistributed, Bidirectional, LSTM\r\n        from tensorflow.keras.models import Model\r\n        from pathlib import Path\r\n\r\n        batch_size = 32\r\n        max_sentence_len = 80\r\n        recurrent_size = 96\r\n        embedding_dim = 1024\r\n\r\n        encoder_inputs = Input(batch_shape=(batch_size, max_sentence_len, embedding_dim), dtype='float32')\r\n        encoder_lstm = Bidirectional(\r\n            LSTM(recurrent_size, return_sequences=True, return_state=True))\r\n        (encoder_out, encoder_fwd_hstate, encoder_fwd_cstate, encoder_back_hstate, encoder_back_cstate) = encoder_lstm(\r\n            encoder_inputs)\r\n\r\n        decoder_lstm = LSTM(recurrent_size * 2, return_sequences=True, return_state=True)\r\n\r\n        **inp = tf.constant(np.zeros((batch_size, 2, 2)), dtype='float32')**\r\n        decoder_out, decoder_hstate, decoder_cstate = decoder_lstm(\r\n            inp, initial_state=[Concatenate(axis=-1)([encoder_fwd_hstate, encoder_back_hstate]),\r\n                                Concatenate(axis=-1)([encoder_fwd_cstate, encoder_back_cstate])]\r\n        )\r\n\r\n        dense = Dense(2, activation='softmax')\r\n        dense_time = TimeDistributed(dense)\r\n        decoder_pred = dense_time(decoder_out)\r\n\r\n        full_model = Model(inputs=encoder_inputs, outputs=decoder_pred)\r\n        full_model.compile(optimizer='adam', loss='binary_crossentropy')\r\n\r\n        save_dir = Path('D:\\\\tmp\\\\train_logs\\\\saved_model\\\\')\r\n        save_dir.mkdir(parents=True, exist_ok=True)\r\n\r\n        full_model.save(str(save_dir))\r\n\r\n\r\n**Other info / logs** \r\n2020-04-01 16:01:14.975117: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nWARNING:tensorflow:From C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n2020-04-01 16:01:17.343365: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-04-01 16:01:17.533497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:04:00.0 name: GeForce GTX 970 computeCapability: 5.2\r\ncoreClock: 1.253GHz coreCount: 13 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 208.91GiB/s\r\n2020-04-01 16:01:17.533758: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-01 16:01:17.537501: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-04-01 16:01:17.541404: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-04-01 16:01:17.542637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-04-01 16:01:17.546712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-04-01 16:01:17.549502: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-04-01 16:01:17.558631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-01 16:01:17.559629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-04-01 16:01:17.559985: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-04-01 16:01:17.568641: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28f8bd6e360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-01 16:01:17.568829: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-01 16:01:17.569437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:04:00.0 name: GeForce GTX 970 computeCapability: 5.2\r\ncoreClock: 1.253GHz coreCount: 13 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 208.91GiB/s\r\n2020-04-01 16:01:17.569701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-01 16:01:17.569832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-04-01 16:01:17.570177: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-04-01 16:01:17.570366: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-04-01 16:01:17.570541: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-04-01 16:01:17.570676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-04-01 16:01:17.570798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-01 16:01:17.571827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-04-01 16:01:18.141216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-01 16:01:18.141366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-04-01 16:01:18.141448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-04-01 16:01:18.142398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2991 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\n2020-04-01 16:01:18.145329: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28fa934abb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-04-01 16:01:18.145500: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 970, Compute Capability 5.2\r\nWARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nWARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nWARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nC:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:820: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\r\n  if (isinstance(inputs, collections.Sequence)\r\nWARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\n\r\nError\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\unittest\\case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\unittest\\case.py\", line 628, in run\r\n    testMethod()\r\n  File \"D:\\Work\\Stuff\\brain\\doctor\\models\\texttoimprovement\\model_test.py\", line 48, in test_reproduce\r\n    full_model.save(str(save_dir))\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1047, in save\r\n    signatures, options)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\", line 138, in save_model\r\n    signatures, options)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\save.py\", line 78, in save\r\n    save_lib.save(model, filepath, signatures, options)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 951, in save\r\n    obj, export_dir, signatures, options, meta_graph_def)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 1008, in _build_meta_graph\r\n    checkpoint_graph_view)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_serialization.py\", line 75, in find_function_to_export\r\n    functions = saveable_view.list_functions(saveable_view.root)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 143, in list_functions\r\n    self._serialization_cache)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1593, in _list_functions_for_serialization\r\n    Model, self)._list_functions_for_serialization(serialization_cache)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\", line 2439, in _list_functions_for_serialization\r\n    .list_functions_for_serialization(serialization_cache))\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\base_serialization.py\", line 87, in list_functions_for_serialization\r\n    fns = self.functions_to_serialize(serialization_cache)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\layer_serialization.py\", line 77, in functions_to_serialize\r\n    serialization_cache).functions_to_serialize)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\layer_serialization.py\", line 92, in _get_serialized_attributes\r\n    serialization_cache)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\model_serialization.py\", line 47, in _get_serialized_attributes_internal\r\n    default_signature = save_impl.default_save_signature(self.obj)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\save_impl.py\", line 203, in default_save_signature\r\n    fn.get_concrete_function()\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 959, in get_concrete_function\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 865, in _get_concrete_function_garbage_collected\r\n    self._initialize(args, kwargs, add_initializers_to=initializers)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 506, in _initialize\r\n    *args, **kwds))\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2667, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 441, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saving_utils.py\", line 132, in _wrapped_model\r\n    outputs = model(inputs, training=False)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\", line 778, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 714, in call\r\n    convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 883, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\", line 778, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 2830, in call\r\n    return self._make_op(inputs)\r\n  File \"C:\\Users\\Gilthans\\anaconda3\\envs\\brain\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 2843, in _make_op\r\n    graph = inputs[0].graph\r\nIndexError: list index out of range\r\n", "comments": ["@Gilthans,\r\nI was able to run the given code without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/65ae786b4d058caba56fa9f4e8a88972/38117.ipynb).\r\n\r\nCould you please try running the code in a virtual environment and check if you are able to reproduce the error? Thanks!", "Any updates regarding this issue? Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Hi there, I am getting this warning too: \r\ntensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\n\r\nAlso, it isn't taking all my samples while running. I have over two million samples but there is only 80286 samples have been taken by LSTM TensorFlow. \r\n\r\nSuccessfully opened dynamic library libcublas.so.10\r\n 9865/80286 [==>...........................] - ETA: 25:56 - loss: 23.0515 - accuracy: 0.6120  \r\n\r\n", "@Madhurananda,\r\nPlease submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "> \r\n> \r\n> @Gilthans,\r\n> I was able to run the given code without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/65ae786b4d058caba56fa9f4e8a88972/38117.ipynb).\r\n> \r\n> Could you please try running the code in a virtual environment and check if you are able to reproduce the error? Thanks!\r\n\r\nhttps://colab.research.google.com/gist/ll01/7e2d7dda7cbd22ca0e547f439e82e012/38117.ipynb"]}, {"number": 38116, "title": "Error converting MobileNet and MobileNetV2 to tflite (FusedBatchedNormV3)", "body": "**System information**\r\nLinux Ubuntu 18.04:\r\nUsed pip install, conda install\r\nUsed 2.0.0 , 2.1.0, tf-nightly 2.2.0-dev20200401\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nimport os\r\nimport tensorflow as tf\r\n#import tensorflow_addons as tfa\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pathlib\r\nfrom sklearn.utils import class_weight\r\nprint(tf.__version__)\r\nprint(\"Eagerly enabled: \", tf.executing_eagerly())\r\n\r\nmodel.load_weights(\"MobileNet_Model3_with_Reg_6_c.h5\") \r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nConverterError: See console for info.\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2020-04-01 07:24:52.175680: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183563: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183617: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183639: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183661: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183680: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183700: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183720: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183739: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183756: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183775: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183795: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183817: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183834: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183853: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183870: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183890: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183907: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183927: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183945: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183965: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183982: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.184002: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.184021: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.184039: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.184058: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.184077: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.185918: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 118 operators, 397 arrays (0 quantized)\r\n2020-04-01 07:24:52.188931: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 118 operators, 397 arrays (0 quantized)\r\n2020-04-01 07:24:52.201162: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 90 operators, 396 arrays (0 quantized)\r\n2020-04-01 07:24:52.205488: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 89 operators, 395 arrays (0 quantized)\r\n2020-04-01 07:24:52.208850: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 88 operators, 393 arrays (0 quantized)\r\n2020-04-01 07:24:52.212215: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 88 operators, 393 arrays (0 quantized)\r\n2020-04-01 07:24:52.215005: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 88 operators, 393 arrays (0 quantized)\r\n2020-04-01 07:24:52.220046: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 4513344 bytes, theoretical optimal value: 4513344 bytes.\r\n2020-04-01 07:24:52.220849: E tensorflow/lite/toco/toco_tooling.cc:462] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, PAD, RELU6, SOFTMAX. Here is a list of operators for which you will need custom implementations: FusedBatchNormV3.\r\nTraceback (most recent call last):\r\n  File \"/home/vectorweb4/.local/bin/toco_from_protos\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/vectorweb4/.local/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/vectorweb4/.local/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, PAD, RELU6, SOFTMAX. Here is a list of operators for which you will need custom implementations: FusedBatchNormV3.\r\n\r\n", "comments": ["@eaaarmah, Try with these lines of code to convert \r\n```\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops=True\r\nconverter.experimental_new_converter =True\r\ntflite_model = converter.convert()\r\n```\r\nLet us know how it progresses. Thanks", "The coverter worked.\r\n\r\nHowever when I tried to save it with the code below, I got this error message\r\n```\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\ninterpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\r\ninterpreter.allocate_tensors()\r\n```\r\nError message\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-64-3ad6e1cdc2f2> in <module>\r\n      1 open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n      2 interpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\r\n----> 3 interpreter.allocate_tensors()\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py in allocate_tensors(self)\r\n    242   def allocate_tensors(self):\r\n    243     self._ensure_safe()\r\n--> 244     return self._interpreter.AllocateTensors()\r\n    245 \r\n    246   def _safe_to_run(self):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)\r\n    104 \r\n    105     def AllocateTensors(self):\r\n--> 106         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n    107 \r\n    108     def Invoke(self):\r\n\r\nRuntimeError: Encountered unresolved custom op: FusedBatchNormV3.Node number 2 (FusedBatchNormV3) failed to prepare.\r\n\r\n", "@eaaarmah Can you please share a standalone code to reproduce the issue? Standalone code leads to faster resolution of your issue. Thanks!", "https://colab.research.google.com/drive/1Sz2B7Tt4x5qhNFXFJc4nJBnQRcHaWus8\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops=True\r\nconverter.experimental_new_converter =True\r\ntflite_model = converter.convert()\r\n\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\ninterpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```", "Hi Team, how's it going with this bug?", "@eaaarmah Did you forgot to add `model` building code? I get this error. \r\n`\r\nNameError: name 'model' is not defined`\r\n\r\nAlways try to please share a standalone code that runs and produces the error you are facing. Thanks!", "I am closing this issue as it is a duplicate of https://github.com/tensorflow/tensorflow/issues/38113. We will follow the issue there. Please feel free to reopen if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38116\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38116\">No</a>\n"]}, {"number": 38115, "title": "TFLu: Fix compilation errors for cmsis-nn/softmax.cc", "body": "", "comments": []}, {"number": 38114, "title": "[BUG] map method of tf.data.Dataset has a bug, TensorFlow version = 2.1.0", "body": "One example of map method in the following official website said that map_func get same shape and dtype as 'tf.Tensor', however it's NOT\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/Dataset#map\r\n```\r\ndataset = Dataset.range(5) \r\n# `map_func` takes a single argument of type `tf.Tensor` with the same \r\n# shape and dtype. \r\nresult = dataset.map(lambda x: x + 1) \r\n```\r\n\r\nAccording to the official example, I think `item` in `_func` of the following code should be an EagetTensor but it turns out to be a Tensor instead.\r\n```\r\nimport tensorflow as tf\r\n\r\ndef _func(item):\r\n    # I expect an EagerTensor bug get a Tensor here\r\n    print(type(item)) # ==> <class 'tensorflow.python.framework.ops.Tensor'>\r\n    return item\r\n\r\ntensor = tf.convert_to_tensor(['hello', 'world'])\r\nprint(type(tensor)) # ==> <class 'tensorflow.python.framework.ops.EagerTensor'>\r\ndataset = tf.data.Dataset.from_tensor_slices(tensor)\r\ndataset.map(_func)\r\n```\r\nI want to use .numpy() to convert an EagerTensor to numpy arrays and then make some operations using numpy, but very suprisingly I got a Tensor in `_func` and sadly I don't know how to make it for Tensor", "comments": ["from the following website I know it is a feature rather than a bug for the issue I submit, but I don't think this is a good feature.\r\nhttps://www.tensorflow.org/api_docs/python/tf/executing_eagerly\r\n\r\n>Eager execution is enabled by default and this API returns `True` in most of cases. However, this API might return `False` in the following use cases.\r\n>\r\n>- Executing inside [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), unless under [`tf.init_scope`](https://www.tensorflow.org/api_docs/python/tf/init_scope) or [`tf.config.experimental_run_functions_eagerly(True)`](https://www.tensorflow.org/api_docs/python/tf/config/experimental_run_functions_eagerly) is previously called.\r\n>- Executing inside a transformation function for `tf.dataset`.\r\n>- [`tf.compat.v1.disable_eager_execution()`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/disable_eager_execution) is called.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38114\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38114\">No</a>\n"]}, {"number": 38113, "title": "Custom Implementation for FusedBatchNormV3", "body": "**System information**\r\n- Linux Ubuntu 18.04:\r\n- TensorFlow installed from (source ):\r\n- TensorFlow version 2.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nConverterError: See console for info.\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/vectorweb4/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2020-04-01 07:24:52.175680: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183563: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183617: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183639: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183661: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183680: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183700: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183720: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183739: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183756: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183775: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183795: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183817: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183834: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183853: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183870: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183890: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183907: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183927: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183945: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183965: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.183982: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.184002: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.184021: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.184039: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.184058: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.184077: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-01 07:24:52.185918: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 118 operators, 397 arrays (0 quantized)\r\n2020-04-01 07:24:52.188931: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 118 operators, 397 arrays (0 quantized)\r\n2020-04-01 07:24:52.201162: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 90 operators, 396 arrays (0 quantized)\r\n2020-04-01 07:24:52.205488: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 89 operators, 395 arrays (0 quantized)\r\n2020-04-01 07:24:52.208850: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 88 operators, 393 arrays (0 quantized)\r\n2020-04-01 07:24:52.212215: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 88 operators, 393 arrays (0 quantized)\r\n2020-04-01 07:24:52.215005: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 88 operators, 393 arrays (0 quantized)\r\n2020-04-01 07:24:52.220046: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 4513344 bytes, theoretical optimal value: 4513344 bytes.\r\n2020-04-01 07:24:52.220849: E tensorflow/lite/toco/toco_tooling.cc:462] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, PAD, RELU6, SOFTMAX. Here is a list of operators for which you will need custom implementations: FusedBatchNormV3.\r\nTraceback (most recent call last):\r\n  File \"/home/vectorweb4/.local/bin/toco_from_protos\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/vectorweb4/.local/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/vectorweb4/.local/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/vectorweb4/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, PAD, RELU6, SOFTMAX. Here is a list of operators for which you will need custom implementations: FusedBatchNormV3.\r\n\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\n\r\n**Any other info / logs**\r\n\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-31-e3b422911500> in <module>\r\n----> 1 tflite_model = converter.convert()\r\n      2 # open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n      3 # interpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\r\n      4 # interpreter.allocate_tensors()\r\n\r\n~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py in convert(self)\r\n    444         input_tensors=input_tensors,\r\n    445         output_tensors=output_tensors,\r\n--> 446         **converter_kwargs)\r\n    447 \r\n    448     if self._is_calibration_quantize():\r\n\r\n~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    447       input_data.SerializeToString(),\r\n    448       debug_info_str=debug_info_str,\r\n--> 449       enable_mlir_converter=enable_mlir_converter)\r\n    450   return data\r\n    451 \r\n\r\n~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    198       stdout = _try_convert_to_unicode(stdout)\r\n    199       stderr = _try_convert_to_unicode(stderr)\r\n--> 200       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    201   finally:\r\n    202     # Must manually cleanup files.\r\n", "comments": ["@eaaarmah \r\n\r\nYou need to enable tensorflow supported ops as follows.\r\n\r\nCan you please try adding the following line.. Please check and let us know. Thanks!\r\n\r\n```\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\nStill if you are facing any issue, please share standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "```\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```\r\nI still get this error\r\n2020-04-07 10:33:42.222432: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.237560: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.237744: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.237841: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.237931: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.238012: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.238095: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.238191: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.238290: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.238375: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.238466: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.238556: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.238655: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.238754: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.238863: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.238968: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.239082: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.239183: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.239300: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.239399: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.239498: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.239587: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.239668: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.239725: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.239779: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.239825: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.239875: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FusedBatchNormV3\r\n2020-04-07 10:33:42.250164: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 118 operators, 397 arrays (0 quantized)\r\n2020-04-07 10:33:42.256413: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 118 operators, 397 arrays (0 quantized)\r\n2020-04-07 10:33:42.275014: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 90 operators, 396 arrays (0 quantized)\r\n2020-04-07 10:33:42.282230: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 89 operators, 395 arrays (0 quantized)\r\n2020-04-07 10:33:42.288136: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 88 operators, 393 arrays (0 quantized)\r\n2020-04-07 10:33:42.294826: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 88 operators, 393 arrays (0 quantized)\r\n2020-04-07 10:33:42.300395: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 88 operators, 393 arrays (0 quantized)\r\n2020-04-07 10:33:42.310516: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 4513344 bytes, theoretical optimal value: 4513344 bytes.\r\n2020-04-07 10:33:42.311749: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311778: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311792: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311805: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311817: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311829: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311840: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311852: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311864: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311875: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311887: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311899: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311910: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311922: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311934: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311945: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311956: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311968: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311979: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.311991: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312002: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312014: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312025: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312037: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312049: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312061: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312072: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312147: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312161: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312174: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312187: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312199: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312210: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312222: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312234: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312246: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312258: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312270: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312282: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312293: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312305: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312317: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312329: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312361: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312374: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312386: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312398: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312410: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312422: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312433: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312446: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312458: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312470: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312482: W tensorflow/lite/toco/tflite/operator.cc:2654] Op FusedBatchNormV3 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-07 10:33:42.312587: E tensorflow/lite/toco/toco_tooling.cc:462] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\n", "```\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops=True\r\nconverter.experimental_new_converter =True\r\ntflite_model = converter.convert()\r\n```\r\nThis works for me", "However the interpreter does not work\r\n```\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\ninterpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\r\ninterpreter.allocate_tensors()\r\n```\r\n\r\nError Message\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-67-b97efc9f1a9b> in <module>\r\n      1 # open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n      2 # interpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\r\n----> 3 interpreter.allocate_tensors()\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py in allocate_tensors(self)\r\n    242   def allocate_tensors(self):\r\n    243     self._ensure_safe()\r\n--> 244     return self._interpreter.AllocateTensors()\r\n    245 \r\n    246   def _safe_to_run(self):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)\r\n    104 \r\n    105     def AllocateTensors(self):\r\n--> 106         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n    107 \r\n    108     def Invoke(self):\r\n\r\nRuntimeError: Encountered unresolved custom op: FusedBatchNormV3.Node number 2 (FusedBatchNormV3) failed to prepare.", "@eaaarmah \r\n\r\nRequest you to provide colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "https://colab.research.google.com/drive/1Sz2B7Tt4x5qhNFXFJc4nJBnQRcHaWus8#scrollTo=kllyHdqIlXIC", "https://colab.research.google.com/drive/1Sz2B7Tt4x5qhNFXFJc4nJBnQRcHaWus8", "Hi, please are there any updates on the issue?", "@eaaarmah Can you please share a standalone code? I cannot run the shared colab. Can you please share the model and data for testing. You can use public/toy data to demonstrate the issue. Thanks! ", "1) If you're converting the model with SELECT ops, you will need to use the tensorflow-lite-select which has the TF ops\r\nSee https://www.tensorflow.org/lite/guide/ops_select#running_the_model\r\n\r\n2) Regarding the conversion issue, FusedBatchNormV3 should be supported. Can you share details about the params to the FusedBatchNormV3 op in the model. Also, which version are you using, it doesn't look from the stack trace that you're using recent build.", "https://colab.research.google.com/drive/1Sz2B7Tt4x5qhNFXFJc4nJBnQRcHaWus8\r\n\r\nAnyone can edit this colab", "> ---------------------------------------------------------------------------\r\n\r\n> RuntimeError                              Traceback (most recent call last)\r\n> <ipython-input-4-1e94fc0f3f29> in <module>\r\n>       1 # open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n>       2 interpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\r\n> ----> 3 interpreter.allocate_tensors()\r\n> \r\n> ~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py in allocate_tensors(self)\r\n>     242   def allocate_tensors(self):\r\n>     243     self._ensure_safe()\r\n> --> 244     return self._interpreter.AllocateTensors()\r\n>     245 \r\n>     246   def _safe_to_run(self):\r\n> \r\n> ~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)\r\n>     104 \r\n>     105     def AllocateTensors(self):\r\n> --> 106         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n>     107 \r\n>     108     def Invoke(self):\r\n> \r\n> RuntimeError: Encountered unresolved custom op: FusedBatchNormV3.Node number 2 (FusedBatchNormV3) failed to prepare.\r\n\r\nI am getting the same runtime error", "> 2\\. build\r\n\r\nI am getting the same error during runtime. The error starts when I run\r\n`interpreter.allocate_tensors()`\r\n\r\nI am using tensorflow 2.0\r\n\r\nWhich build is supported? Does that mean it is not supported in TF 2.0?", "> ---------------------------------------------------------------------------\r\n> RuntimeError                              Traceback (most recent call last)\r\n> <ipython-input-39-1fdbb42a2ac9> in <module>\r\n>       2 #byte-sight/byte_sight/promising_weights/exception-model-10-0.9462.h5\r\n>       3 interpreter = tf.lite.Interpreter(model_path=\"./byte_sight/promising_weights/converted_model.tflite\")\r\n> ----> 4 interpreter.allocate_tensors()\r\n> \r\n> ~/anaconda3/envs/tf21/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py in allocate_tensors(self)\r\n>     245   def allocate_tensors(self):\r\n>     246     self._ensure_safe()\r\n\r\nGot this error with TF 2.1\r\n> --> 247     return self._interpreter.AllocateTensors()\r\n>     248 \r\n>     249   def _safe_to_run(self):\r\n> \r\n> ~/anaconda3/envs/tf21/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)\r\n>     108 \r\n>     109     def AllocateTensors(self):\r\n> --> 110         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n>     111 \r\n>     112     def Invoke(self):\r\n> \r\n> RuntimeError: Encountered unresolved custom op: FusedBatchNormV3.Node number 2 (FusedBatchNormV3) failed to prepare.", "@eaaarmah I think this was resolved in recent `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/bbde9d5ac7e8e66384ef2c5a9a19e31b/tfliteconversion.ipynb) is the gist for your reference.\r\n\r\nCan you please verify once and close the issue if this was resolved for you. Thanks!", "Hi. I have a similar issue when I try to Invoke the interpreter. \r\n![image](https://user-images.githubusercontent.com/3530663/95645424-a9ba2e80-0a73-11eb-8dab-30eee3b6f4eb.png)\r\n\r\nTflite conversion is passed without error:\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('saved_model/')\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops=True\r\nconverter.experimental_new_converter =True\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\ninterpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n\r\n\r\n**model definition:**\r\n\r\n#define LSTM model with skip connection\r\ndef LSTM_net():\r\n    N_CLASSES=4\r\n\r\n    i = Input(shape=(79, 40), name='input')\r\n    x = Masking()(i)\r\n    x = LayerNormalization(name='layer_norm')(x)\r\n    s = TimeDistributed(Dense(64, activation='tanh'),\r\n                        name='td_dense_tanh')(x)\r\n    x = Bidirectional(LSTM(128, return_sequences=True),\r\n                             name='bidirectional_lstm')(s)\r\n    x = concatenate([s, x], axis=2, name='skip_connection')\r\n    x = Dense(64, activation='relu', name='dense_1_relu')(x)\r\n    x = MaxPooling1D(name='max_pool_1d')(x)\r\n    x = Dense(32, activation='relu', name='dense_2_relu')(x)\r\n    x = Flatten(name='flatten')(x)\r\n    x = Dropout(rate=0.5, name='dropout')(x)\r\n    x = Dense(32, activation='relu',\r\n                         activity_regularizer=regularizers.l2(0.001),\r\n                         name='dense_3_relu')(x)\r\n    o = Dense(N_CLASSES, activation='softmax', name='softmax')(x)\r\n\r\n    model = Model(inputs=i, outputs=o, name='long_short_term_memory')\r\n\r\n    return model\r\n\r\nHowever I dont get any error when I use BatchNormalization instead of LayerNormalization. Not sure if that makes any difference.  But Model performance is not as expected with the batchnormalization. I need to use LayerNormalization for my project. Please help as soon as possible.\r\n", "Closing this issue as it is fixed in latest version of TensorFlow [V2.5](https://colab.research.google.com/gist/sushreebarsa/c8842924151b0dd13b2ce7135b3dda91/tfliteconversion.ipynb#scrollTo=mgzrbQIo6CXB). Please feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38113\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38113\">No</a>\n"]}, {"number": 38112, "title": "Add MPI cluster resolver and update documentation of SLURM cluster resolver", "body": "@frankchn As discussed I updated the documentation for the Slurm cluster resolver enhanced in #36159 which became outdated.\r\n\r\nI also added the MPI cluster resolver mentioned there and included it in the documentation update to avoid conflicts or dependencies between those 2 related changes.\r\n\r\n2 design decisions of the MPI cluster resolver I wanted to highlight:\r\n- import of mpi4py only on constructor (only place where it is needed) to allow importing the file even when the package is not installed. Can be useful when conditionally switching on the used resolver and can reduce program startup time.\r\n- removed parameter `tasks_per_node` (actually just didn't add it). As I highlight in the Slurm doc file using it is usually not a good idea as the resolver is able to do that itself and it is very easy to use wrong and hard to use right. I don't see a usecase for that, especially when using MPI", "comments": ["@frankchn The error is\r\n> do_check_futures_test: Check that python files have certain __future__ imports\r\n\r\nI omitted those present in other files as they are only required for Python 2 (AFAIK) and TF announced dropping Python 2 after 2.1(?) Isn't that true? Should the check be changed or shall I add the future imports? That would be\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n```\r\n\r\nAlthough nothing of that is used...", "@Flamefire For consistency can you just add it in? I think all our files have it since we have some code internally still on Py2 so we need to maintain compatibility right now. Thanks!", ">  I think all our files have it since we have some code internally still on Py2 so we need to maintain compatibility right now\r\n\r\nOh, so what is about that announcement:\r\n\r\n> TensorFlow will also stop supporting Python 2 starting January 1, 2020\r\n\r\nDo I also have to adapt the `super()` call to the Python 2 version? Added the future imports for now to make CI happy.", "Yeah for the open source builds we are not supporting Python 2 any more, but internally within Google the migration to Python 3 isn't complete, so that's why the pre-submit checks are still around to enforce the compatibility headers.\r\n\r\nYour code won't be run internally, so I think it would be fine to leave it as is otherwise.", "I see, thanks for the explanation. Then this is ready (again) I believe", "@frankchn Seeing a (possibly unrelated) failure:\r\n> //tensorflow/compiler/mlir/tensorflow/tests/mlir2graphdef:tf_add.mlir.test FAILED\r\n\r\nIs this good to go anyway? Maybe even for 2.2 as https://github.com/tensorflow/tensorflow/pull/36159 is part of 2.2 and this is the corresponding docu update.\r\n\r\nMaybe one final thing: There is a question https://github.com/tensorflow/tensorflow/issues/36094#issuecomment-609973779 about how to use this and indeed there is no documentation on how to use a ClusterResolver with a Strategy in an example. I could add it here, but I think it would be even better to add it to the main docu. So probably out of scope of this PR.", "Yeah that test failure is unrelated. \r\n\r\nI think we are getting some internal tests done and this will be merged in the next day or two. Not sure if this will make it into 2.2 though since there has already been RC cuts and they are only really willing to accept bug fixes at this point. Will definitely be in 2.3 (cut in ~May-ish) though.", "Hi @Flamefire, just got updated on an internal discussion and we are actually thinking of moving some of the cluster resolvers out of the TPU repo into a repo owned by a SIG instead (maybe SIG-Networking). \r\n\r\nStill happy to accept the documentation update though, so can you remove the mpi_cluster_resolver.py from the PR, and create a new Issue re: mpi_cluster_resolver and assign it to me and @jhseu? \r\n\r\nWe'll update you on whether we decide to move the cluster resolvers soon. Thanks!", "Sorry about the back and forth!", "Moved the documentation update to a new PR with an appropriate branch and updated commit and readme. --> https://github.com/tensorflow/tensorflow/pull/38355\r\n\r\nIssue Re MPI resolver added: https://github.com/tensorflow/tensorflow/issues/38356\r\n\r\nClosing this."]}, {"number": 38111, "title": "Add Zephyr support to TF Lite micro", "body": "This PR add support for Zephyr RTOS in TF Lite micro. The PR includes two demos:\r\n\r\n* hello_world\r\n* magic_wand\r\n\r\nThe demos targets sofcore RISC-V implementation running in FPGA. More info and automatic tests can be found in https://github.com/antmicro/litex-vexriscv-tensorflow-lite-demo", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38111) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@kgugala Thank you for your contribution. Can you please sign CLA? Thanks!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38111) for more info**.\n\n<!-- ok -->", "@gbaned done", "@kgugala Can you please fix build failures ? Thanks!", "@gbaned can you explain how to do that? I can see that tests on master also fail. I also can't open the \"Details\" links at least for some of the failures. So what is the criterion we should be looking at?", "Or should we just aim for \"only those tests fail that also fail on master\"? That would probably be doable.", "@petewarden Can you please assist on above comments from @kgugala. Thanks!", "I looked at the following logs:\r\nhttps://source.cloud.google.com/results/invocations/62bc0a7c-d21c-4447-86c2-83250d0f0f26/log\r\n\r\nIn there I see:\r\n\r\n```\r\nIn file included from ./tensorflow/lite/kernels/internal/reference/reference_ops.h:55:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:51,\r\n                 from tensorflow/lite/kernels/split_v.cc:19:\r\n./tensorflow/lite/kernels/internal/reference/reduce.h:23:10: fatal error: tensorflow/lite/kernels/internal/min.h: No such file or directory\r\n #include \"tensorflow/lite/kernels/internal/min.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/lite/kernels/BUILD:425:1: Couldn't build file tensorflow/lite/kernels/_objs/builtin_op_kernels/slice.pic.o: C++ compilation of rule\r\n```\r\n\r\nThis is usually a sign that headers have been added to the repo, but not included in the appropriate Bazel BUILD files. My guess from inspection is that you'll need to add the min.h etc headers to the reference_base target:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/BUILD#L431\r\n\r\nYou can verify this locally by running a `bazel test tensorflow/lite:all`", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38111) for more info**.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38111) for more info**.\n\n<!-- ok -->", "@petewarden @gbaned I updated PR and added `min.h` `max.h` to `cppmath` in `tensorflow/lite/kernels/internal/BUILD`, as I think this is better place then `reference_base`. If it is not correct, I can move it to `reference_base`.", "@petewarden I had to fix formatting in the BUILD file. Can you restart the kokoro?", "@petewarden Could you restart build again? There was missing `,` in the BUILD file. Now it should be OK.", "@petewarden Could you please restart build once more? Now I installed `buildifier` to ensure BUILD file is correctly formatted.", "Hi all, just checking up, what else is needed to merge this?", "@petewarden @gbaned is there anything we need to address in this PR before it can be merged?", "I hope that wraps it up -- please review and merge if that's all once the checks complete!", "What else needs to happen here?", "@petewarden can you please help merge this PR ?", "I took a look at the internal build tests and saw these errors:\r\n\r\n```\r\n0508 09:12:19.938 TASK: [418/418] Transform Verify match '(Copyright 20[1-9][0-9]((.|\r\n)*)Apache License, Version 2\\.0|Licensed to the Apache Software Foundation)'\r\n0508 09:12:28.998 ERROR: File 'tensorflow/lite/micro/examples/hello_world/zephyr_riscv/prj.conf' failed validation 'Verify match '(Copyright 20[1-9][0-9]((.|\r\n)*)Apache License, Version 2\\.0|Licensed to the Apache Software Foundation)''.\r\n0508 09:12:28.998 ERROR: File 'tensorflow/lite/micro/examples/hello_world/zephyr_riscv/src/assert.cc' failed validation 'Verify match '(Copyright 20[1-9][0-9]((.|\r\n)*)Apache License, Version 2\\.0|Licensed to the Apache Software Foundation)''.\r\n0508 09:12:28.998 ERROR: File 'tensorflow/lite/micro/examples/magic_wand/zephyr_riscv/boards/litex_vexriscv.overlay' failed validation 'Verify match '(Copyright 20[1-9][0-9]((.|\r\n)*)Apache License, Version 2\\.0|Licensed to the Apache Software Foundation)''.\r\n0508 09:12:28.998 ERROR: File 'tensorflow/lite/micro/examples/magic_wand/zephyr_riscv/prj.conf' failed validation 'Verify match '(Copyright 20[1-9][0-9]((.|\r\n)*)Apache License, Version 2\\.0|Licensed to the Apache Software Foundation)''.\r\n0508 09:12:28.998 ERROR: File 'tensorflow/lite/micro/examples/magic_wand/zephyr_riscv/src/assert.cc' failed validation 'Verify match '(Copyright 20[1-9][0-9]((.|\r\n)*)Apache License, Version 2\\.0|Licensed to the Apache Software Foundation)''.\r\n0508 09:12:29.002 ERROR: 5 file(s) failed the validation of Verify match '(Copyright 20[1-9][0-9]((.|\r\n)*)Apache License, Version 2\\.0|Licensed to the Apache Software Foundation)'.\r\n```\r\nCan you add license comments to the start of these files?", "@petewarden I updated PR and added missing copyright headers.", "@kgugala can you please check this error , it has been failing internally due to this error \r\n\r\n`ERROR: third_party/tensorflow/core/kernels/BUILD:7075:11: Couldn't build file third_party/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/conv_ops_fused_float.o: C++ compilation of rule '//third_party/tensorflow/core/kernels:portable_tensorflow_kernels' failed (Timeout) wrapped_clang failed: error executing command \r\n(16:41:13) FAILED: Build did NOT complete successfully`", "Hi @rthadur \r\n\r\nI couldn\u2019t find the `//third_party/tensorflow/core/kernels:portable_tensorflow_kernels` target, but I found `//tensorflow/core/kernels:portable_tensorflow_kernels`. I\u2019m assuming `third_party` is just some internal CI folder. Is that correct?\r\n\r\nAssuming, the `//tensorflow/core/kernels:portable_tensorflow_kernels` target is correct, could you provide more details about the fail. To be more precise - which configuration of the target is failing?\r\n\r\nI was able to build the `//tensorflow/core/kernels:portable_tensorflow_kernels` target on my local machine. What I noticed was that when building the target for Android configuration it consumes a lot of memory. Building with `-j12` used over 30GB of RAM and caused the build process to be killed by the OOM killer. However, `-j6` worked perfectly fine. Could this be a reason for the failures in CI?", "@petewarden @rthadur We have rebased this PR on top of current master.\r\n\r\nAlso, we noticed that the build system complains about overriding recipes. We fixed that in #39901", "@kgugala here is the internal we are seeing , can you please check this \r\n\r\n`file included from tensorflow/lite/micro/kernels/reduce.cpp:16:0:\r\ntensorflow/lite/kernels/internal/reference/reduce.h:21:10: fatal error: tensorflow/lite/kernels/internal/max.h: No such file or directory\r\n #include \"tensorflow/lite/kernels/internal/max.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nMultiple libraries were found for \"TensorFlowLite.h\"\r\nError: build failed: exit status 1`", "@rthadur the failing file `tensorflow/lite/micro/kernels/reduce.cpp` does not seem to exist in the public repo https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/kernels (it doesn't seem to be present in the repo history either). Can you point us to the exact version of the TF you're testing?", "This is actually an issue with the Arduino build process, here's a more full version of the error:\r\n\r\n```\r\nIn file included from /root/Arduino/libraries/tensorflow_lite/src/tensorflow/lite/micro/kernels/reduce.cpp:16:0:\r\n/root/Arduino/libraries/tensorflow_lite/src/tensorflow/lite/kernels/internal/reference/reduce.h:21:10: fatal error: tensorflow/lite/kernels/internal/max.h: No such file or directory\r\n #include \"tensorflow/lite/kernels/internal/max.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n```\r\n\r\nYou can run this automated test build yourself with the following script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/tools/ci_build/test_arduino.sh\r\n\r\nFrom looking at the changes, I think you need to add the min/max.h files to this list in the makefile:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/tools/make/Makefile#L187\r\n\r\nThanks for your patience with this process, hopefully we're nearly there!", "@petewarden thanks! Should be fixed now.", "Changes got submitted internally, waiting for auto-merge to happen. Thank you"]}, {"number": 38110, "title": "Semantic search BERT model to TFLITE", "body": "0\r\n\r\n\r\nI have this code for semantic search engine built using the pre-trained bert model. I want to convert this model into tflite for deploying it to google mlkit. I want to know how to convert it. I want to know if its even possible to convert this into tflite. It might be because its mentioned on the official tensorflow site :https://www.tensorflow.org/lite/convert. But I dont know where to begin\r\n\r\ncode:\r\n```python\r\nfrom sentence_transformers import SentenceTransformer\r\n\r\n# Load the BERT model. Various models trained on Natural Language Inference (NLI) https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/nli-models.md and \r\n# Semantic Textual Similarity are available https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md\r\n\r\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')\r\n\r\n# A corpus is a list with documents split by sentences.\r\n\r\nsentences = ['Absence of sanity', \r\n             'Lack of saneness',\r\n             'A man is eating food.',\r\n             'A man is eating a piece of bread.',\r\n             'The girl is carrying a baby.',\r\n             'A man is riding a horse.',\r\n             'A woman is playing violin.',\r\n             'Two men pushed carts through the woods.',\r\n             'A man is riding a white horse on an enclosed ground.',\r\n             'A monkey is playing drums.',\r\n             'A cheetah is running behind its prey.']\r\n\r\n# Each sentence is encoded as a 1-D vector with 78 columns\r\nsentence_embeddings = model.encode(sentences)\r\n\r\nprint('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\r\n\r\nprint('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])\r\n\r\n#@title Sematic Search Form\r\n\r\n# code adapted from https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.py\r\n\r\nquery = 'Nobody has sane thoughts' #@param {type: 'string'}\r\n\r\nqueries = [query]\r\nquery_embeddings = model.encode(queries)\r\n\r\n# Find the closest 3 sentences of the corpus for each query sentence based on cosine similarity\r\nnumber_top_matches = 3 #@param {type: \"number\"}\r\n\r\nprint(\"Semantic Search Results\")\r\n\r\nfor query, query_embedding in zip(queries, query_embeddings):\r\n    distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0]\r\n\r\n    results = zip(range(len(distances)), distances)\r\n    results = sorted(results, key=lambda x: x[1])\r\n\r\n    print(\"\\n\\n======================\\n\\n\")\r\n    print(\"Query:\", query)\r\n    print(\"\\nTop 5 most similar sentences in corpus:\")\r\n\r\n    for idx, distance in results[0:number_top_matches]:\r\n        print(sentences[idx].strip(), \"(Cosine Score: %.4f)\" % (1-distance))\r\n```", "comments": ["@ali9653,\r\nIf you have saved your model to a file, then you can use the TFLite converter Python API to convert your model to TFLite. Please follow [this](https://www.tensorflow.org/lite/convert/python_api) official guide to convert your saved model. Thanks!", "> @ali9653,\r\n> If you have saved your model to a file, then you can use the TFLite converter Python API to convert your model to TFLite. Please follow [this](https://www.tensorflow.org/lite/convert/python_api) official guide to convert your saved model. Thanks!\r\nHow can I save this model its a pre-trained model. The link does not have instructions of converting a bert model", "@ali9653,\r\nYou can select the BERT model of your choice from [this](https://tfhub.dev/s?q=bert) official TF Hub link and follow the user guide given. Thanks!", "> @ali9653,\r\n> You can select the BERT model of your choice from [this](https://tfhub.dev/s?q=bert) official TF Hub link and follow the user guide given. Thanks!\r\n\r\nI'm sorry but I think you are not understanding the question. I know how to import a model from tensorflow hub be it bert or universal sentence encoder. I want to convert it into TfLite is the question. There is no user guide for conversation", "@ali9653,\r\nPlease take a look at [this example](https://www.tensorflow.org/lite/convert/python_api#end-to-end_mobilenet_conversion_) for end to end model conversion.", "> @ali9653,\r\n> Please take a look at [this example](https://www.tensorflow.org/lite/convert/python_api#end-to-end_mobilenet_conversion_) for end to end model conversion.\r\n\r\nI'm sorry you are still not understanding. The model they have used Mobilenet_v2 is directly supported. Mine is imported from tensorflow hub and there is no documentation regarding that. ", "Have you tried using pre-trained BERT models [Mobile Bert](https://www.tensorflow.org/lite/models/bert_qa/overview#performance_benchmarks), [Albert](https://tfhub.dev/tensorflow/albert_lite_base/1).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 38109, "title": "[MLIR][LINALG] Legalizing tensor-type linalg operations to memref-type ones.", "body": "In this PR, we used [Buffer Assignment](https://github.com/tensorflow/tensorflow/pull/37212) to convert the type of Linalg-GenericOp operands and results from Tensor-type to Memref-type.", "comments": ["This looks great, could you please send it as a patch to MLIR core, this is where it should live so we can all use it. \r\nThanks!", "@dfki-ehna Can you please check @silvasean's comments and keep us posted. Thanks!", "@dfki-ehna Any update on this PR? and please resolve conflicts.  Thanks!", "We ported this PR to [MLIR](https://reviews.llvm.org/D78996) and we are closing this."]}, {"number": 38108, "title": "(SOLVE) while convert SavedModel to tflite: ValueError: as_list() is not defined on an unknown TensorShape.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.1.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\ntflite_convert --output_file=model.tflite --saved_model_dir . --input_arrays=serving_default_input_1 --input_shapes=1,800,800,3 --output_arrays=StatefulPartitionedCall_1\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-04-01 15:38:11.537934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2020-04-01 15:38:11.540538: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\r\n2020-04-01 15:38:12.243351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-01 15:38:12.251378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-01 15:38:12.251935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-01 15:38:12.252000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-01 15:38:12.252535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-01 15:38:12.252651: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:12.252693: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:12.252731: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:12.252767: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:12.252806: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:12.252842: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:12.252869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-01 15:38:12.252875: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-04-01 15:38:12.253276: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-01 15:38:12.259296: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3191935000 Hz\r\n2020-04-01 15:38:12.259632: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563e568f9430 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-01 15:38:12.259645: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-01 15:38:12.402933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-01 15:38:12.406735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-01 15:38:12.407499: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563e5698fad0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-04-01 15:38:12.407515: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-04-01 15:38:12.407520: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-04-01 15:38:12.407689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-01 15:38:12.407697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      \r\n2020-04-01 15:38:13.801484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-01 15:38:13.802395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-01 15:38:13.803088: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\r\n2020-04-01 15:38:13.803208: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-04-01 15:38:13.804559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-01 15:38:13.805223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-01 15:38:13.805314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-01 15:38:13.805958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-01 15:38:13.806094: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:13.806151: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:13.806196: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:13.806240: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:13.806283: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:13.806326: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:13.806339: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-01 15:38:13.806346: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-04-01 15:38:13.806423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-01 15:38:13.806432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1 \r\n2020-04-01 15:38:13.806440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N N \r\n2020-04-01 15:38:13.806445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   N N \r\n2020-04-01 15:38:13.922212: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-04-01 15:38:13.922539: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 1542 nodes (1413), 2635 edges (2506), time = 71.453ms.\r\n2020-04-01 15:38:13.922555: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 1.68ms.\r\n2020-04-01 15:38:15.070366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-01 15:38:15.071111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-01 15:38:15.071664: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\r\n2020-04-01 15:38:15.071742: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-04-01 15:38:15.072130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-01 15:38:15.072644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-01 15:38:15.072689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-01 15:38:15.073234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-01 15:38:15.073368: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:15.073410: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:15.073445: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:15.073480: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:15.073520: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:15.073552: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\n2020-04-01 15:38:15.073562: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-01 15:38:15.073568: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-04-01 15:38:15.073658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-01 15:38:15.073664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1 \r\n2020-04-01 15:38:15.073668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N N \r\n2020-04-01 15:38:15.073671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   N N \r\n2020-04-01 15:38:15.346279: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-04-01 15:38:15.346362: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 1300 nodes (-204), 2189 edges (-408), time = 140.857ms.\r\n2020-04-01 15:38:15.346373: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 1300 nodes (0), 2189 edges (0), time = 89.953ms.\r\nTraceback (most recent call last):\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/bin/tflite_convert\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 594, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 577, in run_main\r\n    _convert_tf2_model(tflite_flags)\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 235, in _convert_tf2_model\r\n    tflite_model = converter.convert()\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 442, in convert\r\n    shape_list = tensor.shape.as_list()\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 1166, in as_list\r\n    raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\nValueError: as_list() is not defined on an unknown TensorShape.\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/4413909/model.zip)\r\nit's small and about 1.7MB\r\n```\r\n\r\n**Failure details**\r\ncan not convert to tflite\r\n```\r\n2020-04-01 16:44:20.851662: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-04-01 16:44:20.851708: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 1300 nodes (-204), 2189 edges (-408), time = 106.827ms.\r\n2020-04-01 16:44:20.851715: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 1300 nodes (0), 2189 edges (0), time = 61.656ms.\r\nTraceback (most recent call last):\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/bin/tflite_convert\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 594, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 577, in run_main\r\n    _convert_tf2_model(tflite_flags)\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 235, in _convert_tf2_model\r\n    tflite_model = converter.convert()\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 442, in convert\r\n    shape_list = tensor.shape.as_list()\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 1166, in as_list\r\n    raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\nValueError: as_list() is not defined on an unknown TensorShape.\r\n```\r\n\r\n\r\n**Any other info / logs**\r\n\r\nthe model was train with tensorflow 2.0.0a0, but I convert it under tensorflow 2.1.0\r\n", "comments": ["by the way I am not very familiar with tensorflow, does anyone can tell me how to find the input/output node of my model? currently I check it via Netron but the render is totallly a mess", "To [determine the names of input and output nodes](https://developer.arm.com/solutions/machine-learning-on-arm/developer-material/how-to-guides/optimizing-neural-networks-for-mobile-and-embedded-devices-with-tensorflow/determine-the-names-of-input-and-output-nodes) you may use TensorBoard.\r\nAlso can you test it with tf-nightly?", "after try to compile it with tf-nightly, I got the follow output\r\n```\r\n2020-04-02 10:14:01.528508: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:01.528549: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-04-02 10:14:02.772786: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-02 10:14:03.402604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:14:03.403166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-02 10:14:03.403213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:14:03.403863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 1 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-02 10:14:03.403958: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:03.404002: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:03.404040: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:03.404074: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:03.404113: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:03.404149: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:03.407325: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-02 10:14:03.407352: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1581] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-04-02 10:14:03.407685: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-04-02 10:14:03.414224: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3191935000 Hz\r\n2020-04-02 10:14:03.414733: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x591db70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-02 10:14:03.414748: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-02 10:14:03.416273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1085] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-02 10:14:03.416283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1091]      \r\n2020-04-02 10:14:04.972011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:14:04.972782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:14:04.973468: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\r\n2020-04-02 10:14:04.973567: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-04-02 10:14:05.110304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:14:05.125903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:14:05.126800: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xa01a810 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-04-02 10:14:05.126818: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-04-02 10:14:05.126822: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-04-02 10:14:05.127259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:14:05.127782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-02 10:14:05.127853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:14:05.128495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 1 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-02 10:14:05.128622: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:05.128675: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:05.128709: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:05.128746: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:05.128779: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:05.128815: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:05.128827: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-02 10:14:05.128833: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1581] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-04-02 10:14:05.128894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1085] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-02 10:14:05.128902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1091]      0 1 \r\n2020-04-02 10:14:05.128908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 0:   N N \r\n2020-04-02 10:14:05.128911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 1:   N N \r\n2020-04-02 10:14:05.258266: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:802] Optimization results for grappler item: graph_to_optimize\r\n2020-04-02 10:14:05.258307: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:804]   function_optimizer: Graph size after: 1542 nodes (1413), 2635 edges (2506), time = 69.886ms.\r\n2020-04-02 10:14:05.258311: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:804]   function_optimizer: function_optimizer did nothing. time = 2.362ms.\r\n2020-04-02 10:14:06.511723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:14:06.512397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:14:06.513063: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\r\n2020-04-02 10:14:06.513148: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-04-02 10:14:06.513531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:14:06.514076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-02 10:14:06.514124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:14:06.514794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 1 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-02 10:14:06.514943: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:06.514990: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:06.515029: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:06.515077: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:06.515135: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:06.515171: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:14:06.515182: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-02 10:14:06.515187: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1581] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-04-02 10:14:06.515311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1085] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-02 10:14:06.515319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1091]      0 1 \r\n2020-04-02 10:14:06.515324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 0:   N N \r\n2020-04-02 10:14:06.515328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 1:   N N \r\n2020-04-02 10:14:06.678709: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:802] Optimization results for grappler item: graph_to_optimize\r\n2020-04-02 10:14:06.678785: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:804]   constant_folding: Graph size after: 1300 nodes (-242), 2189 edges (-446), time = 82.118ms.\r\n2020-04-02 10:14:06.678792: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:804]   constant_folding: Graph size after: 1300 nodes (0), 2189 edges (0), time = 42.243ms.\r\nI0402 10:14:06.725161 140247144630016 lite.py:595] Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False\r\nTraceback (most recent call last):\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/bin/tflite_convert\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 638, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 621, in run_main\r\n    _convert_tf2_model(tflite_flags)\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 237, in _convert_tf2_model\r\n    tflite_model = converter.convert()\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 604, in convert\r\n    **converter_kwargs)\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 485, in toco_convert_impl\r\n    input_tensors, output_tensors, *args, **kwargs)\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 387, in build_toco_convert_protos\r\n    for dim in shape:\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 842, in __iter__\r\n    raise ValueError(\"Cannot iterate over a shape with unknown rank.\")\r\nValueError: Cannot iterate over a shape with unknown rank.\r\n```\r\nand I try to visulize it via tensorboard, it still give me error\r\n```\r\n2020-04-02 10:08:36.256005: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2020-04-02 10:08:36.258205: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\r\n2020-04-02 10:08:36.860121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-02 10:08:37.490554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:08:37.491122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-02 10:08:37.491180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:08:37.491838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-02 10:08:37.491941: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-04-02 10:08:37.491988: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:08:37.492022: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:08:37.492064: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:08:37.492101: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:08:37.492138: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 10:08:37.492165: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-02 10:08:37.492173: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-04-02 10:08:37.492545: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-02 10:08:37.498331: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3191935000 Hz\r\n2020-04-02 10:08:37.498776: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c843ef2440 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-02 10:08:37.498796: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-02 10:08:37.651350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:08:37.655401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 10:08:37.656099: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c843f88ae0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-04-02 10:08:37.656115: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-04-02 10:08:37.656120: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-04-02 10:08:37.656311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-02 10:08:37.656318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      \r\nTraceback (most recent call last):\r\n  File \"tensorflow/python/tools/import_pb_to_tensorboard.py\", line 86, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/bigdata/anaconda3/envs/cartoongan_v2/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"tensorflow/python/tools/import_pb_to_tensorboard.py\", line 68, in main\r\n    import_to_tensorboard(FLAGS.model_dir, FLAGS.log_dir)\r\n  File \"tensorflow/python/tools/import_pb_to_tensorboard.py\", line 58, in import_to_tensorboard\r\n    graph_def.ParseFromString(f.read())\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n```\r\nDoes it means there are problems with my model?  the model was train with this [repo](https://github.com/mnicnc404/CartoonGan-tensorflow)", "update:\r\nI regenerate the pb model, and now I have a better model and it render well on Netron, and when I convert it to tflite model, I got \"ValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\"\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/4419478/model.zip)\r\nthis is the new pb model. and the tf output\r\n```\r\n(tf-nightly) zengren@GPU003:~/gomo/CartoonGan-tensorflow-master/optimized_pbs$ tflite_convert --output_file=model.tflite --saved_model_dir ./shinkai --input_arrays Placeholder:0 --output_arrays Cast_1:0 \r\n2020-04-02 14:27:25.319175: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-04-02 14:27:25.319241: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore\r\nI0402 14:27:26.942274 139780310492928 saver.py:1512] Saver not created because there are no variables in the graph to restore\r\n2020-04-02 14:27:26.947938: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-02 14:27:26.956871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 14:27:26.957522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-02 14:27:26.957594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-02 14:27:26.958293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 1 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-04-02 14:27:26.958398: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-04-02 14:27:26.958442: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 14:27:26.958481: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 14:27:26.958519: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 14:27:26.958563: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 14:27:26.958603: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\n2020-04-02 14:27:26.962433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-02 14:27:26.962458: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1581] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-04-02 14:27:26.962909: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-04-02 14:27:26.969604: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3191935000 Hz\r\n2020-04-02 14:27:26.970272: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4abc3d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-02 14:27:26.970299: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-02 14:27:26.972342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1085] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-02 14:27:26.972355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1091]      \r\nTraceback (most recent call last):\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/bin/tflite_convert\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 638, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 621, in run_main\r\n    _convert_tf2_model(tflite_flags)\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 237, in _convert_tf2_model\r\n    tflite_model = converter.convert()\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 520, in convert\r\n    raise ValueError(\"This converter can only convert a single \"\r\nValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\r\n```\r\nI've search this problem and find some information on other issue and stackoverflow, but I still very confused about how to deal with it? does it mean that I need to write some code instead of use tflite_convert binary?", "update: after stuck at this issue for a week, still got nothing\r\nhere is the detaile\r\n```\r\nValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\r\n```\r\nthis error was the output of tflite_convert and the pb model was create with following script\r\n```python\r\nimport os\r\nfrom PIL import Image\r\n\r\ntry:\r\n    import tensorflow.compat.v1 as tf\r\n    tf.disable_v2_behavior()\r\nexcept (ImportError, AttributeError):\r\n    import tensorflow as tf\r\n\r\n\r\nfrom generator import Generator\r\nfrom logger import get_logger\r\n\r\n\r\n# NOTE: TF warnings are too noisy without this\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\n\r\n\r\ndef makedirs(path):\r\n    if not os.path.isdir(path):\r\n        os.makedirs(path)\r\n\r\n\r\ndef main(m_path, out_dir, light=False, test_out=True):\r\n    logger = get_logger(\"tf1_export\", debug=test_out)\r\n    g = Generator(light=light)\r\n    t = tf.placeholder(tf.string, [])\r\n    x = tf.expand_dims(tf.image.decode_jpeg(tf.read_file(t), channels=3), 0)\r\n    x = (tf.cast(x, tf.float32) / 127.5) - 1\r\n    x = g(x, training=False)\r\n    out = tf.cast((tf.squeeze(x, 0) + 1) * 127.5, tf.uint8)\r\n    in_name, out_name = t.op.name, out.op.name\r\n    try:\r\n        with tf.Session() as sess:\r\n            sess.run(tf.global_variables_initializer())\r\n            g.load_weights(tf.train.latest_checkpoint(m_path))\r\n            in_graph_def = tf.get_default_graph().as_graph_def()\r\n            out_graph_def = tf.graph_util.convert_variables_to_constants(\r\n                sess, in_graph_def, [out_name])\r\n        tf.reset_default_graph()\r\n        tf.import_graph_def(out_graph_def, name='')\r\n    except ValueError:\r\n        logger.error(\"Failed to load specified weight.\")\r\n        logger.error(\"If you trained your model with --light, \"\r\n                     \"consider adding --light when executing this script; otherwise, \"\r\n                     \"do not add --light when executing this script.\")\r\n        exit(1)\r\n    makedirs(out_dir)\r\n    m_cnt = 0\r\n    bpath = 'optimized_graph_light' if light else 'optimized_graph'\r\n    # out_path = os.path.join(out_dir, f'{bpath}_{m_cnt:04d}.pb')\r\n    out_path = os.path.join(out_dir, 'saved_model.pb')\r\n    while os.path.exists(out_path):\r\n        m_cnt += 1\r\n        # out_path = os.path.join(out_dir, f'{bpath}_{m_cnt:04d}.pb')\r\n        out_path = os.path.join(out_dir, 'saved_model.pb')\r\n    with tf.gfile.GFile(out_path, 'wb') as f:\r\n        f.write(out_graph_def.SerializeToString())\r\n\r\n    if test_out:\r\n        with tf.Graph().as_default():\r\n            gd = tf.GraphDef()\r\n            with tf.gfile.GFile(out_path, 'rb') as f:\r\n                gd.ParseFromString(f.read())\r\n            tf.import_graph_def(gd, name='')\r\n            tf.get_default_graph().finalize()\r\n            t = tf.get_default_graph().get_tensor_by_name(f\"{in_name}:0\")\r\n            out = tf.get_default_graph().get_tensor_by_name(f\"{out_name}:0\")\r\n            from time import time\r\n            start = time()\r\n            with tf.Session() as sess:\r\n                img = Image.fromarray(sess.run(out, {t: \"input_images/temple.jpg\"}))\r\n                img.show()\r\n            elapsed = time() - start\r\n            logger.debug(f\"{elapsed} sec per img\")\r\n    logger.info(f\"successfully exported ckpt to {out_path}\")\r\n    logger.info(f\"input var name: {in_name}:0\")\r\n    logger.info(f\"output var name: {out_name}:0\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    import argparse\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--m_path\", type=str, default=\"models\")\r\n    parser.add_argument(\"--out_dir\", type=str, default='optimized_pbs')\r\n    parser.add_argument(\"--light\", action='store_true')\r\n    parser.add_argument(\"--not_test_out\", action='store_true')\r\n    args = parser.parse_args()\r\n    main(args.m_path, args.out_dir, args.light, not args.not_test_out)\r\n```\r\n\r\nthe previous error output of tflite_convert\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/bin/tflite_convert\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 638, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 621, in run_main\r\n    _convert_tf2_model(tflite_flags)\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 237, in _convert_tf2_model\r\n    tflite_model = converter.convert()\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 604, in convert\r\n    **converter_kwargs)\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 485, in toco_convert_impl\r\n    input_tensors, output_tensors, *args, **kwargs)\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 387, in build_toco_convert_protos\r\n    for dim in shape:\r\n  File \"/home/bigdata/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 842, in __iter__\r\n    raise ValueError(\"Cannot iterate over a shape with unknown rank.\")\r\nValueError: Cannot iterate over a shape with unknown rank.\r\n```\r\nwas come from the model generate by following script\r\n```python\r\nimport os\r\nfrom subprocess import Popen\r\nimport tensorflow as tf\r\nfrom generator import Generator\r\nfrom logger import get_logger\r\n\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\ntf.get_logger().setLevel(40)\r\n\r\n\r\ndef main(m_path, out_dir, light):\r\n    logger = get_logger(\"export\")\r\n    try:\r\n        g = Generator(light=light)\r\n        g.load_weights(tf.train.latest_checkpoint(m_path))\r\n        t = tf.keras.Input(shape=[800, 800, 3], batch_size=1, name='input_0')\r\n        out = g(t, training=False)\r\n        in_name, out_name = t.op.name, out.op.name\r\n        print('in_name:{}, out_name:{}'.format(in_name,out_name))\r\n        \r\n\r\n        g.summary()\r\n    except ValueError as e:\r\n        logger.error(e)\r\n        logger.error(\"Failed to load specified weight.\")\r\n        logger.error(\"If you trained your model with --light, \"\r\n                     \"consider adding --light when executing this script; otherwise, \"\r\n                     \"do not add --light when executing this script.\")\r\n        exit(1)\r\n    m_num = 0\r\n    smd = os.path.join(out_dir, \"SavedModel\")\r\n    tfmd = os.path.join(out_dir, \"tfjs_model\")\r\n    if light:\r\n        smd += \"Light\"\r\n        tfmd += \"_light\"\r\n    saved_model_dir = f\"{smd}_{m_num:04d}\"\r\n    tfjs_model_dir = f\"{tfmd}_{m_num:04d}\"\r\n    while os.path.exists(saved_model_dir):\r\n        m_num += 1\r\n        saved_model_dir = f\"{smd}_{m_num:04d}\"\r\n        tfjs_model_dir = f\"{tfmd}_{m_num:04d}\"\r\n    tf.saved_model.save(g, saved_model_dir)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    import argparse\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--m_path\", type=str, default='models/')\r\n    parser.add_argument(\"--out_dir\", type=str, default='exported_models')\r\n    parser.add_argument(\"--light\", action='store_true')\r\n    args = parser.parse_args()\r\n    main(args.m_path, args.out_dir, args.light)\r\n```\r\nand the tf.sumary() output\r\n```\r\nin_name:input_0, out_name:Generator/activation_1/Tanh\r\nModel: \"Generator\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nFlatConv (FlatConv)          (1, 800, 800, 96)         7298      \r\n_________________________________________________________________\r\nDownShuffleUnitV2 (DownShuff (1, 400, 400, 192)        29384     \r\n_________________________________________________________________\r\nDownShuffleUnitV2 (DownShuff (1, 200, 200, 384)        114056    \r\n_________________________________________________________________\r\nsequential_15 (Sequential)   (1, 200, 200, 384)        603696    \r\n_________________________________________________________________\r\nUpSampleConv (UpSampleConv)  (1, 400, 400, 192)        93222     \r\n_________________________________________________________________\r\nUpSampleConv (UpSampleConv)  (1, 800, 800, 96)         23574     \r\n_________________________________________________________________\r\nsequential_20 (Sequential)   (1, 800, 800, 3)          7203      \r\n_________________________________________________________________\r\nactivation (Activation)      (1, 800, 800, 3)          0         \r\n=================================================================\r\nTotal params: 878,433\r\nTrainable params: 878,433\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\nconvert command\r\n```\r\ntflite_convert --output_file=model.tflite --saved_model_dir . --input_arrays input_0 --input_shapes=1,800,800,3 --output_arrays Generator/activation_1/Tanh\r\n```\r\n\r\n** pb was generated in tensorflow 2.0.0 alpha 0, and tflite_convert was run in tf_nightly **\r\n", "I'm totally mess up in api of different version and module of tensorflow. does anyone can help me?\r\n\r\nBest Regard", "This problem has been solved thanks to @windmaple\r\nThe reason that cause this error might be the  imcompatibility between tensorflow 2.0 alpha and newest tf_nightly, solved this by adding --enable_v1_converter\r\n```\r\ntflite_convert --saved_model_dir=./SavedModelLight_0000/ --output_file=model.tflite --enable_v1_converter\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38108\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38108\">No</a>\n"]}]