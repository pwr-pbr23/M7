[{"number": 47459, "title": "Error \"failed to connect to all addresses\" when iterating dataset on TPU with Colab or Kaggle", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I'm currently testing out this example [EfficientDet_TPU](https://www.kaggle.com/davidtong/efficientdet-tpu-train-1-epoch-in-4-min)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Kaggle and Google Colab\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nUnable to iterate dataset\r\n![Screenshot from 2021-02-28 16-35-06](https://user-images.githubusercontent.com/19550237/109412442-f8e10b80-79e2-11eb-9822-c07aa298818a.png)\r\n\r\n\r\n**Describe the expected behavior**\r\nAble to iterate dataset\r\n\r\n**Standalone code to reproduce the issue**\r\n You can directly try this example, the tfrecords is provided as well. [EfficientDet_TPU](https://www.kaggle.com/davidtong/efficientdet-tpu-train-1-epoch-in-4-min)\r\n\r\n**Other info / logs** \r\n`UnavailableError                          Traceback (most recent call last)\r\n<ipython-input-33-15ac5583c702> in <module>\r\n----> 1 for batch_x, (batch_regression, batch_classification) in train_dataset_encode:\r\n      2     print(batch_x)\r\n      3     break\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in __next__(self)\r\n    745   def __next__(self):\r\n    746     try:\r\n--> 747       return self._next_internal()\r\n    748     except errors.OutOfRangeError:\r\n    749       raise StopIteration\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    737         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access\r\n    738       except AttributeError:\r\n--> 739         return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n    740 \r\n    741   @property\r\n\r\n/opt/conda/lib/python3.7/contextlib.py in __exit__(self, type, value, traceback)\r\n    117         if type is None:\r\n    118             try:\r\n--> 119                 next(self.gen)\r\n    120             except StopIteration:\r\n    121                 return False\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py in execution_mode(mode)\r\n   2114     finally:\r\n   2115       ctx.executor = executor_old\r\n-> 2116       executor_new.wait()\r\n   2117 \r\n   2118 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/executor.py in wait(self)\r\n     67   def wait(self):\r\n     68     \"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\r\n---> 69     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n     70 \r\n     71   def clear_error(self):\r\n\r\nUnavailableError: failed to connect to all addresses\r\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\r\n:{\"created\":\"@1614500451.196304845\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":4143,\"referenced_errors\":[{\"created\":\"@1614500451.108464009\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":398,\"grpc_status\":14}]}\r\n`\r\n", "comments": ["Related issues:\r\n* https://github.com/tensorflow/tensorflow/issues/39122\r\n* https://github.com/tensorflow/tensorflow/issues/39719\r\n* https://github.com/tensorflow/tensorflow/issues/38762", "> Related issues:\r\n> \r\n> * #39122\r\n> * #39719\r\n> * #38762\r\n\r\n@sayakpaul  I'm currently using GCS Bucket and still facing the same error. Based on the related issues, I couldn't really get a proper solution for this problem.", "So, if you are using `tf.py_function` inside your data loader chances are likely that it is causing the issue. So, in that case, you might want to apply your `tf.py_function` related functionalities to your data, serialize it as shards of TFRecords inside your GCS Bucket and then load them during training time. ", "@StephKua,\r\nPlease take a look at @sayakpaul's comment above and let us know if this is still an issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47459\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47459\">No</a>\n"]}, {"number": 47458, "title": "issue with tf.data.experimental.rejection_resample method", "body": "when I run the below block of code i got this error:\r\n\"Shape must be rank 3 but is rank 2 for '{{node Tile}} = Tile[T=DT_FLOAT, Tmultiples=DT_INT32](ExpandDims, Tile/multiples)' with input shapes: [1,11,11], [2].\"\r\n\r\nI need to balance the \"train_dataset\" which has 11 labels by using the follwoing line of codes:\r\n\r\n1) create class_fun:\r\n```\r\ndef class_func(features, label):\r\n  return label\r\n```\r\n\r\n2)create resampler object with targer_dist:\r\n```\r\nresampler = tf.data.experimental.rejection_resample(\r\n    class_func, target_dist=[0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.1])\r\n\r\n```\r\nWhen run the below line of code I got the error which I have mentioned it above :\r\n\r\n`resample_ds = train_dataset.unbatch().apply(resampler)\r\n`\r\nPlease advise?", "comments": ["@halhwadi \r\n I ran the code shared and face a different issue, please  find the [gist here](https://colab.research.google.com/gist/Saduf2019/e57b5fe550307126e26d8aea24221595/untitled554.ipynb). Please share all dependencies for us to replicate and analyse the issue faced.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47458\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47458\">No</a>\n"]}, {"number": 47456, "title": "how to set C++ standard version for tensenflow as another bazel project dependency", "body": "**System information**\r\n- ubuntu20.04\r\n- TensorFlow version:2.4.0\r\n- Python version:3.8\r\n- Bazel version (if compiling from source):3.7.2\r\n- GCC/Compiler version (if compiling from source):9.3\r\n\r\n**Describe the problem**\r\nmy project depend on tensorflow, compile tensorflow need std=c++14 and above, I set --cxxopt=\"-std=c++14\" in bazel build command for my project, but this option doesn't work for tensorflow repository, I notice -std=c++14 effective for my own project and some other external repository\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nexternal/org_tensorflow/tensorflow/core/platform/default/port.cc:360:46: error: could not convert '{9223372036854775807, 9223372036854775807}' from '<brace-enclosed initializer list>' to 'tensorflow::port::MemoryInfo'\r\n  360 |   MemoryInfo mem_info = {INT64_MAX, INT64_MAX};\r\n      |                                              ^\r\n      |                                              |\r\n      |                                              <brace-enclosed initializer list>\r\n\r\n", "comments": ["@xiedeacc,\r\nIn order to expedite the trouble-shooting process, could you please provide the the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "> @xiedeacc,\r\n> In order to expedite the trouble-shooting process, could you please provide the the exact sequence of commands / steps that you executed before running into the problem. Thanks!\r\n@amahendrakar, I will provide a small project to reproduce this problem, need a few days(sorry for my busy work)\r\n", "copy .tf_configure.bazelrc and .bazelrc from  tensorflow root directory to my own project resolved this problem", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47456\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47456\">No</a>\n"]}, {"number": 47454, "title": "Batch Normalization fails as kernel constraint for Conv layers when using mixed precision", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7\r\n\r\n\r\n**Describe the current behavior**\r\nWhen using `tf.keras.layers.BatchNormalization()` as a constraint in a conv layer using mixed precision, the model cannot train\r\n\r\n**Describe the expected behavior**\r\nUsing `tf.keras.layers.BatchNormalization()` as a constraint in a conv layer behaves the same regardless of using mixed precision or not.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n[Colab link](https://colab.research.google.com/drive/1IFWFwRrYUQx7Kw0I_KdtrKtVhvZbI7hy?usp=sharing).\r\n\r\nI've included a few notes in comments to show that this issue is isolated to conv layers when using mixed precision.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nIt looks like the issue is in the loss_scale_optimizer.\r\n\r\n```txt\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:757 train_step\r\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:498 minimize\r\n        return self.apply_gradients(grads_and_vars, name=name)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py:712 apply_gradients\r\n        args=(grads_and_vars, name, experimental_aggregate_gradients))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2941 merge_call\r\n        return self._merge_call(merge_fn, args, kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2948 _merge_call\r\n        return merge_fn(self._strategy, *args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py:745 _apply_gradients_cross_replica  **\r\n        do_not_apply_fn)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/smart_cond.py:59 smart_cond\r\n        name=name)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:538 new_func\r\n        return func(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py:1180 cond\r\n        return cond_v2.cond_v2(pred, true_fn, false_fn, name)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/cond_v2.py:89 cond_v2\r\n        op_return_value=pred)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:990 func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py:732 apply_fn\r\n        args=(grads, wrapped_vars, name, experimental_aggregate_gradients))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py:755 _apply_gradients\r\n        experimental_aggregate_gradients=experimental_aggregate_gradients)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:635 apply_gradients\r\n        \"name\": name,\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2941 merge_call\r\n        return self._merge_call(merge_fn, args, kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2948 _merge_call\r\n        return merge_fn(self._strategy, *args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:683 _distributed_apply  **\r\n        var, apply_grad_to_update_var, args=(grad,), group=False))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2494 update\r\n        return self._update(var, fn, args, kwargs, group)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3431 _update\r\n        return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3437 _update_non_slot\r\n        result = fn(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:661 apply_grad_to_update_var  **\r\n        return var.assign(var.constraint(var))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/autocast_variable.py:237 assign\r\n        name, read_value)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/autocast_variable.py:209 _apply_assign_update\r\n        assign_op = update_fn(value, use_locking, name, False)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:882 assign\r\n        value_tensor = ops.convert_to_tensor(value, dtype=self.dtype)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py:163 wrapped\r\n        return func(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:1509 convert_to_tensor\r\n        (dtype.name, value.dtype.name, value))\r\n\r\n    ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float16: <tf.Tensor 'cond_1/SGD/SGD/update/batch_normalization_2/FusedBatchNormV3:0' shape=(3, 3, 1, 32) dtype=float16>\r\n```", "comments": ["I am able to replicate the issue reported on tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/0fa70861612a9e33b4ba427d1e506971/untitled554.ipynb)", "@lancerutkin I think this is intended behaviour and kind of limitations of `mixed-precision` computations for some activation layers like BatchNorm and softmax (when it is used as a final layer for prediction). Please check the guide [here](https://www.tensorflow.org/guide/mixed_precision). Also, note that NVDIA recommends small changes required when using mixed precision with BatchNorm and Softmax layers. https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html\r\n\r\nSo, keeping these two lines will work. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/6658f881b52095210ea39df0db3f3dff/untitled554.ipynb)\r\n\r\n```\r\n    self.conv = layers.Conv2D(32, kernel_size=(3, 3), \r\n                              activation=\"relu\",\r\n                              # Comment out below to show have model train \r\n                              # kernel_constraint=keras.layers.BatchNormalization()\r\n                              )\r\n    # Uncomment to show that Batch Norm works as a standalone layer\r\n    self.batch_norm = layers.BatchNormalization() # or\r\n    # self.batch_norm = layers.BatchNormalization(dtype='float32')\r\n```\r\n\r\n### More details:\r\n\r\n> As mentioned in the TF guide,\r\n> \r\n> Each layer has a policy and uses the global policy by default. Each of the Dense layers therefore have the mixed_float16 policy because you set the global policy to mixed_float16 previously. This will cause the dense layers to do float16 computations and have float32 variables. They cast their inputs to float16 in order to do float16 computations, which causes their outputs to be float16 as a result. Their variables are float32 and will be cast to float16 when the layers are called to avoid errors from dtype mismatches.\r\n> \r\n> print('Compute dtype: %s' % policy.compute_dtype) # output: Compute dtype: float16\r\n> print('Variable dtype: %s' % policy.variable_dtype) # output: Variable dtype: float32\r\n\r\n\r\nNVDIA guide recommends the statistics (mean and variance) computed by batch-normalization should be in FP32.\r\n\r\nSo, when you separate activation layer from Conv2D,\r\n\r\n`self.batch_norm = layers.BatchNormalization(dtype='float32')`\r\n\r\n\r\n> Passing dtype='float32' to the BatchNormalization layer constructor overrides the layer's dtype policy to be the float32 policy, which does computations and keeps variables in float32. Equivalently, we could have instead passed dtype=mixed_precision.Policy('float32'); layers always convert the dtype argument to a policy. Because the Activation layer has no variables, the policy's variable dtype is ignored, but the policy's compute dtype of float32 causes BatchNormalization  to be float32.\r\n\r\nI guess we need to add a note in the guide.\r\n\r\nHope it is clear. Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!\r\n ", "@reedwm Do you think we need to add some note on our `mixed_precision` guide about how to use BatchNorm layer. Current guide mentions the issue about `softmax` layer when it is used in the prediction layer. Thanks!", "@jvishnuvardhan thank you for your thorough response - it solved my issue. \r\n\r\nMy two cents: having this information about batch norm on TF's guide would definitely be helpful!\r\n\r\nAlso, FWIW, we don't even need to extract BatchNormalization to its own layer to have it work. This also worked for me:\r\n\r\n```py\r\nself.conv = layers.Conv2D(32, kernel_size=(3, 3), \r\n                              activation=\"relu\",\r\n                              # Comment out below to show have model train \r\n                              kernel_constraint=keras.layers.BatchNormalization(\r\n                                  dtype='float32'\r\n                              )\r\n                            ) \r\n`11", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47454\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47454\">No</a>\n", "Layers are not intended to be passed as a `kernel_constraint`, and the weights of the layers will not be trained when this is done (I'm not sure what your use case is).  But it typically will work as any callable can be passed. With mixed precision, weights are float32 but layer outputs are float16. The `kernel_constraint` output dtype must be the same as the weight dtype, which is why you must pass `dtype='float32'`."]}, {"number": 47453, "title": "[_Derived_]RecvAsync is cancelled Error when training with LSTM on tf-gpu ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n```\r\nmodel = tf.keras.Sequential([\r\n        tf.keras.layers.Embedding(vocab_size, store.embedding_dim,\r\n                                  batch_input_shape=[batch_size, None]),\r\n        tf.keras.layers.Dropout(store.dropout_rate),\r\n        tf.keras.layers.LSTM(store.rnn_units,\r\n                             return_sequences=True,\r\n                             stateful=True,\r\n                             recurrent_initializer=store.rnn_initializer),\r\n        tf.keras.layers.Dropout(store.dropout_rate),\r\n        tf.keras.layers.LSTM(store.rnn_units,\r\n                             return_sequences=True,\r\n                             stateful=True,\r\n                             recurrent_initializer=store.rnn_initializer),\r\n        tf.keras.layers.Dropout(store.dropout_rate),\r\n        tf.keras.layers.Dense(vocab_size)\r\n    ])\r\n    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\r\n\r\n```\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow version (use command below):  v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.8\r\n- CUDA/cuDNN version: CUDA 11.0.3 / CUDNN 8.0.5.77\r\n- GPU model and memory: GTX 1070 (8GB)\r\n\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nI've been working on a LSTM model, when training with $ `model.fit()` , it runs for 6 epochs and then gives this error \r\n\r\n```\r\n  File \"C:\\Users\\Me\\anaconda3\\envs\\tf_gpu24\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"C:\\Users\\Me\\anaconda3\\envs\\tf_gpu24\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\Me\\anaconda3\\envs\\tf_gpu24\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 855, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"C:\\Users\\Me\\anaconda3\\envs\\tf_gpu24\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2942, in __call__\r\n    return graph_function._call_flat(\r\n  File \"C:\\Users\\Me\\anaconda3\\envs\\tf_gpu24\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1918, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"C:\\Users\\Me\\anaconda3\\envs\\tf_gpu24\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 555, in call\r\n    outputs = execute.execute(\r\n  File \"C:\\Users\\Me\\anaconda3\\envs\\tf_gpu24\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.CancelledError:  [_Derived_]RecvAsync is cancelled.\r\n         [[{{node gradient_tape/sequential/embedding/embedding_lookup/Reshape/_20}}]] [Op:__inference_train_function_4800]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nModel should complete training without issue.\r\nI know the code is fine because I trained on the same code with no issue in an old environment I was using 2 months ago. It also runs fine in a CPU only tensorflow environment.  \r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem.\r\n\r\n```\r\nEpoch 1/50\r\n2021-02-27 14:50:38.552734: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-02-27 14:50:38.882403: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-02-27 14:50:39.546250: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-02-27 14:50:39.794953: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n37/37 [==============================] - 7s 55ms/step - loss: 7.0684 - accuracy: 0.1270\r\nEpoch 2/50\r\n37/37 [==============================] - 2s 54ms/step - loss: 4.8889 - accuracy: 0.1828\r\nEpoch 3/50\r\n37/37 [==============================] - 2s 54ms/step - loss: 4.7884 - accuracy: 0.1666\r\nEpoch 4/50\r\n37/37 [==============================] - 2s 54ms/step - loss: 4.6866 - accuracy: 0.1480\r\nEpoch 5/50\r\n37/37 [==============================] - 2s 55ms/step - loss: 4.5179 - accuracy: 0.1630\r\nEpoch 6/50\r\n17/37 [============>.................] - ETA: 1s - loss: 4.2505 - accuracy: 0.14842021-02-27 14:50:55.955000: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(2004): 'cudnnRNNBackwardWeights( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), output_desc.handles(), output_data.opaque(), workspace.opaque(), workspace.size(), rnn_desc.params_handle(), params_backprop_data->opaque(), reserve_space_data->opaque(), reserve_space_data->size())'\r\n2021-02-27 14:50:55.955194: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cudnn_rnn_ops.cc:1926 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 256, 256, 1, 100, 64, 256]\r\n2021-02-27 14:50:55,957 : MainThread : INFO : Saving model history to model_history.csv\r\n2021-02-27 14:50:55,961 : MainThread : INFO : Saving model to D:\\project\\project_engine\\fftest_checkpoints\\batch_0\\synthetic\r\nTraceback (most recent call last):\r\n  File \"runTrain.py\", line 65, in <module>\r\n    model.train()\r\n  ...\r\n  ... \r\n  ...\r\n  File \"D:\\project\\project_engine\\runTrain.py\", line 201, in train_rnn\r\n    model.fit(dataset, epochs=store.epochs, callbacks=_callbacks)\r\n  File \"C:\\Users\\Me\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"C:\\Users\\Me\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\Me\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 855, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"C:\\Users\\Me\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2942, in __call__\r\n    return graph_function._call_flat(\r\n  File \"C:\\Users\\Me\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1918, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"C:\\Users\\Me\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 555, in call\r\n    outputs = execute.execute(\r\n  File \"C:\\Users\\Me\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.CancelledError:  [_Derived_]RecvAsync is cancelled.\r\n         [[{{node gradient_tape/sequential/embedding/embedding_lookup/Reshape/_20}}]] [Op:__inference_train_function_4800]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n", "comments": ["@NasonZ,\r\nOn running the given code snippet, I am facing an error stating `NameError: name 'vocab_size' is not defined`. In order to reproduce the issue reported here, could you please provide the complete code and the dataset you are using.\r\n\r\nAlso, please try limiting GPU memory growth using any of the methods listed [here](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and check if it helps. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47453\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47453\">No</a>\n"]}, {"number": 47452, "title": "Empty Interpreter Input/Output Buffer Pointers", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: Windows 10, building in Visual Studio\r\n- TensorFlow installation: Model trained from PIP package (Tensorflow 2.4.1). TFLite interpreter building 2.0.0 (haven't been able to build TFLite 2.4.1 in Visual Studio yet)\r\n- TensorFlow library: 2.4.1 (Train, conversion) and 2.0.0 (inference)\r\n\r\n### 2. Code\r\n\r\nMy model uses the following layers: \r\nFirst stage, the stage I'm having problems:\r\nTimeDistributed(Conv2D), TimeDistributed(MaxPooling2D), TimeDistributed(BatchNormalization), TimeDistributed(Flatten), TimeDistributed(Dense), \r\n\r\nSecond stage (evaluated independently, haven't gotten to it yet):\r\nBidirectional(GRU), Reshape, Dense, Dropout.\r\n\r\nI am quantizing this model. The inference code is shown below.\r\n\r\n`\tFS_FILE* pModelFile;\r\n\tchar ModelStage1Filename[] = \"\\\\Models\\\\cls_stg1.tflite\";\r\n\tchar ModelStage2Filename[] = \"\\\\Models\\\\cls_stg2.tflite\";\r\n\tint BytesRead;\r\n\r\n\tpModelFile = fs_fopen(ModelStage1Filename, \"r\");\r\n\tif (pModelFile != 0)\r\n\t{\r\n\t\tStage1ModelSize = fs_fread(ModelStage1ModelBinary, 1, sizeof(ModelStage1ModelBinary), pModelFile);\r\n\t\tfs_fclose(pModelFile);\r\n\t}\r\n\tpModelFile = fs_fopen(ModelStage2Filename, \"r\");\r\n\tif (pModelFile != 0)\r\n\t{\r\n\t\tStage1ModelSize = fs_fread(ModelStage2ModelBinary, 1, sizeof(ModelStage2ModelBinary), pModelFile);\r\n\t\tfs_fclose(pModelFile);\r\n\t}\r\n\r\n\tpMicroErrorReporter = new tflite::MicroErrorReporter();\r\n\tpErrorReporter = pMicroErrorReporter;\r\n\r\n\r\n\tif (Stage1ModelSize > 0)\r\n\t{\r\n\t\tStage1Model = tflite::GetModel(ModelStage1ModelBinary);\r\n\t\tif (Stage1Model->version() != TFLITE_SCHEMA_VERSION)\r\n\t\t{\r\n\t\t\t/*TF_LITE_REPORT_ERROR(error_reporter,\r\n\t\t\t\t\"Model provided is schema version %d not equal \"\r\n\t\t\t\t\"to supported version %d.\\n\",\r\n\t\t\t\tmodel->version(), TFLITE_SCHEMA_VERSION);*/\r\n\t\t}\r\n\t\telse\r\n\t\t{\r\n\t\t\tpStage1Interpreter = new tflite::MicroInterpreter(Stage1Model, resolver, stage_1_tensor_arena, stage_1_tensor_arena_size, pErrorReporter);\r\n\t\t\tif (pStage1Interpreter != 0)\r\n\t\t\t{\r\n\t\t\t\tpStage1Interpreter->AllocateTensors();\r\n\t\t\t\t// Obtain a pointer to the model's input tensor\r\n\t\t\t\tTfLiteTensor* input = pStage1Interpreter->input(0);\r\n\t\t\t\tif (input != 0)\r\n\t\t\t\t{\r\n\t\t\t\t\tif (input->dims->size == 4 &&\r\n\t\t\t\t\t\tinput->type == kTfLiteUInt8 &&\r\n\t\t\t\t\t\tinput->dims->data[0] == 1 &&\r\n\t\t\t\t\t\tinput->dims->data[1] == 37 &&\r\n\t\t\t\t\t\tinput->dims->data[2] == 256 &&\r\n\t\t\t\t\t\tinput->dims->data[3] == 1)\r\n\t\t\t\t\t{\r\n\t\t\t\t\t\tTfLiteTensor* output = pStage1Interpreter->output(0);\r\n\t\t\t\t\t\tif (output != 0)\r\n\t\t\t\t\t\t{\r\n\t\t\t\t\t\t\tif (output->dims->size == 2 &&\r\n\t\t\t\t\t\t\t\toutput->type == kTfLiteUInt8 &&\r\n\t\t\t\t\t\t\t\toutput->dims->data[0] == 1 &&\r\n\t\t\t\t\t\t\t\toutput->dims->data[1] == 64)\r\n\t\t\t\t\t\t\t{\r\n\t\t\t\t\t\t\t\t// All Good!\r\n\t\t\t\t\t\t\t\tStage1Good = 1;\r\n\t\t\t\t\t\t\t\tpStage1Input = pStage1Interpreter->typed_input_tensor<unsigned char>(0);\r\n\t\t\t\t\t\t\t\tpStage1Output = pStage1Interpreter->typed_output_tensor<unsigned char>(0);\r\n\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\telse\r\n\t{\r\n\t}`\r\n\r\n\r\n\r\n### 3. Failure after conversion\r\n\r\nMy fully trained model is split into two parts, a non-RNN first stage and an RNN second stage. When I run the inference code above to prepare to do inference on the first stage, everything looks good, the input and output tensor shapes and sizes are correct. But executing the lines below, I get null pointers, so I am unable to perform inference.\r\n\r\npStage1Input = pStage1Interpreter->typed_input_tensor<unsigned char>(0);\r\npStage1Output = pStage1Interpreter->typed_output_tensor<unsigned char>(0);\r\n\r\nWhen I use this code and setup with a simple model with a single dense layer, I get valid pointers. \r\n\r\n### 4. (optional) RNN conversion support\r\nThis first stage does not include the RNN (the Bidirectional(GRU) ) parts of the full model.\r\n\r\n### 5. (optional) Any other info / logs\r\nI have tried to pull in Tensorflow Lite 2.4.1 into my Visual Studio project, however I am currently unable to resolve all of the dependencies, as the Windows make process seemed to work before in version 2.0.0 but not now in 2.4.1. There is a failure getting flatbuffers. So I don't know for sure if there is an issue with the difference in training/converting in 2.4.1 and performing inference using 2.0.0.\r\n", "comments": ["@embeddetech The above code looks correct. Generally, there are two TF versions, one for training and conversion and one for inference. Could you make sure that the TF version used for inference should have the same TF version or higher with the TF version for conversion? We do not guarantee the compatibility that the TF for inference has a lower version than the TF for training and conversion.\r\n", "Please take a look at https://www.tensorflow.org/lite/guide/build_cmake to build the recent TensorFlow Lite version under Windows 10.", "Thank you @abattery, I wasn't having any luck with bazel or make, I will try this cmake method. I just need to be able to see it build in Windows 10 to make sure I have the correct dependencies, then I can pull it into Visual Studio.\r\n\r\nWould it be expected to have those input and output tensor pointers be null if there were version compatibility issues? Should be no issues with the layers I mentioned above executing in TensorFlow Lite, right?\r\n\r\nThank you!", "For bazel, you can build a .dll with ```bazel build -c opt //tensorflow/lite/c:tensorflowlite_c```.", "TFLite 2.0.0 version is quite an old version, which was released more than one year ago to be supported. It would be better to see this problem is happening at the recent TF version.", "OK, I'll try to get it upgraded. The first issue is just getting it to build with all source code properly in place. The second issue is getting it to build in Visual Studio with MSVC compiler. I believe I've come across two compiler incompatibilities, which I was able to resolve, things like MSVC not being happy about empty array initializers. I might do a pull request on those after confirming.\r\n\r\nThat bazel option, is that tested on Windows? I was trying a different target on Windows and it was choking on a flatbuffers package checksum error I believe.", "The above bazel build target has been used for building a dll for Windows. If the above flatbuffer package checksum issue is still happening at the HEAD, please report it to us with the full log messages. I think it might be considered as a separate issue.", "The CMake build _seemed_ to work, though I do get an error below.\r\n\r\n`  quantize.cc\r\n  random_standard_normal.cc\r\n  random_uniform.cc\r\n**C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.28.29333\\include\\random(1876,5): error C2338: invalid template argu\r\nment for uniform_int_distribution: N4659 29.6.1.1 [rand.req.genl]/1e requires one of short, int, long, long long, unsigned short, unsigned int, unsig\r\nned long, or unsigned long long [C:\\Users\\jtork\\CMake\\minimal_build\\tensorflow-lite\\tensorflow-lite.vcxproj]**\r\nC:\\Users\\jtork\\CMake\\tensorflow_src\\tensorflow\\lite\\kernels\\random_uniform.cc(43): message : see reference to class template instantiation 'std::unif\r\norm_int_distribution<int8_t>' being compiled [C:\\Users\\jtork\\CMake\\minimal_build\\tensorflow-lite\\tensorflow-lite.vcxproj]\r\nC:\\Users\\jtork\\CMake\\tensorflow_src\\tensorflow\\lite\\kernels\\random_uniform.cc(145): message : see reference to function template instantiation 'void\r\ntflite::ops::custom::random_uniform::`anonymous-namespace'::RandomUniformSample<int8_t,std::uniform_int_distribution<int8_t>>(std::default_random_eng\r\nine &,T *,size_t,T,T)' being compiled [C:\\Users\\jtork\\CMake\\minimal_build\\tensorflow-lite\\tensorflow-lite.vcxproj]\r\n          with\r\n          [\r\n              T=int8_t\r\n          ]\r\n**C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.28.29333\\include\\random(1876,1): error C2338: note: char, signed ch\r\nar, unsigned char, char8_t, int8_t, and uint8_t are not allowed [C:\\Users\\jtork\\CMake\\minimal_build\\tensorflow-lite\\tensorflow-lite.vcxproj]**\r\n  range.cc\r\n  rank.cc\r\n  read_variable.cc\r\n  Generating Code...\r\n  Compiling...\r\n  reduce.cc\r\n\r\n`\r\n\r\nAfter those errors, it seems to keep going. It finishes printing out all of the files and then stops, I don't get a clear sense of whether linking was happening or just compiling. How do I confirm that it built properly?", "@embeddetech The above random uniform op is recently added and the error can be fixed by a simple fix. Will provide a fix with few days.", "Thank you for your help, @abattery!", "@abattery Do you know if CMake only works with Visual Studio Professional, not Community? I used Professional in the results above, however actually opening Visual Studio Pro and using it for a C# application, I was unable to get a valid license for some reason. So I uninstalled Professional and installed Community and started getting this error when trying to build:\r\n\r\n```\r\nC:\\Users\\jtork\\CMake\\tflite_build>cmake --build . -j\r\nCMake Error:\r\n  Generator\r\n\r\n    Visual Studio 16 2019\r\n\r\n  could not find specified instance of Visual Studio:\r\n\r\n    C:/Program Files (x86)/Microsoft Visual Studio/2019/Professional\r\n```\r\n\r\nI since uninstalled and reinstalled CMake and it still produces the same result....\r\n", "@embeddetech Sorry. I do not know the details of the differences of the Visual Studio editions.", "@abattery I found that when performing step 4 of the CMake build link you provided above, it checks for an installer and uses that from then on. After doing that step is when I uninstalled Pro and installed Community. After blowing everything away and starting over, I can see in that step 4 where it finds Community instead of Professional.\r\n\r\n```\r\nC:\\Users\\jtork\\CMake\\tflite_build>cmake ../tensorflow_src/tensorflow/lite\r\n-- Building for: Visual Studio 16 2019\r\n. . . \r\n**-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.28.29333/bin/Hostx64/x64/cl.exe - skipped**\r\n```\r\n\r\nSo it seems to be working now, other then the issue with random uniform.", "Just FYI, there are a few other suspicious strings emitted that may or may not be real errors during the execution of \"CMake .../tensorflow_src/tensorflow/lite\":\r\n\r\n```\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - not found\r\n```\r\n\r\n\r\n```\r\n-- Performing Test EIGEN_COMPILER_SUPPORT_CPP11\r\n-- Performing Test EIGEN_COMPILER_SUPPORT_CPP11 - Failed\r\n-- Performing Test COMPILER_SUPPORT_std=cpp03\r\n-- Performing Test COMPILER_SUPPORT_std=cpp03 - Failed\r\n```\r\n\r\n```\r\n-- Performing Test FARMHASH_HAS_BUILTIN_EXPECT\r\n-- Performing Test FARMHASH_HAS_BUILTIN_EXPECT - Failed\r\n```\r\n\r\n", "@abattery I just attempted to run the make option for building a tensorflow lite for microcontrollers \"Hello World\" example on this fresh git pull. This resulted in the flatbuffers checksum error I mentioned before. This is when I try to run the following:\r\n\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=arc_emsdp OPTIMIZED_KERNEL_DIR=arc_mli ARC_TAGS=no_arc_mli generate_hello_world_make_project`\r\n\r\nThe results are below. Did you say I needed to post this as an issue in the main head?\r\n\r\n\r\n```\r\nFIND: Parameter format not correct\r\nFIND: Parameter format not correct\r\n--2021-02-28 17:27:56--  http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip\r\nResolving mirror.tensorflow.org (mirror.tensorflow.org)... 142.250.113.128, 2607:f8b0:4023:1006::80\r\nConnecting to mirror.tensorflow.org (mirror.tensorflow.org)|142.250.113.128|:80... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 1760478 (1.7M) [application/zip]\r\nSaving to: '/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip'\r\n\r\n/tmp/dca12522a9f9e37f126ab925 100%[=================================================>]   1.68M  9.18MB/s    in 0.2s\r\n\r\n2021-02-28 17:27:57 (9.18 MB/s) - '/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip' saved [1760478/1760478]\r\n\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 1: $'PK\\003\\004': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 2: nL8Q5: command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 3: nL8Q?: command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 4: $'\\bnL8Qw\\030B\\005\\352\\002\\304\\005V': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 4: $'\\367\\257XY': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'\\002T': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'v\\355\\236\\215\\265M4\\036w]\\027\\262\\036p\\250t5\\256\\367\\245f': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'\\225L\\347\\313l\\036\\020\\350\\341\\320\\265\\254\\3218\\002~\\264B\\323\\302\\371\\016XC\\2408\\313': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'@\\244\\315\\217\\v\\2105': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'\\301\\2173H2\\037.\\342,\\311N\\223\\233d}\\231^\\257\\341': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'^\\255\\342\\345:\\231g\\220\\256': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'m\\237\\220Q7\\037\\324\\242\\304R\\234R\\372T\\235\\317k\\347T\\347\\316\\334\\235\\353F\\352\\217g\\317q': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: 6\u25a1m@0\u25a1cs/d\u25a1\u25a1 \u25a1j\u263c\u25a14\u25a1E)8\u25a1&\u25a1\u25a1\u25a1z@-i#hPo\u25a1q\u25a1\u2192\u263bX\u25a16\u25a1\u25a1\r\n\u25a1l\u25a1\u25a1\u25bc{\u25a1\u25a1A\u2642y+\u25a1\u25a1\u25a1\u2192\u25a1\u25a1\u25a1Ff\u25a1\u203c%\u25a1\u25a13\u23022\u25a1\u25a1\u0707\u25a15\u25a1s\u25a1\u25a1D%\u2192k<\u25a1\u0202\u00a7J\u25a1\u25a1\u25a1\u2194W\u2663\u25a19\u25a1\u25ba\u25a10\u25a1ub\u2666OO\u25a1\u25a1\u25a1_\u25a1\u0255\u25a1@\u25a1\u25a1Y\u2192\u25a10\u25a1!\u25a1\u2193\u25a1m\u25a1\u25a1b\u25a1\u046ah\u25a1R\u25a1(K\u25a1H\u25a12@L\u25a190j\u25a1\u25a163J\u25a1^%,\u25a1G\u25a1\u25a1j-\\\u25a1\u25a1\u2193\u2666\u263aliW\u25a1\u25a1\u25a1A\u2640\u25a127/\u25a11\u25a1\u25a1\u25a1\u221fU\u25a1e\u263a'\u25a1j\u2640Yk\u00a7\u25a1u9\u25a1\u25a1\u25a1#\u25a13\u25a1(l\u0da4\u25a1d7\u25a1\u25a1\u25a1Z4K\u25ac\u25a1Ez\u25a1\u25a1y}\u25a1]O\u25a1\u25a1,\u25a1m\u2302'>!\u25a1(\u25a1}\u25a1\u25a1\u25a1\u25a1\u25ba\u25a1W\u25a1t\u00a7\u25a1\u25a1\u2302|^\u25a1\u25a1\u25a18\u25a1\u25a1Zu\u25a1\u2195\u25a1\u221fv{\u266b\u25a1\u2660\u25a1Jt+{\u07d0%^\u25a1\u25a1k<~.\u25a1\u0640\u25a1Z46\u2666\u25a1\u25a1v9\u25a1\u25a1   \u2195\u25a1\u25a1v\u25a1\u25a1Z\u25a1\u050a\u25a1\u074f\u0732{7\u25a1o\u023d\u2666\u25a1\u2302)\u2640\u203cH\\\u25a1/     \u25a1e\u25bc\u25a1C\u25a1w\u25a1\u25a1@\u25a1\u25a1\u25a1\u25a1\u25a1\u263c\u25a1\u25a1B2\u25a1#-{\u2665\u25a1\u25a10\u25a1\u25a1\u25a1\u25a1\u25a1l>9\ufffd\u25a1t)\u25a1\u25a1s\u2193\u2191_\u25a1\u25a1P2\u25a1     PK\u2665\u2666\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 11: $'OO\\337\\367\\312\\262\\204\\0365\\3341P]\\034\\327\\025ZL4\\237t\\364]r\\363\\024c\\020\\202\\216\\214?h\\210\\254N\\326\\b\\a\\275\\221z]P\\022\\301c\\177\\346\\2608\\274\\350\\234\\3230tl4y\\006\\244\\272h.y\\371\\214i\\201,': command not found\r\nnL8Q>   flatbuffers-dca12522a9f9e37f126ab925fd385c807ab4f84e/.bazelci/UT\u2663\u263a\u25a1\u25a1l_PK\u2665\u2666\r\nnL8Q=\u25a1!X\u25a1K      flatbuffers-dca12522a9f9e37f126ab925fd385c807ab4f84e/.bazelci/presubmit.ymlUT\u2663\u263a\u25a1\u25a1l_\u25a1\u25a1\u25a1\u25a1J*\u25a1\u25a1I\u25a1L\u25a1L-\u25a1R\u25a1I,I-.\u25a1*\u25a1i\u25a1E\u25a1\u25a1V\r\n\u25a1I\u25a1y%\u25a1\u25a1f\u2660& \u25a1\u25a1\u263bX}|IbQzjI1DHWAIOOO        \u25a1\u2660\u2193\u25a1K\u25acj\u25a1\u2663\u00a7\u25a1\u25a1ML\u25a1/\u25a1\u25a1\u00b6PK\u2665\u2666\r\nnL8Q\u25a14%_\u25a10\u263aB    flatbuffers-dca12522a9f9e37f126ab925fd385c807ab4f84e/.clang-formatUT\u2663\u263a\u25a1\u25a1l_m\u25a1Mn\u25a1@\u2640\u25a1\u25a1>\u2663\u21a8)[v\u25ba\u25a1\u25a1*RP\u25a1\u2663\\p\u2660+\u25a1y\u25a1n\u25a1Q\u2302\u25a1E\u25a1: No such file or directory\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 11: $'\\034Z\\303\\211\\346\\236\\223\\377\\b\\277\\211\\215l\\270\\247\\326\\b\\257ot[\\005\\235:\\232\\004\\235\\243\\246\\277\\231\\304m\\242\\3711\\277\\356\\361\\235$\\235\\264\\031Y\\363\\351\\2365\\363\\334\\356\\004\\343\\232-_\\346_\\332+\\355[\\264\\371\\021\\324J\\234\\256\\377B\\252\\252\\002\\370\\004PK\\003\\004': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 12: $'\\bnL8Q\\33088\\322\\236\\337B': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 13: $'\\302@\\020D\\373': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 16: $'\\021\\301\\336\\316\\322?\\020': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 16: r\u25a1\u25a1\u25a1\u25a1\u25b2T\u2191\u25a1bLl\u25a1XN\u25a1\u25a1\u25a1QtK*\u25c44: No such file or directory\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 16: $'\\202\\v%Q\\256\\253\\333\\341^\\211ulh': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 17: $'f\\345eey\\206\\310?S\\336\\253wZ\\270pF\\223\\304\\2026\\306\\3236\\375{\\351\\003PK\\003\\004': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 18: $'\\bnL8Q\\324\\240P3\\211\\375A': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: command substitution: line 19: syntax error near unexpected token `)'/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: command substitution: line 19: `\u263b1\u2640D\u25a1\u25a1\u25a1\u25a1\u25ac\u25ac\u25a1\u25a1\u25a1\u25a1\u2302\u25a1\u25b2d\u25a1mJ\u25a1\u25a1.\u25a1\u25a1[\u25a1^\u011d3o\u2195=\u263b&\u26669bxB \u25a10j\u25a1<\u25a1\u25a1\u25a1\u25a1k$\u25a1\u2666\u21a8x)(b\"1 \u25a1\u25a1P\u25a1x\u25a1\u25a1l'\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 16: $'\\305\\202\\205\\270\\312\\0261\\315\\274\\304RT\\373t\\363\\206\\036t\\371\\261\\204d\\340V\\327\\257v\\230\\036\\236\\332\\300U0\\330_\\246\\345\\f\\343L\\336\\027\\027mg\\252\\333\\256\\230\\235g\\376\\201I\\275\\317\\037PK\\003\\004': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 17: $'\\bnL8Q\\026\\345\\365\\275OUC': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 18: $'\\200': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 18: $'\\365\\370\\243\\371PK\\003\\004': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 19: flatbuffers-dca12522a9f9e37f126ab925fd385c807ab4f84e/.github/UT\u2663\u263a\u25a1\u25a1l_PK\u2665\u2666: No such file or directory\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 20: syntax error near unexpected token `('\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 20: nL8Q}\u25a1\u25a1\u25a1B\u263bN flatbuffers-dca12522a9f9e37f126ab925fd385c807ab4f84e/.github/ISSUE_TEMPLATE.mdUT\u2663\u263a\u25a1\u25a1l_U\u25a1AO\u25a10\u2640\u25a1\u25a1\u25a1\u00a7o\u25a1\u25a1(\u25a1\u25a1v@\u221fABp\u263b  \u266bH\u25a1\u266b^\u25a1\u25a1\u25a1\u04a4\u25a1]`\u25a1\u25b2\u25a1C\u25a1I\u25a1!\u25a1\u25a1\u25a1=\u25a1\u25a1\u25a1\u25a1\u25a1!\u25a1hr\u25a1\u25a1\u25a1>\u25a1\u25a1\u0502\u2195\u25a1\u25a1\u25a1\u2642\u25a1^#\u25a10z\u06b3U\u00b6\u25a1\u25a1C\u25a1q\u25a1\u2193\u25a11\u2195\u25a1,\u25a1\u25a1|\u25a1\u25a1a\u25a1\\#RjGj\u25a1B.+\u25a1\u25a1\u263c!r\u25a1'\u21a8     9\u034fy\u25a1B\u25a1\u25a1\u221fD\u25a1?KR\u25a1\u25a1\u2191I\u25a1GkX\u25a1$g\u25a1\u203cB9\u263bB\u25a1F^9\u25a1\u25a1\u2663A\u25a1q\u25ba\u25a1\u25a1\u25a1u\u25a1\u25a1\u25a1\u00b6\u25a11\u25a1\u25a1\u25a1bj\u25a1\u21931\u25a1=HO\u25a1\u25a1\u25a1\u25a1\u25a1\u25a1,\u25a1\u25a1\u25a1!\u25b2-\u25a19\u25a1G\u25a1s\u25a1\u2192\u263c\u25a1\u25a1\u25a1W\u25a1      \u25a1=\u25a1L\u25a1\u25a1\u25a1\u25a1<\\]Uh\u25a1\u25a1zu[\u25a1\u25a1\u25bc\u25a1\u25a1\u25a1\u25a1\u25a1\u25a1\u25a1S\u25a17\u25a1dvT@\u2191J\u05bceY\u25a1      \u221f\u25a1      q)Z\u25a1\u25a1r\u25a1\u25a1\u25a1\u2666\u25a1;\u25a1\u25a1\uda07\udc61 \u2191\u25a1\u2660,\u25a1\u0660nV\u25a1\u25a1s\u03ff\u25a1\u25a16\u25a1\u25a1\u25a1\u25a1{4=;\u25a1I\u266b%\u25a1\u25b2\u25a1f\u25a1\u25a1\u25a1\u25a1\u263aPK\u2665\u2666'\r\ntensorflow/lite/micro/tools/make/Makefile:525: *** Something went wrong with the flatbuffers download: Bad checksum. Expected: aa9adc93eb9b33fa1a2a90969e48baee, Got: .  Stop.\r\n```", "I managed to resolve the random uniform related error in https://github.com/tensorflow/tensorflow/commit/358948ec8da338b694416b4cd83d5ed3648933b0. Could you try again?\r\n\r\nFor the flatbuffer checksum issue, can you create a new issue in the github to track separately?", "Will do, will notify you shortly to confirm the fix. Added Issue [47467](https://github.com/tensorflow/tensorflow/issues/47467) for the flatbuffers issue.", "Steps:\r\n\r\n- Open command prompt\r\n- Navigate to CMake installation directory\r\n- Run git clone https://github.com/tensorflow/tensorflow.git tensorflow_src\r\n- Run \"mkdir tflite_build\" to create tflite_build folder\r\n- Run \"cd tflite_build\" to go into that folder\r\n- Run \"cmake ../tensorflow_src/tensorflow/lite\". Appears successful, however potential issues listed under \"Build Config Results\" below.\r\n- Run \"cmake --build . -j\" to build Tensorflow Lite. This appears to build correctly, I see tensorflow-lite.lib/pdb under the tflite_build/Debug folder. Lots of warnings produced, listed under \"Library Build Warnings\" below.\r\n\r\nIf the issues/warnings below are not a concern, I'll be working to pull this TensorflowLite source code into Visual Studio and attempt to perform inference on my model and see if I get valid pointers from the inference object.\r\n\r\n\r\n**Build Config Results:**\r\n```\r\n-- Performing Test EIGEN_COMPILER_SUPPORT_CPP11\r\n-- Performing Test EIGEN_COMPILER_SUPPORT_CPP11 - Failed\r\n-- Found Threads: TRUE\r\n-- Performing Test EIGEN_COMPILER_SUPPORT_CPP11\r\n-- Performing Test EIGEN_COMPILER_SUPPORT_CPP11 - Failed\r\n-- Performing Test COMPILER_SUPPORT_std=cpp03\r\n-- Performing Test COMPILER_SUPPORT_std=cpp03 - Failed\r\n```\r\n\r\n```\r\n-- Performing Test FARMHASH_HAS_BUILTIN_EXPECT\r\n-- Performing Test FARMHASH_HAS_BUILTIN_EXPECT - Failed\r\n```\r\n\r\n\r\n**Library Build Warnings:**\r\n\r\n```\r\n\r\n  c_api.cc\r\nC:\\Users\\jtork\\CMake\\tensorflow_src\\tensorflow\\lite\\c\\c_api.cc(88,29): warning C4273: 'TfLiteVersion': inconsistent dll linkage [C:\\Users\\jtork\\CMake\\tflite_build\\ten\r\nsorflow-lite.vcxproj]\r\nC:\\Users\\jtork\\CMake\\tensorflow_src\\tensorflow/lite/c/c_api.h(96,36): message : see previous definition of 'TfLiteVersion' [C:\\Users\\jtork\\CMake\\tflite_build\\tensorfl\r\now-lite.vcxproj]\r\nC:\\Users\\jtork\\CMake\\tensorflow_src\\tensorflow\\lite\\c\\c_api.cc(90,75): warning C4273: 'TfLiteModelCreate': inconsistent dll linkage [C:\\Users\\jtork\\CMake\\tflite_build\r\n\\tensorflow-lite.vcxproj]\r\nC:\\Users\\jtork\\CMake\\tensorflow_src\\tensorflow/lite/c/c_api.h(99,37): message : see previous definition of 'TfLiteModelCreate' [C:\\Users\\jtork\\CMake\\tflite_build\\tens\r\norflow-lite.vcxproj]\r\nC:\\Users\\jtork\\CMake\\tensorflow_src\\tensorflow\\lite\\c\\c_api.cc(97,64): warning C4273: 'TfLiteModelCreateFromFile': inconsistent dll linkage [C:\\Users\\jtork\\CMake\\tfli\r\nte_build\\tensorflow-lite.vcxproj]\r\nC:\\Users\\jtork\\CMake\\tensorflow_src\\tensorflow/lite/c/c_api.h(103,37): message : see previous definition of 'TfLiteModelCreateFromFile' [C:\\Users\\jtork\\CMake\\tflite_b\r\nuild\\tensorflow-lite.vcxproj]\r\n\r\n[Cutting out a lot of these messages]\r\n\r\nC:\\Users\\jtork\\CMake\\tflite_build\\eigen\\unsupported\\Eigen\\CXX11\\src/ThreadPool/ThreadLocal.h(277): message : see reference to class template instantiation 'EigenForTF\r\nLite::TensorEvaluator<ArgType,Device>::EvalParallelContext<DoneCallback,false,false,false,0>::ThreadLocalBlocksInitialize<float *,true>' being compiled [C:\\Users\\jtor\r\nk\\CMake\\tflite_build\\tensorflow-lite.vcxproj]\r\n          with\r\n          [\r\n              ArgType=const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<EigenForTFLite::DenseIndex>,1>,const EigenForTFLite::Tensor\r\n  ReshapingOp<const EigenForTFLite::DSizes<__int64,2>,const EigenForTFLite::TensorImagePatchOp<-1,-1,const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const floa\r\n  t,4,1,EigenForTFLite::DenseIndex>,16,EigenForTFLite::MakePointer>>>,const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<__int64,2>,const tflite::mu\r\n  ltithreaded_ops::ConstEigenTensor>,const EigenForTFLite::NoOpOutputKernel>,\r\n              Device=EigenForTFLite::ThreadPoolDevice,\r\n              DoneCallback=EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<EigenForTFLite::DenseI\r\n  ndex>,1>,const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<__int64,2>,const EigenForTFLite::TensorImagePatchOp<-1,-1,const EigenForTFLite::Tensor\r\n  Map<EigenForTFLite::Tensor<const float,4,1,EigenForTFLite::DenseIndex>,16,EigenForTFLite::MakePointer>>>,const EigenForTFLite::TensorReshapingOp<const EigenForTFLit\r\n  e::DSizes<__int64,2>,const tflite::multithreaded_ops::ConstEigenTensor>,const EigenForTFLite::NoOpOutputKernel>,EigenForTFLite::ThreadPoolDevice>::NoCallback\r\n          ]\r\nC:\\Users\\jtork\\CMake\\tflite_build\\eigen\\unsupported/Eigen/CXX11/src/Tensor/TensorContractionThreadPool.h(803): message : see reference to class template instantiation\r\n 'EigenForTFLite::ThreadLocal<EigenForTFLite::TensorEvaluator<ArgType,Device>::EvalParallelContext<DoneCallback,false,false,false,0>::ThreadLocalBlocks<float *>,Eigen\r\nForTFLite::TensorEvaluator<ArgType,Device>::EvalParallelContext<DoneCallback,false,false,false,0>::ThreadLocalBlocksInitialize<float *,true>,EigenForTFLite::TensorEva\r\nluator<ArgType,Device>::EvalParallelContext<DoneCallback,false,false,false,0>::ThreadLocalBlocksRelease<float *>>' being compiled [C:\\Users\\jtork\\CMake\\tflite_build\\t\r\nensorflow-lite.vcxproj]\r\n          with\r\n          [\r\n              ArgType=const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<EigenForTFLite::DenseIndex>,1>,const EigenForTFLite::Tensor\r\n  ReshapingOp<const EigenForTFLite::DSizes<__int64,2>,const EigenForTFLite::TensorImagePatchOp<-1,-1,const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const floa\r\n  t,4,1,EigenForTFLite::DenseIndex>,16,EigenForTFLite::MakePointer>>>,const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<__int64,2>,const tflite::mu\r\n  ltithreaded_ops::ConstEigenTensor>,const EigenForTFLite::NoOpOutputKernel>,\r\n              Device=EigenForTFLite::ThreadPoolDevice,\r\n              DoneCallback=EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<EigenForTFLite::DenseI\r\n  ndex>,1>,const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<__int64,2>,const EigenForTFLite::TensorImagePatchOp<-1,-1,const EigenForTFLite::Tensor\r\n  Map<EigenForTFLite::Tensor<const float,4,1,EigenForTFLite::DenseIndex>,16,EigenForTFLite::MakePointer>>>,const EigenForTFLite::TensorReshapingOp<const EigenForTFLit\r\n  e::DSizes<__int64,2>,const tflite::multithreaded_ops::ConstEigenTensor>,const EigenForTFLite::NoOpOutputKernel>,EigenForTFLite::ThreadPoolDevice>::NoCallback\r\n          ]\r\n\t\t  \r\n\t\t  \r\n  minimal_logging_default.cc\r\nC:\\Users\\jtork\\CMake\\tensorflow_src\\tensorflow\\lite\\minimal_logging_default.cc(28,9): warning C4068: unknown pragma 'clang' [C:\\Users\\jtork\\CMake\\tflite_build\\tensorf\r\nlow-lite.vcxproj]\r\nC:\\Users\\jtork\\CMake\\tensorflow_src\\tensorflow\\lite\\minimal_logging_default.cc(29,9): warning C4068: unknown pragma 'clang' [C:\\Users\\jtork\\CMake\\tflite_build\\tensorf\r\nlow-lite.vcxproj]\r\nC:\\Users\\jtork\\CMake\\tensorflow_src\\tensorflow\\lite\\minimal_logging_default.cc(31,9): warning C4068: unknown pragma 'clang' [C:\\Users\\jtork\\CMake\\tflite_build\\tensorf\r\nlow-lite.vcxproj]\r\n\r\n\r\n```", "The build config test is used for finding out the correct set of the options for the given compilers. They are just indicators of the MSVC's characteristics.\r\n\r\nFor the compiler warnings, TFLite has a number of other libraries including eigen and so on. It is common to see warnings during compliations.", "I am working on trying to manually pull in the appropriate Tensorflow Lite code into a Visual C project from the CMake build as a reference. I am floundering a bit, as there is just so much code. A working make build of one of the Tensorflow Lite for Microcontrollers examples would really help clear the clutter (that's what I used in the past). I'm running Windows, so I'd have to get up to speed on Docker, I believe, to see a make build of one of the Tensorflow Lite for Microcontrollers examples build. Unless the make issues for Windows were resolved.\r\n\r\nMy options appear to be:\r\n\r\n1. Go through the tedious work of trying to manually pull the source code from the CMake build of Tensorflow Lite into Visual Studio and work through issues. No idea how long this would take.\r\n2. Do a make build of a Tensorflow Lite for Microcontrollers example in Docker, and use that as a reference for pulling in minimal source code into Visual Studio.\r\n3. Pray that the Windows make build for the Tensorflow Lite for Microcontrollers gets fixed, then use that as a reference for pulling in minimal source code into Visual Studio.\r\n\r\nOnce I get the latest and greatest pulled into VS, I can see if my null pointer issues are resolved and I can perform inference with my model.", "I am closing this issue, and have opened two more granularly defined issues that this issue has taken me to.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/47609\r\nhttps://github.com/tensorflow/tensorflow/issues/47610", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47452\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47452\">No</a>\n"]}, {"number": 47451, "title": "Tensorflow Addons connected components not working properly", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Cuda compilation tools, release 10.1, V10.1.243\r\n- GPU model and memory: GEForce GTX 1080 Ti, 11264 MB Dedicated Video Memory (I'm using CPU)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nTFA connected components should produce 2 components with the attached script. It is not.\r\n\r\n**Describe the expected behavior**\r\nTFA result should show same number of components as Scipy.ndimage.measurements.label, as claimed in the documentation\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n\"\"\"\r\n    TFA Addons Connected Components check\r\n\r\n    Using:\r\n    TensorFlow 2.4.0\r\n    Python 3.8.5\r\n    TFA 0.12.1\r\n    Scipy 1.6.1\r\n\"\"\"\r\n\r\nimport tensorflow\r\nimport numpy as np\r\nimport tensorflow_addons as tfa\r\nimport matplotlib.pyplot as plt\r\nimport scipy.ndimage.measurements as meas\r\n\r\n\r\nwith tensorflow.device('/CPU:0'):\r\n\r\n    M = 500\r\n    img = np.zeros((M, M))\r\n\r\n    # One square\r\n    top = 50\r\n    bottom = 270\r\n    left = 300\r\n    right = 450\r\n    img[np.ix_(np.arange(top, bottom + 1), np.arange(left, right + 1))] = 1\r\n\r\n    # Second one\r\n    top = 100\r\n    bottom = 150\r\n    left = 200\r\n    right = 250\r\n    img[np.ix_(np.arange(top, bottom + 1), np.arange(left, right + 1))] = 1\r\n\r\n    # Convert image to tensor\r\n    img = tensorflow.convert_to_tensor(img)\r\n    imgtf = tensorflow.expand_dims(img, -1)\r\n\r\n    # Connected components with TFA\r\n    d_img_tfa = tfa.image.connected_components(imgtf).numpy()\r\n\r\n    # Plot it\r\n    plt.figure()\r\n    plt.title('TFA Connected Components Image')\r\n    _ = plt.imshow(d_img_tfa)\r\n    plt.show(block=False)\r\n\r\n    # Connected components with scipy\r\n    d_img_scipy, _ = meas.label(imgtf)\r\n\r\n    # Count number of non-background components (subtract 1 for background)\r\n    num_comp_tfa = np.unique(d_img_tfa.flatten()).size - 1\r\n    num_comp_scipy = np.unique(d_img_scipy.flatten()).size - 1\r\n\r\n    # These images should be the same\r\n    print(\"\\nTFA connected components has %d components.\" % num_comp_tfa)\r\n    print(\"Scipy connected components has %d components.\\n\" % num_comp_scipy)\r\n\r\n    # Plot\r\n    plt.figure()\r\n    plt.title('Scipy Connected Components Image')\r\n    _ = plt.imshow(d_img_scipy)\r\n    plt.show(block=True)\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@jeball \r\nI ran the code shared and face a different error, please refer to this [gist here](https://colab.sandbox.google.com/gist/Saduf2019/d5326e86b8bd3905ab35163d41e20189/untitled554.ipynb).", "Remove the plot commands \u2013 the print statements will show different numbers. Alternately just squeeze the last dimension out.\r\n\r\nJohn Ball, Ph.D.\r\nAssociate Professor and Robert Guyton Endowed Chair for Teaching Excellence | Electrical and Computer Engineering\r\n\r\n[cid:image002.png@01D70E6E.5D537160]\r\nAssociate Editor | IEEE Signal Processing Letters\r\nAssociate Editor | Journal of Applied Remote Sensing\r\nEditorial Board Member | MDPI Electronics\r\nFaculty Advisory Council | Transactions on Stem Education\r\nLab Director | Simrall Radar Laboratory\r\nLab Co-Director | Center for Advanced Vehicular Systems (CAVS) Sensor Lab\r\nResearch Fellow | Geosystems Research Institute (GRI)\r\nSimrall 233, 406 Hardy Rd., Mississippi State, MS 39762\r\np:662.325.4169  e:jeball@ece.msstate.edu<mailto:jeball@ece.msstate.edu>\r\n\r\n\r\nFrom: Saduf2019 <notifications@github.com>\r\nSent: Monday, March 1, 2021 3:28 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Ball, John <jeball@ece.msstate.edu>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] Tensorflow Addons connected components not working properly (#47451)\r\n\r\n\r\n@jeball<https://secure-web.cisco.com/1vUn2dmy25OnfaqG2wLu54VMP7WdGqLYRUOy2zg4ZhpoAKjb0vS5GtAVO45oQeztm97ds6K1W75j0trRQU5EfdwtRbSN_AHyKKJphuR79fwHToQ4eIOFHB8AQ3NYJZ4bFf6u_5siUoy9sUWrPhUKTkGnS7DYcH53oogiphYX7mpY8dc6MB2MYMHnekokv5Yd9JZuliOMFa2LqYj_u7uzqobnvrKSGSyG90MLS7RD4zYMc1TQ0vgwG1pnn7RD3FZViAM4TJLbmlytvVqW6UPVDChg0kNyJ0B--bx98EUnvzB36zj_e9X9EX2kJ0eZq-c5h/https%3A%2F%2Fgithub.com%2Fjeball>\r\nI ran the code shared and face a different error, please refer to this gist here<https://secure-web.cisco.com/1WIBilSUPM0uE_IQSF69SXlOgInrP054t_ngzJmOVNZQs956TtyBNDKAut26CoSSZIlLhWOixerkjHns14Klp1sCppCHLADR971lIR8tw-7gtDjbdqyGEMk3qOQdbjZ3eQRdg4VkvtfDKYWi8srpFC6gZaKsYf4COUu23rvxrXcPVB35nWOQqV-vpU9Fe0G_5b9C1MU4X6Gt_ZcnqONJQ4v5EdwnNe_CSruqoFLFq1C03AO2kv0sMQ5ljLP6KOoqWvAiuPwByV4b34ZSIoePhuWaACeYz_QJUgQJEdpOezOW5nBAVXyIPq7494Ps9JgI7/https%3A%2F%2Fcolab.sandbox.google.com%2Fgist%2FSaduf2019%2Fd5326e86b8bd3905ab35163d41e20189%2Funtitled554.ipynb>.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://secure-web.cisco.com/1lTlooslH9664ltjbgIYBTXVbY_0VfyLy6vPHaBTSvDEIoQy8aSOkA2u4jQscqtoi0_9srrHXI5lpZCksyyhcdHoZRRRZD3NavafnQd33mmFMo8SwiJEFJo3Qm5BVdvrYmdEz8d3q8l4vBQEI90mvPm2BJhrNi1yQxTzHGFc9ueadCEwTlcc3pn4odjnTDERkvvGGLSOtY0BOeLuVu5lko5MeHJOAXYyieWSQZ5kDR6UC_38D0NLrA04hWrFFqulDHEaItZikaoQy9VIdq-6_w_mV149bV2zoB4xhsQZL_SijMT5rcP69iAtVbVQPhwug/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F47451%23issuecomment-787800601>, or unsubscribe<https://secure-web.cisco.com/1jcyBVTkNV7Sc5UH4REP6DAEFEF85eSEL6DOiFvL3-2wUf0n8CCVeP_0qcVuFf_kM3CXOnuzu5_uT2LgekcCYSU_86rrOP_J26F9_xsCA1VjCv3YpFCL__MEwtr0fTg9FfZC1ypUeM9USIDHDkVQtb5_xyUCaS64ezi3dT-kER-2AK3i_TEmdnKCwJyo3frs7G_TdGmLj0rvtkAMDZmuRXfeOfsZ80C0udRE43VQbesIFxudDUnNhqAYxzpLZB93wSdOVQlbdDKrdEFOFG_50iENPFcb25VmReG-7WIV99tHTYPhn9iUMckfcw1M3V-NG/https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAETBXOA3BD22X3J5TRYJPXLTBNMYTANCNFSM4YKB24YA>.\r\n", "I have made the changes in the notebook. Code runs now.\r\n\r\nJohn Ball, Ph.D.\r\nAssociate Professor and Robert Guyton Endowed Chair for Teaching Excellence | Electrical and Computer Engineering\r\n\r\n[cid:image002.png@01D70E6F.8BE86160]\r\nAssociate Editor | IEEE Signal Processing Letters\r\nAssociate Editor | Journal of Applied Remote Sensing\r\nEditorial Board Member | MDPI Electronics\r\nFaculty Advisory Council | Transactions on Stem Education\r\nLab Director | Simrall Radar Laboratory\r\nLab Co-Director | Center for Advanced Vehicular Systems (CAVS) Sensor Lab\r\nResearch Fellow | Geosystems Research Institute (GRI)\r\nSimrall 233, 406 Hardy Rd., Mississippi State, MS 39762\r\np:662.325.4169  e:jeball@ece.msstate.edu<mailto:jeball@ece.msstate.edu>\r\n\r\n\r\nFrom: Saduf2019 <notifications@github.com>\r\nSent: Monday, March 1, 2021 3:28 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Ball, John <jeball@ece.msstate.edu>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] Tensorflow Addons connected components not working properly (#47451)\r\n\r\n\r\n@jeball<https://secure-web.cisco.com/1vUn2dmy25OnfaqG2wLu54VMP7WdGqLYRUOy2zg4ZhpoAKjb0vS5GtAVO45oQeztm97ds6K1W75j0trRQU5EfdwtRbSN_AHyKKJphuR79fwHToQ4eIOFHB8AQ3NYJZ4bFf6u_5siUoy9sUWrPhUKTkGnS7DYcH53oogiphYX7mpY8dc6MB2MYMHnekokv5Yd9JZuliOMFa2LqYj_u7uzqobnvrKSGSyG90MLS7RD4zYMc1TQ0vgwG1pnn7RD3FZViAM4TJLbmlytvVqW6UPVDChg0kNyJ0B--bx98EUnvzB36zj_e9X9EX2kJ0eZq-c5h/https%3A%2F%2Fgithub.com%2Fjeball>\r\nI ran the code shared and face a different error, please refer to this gist here<https://secure-web.cisco.com/1WIBilSUPM0uE_IQSF69SXlOgInrP054t_ngzJmOVNZQs956TtyBNDKAut26CoSSZIlLhWOixerkjHns14Klp1sCppCHLADR971lIR8tw-7gtDjbdqyGEMk3qOQdbjZ3eQRdg4VkvtfDKYWi8srpFC6gZaKsYf4COUu23rvxrXcPVB35nWOQqV-vpU9Fe0G_5b9C1MU4X6Gt_ZcnqONJQ4v5EdwnNe_CSruqoFLFq1C03AO2kv0sMQ5ljLP6KOoqWvAiuPwByV4b34ZSIoePhuWaACeYz_QJUgQJEdpOezOW5nBAVXyIPq7494Ps9JgI7/https%3A%2F%2Fcolab.sandbox.google.com%2Fgist%2FSaduf2019%2Fd5326e86b8bd3905ab35163d41e20189%2Funtitled554.ipynb>.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://secure-web.cisco.com/1lTlooslH9664ltjbgIYBTXVbY_0VfyLy6vPHaBTSvDEIoQy8aSOkA2u4jQscqtoi0_9srrHXI5lpZCksyyhcdHoZRRRZD3NavafnQd33mmFMo8SwiJEFJo3Qm5BVdvrYmdEz8d3q8l4vBQEI90mvPm2BJhrNi1yQxTzHGFc9ueadCEwTlcc3pn4odjnTDERkvvGGLSOtY0BOeLuVu5lko5MeHJOAXYyieWSQZ5kDR6UC_38D0NLrA04hWrFFqulDHEaItZikaoQy9VIdq-6_w_mV149bV2zoB4xhsQZL_SijMT5rcP69iAtVbVQPhwug/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F47451%23issuecomment-787800601>, or unsubscribe<https://secure-web.cisco.com/1jcyBVTkNV7Sc5UH4REP6DAEFEF85eSEL6DOiFvL3-2wUf0n8CCVeP_0qcVuFf_kM3CXOnuzu5_uT2LgekcCYSU_86rrOP_J26F9_xsCA1VjCv3YpFCL__MEwtr0fTg9FfZC1ypUeM9USIDHDkVQtb5_xyUCaS64ezi3dT-kER-2AK3i_TEmdnKCwJyo3frs7G_TdGmLj0rvtkAMDZmuRXfeOfsZ80C0udRE43VQbesIFxudDUnNhqAYxzpLZB93wSdOVQlbdDKrdEFOFG_50iENPFcb25VmReG-7WIV99tHTYPhn9iUMckfcw1M3V-NG/https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAETBXOA3BD22X3J5TRYJPXLTBNMYTANCNFSM4YKB24YA>.\r\n", "Not sure changes get saved?\r\n\r\nAdd these lines in\r\n\r\nd_img_tfa = np.squeeze(d_img_tfa)\r\n\r\nand\r\n\r\nd_img_scipy = np.squeeze(d_img_scipy)\r\n\r\njust before the plots.\r\n\r\nJohn Ball, Ph.D.\r\nAssociate Professor and Robert Guyton Endowed Chair for Teaching Excellence | Electrical and Computer Engineering\r\n\r\n[cid:image002.png@01D70E6F.C6A1A500]\r\nAssociate Editor | IEEE Signal Processing Letters\r\nAssociate Editor | Journal of Applied Remote Sensing\r\nEditorial Board Member | MDPI Electronics\r\nFaculty Advisory Council | Transactions on Stem Education\r\nLab Director | Simrall Radar Laboratory\r\nLab Co-Director | Center for Advanced Vehicular Systems (CAVS) Sensor Lab\r\nResearch Fellow | Geosystems Research Institute (GRI)\r\nSimrall 233, 406 Hardy Rd., Mississippi State, MS 39762\r\np:662.325.4169  e:jeball@ece.msstate.edu<mailto:jeball@ece.msstate.edu>\r\n\r\n\r\nFrom: Saduf2019 <notifications@github.com>\r\nSent: Monday, March 1, 2021 3:28 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Ball, John <jeball@ece.msstate.edu>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] Tensorflow Addons connected components not working properly (#47451)\r\n\r\n\r\n@jeball<https://secure-web.cisco.com/1vUn2dmy25OnfaqG2wLu54VMP7WdGqLYRUOy2zg4ZhpoAKjb0vS5GtAVO45oQeztm97ds6K1W75j0trRQU5EfdwtRbSN_AHyKKJphuR79fwHToQ4eIOFHB8AQ3NYJZ4bFf6u_5siUoy9sUWrPhUKTkGnS7DYcH53oogiphYX7mpY8dc6MB2MYMHnekokv5Yd9JZuliOMFa2LqYj_u7uzqobnvrKSGSyG90MLS7RD4zYMc1TQ0vgwG1pnn7RD3FZViAM4TJLbmlytvVqW6UPVDChg0kNyJ0B--bx98EUnvzB36zj_e9X9EX2kJ0eZq-c5h/https%3A%2F%2Fgithub.com%2Fjeball>\r\nI ran the code shared and face a different error, please refer to this gist here<https://secure-web.cisco.com/1WIBilSUPM0uE_IQSF69SXlOgInrP054t_ngzJmOVNZQs956TtyBNDKAut26CoSSZIlLhWOixerkjHns14Klp1sCppCHLADR971lIR8tw-7gtDjbdqyGEMk3qOQdbjZ3eQRdg4VkvtfDKYWi8srpFC6gZaKsYf4COUu23rvxrXcPVB35nWOQqV-vpU9Fe0G_5b9C1MU4X6Gt_ZcnqONJQ4v5EdwnNe_CSruqoFLFq1C03AO2kv0sMQ5ljLP6KOoqWvAiuPwByV4b34ZSIoePhuWaACeYz_QJUgQJEdpOezOW5nBAVXyIPq7494Ps9JgI7/https%3A%2F%2Fcolab.sandbox.google.com%2Fgist%2FSaduf2019%2Fd5326e86b8bd3905ab35163d41e20189%2Funtitled554.ipynb>.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://secure-web.cisco.com/1lTlooslH9664ltjbgIYBTXVbY_0VfyLy6vPHaBTSvDEIoQy8aSOkA2u4jQscqtoi0_9srrHXI5lpZCksyyhcdHoZRRRZD3NavafnQd33mmFMo8SwiJEFJo3Qm5BVdvrYmdEz8d3q8l4vBQEI90mvPm2BJhrNi1yQxTzHGFc9ueadCEwTlcc3pn4odjnTDERkvvGGLSOtY0BOeLuVu5lko5MeHJOAXYyieWSQZ5kDR6UC_38D0NLrA04hWrFFqulDHEaItZikaoQy9VIdq-6_w_mV149bV2zoB4xhsQZL_SijMT5rcP69iAtVbVQPhwug/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F47451%23issuecomment-787800601>, or unsubscribe<https://secure-web.cisco.com/1jcyBVTkNV7Sc5UH4REP6DAEFEF85eSEL6DOiFvL3-2wUf0n8CCVeP_0qcVuFf_kM3CXOnuzu5_uT2LgekcCYSU_86rrOP_J26F9_xsCA1VjCv3YpFCL__MEwtr0fTg9FfZC1ypUeM9USIDHDkVQtb5_xyUCaS64ezi3dT-kER-2AK3i_TEmdnKCwJyo3frs7G_TdGmLj0rvtkAMDZmuRXfeOfsZ80C0udRE43VQbesIFxudDUnNhqAYxzpLZB93wSdOVQlbdDKrdEFOFG_50iENPFcb25VmReG-7WIV99tHTYPhn9iUMckfcw1M3V-NG/https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAETBXOA3BD22X3J5TRYJPXLTBNMYTANCNFSM4YKB24YA>.\r\n", "@jeball \r\nI have added the two lines as suggested but face the same [error](https://colab.research.google.com/gist/Saduf2019/f038f646736a2ddf5bba2d995f97e193/untitled554.ipynb), please share a colab gist with the error reported.", "Hello support,\r\n\r\nThe lines were not there?\r\n\r\nPlease install TF 2.4, not a nightly build. TFA does not seem compatible to this version.\r\nPlease use the commands below.\r\n\r\npip install tensorflow==2.4\r\npip install tensorflow-addons[tensorflow]\r\n\r\nThe script is shown below.\r\n\r\n\r\n\"\"\r\n    TFA Addons Connected Components not working properly\r\n\r\n    Using:\r\n    TensorFlow 2.4.0\r\n    Python 3.8.5\r\n    TFA 0.12.1\r\n    Scipy 1.6.1\r\n\r\n    To install:\r\n    pip install tensorflow==2.4\r\n    pip install tensorflow-addons[tensorflow]\r\n\r\n\"\"\"\r\n\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow_addons as tfa\r\nimport matplotlib.pyplot as plt\r\nimport scipy.ndimage.measurements as meas\r\n\r\n\r\nwith tf.device('/CPU:0'):\r\n\r\n    M = 500\r\n    img = np.zeros((M, M))\r\n\r\n    # One square\r\n    top = 50\r\n    bottom = 270\r\n    left = 300\r\n    right = 450\r\n    img[np.ix_(np.arange(top, bottom + 1), np.arange(left, right + 1))] = 1\r\n\r\n    # Second one\r\n    top = 100\r\n    bottom = 150\r\n    left = 200\r\n    right = 250\r\n    img[np.ix_(np.arange(top, bottom + 1), np.arange(left, right + 1))] = 1\r\n\r\n    # Convert image to tensor\r\n    img = tf.convert_to_tensor(img)\r\n    imgtf = tf.expand_dims(img, -1)\r\n\r\n    # Connected components with TFA\r\n    d_img_tfa = tfa.image.connected_components(imgtf).numpy()\r\n\r\n    # Remove last dimension and convert to numpy matrix\r\n    d_img_tfa = tf.squeeze(d_img_tfa, axis=-1).numpy()\r\n\r\n    # Plot it\r\n    plt.figure()\r\n    plt.title('TFA Connected Components Image')\r\n    _ = plt.imshow(d_img_tfa)\r\n    plt.show(block=False)\r\n\r\n    # Connected components with scipy\r\n    d_img_scipy, _ = meas.label(imgtf)\r\n    d_img_scipy = tf.squeeze(d_img_scipy, axis=-1).numpy()\r\n\r\n    # Count number of non-background components (subtract 1 for background)\r\n    num_comp_tfa = np.unique(d_img_tfa.flatten()).size - 1\r\n    num_comp_scipy = np.unique(d_img_scipy.flatten()).size \u2013 1\r\n\r\n\r\n\r\nJohn Ball, Ph.D.\r\nAssociate Professor and Robert Guyton Endowed Chair for Teaching Excellence | Electrical and Computer Engineering\r\n\r\n[cid:image003.png@01D70F38.96F12410]\r\nAssociate Editor | IEEE Signal Processing Letters\r\nAssociate Editor | Journal of Applied Remote Sensing\r\nEditorial Board Member | MDPI Electronics\r\nFaculty Advisory Council | Transactions on Stem Education\r\nLab Director | Simrall Radar Laboratory\r\nLab Co-Director | Center for Advanced Vehicular Systems (CAVS) Sensor Lab\r\nResearch Fellow | Geosystems Research Institute (GRI)\r\nSimrall 233, 406 Hardy Rd., Mississippi State, MS 39762\r\np:662.325.4169  e:jeball@ece.msstate.edu<mailto:jeball@ece.msstate.edu>\r\n\r\n\r\nFrom: Saduf2019 <notifications@github.com>\r\nSent: Tuesday, March 2, 2021 4:34 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Ball, John <jeball@ece.msstate.edu>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] Tensorflow Addons connected components not working properly (#47451)\r\n\r\n\r\n@jeball<https://secure-web.cisco.com/1e-c4_AW70HcU3XZ1qPKrfcApKk5l3DY5GESvMf-MzmQmpJS8_cSwvtwRU59PHVLnb6uhq2MwvHauNM4ilvRyuFavs0q3tzfTmtWC-8Ou4vxz27a6KXzkTb_lDJroD4lc9B22bp2fKnMxNULDoiJ0fM25okfkrSCSGG9FV5JuxVV1md-S1A0dV4nUhUtbNu7q8YNVhR7Qr7hWSl6ql1yZz4cKJ20yq9i8YKyxOd7f4wSplzl6F7PV9fYUKE7SzV1ahBZrR33oRWCxpbYm8LmOob7p_5CWEu2xulMfWCgAzWWUx45PCU3SAEk2mL4p7AKg/https%3A%2F%2Fgithub.com%2Fjeball>\r\nI have added the two lines as suggested but face the same error<https://secure-web.cisco.com/1T1z2rXTkhSV-6Jayk_w2kV4RpzOzkw1swo_14BX4AacG5V47REIz0Etn3G3MYipCLFp7lvAb_LAQmyeDcF3meMcjp64NcURzgenY4mIYAzGYVWckHkYsTLkJ45mS4Jtr27DNcOLxWVb4Jhc6izZchBP87A2xQD0eJFibMidkeCBwDSMpgAkJBj_t4qbI3MvxY2ewmw3kcV36SwwPICLUR225DydIkUcnVWRaxOb0q2jLwAOe-XygwugVH-hBRI5-sKR6IJpCjr9KlmLwLCUb_YXuqAOquh9Vk_7V7fT0Wf_Mn3ziBpT-DoBRltsHpsTt/https%3A%2F%2Fcolab.research.google.com%2Fgist%2FSaduf2019%2Ff038f646736a2ddf5bba2d995f97e193%2Funtitled554.ipynb>, please share a colab gist with the error reported.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://secure-web.cisco.com/1Z21Uh67rr1NJRST5zFxt2aUBigCV_t4lyba7Pncbl-Vnjkt-xbJgJyz63qAWR0ganv2WEdtLkdSRhGFSvDPLQ4vG6DU6fML7XC-4QECfR4FE1GsMFbHvjP977coXt5P-3Hdi4RtL0R4UJy7QWe_4xYUH1fnKCmMSsmJadeuAF7hU60WPzGCvja02JrmrgBApfjcWTH0nU0zO_YVW3GNbKt0qW4r9oD8x0pwkvdJP1f6jcMknhRClCnUg1uBK1Q8fZ5Wh7FLcdU42dZjR5nZYExu3G27IPd8DO4QsxkFeB4taOOjLKyp7hQG0uJoDAW7X/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F47451%23issuecomment-788806159>, or unsubscribe<https://secure-web.cisco.com/15Deiw8azEkVJaSB9nD9qQVew09cEtuQZTPdG1QHUlVJVaPBu1E79ExbOv8ikwRX98pX8QNw3Z-NhZpfcRi6O5bbKy1OZ8rdDMo5Ljzhi1ZnLiMN-kizETvyrWYHauS9UtJRyGfwPFfQ8cRBCxJrPYsx41cZgyR4CN7qlZJaD501iygBD-yiDKp0Xk09iRvbpWsnz8Fry-mA1iw0TRH4-69SxanTOFwETWqd7Jg3tyq5kegHNUTq37XjAegx5tBZk451Jt8fhY3uSNX_5KN0wHveKkWMAYDIt3SYcOVmqvJ4TuPrasaZcwt_u8qyTGMbI/https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAETBXOFMHZX4TTPP557OW6LTBS5KTANCNFSM4YKB24YA>.\r\n", "@jeball Looks like this is more related to `tf-addons` repo. Can you please open this issue in [tf-addons](https://github.com/tensorflow/addons) repo and close the issue here. Thanks!", "Will close this one. Thanks.\r\n\r\nJohn Ball, Ph.D.\r\nAssociate Professor and Robert Guyton Endowed Chair for Teaching Excellence | Electrical and Computer Engineering\r\n\r\n[cid:image002.png@01D711D8.89671000]\r\nAssociate Editor | IEEE Signal Processing Letters\r\nAssociate Editor | Journal of Applied Remote Sensing\r\nEditorial Board Member | MDPI Electronics\r\nFaculty Advisory Council | Transactions on Stem Education\r\nLab Director | Simrall Radar Laboratory\r\nLab Co-Director | Center for Advanced Vehicular Systems (CAVS) Sensor Lab\r\nResearch Fellow | Geosystems Research Institute (GRI)\r\nSimrall 233, 406 Hardy Rd., Mississippi State, MS 39762\r\np:662.325.4169  e:jeball@ece.msstate.edu<mailto:jeball@ece.msstate.edu>\r\n\r\n\r\nFrom: Vishnuvardhan Janapati <notifications@github.com>\r\nSent: Friday, March 5, 2021 2:49 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Ball, John <jeball@ece.msstate.edu>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] Tensorflow Addons connected components not working properly (#47451)\r\n\r\n\r\n@jeball<https://secure-web.cisco.com/17jtGKQmwg5OWWys_tyMqckm6Lmgz-cn-EzYJXu3Oz7oOrj2d75Y-otk0hTA5GFT9QQ4J21W9BGIqAIyduLm2d2r3yzpOIdHOSBDbgX2wZaUf_jhmlOaU3jD9RgzK3bSalPrYOtdKRDLjLPNd-Qs5uc63utCH3wSJ4QM4H20w_4Mvt2JnKKhfrQgkPOHgGcbG3VZUg4NtcAvtcqRjcsEr9N6uMmAPEzVSsdIlQimPYIvtyjc53TLZMIKdm_J8mXPXibW13yfxnt3yO8sUZP9T7cmRmsXLSIP4jJT1vpi9ynt2l-OqboTT9HjiDGkyNjcC/https%3A%2F%2Fgithub.com%2Fjeball> Looks like this is more related to tf-addons repo. Can you please open this issue in tf-addons<https://secure-web.cisco.com/121AV0fV5OQB5e87ktszug_6cqrP5ZetWStAiSi9gnp9RyvS5r2Px7AMY4M72oIwggkyhTyT4UViWXV3yM1B6dfKw6pXW3DXtgDAfztIWRmKpw9d5WgGw2zvKVGOLOM4D9j30CfjKA06PIY5JGc6qisHKaZzx0B63wy5E8rkeENhUucclHf7QizhPJdLssNks_qU9G26aPujtsxUiUaUiTAn0DGyRSIOZePiBikzonnMEFF0nJMB5E5TOHRjqL2ZHwrXyym8s3ioQWGpUAjfzBVatfeFG8oh15zzs6aLHNPx7UK-UPerjZnIKyeS2A3lc/https%3A%2F%2Fgithub.com%2Ftensorflow%2Faddons> repo and close the issue here. Thanks!\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://secure-web.cisco.com/1ElHn27yNDyROq7uQiHP00_YhHaFzqJKYGsM8XZSNOH8HyZJdtCbsZHExEZa225PVbvOI3ZlNDIM9kFjkdClyJjHb8Tq5R5Rx8A4tRI-BMaaloDXpwuhVf2hVe821SgXlV3HsSnXTZXLXmhU4QvhOQtr6RWswoHLD2ZnuY0SwJgBWNXSH-hQZndpoJMxAhFFwzfV0HOkuK-oBne3Wn-K1qevEcJ_MD5kDneLa3wKIg5n2OcnZaUaF5ZEoBdv_5CAvEsE0Hw9ICaTD_tCtqMplU6u2J5k7aaUovSSGDufmWnA6cV_Y5d4bbtv7e0zAAem0/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F47451%23issuecomment-791678885>, or unsubscribe<https://secure-web.cisco.com/1L85QhLE4oNib3qNQfsjByR74n_sNO4CnuK1sLjSTHfvj-iZcOAxNgQmYa7sNPHj62BjCXovqpC1Xye9evO4YYk4--WC2CdwFSw_c33v0QCA9SkZ5kHJCPdJQaJfPw29DqME-p0xr4Jyh87jCFwFiYqFj4F6ucqyOnJ8RCz5zFyzWaLEhjR66W2LfbhjjMIZbox02a7MDZXbASZdaFezDLPu-EJj9hZnAEXMsNQ8dYeFPL0NkHn4XzyT4KCT2AFuAiGRbihf3dVgdyFGgFhfS_zp62EkoE6U9IDmFk1MmOXjlbPQiTXJztKG_cZ7FgRgB/https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAETBXODSQ3VR7JDIFHHM2SLTCE7SHANCNFSM4YKB24YA>.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47451\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47451\">No</a>\n"]}, {"number": 47450, "title": "Keras model saving erroring: TypeError: get_config() missing 1 required positional argument: 'self'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: unknown\r\n- TensorFlow installed from (source or binary): binary, conda\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 7.6\r\n\r\nBug demonstrated here:\r\nhttps://stackoverflow.com/questions/57154799/keras-model-saving-erroring-typeerror-get-config-missing-1-required-position\r\n\r\nSaving model that has weights initialized by tf.keras.initializers.zeros instead of tf.keras.initializers.Zeros() does not work because tf.keras.initializers.zeros is a class while tf.keras.initializers.Zeros() is an instance. Tf tries to look at fields of class but can't because it is not an instance. I cannot save my own model like this, and instead, have to save the weights and python code to construct it.\r\n\r\nI get the following error after calling model.save(filename) on my tf.keras Model:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-52-4285f74de661> in <module>\r\n----> 1 ae.save(\r\n      2     os.path.join(folder, 'model'),\r\n      3     include_optimizer=False\r\n      4 )\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n   1049     ```\r\n   1050     \"\"\"\r\n-> 1051     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n   1052                     signatures, options)\r\n   1053 \r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    135         model, filepath, overwrite, include_optimizer)\r\n    136   else:\r\n--> 137     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n    138                           signatures, options)\r\n    139 \r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)\r\n     76     # we use the default replica context here.\r\n     77     with distribution_strategy_context._get_default_replica_context():  # pylint: disable=protected-access\r\n---> 78       save_lib.save(model, filepath, signatures, options)\r\n     79 \r\n     80   if not include_optimizer:\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n    948   meta_graph_def = saved_model.meta_graphs.add()\r\n    949 \r\n--> 950   _, exported_graph, object_saver, asset_info = _build_meta_graph(\r\n    951       obj, export_dir, signatures, options, meta_graph_def)\r\n    952   saved_model.saved_model_schema_version = constants.SAVED_MODEL_SCHEMA_VERSION\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in _build_meta_graph(obj, export_dir, signatures, options, meta_graph_def)\r\n   1034         function_aliases[fdef.name] = alias\r\n   1035 \r\n-> 1036   object_graph_proto = _serialize_object_graph(saveable_view,\r\n   1037                                                asset_info.asset_index)\r\n   1038   meta_graph_def.object_graph_def.CopyFrom(object_graph_proto)\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in _serialize_object_graph(saveable_view, asset_file_def_index)\r\n    694 \r\n    695   for obj, obj_proto in zip(saveable_view.nodes, proto.nodes):\r\n--> 696     _write_object_proto(obj, obj_proto, asset_file_def_index,\r\n    697                         saveable_view.function_name_map)\r\n    698   return proto\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in _write_object_proto(obj, proto, asset_file_def_index, function_name_map)\r\n    735           version=versions_pb2.VersionDef(\r\n    736               producer=1, min_consumer=1, bad_consumers=[]),\r\n--> 737           metadata=obj._tracking_metadata)\r\n    738       # pylint:enable=protected-access\r\n    739     proto.user_object.CopyFrom(registered_type_proto)\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in _tracking_metadata(self)\r\n   2740   @property\r\n   2741   def _tracking_metadata(self):\r\n-> 2742     return self._trackable_saved_model_saver.tracking_metadata\r\n   2743 \r\n   2744   def _list_extra_dependencies_for_serialization(self, serialization_cache):\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py in tracking_metadata(self)\r\n     52     # TODO(kathywu): check that serialized JSON can be loaded (e.g., if an\r\n     53     # object is in the python property)\r\n---> 54     return json_utils.Encoder().encode(self.python_properties)\r\n     55 \r\n     56   def list_extra_dependencies_for_serialization(self, serialization_cache):\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in python_properties(self)\r\n     39   def python_properties(self):\r\n     40     # TODO(kathywu): Add python property validator\r\n---> 41     return self._python_properties_internal()\r\n     42 \r\n     43   def _python_properties_internal(self):\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py in _python_properties_internal(self)\r\n     33 \r\n     34   def _python_properties_internal(self):\r\n---> 35     metadata = super(ModelSavedModelSaver, self)._python_properties_internal()\r\n     36     metadata.update(\r\n     37         saving_utils.model_metadata(\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/network_serialization.py in _python_properties_internal(self)\r\n     31 \r\n     32   def _python_properties_internal(self):\r\n---> 33     metadata = super(NetworkSavedModelSaver, self)._python_properties_internal()\r\n     34 \r\n     35     # Network stateful property is dependent on the child layers.\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in _python_properties_internal(self)\r\n     55         stateful=self.obj.stateful)\r\n     56 \r\n---> 57     metadata.update(get_config(self.obj))\r\n     58     if self.obj.input_spec is not None:\r\n     59       # Layer's input_spec has already been type-checked in the property setter.\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in get_config(obj)\r\n    113     # When loading, the program will attempt to revive the object from config,\r\n    114     # and if that fails, the object will be revived from the SavedModel.\r\n--> 115     config = generic_utils.serialize_keras_object(obj)['config']\r\n    116 \r\n    117   if config is not None:\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py in serialize_keras_object(instance)\r\n    268     name = get_registered_name(instance.__class__)\r\n    269     try:\r\n--> 270       config = instance.get_config()\r\n    271     except NotImplementedError as e:\r\n    272       if _SKIP_FAILED_SERIALIZATION:\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py in get_config(self)\r\n    966     if not self._is_graph_network:\r\n    967       raise NotImplementedError\r\n--> 968     return copy.deepcopy(get_network_config(self))\r\n    969 \r\n    970   @classmethod\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py in get_network_config(network, serialize_layer_fn)\r\n   2117           filtered_inbound_nodes.append(node_data)\r\n   2118 \r\n-> 2119     layer_config = serialize_layer_fn(layer)\r\n   2120     layer_config['name'] = layer.name\r\n   2121     layer_config['inbound_nodes'] = filtered_inbound_nodes\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py in serialize_keras_object(instance)\r\n    268     name = get_registered_name(instance.__class__)\r\n    269     try:\r\n--> 270       config = instance.get_config()\r\n    271     except NotImplementedError as e:\r\n    272       if _SKIP_FAILED_SERIALIZATION:\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py in get_config(self)\r\n    261         'activation': activations.serialize(self.activation),\r\n    262         'use_bias': self.use_bias,\r\n--> 263         'kernel_initializer': initializers.serialize(self.kernel_initializer),\r\n    264         'bias_initializer': initializers.serialize(self.bias_initializer),\r\n    265         'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/initializers.py in serialize(initializer)\r\n    164 @keras_export('keras.initializers.serialize')\r\n    165 def serialize(initializer):\r\n--> 166   return serialize_keras_object(initializer)\r\n    167 \r\n    168 \r\n\r\n~/anaconda3/envs/tf-2.2/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py in serialize_keras_object(instance)\r\n    268     name = get_registered_name(instance.__class__)\r\n    269     try:\r\n--> 270       config = instance.get_config()\r\n    271     except NotImplementedError as e:\r\n    272       if _SKIP_FAILED_SERIALIZATION:\r\n\r\nTypeError: get_config() missing 1 required positional argument: 'self'\r\n```\r\n\r\n", "comments": ["@CharlesSS07,\r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset you are using.\r\n\r\nAlso, please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same error. Thanks!\r\n", "Certainly @amahendrakar ,\r\n\r\nI'll show two simple python files, and the resulting terminal output, which I ran on the tf 2.4.1 docker on my mac (no GPU). The first does not save successfully. The second does.\r\n\r\n**File 1: failure**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ninput_shape = [64, 64, 3]\r\n\r\nh = inputs = tf.keras.Input(shape=input_shape)\r\n\r\nlayer = tf.keras.layers.Conv2D( # a simple convolutional layer\r\n    32,\r\n    24,\r\n    kernel_initializer=tf.initializers.glorot_uniform, # a CLASS which is used to initialize the weights\r\n)\r\n\r\nh = layer(h)\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=h)\r\nmodel.compile(optimizer=tf.optimizers.Adamax(), loss='mse')\r\nmodel.summary()\r\n\r\noutput_shape = model.predict(tf.zeros(shape=[1,*input_shape], dtype=tf.float32))[0].shape\r\nprint('Output shape:', output_shape)\r\n# model.predict causes the weights to initialize, if not already\r\n\r\n# train as much or as little as you want\r\n\r\nmodel.save('./saved-model')\r\n\r\nprint('saved model')\r\n```\r\n\r\n**File 1 output:**\r\n\r\n```\r\n# python3 failed_test.py\r\n2021-03-01 15:47:15.745023: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-03-01 15:47:15.745236: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2021-03-01 15:47:17.066388: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-01 15:47:17.066701: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2021-03-01 15:47:17.066746: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-03-01 15:47:17.066863: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5ca126ea0fc1): /proc/driver/nvidia/version does not exist\r\n2021-03-01 15:47:17.067202: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-01 15:47:17.067515: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 64, 64, 3)]       0         \r\n_________________________________________________________________\r\nconv2d (Conv2D)              (None, 41, 41, 32)        55328     \r\n=================================================================\r\nTotal params: 55,328\r\nTrainable params: 55,328\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n2021-03-01 15:47:17.137908: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-03-01 15:47:17.138614: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2492380000 Hz\r\nOutput shape: (41, 41, 32)\r\n2021-03-01 15:47:17.291897: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nTraceback (most recent call last):\r\n  File \"failed_test.py\", line 27, in <module>\r\n    model.save('./saved-model')\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 2002, in save\r\n    signatures, options, save_traces)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\", line 157, in save_model\r\n    signatures, options, save_traces)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/save.py\", line 89, in save\r\n    save_lib.save(model, filepath, signatures, options)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py\", line 1033, in save\r\n    obj, signatures, options, meta_graph_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py\", line 1198, in _build_meta_graph\r\n    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py\", line 1163, in _build_meta_graph_impl\r\n    asset_info.asset_index)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py\", line 755, in _serialize_object_graph\r\n    saveable_view.function_name_map)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py\", line 800, in _write_object_proto\r\n    metadata=obj._tracking_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 3079, in _tracking_metadata\r\n    return self._trackable_saved_model_saver.tracking_metadata\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py\", line 55, in tracking_metadata\r\n    return json_utils.Encoder().encode(self.python_properties)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 41, in python_properties\r\n    return self._python_properties_internal()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py\", line 35, in _python_properties_internal\r\n    metadata = super(ModelSavedModelSaver, self)._python_properties_internal()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 59, in _python_properties_internal\r\n    metadata.update(get_config(self.obj))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 118, in get_config\r\n    config = generic_utils.serialize_keras_object(obj)['config']\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\", line 245, in serialize_keras_object\r\n    config = instance.get_config()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\", line 650, in get_config\r\n    return copy.deepcopy(get_network_config(self))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\", line 1349, in get_network_config\r\n    layer_config = serialize_layer_fn(layer)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\", line 245, in serialize_keras_object\r\n    config = instance.get_config()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py\", line 321, in get_config\r\n    initializers.serialize(self.kernel_initializer),\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers/__init__.py\", line 135, in serialize\r\n    return generic_utils.serialize_keras_object(initializer)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\", line 245, in serialize_keras_object\r\n    config = instance.get_config()\r\nTypeError: get_config() missing 1 required positional argument: 'self'\r\n# \r\n```\r\n\r\n**File 2: success**\r\n```\r\nimport tensorflow as tf\r\n\r\ninput_shape = [64, 64, 3]\r\n\r\nh = inputs = tf.keras.Input(shape=input_shape)\r\n\r\nlayer = tf.keras.layers.Conv2D( # a simple convolutional layer\r\n    32,\r\n    24,\r\n    kernel_initializer=tf.initializers.glorot_uniform(), # a INSTANCE which is used to initialize the weights\r\n)\r\n\r\nh = layer(h)\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=h)\r\nmodel.compile(optimizer=tf.optimizers.Adamax(), loss='mse')\r\nmodel.summary()\r\n\r\noutput_shape = model.predict(tf.zeros(shape=[1,*input_shape], dtype=tf.float32))[0].shape\r\nprint('Output shape:', output_shape)\r\n# model.predict causes the weights to initialize, if not already\r\n\r\n# train as much or as little as you want\r\n\r\nmodel.save('./saved-model')\r\n\r\nprint('saved model')\r\n```\r\n\r\n**File 2 output:**\r\n```\r\n# python3 sucessful_test.py\r\n2021-03-01 15:48:05.010560: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-03-01 15:48:05.010612: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2021-03-01 15:48:06.358511: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-01 15:48:06.358718: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2021-03-01 15:48:06.358737: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-03-01 15:48:06.358755: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5ca126ea0fc1): /proc/driver/nvidia/version does not exist\r\n2021-03-01 15:48:06.359019: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-01 15:48:06.360054: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 64, 64, 3)]       0         \r\n_________________________________________________________________\r\nconv2d (Conv2D)              (None, 41, 41, 32)        55328     \r\n=================================================================\r\nTotal params: 55,328\r\nTrainable params: 55,328\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n2021-03-01 15:48:06.432318: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-03-01 15:48:06.432802: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2492380000 Hz\r\nOutput shape: (41, 41, 32)\r\n2021-03-01 15:48:06.587815: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nsaved model\r\n# \r\n```\r\n\r\nThe only difference between these two files is this line:\r\n```kernel_initializer=tf.initializers.glorot_uniform```, which is\r\n```kernel_initializer=tf.initializers.glorot_uniform()``` in file 2.\r\nIn both cases, the weights are initialized without error and successfully predict on a dummy example. It seems to me that since there was no errors raised during initialization, saving should therefore work in both cases? If they all represent the same thing, why do we have the option to use tf.keras.initializers.glorot_uniform, tf.keras.initializers.glorot_uniform(), \"glorot_uniform\", or tf.keras.initializers.GlorotUniform as the initializer in a layer?\r\n\r\nThanks, and hoping to understand this better.", "@CharlesSS07,\r\nI did not face any errors while running both the code snippets with TF v2.4.1, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b5e7b289755797b911f71af5a098be7a/47450.ipynb).\r\n\r\nCould you please try running the code in a new virtual environment and check if it helps. Thanks!", "Hi @amahendrakar ,\r\n\r\nLooks like I made a mistake while copying and pasting those two code snippets. I have edited my previous comment. The mistake was in the first code snippet which was supposed to error, and I had accidentally added a '()'. This mistake made the two code snippets equivalent, and so the error never occurred in your gist.\r\n\r\nThanks for catching that, I have no idea how it happened. Could you try it again in your gist, without the parentheses after the initializer of the conv2d layer?", "@CharlesSS07,\r\nThank you for the update.\r\n\r\nPlease take a look at similar issue [#47054](https://github.com/tensorflow/tensorflow/issues/47054), the fix for it [#47128](https://github.com/tensorflow/tensorflow/pull/47128) and check if it helps.", "Hi @amahendrakar ,\r\n\r\nBoth of those issues display the same behavior, and they say that tf.initializers.zeros is supposed to be a pre-initialized version of tf.initializers.Zeros, for example, but that you have to call tf.initializers.zeros() to avoid the error. The bug behavior is in that passing tf.initializers.zeros to a model works for training, but not for saving, whereas passing tf.initializers.zeros() works for both saving and training. It's logical that if a function works for training, then it should also work for saving, so there should either be an error on passing tf.initializers.zeros to a layer, or getting tf.initializers.zeros would return the value of calling tf.initializers.zeros().\r\n\r\nThanks, and is it possible to change the behavior of tf.initializers.zeros to return the value of tf.initializers.zeros()?", "@ymodak,\r\nI was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/18ca363a469ad1beede9cd52103b9983/47450.ipynb#scrollTo=-xSgFNJafS94). Thanks!", "This is fixed latest tf-nightly and TF 2.5. See [gist](https://colab.research.google.com/gist/ymodak/e9a4b5c63742ab13bd2eb94718681190/47450.ipynb)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thanks everyone. I think we can close this out now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47450\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47450\">No</a>\n"]}, {"number": 47449, "title": "Lr workaround for wrapped  optimizers", "body": "Investigate side effects in CI tests with a Workaround for mixed precision hyper access with wrapped optimizers.\r\n\r\nSee:\r\nhttps://github.com/tensorflow/addons/pull/2404\r\n", "comments": ["/cc @reedwm ", "It looks like this passes tests and will fix tensorflow/addons#2404, but I have two concerns:\r\n\r\n1. We assume an optimizer is an optimzier wrapper if and only if it has an `_optimizer` attribute. But an optimizer wrapper could store the inner optimizer in a different attribute (say `_inner_optimizer`) and it's possible for a non-wrapper to have an attribute called `_optimizer`.\r\n2. This approach doesn't work with triply nested optimizers. E.g. if LossScaleOptimizer wrapps Lookahead which wraps SWA which wraps Adam.\r\n\r\nI think we either need a stable overridable method which gets the hypereparameters, or have an `OptimizerWrapper` class which all optimizers use and handles this case. I prefer the former, and will try to come up with more solutions.", "Another potential solution would be to require (by convention) that all optimizer wrappers expose an `_optimizer` wrapper, which takes care of (1). For (2), we keep accessing the `_optimizer` attribute in a loop until we get the inner-most optimizer.\r\n\r\n/CC @omalleyt12 ", "> Another potential solution would be to require (by convention) that all optimizer wrappers expose an _optimizer wrapper, which takes care of (1)\r\n\r\nYes sorry I supposed that `_optimizer` was already a quite  \"de-facto\" standard.", "Closing as tensorflow/addons#2403 has been fixed.", "Just as reference fixed in Tensorflow for `learning_rate` hyper with commit: https://github.com/tensorflow/tensorflow/commit/31d58c65625592453cb9456e9cf663d891de599e"]}, {"number": 47447, "title": "Quantize for pooling op", "body": "The int8 pooling(avg and max) does not have rescale because operators have the same scale and zero point for\r\ninput and output tensors. \r\nI'm wondering how to keep same scale, since distributions for different for intput and output tensors after max or avg operation.\r\n![image](https://user-images.githubusercontent.com/19774920/109381833-97a03600-7917-11eb-8d9d-c87d2d6be964.png)\r\n", "comments": ["@yisongsong \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced], also please share  data in text format its easier to search.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47446, "title": "Modify Transpose kernel to work in TFLu", "body": "This PR modifies the transpose kernel to make it run in micro, ports the tests and adds the kernel to the micro build.\r\n\r\nThis is PR 4/4 in delivering #45695", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "This PR also impacts the performance changes on the TFLite. Could you use the micro macro to divide the logics? For example, keeping the original logic for the TFlite and using a new logics for only micro cases.", "@abattery I am not sure I understand what you mean. The changes in this PR should only affect the micro kernel. How does this impact the performance changes on TFLite?", "Sorry @patriklaurell you are right. I misread the filename. Please ignore my comment.", "Hi, which is the status of this PR? Thank you.", "@petewarden I updated this PR and added two commits for porting the tests and adding transpose to the micro build. It is ready for review.", "@advaitjain @petewarden The MacOS CPU Python3 test is failing but there is very little information in the test log and I have no way of reproducing the error locally. Do you have any insight into why it fails?", "> @advaitjain @petewarden The MacOS CPU Python3 test is failing but there is very little information in the test log and I have no way of reproducing the error locally. Do you have any insight into why it fails?\r\n\r\nUnfortunately, I don't know why it is failing either. Let's have you address the review comments and then we can try and figure out what is going on with the MacOS build.", "@advaitjain ready for review. Did you find anything regarding the MacOS test?", "> @advaitjain ready for review. Did you find anything regarding the MacOS test?\r\n\r\nI really do not know what is going on with the MacOS failure. @petewarden, any ideas?", "Can you merge master and then get this PR building again after changes from https://github.com/tensorflow/tensorflow/commit/822ddbf71817de64df9b6741e007a240e7b61c8d\r\n\r\nThat will fix the MacOS and internal CI errors."]}, {"number": 47445, "title": "\"ValueError: No gradients provided \" error in TF v2.4.1, works fine on v2.3.1", "body": "**System information**\r\nPython : 3.6.3, TF: 2.4.1, TF probability: 0.12.1\r\nPython : 3.6.9, TF: 2.3.1, TF probability: 0.11.1\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu and CentOS\r\n- TensorFlow installed from (source or binary): Pip\r\n- TensorFlow version (use command below): 2.4.1 and 2.3.1\r\n- Python version: 3.6.9 and 3.6.3\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\nI have copied simple Gaussian process example from the tensorflow probability site. It works fine on version 2.3.1 but yields \r\n`ValueError: No gradients provided for any variable:` error on tensorflow 2.4.1\r\n\r\nThe example file  colab has been attached alongwith.\r\n\r\n**Describe the expected behavior**\r\nBoth shall result in identical behaviour. And should produce gradients.\r\n\r\n**Standalone code to reproduce the issue**\r\nReproduced here:\r\n\r\nhttps://colab.research.google.com/drive/1Tx-46aoF4i5mHMuJ7Et13B1km7u6i7YV#scrollTo=RXrq442mMGJD\r\n\r\nExample adapted from here:\r\n\r\nhttps://www.tensorflow.org/probability/examples/Gaussian_Process_Regression_In_TFP\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nError:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\n\r\nValueError                                Traceback (most recent call last)\r\n\r\n<ipython-input-1-6493a18eac89> in <module>()\r\n    116                             observation_noise_variance_var)\r\n    117   grads = tape.gradient(loss, trainable_variables)\r\n--> 118   optimizer.apply_gradients(zip(grads, trainable_variables))\r\n    119   lls_[i] = loss\r\n    120 \r\n\r\n1 frames\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name, experimental_aggregate_gradients)\r\n    596       RuntimeError: If called in a cross-replica context.\r\n    597     \"\"\"\r\n--> 598     grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\r\n    599     var_list = [v for (_, v) in grads_and_vars]\r\n    600 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/utils.py in filter_empty_gradients(grads_and_vars)\r\n     77   if not filtered:\r\n     78     raise ValueError(\"No gradients provided for any variable: %s.\" %\r\n---> 79                      ([v.name for _, v in grads_and_vars],))\r\n     80   if vars_with_empty_grads:\r\n     81     logging.warning(\r\n\r\nValueError: No gradients provided for any variable: ['amplitude:0', 'length_scale:0', 'observation_noise_variance_var:0'].\r\n```", "comments": ["@ipcamit  can you please grant access to the reproducible code ?", "Added file permission\n\nOn Sun, 28 Feb 2021, 15:11 Hiran Sarkar, <notifications@github.com> wrote:\n\n> @ipcamit <https://github.com/ipcamit> can you please grant access to the\n> reproducible code ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47445#issuecomment-787424262>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABTZ2D3ZB73OGK6Y7B6BBJDTBIFTTANCNFSM4YJUDFVA>\n> .\n>\n", "Its basically copy pasted code from tensorflow docs. \r\n\r\nPublic Link: https://colab.research.google.com/drive/1Tx-46aoF4i5mHMuJ7Et13B1km7u6i7YV?usp=sharing\r\n\r\nreproducing here, as well.\r\n\r\n```\r\nimport time\r\n \r\nimport numpy as np\r\nimport tensorflow.compat.v2 as tf\r\nimport tensorflow_probability as tfp\r\ntfb = tfp.bijectors\r\ntfd = tfp.distributions\r\ntfk = tfp.math.psd_kernels\r\nimport matplotlib.pyplot as plt\r\n \r\nfrom mpl_toolkits.mplot3d import Axes3D\r\n# Configure plot defaults\r\nplt.rcParams['axes.facecolor'] = 'white'\r\nplt.rcParams['grid.color'] = '#666666'\r\n \r\n \r\ndef sinusoid(x):\r\n  return np.sin(3 * np.pi * x[..., 0])\r\n \r\ndef generate_1d_data(num_training_points, observation_noise_variance):\r\n  \"\"\"Generate noisy sinusoidal observations at a random set of points.\r\n \r\n  Returns:\r\n     observation_index_points, observations\r\n  \"\"\"\r\n  index_points_ = np.random.uniform(-1., 1., (num_training_points, 1))\r\n  index_points_ = index_points_.astype(np.float64)\r\n  # y = f(x) + noise\r\n  observations_ = (sinusoid(index_points_) +\r\n                   np.random.normal(loc=0,\r\n                                    scale=np.sqrt(observation_noise_variance),\r\n                                    size=(num_training_points)))\r\n  return index_points_, observations_\r\n \r\n# Generate training data with a known noise level (we'll later try to recover\r\n# this value from the data).\r\nNUM_TRAINING_POINTS = 100\r\nobservation_index_points_, observations_ = generate_1d_data(\r\n    num_training_points=NUM_TRAINING_POINTS,\r\n    observation_noise_variance=.1)\r\n \r\ndef build_gp(amplitude, length_scale, observation_noise_variance):\r\n  \"\"\"Defines the conditional dist. of GP outputs, given kernel parameters.\"\"\"\r\n \r\n  # Create the covariance kernel, which will be shared between the prior (which we\r\n  # use for maximum likelihood training) and the posterior (which we use for\r\n  # posterior predictive sampling)\r\n  kernel = tfk.ExponentiatedQuadratic(amplitude, length_scale)\r\n \r\n  # Create the GP prior distribution, which we will use to train the model\r\n  # parameters.\r\n  return tfd.GaussianProcess(\r\n      kernel=kernel,\r\n      index_points=observation_index_points_,\r\n      observation_noise_variance=observation_noise_variance)\r\n \r\ngp_joint_model = tfd.JointDistributionNamed({\r\n    'amplitude': tfd.LogNormal(loc=0., scale=np.float64(1.)),\r\n    'length_scale': tfd.LogNormal(loc=0., scale=np.float64(1.)),\r\n    'observation_noise_variance': tfd.LogNormal(loc=0., scale=np.float64(1.)),\r\n    'observations': build_gp,\r\n})\r\n \r\nx = gp_joint_model.sample()\r\nlp = gp_joint_model.log_prob(x)\r\n \r\nprint(\"sampled {}\".format(x))\r\nprint(\"log_prob of sample: {}\".format(lp))\r\n \r\n# Create the trainable model parameters, which we'll subsequently optimize.\r\n# Note that we constrain them to be strictly positive.\r\n \r\nconstrain_positive = tfb.Shift(np.finfo(np.float64).tiny)(tfb.Exp())\r\n \r\namplitude_var = tfp.util.TransformedVariable(\r\n    initial_value=1.,\r\n    bijector=constrain_positive,\r\n    name='amplitude',\r\n    dtype=np.float64)\r\n \r\nlength_scale_var = tfp.util.TransformedVariable(\r\n    initial_value=1.,\r\n    bijector=constrain_positive,\r\n    name='length_scale',\r\n    dtype=np.float64)\r\n \r\nobservation_noise_variance_var = tfp.util.TransformedVariable(\r\n    initial_value=1.,\r\n    bijector=constrain_positive,\r\n    name='observation_noise_variance_var',\r\n    dtype=np.float64)\r\n \r\ntrainable_variables = [v.trainable_variables[0] for v in \r\n                       [amplitude_var,\r\n                       length_scale_var,\r\n                       observation_noise_variance_var]]\r\n \r\n# Use `tf.function` to trace the loss for more efficient evaluation.\r\n@tf.function(autograph=False, experimental_compile=False)\r\ndef target_log_prob(amplitude, length_scale, observation_noise_variance):\r\n  return gp_joint_model.log_prob({\r\n      'amplitude': amplitude,\r\n      'length_scale': length_scale,\r\n      'observation_noise_variance': observation_noise_variance,\r\n      'observations': observations_\r\n  })\r\n# Now we optimize the model parameters.\r\nnum_iters = 1000\r\noptimizer = tf.optimizers.Adam(learning_rate=.01)\r\n \r\n# Store the likelihood values during training, so we can plot the progress\r\nlls_ = np.zeros(num_iters, np.float64)\r\nfor i in range(num_iters):\r\n  with tf.GradientTape() as tape:\r\n    loss = -target_log_prob(amplitude_var, length_scale_var,\r\n                            observation_noise_variance_var)\r\n  grads = tape.gradient(loss, trainable_variables)\r\n  optimizer.apply_gradients(zip(grads, trainable_variables))\r\n  lls_[i] = loss\r\n \r\nprint('Trained parameters:')\r\nprint('amplitude: {}'.format(amplitude_var._value().numpy()))\r\nprint('length_scale: {}'.format(length_scale_var._value().numpy()))\r\nprint('observation_noise_variance: {}'.format(observation_noise_variance_var._value().numpy()))\r\n```", "Was able to reproduce the error with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/7e5b6c862b2f856bbeffb4ed9898152c/47445.ipynb) and TF-nightly. \r\n\r\nHowever, I did not face any errors while running the code with [TF v2.3.2](https://colab.research.google.com/gist/amahendrakar/0806b3b0ce33a030bb9885261ad1800c/47445-2-3.ipynb#scrollTo=mn_knFxrME8c). Please check the linked gist for reference. Thanks!", "@ipcamit Thank you for bringing this to our attention.  We're still investigating, but in the meantime the easiest thing to do is to _not_ apply `tf.function` to `target_log_prob` and instead apply it to a wrapper with no arguments:\r\n\r\n```\r\n# remove the `tf.function` around `target_log_prob` and create a new closure\r\n@tf.function(autograph=False, experimental_compile=False)\r\ndef loss_fn():\r\n  return -target_log_prob(amplitude_var, length_scale_var, observation_noise_variance_var)\r\n\r\n# and inside the training loop write\r\n...\r\n  with tf.GradientTape() as tape:\r\n    loss = loss_fn()\r\n...\r\n```\r\n\r\nWe will update the colab example similarly.  In the meantime we will continue to investigate and report back if we have further findings.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47445\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47445\">No</a>\n"]}, {"number": 47444, "title": "fix erroneous assign in Env::StartTransaction()", "body": "env.h StartTransaction(), token isn't being properly null'd (in master)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47444) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!\n\nOn Fri, Feb 26, 2021 at 7:39 PM google-cla[bot] <notifications@github.com>\nwrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here with @googlebot\n> I signed it! and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2139\ufe0f *Googlers: Go here\n> <https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47444>\n> for more info*.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/47444#issuecomment-786997270>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACVWZ7JUS3O76S56RKRKFXTTBBSN7ANCNFSM4YJP5OLQ>\n> .\n>\n"]}, {"number": 47443, "title": "First step towards prototyping v2 of TFLM integration with external IDEs.", "body": "With this change we can create an output directory containing all the sources and headers (including third_party) needed to build the TFLM static library.\r\n\r\nSee tools/ci_build/test_project_generation.sh for some sample commands.\r\n\r\nA next step will be to add the sources for the examples as well.\r\n\r\nProgress towards: #47413\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47442, "title": "AttributeError: module 'tensorflow' has no attribute 'Session' with tensorflow 2.4.1", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **windows 10**\r\n- TensorFlow installed from (source or binary): installed from source\r\n- TensorFlow version (use command below): **2.4.1**\r\n- Python version: **3.8.5**\r\n- CUDA/cuDNN version: Cuda 11.2/11.1\r\n- GPU model and memory: **Intel(R) UHD Graphics 630\r\nNVIDIA GeForce GTX 1050**\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\nv2.4.0-49-g85c8b2a817f 2.4.1\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI installed the tensorflow-gpu:\r\n```\r\nconda create -n tensorflow-gpu\r\nactivate tensorflow-gpu\r\npip install tensorflow-gpu\r\n\r\nactivate tensorflow-gpu\r\n\r\npython\r\nimport tensorflow as tf\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n\r\nI got this error: AttributeError: module 'tensorflow' has no attribute 'Session'\r\n```\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nThe Session() should be executed correctly.\r\n\r\n", "comments": ["@slim-hmidi,\r\n`tf.Session` is used in TensorFlow 1.x and has been deprecated in TensorFlow 2.x. In Tensorflow 2.x, eager execution is enabled by default, thus eliminating the need for Sessions.\r\n\r\nFor more information, please take a look at similar issues [#34658](https://github.com/tensorflow/tensorflow/issues/34658#issuecomment-559511245) and [#18538](https://github.com/tensorflow/tensorflow/issues/18538\r\n). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47442\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47442\">No</a>\n"]}, {"number": 47441, "title": "tensorflow-gpu 2.2.0 doesn't recognize Nvidia MX130 GPU", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10 Build 19042\r\n- TensorFlow installed from (source or binary): `pip install tensorflow-gpu==2.2.0` in conda environment.\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.7.9\r\n- Installed using pip.\r\n- CUDA/cuDNN version: `CUDA 10.1.243`, `cuDNN 7.6.5`\r\n- GPU model and memory: GeForce MX130, 2GB\r\n\r\n**Problem Description:**\r\nTensorFlow 2.2.0 doesn't recognize my GPU.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem:**\r\n\r\n1. Installed [CUDA 10.1 (Update 2)](https://developer.nvidia.com/cuda-10.1-download-archive-update2?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal) to `D:\\Program Files\\CUDA\\v10.1`.\r\n2. Extracted [cuDNN 7.6.5](https://developer.nvidia.com/rdp/cudnn-archive#a-collapse765-101) to `D:\\cuda`.\r\n3. Set the correct path, as instructed in [https://www.tensorflow.org/install/gpu for TensorFlow 2.2.0](https://web.archive.org/web/20200603082035if_/https://www.tensorflow.org/install/gpu#software_requirements).\r\n```batch\r\nSET PATH=D:\\Program Files\\CUDA\\v10.1\\bin;%PATH%\r\nSET PATH=D:\\Program Files\\CUDA\\v10.1\\extras\\CUPTI\\lib64;%PATH%\r\nSET PATH=D:\\Program Files\\CUDA\\v10.1\\include;%PATH%\r\nSET PATH=D:\\cuda\\bin;%PATH%\r\n```\r\n\r\n4. In Python, imported TensorFlow and tried to check for the GPU:\r\n```python\r\nimport tensorflow as tf\r\ntf.test.is_built_with_cuda()\r\ntf.config.experimental.list_physical_devices('gpu')\r\n```\r\nOutput is:\r\n```python\r\n>>> import tensorflow as tf\r\n2021-02-26 19:58:38.967202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n\r\n>>> tf.test.is_built_with_cuda()\r\nTrue\r\n\r\n>>> tf.config.experimental.list_physical_devices('gpu')\r\n2021-02-26 21:14:53.525272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2021-02-26 21:14:54.056784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2021-02-26 21:14:54.057159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2021-02-26 21:14:54.068576: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2021-02-26 21:14:54.076899: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2021-02-26 21:14:54.079739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2021-02-26 21:14:54.090196: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2021-02-26 21:14:54.095665: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2021-02-26 21:14:55.099130: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2021-02-26 21:14:55.300935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n[]\r\n```\r\n\r\nNot a duplicate of [issue #41892](https://github.com/tensorflow/tensorflow/issues/41892), as I use Windows 10.\r\n\r\nEdit:\r\nAs @ymodak suggested:\r\n```python\r\nimport tensorflow as tf\r\ntf.config.list_logical_devices('GPU')\r\n```\r\ndoes return:\r\n```python\r\n>>> import tensorflow as tf\r\n2021-02-27 14:21:36.675965: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n\r\n>>> tf.config.list_logical_devices('GPU')\r\n2021-02-27 14:26:59.304811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2021-02-27 14:26:59.803139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2021-02-27 14:26:59.803491: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2021-02-27 14:27:00.179875: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2021-02-27 14:27:00.224576: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2021-02-27 14:27:00.247267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2021-02-27 14:27:00.290612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2021-02-27 14:27:00.313676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2021-02-27 14:27:00.834454: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2021-02-27 14:27:01.102483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2021-02-27 14:27:01.111434: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2021-02-27 14:27:01.160065: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e9c1582e60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-02-27 14:27:01.160517: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-02-27 14:27:01.207202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2021-02-27 14:27:01.207683: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2021-02-27 14:27:01.212166: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2021-02-27 14:27:01.215100: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2021-02-27 14:27:01.216437: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2021-02-27 14:27:01.219085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2021-02-27 14:27:01.220720: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2021-02-27 14:27:01.221751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2021-02-27 14:27:01.223134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2021-02-27 14:27:16.215597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-02-27 14:27:16.216163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0\r\n2021-02-27 14:27:16.221461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N\r\n2021-02-27 14:27:16.304146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1377 MB memory) -> physical GPU (device: 0, name: GeForce MX130, pci bus id: 0000:02:00.0, compute capability: 5.0)\r\n2021-02-27 14:27:16.432698: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e9dec28fe0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-02-27 14:27:16.433279: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX130, Compute Capability 5.0\r\n[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\r\n```\r\n_but_ in Jupyter notebook it returns:\r\n```python\r\n[]\r\n```\r\n_AND_ it still does not train my model using the GPU.", "comments": ["I have the same problem with `tensorflow-2.4.1` on jupyter notebook.\r\nIt does not detect the GPU.", "> I have the same problem with `tensorflow-2.4.1` on jupyter notebook.\r\n> It does not detect the GPU.\r\n\r\nAs far as I understand it, after TensorFlow 2.2.0 they do not support compute capability 5.0, therefore TensorFlow will not support this MX130 unless you compile it from source to add support for that (I might have got that wrong? I'd appreciate if someone can correct me on this).\r\n\r\nCheck [issue #41892](https://github.com/tensorflow/tensorflow/issues/41892) if you're using linux.", "Can you test with with uppercase string `'GPU'`? Thanks!\r\n```python\r\ntf.config.list_logical_devices('GPU')\r\n```", "> Can you test with with uppercase string `'GPU'`? Thanks!\n> \n> ```python\n> \n> tf.config.list_logical_devices('GPU')\n> \n> ```\n\nThanks, that does list my GPU - but it still does not use it while training. Also, in Jupiter notebook it doesn't list my GPU at all. ", "You may try [manual device placement](https://www.tensorflow.org/guide/gpu#manual_device_placemen);\r\n```python\r\ntf.debugging.set_log_device_placement(True)\r\n# Place ops on the GPU\r\nwith tf.device('/GPU:0'):\r\n  # Your code here...\r\n```", "> You may try [manual device placement](https://www.tensorflow.org/guide/gpu#manual_device_placemen);\r\n> \r\n> ```python\r\n> tf.debugging.set_log_device_placement(True)\r\n> # Place ops on the GPU\r\n> with tf.device('/GPU:0'):\r\n>   # Your code here...\r\n> ```\r\n\r\nThis seems to solve the issue when using Python directly instead of Jupyter Notebook - so I guess that's a different bug. Also experiencing some performence issues, so I'll probably end up compiling TensorFlow myself. Anyway, both issues are not so related to this one, so I'll consider it solved.\r\nThanks a lot!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47441\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47441\">No</a>\n"]}, {"number": 47440, "title": "add MNIST training to `mnist_grad_test`", "body": "@saxenasaurabh \r\n\r\nOne last PR for the cleanup process. I ran the test 500 times and the accuracy is around 92% - 95%. I think it's because we don't have any validation set yet. Please take a look at this PR ! Thank you !", "comments": ["@saxenasaurabh could you take a look at it, please ?", "@saxenasaurabh Can you please review this PR ? Thanks!", "Since we are trying another approach, I will close this PR."]}, {"number": 47439, "title": "Make the output for model summary wrap", "body": "This is just a small alteration to the printing logic of the model summary to allow the cells to wrap, this is useful for model with long outputs/inputs (such as BERT, which is what motivated me to do this). Useful for rigorous storage of model summaries.\r\n\r\n[x] Read contributing guidelines.\r\n[x] Read Code of Conduct.\r\n[x] Ensure you have signed the Contributor License Agreement (CLA).\r\n[x] Check if my changes are consistent with the guidelines. (I think they are)\r\n[x] Changes are consistent with the Coding Style. (File's pylint score unchanged from 8.56)\r\n[ ] Run Unit Tests.\r\n\r\nI am really struggling to be able to run these tests, been trying for a good 4/5 hours, I am on Ubuntu 20.04, I have:\r\n1. Tried running `tensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/...` from repo root, this consistently errors due to `tensorflow/tools/ci_build/install/install_pip_packages.sh` not seeing the correct python version (which weirdly is trying to use python3.6 even though the git blame for the changes to line 26 suggest the purpose was to migrate to python 3.7)\r\n2. Tried changing the docker file to some other one, I have tried the `tensorflow/tools/ci_build/Dockerfile.pi-python38` (which i now realise is for raspberry pi's - derp!) and `tensorflow/tools/ci_build/Dockerfile.local-toolchain-ubuntu18.04-manylinux2010`. Neither worked.\r\n3. Ultimately, I have pulled `tensorflow/tensorflow   latest-devel   a4c4b6decc02` and exec'd into the container and tried to run `bazel test //tensorflow/python/keras` but that has errored due to:\r\n```\r\nERROR: /tensorflow_src/tensorflow/core/kernels/BUILD:2736:18: C++ compilation of rule '//tensorflow/core/kernels:resource_variable_ops' failed (Exit 4): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 240 argument(s) skipped)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\n```\r\n\r\nI am really keen to try and get these tests running to verify my changes haven't affected anything (although searching the repo for tests using `model.summary()` suggests everything should still pass.\r\n", "comments": ["would be happy to write a test along with this if I could get the tests working on my machine \ud83d\ude48 ", "@Llamrei Thank you for your contribution. This could be useful but requires some testing. Could you please supply some screenshots that illustrate the old and new behavior and how it's supposed to work?", "Hey sorry for the late reply - please see attached examples of changes proposed. As i said in post above I would be pretty keen to write some tests but I cannot for the life of me get them working locally so it would be pretty darn painful making them without any feedback and just waiting for comments from you guys\r\n\r\n\r\n\r\nBefore:\r\n![Screenshot from 2021-03-15 11-48-13](https://user-images.githubusercontent.com/22492881/111149503-dde6cd80-8584-11eb-82e0-427f0caf9f45.png)\r\nAfter:\r\n![Screenshot from 2021-03-15 11-45-43](https://user-images.githubusercontent.com/22492881/111149500-dd4e3700-8584-11eb-95e9-599dc380c14f.png)\r\n\r\n", "@Llamrei  Can you please check @deeb02's comments and keep us posted ? Thanks!", "Thank you for the response! Hadn't checked in on this in a while my bad - I know what's causing this and will push a fix up in the next couple of days.", "I have implemented a simple fix for the issue as requested, see the new output attached - I admit this lowers the quality of `connected to` column a little.\r\n\r\nHowever, I think it still adds a benefit as now layers with long names that are `connected to` are no longer truncated and thus the original purpose of this proposal - to make sure the summary presents all the information it has access to - is fulfilled. \r\n\r\nI could add logic for handling lists/dictionaries whilst being aware of key/entry string lengths for an optimal presentation but I wonder how much value it would add relative to added complexity in the code?\r\n\r\n![connected_to_not_generating_extra_lines](https://user-images.githubusercontent.com/22492881/116937731-9534c200-ac61-11eb-915f-b48368473458.png)\r\n", "@deeb02  Can you please assist on above comments from @Llamrei. Thanks!", "Hey so I can see some of the tests are failing - as I said in the original post after like a day of messing around with everything under the sun I couldn't get tests to work locally so i guess this is to be expected. \r\n\r\nLooking at the [output of the failed tests](https://source.cloud.google.com/results/invocations/faa0f8f0-345d-4252-95ae-12cd9e05c2fb/targets/%2F%2Ftensorflow%2Fpython%2Fkeras%2Ftests:model_subclassing_test;shard=3;run=1;attempt=1/tests;group=__main__.ModelSubclassingTest;test=test_summary_v2_eager;row=2) I think it is because I was a little lazy with spacing of the final column. That should be fixed now.\r\n\r\nMore generally I can envisage a reality where someone could write a test that fails due to the output being broken out over multiple lines but that should not be any of the current test suite and I think writing tooling to introspect the output more carefully (column by column) is overkill pre-emptive action.\r\n\r\nEDIT:\r\nI am not sure why the ROCm tests are [failing](http://ml-ci.amd.com:21096/job/tensorflow/job/github-prs-upstream-master/job/AMD-ROCm-Community-CI-Build/job/PR-47439/34/console) but it seems to be a build failure instead of something related to the proposed changes.", "Just realised condition checking in above fix was the wrong way round - silly me. \r\n\r\nShould be all ok now - still not sure what's up with the ROCm build.", "Is there anything else I ought to do? As in both the failed tests it looks like the tool died not the code failing?", "Bump ^^", "@deeb02 Can you please assist on above comments from @Llamrei. Thanks!", "@Llamrei, we are moving active development of Keras to https://github.com/keras-team/keras and streamlining the process(including testing). Could you please move this PR to that repository?", "Sure thing!", "Closing this PR, since it is moved to keras-team/kears repo (https://github.com/keras-team/keras/pull/14865). Thank you."]}, {"number": 47438, "title": "Correctly close file descriptors when loading saved models", "body": "This PR uses the `FileIO` contextmanager in order to ensure that filedescriptors are correctly closed after use when loading saved models.", "comments": []}, {"number": 47437, "title": "Building TensorFlow Lite - macos_arm64 support using cmake", "body": "with respect to this [issue](https://github.com/google/XNNPACK/issues/1302) in the XNNPACK repository:  `bazel` builds for macos_arm64 platform are not yet supported.\r\nhowever building with cmake works successfully.\r\n\r\nthis change allows to build the `TensorFlow Lite library` and the `benchmark_model` on a macos_arm64 host platform when using `-DTFLITE_ENABLE_XNNPACK=ON` (default). see [instructions](https://www.tensorflow.org/lite/guide/build_cmake).", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47437) for more info**.\n\n<!-- need_sender_cla -->", "@terryheo could you review this PR?", "@googlebot I signed it!", "oneDNN unit test failed?", "once this [PR](https://github.com/google/XNNPACK/pull/1327) is accepted in [XNNPACK](https://github.com/google/XNNPACK) repo, we can then also proceed with a new PR so `TensorFlow Lite`and `benchmark_model`can be built for `macos_arm64`using:\r\n\r\n> bazel build --config=macos_arm64 -c opt //tensorflow/lite:libtensorflowlite.so\r\nbazel build --config=macos_arm64 -c opt //tensorflow/lite/tools/benchmark:benchmark_model\r\n", "Oh, are you going to send another PR when https://github.com/google/XNNPACK/pull/1327 is merged?\r\nOtherwise, it might be better to wait?", "> Oh, are you going to send another PR when [google/XNNPACK#1327](https://github.com/google/XNNPACK/pull/1327) is merged?\r\n> Otherwise, it might be better to wait?\r\n\r\nthank you. mainly this PR was for updating the cmake build files for `macos_arm64`, but you're right.. it might indeed be better to wait for the `XNNPACK` PR to be accepted. so we can pull in all changes at once.\r\n\r\nif you're ok with the PR naming I will wait and add the remaining commits of `XNNPACK` and `pthreadpool` too here. would be nice if you could review again to be safe.\r\n\r\nthere also needs to be a change for cpuinfo. did some testing and pulled the latest master branch in , but the [cpuinfo.patch](https://github.com/tensorflow/tensorflow/blob/master/third_party/cpuinfo/cpuinfo.patch) is not compatible so I deactivated applying this patch in [WORKSPACE](https://github.com/tensorflow/tensorflow/blob/master/third_party/cpuinfo/workspace.bzl) for verification. this went fine for `macos_arm64` and host builds on `macos_x86-64` but I'm not sure if this is the right way to go.\r\n\r\nwhat do you think?", "I see. It might take time to wait for all the XNNPack related changes are merged.\r\nI'm fine with having the second PR so we have can more time to verify all XNNPack related patches.", "This PR is merged and the [XNNPack PR](https://github.com/google/XNNPACK/pull/1327) got LGTM.\r\n\r\n@simonmaurer I have one question. Does the \"--config=macos_arm64\" option only works on M1 machine?\r\nI can't use it on my Ubuntu desktop and x86 Mac machine.", "@terryheo thanks. if you're talking about the TensorFlow repo: this accepted PR here only allows the host build for `macos_arm64` using cmake as described [here](https://www.tensorflow.org/lite/guide/build_cmake).\r\nI'll make the additional PR here to pull in the latest `pthreadpool` (PR accepted & merged), `XNNPACK` (PR accepted) and `cpuinfo` once they are both merged.\r\n\r\nhowever if you're talking about the XNNPACK repo (the PR is indeed accepted, but not yet merged):\r\n\r\n> bazel build --config=macos_arm64 -c opt :end2end_bench\r\n\r\nwill work on a host M1 machine. on different platforms (like Linux or even macos x86-64) that does not work (yet) because of missing toolchains\r\nat least I get the following errors when building on a macOS_x86-64 platform for `macos_arm64`:\r\n\r\n`ERROR: /private/var/tmp/**/**/external/local_config_cc/BUILD:48:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'darwin_arm64'\r\nINFO: Repository cpuinfo instantiated at:\r\n  /**/**/**/**/xnnpack_simonmaurer/WORKSPACE:66:13: in <toplevel>\r\nRepository rule http_archive defined at:\r\n  /private/var/tmp/**/**/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\nERROR: Analysis of target '//:end2end_bench' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed`", "@terryheo another note:\r\nideally we would want bazel to work like this:\r\n\r\n**M1 host machine compilation** (without specifying the platform):\r\n> bazel build -c opt :end2end_bench (XNNPACK)\r\n> bazel build -c opt //tensorflow/lite/tools/benchmark:benchmark_model (TF)\r\n\r\n**Cross compilation** on a different platform (e.g. Linux x86):\r\n> bazel build -c opt --config=macos_arm64 :end2end_bench (XNNPACK)\r\n> bazel build -c opt --config=macos_arm64 //tensorflow/lite/tools/benchmark:benchmark_model (TF)\r\n\r\nthe `pip_package` should work already according to this [PR](https://github.com/tensorflow/tensorflow/commit/bab0d14036efd0adcd4e48303d045cee3c342cb0), which builds fine on a M1 host but I got errors about missing symbols when importing the tensorflow package in python3", "> the `pip_package` should work already according to this [PR](https://github.com/tensorflow/tensorflow/commit/bab0d14036efd0adcd4e48303d045cee3c342cb0), which builds fine on a M1 host but I got errors about missing symbols when importing the tensorflow package in python3\r\n\r\nwith this [PR](https://github.com/tensorflow/tensorflow/pull/47594) the pip package can be built using a native build on M1 host machine. the only dependencies that need manual installation are `grpcio`/`h5py`", "@terryheo / @gbaned also thanks for reviewing [#47639](https://github.com/tensorflow/tensorflow/pull/47639).  this includes the changes on the follow-up PR we discussed:\r\n> once this [PR](https://github.com/google/XNNPACK/pull/1327) is accepted in [XNNPACK](https://github.com/google/XNNPACK) repo, we can then also proceed with a new PR so `TensorFlow Lite`and `benchmark_model`can be built for `macos_arm64`using:\r\n> \r\n> > bazel build --config=macos_arm64 -c opt //tensorflow/lite:libtensorflowlite.so\r\n> > bazel build --config=macos_arm64 -c opt //tensorflow/lite/tools/benchmark:benchmark_model\r\n\r\nthat being said, with [#47639](https://github.com/tensorflow/tensorflow/pull/47639) we can now also build using bazel for `macos_arm64` platform:\r\n\r\n**Cross compilation** on a M1 host or a different platform (e.g. Linux x86):\r\n> bazel build -c opt --config=macos_arm64 //tensorflow/lite:libtensorflowlite.so\r\n> bazel build -c opt --config=macos_arm64 //tensorflow/lite/tools/benchmark:benchmark_model\r\n\r\nand due to the additional changes [#47594](https://github.com/tensorflow/tensorflow/pull/47594) by @freedomtan (thanks!) when using a more recent version of bazel ([bazel #4928295](https://github.com/bazelbuild/bazel/commit/4928295b236ec8f590a7e9d863502bc2f50a77d9)):\r\n\r\n**M1 host machine compilation** (without specifying the platform):\r\n> bazel build -c opt //tensorflow/lite:libtensorflowlite.so\r\n> bazel build -c opt //tensorflow/lite/tools/benchmark:benchmark_model\r\n\r\nTo summarize: `TensorFlow Lite library` and `benchmark_model`, but also the full `TensorFlow pip package` can be built using `bazel`"]}, {"number": 47436, "title": "Distributed computing, extreme distribution latency ", "body": "tf 2.2\r\npython 3.7\r\n\r\nI'm trying to distribute a training step over multiple GPU and although I'm not shown any error and the memory seems to be correctly allocated to all the GPUs.\r\nI am experiencing extreme latency in the distribution and collection:\r\n```\r\n@tf.function\r\ndef distributed_train_step(dist_inputs):\r\n    per_replica_losses = mirrored_strategy.run(model._train_step, args=(dist_inputs,))\r\n    return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\r\n                                    axis=None)\r\ndist_dataset = mirrored_strategy.experimental_distribute_dataset(train_dataset.dataset)\r\nfor dist_input in dist_dataset:\r\n    output = distributed_train_step(dist_input)\r\n    print(f'\\r{output}')\r\n```\r\nSpecifically it seems that a lot of time is spent for this operation (~5 seconds)\r\n```\r\nmirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\r\n                                    axis=None)\r\n```\r\nas well as between calls of \r\n```\r\nmirrored_strategy.run(model._train_step, args=(dist_inputs,))\r\n```\r\nwhere I very rudimentally print \"inside train step\" at the beginning of model._train_step and I see a print every ~ 1s per GPU (sequentially), so ~3s in total.\r\nIn a single GPU settings this train step (with the same total batch size) is performed in less than a second. \r\n\r\nWhen starting training everything seems fine\r\n\r\n```\r\n2021-02-26 13:31:14.658769: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2021-02-26 13:31:14.714487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:3b:00.0 name: Quadro RTX 5000 computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 48 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-02-26 13:31:14.715401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:\r\npciBusID: 0000:af:00.0 name: Quadro RTX 5000 computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 48 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-02-26 13:31:14.715624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-02-26 13:31:14.717261: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2021-02-26 13:31:14.718644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2021-02-26 13:31:14.718874: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2021-02-26 13:31:14.720288: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2021-02-26 13:31:14.721085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2021-02-26 13:31:14.724158: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2021-02-26 13:31:14.727302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2021-02-26 13:31:14.727631: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\n2021-02-26 13:31:14.734379: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3200000000 Hz\r\n2021-02-26 13:31:14.736749: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56099e0aead0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-02-26 13:31:14.736805: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-02-26 13:31:14.933093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:3b:00.0 name: Quadro RTX 5000 computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 48 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-02-26 13:31:14.933919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:\r\npciBusID: 0000:af:00.0 name: Quadro RTX 5000 computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 48 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-02-26 13:31:14.933976: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-02-26 13:31:14.933988: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2021-02-26 13:31:14.933999: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2021-02-26 13:31:14.934009: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2021-02-26 13:31:14.934019: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2021-02-26 13:31:14.934029: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2021-02-26 13:31:14.934040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2021-02-26 13:31:14.937073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2021-02-26 13:31:14.937121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-02-26 13:31:14.938810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-02-26 13:31:14.938835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1\r\n2021-02-26 13:31:14.938840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y\r\n2021-02-26 13:31:14.938843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N\r\n2021-02-26 13:31:14.941865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15037 MB memory) -> physical GPU (device: 0, name: Quadro RTX 5000, pci bus id: 0000:3b:00.0, compute capability: 7.5)\r\n2021-02-26 13:31:14.943851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15037 MB memory) -> physical GPU (device: 1, name: Quadro RTX 5000, pci bus id: 0000:af:00.0, compute capability: 7.5)\r\n2021-02-26 13:31:14.945733: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56099d69c690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-02-26 13:31:14.945750: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 5000, Compute Capability 7.5\r\n2021-02-26 13:31:14.945754: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Quadro RTX 5000, Compute Capability 7.5\r\n2021-02-26 13:31:40.674717: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n```\r\nLoss is actually decreasing so backprop seems to work just fine.\r\n\r\nHere are the relevant code snippets:\r\n```\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\nwith mirrored_strategy.scope():\r\n    opt = tf.keras.optimizers.Adam(0.0001,\r\n                                   beta_1=0.9,\r\n                                   beta_2=0.98,\r\n                                   epsilon=1e-9)\r\n    model = config_manager.get_model()\r\n    model.loss_weights = [1., 1.]\r\n    model.compile(loss=[masked_mean_absolute_error,\r\n                        new_scaled_crossentropy(index=2, scaling=8)],\r\n                  loss_weights=model.loss_weights,\r\n                  optimizer=opt)\r\n```\r\nall the loss functions have a SUM  reduction strategy\r\n\r\nwhere train_dataset.dataset is this binned_data\r\n```\r\ndataset = tf.data.Dataset.from_generator(lambda: self._datagen(shuffle),\r\n                                                 output_types=output_types)\r\n\r\nbinned_data = dataset.apply(\r\n    tf.data.experimental.bucket_by_sequence_length(\r\n        len_function,\r\n        bucket_boundaries=bucket_boundaries,\r\n        bucket_batch_sizes=bucket_batch_sizes,\r\n        padded_shapes=padded_shapes,\r\n        drop_remainder=drop_remainder,\r\n        padding_values=padding_values\r\n    ))\r\n```\r\n\r\nand model._train_step is\r\n```\r\n    def _train_step(self, dist_inputs):\r\n        mel, phonemes, stop, sample_name = dist_inputs\r\n        tar_inp = mel[:, :-1]\r\n        tar_real = mel[:, 1:]\r\n        tar_stop_prob = stop[:, 1:]\r\n\r\n        mel_len = int(tf.shape(tar_inp)[1])\r\n        tar_mel = tar_inp[:, 0::self.r, :]\r\n\r\n        with tf.GradientTape() as tape:\r\n            model_out = self.__call__(inputs=phonemes,\r\n                                      targets=tar_mel,\r\n                                      training=True)\r\n            loss, loss_vals = weighted_sum_losses((tar_real,\r\n                                                   tar_stop_prob),\r\n                                                  (model_out['mel'][:, :mel_len, :],\r\n                                                   model_out['stop_prob'][:, :mel_len, :]),\r\n                                                  self.loss,\r\n                                                  self.loss_weights)\r\n            loss = loss / 64.\r\n        gradients = tape.gradient(loss, self.trainable_variables)\r\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\r\n        return loss\r\n```\r\n\r\nEverything seems according to the guide https://www.tensorflow.org/guide/distributed_training ", "comments": ["Removing @tf.function removes the latency.", "@cfrancesco,\r\nOn running the given code snippet, I am facing an error stating `NameError: name 'config_manager' is not defined`. Could you please provide the complete code to reproduce the issue on both single GPU and multiple GPUs?\r\n\r\nAlso, please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47435, "title": "Remove unnecessary model parsing from keras.model.load_model", "body": "This PR removes duplicated SavedModel parsing from `keras.model.load_model`. The result of parsing was never used and it is also not needed for error handling either since the same function will be called at the start of `saved_model_load.load`:\r\nhttps://github.com/tensorflow/tensorflow/blob/7e1c942100039dc0a9adda5d0d45c7e1d2e3b76c/tensorflow/python/keras/saving/saved_model/load.py#L124\r\n\r\nThis removes the need to read the file content and parse the saved model twice which can improve performance when loading models from a remote file system like GCS.", "comments": []}, {"number": 47434, "title": "Add link to presentation, minor fixes", "body": "Signed-off-by: Michael Gielda <mgielda@antmicro.com>\r\n\r\nAdding link to slides as proposed by TF Lite Micro team", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47433, "title": "Model.predict accepts tensors of incorrect rank", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab (Ubuntu 18.04)\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f 2.4.1\r\n- Python version: 3.7.10\r\n\r\n**Describe the current behavior**\r\n* When defining a model with `tf.keras.Input(shape=[1])`, `model.predict` should only accept a tensor of shape `[batch,1]`, but it incorrectly accepts a tensor of shape `[batch]` (and appears to reshape it to `[batch, 1]`).\r\n* When defining a model with `tf.keras.Input(shape=[2])`, `model.predict` should only accept a tensor of shape `[batch,2]`, but it incorrectly accepts a tensor of shape `[1, batch, 2]`. The erroneous extra dimension is retained in the value returned by `model.predict`.\r\n\r\n(These two bugs feel closely related, which is why I'm reporting them together. If not, I can file two separate issues.)\r\n\r\n**Describe the expected behavior**\r\nWhen defining a model with `tf.keras.Input(shape=[d1,d2,...,dn])`, `model.predict` should only accept a tensor of shape `[batch,d1,d2,...,dn]`. All other shapes should raise an exception.\r\n\r\n**Standalone code to reproduce the issue**\r\n[The issues are shown in this Colab notebook.](https://colab.research.google.com/drive/1qP7wa2b7-8Sb5Z5ODNEXWZeQ0cDc-ttY?usp=sharing)\r\n\r\n**Other info / logs**\r\nIf this is expected behavior, [it should be documented here](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict). It is not. And I hope it is not expected behavior.\r\n", "comments": ["@ymodak \r\nI ran the code shared on tf 2.4 and tf-nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/60f4f4d4b88be8b92afb54c1c58cfd79/untitled554.ipynb).", "@jameshfisher Thank you for the bug report and for the repro code. We took the other bug you submitted recently as a P1, but in this case, we believe the code behaves as intended. There is a relaxation in the rank and shape requirements when oen of the dimensions is trivially ineffective, or if the shapes can be trivially mapped. If you find a case where the code is clearly doing the wrong thing, please submit it again. For now, I'll close this as \"Behaves Correctly\".", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47433\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47433\">No</a>\n", "@deeb02 ok, thanks. Perhaps the docs could be updated to describe this behavior more precisely? I think most people will expect that specifying a `tf.keras.Input(shape=...)` will strictly enforce that shape."]}, {"number": 47432, "title": "Make libtensorflow_jni target compatible with Tensorflow Text", "body": "Export tensorflow symbols to solve global static variable lookup issue.\r\n\r\nFix #47431 ", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47432) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47432) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "@jhseu Can you please take a look on this PR ? Thanks!", "@jhseu Can you please take a look on this PR ? Thanks!", "@gharibian  Can you please take a look on this PR ? Thanks!", "libtensorflow_jni.so links against libtensorflow_framework.so, which exports all TF symbols. Does linking to libtensorflow_framework.so work for you?", "@liyinhgqw  Can you please check @allenlavoie's comments and keep us posted ? Thanks!", "This will not work if one uses `libtensorflow_jni`. The symbol does exist in both libtensorflow_framework.so and libtensorflow_text.so.\r\n```\r\nSymbol in TF\r\n$ readelf -sW org/tensorflow/native/linux-x86_64/libtensorflow_framework.so.2 | grep hash_bit | grep LookupInterface\r\n\r\n 42807: 0000000001cbcb6c     1 OBJECT  UNIQUE DEFAULT   30 _ZZN10tensorflow9TypeIndex4MakeINS_6lookup15LookupInterfaceEEES0_vE8hash_bit\r\n 62977: 0000000001cbcb6c     1 OBJECT  UNIQUE DEFAULT   30 _ZZN10tensorflow9TypeIndex4MakeINS_6lookup15LookupInterfaceEEES0_vE8hash_bit\r\n```\r\n```\r\nSymbol in TF Text\r\n$ readelf -sW tfplugins/linux/libtftextops.so | grep hash_bit  | grep LookupInterface\r\n  6015: 000000000049790a     1 OBJECT  UNIQUE DEFAULT   30 _ZZN10tensorflow9TypeIndex4MakeINS_6lookup15LookupInterfaceEEES0_vE8hash_bit\r\n  8799: 000000000049790a     1 OBJECT  UNIQUE DEFAULT   30 _ZZN10tensorflow9TypeIndex4MakeINS_6lookup15LookupInterfaceEEES0_vE8hash_bit\r\n```\r\n\r\nHowever, if it is not exposed by `libtensorflow_jni.so`, when TF JNI tries to use this variable, the variable will not be the same as the one TF Text refers to as TF Text does not know the symbol is already stored in `libtensorflow_jni.so`. They can only share the same variable of the same address if both expose the symbol.\r\n", "Ah, it's not a missing symbol issue but an issue of a duplicated definition of LookupInterface. I don't see why libtensorflow_jni.so exposing the symbol is a good solution; typically we'd try to remove the second copy of the type, having it rely on a header instead.\r\n\r\nIn this case it seems like the duplication is from the wordpiece kernel including a second copy of LookupInterface. And I'm guessing libtensorflow_jni.so exporting the symbol puts it into the global symbol table, which would certainly override it in any plugins. But generally just linking against libtensorflow_framework.so should have it use that symbol anyway; is libtftextops.so linked against libtensorflow_framework.so? (ldd should tell you)", "It does link against `libtensorflow_framework.so` and by looking at the implementation https://github.com/tensorflow/text/blob/master/tensorflow_text/core/kernels/wordpiece_kernel.cc#L21\r\nIt seems that it includes the `tensorflow/core/framework/lookup_interface.h` without having a duplication.\r\n\r\n```\r\nldd linux/libtftextops.so\r\nldd: warning: you do not have execution permission for `linux/libtftextops.so'\r\n\tlinux-vdso.so.1 =>  (0x00007ffce9cf6000)\r\n\tlibstdc++.so.6 => /lib64/libstdc++.so.6 (0x00007fbfa0d8d000)\r\n\tlibm.so.6 => /lib64/libm.so.6 (0x00007fbfa0a8b000)\r\n\tlibtensorflow_framework.so.2 => not found\r\n\tlibpthread.so.0 => /lib64/libpthread.so.0 (0x00007fbfa086f000)\r\n\tlibdl.so.2 => /lib64/libdl.so.2 (0x00007fbfa066b000)\r\n\tlibgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007fbfa0455000)\r\n\tlibc.so.6 => /lib64/libc.so.6 (0x00007fbfa0087000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007fbfa152e000)\r\n```", "So the kernel imports the header, and the header includes a definition of the class, so the resource manager sees these as different types. That seems like an issue with how resources are managed in custom ops (CC @wangpengmit in case he has ideas).\r\n\r\nI don't understand why exporting from libtensorflow_jni.so overrides the symbol while linking against libtensorflow_framework.so doesn't. Do you know why that is?", "Not sure about it, but is it related to the dlopen flag (not `RTLD_GLOBAL`?) that java `loadLibrary` method uses?", "Maybe what's happening in Python is that _pywrap_tensorflow_internal.so is doing the loading, so even though its symbols aren't in the global symbol table (Python does not use RTLD_GLOBAL either) it still overrides this symbol for kernels it loads. And again in Java since libtensorflow_jni.so is doing the loading the thing being loaded prefers its symbols even if they aren't in the global table.\r\n\r\nCan you check if that's really what's happening, e.g. with `LD_DEBUG=bindings`? i.e. that after your change the kernel .so picks up the symbol from libtensorflow_jni.so even though its symbols aren't in the global table.\r\n\r\nIf so I'd leave a comment explaining why these symbols are exposed. I'd also file a bug explaining that we have three different copies of LookupInterface floating around causing resource ID lookup errors (custom op/kernel .so, language bindings like libtensorflow_jni.so and _pywrap_tensorflow_internal.so, and libtensorflow_framework.so) and then reference that bug in a TODO to remove the symbols from libtensorflow_jni.so. The way this is \"working\" right now seems extremely fragile.\r\n\r\nAnd do you know why https://github.com/tensorflow/java/issues/82 was fixed for some folks but you're still having issues?", "Sounds a good plan. I am closing it for now.\r\n\r\n[tensorflow/java#82 ](https://github.com/tensorflow/java/issues/82) solved this issue by linking against `tensorflow_cc` target which exposes almost all the [symbols](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tf_version_script.lds), while I am using the java library in the main tensorflow repo where it depends on `tensorflow_jni` target that does not expose the [symbol](https://github.com/tensorflow/tensorflow/pull/47432/commits/336957727a81b9cd4fbac48fd9673fef1b12dce9).\r\n\r\nI believe many people have migrated to `tensorflow/java` instead, while we have not done that for legacy issues."]}, {"number": 47431, "title": "Failed to run models with libtensorflow_jni and tensorflow text together", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.1\r\n- Python version:  n/a\r\n- Installed using virtualenv? pip? conda?: n/a\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): gcc7\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nWhen using [libtensorflow_jni.so](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/BUILD#L437) with [tensorflow text](https://github.com/tensorflow/text), it crashes.\r\n\r\nSame issue: https://github.com/tensorflow/java/issues/82\r\n\r\n**Any other info / logs**\r\n```\r\n2021-02-17 19:20:45.283001: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at wordpiece_kernel.cc:204 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorflow6lookup15LookupInterfaceE got N10tensorflow6lookup15LookupInterfaceE\r\n```", "comments": ["@liyinhgqw,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.\r\n\r\nAlso, please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same error. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47431\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47431\">No</a>\n"]}, {"number": 47430, "title": "Cannot import name 'boosted_trees_test' from 'tensorflow_estimator.python.estimator.canned' ", "body": "When I am trying to import boosted_trees_test, I face the following error:\r\n\r\n\r\n`cannot import name 'boosted_trees_test' from 'tensorflow_estimator.python.estimator.canned' (C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\__init__.py)\r\n`\r\n\r\n\r\nAlso, I could not find the boosted_trees_test in the latest TF package (2.4)", "comments": ["@samanemami,\r\nLooks like you're trying to run code meant for TF 1.x on TF 2.4. The last TF version with a reference to [boosted_trees_test.py](https://github.com/tensorflow/tensorflow/blob/v1.12.3/tensorflow/python/estimator/canned/boosted_trees_test.py) was v1.12.3\r\n\r\nCould you please provide the Python script/notebook you're running, so that we can look into this issue? Thanks!", "Thank you for the reply\r\n\r\nThe code is as follows, which I tried to import a model:\r\n\r\n`from tensorflow_estimator.python.estimator.canned import boosted_trees_test`\r\n\r\nThe model which I am interested to use is as follow;\r\n[boosted_trees_test](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees_test.py#L403)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47430\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47430\">No</a>\n", "@ymodak\r\nI have installed TF 1.15.0, but the problem has not solved. Got the same error.\r\n\r\nPlease note that I have downloaded **different versions of TF** and consequently installed different versions of python.", "@samanemami  Importing `boosted_trees` test script fails since these files (*_test.py) are not exposed to your python session.\r\nSimply importing boosted_trees module should work with TF 2.4 version.\r\n`from tensorflow_estimator.python.estimator.canned import boosted_trees`\r\nFor running test files you need to try [Bazel command](from tensorflow_estimator.python.estimator.canned import boosted_trees).\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47430\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47430\">No</a>\n"]}, {"number": 47429, "title": "Simplify Huber Loss implementation", "body": "This PR simplifies the Huber Loss implementation and also updates the docstring to use a more readable formulation matching the one on Wikipedia.\r\n\r\nThis PR does not change the mathematics, but removes the need for one operation in the code path that cannot be constant folded away which should improve performance.", "comments": ["I'm wondering if it makes sense to use `math_ops.square(error)` instead of `math_ops.pow(error, 2)`? (Similarly for `math_ops.pow(delta, 2)`).", "> I'm wondering if it makes sense to use math_ops.square(error) instead of math_ops.pow(error, 2)\r\n\r\nGood idea. If I recall correctly, grappler will automatically apply this simplification, but it's good to do it already here."]}, {"number": 47428, "title": "Normalization layer supresses ValueError when model is called with bad input shape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab notebook, and everywhere else tested\r\n- TensorFlow installed from (source or binary):  Colab notebook, and everywhere else tested\r\n- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f 2.4.1\r\n- Python version: 3.7.10\r\n\r\n**Describe the current behavior**\r\nAfter adding a `tf.keras.layers.experimental.preprocessing.Normalization` layer to a model, badly shaped input is accepted by the model's `.predict`, generating only a warning log line, and returning some nonsense output.\r\n\r\n**Describe the expected behavior**\r\nAfter adding a `tf.keras.layers.experimental.preprocessing.Normalization` layer to a model, badly shaped input should be rejected by the model's `.predict`, raising a `ValueError`.\r\n\r\n**Standalone code to reproduce the issue**\r\n[Here is a Colab notebook that shows the issue](https://colab.research.google.com/drive/1ud_9UtZaTwmlL6WTnJkWMXG162DJ57kr?usp=sharing)", "comments": ["@jameshfisher \r\nI ran the code shared in colab but do not face same error reported, please have a look at the [gist here](https://colab.research.google.com/gist/Saduf2019/e61628339732c736514c00a900143689/untitled554.ipynb).", "@Saduf2019 your gist shows the same problem. See the final code cell in your gist. `model_with_normalization_layer.predict(bad_input)` only produces a `WARNING` log line, and returns a nonsense value of shape `(5,1)`. But what it _should_ do is raise a `ValueError`, and not return any value, because its input has shape `(5,)`, which does not match the shape enforced by its first layer, `tf.keras.Input(shape=(3,))`. This is the normal behavior when a `Normalization` layer is not present (see the line `model_without_normalization_layer.predict(bad_input)` for reference); adding a `Normalization` layer should not change this behavior.", "@jvishnuvardhan \r\nI ran the code on nightly as well, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/a80eb59cba101df53317a6b74a8bf35d/untitled555.ipynb).", "I'd like to work on this.", "Thank you for submitting this. It looks like a serious bug. We've raised it to P1.", "Was able to reproduce this issue using TF v2.5 ,please check the gist [here](https://colab.research.google.com/gist/sushreebarsa/2c8bd3d92956c4b944be384cf95ba4a9/untitled141.ipynb?authuser=1)..Thanks !", "Was able to replicate the issue with TF 2.6.0-dev20210606,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/b3af00c5c5b2dfa4fa6f34668634dba9/untitled168.ipynb?authuser=1) ..Thanks!", "@jameshfisher,\r\nI have tried in Colab with Tensorflow 2.8 and i am seeing `InvalidArgumentError` with `model_with_normalization_layer.predict(bad_input)`. Please, find the gist [here](https://colab.research.google.com/gist/chunduriv/badca4e6aac1b0028109bf628852ecc8/47428.ipynb) and confirm if this issue is resolved for you.Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47428\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47428\">No</a>\n"]}, {"number": 47427, "title": "Pass for Dialect conversion from Tflite to TF  ", "body": "Does there exists a Pass that can take back for Dialect conversion from Tflite to TF, TFlite->TF\r\nWe need something which preserves the quantization info generated during Tflite and detours back to TF where we can lower it down to HLO and Linalg thereon.\r\nWe don't need a new Dialect but just a Pass that can visit the Tflite IR and reconvert it back to TF getting back the Quantization info.\r\n                CustomPass\r\nTF->TFlite->TF->HLO->Linalg\r\nWhat would be the other available ways to achieve this.", "comments": ["Thanks for filing a feature request. We do not have such feature yet.", "Thanks @abattery so as of now what are the alternatives we can have for this I mean to say how do we go about if we need to get it to happen.", "See also https://google.github.io/iree/design-roadmap#quantization They have a long term plan for supporting quantization. I don't know good answers on the possible alternatives for the short terms. CCing @liufengdb and @jianlijianli ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]