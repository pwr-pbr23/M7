[{"number": 47522, "title": "Model Accuracy Goes Static When Arbitrary Lines Are Added", "body": "**System information**\r\n- I have written custom code, repo link below.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10 Home, build 19041.804**\r\n- TensorFlow installed from (source or binary): source, **Anaconda3**\r\n- TensorFlow version (use command below): **2.1.0**\r\n- Python version: **3.7.7**\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: EVGA Geforce GTX 1080 SC, 8GB\r\n\r\n**Describe the current behavior**\r\nAdding arbitrary comment and print lines above the model training function causes accuracy to become static and not improve past 0.5024.  Removing the lines allows it to train correctly.  \r\n\r\nPlease see these screenshots for a better explanation:\r\nhttps://imgur.com/a/weV5mgL\r\n\r\n**Describe the expected behavior**\r\nModel should train correctly regardless of arbitrary lines.\r\n\r\n**Standalone code to reproduce the issue**\r\nrepo: https://github.com/Cnotey/TF_Issue_003\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nI have a hard time believing this is a TF issue, or an issue with my code.  It seems like this is a compatibility issue somehow.\r\n", "comments": ["@Cnotey \r\nThe code provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily. \r\nOr share a colab gist with the error reported.Thanks!\r\n", "@Saduf2019 \r\nHi Saduf. I already spent about 30 hours cutting out 1 000 lines of intellectual property to even be able to share the repo. This is the simplest form that will function.\r\n\r\nI have not used Google Colab before but could try it. I would have ignored the issue, but it's just so weird and has persisted for nearly a year and is preventing further progress on the project.", "@ymodak   Thanks for keeping this one going.  Still seems like a bug to me rather than a support request.  I'm here to answer any further questions about the issue.", "@ymodak I have done some further investigation into the matter.  I am also able to mitigate the issue by changing the random seed in TF.  Is it possible that the random seed is somehow related to the number of lines in the program?", "@Cnotey,\r\n\r\nI've checked adding arbitrary lines with a sample keras model, I am not able to reproduce the issue and accuracy is varying for each epochs as expected. You can take a look at the [gist here](https://colab.sandbox.google.com/gist/sanatmpa1/f658cec725ad9a07af652a3f517166cb/mnist_convnet.ipynb). Can you update your tensorflow to latest stable version i.e `2.7.0` and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47522\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47522\">No</a>\n"]}, {"number": 47521, "title": "Fixes Github #47520 Step number of batch_steps_per_second ", "body": "", "comments": ["@brychcy Can you please address Ubuntu Sanity errors? Thanks!", "> @brychcy Can you please address Ubuntu Sanity errors? Thanks!\r\n\r\nI'll have a look.\r\n\r\nProblem is:\r\n```\r\nFound 1 non-allowlisted pylint errors:\r\ntensorflow/python/keras/callbacks.py:2448: [C0301(line-too-long), ] Line too long (97/80)`\r\n```", "@gbaned, Fix is pushed. \r\nI guess I cannot retrigger those checks myself, right?", "@gbaned, my updated patch just added a linebreak to the patch already approved by @fchollet.\r\ndo you really think he needs to approve it again?", "> @gbaned, my updated patch just added a linebreak to the patch already approved by @fchollet.\r\n> do you really think he needs to approve it again?\r\n\r\n@brychcy  There are commits after approved by @fchollet. Hence it requires approval again. Thanks!\r\n"]}, {"number": 47520, "title": "TensorBoard callback: step is always 0 for batch_steps_per_second", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.2.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-51814-gb08c055e840 2.5.0-dev20210301\r\n- Python version: 3.8.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nThe current tf-nightly (for 2.5.0) has  a new parameter  `write_steps_per_second` to tf.keras.callbacks.TensorBoard.\r\nIf that is used with `update_freq` > 0, the resulting batch_steps_per_second all have `0` as step value, which makes the graph unreadable in tensorboard.\r\n\r\n**Describe the expected behavior**\r\nThe step number is the corresponding training step\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport shutil\r\nimport glob\r\nimport tensorflow as tf\r\nfrom tensorflow.core.util import event_pb2\r\n\r\nworkdir = 'tmp_workdir/'\r\nshutil.rmtree(workdir, ignore_errors=True)\r\n\r\nx = [[0, 0], [0, 1], [1, 0], [1, 1]]\r\ny = [0, 1, 1, 0]\r\n\r\ninputs = tf.keras.Input(shape=(2,))\r\noutputs = tf.keras.layers.Dense(units=1, activation='sigmoid')(inputs)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError())\r\n\r\nmodel.fit(x=x, y=y, batch_size=2, epochs=2, callbacks=[\r\n    tf.keras.callbacks.TensorBoard(log_dir=workdir, write_steps_per_second=True, update_freq=1,\r\n                            profile_batch=0)])\r\n\r\nmax_step = 0\r\nfor record in tf.data.TFRecordDataset(glob.glob(workdir + 'train/events.out*')[0]):\r\n    event = event_pb2.Event.FromString(record.numpy())\r\n    if event.HasField('summary') and event.summary.value[0].tag == 'batch_steps_per_second':\r\n        max_step = max(max_step, event.step)\r\n\r\nif max_step == 0:\r\n    print(\"FAILURE\")\r\nelse:\r\n    print(\"OK\")\r\n```\r\n\r\n**Other info / logs** \r\nThe step number 0 comes from passing `step.value()` instead of `step` in _push_writer. There is a TODO saying `# TODO(b/151339474): Fix deadlock when not using .value() here.` above that line - I suppose the details are in some internal google bug database.\r\n\r\nBut as the `batch_steps_per_second` event is only written during training, this can easily be fixed by explicitly passing  self._train_step  as step when writing it.\r\n\r\nI'll add a pull request.", "comments": ["PR is #47521 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47520\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47520\">No</a>\n"]}, {"number": 47519, "title": "Fix API compatibility test under Python 3.9", "body": "Thread.isAlive removed in Py 3.9: https://bugs.python.org/issue37804\r\n\r\nThanks @mdanatg for the pointer.\r\n\r\nSee https://github.com/tensorflow/tensorflow/issues/44485#issuecomment-789360728 for the context.", "comments": []}, {"number": 47518, "title": "Change cudnn version macros for RNN APIs", "body": "The new CUDNN RNN v8 APIs should be with CuDNN v8.0.2 as noted in : https://docs.nvidia.com/deeplearning/cudnn/api/index.html#release-802\r\n\r\nfyi. @nluehr @benbarsdell ", "comments": ["@sanjoy for visibility.", "Does this mean if we build against cuDNN 8.0.2 but run try to run with 8.0.0 we'll fail at runtime?  If yes, users might find it confusing: IIUC 8.0.x is generally forwards and backwards with 8.0.y (where x != y).  Maybe make the code conditional on cuDNN >= 8.1?", "I am actually sync'ing with our cudnn team about when these APIs were added, since I noticed that they also appeared in the v8.0.0: https://docs.nvidia.com/deeplearning/cudnn/api/index.html#release-800-preview."]}, {"number": 47517, "title": "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.", "body": "**System information**\r\n- windows 10\r\n- TensorFlow installed using pip\r\n- tensorflow-2.0.0\r\n- Python 3.6.2\r\n\r\nHey, I've been trying to use [this](https://www.youtube.com/watch?v=wypVcNIH6D4) tutorial to get a simple chatbot running but am running into this issue when I try compiling main.py file:\r\n\r\n`Traceback (most recent call last):\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 6, in <module>\r\n    import tflearn\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tflearn\\__init__.py\", line 4, in <module>\r\n    import tensorflow.compat.v1 as tf\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Micha\\anaconda3\\envs\\chatbot\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.`\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "Hooray! I fixed it by doing this: https://github.com/tensorflow/tensorflow/issues/23683#issuecomment-510718458\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47517\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47517\">No</a>\n"]}, {"number": 47515, "title": " Add hello_world example to the output tree (project generation v2)", "body": "This is a first-pass implementation of what exporting the example code would look like. In particular:\r\n * We will only export the x86 versions of the examples.\r\n * Any target specific specializations or additional sources will be handled outside of the TFLM codebase (i.e. in the target's example repository).\r\n\r\nProgress towards: #47413", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@njeffrie This PR is ready for review."]}, {"number": 47514, "title": "[Cherrypick:r2.4] Add TPU embedding profile data directory to both TPU estimator and TPU embedding", "body": "", "comments": []}, {"number": 47513, "title": "Missing documentation for bucket_by_sequence_length api", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/data/experimental/ops/grouping.py\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\nI think `bucket_by_sequence_length` is a very important api, but it's lack of documentation example makes it usage quiet restricted to some users.\r\n### Clear description\r\nThis API is very useful in NLP where we can batch the data efficiently keeping in mind the memory constraints that is involved. \r\nThis API can pad the data based on the longest sequence in that particular batch in place of padding the complete dataset with a constant number.\r\nFor example, why should someone use this method? How is it useful?\r\nAs I said above it can be used for large dataset as well as small as it will reduce the padding of the dataset.\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/data/experimental/ops/grouping.py\r\n### Parameters defined\r\nYes\r\nAre all parameters defined and formatted correctly?\r\nYes\r\n### Returns defined\r\nYes\r\nAre return values defined?\r\nYes\r\n### Raises listed and defined\r\nYes\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\nYes\r\n### Usage example\r\nThere is only one example in the nightly build that doesn't involve all the parameter such as `padded_shapes , padding_values, pad_to_bucket_boundary, no_padding, drop_remainder`. Basically it doesn't involve a close to real scenario.\r\nIs there a usage example?\r\nYes, there is one example.\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\nI will be willing to contribute and I can have the PR ready within a day, as I use this API, if the other contributors agree to it. If you accept this issue, you can assign it to me.\r\n\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["Can I work on it and provide a PR @ymodak ?", "I have completed working on the PR and it is ready for the review @ymodak . The pull request is named \"Update docs for bucket_by_sequence_length api.\".", "Thanks for the PR. ", "The pull request has been merged. This issue can be closed?", "@PratsBhatt Yes, your [PR](https://www.tensorflow.org/api_docs/python/tf/data/experimental/bucket_by_sequence_length?version=nightly) is up with tf-nightly docs. Thanks for the fix."]}, {"number": 47512, "title": "Fix AbcTest.testAbstract assertion", "body": "Fixes the following test failure:\r\n```\r\n======================================================================\r\nFAIL: testAbstract (__main__.AbcTest)\r\ntestAbstract (__main__.AbcTest)\r\n----------------------------------------------------------------------\r\nTypeError: Can't instantiate abstract class AbstractModule with abstract method __call__\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/buildfarm/default/operations/52eafc82-3509-47a0-ad8a-d565ef4de86b/bazel-out/k8-opt/bin/tensorflow/python/module/module_test.runfiles/org_tensorflow/tensorflow/python/module/module_test.py\", line 340, in testAbstract\r\n    AbstractModule()  # pylint: disable=abstract-class-instantiated\r\nAssertionError: \"Can't instantiate .* abstract methods\" does not match \"Can't instantiate abstract class AbstractModule with abstract method __call__\"\r\n\r\n----------------------------------------------------------------------\r\n```", "comments": ["@park-junha can you please check sanity build failures ?", "> @park-junha can you please check sanity build failures ?\r\n\r\nThey look like transient failures. Mind to kick off CI again?"]}, {"number": 47511, "title": "[INTEL MKL] Fix BF16 regression - failures with unit tests and BF16 model execution. ", "body": "The PR completes MKL build option cleanup  (related to the merged PR: https://github.com/tensorflow/tensorflow/pull/47375).\r\n\r\n1. It will fix a few MKL unit test failures related to BFLOAT16)\r\n2. It will fix execution errors of a few BFLOAT16 models. ", "comments": ["@penpornk Thank you for the code review and approval.", "@gbaned There are 3 CI checks not passed. Please let me know if there is any need on my side. Thanks!\r\n\r\nBTW, I need this PR merged before submitting another single-binary PR (TF with oneDNN as default in Stock TF). "]}, {"number": 47510, "title": "[TFL] Enhance MoveBinaryOpBeforeReshape pattern to enable fusion of 2D bias.", "body": "Some uses of EinsumDense layer will create 2D or higher bias. For example,\r\n\r\n```python\r\nlayer = tf.keras.layers.MultiHeadAttention(num_heads=3, key_dim=5)\r\ntarget = tf.keras.Input(shape=[8, 16], batch_size=1)\r\nsource = tf.keras.Input(shape=[4, 16], batch_size=1)\r\noutput_tensor = layer(target, source, return_attention_scores=False)\r\nmodel = tf.keras.Model([target, source], output_tensor)\r\n```\r\n\r\nThis PR reorders Reshape and BinaryOp to make 2D bias flatten and fusable to FullyConnected. In the case above, bias add can be fused into FullyConnected at location 0, 2 and 172.", "comments": ["Thanks for the contributions!", "Please add additional checks in the new patterns to make sure that the newly generated binary ops will have <=4D inputs since TFLite binary kernels support broadcasting up to 4D inputs.", "Also it would be nice to add a test case for >4D input cases.", "Also can we limit the reorderings only when the FullyConnected op is appeared next to it?", "> Also can we limit the reorderings only when the FullyConnected op is appeared next to it?\r\n\r\nDo you mean this pattern only, or all other reordering pattern?", "> Do you mean this pattern only, or all other reordering pattern?\r\n\r\nIt is okay for the newly added patterns only for now.", "> Also it would be nice to add a test case for >4D input cases.\r\n\r\nBecause we are restricting input to be defined by FullyConnected, which usually outputs 2D, I cannot really find a use case with 4D input. I do include one test about high-D input in e26e2a7. Let me know if it can pass all internal checks. Thank you!"]}, {"number": 47509, "title": "Scores are always inverse of each other in person_detection example with OV7670 Camera", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Version: 20H2, OS Build: 19042.804\r\n- TensorFlow installed from (source or binary): Arduino IDE 1.8.13 Library Manager\r\n- Tensorflow version (commit SHA if source): 2.4.0-ALPHA (Arduino IDE 1.8.13 Library Manager)\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arduino Nano 33 BLE Sense\r\n\r\n**Describe the problem**\r\nWhen using the person_detection example with the OV7670 camera (not from ArduCAM) and the edited image provider below, person_score and no_person_score are always inverse of each other. For example, 1 and -1, -8 and 8, etc. Inference does not fail.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n- Made a copy of person_detection example from Arduino_TensorFlowLite library\r\n- Deleted all SparkFun Edge and HM01B0 related files\r\n- Edited arduino_image_provider.cpp to the following for use with OV7670\r\n- No other files were edited\r\n\r\n```\r\n/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\r\n\r\n  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n  you may not use this file except in compliance with the License.\r\n  You may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n  Unless required by applicable law or agreed to in writing, software\r\n  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n  See the License for the specific language governing permissions and\r\n  limitations under the License.\r\n  ==============================================================================*/\r\n\r\n#include \"image_provider.h\"\r\n\r\n#if defined(ARDUINO) && !defined(ARDUINO_ARDUINO_NANO33BLE)\r\n#define ARDUINO_EXCLUDE_CODE\r\n#endif  // defined(ARDUINO) && !defined(ARDUINO_ARDUINO_NANO33BLE)\r\n\r\n#ifndef ARDUINO_EXCLUDE_CODE\r\n\r\n//Ov767X Library\r\n#include <Arduino_OV767X.h>\r\n\r\n#define CAMERA_WIDTH 160\r\n#define CAMERA_HEIGHT 120\r\n\r\nint bytesPerFrame;\r\n\r\nbyte data[CAMERA_WIDTH * CAMERA_HEIGHT]; //unsigned\r\n//int8_t data[CAMERA_WIDTH * CAMERA_HEIGHT]; //signed\r\n\r\n#define START_Y 0\r\n#define END_Y 96\r\n#define START_X 0\r\n#define END_X 96\r\n\r\n//Get the camera module ready\r\nTfLiteStatus InitCamera(tflite::ErrorReporter* error_reporter) {\r\n  TF_LITE_REPORT_ERROR(error_reporter, \"Attempting to start 0V767X\");\r\n  if (!Camera.begin(QQVGA, GRAYSCALE, 5)) {\r\n    TF_LITE_REPORT_ERROR(error_reporter, \"Failed to initialize OV767X\");\r\n    delay(1000);\r\n    return kTfLiteError;\r\n  }\r\n  bytesPerFrame = Camera.width() * Camera.height() * Camera.bytesPerPixel();\r\n  delay(100);\r\n  return kTfLiteOk;\r\n}\r\n\r\n//Begin the capture and wait for it to finish\r\nTfLiteStatus PerformCapture(tflite::ErrorReporter* error_reporter) {\r\n  TF_LITE_REPORT_ERROR(error_reporter, \"Starting capture\");\r\n  // Start capture\r\n  Camera.readFrame(data);\r\n  TF_LITE_REPORT_ERROR(error_reporter, \"Frame read\");\r\n  delay(50);\r\n  return kTfLiteOk;\r\n}\r\n\r\nTfLiteStatus ReadData(tflite::ErrorReporter* error_reporter) {\r\n  //  TF_LITE_REPORT_ERROR(error_reporter, \"%d bytes per frame from OV767X\", bytesPerFrame);\r\n  //  delayMicroseconds(15);\r\n  //  TF_LITE_REPORT_ERROR(error_reporter, \"Exiting read data function\");\r\n  return kTfLiteOk;\r\n}\r\n\r\n//Crop frame\r\nTfLiteStatus DecodeAndProcessImage(tflite::ErrorReporter* error_reporter,\r\n                                   int image_width, int image_height,\r\n                                   int8_t* image_data) {\r\n  TF_LITE_REPORT_ERROR(error_reporter, \"Cropping image\");\r\n\r\n  //Crop image\r\n  uint32_t index = 0;\r\n  for (int y = START_Y; y < END_Y; y++) {\r\n    for (int x = START_X; x < END_X; x++) {\r\n      image_data[index++] = static_cast<int8_t>(data[(y * image_width) + x] - 128); //Unsigned to signed conversion\r\n    }\r\n  }\r\n\r\n  TF_LITE_REPORT_ERROR(error_reporter, \"Image processed\");\r\n  return kTfLiteOk;\r\n}\r\n\r\n// Get an image from the camera module\r\nTfLiteStatus GetImage(tflite::ErrorReporter* error_reporter, int image_width,\r\n                      int image_height, int channels, int8_t* image_data) {\r\n  static bool g_is_camera_initialized = false;\r\n  if (!g_is_camera_initialized) {\r\n    TfLiteStatus init_status = InitCamera(error_reporter);\r\n    if (init_status != kTfLiteOk) {\r\n      TF_LITE_REPORT_ERROR(error_reporter, \"InitCamera failed\");\r\n      return init_status;\r\n    }\r\n    g_is_camera_initialized = true;\r\n  }\r\n\r\n  TfLiteStatus capture_status = PerformCapture(error_reporter);\r\n  if (capture_status != kTfLiteOk) {\r\n    TF_LITE_REPORT_ERROR(error_reporter, \"PerformCapture failed\");\r\n    return capture_status;\r\n  }\r\n\r\n  TfLiteStatus read_data_status = ReadData(error_reporter);\r\n  if (read_data_status != kTfLiteOk) {\r\n    TF_LITE_REPORT_ERROR(error_reporter, \"ReadData failed\");\r\n    return read_data_status;\r\n  }\r\n\r\n  TfLiteStatus decode_status = DecodeAndProcessImage(\r\n                                 error_reporter, image_width, image_height, image_data);\r\n  if (decode_status != kTfLiteOk) {\r\n    TF_LITE_REPORT_ERROR(error_reporter, \"DecodeAndProcessImage failed\");\r\n    return decode_status;\r\n  }\r\n\r\n  return kTfLiteOk;\r\n}\r\n\r\n#endif  // ARDUINO_EXCLUDE_CODE\r\n```\r\nDid I write the image provider incorrectly, or is this an issue with the original example as well?", "comments": ["I don't believe that you are doing anything wrong, that is the expected behavior. Tagging @njeffrie for confirmation.", "This is expected. It's a function of using a softmax activation on the classifier outputs, along with a symmetric quantized output tensor. Post-softmax, the scores always sum to 1.0, meaning post quantization they always sum to 0.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47509\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47509\">No</a>\n", "Does this mean that the farther away the scores are from 0, the more confident the model is? For example, if two person_scores were 100 and 50, would the model be more confident that the image showed a person on the first score?", "That is correct."]}, {"number": 47508, "title": "[ROCm] Enabling device tracing/profling support for AMD GPUs", "body": "/cc @cheshire @chsigg ", "comments": ["@cheshire @chsigg gentle ping", "@cheshire @chsigg @gbaned gentle ping", "@kkimdev @rohan100jain gentle ping", "@cheshire @chsigg @gbaned gentle ping", "@kkimdev @rohan100jain @cheshire @chsigg @gbaned  gentle ping", "@kkimdev @rohan100jain @cheshire @chsigg @gbaned gentle ping\r\n\r\nthe goal is to have this PR merged before we branch for TF 2.5, and hence the urgency .... please review.", "Updated reviewers based on the files changed.  Reviewers: please redirect if needed.", "@jbaiocchi @timshen91 gentle ping", "@timshen91 please re-review", "@cheshire @chsigg @jbaiocchi gentle ping", "Hi @deven-amd ! Sorry for the late reply. I just wanted to let you know that I am aware of this case and that this will take another 2 weeks or so. This is because there is a related RFC coming for adding pluggable profiler to TensorFlow, and you may be interested in that approach instead.", "@yisitu thanks for the response.\r\n\r\nI appreciate the heads-up on the forth coming RFC...can you provide me a pointer to it.\r\n\r\nIn the meantime can we review and approve this quicker? I really want this functionality in TF 2.5, and would like to get this functionality merged in `master` so that it can be cherry-picked into the `r2.5` branch.\r\n\r\nMost of the changes in this PR  (all if you exclude the file renaming for cuda) are ROCm specific and should not have any impact on existing functionality.\r\n\r\nw.r.t the new profiler add-on, we will be more than happy to update our implementation to use it, when it becomes available.\r\n\r\nthanks", "@yisitu I have pushed out a commit with the changes you requested. please approve. thanks", "@deven-amd  I am unable to test the the functionality on AMD GPUs so you'll have to do that for us.\r\nSince most of the code is in device_tracer_rocm I think the PR is relatively low risk.\r\n\r\nI just made some comments regarding styles, but beyond that I think this PR should be good to go.\r\n\r\nPing @chsigg for any last comments before we stamp this.", "@yisitu I have addressed most of your feedback and rebased PR to resolve merge conflicts.\r\n\r\nplease re-review / re-approve.\r\n\r\n@chsigg gentle ping", "> Hi, I am aware of the intention to pull this into TF 2.5. The quickest way forward is to apply the suggested review comments. Most of these are straightforward refactors. For example, the wrong copyright year might seem inconsequential but we do try to do our part to not raise tech debt. Please unresolve the earlier review feedback that have not been addressed.\r\n\r\n@yisitu , m guessing you were reviewing an outdated version of the changes when you left this comment. \r\nI did update the copyright year in my previous commits (https://github.com/tensorflow/tensorflow/pull/47508/commits/8625366d5b0d8a93257f48ea177914e5296b14d4) , and only marked as resolved, all the comments that I did address in my previous commits (https://github.com/tensorflow/tensorflow/pull/47508/commits/090e71f42875d09dc51dea6a7b4fbe2e47d27885, https://github.com/tensorflow/tensorflow/pull/47508/commits/8625366d5b0d8a93257f48ea177914e5296b14d4).", "@yisitu I have addressed all of your PR feedback. please re-review.\r\n\r\n@chsigg gentle ping", "@yisitu @chsigg gentle ping", "@deven-amd \r\n\r\n12:20 PM\r\nFrom Presubmit:Lint:\r\n> CheckLint/0: Warning (//depot/google3/third_party/tensorflow/core/profiler/METADATA:5:3)\r\n> CheckLint found errors.\r\n> <chrono> is an unapproved C++11 header.  [build/c++11] [5]\r\n> \t//depot/google3/third_party/tensorflow/core/profiler/internal/gpu/rocm_tracer.cc:18\r\n> <thread> is an unapproved C++11 header.  [build/c++11] [5]\r\n> \t//depot/google3/third_party/tensorflow/core/profiler/internal/gpu/rocm_tracer.cc:21\r\n> Redundant blank line at the end of a code block should be deleted.  [whitespace/blank_line] [3]\r\n> \t//depot/google3/third_party/tensorflow/core/profiler/internal/gpu/device_tracer_rocm.cc:436\r\n> Tab found; better to use spaces  [whitespace/tab] [1]\r\n> \t//depot/google3/third_party/tensorflow/core/profiler/internal/gpu/device_tracer_rocm.cc:722\r\n\r\nThese are blocking the submit. Please fix. \r\nPlease replace <thread> and <chrono> with time_utils.h\r\nhttps://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/profiler/utils/time_utils.h\r\n", "@yisitu I have made the changes you have suggested. \r\n\r\nPlease re-approve and re-submit.\r\n\r\nthanks again\r\n\r\ndeven", "@gbaned @yisitu PR seems to have gotten stuck in the merge pipeline...anything I can do on my end to move it along? thanks", "> @gbaned @yisitu PR seems to have gotten stuck in the merge pipeline...anything I can do on my end to move it along? thanks\r\n\r\nThere were some internal changes needed to make it work, I took care of it. Still waiting for approvals though, but hopefully it will be merged tomorrow.", "@yisitu @akuegel @gbaned \r\n\r\nthank you all very much for your help in getting this PR merged, it is much appreciated.", "@deven-amd @goldiegadde @geetachavan1 \r\n\r\nDevan, if you need this in TF 2.5 you might have to request for a cherry pick. I'll let you drive from here."]}, {"number": 47507, "title": "Do *NOT* merge: draft PR5 for micro op expand_dims.", "body": " Issue #46258 draft PR5. It's the debug version of PR4 with printf in each function.\r\n\r\nThe purpose of this draft is to debug why PR4 is failing the MacOS CPU Python3 \u2014 Internal CI build.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "TFLIte Micro internal CI build failed on the bluepill target, caused by the additional std::cout instructions. That's not too surprising. It's not the failure I am trying to debug.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47507) for more info**.\n\n<!-- need_author_consent -->", "That's OK. This is the draft PR for debugging purpose. That bug has been resolved and I can close this draft PR now."]}, {"number": 47506, "title": "Upgrade pylint 2.7.2", "body": "Fix https://github.com/tensorflow/tensorflow/issues/47505\r\n\r\n/cc @mihaimaruseac", "comments": []}, {"number": 47505, "title": "install_pip_packages.sh specifies incompatible version combo for pylint and astroid", "body": "This PR https://github.com/tensorflow/tensorflow/pull/46046 introduces what seems to be an incompatible version combination for pylint and astroid.\r\n\r\nsee this line : https://github.com/tensorflow/tensorflow/pull/46046/files#diff-c8d38bb153d3e8107f198a493594dba5a8725176bbc6069304aec7fcec2fabc6R89\r\n\r\nThe nightly TF ROCm build (which uses the `install_pip_packages.sh` script when creating the docker container), ran into the following error\r\n```\r\nERROR: Cannot install astroid==2.5 and pylint==2.6.2 because these package versions have conflicting dependencies.\r\n\r\nThe conflict is caused by:\r\n    The user requested astroid==2.5\r\n    pylint 2.6.2 depends on astroid<2.5 and >=2.4.0\r\n\r\nTo fix this you could try to:\r\n1. loosen the range of package versions you've specified\r\n2. remove package versions to allow pip attempt to solve the dependency conflict\r\n\r\nERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\r\n```\r\n\r\nThe complete build log can be seen here : http://ml-ci.amd.com:21096/job/tensorflow-rocm-nightly/630/console\r\n\r\n/cc @bhack ", "comments": ["I will check this constrain today", "Check https://github.com/tensorflow/tensorflow/pull/47506", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47505\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47505\">No</a>\n", "TF ROCm nightly build passed with this PR merged. Thanks @bhack for the quick response.\r\n\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-rocm-nightly/632/"]}, {"number": 47503, "title": "`tf.distribute.MirroredStrategy(),` but getting unbalance GPU usage", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes, with reference.  \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: CUDA 11 and cuDNN 8\r\n- GPU model and memory: 3 GPU's 32GB each (same GPU model)\r\n\r\n\r\n**Describe the current behavior**\r\nTrying to use `tf.distribute.MirroredStrategy(),` but getting unbalance GPU usage. The 1st GPU is used most of the time and other 2 GPU's are seldom used for the same code, same training instance. \r\n\r\n**Describe the expected behavior**\r\nget almost uniform usage of all 3 GPU (for better performance).\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://github.com/rrklearn2020/keras-retinanet \r\n\r\nRunning directly from the repository with tf.distribute.MirroredStrategy():\r\n`keras_retinanet/bin/train_v2.py coco /path/to/MS/COCO`\r\n\r\n\r\nI also tried the \"tf.data.Dataset.from_generator\" by adding below lines (currently not added to train_v2.py due to error)\r\n```\r\ntrain_generator=tf.data.Dataset.from_generator(train_generator)\r\nvalidation_generator=tf.data.Dataset.from_generator(validation_generator)\r\n```\r\nBut getting an error as shown below.\r\n```\r\nraise TypeError(\"`generator` must be callable.\")\r\nTypeError: `generator` must be callable.\r\n```\r\n\r\n\r\n Please guide me to get almost uniform usage of all 3 GPU (for better performance).\r\n\r\n", "comments": ["@rrklearn2020 \r\nThe code provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily, or if possible share a colab gist with the issue reported.\r\nThanks!\r\n", "@Saduf2019 \r\n\r\n**Point 1:** The performance concern while using tf.distribute.MirroredStrategy(). Screenshot attached. Question: The 1st GPU is used most of the time and other 2 GPU's are seldom used for the same code, same training instance. **_How to improve the performance by having all GPU running with similar load?_**\r\n\r\n![GPU_Use_MirroredStrategy](https://user-images.githubusercontent.com/70491128/109673025-9bb09b80-7b43-11eb-89a6-08f5fa447f3f.png)\r\n\r\n**Point 2:** While trying to improve the performance using the \"tf.data.Dataset.from_generator\", its creating error (code and the error are shared before. Please inform the points to be considered while using \"tf.data.Dataset.from_generator\". I had checked the [https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator](url), but not very clear. \r\n\r\n\r\n\r\n\r\n\r\n", "Hi @rrklearn2020, [have you seen the GPU performance guide](https://www.tensorflow.org/guide/gpu_performance_analysis)? It might be worth trying to profile your training job to try and see what's happening on each GPU at each step.\r\n\r\nAdditionally, are you also seeing this imbalance if you remove the validation data or callbacks?\r\n\r\nAs @Saduf2019 mentioned earlier, if you can provide a simpler example I can try to reproduce this on my end.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47502, "title": "Error when using quantization aware training with custom activation function", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nCustom code\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n\r\n- TensorFlow installed from (source or binary):\r\nAnaconda Navigator\r\n\r\n- TensorFlow version (use command below):\r\ntensorflow-gpu 2.3.0\r\n\r\n- Python version:\r\n3.7.9\r\n\r\n- CUDA/cuDNN version:\r\ncudatoolkit 10.1.243\r\ncudnn 7.6.5\r\n\r\n- GPU model and memory:\r\nGTX 1070 8GB\r\n\r\n**Describe the current behavior**\r\nI have a model with a custom activation function, ReLU6, as defined below. When using tfmot.quantization.keras.quantize_model I get the following error output:\r\n\r\n> ValueError: Unable to clone model. This generally happens if you used custom Keras layers or objects in your model. Please specify them via `quantize_scope` for your calls to `quantize_model` and `quantize_apply`.\r\n\r\nWhen simply using 'relu' I don't get this error.\r\n\r\nActivation function:\r\n`def _relu6(x):\r\n    return activations.relu(x, max_value=6.0)`\r\n\r\nUsage:\r\n`x = Activation(_relu6)(x)`\r\n\r\n\r\n**Describe the expected behavior**\r\nI need to be able to quantize a model with the relu6 activation function. I am not sure how to approach this problem.\r\n\r\n**Standalone code to reproduce the issue**\r\nCreating any model with the custom activation function as above seems to yields this error.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\n", "comments": ["@ethkim could you triage this issue?", "@LucasStromberg,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "Hi again, I was able to get around this problem by replacing my custom activation function with the tensorflow.keras.layers.ReLU(max_value=6.0) layer.", "@LucasStromberg,\r\nThank you for the update. Closing this issue as it is resolved. \r\n\r\nPlease feel free to re-open if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47502\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47502\">No</a>\n"]}, {"number": 47501, "title": "Tensorflow failed to build due to error C2039 C3861 on windows with MSVC", "body": "Hi All,\r\n\r\nTensorflow failed to build due to error C2039 C3861 on windows with MSVC. It can be reproduced on latest master branch commit. Could you please help take a look at this issue? Thanks in advance!\r\n\r\nEnvironment\r\nTensorflow(latest master branch revision from github) + VS 2019 + Windows Server 2016\r\n\r\nNote: \r\nbuild.log in build.zip attachment.\r\n[build.zip](https://github.com/tensorflow/tensorflow/files/6067995/build.zip)\r\n\r\ntensorflow_set_bazel_version.patch in tensorflow_set_bazel_version.zip\r\n[tensorflow_set_bazel_version.zip](https://github.com/tensorflow/tensorflow/files/6068012/tensorflow_set_bazel_version.zip)\r\n\r\nRepro steps:\r\n1. Open x64 native tools command prompt for VS2019\r\n2. git clone https://github.com/tensorflow/tensorflow F:\\gitP\\tensorflow\\tensorflow\r\n3. git submodule init\r\n4. git submodule update --recursive\r\n5. Apply tensorflow_set_bazel_version.patch in attachment tensorflow_set_bazel_version.zip\r\n6. cd F:\\gitP\\tensorflow\\tensorflow\r\n7. pip3 install six numpy wheel\r\n8. pip3 install keras_applications==1.0.6 --no-deps\r\n9. pip3 install keras_preprocessing==1.0.5 --no-deps\r\n10. set PATH=F:\\gitP\\tensorflow\\tensorflow\\..\\tools;%path%\r\n11. set PATH=F:\\gitP\\tensorflow\\tensorflow\\..\\tools\\msys64\\usr\\bin;%path%\r\n12. yes \"\" 2>nul | python ./configure.py\r\n13. set BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\r\n14. set BAZEL_VC_FULL_VERSION=14.28.29333\r\n15. bazel --output_user_root F:\\bazelTemp build --config=opt --subcommands //tensorflow/tools/pip_package:build_pip_package\r\n\r\nActual result:\r\nSUBCOMMAND: # //tensorflow/core/platform:error [action 'Compiling tensorflow/core/platform/error.cc', configuration: b0b8ead46223da3f6648bba4bb6404871eb4a1826bc4e150e3d35eeda28b2f74]\r\nSUBCOMMAND: # //tensorflow/core/platform:error [action 'Linking tensorflow/core/platform/error.lib', configuration: b0b8ead46223da3f6648bba4bb6404871eb4a1826bc4e150e3d35eeda28b2f74]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.28.29333\\include\\complex(674): error C2039: 'copysign': is not a member of '`global namespace''\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.28.29333\\include\\complex(674): error C3861: 'copysign': identifier not found\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 9803.671s, Critical Path: 681.63s\r\nINFO: 9288 processes: 9288 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n##[debug] Command #6 exited with code [1].\r\n##[error] Detected error code [1].", "comments": ["@spacelg,\r\nLooks like this is a duplicate of issue [#46902](https://github.com/tensorflow/tensorflow/issues/46902), which has already been addressed by a member of the TensorFlow team.\r\n\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/46902#issuecomment-780667266) for more information. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47501\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47501\">No</a>\n"]}, {"number": 47500, "title": "Fix comment", "body": "", "comments": ["@stellaraccident As you reviewed the original PR, this is probably a copy-paste-error and you might want to merge? :)"]}, {"number": 47499, "title": "[Grappler] Add dataset.shard(1, 0) to noop_elimination pass", "body": "Thank you for your time on reviewing this PR. There are some addition to the test as `ShardDataset` is not a unary dataset op.", "comments": ["@zhuzilin  Can you please check @jsimsa's comments and keep us posted ? Thanks!", "@jsimsa @gbaned Really sorry for the late update... was trapped in some other things lately... I've updated the test according to the review."]}, {"number": 47498, "title": "-1073741795", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Junaid\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Junaid\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Junaid\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Junaid\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Junaid\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n- first I run the command line pip install tensorflow==2.2.0\r\n- then while importing the tensorflow it throws the above error\r\n\r\n![image](https://user-images.githubusercontent.com/72540582/109615543-079dfe80-7b5a-11eb-82d9-9e33fd4b8909.png)\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@Junaidkhan1809 \r\nPlease verify your system meets the requirements mentioned above mentioned for you to use tensorflow.", "> @Junaidkhan1809\r\n> Please verify your system meets the requirements mentioned above mentioned for you to use tensorflow.\r\n\r\nMy machine has windows 7 64bit i3 ", "What specification are required to run tensorflow @Saduf2019  ", "@Junaidkhan1809\r\nPlease click on the links given by bot above and verify [ex python 64 bit, AVX support, os 64 bit etc].\r\n\r\n\r\nYou might be facing this issue because of the following reasons\r\n\r\nYou are running 32-bit Python or 32-bit OS\r\nYou have not installed the M[icrosoft Visual C++ Redistributable](https://support.microsoft.com/en-us/topic/the-latest-supported-visual-c-downloads-2647da03-1eea-4433-9aff-95f26a218cc0) package\r\nYour CPU does not support AVX instructions.\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check this similar duplicate issue: #36167\r\n\r\nThanks!", "My machine is not compatible !\r\none last query What type of machine is required in terms of specification @Saduf2019 ", "Thank You !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47498\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47498\">No</a>\n"]}, {"number": 47497, "title": "rtx3090 tf2.4.1 error with single machine multi-card", "body": "strategy is tf.distribute.MirroredStrategy, I train the model on two cards, it reports that\r\nF tensorflow/stream_executor/cuda/cuda_driver.cc:1289] Check failed: context != nullptr success should entail non-null context.\r\nhow can I solve it?\r\n", "comments": ["@jiangjing1989,\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nalong with the complete code and the dataset you are using. Thanks!", "Also, please take a look at issue [#46851](https://github.com/tensorflow/tensorflow/issues/46851) with a similar error log and check if it helps. Thanks!", "> @jiangjing1989,\r\n> In order to expedite the trouble-shooting process, could you please provide the following information\r\n> \r\n> * Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n> * TensorFlow installed from (source or binary):\r\n> * TensorFlow version (use command below):\r\n> * Python version:\r\n> * Bazel version (if compiling from source):\r\n> * GCC/Compiler version (if compiling from source):\r\n> * CUDA/cuDNN version:\r\n> * GPU model and memory:\r\n> \r\n> along with the complete code and the dataset you are using. Thanks!\r\n\r\n**my environment is :**\r\nUbuntu 18.04.5 LTS\r\npython3.8\r\nrtx3090\r\nconda install cudatoolkit=11.0 \r\nconda install cudnn=8.0.5 \r\npip3 install tensorflow_gpu-2.4.1-cp38-cp38-manylinux2010_x86_64.whl\r\npip install tfa-nightly\r\n\r\ncode address is https://github.com/jiangjing1989/asr_test/tree/master/my, the start script if train_sublm.py.\r\n\r\nI run the code with RTX2080 successfully.\r\n", "> Also, please take a look at issue [#46851](https://github.com/tensorflow/tensorflow/issues/46851) with a similar error log and check if it helps. Thanks!\r\n\r\nsorr, tensorflow version is tf_nightly_gpu-2.5.0.dev20210130-cp38-cp38-manylinux2010_x86_64.whl", "> Also, please take a look at issue [#46851](https://github.com/tensorflow/tensorflow/issues/46851) with a similar error log and check if it helps. Thanks!\r\n\r\nsorry again, cudnn version is 11.2", "@jiangjing1989,\r\nCurrently TensorFlow is not compatible with **CUDA 11.2**. \r\n\r\nTF v2.4 and TF-nightly are built and tested against **CUDA 11.0**. Could you please check if you are facing the same error with **CUDA 11.0** and **cuDNN 8**. Thanks!", "> @jiangjing1989,\r\n> Currently TensorFlow is not compatible with **CUDA 11.2**.\r\n> \r\n> TF v2.4 and TF-nightly are built and tested against **CUDA 11.0**. Could you please check if you are facing the same error with **CUDA 11.0** and **cuDNN 8**. Thanks!\r\n\r\nThanks. With CUDA 11.0 and cuDNN8.0.2, it run successfully.", "> Thanks. With CUDA 11.0 and cuDNN8.0.2, it run successfully.\r\n\r\n@jiangjing1989 \r\nThank you for the update. Closing the issue as it is resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47497\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47497\">No</a>\n"]}, {"number": 47496, "title": "zero gradient for higher-order derivatives when using tf.function and tf.scan", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f\r\n- Python version: 3.0\r\n\r\n**Describe the current behavior**\r\nWhen using tf.function decorator on functions involving tf.scan, the second-order derivative goes to zero.\r\n\r\n**Describe the expected behavior**\r\nI would expect the gradient not to be zero when tf.function is used.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1M5-ua3LXgrc8QpwtcwY5tEsQ1gSkY9TD", "comments": ["I just tried with latest version of tensorflow:\r\n`!pip install tf_nightly`\r\n\r\nBug no more!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47496\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47496\">No</a>\n"]}, {"number": 47495, "title": "keras lstm layer produces different results in tf 1 and tf 2 due to different default activation", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- TensorFlow version:\r\n1. TF 1.0: v1.15.4-39-g3db52be7be8 1.15.5\r\n2. TF 2.0: v2.4.0-49-g85c8b2a817f 2.4.1\r\n\r\n\r\n**Describe the current behavior**\r\nThe LSTM layer in tf 1.0 uses \"hard_sigmoid\" as the default recurrent activation: https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/keras/layers/recurrent.py#L2489\r\n\r\nwhile it uses  \"sigmoid\" in tf 2.0: https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/layers/recurrent_v2.py#L1077\r\n\r\nAs a result, when I try to execute the same code (without specifying recurrent_activation flag explicitly) in tf 1 and 2, I get slightly different results and took quite a while to find the cause.\r\n\r\n**Describe the expected behavior**\r\nSame code should produce the same result in both versions. Or some prompt should be given for easier debugging.\r\n", "comments": ["@VincentYu68 \r\n\r\n1. There is no suppport for tf 1.x now, there is support only for 2.x.\r\n2. I ran the code shared, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/2a200017506a101be4189ac992cef9bf/untitled556.ipynb)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47495\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47495\">No</a>\n"]}, {"number": 47494, "title": "https://www.tensorflow.org/guide/estimators - HTTP Error 404: Page not Found", "body": "Looks like this shows a 404 to me at the time of writing\r\nhttps://www.tensorflow.org/guide/estimators\r\n", "comments": ["@xcodz-dot,\r\nCould you please let us know the source from where you got the link, so that we can correct it. The working link is\r\n \r\nhttps://www.tensorflow.org/guide/estimator\r\n\r\nThanks!", "It's all over this page:\r\nhttps://github.com/tensorflow/estimator", "@xcodz-dot,\r\nThank you for reporting the issue. I've submitted a fix for the broken link in PR [#63](https://github.com/tensorflow/estimator/pull/63).\r\n\r\nThis issue will be closed once the PR is merged.", "> It's all over this page:\r\n> https://github.com/tensorflow/estimator\r\n\r\n@xcodz-dot,\r\nPR [#63](https://github.com/tensorflow/estimator/pull/63) has been merged. Now the guide points to the working link.\r\n\r\nClosing this issue as it is resolved. Please feel free to re-open if mistaken. Thanks!"]}, {"number": 47493, "title": "Cosine annealing learning rate scheduler with minimum learning rate boundary", "body": "**System information**\r\n- TensorFlow version (you are using): Tensorflow 2.x\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIn the nightly build, we have API called [tf.keras.optimizers.schedules.CosineDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay)  which schedule decaying learning rate. However, we cannot set the range of learning rate incremental decay. For example, this current API allow us to change learning from alpha then decrease it in cosine manner and beyond. My proposal is to decrease the learning rate incrementally in cosine manner from alpha to beta.\r\nnote - alpha : initial learning rate , beta : final learning rate\r\n\r\n**Will this change the current api? How?**\r\nIt could be implemented in tf.keras.optimizers.schedules.CosineDecay or implemented in different API name.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who want to use cosine annealing learning rate and worry about vanishing gradient issue.\r\n\r\n", "comments": ["Could i start working on this?", "@hfahrudin Sure. Thanks for your contributions. You can raise a PR to update it. Thanks!", "@jvishnuvardhan Should i make another class, or just edit this [API](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay)?", "@hfahrudin I think it's sensible to add this as an optional argument to the current API.", "> @hfahrudin I think it's sensible to add this as an optional argument to the current API.\r\n\r\nRoger that!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I already issued the pull request but it seems that i issued it to protected branch. Which branch should i use for this PR?", "Always master branch", "Hi @hfahrudin,\r\nI took a look at the PR you submitted. Upon looking again at the math, it does seem that the existing implementation provides a minimum/final learning rate, but it's defined as a fraction of the initial learning rate (not as the actual final learning rate).\r\nAnother point that struck me, is that your PR changes the calculation (whereas I initially though it would just add an optional minimum), this makes it backwards incompatible and makes me very hesitent to accept it unless it actually fixes a bug.", "@deeb02 Thanks for the review. I would keep it in mind in the future", "Absolutely. Thank you for trying to make Keras better!\r\nKeep looking for things that need to be improved!"]}, {"number": 47492, "title": "GPUOptions C++ free(): invalid pointer", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version (use command below):1.14\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):0.24.1\r\n- GCC/Compiler version (if compiling from source):7.5.0\r\n- CUDA/cuDNN version:10.0/7.4.2\r\n- GPU model and memory: 2070 8G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nc++\r\nint main()\r\n{   \r\n    tensorflow::Session* session;\r\n    tensorflow::Status status;\r\n    ConfigProto configProto;\r\n    GPUOptions gpuOptions;\r\n\r\n    gpuOptions.set_per_process_gpu_memory_fraction(0.5);\r\n    configProto.set_allocated_gpu_options(&gpuOptions);\r\n}\r\n\r\nit will report free(): invalid pointer after the code is finished.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@qurugou \r\nWe see that you are using an old version of tensorflow for which there is no support, please upgrade to tf 2.x and let us know in case you face any issues.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47492\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47492\">No</a>\n"]}, {"number": 47491, "title": "[ROCm] Updates to account for the removal of $ROCM_PATH/include/hiprand,rocrand dirs", "body": "The following dirs (which are soft-links really) will removed in a (near) future ROCm release. This commit updates `rocm_configure.bzl` to account for that change by explicitly copying over the header files (for `local_config_rocm` repo) from the following directories instead\r\n* $ROCM_PATH/hiprand/include\r\n* $ROCM_PATH/rocrand/include\r\n\r\nUnfortunately making the above change also breaks the build, if the directories that will be going away, are still present. This is because we end up with two `make_copy_dir_rule`s in `rocm_configure.bzl` that try to write to the same destination dirs\r\n\r\n```\r\nERROR: Traceback (most recent call last):\r\n\tFile \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/local_config_rocm/rocm/BUILD\", line 1588, column 8, in <toplevel>\r\n\t\tgenrule(\r\nError in genrule: generated file 'rocm/include/hiprand/hiprand.h' in rule 'hiprand-include' conflicts with existing generated file from rule 'rocm-include', defined at /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/local_config_rocm/rocm/BUILD:175:8\r\n```\r\n\r\nIn order to get around this error, this commit is also updating the `Dockerfile.rocm` to explicitly delete those two dirs from the `$ROCM_PATH/include` dir, immediately after installing ROCm\r\n\r\n\r\n------------------------------------\r\n\r\n\r\n@cheshire @chsigg", "comments": ["@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@deven-amd I have no idea what this code is doing, but since it's ROCM-only I trust your judgement on it.", "@gbaned this PR seems to have gotten stuck in the merge pipeline...anything we can do on our end to get it merged?", "@gbaned gentle ping.  @chsigg already approved this PR", "> @gbaned gentle ping. @chsigg already approved this PR\r\n\r\n@deven-amd There are commits after approval, hence re-requested @chsigg to review. Thanks!", "@cheshire @chsigg gentle ping"]}]