[{"number": 43560, "title": "Predict results by java and python api are not exactly same?", "body": "Hi, I'm trying to run tensorflow model on spark, and testing the java api. I built a simple model by tensorflow and tried to run it with both java and python API. I found many results are not exactly same, but with different bit precision, and java results' precision bits are usually less then python's, for example:\r\n\r\n```\r\njava results:\r\n       [9.9787010e-01, 1.7358017e-05, 2.1125420e-03],\r\n       [9.9581650e-01, 8.3105200e-04, 3.3524006e-03],\r\n       [9.9857880e-01, 3.4846460e-05, 1.3863753e-03],\r\n       [9.9834824e-01, 3.1828644e-05, 1.6199955e-03],\r\n       [9.9810120e-01, 2.9924562e-05, 1.8689097e-03],\r\n       [9.8520744e-01, 7.6777805e-03, 7.1147494e-03],\r\n\r\npython results:\r\n       [9.9787009e-01, 1.7358017e-05, 2.1125420e-03],\r\n       [9.9581653e-01, 8.3105202e-04, 3.3524006e-03],\r\n       [9.9857879e-01, 3.4846460e-05, 1.3863753e-03],\r\n       [9.9834824e-01, 3.1828644e-05, 1.6199955e-03],\r\n       [9.9810117e-01, 2.9924562e-05, 1.8689097e-03],\r\n       [9.8520744e-01, 7.6777805e-03, 7.1147499e-03],\r\n```\r\n\r\nThe example I used is iris classification, results are iris's three types, so I used one-hot code.\r\nThe most wired thing is that java results' precision bits are less then python's obviously. So, is there any config or any way to make sure java result get same precision with python?\r\n\r\nThe codes I used to generate model:\r\n```\r\ndef create_model():\r\n    inputs = tf.keras.Input(shape=(4,), name='k_input')\r\n    x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(inputs)\r\n    outputs = tf.keras.layers.Dense(3, activation=tf.nn.softmax, name='out')(x)\r\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n    model.compile(optimizer='adam',\r\n                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                metrics=[tf.metrics.SparseCategoricalAccuracy()])\r\n\r\n    return model\r\n\r\n\r\nmodel = create_model()\r\nmodel.fit(vec_train, res_train, epochs=10)\r\nmodel.save(\"saved_model/iris\")\r\n```\r\nThe codes I used to run model in python:\r\n```\r\nloaded = keras.models.load_model(\"saved_model/iris\")\r\nres = loaded.predict(vec_sample)\r\n```\r\nThe codes I used in java:\r\n```\r\n    val model = SavedModelBundle.load(s\"$resourceDir/iris\", \"serve\")\r\n    val session = model.session()\r\n\r\n    val ret = session\r\n      .runner()\r\n      .feed(\"serving_default_k_input\", Tensor.create(inputData))\r\n      .fetch(\"StatefulPartitionedCall\")\r\n      .run()\r\n    val size = inputData.length * 3\r\n    val r = FloatBuffer.allocate(size.toInt)\r\n    val result = ret.get(0).writeTo(r)\r\n```\r\n\r\ntensorflow version: 2.1\r\npython version: 3.7\r\njava version: 1.8\r\njava api version: 1.15.0", "comments": ["Can you try to print the inputs in python and java?", "@Luminosite I have couple of questions.\r\n1) Is there any model conversion involved here? \r\n2) I checked the above results. 2nd and third columns are exactly same. Only change is in first column of results and the change is in 6th or 7 decimal. Do you want 6th decimal accuracy for your use-case? Is there any rounding error involved?\r\n3) is there any difference in the predictions? Thanks!\r\n4) can you please use recent TF versions and let us know how those perform.\r\n\r\nI guess the discrepancy is very very low or insignificant. But, may be your use-case needs that accuracy. Thanks!", "> Can you try to print the inputs in python and java?\r\n\r\n```\r\ninput: [7.174808,2.6553774,5.12266,0.41229987], output: [0.9990282,3.3963486E-5,9.378085E-4]\r\ninput: [7.191363,2.555118,6.765061,2.0554824], output: [0.99972814,4.5734428E-6,2.6736E-4]\r\ninput: [6.6439853,3.5317183,5.3618083,1.0571597], output: [0.99917054,2.1115382E-5,8.0844515E-4]\r\ninput: [6.6596174,2.955047,5.980057,1.1314213], output: [0.999271,1.4503375E-5,7.145047E-4]\r\ninput: [5.7817345,4.1580234,3.0035162,0.73943543], output: [0.99696285,4.2656367E-4,0.0026106567]\r\ninput: [5.461476,4.3531785,5.786151,1.8375901], output: [0.9990048,1.573877E-5,9.795206E-4]\r\ninput: [7.2582817,3.4890115,4.6998153,2.4146466], output: [0.999686,3.089602E-5,2.8309433E-4]\r\n```\r\nand I used same data directly in python to run the model like:\r\n```\r\ninput_sample = [\r\n    [7.174808,2.6553774,5.12266,0.41229987],\r\n    [7.191363,2.555118,6.765061,2.0554824],\r\n    [6.6439853,3.5317183,5.3618083,1.0571597],\r\n    ... ...\r\n]\r\nloaded.predict(input_sample)\r\n```\r\nfinally I got result with more precision bits:\r\n```\r\narray([[9.99028206e-01, 3.39634862e-05, 9.37808480e-04],\r\n       [9.99728143e-01, 4.57344277e-06, 2.67360010e-04],\r\n       [9.99170542e-01, 2.11153820e-05, 8.08445155e-04],\r\n       [9.99270976e-01, 1.45033746e-05, 7.14504684e-04],\r\n       [9.96962845e-01, 4.26563667e-04, 2.61065667e-03],\r\n       [9.99004781e-01, 1.57387694e-05, 9.79520613e-04],\r\n       [9.99686003e-01, 3.08960189e-05, 2.83094327e-04],\r\n       ... ...\r\n])\r\n```", "> @Luminosite I have couple of questions.\r\n> \r\n> 1. Is there any model conversion involved here?\r\n> 2. I checked the above results. 2nd and third columns are exactly same. Only change is in first column of results and the change is in 6th or 7 decimal. Do you want 6th decimal accuracy for your use-case? Is there any rounding error involved?\r\n> 3. is there any difference in the predictions? Thanks!\r\n> 4. can you please use recent TF versions and let us know how those perform.\r\n> \r\n> I guess the discrepancy is very very low or insignificant. But, may be your use-case needs that accuracy. Thanks!\r\n\r\n@jvishnuvardhan  Thanks for the reply.\r\nHere're my answers:\r\n1, no, I only used a NN like the codes above.\r\n2, we want to provide model prediction service by spark, so we need to make sure the results are as precise as python results.  And I didn't involve any rounding process, so I guess it should not be the rounding problem\r\n3, results are almost same, all differeces are at 10^-7 level, so I think it's caused by different number of precision bits\r\n4, I've used latest tensorflow dependency in maven, which is 1.15.0. I couldn't find any dependencies later than 1.15.0 in https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow/\r\n\r\nAnd also, I noticed something may related to the issue. In java codes, I have to convert input data as float, and result data is also of type float. In java, float type is only 34 bits precision which support around 6~7 bits precision in decimalism. But in python the default type of float number is 'float' which is not same in java but has more precision bits.\r\nI wonder if there're some more precision API in java to provide more precise result?", "@Luminosite I am not sure about higher precision API in java. May be experts at [`tensorflow.js`](https://github.com/tensorflow/tfjs/issues) repository might be able to help you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43559, "title": "Decoding error during vocabulary loading from TextVectorization layer", "body": "**Intro**\r\nError occurs within get_vocabulary() method from TextVectorization when one of tokens can't be decoded. The exact place in a code is string_lookup's get_vocabulary() method:\r\n\r\n`return [x.decode(self.encoding) for _, x in sorted(zip(values, keys))]`\r\n\r\n**How to reproduce?**\r\n```\r\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n\r\ntext = 'By\u0142 to \u015bwietny pomys\u0142, bo punktowa\u0142 Prawo i Sprawiedliwo\u015b\u0107 tam, gdzie jest ono najs\u0142absze, mimo \u017ce udaje najsilniejsze. Uderza\u0142 w wizerunek pa\u0144stwa dobrobytu, kt\u00f3re nikogo nie zostawia z ty\u0142u i wyr\u00f3wnuje szanse. Tutaj mamy pewnego rodzaju d\u00e9j\u00e0 vu.'\r\n\r\nvectorize_layer = TextVectorization()\r\nvectorize_layer.adapt([text])\r\nprint(vectorize_layer.get_vocabulary())\r\n```\r\n\r\nSimpler code:\r\n\r\n```\r\nvalues = [1]\r\nkeys = [b'warszawie\\xc2']\r\n[x.decode('utf-8') for _, x in sorted(zip(values, keys))]\r\n```\r\n\r\n> Error: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 9: unexpected end of data\r\n\r\n**Fix**\r\n\r\nI was able to fix it simply by writing my own _get_vocabulary() method, just by ignoring decoding errors which are rare but frustrating:\r\n\r\n```\r\ndef _get_vocabulary():\r\n    keys, values = vectorize_layer._index_lookup_layer._table_handler.data()\r\n    return [x.decode('utf-8', errors='ignore') for _, x in sorted(zip(values, keys))]\r\n```\r\n\r\n***Can option to ignore decoding errors be passed to string_lookup?***\r\n\r\n- Windows 10 \r\n- TensorFlow 2.2\r\n- Python 3.8\r\n", "comments": ["@mikolajkania,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "@amahendrakar I updated *how to reproduce* part - you can run it easily now as script.\r\n\r\nThe error is: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc3 in position 4: unexpected end of data", "@mikolajkania,\r\nThank you for the update. I was able to run the code without any issues with TF v2.3. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/8ba8ae4260d781f55361850b76100b32/43559.ipynb#scrollTo=CsRQcxJcgi2B).\r\n\r\nCould you please update TensorFlow to v2.3 and check if you are still facing the same error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@amahendrakar I tried again running this script:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n\r\nprint(tf.__version__)\r\n\r\ntext = 'By\u0142 to \u015bwietny pomys\u0142, bo punktowa\u0142 Prawo i Sprawiedliwo\u015b\u0107 tam, gdzie jest ono najs\u0142absze, mimo \u017ce udaje najsilniejsze. Uderza\u0142 w wizerunek pa\u0144stwa dobrobytu, kt\u00f3re nikogo nie zostawia z ty\u0142u i wyr\u00f3wnuje szanse. Tutaj mamy pewnego rodzaju d\u00e9j\u00e0 vu.'\r\n\r\nvectorize_layer = TextVectorization()\r\nvectorize_layer.adapt([text])\r\nprint(vectorize_layer.get_vocabulary())\r\n```\r\n\r\nResponse is:\r\n\r\n2.3.0\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xc3 in position 4: unexpected end of data\r\n\r\non Python 3.8.5. But I tested it on Python 3.6.9 that you used and it worked. So it looks that the problem is with combination:\r\n\r\n*Python 3.8.X and Tensorflow 2.3*\r\n\r\nCan you test it that why on Collab or your local machine?\r\n\r\n", "> Can you test it that why on Collab or your local machine?\r\n\r\n@mikolajkania,\r\nI was able to run the script without any issues with Python v3.8.5 and TensorFlow v2.3.1. Please check the attached screenshot for reference. \r\n![Screenshot 2020-10-10 at 9 11 59 PM](https://user-images.githubusercontent.com/57165142/95659269-7c39b780-0b3d-11eb-97f2-1e77287e3735.png)\r\n\r\nThanks!\r\n ", "@amahendrakar can you test it on Windows? It works for me on Ubuntu from WSL2 (Windows Subsystem For Linux):\r\n\r\n![obraz](https://user-images.githubusercontent.com/14984193/95675567-7fbe5480-0bb8-11eb-8662-f90335b51f5f.png)\r\n\r\nBut not on Windows itself (Python 3.8.5):\r\n\r\n![obraz](https://user-images.githubusercontent.com/14984193/95675610-d7f55680-0bb8-11eb-997e-b8352d4a2faa.png)\r\n\r\nIt also worked on Windows on version 3.6.9 (as pasted above).", "@amahendrakar hi, any news?", "@mikolajkania,\r\nSorry for the delayed response. I do not have a Windows machine to test the code. Could you please try running the code in a virtual environment and check if you are facing the same issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "It's a Windows issue. The same code runs on Colab without generating the error.", "This happens in:\r\n    os: Windows\r\n    python: >= 3.7\r\n    tf: >= 2.3\r\n\r\n\r\n\r\nInstalling `python == 3.6`  worked in my case.", "I'm currently running into this problem as well with `TextVectorization.get_vocabulary`. \r\n\r\nOS: Windows 10\r\nPython: 3.8.5\r\nTensorFlow: 2.3.0", "@princyok,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "Hi, \r\nI'm currently running into this problem too as well with TextVectorization.get_vocabulary.\r\n\r\nOS: Windows 10\r\nPython: 3.8.8\r\nTensorFlow: 2.4.2", "@ClementViricel I believe the cause of the issue is tensorflow being unable to handle certain characters when working on Windows. A crappy temporary solution is to find such characters and omit them if possible. I reported [the issue here](https://github.com/tensorflow/tensorflow/issues/47022).", "Problem occurs due to windows default encoding `utf-16`. Everything works fine until a character that is not handled properly during encoding is encountered in decoding step inside `TextVectorization.get_vocabulary()`\r\nSolved this problem by changing default encoding for windows to utf-8. To change, go to `Language Settings -> Administrative Language settings -> change system locale -> select beta: Use Unicode UTF-8 for worldwide language support`. Restart. Done."]}, {"number": 43558, "title": "Add support lite model conversion with zero inputs", "body": "When I am going to convert a .pb model with no inputs to lite format, the lite converter reports an error like this:\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"test_convert_tf.py\", line 5, in <module>\r\n    \"model.pb\", input_arrays=[], output_arrays=[\"add\"], input_shapes=[])\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1864, in from_frozen_graph\r\n    return cls(sess.graph_def, input_tensors, output_tensors)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1767, in __init__\r\n    experimental_debug_info_func)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1621, in __init__\r\n    \"If input_tensors and output_tensors are None, both \"\r\nValueError: If input_tensors and output_tensors are None, both input_arrays_with_shape and output_arrays must be defined.\r\n```\r\n\r\nAfter this fix, the model will be successfully converted. Bellow is my codes and .pb model([model.pb.zip](https://github.com/tensorflow/tensorflow/files/5281464/model.pb.zip)) just for testing.\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Convert the model\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\r\n    \"model.pb\", input_arrays=[], output_arrays=[\"add\"], input_shapes=[])\r\n\r\ntflite_model = converter.convert()\r\n\r\n# Save the model.\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43558) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43558) for more info**.\n\n<!-- need_author_cla -->", "@hjchen2  Thank you for your contribution. Can you please sign CLA? Thanks!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43558) for more info**.\n\n<!-- ok -->", "Hi @abattery,\r\n\r\nPython logic looks good, but want to make sure passing empty input/output tensors don't cause problem in other logics.", "@hjchen2 Can you please check @teijeong's comments and keep us posted ? Thanks!", "Thanks @hjchen2 for your contribution!\r\n\r\nThe equivalent change with a test case is merged.\r\nhttps://github.com/tensorflow/tensorflow/commit/74f6c2b599260ef96eeb09b43d2fd8d7623f9e0d "]}, {"number": 43556, "title": "[MLIR / MHLO ]: tf_to_gpu_binary  Internal: Lowering to GPU kernels failed.", "body": "I am using **tf_to_gpu_binary** to convert the following mlir file to a gpu kernel \r\n\r\n```\r\nfunc @abs(%arg0: tensor<*xf32>) -> tensor<*xf32> {\r\n %0 = \"tf.Abs\"(%arg0) { }\r\n : (tensor<*xf32>) -> tensor<*xf32>\r\n return %0 : tensor<*xf32>\r\n}\r\n```\r\n\r\n\r\nOn TF top of master (SHA 7f5b8ad3708d0e2cd98e63714352df60b772db36)  it fails with:\r\n(tf_env) foo@bar:$ MLIR_CRASH_REPRODUCER_DIRECTORY=. ./build/install/bin/tf_to_gpu_binary --input=tf_abs.mlir --output=abs_kernel.o\r\nloc(\"-\":2:7): error: failed to legalize operation 'mhlo.abs' that was explicitly marked illegal\r\nloc(\"-\":0:0): error: A failure has been detected while processing the MLIR module, a reproducer has been generated in './mlir_reproducer_ironman-725e3dc1-43003-5b01dc8776cbc.mlir'\r\n2020-09-24 23:54:07.367852: E tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary.cc:95] Internal: Lowering to GPU kernels failed.\r\n\r\n\r\n**MLIR CRASH Repro:**\r\n\r\n```\r\n\r\n// configuration: -pass-pipeline='func(xla-legalize-tf{allow-partial-conversion=false device-type=INVALID_DEVICE_TYPE legalize-chlo=true use-tf2xla-fallback=false}, materialize-broadcast, unfuse-batch-norm), unknown<mlir::mhlo::{anonymous}::HloLegalizeToLhlo>{results-escape-function=true}, func(buffer-placement, unknown<{anonymous}::CopyRemovalPass>), shape-to-descriptors, canonicalize, func(unknown<mlir::{anonymous}::LhloLegalizeToLinalgPass>, unknown<mlir::lmhlo::{anonymous}::LhloFuseLinalgPass>{tile-sizes= use-parallel-loops=true}, convert-linalg-to-parallel-loops, canonicalize, cse, unknown<xla::mlir_gpu::{anonymous}::FuseInnerParallelLoopsPass>, cse, unknown<xla::mlir_gpu::{anonymous}::StoreForwardingPass>, unknown<xla::mlir_gpu::{anonymous}::DeadTempBufferRemovalPass>, canonicalize, cse, unknown<xla::mlir_gpu::{anonymous}::MapParallelLoopsPass>), convert-parallel-loops-to-gpu, func(canonicalize, cse, unknown<mlir::mhlo::{anonymous}::LegalizeTrigonometricToApproximationPass>, unknown<xla::mlir_gpu::{anonymous}::MoveScalarComputationsIntoGpuLaunchPass>), gpu-kernel-outlining, func(unknown<xla::mlir_gpu::{anonymous}::RewriteKernelSignaturePass>), lower-affine, convert-scf-to-std'\r\n// note: verifyPasses=true\r\n\r\n\r\nmodule {\r\n  func @abs(%arg0: tensor<*xf32>) -> tensor<*xf32> {\r\n    %0 = \"tf.Abs\"(%arg0) : (tensor<*xf32>) -> tensor<*xf32>\r\n    return %0 : tensor<*xf32>\r\n  }\r\n}\r\n\r\n```\r\n", "comments": ["`tf_to_gpu_binary` does not support unranked code generation, it requires a ranked input. For the unranked case, you can generate a version for a 1d vector and use that with some host-side glue code to transform the unranked tensor into a 1d vector and the inverse for the result.\r\n\r\nIf you want to have that glue code also be generated, you need to use `tf_to_kernel`, which will produce a library file that contains both host and device code. Your input is also one of the examples.\r\n\r\nLikely, this had not landed when you tried.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43556\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43556\">No</a>\n"]}, {"number": 43555, "title": "Spurious tf.function retracing warnings, when developing Keras layer in colab", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nDeveloping in colab with TF/Keras I always get these random seemingly unrelated warnings. \r\n\r\n**Describe the expected behavior**\r\nNo warnings.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/18XwRl5d8qY1L1qKPMm-A9NT_m4qB0UxZ?usp=sharing\r\n\r\nCode that generate issue:\r\n\r\n```\r\nimport tensorflow as tf\r\nfor ii in range(10):\r\n  # Notice class definition *inside the loop*.\r\n  class MyLayer(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n      super(MyLayer, self).__init__(**kwargs)\r\n\r\n    def call(self, inputs, training=None):\r\n      return 2*inputs\r\n\r\n  i1 = tf.keras.Input((1,))\r\n  logits = MyLayer()(i1)\r\n  model = tf.keras.Model(inputs=i1, outputs=logits)\r\n  model.predict(x=[[0.0], [1.0], [3.0]], batch_size=1)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/1a6760a344736f1c30dfb1265f6db4d7/43555.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/3328ac18feba642ee515c71e38f469f4/43555-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@amahendrakar I think is a dup of https://github.com/tensorflow/tensorflow/issues/38561", "@janpfeifer #38561 is solved in nightly but in your specific example it is an expected behavior. \r\nSee: https://github.com/tensorflow/tensorflow/issues/42441#issuecomment-675654182", "@bhack @janpfeifer After a closer look this warning is confusing and should probably not trigger in this case. Until then, if you want to manage the tf.function yourself you can use `predict_step()` as suggested in #42441, which should not result in the warning.\r\n\r\n", "Thanks, yes, this seems a dup. \r\n\r\nAnd thanks for the fix suggestion. I think i'll live with with the warning eyesore ... I dislike having these hacks (with potential unintended consequences) that need to be undone later.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Some work has been done to reduce retracing warnings, and testing with nightly results in only two warnings in this case.\r\n[Gist here.](https://colab.research.google.com/gist/nikitamaia/0cc6732434b5b32782a4ab0d3d5b254a/43555.ipynb)\r\n", "@nikitamaia Have you tried in your colab:\r\n```python\r\nimport tensorflow as tf\r\nfor ii in range(10):\r\n  # Notice class definition *inside the loop*.\r\n  class MyLayer(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n      super(MyLayer, self).__init__(**kwargs)\r\n\r\n    def call(self, inputs, training=None):\r\n      return 2*inputs\r\n\r\n  i1 = tf.keras.Input((1,))\r\n  logits = MyLayer()(i1)\r\n  model = tf.keras.Model(inputs=i1, outputs=logits)\r\n  model.build(input_shape=(1,))\r\n  model.predict(x=[[0.0], [1.0], [3.0]], batch_size=1)\r\n```", "@bhack I just updated the colab to force install of the new TF 2.5.0 (by default it comes with 2.4.1) and verified that the retrace was only called twice:\r\n\r\nhttps://colab.research.google.com/drive/18XwRl5d8qY1L1qKPMm-A9NT_m4qB0UxZ?usp=sharing\r\n\r\nI feel the issue is fixed. I mean better yet if there were no retracing ... :)", "Have you tried with my example?", "Interesting, I just did, I [added it to the colab](https://colab.research.google.com/drive/18XwRl5d8qY1L1qKPMm-A9NT_m4qB0UxZ#scrollTo=B26kne4tr_ck).\r\n\r\nThis time around there were no warnings at all, neither on yours or mine.\r\n\r\nBtw, I believe you don't need to call `model.build(...)`, since the input is specified (by `tf.kerars.Input(...)`). \r\n\r\nAt least according to [tf.keras.Sequential doc page](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)\r\n\r\n", "Yes but I think It Is similar to https://github.com/tensorflow/tensorflow/issues/48851#issuecomment-830858393", "P.s. Just to summarize `tf.keras.Input` is going to create a placeholder internally if you don't pass your tensor with the `tensor` arg (it is in the documentation).\r\n\r\nE.g. You can try yourself, this will not retrace as with `build`:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfor ii in range(10):\r\n  # Notice class definition *inside the loop*.\r\n  class MyLayer(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n      super(MyLayer, self).__init__(**kwargs)\r\n\r\n    def call(self, inputs, training=None):\r\n      return inputs * 2\r\n\r\n  b = tf.constant([5])\r\n  i1 = tf.keras.Input(shape=(1,), tensor=b)\r\n  logits = MyLayer()(i1)\r\n  model = tf.keras.Model(inputs=i1, outputs=logits)\r\n  #model.build(input_shape=(1,))\r\n  print(model.predict(x=[[0.0], [1.0], [3.0]], batch_size=1))\r\n```", "Sorry I'm not sure I'm following.\r\n\r\n`tf.Keras.Input` is a placeholder exactly! And it can have a variable size (let's say the batch dimension).\r\n\r\nSearching the docs a bit, `build()` seems to be [a method of `keras.Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#build), not of [`keras.Model`](https://keras.io/api/models/model/). Since Keras models can be a Layer, I believe that's why they have also the `build()` method. I don't think it needs calling in your case (it will be called automatically for the layers in your Model once you \"call\" it).\r\n\r\n\r\n\r\n\r\n", "The difference is that it will be re-trigger the tracing cause what you do in fit is executed in graph mode.\r\n\r\n> And it can have a variable size (let's say the batch dimension).\r\n\r\nAs fit is running in `graph_mode` and in your examples  you cannot explicitly control `@tf.function(experimental_relax_shapes=True)` it will going to trigger retrace.", "Closing issue since the number of warnings has been decreased. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43555\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43555\">No</a>\n", "Update: I'm having the same problem described above with ``.predict()``, but am able to use the ``.predict_step()`` workaround.", "@timhdesilva As this issue closed long timme back, please open a new issue with a simple standalone code to reproduce the issue. Thanks!"]}, {"number": 43554, "title": "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues.", "body": "\r\n```\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.gamma\r\n> W0925 09:38:56.472692  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.gamma\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.beta\r\n> W0925 09:38:56.473077  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.beta\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.0.0.kernel\r\n> W0925 09:38:56.473470  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.0.0.kernel\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.0.0.bias\r\n> W0925 09:38:56.473858  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.0.0.bias\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.1.0.kernel\r\n> W0925 09:38:56.474293  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.1.0.kernel\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.1.0.bias\r\n> W0925 09:38:56.474685  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.1.0.bias\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.0.0.kernel\r\n> W0925 09:38:56.475035  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.0.0.kernel\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.0.1.gamma\r\n> W0925 09:38:56.475502  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.0.1.gamma\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.0.1.beta\r\n> W0925 09:38:56.475951  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.0.1.beta\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.1.0.kernel\r\n> W0925 09:38:56.476325  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.1.0.kernel\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.1.1.gamma\r\n> W0925 09:38:56.476714  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.1.1.gamma\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.1.1.beta\r\n> W0925 09:38:56.477157  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.1.1.beta\r\n> WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\n> W0925 09:38:56.477508  6844 util.py:158] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\n> \r\n```\r\n\r\ni am getting this error while training steps are 25000. tensorflow v2.3 python 3.7.3 \r\n\r\n", "comments": ["@muhosevo \r\nPlease refer to [this link](https://stackoverflow.com/questions/58289342/tf2-0-translation-model-error-when-restoring-the-saved-model-unresolved-objec) and let us know. [also [this link](https://github.com/tensorflow/models/issues/8892) #33150]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "just tf.keras.Model.load_weights(path).expect_partial()\r\n\r\ncalls expect_partial() on the loaded status, since SavedModels saved from Keras often generates extra keys in the checkpoint. Otherwise, the program prints a lot of warnings about unused keys at exit time.\r\n\r\n"]}, {"number": 43553, "title": "[INTEL MKL] Bug fix for remapper test", "body": "This PR fixes UT failure of mkl remapper_test:\r\n1. Increase the tolerance for bf16, just like half.\r\n2. Register a BF16 kernel for elu/elugrad.", "comments": []}, {"number": 43552, "title": "'Operation' object has no attribute '_unconditional_update' when using pre-trained model that used batchnorm", "body": "When trying to use tf.keras.applications that use bacthnorm(like mobilenet or inceptionv3) i'm getting:\r\n`'Operation' object has no attribute '_unconditional_update'`\r\nOther keras.applications that do not use batch-norm(like vgg) work fine with the same code.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): debian  8\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.8.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Nvidia titan xp\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen trying to use tf.keras.applications that use bacthnorm(like mobilenet or inceptionv3) i'm getting:\r\n`'Operation' object has no attribute '_unconditional_update'` when running fit\r\nOther keras.applications that do not use batch-norm(like vgg) work fine with the same code.\r\n**Describe the expected behavior**\r\nto be able to run fit on keras.applications models\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nuse applications model: model_base = keras.applications.InceptionV3(input_shape=INPUT_SIZE,weights = \"imagenet\",include_top=False)\r\nadd fully connected layers to it and then call fit() \r\n\r\n", "comments": ["@llealgt \r\n\r\nCan you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "I tried to isolate it,i found that this happens when eager mode is disabled with tf.compat.v1.disable_eager_execution() , i disable it because i need to export a .pb file for deployment and export fails if eager is enabled, so it seems using pre-trained model(for transfer learning) with batch norm is mutually exclusive with exporting .pb file for deploy: i can do transfer learning but not deploy it, or i can deploy it but not with pre-trained model that uses batch-norm", "@llealgt Can you please share a simple standalone code to reproduce the error? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43552\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43552\">No</a>\n"]}, {"number": 43551, "title": "[comp:lite] App crashes while using GPU Delegate with Float_MobileNet or Float_EffieintNet", "body": "Hi,\r\nI am trying out the following example: [TFLite on Android (with Android (Studio)](https://www.tensorflow.org/lite/performance/gpu)\r\n\r\nI could see it working for CPU, but while using GPU Delegate with Float_MobileNet or Float_EffieintNet, the app is crashing. Added\r\n```\r\ndependencies {\r\n    ...\r\n    implementation 'org.tensorflow:tensorflow-lite:2.3.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'\r\n}\r\n```\r\n\r\nin order to use GPU delegate. And yes, the GPU delegate does not support Quantized models yet, so I can not even test that.\r\n\r\nAs per @jdduke's suggestion, I tried\r\n```\r\nimplementation('org.tensorflow:tensorflow-lite:0.0.0-nightly') { changing = true }\r\nimplementation('org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly') { changing = true }\r\n```\r\nbut could not succeed.\r\n\r\n**Hardware Details**\r\nPixel 3a XL 6.0 1080x2160 xhdpi\r\nQ Android 10.0  x86\r\n\r\nIf someone can help me out here, it would be a great help. \r\n\r\nReference: [TFLite Google Group](https://groups.google.com/a/tensorflow.org/g/tflite/c/h2wSUviSywo/m/BSWIETLZBgAJ)\r\n\r\nThanks", "comments": ["Can you attach the logcat that includes the crash? https://developer.android.com/studio/debug/am-logcat#running. Tentatively to @lintian06 to try to repro.", "[Logcat_Failed_To_Run_Inferenec_On_GPU.txt](https://github.com/tensorflow/tensorflow/files/5284623/Logcat_Failed_To_Run_Inferenec_On_GPU.txt)\r\n\r\nKindly let me know if you want me to manually log any information. \r\n\r\nThanks.\r\n", "```\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Following operations are not supported by GPU delegate:\r\n    SQUEEZE: Operation is not supported.\r\n    29 operations will run on the GPU, and the remaining 2 operations will run on the CPU.\r\n    Can not open OpenCL library on this device - dlopen failed: library \"libOpenCL.so\" not found\r\n    Falling back to OpenGL\r\n    TfLiteGpuDelegate Init: No EGL error, but eglChooseConfig failed.\r\n    TfLiteGpuDelegate Prepare: delegate is not initialized\r\n    Node number 31 (TfLiteGpuDelegateV2) failed to prepa\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n```\r\n\r\nJust be clear, in your hardware details you state:\r\n```\r\nPixel 3a XL 6.0 1080x2160 xhdpi\r\nQ Android 10.0 x86\r\n```\r\n\r\nWhat do you mean by x86 here? Are you running your app in the emulator or on the (ARM) Pixel 3a?", "I am using Emulator. \r\nx86 is the ABI (System Image), API Level 29. ", "Generally speaking, the GPU delegate does not work on emulators, and requires GLES 3.1+. We can try to make the error message a bit more specific on this.", "Oh, I see. Thanks, that'd be helpful. \r\n\r\nCan you redirect me to any end-to-end official tutorial for TFLite with OpenGL ES? I wanted to explore the GPU Delegate. ", "Hi Gaurav,\r\n\r\nI just want to confirm with you if you are using [tensorflow/examples](https://github.com/tensorflow/examples)'s demo. That one contains a class [ClassifierFloatEfficientNet.java](https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/android/lib_support/src/main/java/org/tensorflow/lite/examples/classification/tflite/ClassifierFloatEfficientNet.java) to handle EfficientNet. We use [EfficientNet-lite](https://github.com/tensorflow/examples/blob/0ef3d93e2af95d325c70ef3bcbbd6844d0631e07/lite/examples/image_classification/android/models/download.gradle#L4) optimized by making ops GPU-compatible, and we have [EfficientNet-lite in TFHub](https://tfhub.dev/tensorflow/efficientnet/lite0/classification/2) (efficientnet-lite0 to lite4) to make it reusable.\r\n\r\n```\r\ngit clone https://github.com/tensorflow/examples\r\ncd examples/lite/examples/image_classification/android\r\n\r\n# Use Android Studio to build\r\n# Or use gradle with ANDROID_HOME set correctly to your SDK.\r\n./gradlew build\r\n```\r\n\r\nThe code inside [TFLite on Android (with Android (Studio)](https://www.tensorflow.org/lite/performance/gpu) is OK for EfficientNet CPU, however it didn't contain the treatment that is mentioned above.", "Hi Tian,\r\n\r\nYes, I was using the same demo. Now, as per your suggestion, I have updated the `ClassifierFloatEfficientNet.java` located at `..\\android\\examples\\lite\\examples\\image_classification\\android\\lib_support\\src\\main\\java\\org\\tensorflow\\lite\\examples\\classification\\tflite` from [here](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android/lib_support/src/main/java/org/tensorflow/lite/examples/classification/tflite). Also, this is my newest [download.gradle](https://github.com/tensorflow/examples/blob/0ef3d93e2af95d325c70ef3bcbbd6844d0631e07/lite/examples/image_classification/android/models/download.gradle#L4). Still it is crashing for me. I understand there is no OpenCL but OpenGL should not fail, right? Below is the trace: \r\n\r\n\r\n\r\n```\r\n2020-09-23 15:53:19.618 8675-8714/? I/tflite: Created 0 GPU delegate kernels.\r\n2020-09-23 15:53:19.619 8675-8714/? E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.classification, PID: 8675\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Following operations are not supported by GPU delegate:\r\n    SQUEEZE: Operation is not supported.\r\n    29 operations will run on the GPU, and the remaining 2 operations will run on the CPU.\r\n    Can not open OpenCL library on this device - dlopen failed: library \"libOpenCL.so\" not found\r\n    Falling back to OpenGL\r\n    TfLiteGpuDelegate Init: No EGL error, but eglChooseConfig failed.\r\n    TfLiteGpuDelegate Prepare: delegate is not initialized\r\n    Node number 31 (TfLiteGpuDelegateV2) failed to prepa\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:351)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:82)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:266)\r\n        at org.tensorflow.lite.examples.classification.tflite.Classifier.<init>(Classifier.java:214)\r\n        at org.tensorflow.lite.examples.classification.tflite.ClassifierFloatMobileNet.<init>(ClassifierFloatMobileNet.java:47)\r\n        at org.tensorflow.lite.examples.classification.tflite.Classifier.create(Classifier.java:118)\r\n        at org.tensorflow.lite.examples.classification.ClassifierActivity.recreateClassifier(ClassifierActivity.java:147)\r\n        at org.tensorflow.lite.examples.classification.ClassifierActivity.lambda$onInferenceConfigurationChanged$0$ClassifierActivity(ClassifierActivity.java:126)\r\n        at org.tensorflow.lite.examples.classification.-$$Lambda$ClassifierActivity$83lGy2TUjuj0M5n4BhMB9qlLgSY.run(Unknown Source:8)\r\n        at android.os.Handler.handleCallback(Handler.java:883)\r\n        at android.os.Handler.dispatchMessage(Handler.java:100)\r\n        at android.os.Looper.loop(Looper.java:214)\r\n        at android.os.HandlerThread.run(HandlerThread.java:67)\r\n2020-09-23 15:53:19.627 1995-6756/system_process W/ActivityTaskManager:   Force finishing activity org.tensorflow.lite.examples.classification/.ClassifierActivity\r\n```\r\n\r\nAppreciate your help. ", "> I am using Emulator.\r\n> x86 is the ABI (System Image), API Level 29.\r\n\r\nDo you still use Emulator or a real phone?\r\n- Just as the error message indicated, OpenCL was not found, and there was no OpenGL EGL. No GPU related libraries were available. \r\n\r\nIf you use the latest code of [tensorflow/examples](https://github.com/tensorflow/examples)'s demo, it should directly work on a real phone. FYI, just as Jared did, I tried on Pixel 3 with Android 10 again, which works.", "Thanks, Tian. It worked on a real phone. ", "Hi everybody,\r\n\r\nApologies if this is not the appropriate place for this, but I'm trying to run [this](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android) example\r\n on my device and I'm having the same symptoms described on this ticket. In my case i cannot run both Float classifiers at all (neither in CPU nor GPU mode)\r\nHere's my configuration:\r\nOnePlus One\r\nAndroid 10\r\n\r\nAnd here's what i got from logs:\r\n\r\n2021-01-31 13:31:51.479 27048-27099/org.tensorflow.lite.examples.classification I/Manager: DeviceManager::DeviceManager\r\n2021-01-31 13:31:51.480 27048-27099/org.tensorflow.lite.examples.classification I/Manager: findAvailableDevices\r\n2021-01-31 13:31:51.493 27048-27099/org.tensorflow.lite.examples.classification D/ClassifierWithSupport: Created a Tensorflow Lite Image Classifier.\r\n2021-01-31 13:31:55.660 27048-27086/org.tensorflow.lite.examples.classification W/Adreno-EGL: <qeglDrvAPI_eglGetConfigAttrib:607>: EGL_BAD_ATTRIBUTE\r\n2021-01-31 13:31:59.282 27048-27048/org.tensorflow.lite.examples.classification D/tensorflow: CameraActivity: Updating  model: FLOAT_EFFICIENTNET\r\n2021-01-31 13:31:59.300 27048-27099/org.tensorflow.lite.examples.classification D/tensorflow: ClassifierActivity: Closing classifier.\r\n2021-01-31 13:31:59.311 27048-27099/org.tensorflow.lite.examples.classification D/tensorflow: ClassifierActivity: Creating classifier (model=FLOAT_EFFICIENTNET, device=NNAPI, numThreads=1)\r\n2021-01-31 13:31:59.327 27048-27099/org.tensorflow.lite.examples.classification D/ClassifierWithSupport: Created a Tensorflow Lite Image Classifier.\r\n2021-01-31 13:32:14.213 27048-27086/org.tensorflow.lite.examples.classification W/Adreno-EGL: <qeglDrvAPI_eglGetConfigAttrib:607>: EGL_BAD_ATTRIBUTE\r\n2021-01-31 13:32:19.638 27048-27048/org.tensorflow.lite.examples.classification D/tensorflow: CameraActivity: Updating  device: GPU\r\n2021-01-31 13:32:19.743 27048-27099/org.tensorflow.lite.examples.classification D/tensorflow: ClassifierActivity: Closing classifier.\r\n2021-01-31 13:32:19.747 27048-27099/org.tensorflow.lite.examples.classification D/tensorflow: ClassifierActivity: Creating classifier (model=FLOAT_EFFICIENTNET, device=GPU, numThreads=1)\r\n2021-01-31 13:32:19.756 27048-27099/org.tensorflow.lite.examples.classification I/tflite: Created TensorFlow Lite delegate for GPU.\r\n2021-01-31 13:32:19.766 27048-27099/org.tensorflow.lite.examples.classification A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0 in tid 27099 (inference), pid 27048 (.classification)\r\n\r\nThank you in advance\r\n"]}, {"number": 43550, "title": "'tf.Conv2D' op", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 43549, "title": "Update README.md", "body": "Renaming the title of TensorFlow in Practice from Coursera since\r\nhttps://www.coursera.org/specializations/tensorflow-in-practice points to DeepLearning.AI TensorFlow Developer Professional Certificate\r\nFixes https://github.com/tensorflow/tensorflow/issues/43539", "comments": []}, {"number": 43548, "title": "[INTEL MKL] MKL DNN 0.x code cleanup - FusedBatchNorm op", "body": "DNN 0.x cleanup of MKL FusedBatchNorm op:\r\n (1) Remove all DNN 0.x related code;\r\n\r\n (2) Replace all DNN 1.x macro usages", "comments": ["@gzmkl Can you please resolve conflicts? Thanks!", "@gbaned.  There was a recent commit (by my teammate at Intel, 11 days ago) on the same source file. That commit had changes which did not clean up DNN 0.x code block (also DNN 1.x macros are still used).  I will rebase and then re-do the clean-up.\r\nThanks!", "@gbaned Also, the following PR has to go first as part of Tensorflow 2.4 release. \r\nhttps://github.com/tensorflow/tensorflow/pull/43504\r\nThe above PR does a lot of change in the same source file tensorflow/core/kernels/mkl/mkl_fused_batch_norm_op.cc. \r\n\r\nSo I may need to wait a couple of weeks before I can rebase and do cleanup. I will notify you once it is ready.", "@gzmkl Thank you for the note! I just approved #43504. It should be merged in a day or two if there's no problem.", "@penpornk Thank you for letting me know!", "@gzmkl  FYI, #43504 is merged.", "@gzmkl Can you please resolve conflicts? Thanks!", "@gbaned  Because there are a lot of changes in a recent merged PR (related to MKL native format support) on the same source file mkl_fused_batch_norm_op.cc, I am in the process of getting approval of an INTERNAL PR. Once it is done, I will submit a new PR to replace this one.  Thanks!", "@penpornk @gbaned  Hi, I have done any round of cleanup after rebase with PR #43504 changes. So this one shall be fine and there is no need to submit a new PR.  Thanks!"]}, {"number": 43546, "title": "Problem In tensorflow-gpu with error \"Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB with freed_by_count=0.\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS: Windows 10 Home with 8GB ram, NVIDIA MX250 2GB graphic card.- TensorFlow installed from (source or binary):\r\n- TensorFlow version :2.2.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: CUDA 10.2 and CUDNN: 8.0.3.33 i.e. for cuda 10.1\r\n- GPU model and memory: NVIDIA MX250 with 2GB Graphic\r\n\r\n**Describe the current behavior**\r\nMy tensorflow-gpu was working fine. Once I have reinstall my anaconda and install tensorflow-gpu again after that\r\nwhen every I try to train any model on tensorflow-gpu It always gives me this error:\r\n_Relying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n2020-09-24 21:35:23.361799: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-09-24 21:35:23.363348: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-09-24 21:35:23.627463: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available._\r\n**Describe the expected behavior**\r\nI was Expecting that my train will work fine.\r\n**Standalone code to reproduce the issue**\r\n\r\nfrom tensorflow.keras.applications import VGG16\r\nfrom tensorflow.keras.layers import Dense, Flatten,Conv2D,MaxPool2D,AvgPool2D,Dropout\r\nfrom tensorflow.keras.models import Model,Sequential\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom glob import glob\r\nimport matplotlib.pyplot as plt\r\n\r\ntrain_path=\"E:/AI-Application-Implementation/trained_model/Classification/Cifar-10/data/train\"\r\ntest_path=\"E:/AI-Application-Implementation/trained_model/Classification/Cifar-10/data/test\"\r\n\r\nfolders=glob(\"E:/All Data Set/CIFAR10/train/*\")\r\n\r\ndatagen=ImageDataGenerator(rotation_range=0.5,\r\n                                 brightness_range=[0.2,0.5],\r\n                                 zoom_range=[0.1,0.8],\r\n                                 horizontal_flip=True,\r\n                                 validation_split=0.2,\r\n                                 rescale=1./255)\r\n\r\ntrain=datagen.flow_from_directory(directory=train_path,\r\n                                        target_size=(256,256),\r\n                                        # color_mode=\"grayscale\",\r\n                                        shuffle=True,\r\n                                        class_mode='categorical',\r\n                                        subset='training')\r\n\r\ntest=datagen.flow_from_directory(directory=train_path,\r\n                                        target_size=(256,256),\r\n                                        # color_mode=\"grayscale\",\r\n                                        shuffle=True,\r\n                                        class_mode='categorical',\r\n                                        subset='validation')\r\n\r\nmodel=Sequential()\r\n\r\nmodel.add(Conv2D(filters=32,kernel_size=(3,3),input_shape=(256,256,3),activation='relu'))\r\nmodel.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu'))\r\nmodel.add(MaxPool2D(pool_size=(3,3)))\r\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\r\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\r\nmodel.add(MaxPool2D(pool_size=(3,3)))\r\nmodel.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))\r\nmodel.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))\r\nmodel.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))\r\nmodel.add(MaxPool2D(pool_size=(3,3)))\r\nmodel.add(AvgPool2D(pool_size=(6,6)))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(units=64,activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(units=10,activation='softmax'))\r\nmodel.summary()\r\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\r\n\r\nhistory=model.fit(train,validation_data=test,epochs=5,steps_per_epoch=len(train),validation_steps=len(test))\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nMy full console wile running my code:\r\n2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\nFound 40000 images belonging to 10 classes.\r\n\r\n2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n\r\n2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\nFound 10000 images belonging to 10 classes.\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param  \r\n=================================================================\r\nconv2d (Conv2D)              (None, 254, 254, 32)      896       \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 252, 252, 32)      9248      \r\n_________________________________________________________________\r\nmax_pooling2d (MaxPooling2D) (None, 84, 84, 32)        0         \r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 82, 82, 64)        18496     \r\n_________________________________________________________________\r\nconv2d_3 (Conv2D)            (None, 80, 80, 64)        36928     \r\n_________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2 (None, 26, 26, 64)        0         \r\n_________________________________________________________________\r\nconv2d_4 (Conv2D)            (None, 24, 24, 128)       73856     \r\n_________________________________________________________________\r\nconv2d_5 (Conv2D)            (None, 22, 22, 128)       147584    \r\n_________________________________________________________________\r\nconv2d_6 (Conv2D)            (None, 20, 20, 128)       147584    \r\n_________________________________________________________________\r\nmax_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \r\n_________________________________________________________________\r\naverage_pooling2d (AveragePo (None, 1, 1, 128)         0         \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 128)               0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 64)                8256      \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 64)                0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 10)                650       \r\n\r\nTotal params: 443,498\r\nTrainable params: 443,498\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n\r\n2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\nEpoch 1/5\r\n\r\n2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n\r\n2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n\r\n2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:22.184494: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\nRelying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n\r\n2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:22.184494: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\nRelying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n2020-09-24 21:35:23.361799: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-09-24 21:35:23.363348: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-09-24 21:35:23.627463: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n\r\n2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-24 21:35:22.184494: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\nRelying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n2020-09-24 21:35:23.361799: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-09-24 21:35:23.363348: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-09-24 21:35:23.627463: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-09-24 21:35:24.619275: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 796.38MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n", "comments": ["If any one can help me or have any suggestions then please comment or contact me on:                                GitHub: https://github.com/gauravrajwada\r\nMedium: https://medium.com/@GauravRajwada\r\nLinkedIn: https://www.linkedin.com/in/gaurav-singh-b90369191/\r\nInstagram: https://www.instagram.com/gauravrajwada/\r\nTwitter: https://twitter.com/GauravRajwada s", "@GauravRajwada,\r\nTry setting a hard limit on the total GPU memory as shown in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and let us know if it works.\r\n\r\nAlso, take a look at similar issue [#35264](https://github.com/tensorflow/tensorflow/issues/35264) and check if it helps. Thanks!", "> @GauravRajwada,\r\n> Try setting a hard limit on the total GPU memory as shown in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and let us know if it works.\r\n> \r\n> Also, take a look at similar issue [#35264](https://github.com/tensorflow/tensorflow/issues/35264) and check if it helps. Thanks!\r\n\r\nNo it doent solve problem.\r\nIt gives this error:\r\n```\r\n\r\n2020-09-26 14:00:53.182812: I tensorflow/stream_executor/cuda/cuda_driver.cc:775] failed to allocate 512.00M (536870912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-09-26 14:00:53.183308: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 202.33MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-09-26 14:00:54.044314: I tensorflow/stream_executor/cuda/cuda_driver.cc:775] failed to allocate 512.00M (536870912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-09-26 14:00:54.044855: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 466.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-09-26 14:00:54.481150: I tensorflow/stream_executor/cuda/cuda_driver.cc:775] failed to allocate 512.00M (536870912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-09-26 14:00:54.941318: I tensorflow/stream_executor/cuda/cuda_driver.cc:775] failed to allocate 512.00M (536870912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-09-26 14:00:55.468457: I tensorflow/stream_executor/cuda/cuda_driver.cc:775] failed to allocate 512.00M (536870912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-09-26 14:00:55.588451: I tensorflow/stream_executor/cuda/cuda_driver.cc:775] failed to allocate 512.00M (536870912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-09-26 14:00:55.985935: I tensorflow/stream_executor/cuda/cuda_driver.cc:775] failed to allocate 512.00M (536870912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-09-26 14:00:56.097583: I tensorflow/stream_executor/cuda/cuda_driver.cc:775] failed to allocate 512.00M (536870912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n```", "> **System information**\r\n> \r\n> * Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n> * OS: Windows 10 Home with 8GB ram, NVIDIA MX250 2GB graphic card.- TensorFlow installed from (source or binary):\r\n> * TensorFlow version :2.2.0\r\n> * Python version: 3.6\r\n> * CUDA/cuDNN version: CUDA 10.2 and CUDNN: 8.0.3.33 i.e. for cuda 10.1\r\n> * GPU model and memory: NVIDIA MX250 with 2GB Graphic\r\n> \r\n> **Describe the current behavior**\r\n> My tensorflow-gpu was working fine. Once I have reinstall my anaconda and install tensorflow-gpu again after that\r\n> when every I try to train any model on tensorflow-gpu It always gives me this error:\r\n> _Relying on driver to perform ptx compilation. Modify $PATH to customize ptxas location. This message will be only logged once. 2020-09-24 21:35:23.361799: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2020-09-24 21:35:23.363348: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2020-09-24 21:35:23.627463: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available._\r\n> **Describe the expected behavior**\r\n> I was Expecting that my train will work fine.\r\n> **Standalone code to reproduce the issue**\r\n> \r\n> from tensorflow.keras.applications import VGG16\r\n> from tensorflow.keras.layers import Dense, Flatten,Conv2D,MaxPool2D,AvgPool2D,Dropout\r\n> from tensorflow.keras.models import Model,Sequential\r\n> from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n> from glob import glob\r\n> import matplotlib.pyplot as plt\r\n> \r\n> train_path=\"E:/AI-Application-Implementation/trained_model/Classification/Cifar-10/data/train\"\r\n> test_path=\"E:/AI-Application-Implementation/trained_model/Classification/Cifar-10/data/test\"\r\n> \r\n> folders=glob(\"E:/All Data Set/CIFAR10/train/*\")\r\n> \r\n> datagen=ImageDataGenerator(rotation_range=0.5,\r\n> brightness_range=[0.2,0.5],\r\n> zoom_range=[0.1,0.8],\r\n> horizontal_flip=True,\r\n> validation_split=0.2,\r\n> rescale=1./255)\r\n> \r\n> train=datagen.flow_from_directory(directory=train_path,\r\n> target_size=(256,256),\r\n> # color_mode=\"grayscale\",\r\n> shuffle=True,\r\n> class_mode='categorical',\r\n> subset='training')\r\n> \r\n> test=datagen.flow_from_directory(directory=train_path,\r\n> target_size=(256,256),\r\n> # color_mode=\"grayscale\",\r\n> shuffle=True,\r\n> class_mode='categorical',\r\n> subset='validation')\r\n> \r\n> model=Sequential()\r\n> \r\n> model.add(Conv2D(filters=32,kernel_size=(3,3),input_shape=(256,256,3),activation='relu'))\r\n> model.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu'))\r\n> model.add(MaxPool2D(pool_size=(3,3)))\r\n> model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\r\n> model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\r\n> model.add(MaxPool2D(pool_size=(3,3)))\r\n> model.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))\r\n> model.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))\r\n> model.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))\r\n> model.add(MaxPool2D(pool_size=(3,3)))\r\n> model.add(AvgPool2D(pool_size=(6,6)))\r\n> model.add(Flatten())\r\n> model.add(Dense(units=64,activation='relu'))\r\n> model.add(Dropout(0.5))\r\n> model.add(Dense(units=10,activation='softmax'))\r\n> model.summary()\r\n> model.compile(loss='sparse_categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\r\n> \r\n> history=model.fit(train,validation_data=test,epochs=5,steps_per_epoch=len(train),validation_steps=len(test))\r\n> \r\n> **Other info / logs** Include any logs or source code that would be helpful to\r\n> My full console wile running my code:\r\n> 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> Found 40000 images belonging to 10 classes.\r\n> \r\n> 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> \r\n> 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> 2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> 2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n> 2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> 2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> Found 10000 images belonging to 10 classes.\r\n> Model: \"sequential\"\r\n> \r\n> # Layer (type) Output Shape Param\r\n> conv2d (Conv2D) (None, 254, 254, 32) 896\r\n> \r\n> conv2d_1 (Conv2D) (None, 252, 252, 32) 9248\r\n> \r\n> max_pooling2d (MaxPooling2D) (None, 84, 84, 32) 0\r\n> \r\n> conv2d_2 (Conv2D) (None, 82, 82, 64) 18496\r\n> \r\n> conv2d_3 (Conv2D) (None, 80, 80, 64) 36928\r\n> \r\n> max_pooling2d_1 (MaxPooling2 (None, 26, 26, 64) 0\r\n> \r\n> conv2d_4 (Conv2D) (None, 24, 24, 128) 73856\r\n> \r\n> conv2d_5 (Conv2D) (None, 22, 22, 128) 147584\r\n> \r\n> conv2d_6 (Conv2D) (None, 20, 20, 128) 147584\r\n> \r\n> max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128) 0\r\n> \r\n> average_pooling2d (AveragePo (None, 1, 1, 128) 0\r\n> \r\n> flatten (Flatten) (None, 128) 0\r\n> \r\n> dense (Dense) (None, 64) 8256\r\n> \r\n> dropout (Dropout) (None, 64) 0\r\n> \r\n> dense_1 (Dense) (None, 10) 650\r\n> \r\n> Total params: 443,498\r\n> Trainable params: 443,498\r\n> Non-trainable params: 0\r\n> \r\n> 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> 2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> 2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n> 2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> 2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0\r\n> 2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N\r\n> 2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n> 2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> 2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n> Epoch 1/5\r\n> \r\n> 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> 2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> 2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n> 2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> 2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0\r\n> 2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N\r\n> 2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n> 2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> 2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n> 2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> \r\n> 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> 2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> 2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n> 2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> 2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0\r\n> 2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N\r\n> 2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n> 2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> 2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n> 2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> \r\n> 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> 2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> 2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n> 2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> 2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0\r\n> 2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N\r\n> 2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n> 2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> 2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n> 2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:22.184494: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\n> Relying on driver to perform ptx compilation.\r\n> Modify $PATH to customize ptxas location.\r\n> This message will be only logged once.\r\n> \r\n> 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> 2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> 2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n> 2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> 2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0\r\n> 2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N\r\n> 2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n> 2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> 2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n> 2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:22.184494: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\n> Relying on driver to perform ptx compilation.\r\n> Modify $PATH to customize ptxas location.\r\n> This message will be only logged once.\r\n> 2020-09-24 21:35:23.361799: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n> 2020-09-24 21:35:23.363348: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n> 2020-09-24 21:35:23.627463: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n> \r\n> 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> 2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> 2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n> 2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> 2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> 2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> 2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0\r\n> 2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N\r\n> 2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n> 2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> 2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n> 2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> 2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020-09-24 21:35:22.184494: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\n> Relying on driver to perform ptx compilation.\r\n> Modify $PATH to customize ptxas location.\r\n> This message will be only logged once.\r\n> 2020-09-24 21:35:23.361799: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n> 2020-09-24 21:35:23.363348: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n> 2020-09-24 21:35:23.627463: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n> 2020-09-24 21:35:24.619275: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 796.38MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n\r\ndid you solve that ? I have same \u0131ssue... :(", "> > **System information**\r\n> > \r\n> > * Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n> > * OS: Windows 10 Home with 8GB ram, NVIDIA MX250 2GB graphic card.- TensorFlow installed from (source or binary):\r\n> > * TensorFlow version :2.2.0\r\n> > * Python version: 3.6\r\n> > * CUDA/cuDNN version: CUDA 10.2 and CUDNN: 8.0.3.33 i.e. for cuda 10.1\r\n> > * GPU model and memory: NVIDIA MX250 with 2GB Graphic\r\n> > \r\n> > **Describe the current behavior**\r\n> > My tensorflow-gpu was working fine. Once I have reinstall my anaconda and install tensorflow-gpu again after that\r\n> > when every I try to train any model on tensorflow-gpu It always gives me this error:\r\n> > _Relying on driver to perform ptx compilation. Modify $PATH to customize ptxas location. This message will be only logged once. 2020-09-24 21:35:23.361799: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2020-09-24 21:35:23.363348: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2020-09-24 21:35:23.627463: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available._\r\n> > **Describe the expected behavior**\r\n> > I was Expecting that my train will work fine.\r\n> > **Standalone code to reproduce the issue**\r\n> > from tensorflow.keras.applications import VGG16\r\n> > from tensorflow.keras.layers import Dense, Flatten,Conv2D,MaxPool2D,AvgPool2D,Dropout\r\n> > from tensorflow.keras.models import Model,Sequential\r\n> > from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n> > from glob import glob\r\n> > import matplotlib.pyplot as plt\r\n> > train_path=\"E:/AI-Application-Implementation/trained_model/Classification/Cifar-10/data/train\"\r\n> > test_path=\"E:/AI-Application-Implementation/trained_model/Classification/Cifar-10/data/test\"\r\n> > folders=glob(\"E:/All Data Set/CIFAR10/train/*\")\r\n> > datagen=ImageDataGenerator(rotation_range=0.5,\r\n> > brightness_range=[0.2,0.5],\r\n> > zoom_range=[0.1,0.8],\r\n> > horizontal_flip=True,\r\n> > validation_split=0.2,\r\n> > rescale=1./255)\r\n> > train=datagen.flow_from_directory(directory=train_path,\r\n> > target_size=(256,256),\r\n> > # color_mode=\"grayscale\",\r\n> > shuffle=True,\r\n> > class_mode='categorical',\r\n> > subset='training')\r\n> > test=datagen.flow_from_directory(directory=train_path,\r\n> > target_size=(256,256),\r\n> > # color_mode=\"grayscale\",\r\n> > shuffle=True,\r\n> > class_mode='categorical',\r\n> > subset='validation')\r\n> > model=Sequential()\r\n> > model.add(Conv2D(filters=32,kernel_size=(3,3),input_shape=(256,256,3),activation='relu'))\r\n> > model.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu'))\r\n> > model.add(MaxPool2D(pool_size=(3,3)))\r\n> > model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\r\n> > model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\r\n> > model.add(MaxPool2D(pool_size=(3,3)))\r\n> > model.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))\r\n> > model.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))\r\n> > model.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))\r\n> > model.add(MaxPool2D(pool_size=(3,3)))\r\n> > model.add(AvgPool2D(pool_size=(6,6)))\r\n> > model.add(Flatten())\r\n> > model.add(Dense(units=64,activation='relu'))\r\n> > model.add(Dropout(0.5))\r\n> > model.add(Dense(units=10,activation='softmax'))\r\n> > model.summary()\r\n> > model.compile(loss='sparse_categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\r\n> > history=model.fit(train,validation_data=test,epochs=5,steps_per_epoch=len(train),validation_steps=len(test))\r\n> > **Other info / logs** Include any logs or source code that would be helpful to\r\n> > My full console wile running my code:\r\n> > 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > Found 40000 images belonging to 10 classes.\r\n> > 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> > 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> > 2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> > pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> > 2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> > 2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2\r\n> > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> > 2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> > 2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n> > 2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> > pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> > 2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> > Found 10000 images belonging to 10 classes.\r\n> > Model: \"sequential\"\r\n> > # Layer (type) Output Shape Param\r\n> > conv2d (Conv2D) (None, 254, 254, 32) 896\r\n> > conv2d_1 (Conv2D) (None, 252, 252, 32) 9248\r\n> > max_pooling2d (MaxPooling2D) (None, 84, 84, 32) 0\r\n> > conv2d_2 (Conv2D) (None, 82, 82, 64) 18496\r\n> > conv2d_3 (Conv2D) (None, 80, 80, 64) 36928\r\n> > max_pooling2d_1 (MaxPooling2 (None, 26, 26, 64) 0\r\n> > conv2d_4 (Conv2D) (None, 24, 24, 128) 73856\r\n> > conv2d_5 (Conv2D) (None, 22, 22, 128) 147584\r\n> > conv2d_6 (Conv2D) (None, 20, 20, 128) 147584\r\n> > max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128) 0\r\n> > average_pooling2d (AveragePo (None, 1, 1, 128) 0\r\n> > flatten (Flatten) (None, 128) 0\r\n> > dense (Dense) (None, 64) 8256\r\n> > dropout (Dropout) (None, 64) 0\r\n> > dense_1 (Dense) (None, 10) 650\r\n> > Total params: 443,498\r\n> > Trainable params: 443,498\r\n> > Non-trainable params: 0\r\n> > 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> > 2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> > pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> > 2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> > 2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2\r\n> > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> > 2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> > 2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n> > 2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> > pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> > 2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> > 2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> > 2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0\r\n> > 2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N\r\n> > 2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n> > 2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> > 2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n> > Epoch 1/5\r\n> > 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> > 2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> > pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> > 2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> > 2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2\r\n> > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> > 2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> > 2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n> > 2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> > pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> > 2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> > 2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> > 2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0\r\n> > 2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N\r\n> > 2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n> > 2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> > 2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n> > 2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> > 2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> > pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> > 2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> > 2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2\r\n> > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> > 2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> > 2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n> > 2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> > pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> > 2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> > 2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> > 2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0\r\n> > 2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N\r\n> > 2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n> > 2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> > 2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n> > 2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> > 2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> > pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> > 2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> > 2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2\r\n> > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> > 2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> > 2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n> > 2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> > pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> > 2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> > 2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> > 2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0\r\n> > 2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N\r\n> > 2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n> > 2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> > 2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n> > 2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:22.184494: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\n> > Relying on driver to perform ptx compilation.\r\n> > Modify $PATH to customize ptxas location.\r\n> > This message will be only logged once.\r\n> > 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> > 2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> > pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> > 2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> > 2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2\r\n> > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> > 2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> > 2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n> > 2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> > pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> > 2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> > 2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> > 2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0\r\n> > 2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N\r\n> > 2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n> > 2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> > 2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n> > 2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:22.184494: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\n> > Relying on driver to perform ptx compilation.\r\n> > Modify $PATH to customize ptxas location.\r\n> > This message will be only logged once.\r\n> > 2020-09-24 21:35:23.361799: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n> > 2020-09-24 21:35:23.363348: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n> > 2020-09-24 21:35:23.627463: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n> > 2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n> > 2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> > pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> > 2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> > 2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2\r\n> > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> > 2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> > 2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n> > 2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\n> > pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1\r\n> > coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\r\n> > 2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n> > 2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> > 2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> > 2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0\r\n> > 2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N\r\n> > 2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n> > 2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> > 2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce MX250, Compute Capability 6.1\r\n> > 2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020-09-24 21:35:22.184494: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\n> > Relying on driver to perform ptx compilation.\r\n> > Modify $PATH to customize ptxas location.\r\n> > This message will be only logged once.\r\n> > 2020-09-24 21:35:23.361799: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n> > 2020-09-24 21:35:23.363348: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n> > 2020-09-24 21:35:23.627463: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n> > 2020-09-24 21:35:24.619275: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 796.38MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n> \r\n> did you solve that ? I have same \u0131ssue... :(\r\n\r\nNo till now I am unable to resolve", "I am having the same issue. \r\n\"Allocator (GPU_0_bfc) ran out of memory trying to allocate 80.38MiB (rounded to 84279296).  Current allocation summary follows.\"\r\nIt happens when I train my model multiple times in a loop (e.g., K-Folds). Works fine up tp 5-6 folds then throws up this error. Batch size does not seem to be an issue since it works fine for 5 or 6 folds and then crashes for the next fold.   ", "CC @imintz ", "@GauravRajwada Could you clarify the failure symptom? Does the program hang/take longer than expected or are you observing a crash?\r\n\r\nThe warning you're seeing in the logs indicates that TF is failing to allocate scratch space for the conv op, but that shouldn't necessarily cause a hard failure.", "@imintz Hey, I also see this warning. For me it does not cause a halt, the network trains fine. I cannot judge if it is slower that it should be. Just to clarify, tf fails to allocate CPU memory, which is bottlenecking the GPU operations? ", "Hi, I was getting that error and mine didn't continue working. It exited the code. Further digging lead me to try to put the training code on the CPU with this code:\r\ntf.debugging.set_log_device_placement(True)\r\nwith tf.device('/CPU:0'):\r\nfound here:   https://www.tensorflow.org/guide/gpu#manual_device_placement\r\nmeant the code ran without exit. \r\nWhat I cannot figure out is why the training steps now run at a blazingly fast time? I ran this code without GPU or CUDA/cuDNN installed. I installed it and got the warning, the code exited. Now for the training steps ? (I'm new at this)  \r\nfor m in range(steps_per_epoch):\r\n      step += 1\r\n      train_step(image)\r\n      print(\".\", end='')\r\n\r\nIt works fine, actually even much faster than before!   ??\r\n\r\n", "i am also getting the same error .", "I have two conda environments. \r\n\r\n- tf-conda: Installed tensorfow using conda\r\n- tf-pip: Installed tf with pip and all installed CUDA and CUDNN manually\r\n\r\nI get this error only on the tf-pip env and not the tf-conda. This appears to show that the error is more linked with a software installation than the implementation or hardware. (Hope it helps to debug, please let me know which commands output could be helpful to solve).", "@adi1220, In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43546\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43546\">No</a>\n"]}, {"number": 43545, "title": "Update the CMSIS-NN glue code to support int16 for softmax.", "body": "Add int16 support to the CMSIS-NN version of softmax.\r\n\r\nThe changes are along the same lines as what was done for the reference kernel with #43320.\r\n\r\nFixes #43543 ", "comments": ["@njeffrie does it look ok to you?", "This is mostly copied over from the changes in the reference kernel from https://github.com/tensorflow/tensorflow/pull/43320/, right? Can you mention that in the pull request description? And do mention any notable differences from the reference implementation.\r\n\r\nWe would like to change how the optimized kernels are implemented to allow for better code sharing but don't have a timeline for that. So let's update the commit description, resolve any conflicts and then we can get this merged.", "@jenselofsson, @freddan80  Can you please check @advaitjain's comments and keep us posted ? Thanks!", "This is partly fixing: https://github.com/tensorflow/tensorflow/issues/44455"]}, {"number": 43544, "title": "Update the CMSIS-NN glue code to support int16 for softmax.", "body": "Add int16 support to the CMSIS-NN version of softmax.\r\nFixes #43543 ", "comments": ["Disregard this PR"]}, {"number": 43543, "title": "Add int16 support to the CMSIS-NN version of softmax", "body": "@tensorflow/micro\r\n\r\nThe softmax reference kernel have been updated with int16 support, and the CMSIS-NN version should to updated with int16 support as well.\r\n\r\n", "comments": ["Sorry for the confusion, 43545 is the correct PR", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43543\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43543\">No</a>\n"]}, {"number": 43542, "title": "tf.numpy_function() does not support symbolic Tensors", "body": "**System information**\r\n- I have written custom code (see below)\r\n- Red Hat Enterprise Linux Server release 7.7 (Maipo)\r\n- TensorFlow installed from: binary (pip)\r\n- TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\nCalling `tf.numpy_function()` with symbolic Tensor crashes (see below). This prevents `tf.numpy_function()` from being directly used when building a Keras Model with the Functional API.\r\n\r\n**Describe the expected behavior**\r\nCalling `tf.numpy_function()` with symbolic Tensor should return the symbolic Tensors which are the result of the operation.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\n>>> import tensorflow as tf, numpy as np\r\n>>> (tf.__version__, np.__version__)\r\n('2.3.0', '1.18.5')\r\n\r\n# Create a symbolic Tensor.\r\n>>> x = tf.keras.layers.Input(shape=(2,))\r\n>>> x\r\n<tf.Tensor 'input_2:0' shape=(None, 2) dtype=float32>\r\n\r\n# Calling it on tf.sin() gives a symbolic Tensor as output.\r\n>>> tf.sin(x)\r\n<tf.Tensor 'Sin:0' shape=(None, 2) dtype=float32>\r\n\r\n# Calling it on tf.numpy_function() with np.sin() fails:\r\n>>> tf.numpy_function(np.sin, [x], [tf.float32])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 632, in numpy_function\r\n    return py_func_common(func, inp, Tout, stateful=True, name=name)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 519, in py_func_common\r\n    result = func(*[np.array(x) for x in inp])\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 519, in <listcomp>\r\n    result = func(*[np.array(x) for x in inp])\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 848, in __array__\r\n    \" a NumPy call, which is not supported\".format(self.name))\r\nNotImplementedError: Cannot convert a symbolic Tensor (input_2:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n```\r\n\r\n**Other info / logs**\r\n`tf.numpy_function()` works as expected under other circumstances.\r\n```python\r\n# With an eager Tensor.\r\n>>> x = tf.constant([0., 1.])\r\n>>> x\r\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 1.], dtype=float32)>\r\n>>> tf.sin(x)\r\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.        , 0.84147096], dtype=float32)>\r\n>>> tf.numpy_function(np.sin, [x], [tf.float32])\r\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.        , 0.84147096], dtype=float32)>\r\n```\r\n```python\r\n# When eager execution is disabled.\r\n>>> import tensorflow as tf, numpy as np\r\n>>> tf.python.framework.ops.disable_eager_execution()\r\n>>> x = tf.keras.layers.Input(shape=(2,))\r\n>>> tf.numpy_function(np.sin, [x], [tf.float32])\r\n[<tf.Tensor 'PyFunc:0' shape=<unknown> dtype=float32>]\r\n```\r\n```python\r\n# When wrapping tf.numpy_function() inside a Keras Lambda Layer.\r\n>>> import tensorflow as tf, numpy as np\r\n>>> x = tf.keras.layers.Input(shape=(2,))\r\n>>> tf.keras.layers.Lambda(lambda t: tf.numpy_function(np.sin, [t], [tf.float32]))(x)\r\n[<tf.Tensor 'lambda/PyFunc:0' shape=<unknown> dtype=float32>]\r\n```", "comments": ["```\r\ntf.py_function` and `tf.numpy_function` are very similar, except that \r\n`tf.numpy_function` takes numpy arrays, and not `tf.Tensor`s. If you want the\r\nfunction to contain `tf.Tensors`, and have any TensorFlow operations executed\r\nin the function be differentiable, please use `tf.py_function`.\r\n```\r\nIt was in the documentation. `tf.keras.layers.Input()` will work on `tf.py_function`.", "@loic-ehrhardt \r\nPlease refer to this issue and let us know: #43154 ", "Thanks @meSajied and @Saduf2019 for your replies.\r\n\r\n@meSajied\r\n```\r\ntf.py_function` and `tf.numpy_function` are very similar,\r\nexcept that `tf.numpy_function` takes numpy arrays, and not `tf.Tensor`s.\r\nIf you want the function to contain `tf.Tensors`, [...] please use `tf.py_function`.\r\n```\r\nI do not want the function to contain `tf.Tensors`. I would like to use `np.sin()`, *not* `tf.sin()` (obviously, the numpy function I actually want to use is more complex but I use `np.sin()` here as minimal example to show the bug).\r\nMoreover, `tf.py_function()` also fails with `tf.keras.layers.Input()`:\r\n```python\r\n>>> import tensorflow as tf\r\n>>> x = tf.keras.layers.Input(shape=(2,))\r\n>>> tf.py_function(tf.sin, [x], [tf.float32])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 457, in eager_py_func\r\n    func=func, inp=inp, Tout=Tout, eager=True, name=name)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 342, in _internal_py_func\r\n    name=name)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py\", line 55, in eager_py_func\r\n    ctx=_ctx)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py\", line 96, in eager_py_func_eager_fallback\r\n    _attr_Tin, input = _execute.convert_to_mixed_eager_tensors(input, ctx)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 291, in convert_to_mixed_eager_tensors\r\n    types = [t._datatype_enum() for t in v]  # pylint: disable=protected-access\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 283, in <listcomp>\r\n    types = [t._datatype_enum() for t in v]  # pylint: disable=protected-access\r\nAttributeError: 'Tensor' object has no attribute '_datatype_enum'\r\n```\r\n\r\n@Saduf2019\r\nIt looks like #43154 refers to a warning about AutoGraph not being able to transform some function and then falling back to running it as is. This seems unrelated to my issue. In my case, I have a plain crash. Moreover, I see in the release notes (https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md#release-220) that since tensorflow 2.2.0, `AutoGraph no longer converts functions passed to tf.py_function, tf.py_func and tf.numpy_function` (by the way I tried with TF 2.1 and it also crashes).\r\n", "Hi @Saduf2019. Do you have further insights into this or do you think this should be escalated? It looks like #43154  is closed.", "@loic-ehrhardt I am able to reproduce the error with `TF2.3`. However, there is no error when `tf-nightly` was used. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/c94aec48f9fbd5df7d781a5423206efe/untitled54.ipynb). Thanks!\r\n", "@jvishnuvardhan Thanks for your reply. This seems indeed fixed in the new TF version. I think we can close this ticket then. My goal was to actually use `tf.numpy_function()` with `tf.custom_gradient()` to compute both the output and the gradient with numpy. This is still not working with tf_nightly. I opened the new issue #43916, would you mind looking into this too?\r\nBests, Lo\u00efc.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43542\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43542\">No</a>\n"]}, {"number": 43541, "title": "NNAPI Delegate BUG", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Xiaomi 9Pro (SnapDragon 855+), Android Q\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nI have several Questions.\r\n\r\n**1.** When I run the model  **mobilenet_v1_1.0_224_quant.tflite** in tflite examples, and I select NNAPI device, there are log output as followings in LOGCAT. All layers of the model are supported by NNAPI, so what does these outputs mean? \r\n```\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {AVERAGE_POOLING_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {SOFTMAX, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.031 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {AVERAGE_POOLING_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {SOFTMAX, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {AVERAGE_POOLING_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.\r\n2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {SOFTMAX, TENSOR_QUANT8} is not supported.\r\n```\r\n\r\n**2.**  When I run the model **efficientnet-lite0-int8.tflite** in the examples, I find that the performance using NNAPI is much slower than using CPU. However, all the layers are supported by NNAPI. So Why ?\r\n\r\n**3.** About NNAPI.   The following two APIs can not be set in the meantime, otherwise the error log occurs. So which API is valid\uff0cor both are valid but can not be set in the meantime?\r\n```\r\n tfliteOptions.addDelegate(nnApiDelegate);\r\n tfliteOptions.setUseNNAPI(true);\r\n\r\nInternal error: Failed to apply delegate: ModifyGraphWithDelegate is disallowed when graph is immutable.\r\n```\r\n\r\n**4.** How can I set GPU inference using fp16 or fp32? I find the following api has been deprecated.\r\n```\r\nsetAllowFp16PrecisionForFp32(boolean allow);\r\n```\r\n\r\n**5.** I have tried to quantize my model to 8bit using the following code. But I find that some conv op's weights are still fp32, some conv op's weights are int8. How can I solve it ?\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflite_quant_model = converter.convert()\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n**1.** solve the not supported log output in logcat.\r\n**2.** NNAPI 's performance is not slower than cpu.\r\n**3.** use proper API to enable NNAPI delegate.\r\n**4.** I can set to use fp16 or fp32 in GPU delegate.\r\n**5.** quantize model to 8bit successfully.\r\n\r\nThanks very much, looking forward to your reply.\r\n", "comments": ["Can you try with a recent TFlite release?", "@bhack \r\nYeah\uff0c I have tried to update tensorflow lite AAR to 2.3.0 in build.gradle dependency, which is the lastest version. But that did not solve the problems.\r\n```\r\ndependencies {\r\n    ......\r\n   \r\n    implementation('org.tensorflow:tensorflow-lite:2.3.0')\r\n    implementation('org.tensorflow:tensorflow-lite-gpu:2.3.0')\r\n}\r\n```", "@bhack \r\nI just test the second question about **efficientnet-lite0-int8.tflite**'s performance on Xiaomi10 (SnapDragon 865). I found that NNAPI runs  faster than cpu. Maybe it only works on several clips \uff1f\r\n\r\nBut the other 4 questions still exist.", "1. that's from an underlying NNAPI driver. When you delegate your model to NNAPI, the NNAPI runtime asks all the drivers to decide which driver / hardware is the right one to dispatch the an op to run on it.\r\n2. you can try \r\n```\r\nadb shell setprop debug.nn.vlog 1\r\n```\r\nand then `adb logcat | grep -i best` to see how your model is handled by NNAPI. Check https://developer.android.com/ndk/guides/neuralnetworks for more NNAPI related information\r\n\r\n3. use either one of them, not both.\r\n4. check GPU delegate option https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/java/src/main/java/org/tensorflow/lite/gpu/GpuDelegate.java#L54-L64\r\n5. you may want to read related guides first, https://www.tensorflow.org/lite/performance/model_optimization\r\n", "@freedomtan  \r\nThanks for your reply. And I have another question. I have printed the device list on Xiaomi 10 (SnapDragon 865) and found have five devices.\r\n\r\nWhat does the device **qti-default** mean?\r\nnnapi-reference   -> cpu\r\nqti-gpu    ->   hardware accelator\r\nqti-dsp     ->   hardware accelator\r\nqti-hta      ->   hardware  accelator\r\n**qti-default**  -> what is this device?\r\n\r\n\r\n**qti-default** in this situation means GPU device ?\r\n```\r\n2020-09-25 18:41:04.828 32446-32446/* I/ExecutionPlan: ModelBuilder::partitionTheWork: deviceCount = 5, operationCount = 32\r\n2020-09-25 18:41:04.834 32446-32446/* I/ExecutionPlan: Device qti-dsp can't do operation CONV_2D\r\n2020-09-25 18:41:04.834 32446-32446/* I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-25 18:41:04.834 32446-32446/* I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-25 18:41:04.834 32446-32446/* I/ExecutionPlan: Device qti-dsp can't do operation MAX_POOL_2D\r\n2020-09-25 18:41:04.834 32446-32446/* I/ExecutionPlan: Device qti-hta can't do operation MAX_POOL_2D\r\n2020-09-25 18:41:04.834 32446-32446/* I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(MAX_POOL_2D) = 0 (qti-default)\r\n2020-09-25 18:41:04.834 32446-32446/* I/ExecutionPlan: Device qti-dsp can't do operation RESIZE_BILINEAR\r\n2020-09-25 18:41:04.834 32446-32446/* I/ExecutionPlan: Device qti-hta can't do operation RESIZE_BILINEAR\r\n2020-09-25 18:41:04.834 32446-32446/* I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(RESIZE_BILINEAR) = 0 (qti-default)\r\n2020-09-25 18:41:04.834 32446-32446/* I/ExecutionPlan: Device qti-dsp can't do operation CONCATENATION\r\n2020-09-25 18:41:04.834 32446-32446/* I/ExecutionPlan: Device qti-hta can't do operation CONCATENATION\r\n2020-09-25 18:41:04.834 32446-32446/* I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONCATENATION) = 0 (qti-default)\r\n```", "@yizhaoyanbo no idea, I guess qti-default a virtual driver which dispatches models to qti-gpu, qti-dsp, or qti-hat.", "@freedomtan \r\n\r\nPhone Xiaomi10 (SnapDragon 865), tflite 2.3.0\r\n\r\nI have tried all quantization methods, including dynamic range, Integer with float fallback, Integer only and Float16 quantization. All the layers of my model is supported by NNAPI. But the performance are very strange (only the tflite.run() API time), as following:\r\n```\r\nfp32                           nnapi: 263   gpu: 128   cpu: 1214\r\ndynamic range                  nnapi: 263   gpu: 129   cpu: 649   \r\nInteger with float fallback    nnapi: 388   gpu: 574   cpu: 569    \r\nInteger only                   nnapi: 392   gpu: 577   cpu:569     \r\nFloat16 quantization           nnapi: 2599  gpu: 129   cpu: 1214     \r\n```\r\nnnapi means use nnapi delegate.  gpu means using gpu delegate.\r\n1. Why **Integer** model using nnapi is slower than **dynamic range** model?  \r\n2. why **dynamic range** model has the same speed with **fp32** model?\r\n3. Float16 quantization model on NNAPI is  the slowest, maybe it will do the dequantization, but why it is slower than fp32 model?\r\n\r\nI used the following code to add delegate\r\n```\r\n        switch (device) {\r\n            case NNAPI:\r\n                nnApiDelegate = new NnApiDelegate();\r\n                tfliteOptions.addDelegate(nnApiDelegate);\r\n                Log.v(TAG, \"use NNAPI Delegate\");\r\n                break;\r\n            case GPU:\r\n                Log.v(TAG, \"use GPU Delegate\");\r\n                gpuDelegate = new GpuDelegate();\r\n                tfliteOptions.addDelegate(gpuDelegate);\r\n                break;\r\n            case CPU:\r\n                Log.v(TAG, \"use CPU Delegate\");\r\n                break;\r\n        }\r\n        tfliteOptions.setNumThreads(4);\r\n        tflite = new Interpreter(tfliteModel, tfliteOptions);\r\n```\r\n", "for performance measurement of TFLite models:\r\n1. Try [`benchmark_model`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark). If you didn't read the TFLite [performance measurement guide](https://www.tensorflow.org/lite/performance/measurement) yet, read it.\r\n2. ops supported by NNAPI != supported by all NNAPI drivers. Remember `ModelBuilder::findBestDeviceForEachOperation` related information you saw before", "@freedomtan \r\nThanks for your help. I have read the [ performance measurement guide ](https://www.tensorflow.org/lite/performance/measurement) and do the model's benchmark. But the int8 model's performance on dsp is still slower than float32(float16) precision model on gpu. \r\n\r\n----\r\n\r\n- Firstly, I run the fp32 model using gpu delegate, both fp32 and fp16 precision are tested. Command is as following:\r\n\r\n1. fp32 precision, set gpu_precision_loss_allowed=false:\r\n```\r\nadb shell am start -S -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity  \\\r\n\t\t\t\t\t--es args '\"--graph=/data/local/tmp/tflite/model_float.tflite \\\r\n\t\t\t\t\t--num_threads=4 \\\r\n\t\t\t\t\t--use_gpu=true \\\r\n\t\t\t\t\t--gpu_precision_loss_allowed=false\"'\r\n```\r\nHere are the benchmark results from logcat:\r\n\r\n2020-09-28 16:25:50.072 11580-11580/org.tensorflow.lite.benchmark E/tflite: Average inference timings in us: Warmup: 239907, Init: 838811, **Inference: 246387** Overall max resident set size = 495.105 MB, total malloc-ed size = 0 MB, in-use allocated/mmapped size = 34.5652 MB\r\n\r\n\r\n2. fp16 precision, set gpu_precision_loss_allowed=true:\r\n```\r\nadb shell am start -S -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity  \\\r\n\t\t\t\t\t--es args '\"--graph=/data/local/tmp/tflite/model_float.tflite \\\r\n\t\t\t\t\t--num_threads=4 \\\r\n\t\t\t\t\t--use_gpu=true \\\r\n\t\t\t\t\t--gpu_precision_loss_allowed=true\"'\r\n```\r\nHere are the benchmark results from logcat:\r\n\r\n2020-09-28 16:25:17.437 11526-11526/org.tensorflow.lite.benchmark E/tflite: Average inference timings in us: Warmup: 120365, Init: 824325, **Inference: 128129** Overall max resident set size = 283.926 MB, total malloc-ed size = 0 MB, in-use allocated/mmapped size = 30.7261 MB\r\n\r\nSo on GPU delegate, fp16 precision is about 128ms and fp32 precision is about 246ms. It makes sense. \r\n  \r\n  \r\n  ----\r\n  \r\n\r\n- Secondly, I have tried the int8 model using nnapi delegate, which is quantized by **Integer only** following [this post-training quantization guide](https://www.tensorflow.org/lite/performance/post_training_quantization).  Command is as following:\r\n\r\n1. set nnapi_accelerator_name as a  blank, NNAPI will automatically select which of the available accelerators to use.\r\n```\r\nadb shell am start -S -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity  \\\r\n\t\t\t\t\t--es args '\"--graph=/data/local/tmp/tflite/model_int8.tflite \\\r\n\t\t\t\t\t--num_threads=4 \\\r\n\t\t\t\t\t--nnapi_accelerator_name=\"\" \\\r\n\t\t\t\t\t--use_nnapi=true \"'\r\n```\r\nHere are the benchmark results from logcat:\r\n\r\n2020-09-28 16:28:32.823 11644-11644/org.tensorflow.lite.benchmark E/tflite: Average inference timings in us: Warmup: 268514, Init: 162890, **Inference: 352649** Overall max resident set size = 115.781 MB, total malloc-ed size = 0 MB, in-use allocated/mmapped size = 42.0323 MB\r\n\r\n2. set nnapi_accelerator_name as \"qti-dsp\" to force running model on dsp:\r\n```\r\nadb shell am start -S -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity  \\\r\n\t\t\t\t\t--es args '\"--graph=/data/local/tmp/tflite/model_int8.tflite \\\r\n\t\t\t\t\t--num_threads=4 \\\r\n\t\t\t\t\t--nnapi_accelerator_name=\"qti-dsp\" \\\r\n\t\t\t\t\t--use_nnapi=true \"'\r\n```\r\nHere are the benchmark results from logcat:\r\n\r\n2020-09-28 16:29:52.545 11702-11702/org.tensorflow.lite.benchmark E/tflite: Average inference timings in us: Warmup: 267124, Init: 141661, **Inference: 345685** Overall max resident set size = 115.867 MB, total malloc-ed size = 0 MB, in-use allocated/mmapped size = 42.0036 MB\r\n\r\n----\r\nThe performance about auto-selected and fixed_running_dsp are almost same.\r\n\r\nFurthmore, to confirm the model's running situation on accelerators, I got the following logs from logcat. It means that all ops are running on qti-dsp and **best device: 0 = qti-default** is also qti-dsp . So I am confused that the int8 model's performance using dsp is worse than float model's performance using gpu. I have run the model's benchmark on SnapDragon 845, 855+ and 865, and they all give me the same results.  I don't know whether I am using nnapi on the right way:\r\n\r\n **set nnapi_accelerator_name as a  blank**\r\n```\r\n2020-09-28 10:00:57.561 24378-24378/org.tensorflow.lite.benchmark I/Manager: Found interface qti-default\r\n2020-09-28 10:00:57.563 24378-24378/org.tensorflow.lite.benchmark I/Manager: Found interface qti-dsp\r\n2020-09-28 10:00:57.564 24378-24378/org.tensorflow.lite.benchmark I/Manager: Found interface qti-gpu\r\n2020-09-28 10:00:57.566 24378-24378/org.tensorflow.lite.benchmark I/Manager: Found interface qti-hta\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation MAX_POOL_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(MAX_POOL_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation MAX_POOL_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(MAX_POOL_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation MAX_POOL_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(MAX_POOL_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation MAX_POOL_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(MAX_POOL_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.605 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation RESIZE_BILINEAR\r\n2020-09-28 10:00:57.606 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(RESIZE_BILINEAR) = 0 (qti-default)\r\n2020-09-28 10:00:57.606 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::partitionTheWork: only one best device: 0 = qti-default\r\n2020-09-28 10:00:57.671 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONCATENATION\r\n2020-09-28 10:00:57.671 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONCATENATION) = 0 (qti-default)\r\n2020-09-28 10:00:57.671 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.671 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.671 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.671 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.671 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.671 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.671 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation RESIZE_BILINEAR\r\n2020-09-28 10:00:57.671 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(RESIZE_BILINEAR) = 0 (qti-default)\r\n2020-09-28 10:00:57.671 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::partitionTheWork: only one best device: 0 = qti-default\r\n2020-09-28 10:00:57.698 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONCATENATION\r\n2020-09-28 10:00:57.698 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONCATENATION) = 0 (qti-default)\r\n2020-09-28 10:00:57.698 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.698 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.698 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.698 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.698 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.698 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.698 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation RESIZE_BILINEAR\r\n2020-09-28 10:00:57.698 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(RESIZE_BILINEAR) = 0 (qti-default)\r\n2020-09-28 10:00:57.698 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::partitionTheWork: only one best device: 0 = qti-default\r\n2020-09-28 10:00:57.730 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONCATENATION\r\n2020-09-28 10:00:57.730 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONCATENATION) = 0 (qti-default)\r\n2020-09-28 10:00:57.730 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.730 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.730 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.730 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.730 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.730 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.730 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation RESIZE_BILINEAR\r\n2020-09-28 10:00:57.730 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(RESIZE_BILINEAR) = 0 (qti-default)\r\n2020-09-28 10:00:57.731 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::partitionTheWork: only one best device: 0 = qti-default\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONCATENATION\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONCATENATION) = 0 (qti-default)\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation CONV_2D\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation CONV_2D\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-default)\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation SUB\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(SUB) = 0 (qti-default)\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation DEQUANTIZE\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation DEQUANTIZE\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(DEQUANTIZE) = 0 (qti-default)\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-gpu can't do operation QUANTIZE\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: Device qti-hta can't do operation QUANTIZE\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(QUANTIZE) = 0 (qti-default)\r\n2020-09-28 10:00:57.766 24378-24378/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::partitionTheWork: only one best device: 0 = qti-default\r\n```\r\n\r\n**set nnapi_accelerator_name as \"qti-dsp\" to force running model on dsp:**\r\n```\r\n2020-09-28 17:01:29.526 12310-12310/org.tensorflow.lite.benchmark I/Manager: Found interface qti-default\r\n2020-09-28 17:01:29.528 12310-12310/org.tensorflow.lite.benchmark I/Manager: Found interface qti-dsp\r\n2020-09-28 17:01:29.529 12310-12310/org.tensorflow.lite.benchmark I/Manager: Found interface qti-gpu\r\n2020-09-28 17:01:29.531 12310-12310/org.tensorflow.lite.benchmark I/Manager: Found interface qti-hta\r\n2020-09-28 17:01:29.579 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.580 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.580 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(MAX_POOL_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.580 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.580 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.580 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(MAX_POOL_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.580 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.580 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.580 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(MAX_POOL_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.580 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.580 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.580 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(MAX_POOL_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.580 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.580 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.580 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(RESIZE_BILINEAR) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.580 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::partitionTheWork: only one best device: 0 = qti-dsp\r\n2020-09-28 17:01:29.642 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONCATENATION) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.642 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.642 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.642 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(RESIZE_BILINEAR) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.642 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::partitionTheWork: only one best device: 0 = qti-dsp\r\n2020-09-28 17:01:29.668 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONCATENATION) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.668 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.668 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.668 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(RESIZE_BILINEAR) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.668 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::partitionTheWork: only one best device: 0 = qti-dsp\r\n2020-09-28 17:01:29.700 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONCATENATION) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.700 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.700 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.700 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(RESIZE_BILINEAR) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.700 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::partitionTheWork: only one best device: 0 = qti-dsp\r\n2020-09-28 17:01:29.741 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONCATENATION) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.741 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.741 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(CONV_2D) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.741 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(SUB) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.741 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(DEQUANTIZE) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.741 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::findBestDeviceForEachOperation(QUANTIZE) = 0 (qti-dsp)\r\n2020-09-28 17:01:29.741 12310-12310/org.tensorflow.lite.benchmark I/ExecutionPlan: ModelBuilder::partitionTheWork: only one best device: 0 = qti-dsp\r\n```\r\n", "It seems your model cannot be fully delegated to NNAPI (there are more than one partition). My 2 cents:\r\n1. use benchmark_model command line, you should be able to something like\r\n```\r\nExplicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ xxxx delegate kernels.\r\n```\r\n2. try per-op performance, see https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#profiling-model-operators\r\n\r\nUsually, when you cannot fully delegate your model and there is large amount of I/O between CPUs and your accelerators, you are unlikely to get good latency.", "@freedomtan \r\nYeah, I have tried the android_arm_benchmark_model on device. It seems that my model is executed by 5 delegate kernels. I think that something goes wrong with the **resizeBilinear** Op, and there are 4 resizeBilinear op in the graph.  The graphs around the  resizeBiliear op is as following.  I do not understand the **Quantize** op following the resizeBilear, because its input type and output type are all int8\r\nMaybe there is some wrong when the converter quantize the model or other steps?\r\n\r\n- resizeBilinear Op\r\n\r\n![image](https://user-images.githubusercontent.com/26928415/94437619-9637d980-01d0-11eb-89f3-f6d75f401fd1.png)\r\n\r\n---\r\n\r\n- Quantize Op\r\n\r\n![image](https://user-images.githubusercontent.com/26928415/94437310-26295380-01d0-11eb-8d05-066370198979.png)\r\n\r\n---\r\n\r\n```\r\n adb shell /data/local/tmp/tflite/android_arm_benchmark_model \\\r\n>   --num_threads=4 \\\r\n>   --graph=/data/local/tmp/tflite/model_int8.tflite \\\r\n>   --warmup_runs=1 \\\r\n>   --num_runs=50  \\\r\n>   --use_nnapi=true  \\\r\n>   --enable_op_profiling=true\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nMin num runs: [50]\r\nNum threads: [4]\r\nMin warmup runs: [1]\r\nGraph: [/data/local/tmp/tflite/model_int8.tflite]\r\nEnable op profiling: [1]\r\n#threads used for CPU inference: [4]\r\nUse NNAPI: [1]\r\nNNAPI accelerators available: [qti-default,qti-dsp,qti-gpu,qti-hta,nnapi-reference]\r\nLoaded model /data/local/tmp/tflite/model_int8.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for NNAPI.\r\nExplicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 5 delegate kernels.\r\nThe input model file size (MB): 2.00014\r\nInitialized session in 213.434ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=2 first=346041 curr=325237 min=325237 max=346041 avg=335639 std=10402\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=332731 curr=445860 min=332731 max=464676 avg=425596 std=38163\r\n\r\nInference timings in us: Init: 213434, First inference: 346041, Warmup (avg): 335639, Inference (avg): 425596\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=3.03516 overall=113.98\r\nProfiling Info for Benchmark Initialization:\r\n============================== Run Order ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n         ModifyGraphWithDelegate                    0.000         212.719         212.719        51.169%         51.169%          3108.000              1       ModifyGraphWithDelegate/0\r\n                 AllocateTensors                  111.246         202.994         101.501        48.831%        100.000%             0.000              2       AllocateTensors/0\r\n\r\n============================== Top by Computation Time ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n         ModifyGraphWithDelegate                    0.000         212.719         212.719        51.169%         51.169%          3108.000              1       ModifyGraphWithDelegate/0\r\n                 AllocateTensors                  111.246         202.994         101.501        48.831%        100.000%             0.000              2       AllocateTensors/0\r\n\r\nNumber of nodes executed: 2\r\n============================== Summary by node type ==============================\r\n                     [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]\r\n         ModifyGraphWithDelegate                1          212.719          51.169%         51.169%       3108.000              1\r\n                 AllocateTensors                1          203.001          48.831%        100.000%          0.000              2\r\n\r\nTimings (microseconds): count=1 curr=415720\r\nMemory (bytes): count=0\r\n2 nodes observed\r\n\r\n\r\n\r\n\r\n\r\nOperator-wise Profiling Info for Regular Benchmark Runs:\r\n============================== Run Order ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n                        QUANTIZE                    0.000           0.893           2.333         0.548%          0.548%             0.000              1       [Placeholder_int8]:0\r\n             TfLiteNnapiDelegate                    2.334          76.687         101.501        23.851%         24.399%             0.000              1       [network_unet/decoder/upsample_2x/ResizeBilinear, network_unet/encoder/conv_block/conv2d_1/Relu, network_unet/encoder/conv_block_1/conv2d_1/Relu, network_unet/encoder/conv_block_2/conv2d_1/Relu, network_unet/encoder/conv_block_3/conv2d_1/Relu]:38\r\n                        QUANTIZE                  103.838           1.084           1.225         0.288%         24.687%             0.000              1       [network_unet/decoder/upsample_2x/ResizeBilinear_requantized]:16\r\n             TfLiteNnapiDelegate                  105.064          28.438          27.008         6.346%         31.033%             0.000              1       [network_unet/decoder/upsample_2x_1/ResizeBilinear]:39\r\n                        QUANTIZE                  132.076           7.225           3.313         0.779%         31.812%             0.000              1       [network_unet/decoder/upsample_2x_1/ResizeBilinear_requantized]:21\r\n             TfLiteNnapiDelegate                  135.390          46.169          64.262        15.100%         46.912%             0.000              1       [network_unet/decoder/upsample_2x_2/ResizeBilinear]:40\r\n                        QUANTIZE                  199.655           4.202           7.229         1.699%         48.610%             0.000              1       [network_unet/decoder/upsample_2x_2/ResizeBilinear_requantized]:26\r\n             TfLiteNnapiDelegate                  206.885          78.957         107.667        25.300%         73.910%             0.000              1       [network_unet/decoder/upsample_2x_3/ResizeBilinear]:41\r\n                        QUANTIZE                  314.557           8.457           9.610         2.258%         76.168%             0.000              1       [network_unet/decoder/upsample_2x_3/ResizeBilinear_requantized]:31\r\n             TfLiteNnapiDelegate                  324.168          80.592         101.422        23.832%        100.000%             0.000              1       [denoised_img]:42\r\n\r\n============================== Top by Computation Time ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n             TfLiteNnapiDelegate                  206.885          78.957         107.667        25.300%         25.300%             0.000              1       [network_unet/decoder/upsample_2x_3/ResizeBilinear]:41\r\n             TfLiteNnapiDelegate                    2.334          76.687         101.501        23.851%         49.150%             0.000              1       [network_unet/decoder/upsample_2x/ResizeBilinear, network_unet/encoder/conv_block/conv2d_1/Relu, network_unet/encoder/conv_block_1/conv2d_1/Relu, network_unet/encoder/conv_block_2/conv2d_1/Relu, network_unet/encoder/conv_block_3/conv2d_1/Relu]:38\r\n             TfLiteNnapiDelegate                  324.168          80.592         101.422        23.832%         72.982%             0.000              1       [denoised_img]:42\r\n             TfLiteNnapiDelegate                  135.390          46.169          64.262        15.100%         88.082%             0.000              1       [network_unet/decoder/upsample_2x_2/ResizeBilinear]:40\r\n             TfLiteNnapiDelegate                  105.064          28.438          27.008         6.346%         94.428%             0.000              1       [network_unet/decoder/upsample_2x_1/ResizeBilinear]:39\r\n                        QUANTIZE                  314.557           8.457           9.610         2.258%         96.687%             0.000              1       [network_unet/decoder/upsample_2x_3/ResizeBilinear_requantized]:31\r\n                        QUANTIZE                  199.655           4.202           7.229         1.699%         98.385%             0.000              1       [network_unet/decoder/upsample_2x_2/ResizeBilinear_requantized]:26\r\n                        QUANTIZE                  132.076           7.225           3.313         0.779%         99.164%             0.000              1       [network_unet/decoder/upsample_2x_1/ResizeBilinear_requantized]:21\r\n                        QUANTIZE                    0.000           0.893           2.333         0.548%         99.712%             0.000              1       [Placeholder_int8]:0\r\n                        QUANTIZE                  103.838           1.084           1.225         0.288%        100.000%             0.000              1       [network_unet/decoder/upsample_2x/ResizeBilinear_requantized]:16\r\n\r\nNumber of nodes executed: 10\r\n============================== Summary by node type ==============================\r\n                     [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]\r\n             TfLiteNnapiDelegate                5          401.856          94.429%         94.429%          0.000              5\r\n                        QUANTIZE                5           23.709           5.571%        100.000%          0.000              5\r\n\r\nTimings (microseconds): count=50 first=332704 curr=445833 min=332704 max=464648 avg=425570 std=38163\r\nMemory (bytes): count=0\r\n10 nodes observed\r\n\r\n\r\n```", "Yes, mostly QUANTIZE op is the culprit.\r\n- Why there are INT8 -> INT8 QUANTIZE? Well, as you can see, this kind of requantization is to change the scale of tensor(s). Why changing a tensor's scale? For this case, because the CONCATENATION op needs to normalize all of its input tensors to the same range / scale.\r\n- Why are there QUANTIZED ops not delegated to NNAPI? 'cause NNAPI just doesn't support INT8 -> INT8 quantize.\r\n- Since there is considerable amount of I/O (80x80x256 = 1.6 MiB in the figure you showed) between CPUs are accelerators, it's not surprising that this is not fast.", "@freedomtan \r\nThanks for your reply.\r\nCan I take some measures to avoid the int8->int8 quantize?  The resizeBilinear op is a so common op...\r\nThe scales of tensors after resizeBilinear are also int8 type. Can I think that  the int8->int8 quantize op is useless, or can I remove it from the graph?", "It's not because of resize_bilinear ops. It's mostly because of concatenation. There was `--change_concat_input_ranges` flag for older tflite_convert https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/tflite_convert.py#L454-L464", "I have tried to use  **--change_concat_input_ranges flag** in tflite_converter. But the quantized model still has the int8->int8 quantization op. Does the tf2.0 solve this problem?\r\n\r\n```\r\n        tflite_converter.representative_dataset = representative_dataset_gen\r\n        tflite_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n        tflite_converter.change_concat_input_ranges = True\r\n        tflite_converter.inference_input_type = tf.uint8  # or tf.uint8\r\n        tflite_converter.inference_output_type = tf.uint8  # or tf.uint8\r\n```", "@freedomtan \r\nI tried to add a conv+relu layer before concat to make the two inputs of concat are in the same range. But it does not work.\r\n\r\n![image](https://user-images.githubusercontent.com/26928415/94521792-31788f80-0261-11eb-9f99-dab62e3d2061.png)\r\n", "It's a bit complicated. For current tllite_converter, there are (non-MLIR converter or MLIR-based converter) + (non-MLIR quantizer or MLIR-based quantizer). In order to check what caused your problem and how to fix it, you have to know which one you're using first. Unfortunately, I don't know if there is good documentation except the source code.", "@freedomtan \r\nI have modify the model to avoid using concat. Now the model can be executed by only one nnapi delegate kernel, and running on dsp. But the performance using nnapi-8bit (90ms) is just a little better than gpu-fp16\uff08120ms). I want to get about 2x speed up on dsp, but it doesn't.\r\n\r\n- nnapi-delegate, qti-dsp\r\n```\r\nadb shell /data/local/tmp/tflite/android_arm_benchmark_model \\\r\n  --num_threads=4 \\\r\n  --graph=/data/local/tmp/tflite/denoising_8bit.tflite \\\r\n  --warmup_runs=1 \\\r\n  --num_runs=50  \\\r\n  --use_nnapi=true  \\\r\n  --enable_op_profiling=true\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nMin num runs: [50]\r\nNum threads: [4]\r\nMin warmup runs: [1]\r\nGraph: [/data/local/tmp/tflite/denoising_8bit.tflite]\r\nEnable op profiling: [1]\r\n#threads used for CPU inference: [4]\r\nUse NNAPI: [1]\r\nNNAPI accelerators available: [qti-default,qti-dsp,qti-gpu,qti-hta,nnapi-reference]\r\nLoaded model /data/local/tmp/tflite/denoising_8bit.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for NNAPI.\r\nExplicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.\r\nThe input model file size (MB): 1.80262\r\nInitialized session in 115.176ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=6 first=81323 curr=95250 min=81323 max=98725 avg=90631.7 std=5725\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=94063 curr=86265 min=77708 max=98974 avg=90720.8 std=5872\r\n\r\nInference timings in us: Init: 115176, First inference: 81323, Warmup (avg): 90631.7, Inference (avg): 90720.8\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=2.90625 overall=12.2812\r\nProfiling Info for Benchmark Initialization:\r\n============================== Run Order ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n         ModifyGraphWithDelegate                    0.000         114.412         114.412        52.416%         52.416%           976.000              1       ModifyGraphWithDelegate/0\r\n                 AllocateTensors                   62.511         103.859          51.932        47.584%        100.000%             0.000              2       AllocateTensors/0\r\n\r\n============================== Top by Computation Time ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n         ModifyGraphWithDelegate                    0.000         114.412         114.412        52.416%         52.416%           976.000              1       ModifyGraphWithDelegate/0\r\n                 AllocateTensors                   62.511         103.859          51.932        47.584%        100.000%             0.000              2       AllocateTensors/0\r\n\r\nNumber of nodes executed: 2\r\n============================== Summary by node type ==============================\r\n                     [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]\r\n         ModifyGraphWithDelegate                1          114.412          52.416%         52.416%        976.000              1\r\n                 AllocateTensors                1          103.864          47.584%        100.000%          0.000              2\r\n\r\nTimings (microseconds): count=1 curr=218276\r\nMemory (bytes): count=0\r\n2 nodes observed\r\n\r\n\r\n\r\nOperator-wise Profiling Info for Regular Benchmark Runs:\r\n============================== Run Order ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n                        QUANTIZE                    0.000           8.335           7.828         8.633%          8.633%             0.000              1       [Placeholder_int8]:0\r\n             TfLiteNnapiDelegate                    7.836          85.694          82.846        91.367%        100.000%             0.000              1       [denoised_img]:30\r\n\r\n============================== Top by Computation Time ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n             TfLiteNnapiDelegate                    7.836          85.694          82.846        91.367%         91.367%             0.000              1       [denoised_img]:30\r\n                        QUANTIZE                    0.000           8.335           7.828         8.633%        100.000%             0.000              1       [Placeholder_int8]:0\r\n\r\nNumber of nodes executed: 2\r\n============================== Summary by node type ==============================\r\n                     [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]\r\n             TfLiteNnapiDelegate                1           82.846          91.368%         91.368%          0.000              1\r\n                        QUANTIZE                1            7.827           8.632%        100.000%          0.000              1\r\n\r\nTimings (microseconds): count=50 first=94029 curr=86243 min=77682 max=98940 avg=90674.2 std=5857\r\nMemory (bytes): count=0\r\n2 nodes observed\r\n```\r\n\r\n- gpu delegate, use fp16 precision\r\n\r\n```\r\n adb shell /data/local/tmp/tflite/android_arm_benchmark_model \\\r\n  --num_threads=4 \\\r\n  --graph=/data/local/tmp/tflite/denoising_32bit.tflite \\\r\n  --warmup_runs=1 \\\r\n  --num_runs=50  \\\r\n  --use_gpu=true  \\\r\n  --enable_op_profiling=true\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nMin num runs: [50]\r\nNum threads: [4]\r\nMin warmup runs: [1]\r\nGraph: [/data/local/tmp/tflite/denoising_32bit.tflite]\r\nEnable op profiling: [1]\r\n#threads used for CPU inference: [4]\r\nUse gpu: [1]\r\nLoaded model /data/local/tmp/tflite/denoising_32bit.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\nExplicitly applied GPU delegate, and the model graph will be completely executed by the delegate.\r\nThe input model file size (MB): 7.07479\r\nInitialized session in 870.557ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=4 first=130565 curr=120829 min=117370 max=130565 avg=122373 std=4930\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=119212 curr=118055 min=113737 max=124776 avg=119375 std=1948\r\n\r\nInference timings in us: Init: 870557, First inference: 130565, Warmup (avg): 122373, Inference (avg): 119375\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=184.941 overall=205.57\r\nProfiling Info for Benchmark Initialization:\r\n============================== Run Order ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n         ModifyGraphWithDelegate                    0.000         869.587         869.587        99.995%         99.995%        188000.000              1       ModifyGraphWithDelegate/0\r\n                 AllocateTensors                  869.590           0.038           0.021         0.005%        100.000%             0.000              2       AllocateTensors/0\r\n\r\n============================== Top by Computation Time ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n         ModifyGraphWithDelegate                    0.000         869.587         869.587        99.995%         99.995%        188000.000              1       ModifyGraphWithDelegate/0\r\n                 AllocateTensors                  869.590           0.038           0.021         0.005%        100.000%             0.000              2       AllocateTensors/0\r\n\r\nNumber of nodes executed: 2\r\n============================== Summary by node type ==============================\r\n                     [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]\r\n         ModifyGraphWithDelegate                1          869.587          99.995%         99.995%     188000.000              1\r\n                 AllocateTensors                1            0.041           0.005%        100.000%          0.000              2\r\n\r\nTimings (microseconds): count=1 curr=869628\r\nMemory (bytes): count=0\r\n2 nodes observed\r\n\r\n\r\n\r\nOperator-wise Profiling Info for Regular Benchmark Runs:\r\n============================== Run Order ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n             TfLiteGpuDelegateV2                    0.000         119.184         119.346       100.000%        100.000%             0.000              1       [denoised_img]:28\r\n\r\n============================== Top by Computation Time ==============================\r\n                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n             TfLiteGpuDelegateV2                    0.000         119.184         119.346       100.000%        100.000%             0.000              1       [denoised_img]:28\r\n\r\nNumber of nodes executed: 1\r\n============================== Summary by node type ==============================\r\n                     [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]\r\n             TfLiteGpuDelegateV2                1          119.345         100.000%        100.000%          0.000              1\r\n\r\nTimings (microseconds): count=50 first=119184 curr=118025 min=113721 max=124745 avg=119346 std=1947\r\nMemory (bytes): count=0\r\n1 nodes observed\r\n```", "@freedomtan \r\nI fount another difference:\r\n\r\nI tested the **mobilenet_v1_1.0_224_quant.tflite** and **efficientnet-lite0-int8.tflite**  on SnapDragon 855+. \r\n\r\nThis is the logout of mobilenet_v1_1.0_224_quant.tflite\r\n```\r\nExplicitly applied NNAPI delegate, and the model graph will be completely executed by the delegate.\r\n```\r\n\r\nThis is the logout of efficientnet-lite0-int8.tflite\r\n```\r\nExplicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.\r\n\r\n```\r\n\r\nWhat does the w/1 delegate kernels mean? Initially, I think that it is only executed by 1 delegate kernel. \r\n\r\n\r\n--- \r\nThe only difference in the benchmark log of two model is also about Quantize. mobilenet_v1_1.0_224_quant model's input type, output type and inference type are all uint8. But efficientnet-lite0-int8.tflite model's input type and output type are unit8, and inference type is int8. So efficientnet-int8 model has a extra Quantize op than Mobilenet_quant.\r\n\r\nHowever, the Quantize op (uint8->int8) seems not time-consuming.\r\n\r\nDoes tflite have documents for which op can be executed by dsp, hta, npu, gpu or cpu in detail?  The listed ops in [nnapi documents ](https://developer.android.google.cn/ndk/guides/neuralnetworks)are mostly supported by nnapi-cpu and gpu. But we donnot know which op can be executed by dsp or hta, except the logcat's output.\r\n", "@yizhaoyanbo for the running int8 on DSP performance problem, it's both easy and hard. I don't know why you expect an INT8 model on DSP to be 2x faster than fp16 on GPU, as you might know inference latency is not always I/O or computation bound. To know if the latency is reasonable, you should try to identify the performance bottleneck(s) of the model and do performance profiling on your target device(s). Unfortunately, for NNAPI, currently there is no standard mechanism to performance per-op/per-layer profiling.", "for the mobilenet and efficientnet lite question, the Mobilenet model was from quantization-aware training and it uses UINT8. The efficientnet lite is produced by post-training quantized so it uses INT8. The input and output are changed UINT8 so that UINT8->INT8 and INT8->UINT8 quant ops are added in the beginning and end of the model. Why UINT8 input and output? You may ask. For compatibility with some existing code, I think. Since the efficientnet lite cannot be fully delegate, the delegate tells you some information.\r\n\r\nI think there is no TFLite docs on ops supported NNAPI drivers, because NNAPI is not a part of TFLite :-)", "> It's not because of resize_bilinear ops. It's mostly because of concatenation. There was `--change_concat_input_ranges` flag for older tflite_convert https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/tflite_convert.py#L454-L464\r\n\r\n@freedomtan  I have tried TF-nightly with `converter.experimental_new_converter = False` Converted model has `description\r\nTOCO Converted.` but `converter.change_concat_input_ranges = False` does not affect graph in any visible way. Does this flag still parsed in TF2?\r\n\r\nIn INT quantized models under some conditions `Quantize` node appears inside graph before `Concatenation`. NNAPI [supports](https://developer.android.google.cn/ndk/reference/group/neural-networks#group___neural_networks_1ggaabbe492c60331b13038e39d4207940e0a60e0015c8f08ed26d59afe92f728068d) only FP operands for this OP so I think we get problem here because inside quantized model input-output values are integer. ", "@freedomtan \r\nDoes tensorflow have tools to modify the tflite model, such as the coremltools can be used to modify apple's mlmodel?\r\nI want to remove the int8->int8 Quantize op in the tflite graph manually. The op seems useless, because both of two tensors are all from conv+relu, and thay should have the same data range.\r\n\r\n----\r\n\r\n**real_value = (integer_value - zeroPoint) * scale**\r\nWell, I found that the int8->int8 Quantize op adjust the  **scale** value to make the two tensors have the same range, even though they are both int8 type. So it seems that I can not just remove it from graph...\r\n![image](https://user-images.githubusercontent.com/26928415/95541135-22de6500-0a25-11eb-84f7-530694b974c0.png)\r\n\r\n", "@stalex91 sorry, no idea. I have several python venv for different TensorFlow versions. You many want to check related code, such as https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/graph_transformations/quantize.cc and code in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/. Usually, I just use 1.1x ones when necessary. It seems for full integer quantization doesn't check `converter.change_concat_input_ranges`.\r\n\r\nFor floating point one, input and output tensors are already in the same range.", "@yizhaoyanbo It should be safe to remove a INT8->INT8 requantization node if the next node (concatenation in your case) can handle it. As far as I know, there is no convenient \"surgery\" tool that you can use to remove a node.  @srjoglekar246 may have more information. For full-integer quantization case, I guess removing https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/operator_property.cc#L130 works.", "@yizhaoyanbo Thats correct. The `Quantize` nodes in TFLite also act as `Requantize`, so they serve to adjust quant parameters before operations like tensor-concatenation.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43541\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43541\">No</a>\n"]}, {"number": 43540, "title": "TF 2.1: ModelCheckpoint save best model fails", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n\r\nMac OS 10.15.16\r\n\r\n```\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\nv2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n```\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen trying to use `tf.keras.callbacks.ModelCheckpoint` a crash occurs when setting `save_best_only` to `True`\r\n\r\n**Describe the expected behavior**\r\n\r\nTraining should not crash between epochs\r\n\r\n**NOTE**: Upgrading to TF 2.3 does not cause this to happen. However the 2.1 docs still say this option should work.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/18jS5tqPo-PRaoxR1qSMMlgUjyM0sIll1?usp=sharing\r\n", "comments": ["@johntmyers,\r\nI was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/f19bd537f51e19c32c69c25fa96e90c1/43540-2-1.ipynb), and as you mentioned the issue seems to be fixed with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/308b04bce2d7ed59a70859ac72c6dee7/43540-2-3.ipynb). In this case, could you please update TensorFlow to the latest version.\r\n\r\n\r\nAlso, I was able to run a sample code using `tf.keras.callbacks.ModelCheckpoint` with `save_best_only` set to `True` on TensorFlow 2.1. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/a4b2da4c26698ca78da5b2af163858be/modelcheckpoint.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43539, "title": "TensorFlow in Practice Specialization has been renamed on Coursera", "body": "[Here](https://github.com/tensorflow/tensorflow#resources), the \"TensorFlow in Practice from Coursera\" should be renamed as \"DeepLearning.AI TensorFlow Developer Professional Certificate\" to reflect the current name of it.", "comments": []}, {"number": 43538, "title": "TFLu: Allocate scratch tensors even though they are not in subgraph", "body": "This is fixing issue: https://github.com/tensorflow/tensorflow/issues/43537", "comments": ["@nkreeger, can you take a look?", "@nkreeger Ping for review", "@advaitjain @nkreeger ping for review", "Hi @mansnils this looks like it doesn't build the micro_allocator_test:\r\n\r\n```\r\ntensorflow/lite/micro/micro_allocator_test.cc:836:70: error: too few arguments to function call, expected 3, have 2\r\n      kTfLiteOk, allocator->FinishModelAllocation(model, eval_tensors));\r\n                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                    ^\r\n./tensorflow/lite/micro/testing/micro_test.h:118:15: note: expanded from macro 'TF_LITE_MICRO_EXPECT_EQ'\r\n    auto vy = y;                                                               \\\r\n              ^\r\n./tensorflow/lite/micro/micro_allocator.h:137:16: note: 'FinishModelAllocation' declared here\r\n  TfLiteStatus FinishModelAllocation(\r\n               ^\r\n1 error generated.\r\n```\r\n"]}, {"number": 43537, "title": "Ethos-U scratch tensors not allocated", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source): b023b033299a2e88a8f25980e234144657df5fc2\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Ethos-U\r\n\r\n**Describe the problem**\r\nEthos-u relies on differentiating between having operator input tensors as subgraph inputs or not. It is depending on the offline planner and is using one or two additional operator input tensors (similar to scratch tensors, but part of the tflite file). We want these to remain as input tensors to the operator, but not as inputs to the subgraph. There has been a workaround for this, see PR: https://github.com/tensorflow/tensorflow/pull/42697\r\nHowever it does not work if there is any CPU operator before the Ethos-U operator. Also a more general solution is wanted.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["/cc @advaitjain", "This can be closed now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43537\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43537\">No</a>\n", "Fixed"]}, {"number": 43536, "title": "help!!!!! tensorflow error", "body": "![image](https://user-images.githubusercontent.com/27395455/94133874-72505d00-fe8b-11ea-9cb8-dae6a43a7926.png)\r\n", "comments": ["It is not a bug you need python 64 bit", "> It is not a bug you need python 64 bit\r\n\r\nThanks!", "@quangta2017,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "* You may try Google Colab to use TensorFlow.\r\n    * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip``` to install any other preferred TF version.\r\n    * It has an added advantage since you can you easily switch to different hardware accelerators     \r\n      (cpu, gpu, tpu) as per the task. \r\n    * All you need is a good internet connection and you are all set.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43536\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43536\">No</a>\n"]}, {"number": 43534, "title": "Duplicate node name in graph: 'ones'", "body": "Hi,\r\n\r\nI have the following error testing the yolov3 example: https://github.com/pythonlessons/TensorFlow-2.x-YOLOv3\r\n\r\n2020-09-24 11:28:23.492471: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\c81441\\AppData\\Local\\Continuum\\Anaconda3_64_2020_02\\envs\\tensor_flow_env_test\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1610, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Duplicate node name in graph: 'ones'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2017.3\\helpers\\pydev\\pydevd.py\", line 1683, in <module>\r\n    main()\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2017.3\\helpers\\pydev\\pydevd.py\", line 1677, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2017.3\\helpers\\pydev\\pydevd.py\", line 1087, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2017.3\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"C:/Users/c81441/Documents/75-GIT-PYTHON-PHOTOGRAMMETRY/deep_learning/20-TensorFlow-2.x-YOLOv3-master/detection_demo.py\", line 22, in <module>\r\n    yolo = Load_Yolo_model()\r\n  File \"C:/Users/c81441/Documents/75-GIT-PYTHON-PHOTOGRAMMETRY/deep_learning/20-TensorFlow-2.x-YOLOv3-master\\yolov3\\utils.py\", line 84, in Load_Yolo_model\r\n    yolo = Create_Yolo(input_size=YOLO_INPUT_SIZE, CLASSES=YOLO_COCO_CLASSES)\r\n  File \"C:/Users/c81441/Documents/75-GIT-PYTHON-PHOTOGRAMMETRY/deep_learning/20-TensorFlow-2.x-YOLOv3-master\\yolov3\\yolov4.py\", line 398, in Create_Yolo\r\n    pred_tensor = decode(conv_tensor, NUM_CLASS, i)\r\n  File \"C:/Users/c81441/Documents/75-GIT-PYTHON-PHOTOGRAMMETRY/deep_learning/20-TensorFlow-2.x-YOLOv3-master\\yolov3\\yolov4.py\", line 427, in decode\r\n    xy_grid = tf.meshgrid(tf.range(output_size), tf.range(output_size))\r\n  File \"C:\\Users\\c81441\\AppData\\Local\\Continuum\\Anaconda3_64_2020_02\\envs\\tensor_flow_env_test\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 2954, in meshgrid\r\n    mult_fact = ones(shapes, output_dtype)\r\n  File \"C:\\Users\\c81441\\AppData\\Local\\Continuum\\Anaconda3_64_2020_02\\envs\\tensor_flow_env_test\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 2583, in ones\r\n    output = fill(shape, constant(one, dtype=dtype), name=name)\r\n  File \"C:\\Users\\c81441\\AppData\\Local\\Continuum\\Anaconda3_64_2020_02\\envs\\tensor_flow_env_test\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 171, in fill\r\n    result = gen_array_ops.fill(dims, value, name=name)\r\n  File \"C:\\Users\\c81441\\AppData\\Local\\Continuum\\Anaconda3_64_2020_02\\envs\\tensor_flow_env_test\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\", line 3601, in fill\r\n    \"Fill\", dims=dims, value=value, name=name)\r\n  File \"C:\\Users\\c81441\\AppData\\Local\\Continuum\\Anaconda3_64_2020_02\\envs\\tensor_flow_env_test\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 793, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\c81441\\AppData\\Local\\Continuum\\Anaconda3_64_2020_02\\envs\\tensor_flow_env_test\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 548, in create_op\r\n    compute_device)\r\n  File \"C:\\Users\\c81441\\AppData\\Local\\Continuum\\Anaconda3_64_2020_02\\envs\\tensor_flow_env_test\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3429, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"C:\\Users\\c81441\\AppData\\Local\\Continuum\\Anaconda3_64_2020_02\\envs\\tensor_flow_env_test\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1773, in __init__\r\n    control_input_ops)\r\n  File \"C:\\Users\\c81441\\AppData\\Local\\Continuum\\Anaconda3_64_2020_02\\envs\\tensor_flow_env_test\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1613, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Duplicate node name in graph: 'ones'\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7, 64bits\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0. I have tested with different tf/pythons versions and I get the same error\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n", "comments": ["Please open the ticket in the third_party repository https://github.com/pythonlessons/TensorFlow-2.x-YOLOv3 and close this one. Ten reopen this if they debug that is a Tensorflow bug.", "The error is inside of tf module and I have seen some similar issues in tf", "Have you seen?\r\nhttps://github.com/pythonlessons/TensorFlow-2.x-YOLOv3/issues/37", "Can you close this?", "I tested with tf 2.3 and I have the same error. I think that it could be a tf bug", "We don't support Anaconda setup directly  here.\nAs the same ticket on the model repository was close as solved with TF 2.3 Can you try with the official installation https://www.tensorflow.org/install/pip", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43534\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43534\">No</a>\n"]}, {"number": 43533, "title": "Add test ReadWhileOverwriting", "body": "This is a PR from TaiJi AI platform in Tencent.", "comments": ["cc  @mihaimaruseac @vnvo2409 merging https://github.com/tensorflow/tensorflow/pull/43220 into the HDFS modular filesystem implementation"]}, {"number": 43532, "title": "Support HDFS ReadWhileOverwriting #42597", "body": "This is a PR from TaiJi AI platform in Tencent.", "comments": ["cc  @mihaimaruseac @vnvo2409 merging https://github.com/tensorflow/tensorflow/pull/43197 into the HDFS modular filesystem implementation"]}, {"number": 43531, "title": "Installation via Miniconda fails", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 Version 2004 (Build 19041.508)\r\n- TensorFlow installed from: with the code `conda install -c conda-forge tensorflow` \r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: yes, with conda\r\n- GPU model and memory: NVIDIA GeForce GTX 860M \r\n\r\n**Describe the problem**\r\nWhen trying to install the software, the following message appears:\r\n\r\nUnsatisfiableError: The following specifications were found\r\nto be incompatible with the existing python installation in your environment:\r\n\r\nSpecifications:\r\n\r\n  - tensorflow -> python[version='3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.6,<3.7.0a0|>=3.7,<3.8.0a0|3.7.*']\r\n\r\nYour python: python=3.8\r\n\r\nIf python is on the left-most side of the chain, that's the version you've asked for.\r\nWhen python appears to the right, that indicates that the thing on the left is somehow\r\nnot available for the python version you are constrained to. Note that conda will not\r\nchange your python version to a different minor version unless you explicitly specify\r\nthat.\r\n\r\nThe following specifications were found to be incompatible with your system:\r\n\r\n  - feature:/win-64::__cuda==8.0=0\r\n  - feature:|@/win-64::__cuda==8.0=0\r\n\r\nYour installed version is: 8.0\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Install Miniconda3 Windows 64-bit (SHA 1f4ff67f051c815b6008f144fdc4c3092af2805301d248b56281c36c1f4333e5)\r\n2. Install Spyder: `conda install -c anaconda spyder`\r\n3. Install matplotlib: `conda install -c conda-forge matplotlib`\r\n4. Install scipy: `conda install -c anaconda scipy`\r\n5. Install sympy: `conda install -c anaconda sympy`\r\n6. Install tensorflow: `conda install -c conda-forge tensorflow`\r\n", "comments": ["We don't officially support conda here.\r\nYou have two solutions:\r\n- Use the default install guide without Miniconda/Anaconda: https://www.tensorflow.org/install/pip\r\n- Continue to use conda. But you need to ask support to the Conda project https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/", "Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43531\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43531\">No</a>\n"]}, {"number": 43530, "title": "Import error | from pip | At ubuntu 18.04", "body": "System information\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):pypi\r\n- TensorFlow version:2.0.0a0\r\n- Python version:3.6.9\r\n- Installed using virtualenv? pip? conda?:pip\r\n- CUDA/cuDNN version:I don't have a cuda\r\n- GPU model and memory: Atom/Celeron/Pentium Processor x5-E8000/J3xxx/N3xxx Integrated Graphics Controller [1.9G]\r\n\r\n### **Describe the problem**\r\nI tried the code below\r\nand I got the output something like this:\r\n\r\n>Illegal instruction\r\n\r\n in my bash terminal\r\n\r\n\r\n###**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n**I created a new python3 virtual environment and ran the command:**\r\n\r\n> $ pip install tensorflow==2.0.0a0\r\n\r\n> $ pip install protobuf==3.10 #installed protobuf \r\n\r\n**Then I tested the code using:**\r\n\r\n> $ python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n", "comments": ["@Midhlaj2006,\r\nIs there any specific reason you are installing an order release of TensorFlow? Please try installing TensorFlow v2.3 using the below commands and let us know if it works.\r\n```\r\npip install --upgrade pip\r\n\r\npip install tensorflow\r\n```\r\n\r\nThanks!", "Also, please go through similar issue [#40978](https://github.com/tensorflow/tensorflow/issues/40978) and let us know if it helps. Thanks!", "Your CPU `Processor x5-E8000` doesn't support AVX (but you can still build from source). See https://github.com/tensorflow/tensorflow/issues/19584", "> @Midhlaj2006,\r\n> Is there any specific reason you are installing an order release of TensorFlow? Please try installing TensorFlow v2.3 using the below commands and let us know if it works.\r\n> \r\n> ```\r\n> pip install --upgrade pip\r\n> \r\n> pip install tensorflow\r\n> ```\r\n> \r\n> Thanks!\r\n\r\nIt was brilliant but this don't help me\r\n", "@Midhlaj2006,\r\nAs per Intel's [product specification](https://ark.intel.com/content/www/us/en/ark/products/92124/intel-atom-x5-e8000-processor-2m-cache-up-to-2-00-ghz.html), your CPU doesn't support AVX. For more information, please take a look at the [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements) to install TensorFlow with pip. \r\n\r\nAlternatively, you can [build TensorFlow from source](https://www.tensorflow.org/install/source) or use [Google Colab](https://colab.research.google.com/). Thanks!", "> @Midhlaj2006,\r\n> As per Intel's [product specification](https://ark.intel.com/content/www/us/en/ark/products/92124/intel-atom-x5-e8000-processor-2m-cache-up-to-2-00-ghz.html), your CPU doesn't support AVX. For more information, please take a look at the [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements) to install TensorFlow with pip.\r\n> \r\n> Alternatively, you can [build TensorFlow from source](https://www.tensorflow.org/install/source) or use [Google Colab](https://colab.research.google.com/). Thanks!\r\n\r\nThanks brother, it works!\ud83d\udc4f\r\nBut I has some connection issues by now days \ud83d\udce1\r\nCan I use colab offline?", "@Midhlaj2006,\r\nColab is not available offline. You need to be connected to the internet to access Google Colab. For more information please take a look at the [FAQs](https://research.google.com/colaboratory/faq.html). Thanks!", "Thanks for your responses", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43530\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43530\">No</a>\n"]}, {"number": 43529, "title": " 'tf.TensorScatterUpdate' Conversion to tflite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nwindows 10\r\n- TensorFlow installed from (source or binary):\r\npython binary \r\n- TensorFlow version (or github SHA if from source):\r\n2.3\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\nerror: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n(im already using TF ops)\r\n```\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["> Standalone code to reproduce the issue\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.", "@shlomi-amitai \r\nPlease share simple stand alone code to replicate the issue along with error log for us to analyse.", "I will add tf.TensorScatterUpdate op and related ops to the Select TF ops option. Please stay tuned.", "I added tf.TensorScatterUpdate op support in the Select TF ops option. Please try the conversion again with the tomorrow's tf-nightly version.", "Hi @abattery , \r\nThank you so much, I tried, and don't see any error regarding the tf.TensorScatterUpdate op, \r\nthough I get now a new error related to tf.concat op.\r\nMaybe something is broken there? \r\n(problem in concat_415)", "<unknown>:0: error: loc(\"Concat_415\"): operand type 'tensor<1x32x112x20xf32>' is not compatible with preceding operands; expected dimension at index 3: 160\r\n<unknown>:0: note: loc(\"Concat_415\"): see current operation: %1202 = \"tf.ConcatV2\"(%525, %652, %1201, %1197, %1189, %1185, %24) {device = \"\"} : (tensor<1x64x112x160xf32>, tensor<1x128x112x160xf32>, tensor<1x32x112x20xf32>, tensor<1x32x112x10xf32>, tensor<1x32x112x5xf32>, tensor<1x32x112x2xf32>, tensor<i32>) -> tensor<1x?x112x160xf32>\r\n"]}, {"number": 43528, "title": "Tensorflow 2.3.0 can't detect CUDA on Python 3.7.9", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.7.9\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory: Nvidia GTX 960m\r\n\r\nI just discovered compatibility problem between Tensorflow 2.3.0 and Python 3.7.9. So, as you might have noticed, there're A LOT of \"cudart64_X.dll not found\" problems floating around, with standard fixes here and there.\r\n\r\nBut I just couldn't find any single solution that can fix it. I even did everything cleanly, the CUDA/cuDNN install process. I also make sure my environment vars & PATH are as complete as possible.\r\n\r\nUntil I downgraded to Python 3.7.1, and the problem magically gone, that's all. This is my steps:\r\n\r\n1. Have CUDA/cuDNN installed complete with path & environment.\r\n2. Conda create environment python 3.7 (so I get Python 3.7.9).\r\n3. Activate that new env.\r\n4. pip install tensorflow (so I get v2.3.0).\r\n5. try to import tensorflow, get \"cudart64_101.dll not found\".\r\n6. Conda install python=3.7.1 (so, downgrade).\r\n7. try to import tensorflow, get \"Successfully opened dynamic library cudart64_101.dll\"", "comments": ["We don't officially support conda here.\r\n\r\nYou have two solutions:\r\n- Use the default install guide without Miniconda/Anaconda: https://www.tensorflow.org/install/pip\r\n- Continue to use conda. But you need to ask support to the Conda project https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/\r\n\r\nCan you verify that this problem (3.7.9) exist with the official installation guide.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43528\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43528\">No</a>\n"]}]