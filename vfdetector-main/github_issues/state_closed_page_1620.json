[{"number": 4311, "title": "Configure takes a lot of time and resources ", "body": "in the last few weeks I notice that while developing operations for tensorflow I find myself doing ./configure many more times and each time takes few minutes on a good internet connection.\n\nCan we cache some of the artifacts ? Do we have to remove all the time the complete protobuf repository ? How is this handled internally at google ? \n", "comments": ["I wonder whether simply commenting out the `bazel clean --expunge` at the end of the `configure` script would lead to problems.\nI assume there must be a reason it's there, but I haven't figured out what it might be (assuming that bazel can correctly pick up the changes in all files touched by `configure`).\n", "We have the clean --expunge there for safety. You can try removing it, which should speed this up considerably, and most of the time it should be fine. Why are you running configure so often?\n", "I'll close this issue. Please comment to reope if there something that we can/should do to our configure script. \n\nThe `--expunge` is net beneficial in terms of issue load, I fear, so that'll stay for now.\n"]}, {"number": 4310, "title": "Add PIP package creation to the CMake build.", "body": "", "comments": ["Since the setup.py is mostly identical, is it possible to somehow reuse the existing one? \n", "@martinwicke I'd rather keep this simplified one as a temporary measure (until the two PIP packages are at parity) than refactor the main `setup.py` with a bunch of conditional flags that we're eventually not going to need. Does that make sense?\n", "Makes sense to me. Just wanted to check that this is deliberate.\nOn Sat, Sep 10, 2016 at 09:21 Derek Murray notifications@github.com wrote:\n\n> @martinwicke https://github.com/martinwicke I'd rather keep this\n> simplified one as a temporary measure (until the two PIP packages are at\n> parity) than refactor the main setup.py with a bunch of conditional flags\n> that we're eventually not going to need. Does that make sense?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4310#issuecomment-246120673,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_Tpyl6t0I4jWCXd5z6AEyjL0pV4Cks5qotkIgaJpZM4J5ju4\n> .\n", "I think we need a new build for this in jenkins, the existing pull-requests-cmake would probably not cut it.\nDo we have any scripts for building the pip package for this build?\nWhat is the command to build it?\n", "Found it, we will first merge this, and refer to this one through a script similar to what we have here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/build_pip_package.sh\n"]}, {"number": 4309, "title": "In-place operations (dropout, ReLU, etc..)", "body": "As far as I understand, tensorflow does not support in-place operation of operations such as ReLU, Dropout, etc..\n\nIf an operation outputs the same type and shape tensor and does not involve any branching in the graph then it should be possible to operate on the same chunk of memory.\n\nAs it stands, tensorflow doesn't support this which severely increases memory usage for large deep learning models.\n", "comments": ["Since most of the memory is concentrated in the weights, I don't think this would lead to significant savings. The examples you give only affect activations, which are a tiny part of a typical model's memory consumption.\n", "Martin: I wouldn't say that all the memory is always concentrated in weights.\n\nHowever, we have not seen proof that you need in-place operations to be memory efficient.\n- If there is only one consumer of an activation, the memory will be freed almost instantaneously after the consuming op is completed.  Yes, you can avoid this one memory allocation for an activation buffer, but in practice this is never the dominant memory cost of a model (One case where this would be practically important is if the activation memory of one layer was the majority of the GPU memory, but I don't believe that's common.)\n- As some external studies have shown, TensorFlow is already quite efficient when compared to many other frameworks in using GPU memory already, and I'd like to see proof that in-place updates would help dramatically.  Until then, I think we'd rather spend time on the potential bigger wins in placement and scheduling before trying to implement in-place updates.\n", "Thank you for your reply. Consider a 3d convolutional model (say for example over a video tensor - these are quite commonly used). @martinwicke in this situation the dominant memory by far is in the feature tensors. Having to create deep copies of these tensors for both activation and dropout severely limits the depth of networks you can train. This contrasts with other libraries (e.g. Caffe/ Torch) which allow in-place operation.\n", "This is also true with some fully convolutional models and generative architectures.", "https://github.com/tensorflow/tensorflow/blob/52dcb2590bb9274262656c958c105cb5e5cc1300/tensorflow/core/framework/op_kernel.h#L714 this has now been implemented and used where we have discovered it is safe to do so.  cc @rmlarsen ", "> [tensorflow/tensorflow/core/framework/op_kernel.h](https://github.com/tensorflow/tensorflow/blob/52dcb2590bb9274262656c958c105cb5e5cc1300/tensorflow/core/framework/op_kernel.h#L714)\r\n> \r\n> Line 714 in [52dcb25](/tensorflow/tensorflow/commit/52dcb2590bb9274262656c958c105cb5e5cc1300)\r\n> \r\n>  Status forward_input_or_allocate_output( \r\n> this has now been implemented and used where we have discovered it is safe to do so. cc @rmlarsen\r\n\r\nHello, thanks for your reply, does this mean that tensorflow automatically does inplace operation when available ? is it necessary to check some option when creating the graph ?"]}, {"number": 4308, "title": "tf.image.rot90() returns `None` if the argument is not a Python integer", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nPointed out in this [Stack Overflow](http://stackoverflow.com/q/39418948/3574081) question.\n### Environment info\n1. The commit hash (`git rev-parse HEAD`): bf5b2f0185d4b61329a0bf7a56827661ef6526a1\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n``` python\nimage = tf.random_uniform([100, 100, 3], dtype=tf.float32)\nassert tf.image.rot90(image, tf.constant(7)) is None\n```\n", "comments": []}, {"number": 4307, "title": "Branch 132710890", "body": "", "comments": ["@tensorflow-jenkins Test this please\n"]}, {"number": 4306, "title": "How does conv2d_transpose exactly space out inputs in tensorflow?", "body": "This may not be the best place to ask this question but I don't get any help from stackoverflow. Sorry if I ask this question in the wrong place.\n\nI understand how transpose convolution works dealing with 2-D images. I just wonder how conv2d_transpose() function in tensorflow exactly adds zeros to the inputs.\n\nDoes it only pad zeros along the border, or does it insert zeros to space out inputs in a certain way?\n", "comments": ["I just figure this out by running a tinny example. If you have the same question, please let me know.\n", "Hey @wgmao could you please share your thoughts? :)", "Suppose we start from a 2 by 2 matrix and apply a 2 by 2 filter in order to get a 4 by 4 matrix after the deconvolution operation.\r\n\r\nThis the starting point of the operation. (left: matrix; right: filter, no bias term)\r\n![1](https://cloud.githubusercontent.com/assets/5178826/22154385/4cc7a076-def9-11e6-85e9-0583b5227c5e.PNG)\r\n\r\nYou can tell the way how tensorflow add zeros and do the convolution afterwards by the following figure. Basically it pads zeros after each column and row of the starting matrix. I don't try all the conv2d_transpose configuration and I think this is how the default works. \r\n![2](https://cloud.githubusercontent.com/assets/5178826/22154423/794786fc-def9-11e6-97d3-96e715990e99.PNG)\r\n", "@wgmao Are you sure that conv2d_transpose wraps around instead of applying more zeros ? I am referring to the last step.\r\n\r\n![selection_011](https://user-images.githubusercontent.com/11773009/43992607-29358888-9d81-11e8-85a7-abd5dc0aa603.png)\r\n\r\nThis contradicts the output of the following code"]}, {"number": 4305, "title": "Documentation for Adding New Hardware", "body": "Adding documentation about the internals of TensorFlow.\n\nThe motivation is to instruct a user on how to add new hardware to TensorFlow. It is a work in progress as there is little documentation on the subject so I am doing everything by trial and error.\n\nFeedback is appreciated as this is a work in progress.\n\nMy company is KnuEdge and they signed the CLA. The person who actually signed is named Dave Eames.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n\nCheck for my company KnuEdge and my supervisor Dave Eames who did the signing.\n", "Thanks, I see KnuEdge in the list -- is your commit email part of the google group defined when signing up that allows our bot to verify CLA status?\n", "The email within my commits is amacdonald@knupath.com, but the email I have for the Google Group is amacdonald@knedge.com. Same company, just some weird business naming scheme.\n\nI emailed you from the Google Group with my amacd...@knuedge.com email\n", "Our bot doesn't know that :(  Can you add both emails somehow?\n", "(alternatively, amend the commits with the other email and it should work)\n", "I think I did it. Can you verify? I changed the commit email\n", "Thanks, I can verify it's now knuedge.com, not sure why the bot isn't picking it up.\n\nPing for @willnorris: is there any other debugging I can do?  KnuEdge is in the CLA list.  How do I check the google groups membership myself?\n", "@aidan-plenert-macdonald, could you add your knuedge.com email to your GitHub account at https://github.com/settings/emails (it doesn't necessarily need to be the primary email).  Then leave a comment here to force our CLA system to rescan this pull request.\n", "I added my knuedge email.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@aidan-plenert-macdonald friendly ping?\n", "(I think Aidan is working on the end-to-end example at the moment, and once that's working, I'm sure the documentation can be updated).\n", "Yes, I am working on a different issue that will replace this one. @vrv brought it to my attention that you can compile a .so file that contains the whole device interface, so I am making this work and writing documentation. https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-248044087\n"]}, {"number": 4304, "title": "Add doc of conv3d_transpose", "body": "This PR add the API doc of `conv3d_transpose`.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "The way to add documentation is to call it out here: https://github.com/tensorflow/tensorflow/blob/f974e8d0c2420c6f7e2a2791febb4781a266823f/tensorflow/python/ops/nn.py#L111\n\nJust add @conv3d_transpose there and our doc generator will update afterwards.\n\nYou can drop the change to the .md file in the commit\n", "@vrv thanks, PR updated.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 4303, "title": "Cannot use TensorArray with tf.scan", "body": "I seem to be unable to use `TensorArray`s with `tf.scan`. I am running 0.10.0rc0 on Ubuntu 14.04 with Cuda Toolkit 7.5 and cuDNN v4.\n\nHere is a minimal example:\n\n``` python\nimport tensorflow as tf\n\narray = tf.TensorArray(tf.float32, size=10)\nelements = list(range(10))\n\ndef step(state, current_input):\n    return state.write(current_input, tf.zeros(shape=[100]))\n\nout = tf.scan(step, elements, initializer=array)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(out))\n```\n\nAnd the output is:\n\n```\nTraceback (most recent call last):\n  File \"array_test.py\", line 9, in <module>\n    out = tf.scan(step, elements, initializer=array)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/functional_ops.py\", line 521, in scan\n    a_flat = [ops.convert_to_tensor(init) for init in initializer_flat]\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/functional_ops.py\", line 521, in <listcomp>\n    a_flat = [ops.convert_to_tensor(init) for init in initializer_flat]\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 628, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/constant_op.py\", line 180, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/constant_op.py\", line 163, in constant\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/tensor_util.py\", line 422, in make_tensor_proto\n    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/tensor_util.py\", line 422, in <listcomp>\n    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/compat.py\", line 45, in as_bytes\n    (bytes_or_text,))\nTypeError: Expected binary or unicode string, got <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7ff0db9309e8>\n```\n", "comments": ["@ebrevdo can correct / expand as necessary\n\nThanks @jeanm for filing this issue!  I can indeed reproduce the output.\n\nI believe the purpose of `TensorArray` is to implement dynamic iteration primitives - e.g. `scan` is implemented using `TensorArray`:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/functional_ops.py#L506\n\nI'm not sure whether `TensorArray` is itself meant to be used as you are doing.  For the simple example program above, obviously a regular tensor should be sufficient.\n\nCan you expand a bit on what problem you're actually trying to solve?  Or were you just playing around with things?\n", "Thanks for the quick response @tatatodd \n\n> Can you expand a bit on what problem you're actually trying to solve? Or were you just playing around with things?\n\nI am implementing an RNN which, at time `t`, may need to access not just the output at `t-1` but also outputs from earlier time steps. That's why I'd like to pass around a `TensorArray` and update it as I go along. In #2237 there are several examples of RNNs of this kind, such as [this simple Tree-RNN](https://github.com/tensorflow/tensorflow/issues/2237#issuecomment-233439285) by @lukasr0.\n\nI can emulate a scan by using `tf.while_loop` instead, like in the linked Tree-RNN example, but it's a bit cumbersome and it doesn't seem to be very efficient.\n", "Please see the new `tf.nn.raw_rnn`, which allows you to keep an arbitrary list of tensors and TensorArrays in the loop_state, as passed through loop_fn.\n\nYou can write the rnn output state to your loop state.\n", "Thanks, that seems like a better solution.\n"]}, {"number": 4302, "title": "Develop newhardware docs", "body": "Adding documentation about the internals of TensorFlow.\n\nThe motivation is to instruct a user on how to add new hardware to TensorFlow. It is a work in progress as there is little documentation on the subject so I am doing everything by trial and error.\n\nFeedback is appreciated as this is a work in progress.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "(If you are able to sign the CLA, we would be happy to help iterate on these docs, this would be a great start!)\n", "My company signed it, but I should have submitted under a Git Group that my\ncompany has. So let me have them add me to the group and I will resubmit\nthe pull request under their name.\n\nYou can delete this pull request as I will just submit a new one soon.\n\nAidan Plenert Macdonald\nWebsite http://acsweb.ucsd.edu/~amacdona/\n\nOn Fri, Sep 9, 2016 at 9:41 AM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> (If you are able to sign the CLA, we would be happy to help iterate on\n> these docs, this would be a great start!)\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4302#issuecomment-245968273,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AGYXJ5KqfAfjoVlnHkaH4VOH5F4vIr4wks5qoYwwgaJpZM4J5NNR\n> .\n", "Okay no problem.  Also keep in mind that the actual code samples you had are a proof of concept but wouldn't be able to be checked into our repo -- we need to make it possible to add a device without any device-specific changes to the TensorFlow code, so that will likely take some work to do, I think.\n", "The code was contained within the documentation not affecting the actual repo. Is this a problem? I am not actually writing any support for new hardware, but documentation on how to as I develop for my own hardware. I can't share code for my companies hardware just yet. I am a little confused by what you mean.\n", "Some of the code snippets are suggesting changes to the TF codebase (e.g., change to DeviceTypeOrder).  In practice that's not a change that we would be able to accept right now, and TF would have to be modified to support that case through some other mechanism.\n\nIt's fine to check that in to the docs you are writing, with the caveat that it will have to change once we have better mechanisms to add Devices as external code only.\n", "Ah. Okay. I am happy to change it once such an interface exists. I can do some development on the framework to make it cleaner for new hardware to be added. Once I get the CLA and pull request, we can discuss details.\n"]}, {"number": 4301, "title": "Add CUDA compute cap support of 3.0 for gpu.devel", "body": "See discussion in [#4048](https://github.com/tensorflow/tensorflow/issues/4048)\n", "comments": ["@chenliu0831, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @vrv and @caisq to be potential reviewers\n", "Can one of the admins verify this patch?\n", "@caisq can you review? I'll merge without tests since no tests are affected by this change.\n", "OK. Will take a look.\n", "LGTM. Thanks.\n\n@tensorflow-jenkins test this please.\n", "Test failure appears to be unrelated.\n\n@chenliu0831, can you please sign the CLA? \n", "@caisq Just signed. Thanks!\n", "@tensorflow-jenkins test this please.\n", "merged. thanks!\n"]}, {"number": 4300, "title": "Add -D_FORCE_INLINES to nvcc opts", "body": "Fixes #4103.\n", "comments": ["@martinwicke, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @keveman and @jdoerrie to be potential reviewers\n", "@zheng-xq please let me know if this is a bad idea (it seems to fix a problem on recent Ubuntu)\n\n@damienmg let me know if there's a better way to set this define\n", "@martinwicke I like your mention bot, can we have the same? :) Can we train tensorflow to do review instead of us :D \n\nAnyway LGTM AFAICT but I don't now about the nvcc flag itself. For gcc we call the compiler to know if the option is supported but for NVCC it is probably ok to just include it raw.\n", "Since it's just a `-D` I'm pretty confident it will be supported. Whether it's honored is another question, but it seems to solve problems. :)\n", "Oh, and the mention bot, just add it as a webhook!\n", "The mention bot can just be added as a webhook. It's awesome. You should\nadd it too. I wish I had the same for issues, and we'll be working on\nsomething like that soon. :)\n\nOn Fri, Sep 9, 2016 at 8:59 AM, Damien Martin-Guillerez <\nnotifications@github.com> wrote:\n\n> @martinwicke https://github.com/martinwicke I like your mention bot,\n> can we have the same? :) Can we train tensorflow to do review instead of us\n> :D\n> \n> Anyway LGTM AFAICT but I don't now about the nvcc flag itself. For gcc we\n> call the compiler to know if the option is supported but for NVCC it is\n> probably ok to just include it raw.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4300#issuecomment-245956479,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_efnWGxyoswG0IQmPjxaMlCAoMYcks5qoYJ8gaJpZM4J5MIH\n> .\n"]}, {"number": 4299, "title": "tensorflow-0.8.0-cp27-none-linux_86_64.whl is not a supported wheel on this platform.", "body": "I encounted this issue when trying to install the tensorflow on mine Ubuntu system. here is the details of my system.\nLinux ... 4.4.0-21-generic #37-Ubuntu SMP Mon Apr 18 18:33:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n", "comments": ["\"tensorflow-0.8.0-cp27-none-linux_86_64.whl\"\nin your title implies the file is missing an x. Post the whl filename that you downloaded. Also, 0.8 is an old version try 0.10rc1 or 0.9.\n"]}, {"number": 4298, "title": "it allocates on all specified devices (GPU Memory usage is high on Nvidia-smi), but does not do computations (no memory utilization from Nvidia-smi while the program is running)", "body": "Tensorflow it allocates on all specified devices (GPU Memory usage is high on Nvidia-smi), but does not do computations (no memory utilization from Nvidia-smi while the program is running),what's wrong\n", "comments": ["@gao8954 we primarily use github issues for tracking bugs and installation issues.  For general debugging and help, please ask your question on StackOverflow, and tag it with the `tensorflow` tag.\n\nI'd suggest you start on the tensorflow.org site and run some of the simple tutorials and examples, and confirm that your installation is correct.  Perhaps take a look here:\nhttps://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#optional-install-cuda-gpus-on-linux\n\nThanks!\n"]}, {"number": 4297, "title": "'SAME' padding works incorrect", "body": "I have 28x28 image. When I apply convolution (or pooling) with 'SAME' padding, kernel size 2x2, stride 2, it produces feature map of size 14x14, but it should be 15x15, so the last (bottom and right) image pixels would't be processed. The output size should be calculated with formula output = [(input+2*padding-kernel) / stride] + 1 (look https://arxiv.org/pdf/1603.07285.pdf). In case of 'SAME', padding size would be [kernel/2]\nTF version 0.9.0\n", "comments": ["@VaWheel can you provide a small test example that reproduces your problem?  Thanks!\n", "```\nimport tensorflow as tf\n\ndata = tf.placeholder(\"float\", shape=[1, 8, 8, 1])\nconv = tf.nn.max_pool(data, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n\nprint(conv.get_shape())  # prints (1, 4, 4, 1), but should be (1, 5, 5, 1)\n```\n", "@VaWheel I think I see where the confusion occurred.\n\nIn the paper, I believe the output formula you're using is is from section 2.4 \"Zero padding, non-unit strides\".  That formula is correct, but make sure you don't miss the floor function!\n\nBut you mention that SAME padding should be [kernel/2], which you're getting from section 2.2.1 \"Half (same) padding\".  Note that in the formula s=1, i.e. there is a unit stride.  All of section 2.2 is only discussing the s=1 case, so it's not applicable to your stride=2 scenario.  It doesn't make sense to combine the two formulas.\n\nAlso note that the use of \"same\" in section 2.2.1 is different than the terminology we use in tensorflow.  Here's how we define SAME and VALID padding in tensorflow:\nhttps://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#convolution\n\nRoughly speaking VALID means we never pad, and only apply the filter (i.e. kernel) over real inputs.  SAME means we pad equally on all sides, such that the output size = ceil(input/stride)\n\nLet's consider your original example of a 28x28 image, kernel size 2x2, stride 2.  Starting from the top left corner (0,0), we will move to (0,2), (0,4), ..., (0,26).  That's a total of 14 steps, which is why the output is 14x14.  There are no missed pixels.\n\nHope that helps.\n", "@tatatodd, as I understand, SAME pads with half the filter size. So in the case of 8x8 image, 2x2 kernel, stride 2 it should look like\n![image](https://cloud.githubusercontent.com/assets/12466121/18409407/fb3d885e-774e-11e6-873f-4cfff385b62c.png)\n\nHow SAME padding should look from tensorflow point of view? \n", "@VaWheel please re-read my explanation.  You're deriving the definition of \"SAME\" from the paper you referenced, section 2.2.1 \"Half (same) padding\", which is only valid for unit strides, i.e. s=1.  It is not valid for any other stride.  It is also not the same meaning that we use in tensorflow for the SAME padding.\n\nIn tensorflow for an 8x8 image, 2x2 kernel, stride 2 with SAME padding, we will end up with no padding.\n", "Ok, no padding - that's what I've asked. Thanks\n\nps http://lasagne.readthedocs.io/en/latest/modules/layers/conv.html also defines SAME padding as half the filter size (+ rounded down). So, different libs define different meanings\n", "I got the similar issue. In my case, input shape is (3, 75, 75), the output shape is (3, 37, 37). But according to the formula: https://www.tensorflow.org/api_docs/python/tf/nn/convolution, the output shape should be (3, 38, 38).\r\n", "Zero padding layer can help you with this issue."]}, {"number": 4296, "title": "Grid RNN Cell does not support dynamic batch sizes", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nhttp://stackoverflow.com/questions/38442025/tensorflow-grid-lstm-rnn-typeerror\n### Environment info\n\nOperating System: \nUbuntu 16.04 LTS (GNU/Linux 4.4.0-31-generic x86_64)\n\nInstalled version of CUDA and cuDNN: \n-rw-r--r-- 1 root root   322936 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 M\u00e4r 30 14:25 /usr/lib/x86_64-linux-gnu/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 M\u00e4r 30 14:25 /usr/lib/x86_64-linux-gnu/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rw-r--r-- 1 root root   383336 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudart_static.a\nlrwxrwxrwx 1 root root       12 Apr 14 17:53 /usr/lib/x86_64-linux-gnu/libcuda.so -> libcuda.so.1\nlrwxrwxrwx 1 root root       17 Apr 14 17:53 /usr/lib/x86_64-linux-gnu/libcuda.so.1 -> libcuda.so.361.42\n-rw-r--r-- 1 root root 16881416 M\u00e4r 23 01:42 /usr/lib/x86_64-linux-gnu/libcuda.so.361.42\nlrwxrwxrwx 1 root root       13 Aug 31 13:32 /usr/lib/x86_64-linux-gnu/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 root root       17 Aug 31 13:32 /usr/lib/x86_64-linux-gnu/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxr-xr-x 1 root root 61453024 Aug 31 13:32 /usr/lib/x86_64-linux-gnu/libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 62025862 Aug 31 13:32 /usr/lib/x86_64-linux-gnu/libcudnn_static.a\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp35-cp35m-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`: '0.10.0rc0'\n### What other attempted solutions have you tried?\n\nchanged file tensorflow/tensorflow/contrib/grid_rnn/python/ops/grid_rnn_cell.py:\n\n```\n  # project input\n  if inputs is not None and sum(inputs.get_shape().as_list()) > 0 and len(\n      conf.inputs) > 0:\n```\n\nto:\n\n```\n  # project input\n  if inputs is not None and inputs.get_shape()[1] > 0 and len(\n      conf.inputs) > 0:\n```\n\nfixed the issue, but I'm not sure if it is the intended behavior\n### Logs or other output that would be helpful\n\nif not changed the following error occurs:\n\n**\\* TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'\n\n-> probably because of sum(inputs.get_shape().as_list())\n", "comments": ["Thanks for filing the issue @StefOe \n\nYou might also be interested in the comments in #1665 (toward the bottom), which is also linked in the comments of the SO post you provided.\n\nNote that we have limited resources for maintaining contrib code.  We do welcome pull requests with tests to fix things!\n", "FYI, I am working on a fix for this. Will also fix all the warnings related to `state_is_tuple` and `input_size`.\n", "Thank you! Sadly I was lacking the time to do it properly.\n", "PR is submitted: https://github.com/tensorflow/tensorflow/pull/4631\nComments and suggestions will be appreciated.\n", "PR #4631 fixes the issue. Thank you!\nShould I close this, or wait untill the PR is reviewed?\n", "I guess we should wait until the PR is reviewed.\n"]}, {"number": 4295, "title": "fixes load_csv_with_header to respect features_dtype", "body": "and removes unnecessary code in load_csv_without header\n", "comments": ["@jonasrauber, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener and @ilblackdragon to be potential reviewers\n", "Can one of the admins verify this patch?\n", "@ilblackdragon PTAL\n", "Jenkins, test this please.\n"]}, {"number": 4294, "title": "Fix a typo in g3doc/how_tos/tool_developers", "body": "Add `()' after f.read so we pass a string rather than a\nfunction.\n", "comments": ["Can one of the admins verify this patch?\n", "@shinh, thanks for your PR! By analyzing the annotation information on this pull request, we identified @petewarden, @ebrevdo and @tensorflower-gardener to be potential reviewers\n", "@tensorflow-jenkins test this please.\n", "LGTM\n"]}, {"number": 4293, "title": "The train and test data of wide and deep example is broken", "body": "Wide and deep example code will download train and test data from `https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data` and `https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test`. But those files are broken now and you can `wget` to read the raw data.\n\n```\n|1x3 Cross validator\n25, Private, 226802, 11th, 7, Never-married, Machine-op-inspct, Own-child, Black, Male, 0, 0, 40, United-States, <=50K.\n38, Private, 89814, HS-grad, 9, Married-civ-spouse, Farming-fishing, Husband, White, Male, 0, 0, 50, United-States, <=50K.\n28, Local-gov, 336951, Assoc-acdm, 12, Married-civ-spouse, Protective-serv, Husband, White, Male, 0, 0, 40, United-States, >50K.\n44, Private, 160323, Some-college, 10, Married-civ-spouse, Machine-op-inspct, Husband, Black, Male, 7688, 0, 40, United-States, >50K.\n```\n\nIf I modify the file and run with this command, it works. Maybe we should fix the raw dataset or change to other valid files.\n\n```\npython ./wide_n_deep_tutorial.py --train_data /home/data/train_data --test_data /home/data/test_data\n```\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nSomeone has asked the similar question in stackoverflow, http://stackoverflow.com/questions/38558976/tensorflow-wide-deep-example-not-working .\n### Environment info\n\nOperating System: Ubuntu 14.04\nTensorFlow: 0.9.0\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nClone the tensorflow project and run `python wide_n_deep_tutorial.py` directly.\n### Logs or other output that would be helpful\n\n```\n# python ./wide_n_deep_tutorial.py\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nTraining data is downloaded to /tmp/tmpUs2und\nTest data is downloaded to /tmp/tmp1aH4KY\nTraceback (most recent call last):\n  File \"./wide_n_deep_tutorial.py\", line 213, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"./wide_n_deep_tutorial.py\", line 209, in main\n    train_and_eval()\n  File \"./wide_n_deep_tutorial.py\", line 194, in train_and_eval\n    df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n  File \"/usr/lib/python2.7/dist-packages/pandas/core/series.py\", line 2023, in apply\n    mapped = lib.map_infer(values, f, convert=convert_dtype)\n  File \"inference.pyx\", line 920, in pandas.lib.map_infer (pandas/lib.c:44780)\n  File \"./wide_n_deep_tutorial.py\", line 194, in <lambda>\n    df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\nTypeError: argument of type 'float' is not iterable\n```\n", "comments": ["@tobegit3hub Thanks for filing this issue!\n\nI see that the `|1x3 Cross validator` line in `adult.test` is unexpected.  Did you find any other issues in the two files?\n\nWe welcome pull requests to fix these kinds of problems!\n", "Thanks @tatatodd .\n\nThese two files have unexpected empty line at the end of the files. After removing the first line of `|1x3 Cross validator` and the empty lines at the end, it will work.\n\nI'm not sure who maintains these datasets. How can we create the pull-request and contribute for this?\n", "You can fork the repo, work on your own branch to make the changes, and then submit a pull request. \n", "Thanks @terrytangyuan and I knew how to create pull-request. I mean how to fix this issue. The dataset is from `https://archive.ics.uci.edu` which may be beyond the scope of TensorFlow.\n\nWould you like to maintain the dataset by yourselves? Or contact someone who can fix this.\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 4292, "title": "Master", "body": "Install numpy from pip\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 4291, "title": "Tensor slice is too large to serialize", "body": "Hi,\nI met a issue said that \"**Tensor slice is too large to serialize** (conservative estimate: 2268204567 bytes)\". And i am wondered that why there is a limitation to prevents the TensorSliceWriter from attempting to serialize variables that are larger than 2GB. How can I to skip this limitation if I want to use a variable large than 2GB? \n\nThanks\nJinlong\n", "comments": ["There is a hard limit of 2GB for serializing individual tensors because of the 32bit signed size in protobuf.\n\nSee #3766, #2648, etc.\nhttp://stackoverflow.com/questions/34128872/google-protobuf-maximum-size\n\nYou need to find a way to break things up into smaller tensors/variables.\n", "@aselle Thanks for your quick response. Are you planning to provide some operations to break big tensors/variables into smaller ones, since it is very easy to exceed 2 GB limitation for some big models. \n", "@concretevitamin can comment further if need be, but the plan is to make tensors be  checkpointed in a format that does not use protobufs, sidestepping this limitation.\n", "Agreed with @aselle.  @jinhou, a quick workaround is to use, for variables, `tf.get_variable(..., partitioner=..)`, and for non-ref tensors, `tf.split()`.\n", "@concretevitamin Are you able to provide an example on how to use tf.split(). Currently my code is something like:\r\n`dataset = tf.data.Dataset.from_tensor_slices((x,y)).map(parse)`\r\n`dataset = dataset.shuffle(buffer_size).repeat().batch(batch_size)`\r\n`return dataset.make_one_shot_iterator().get_next()`", "finally, how to solve that problem?"]}, {"number": 4290, "title": "Aspect Preserving Image Downsample That does not Require Cropping", "body": "Right now it looks like all of the image downsize (ie resize to a smaller size) operations involve either changing aspect ratio (e.g. [resize_images](https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html#resize_images)) or cropping (e.g. [resize_image_with_crop_or_pad](https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html#resize_image_with_crop_or_pad)). Compare this to the [thumbnail operation in Pillow](http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail) that maintains the aspect ratio but does not crop.\n\nThere are some related [questions on SO](http://stackoverflow.com/questions/35208832/tensorflow-resize-image-tensor-to-dynamic-shape).\n", "comments": ["Thanks for filing the issue @cancan101 \n\nThe SO link provides examples of how to resize to dynamic dimensions, which allows you to code this up manually.  It's true that this isn't as convenient as a built-in mechanism.  We welcome pull requests!\n", "It's been a while since this issue has had updates. @cancan101, did you have any chance to work on this? Thanks.", "Hi, more than 1 year and this issue is still open. I also think that a function which maintains the aspect ratio would be great and very useful. You can see #14213 for a small piece of code.\r\nHope someone will work on it! Thanks\r\n", "@Kayoku Would you like to create a PR with your porposal?", "Hi @gunan , sorry I never contribute to a project like TF and I didn't have time to do it this week, I guess it's a simple copy/paste but can you do it for me ? (Else I'll do it next week)\r\nAnyway I'm happy that you think about add this function :)", "Added `preserve_aspect_ratio` option to `resize_images` in commit https://github.com/tensorflow/tensorflow/commit/ffe3d1b4dba7c39a291861e75060a871caab92c3."]}, {"number": 4289, "title": "sigmoid", "body": "which function can replace sigmoid in LSTM or GRU?\n", "comments": ["Since this involves using tensorflow rather than an issue or feature request, please ask this on stack overflow.\n"]}, {"number": 4288, "title": "Error while installing tensorflow on Ubuntu 16.04", "body": "Operating System: Ubuntu 16.04 \n\nInstalled version of CUDA and cuDNN:  CUDA 7.5, cuDNN 5\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n-rw-r--r-- 1 root root 189170 Sep  8 09:47 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Sep  8 09:47 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Sep  8 09:47 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Sep  8 09:47 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Sep  8 09:47 /usr/local/cuda/lib/libcudart_static.a\n```\n\noutput of `ldconfig -p | grep libcudnn`:\n\n```\nlibcudnn.so.5 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudnn.so.5\nlibcudnn.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudnn.so\n```\n\nThe output of `bazel version` :\n\n```\nBuild label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392\n```\n\nThe error I have is: \n\n```\nERROR: /home/cortana/Downloads/tensorflow/tensorflow/core/kernels/BUILD:1711:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:scatter_op_gpu':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/scatter_op_gpu.cu.cc':\n  '/usr/local/cuda-7.5/include/cuda_runtime.h'\n  '/usr/local/cuda-7.5/include/host_config.h'\n  '/usr/local/cuda-7.5/include/builtin_types.h'\n  '/usr/local/cuda-7.5/include/device_types.h'\n  '/usr/local/cuda-7.5/include/host_defines.h'\n  '/usr/local/cuda-7.5/include/driver_types.h'\n  '/usr/local/cuda-7.5/include/surface_types.h'\n  '/usr/local/cuda-7.5/include/texture_types.h'\n  '/usr/local/cuda-7.5/include/vector_types.h'\n  '/usr/local/cuda-7.5/include/channel_descriptor.h'\n  '/usr/local/cuda-7.5/include/cuda_runtime_api.h'\n  '/usr/local/cuda-7.5/include/cuda_device_runtime_api.h'\n  '/usr/local/cuda-7.5/include/driver_functions.h'\n  '/usr/local/cuda-7.5/include/vector_functions.h'\n  '/usr/local/cuda-7.5/include/vector_functions.hpp'\n  '/usr/local/cuda-7.5/include/common_functions.h'\n  '/usr/local/cuda-7.5/include/math_functions.h'\n  '/usr/local/cuda-7.5/include/math_functions.hpp'\n  '/usr/local/cuda-7.5/include/math_functions_dbl_ptx3.h'\n  '/usr/local/cuda-7.5/include/math_functions_dbl_ptx3.hpp'\n  '/usr/local/cuda-7.5/include/cuda_surface_types.h'\n  '/usr/local/cuda-7.5/include/cuda_texture_types.h'\n  '/usr/local/cuda-7.5/include/device_functions.h'\n  '/usr/local/cuda-7.5/include/device_functions.hpp'\n  '/usr/local/cuda-7.5/include/device_atomic_functions.h'\n  '/usr/local/cuda-7.5/include/device_atomic_functions.hpp'\n  '/usr/local/cuda-7.5/include/device_double_functions.h'\n  '/usr/local/cuda-7.5/include/device_double_functions.hpp'\n  '/usr/local/cuda-7.5/include/sm_20_atomic_functions.h'\n  '/usr/local/cuda-7.5/include/sm_20_atomic_functions.hpp'\n  '/usr/local/cuda-7.5/include/sm_32_atomic_functions.h'\n  '/usr/local/cuda-7.5/include/sm_32_atomic_functions.hpp'\n  '/usr/local/cuda-7.5/include/sm_35_atomic_functions.h'\n  '/usr/local/cuda-7.5/include/sm_20_intrinsics.h'\n  '/usr/local/cuda-7.5/include/sm_20_intrinsics.hpp'\n  '/usr/local/cuda-7.5/include/sm_30_intrinsics.h'\n  '/usr/local/cuda-7.5/include/sm_30_intrinsics.hpp'\n  '/usr/local/cuda-7.5/include/sm_32_intrinsics.h'\n  '/usr/local/cuda-7.5/include/sm_32_intrinsics.hpp'\n  '/usr/local/cuda-7.5/include/sm_35_intrinsics.h'\n  '/usr/local/cuda-7.5/include/surface_functions.h'\n  '/usr/local/cuda-7.5/include/surface_functions.hpp'\n  '/usr/local/cuda-7.5/include/texture_fetch_functions.h'\n  '/usr/local/cuda-7.5/include/texture_fetch_functions.hpp'\n  '/usr/local/cuda-7.5/include/texture_indirect_functions.h'\n  '/usr/local/cuda-7.5/include/texture_indirect_functions.hpp'\n  '/usr/local/cuda-7.5/include/surface_indirect_functions.h'\n  '/usr/local/cuda-7.5/include/surface_indirect_functions.hpp'\n  '/usr/local/cuda-7.5/include/device_launch_parameters.h'\n  '/usr/local/cuda-7.5/include/cuda_fp16.h'\n  '/usr/local/cuda-7.5/include/math_constants.h'\n  '/usr/local/cuda-7.5/include/curand_kernel.h'\n  '/usr/local/cuda-7.5/include/curand.h'\n  '/usr/local/cuda-7.5/include/curand_discrete.h'\n  '/usr/local/cuda-7.5/include/curand_precalc.h'\n  '/usr/local/cuda-7.5/include/curand_mrg32k3a.h'\n  '/usr/local/cuda-7.5/include/curand_mtgp32_kernel.h'\n  '/usr/local/cuda-7.5/include/cuda.h'\n  '/usr/local/cuda-7.5/include/curand_mtgp32.h'\n  '/usr/local/cuda-7.5/include/curand_philox4x32_x.h'\n  '/usr/local/cuda-7.5/include/curand_globals.h'\n  '/usr/local/cuda-7.5/include/curand_uniform.h'\n  '/usr/local/cuda-7.5/include/curand_normal.h'\n  '/usr/local/cuda-7.5/include/curand_normal_static.h'\n  '/usr/local/cuda-7.5/include/curand_lognormal.h'\n  '/usr/local/cuda-7.5/include/curand_poisson.h'\n  '/usr/local/cuda-7.5/include/curand_discrete2.h'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\n/usr/include/string.h: In function 'void* __mempcpy_inline(void*, const void*, size_t)':\n/usr/include/string.h:652:42: error: 'memcpy' was not declared in this scope\n   return (char *) memcpy (__dest, __src, __n) + __n;\n                                          ^\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 46.586s, Critical Path: 44.80s\n```\n", "comments": ["This might be related to #3985, where it suggests a few things:\n1. Run `bazel clean`\n2. Then run `configure` again, manually specifying the cuda version.\n\nPlease report back if that helps, or if you think #3985 is or isn't related.  Thanks!\n", "I got similar errors, and tatatodd's way doesn't work.\n\n@tatatodd \n", "@tatatodd 's way didn't work for me either . Haven't been able to successfully install and am having same errors as before \n", "@daemonSlayer \nhttps://github.com/tensorflow/tensorflow/issues/1157\n\nmaybe help, I'm trying.\n", "I wasn't able to work with that configuration on other libraries like caffe either. \n(On this config, I successfully installed and tested caffe and theano)\nNow my system details are:\nOS: ubuntu 16.04\ngpu: nvidia 940mx\ncuda : 8.0rc\ncudnn: 5.1\n\nI am having this error: \n\n```\nERROR: /home/cortana/Desktop/tensorflow/tensorflow/core/kernels/BUILD:2184:1: no such target '//tensorflow/core:android_tensorflow_lib_lite': target 'android_tensorflow_lib_lite' not declared in package 'tensorflow/core' defined by /home/cortana/Desktop/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels'.\nERROR: /home/cortana/Desktop/tensorflow/tensorflow/core/kernels/BUILD:2184:1: no such target '//tensorflow/core:android_tensorflow_lib_lite': target 'android_tensorflow_lib_lite' not declared in package 'tensorflow/core' defined by /home/cortana/Desktop/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels'.\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\nConfiguration finished\n\n```\n\nIn order to remove these errors, I tried commenting out if_android, if_ios from tensorflow/core/BUILD but that hasnt helped.\n", "Can I use the local copy of all the github repos required by tensorflow during configure. It takes a lot of time as my connection is slow. So a way to use a local copy would be really helpful.\n", "@daemonSlayer perhaps the version of tensorflow you're installing is old?  Please answer the following, to let me know how you installed tensorflow.\n\n```\nIf installed from binary pip package, provide:\n\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n\n1. The commit hash (`git rev-parse HEAD`)\n```\n", "I installed tensorflow from source. I am using the master branch of tensorflow .\nThe output to commit hash is:\n270cf03d58922f728b0d14636e0afc28d192b6fd\n\nIt'd be great if you knew of a way to use local copy of grpc and other repos. It takes too much time for every time I try to install the lib again.\n", "Also, even though I am having these errors:\n\n```\nERROR: /home/cortana/Desktop/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1: no such target '//tensorflow/core:android_lib_lite': target 'android_lib_lite' not declared in package 'tensorflow/core' defined by /home/cortana/Desktop/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle'.\nINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/INFO: Cloning submodules for httINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for https://github.com/grpc/grpc.git: Receiving objectsINFO: Cloning submodules for httERROR: /home/cortana/Desktop/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1: no such target '//tensorflow/core:android_lib_lite': target 'android_lib_lite' not declared in package 'tensorflow/core' defined by /home/cortana/Desktop/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle'.\nERROR: /home/cortana/Desktop/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by /home/cortana/Desktop/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle'.\nERROR: /home/cortana/Desktop/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by /home/cortana/Desktop/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle'.\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\nConfiguration finished\n```\n\nI still tried `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`.\nIt gives these errors:\n\n```\nERROR: /home/cortana/Desktop/tensorflow/tensorflow/core/kernels/BUILD:918:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:resize_bilinear_op_gpu':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/resize_bilinear_op_gpu.cu.cc':\n  '/usr/local/cuda-8.0/include/cuda_runtime.h'\n  '/usr/local/cuda-8.0/include/host_config.h'\n  '/usr/local/cuda-8.0/include/builtin_types.h'\n  '/usr/local/cuda-8.0/include/device_types.h'\n  '/usr/local/cuda-8.0/include/host_defines.h'\n  '/usr/local/cuda-8.0/include/driver_types.h'\n  '/usr/local/cuda-8.0/include/surface_types.h'\n  '/usr/local/cuda-8.0/include/texture_types.h'\n  '/usr/local/cuda-8.0/include/vector_types.h'\n  '/usr/local/cuda-8.0/include/library_types.h'\n  '/usr/local/cuda-8.0/include/channel_descriptor.h'\n  '/usr/local/cuda-8.0/include/cuda_runtime_api.h'\n  '/usr/local/cuda-8.0/include/cuda_device_runtime_api.h'\n  '/usr/local/cuda-8.0/include/driver_functions.h'\n  '/usr/local/cuda-8.0/include/vector_functions.h'\n  '/usr/local/cuda-8.0/include/vector_functions.hpp'\n  '/usr/local/cuda-8.0/include/common_functions.h'\n  '/usr/local/cuda-8.0/include/math_functions.h'\n  '/usr/local/cuda-8.0/include/math_functions.hpp'\n  '/usr/local/cuda-8.0/include/math_functions_dbl_ptx3.h'\n  '/usr/local/cuda-8.0/include/math_functions_dbl_ptx3.hpp'\n  '/usr/local/cuda-8.0/include/cuda_surface_types.h'\n  '/usr/local/cuda-8.0/include/cuda_texture_types.h'\n  '/usr/local/cuda-8.0/include/device_functions.h'\n  '/usr/local/cuda-8.0/include/device_functions.hpp'\n  '/usr/local/cuda-8.0/include/device_atomic_functions.h'\n  '/usr/local/cuda-8.0/include/device_atomic_functions.hpp'\n  '/usr/local/cuda-8.0/include/device_double_functions.h'\n  '/usr/local/cuda-8.0/include/device_double_functions.hpp'\n  '/usr/local/cuda-8.0/include/sm_20_atomic_functions.h'\n  '/usr/local/cuda-8.0/include/sm_20_atomic_functions.hpp'\n  '/usr/local/cuda-8.0/include/sm_32_atomic_functions.h'\n  '/usr/local/cuda-8.0/include/sm_32_atomic_functions.hpp'\n  '/usr/local/cuda-8.0/include/sm_35_atomic_functions.h'\n  '/usr/local/cuda-8.0/include/sm_60_atomic_functions.h'\n  '/usr/local/cuda-8.0/include/sm_60_atomic_functions.hpp'\n  '/usr/local/cuda-8.0/include/sm_20_intrinsics.h'\n  '/usr/local/cuda-8.0/include/sm_20_intrinsics.hpp'\n  '/usr/local/cuda-8.0/include/sm_30_intrinsics.h'\n  '/usr/local/cuda-8.0/include/sm_30_intrinsics.hpp'\n  '/usr/local/cuda-8.0/include/sm_32_intrinsics.h'\n  '/usr/local/cuda-8.0/include/sm_32_intrinsics.hpp'\n  '/usr/local/cuda-8.0/include/sm_35_intrinsics.h'\n  '/usr/local/cuda-8.0/include/surface_functions.h'\n  '/usr/local/cuda-8.0/include/texture_fetch_functions.h'\n  '/usr/local/cuda-8.0/include/texture_indirect_functions.h'\n  '/usr/local/cuda-8.0/include/surface_indirect_functions.h'\n  '/usr/local/cuda-8.0/include/device_launch_parameters.h'\n  '/usr/local/cuda-8.0/include/cuda_fp16.h'\n  '/usr/local/cuda-8.0/include/math_constants.h'\n  '/usr/local/cuda-8.0/include/curand_kernel.h'\n  '/usr/local/cuda-8.0/include/curand.h'\n  '/usr/local/cuda-8.0/include/curand_discrete.h'\n  '/usr/local/cuda-8.0/include/curand_precalc.h'\n  '/usr/local/cuda-8.0/include/curand_mrg32k3a.h'\n  '/usr/local/cuda-8.0/include/curand_mtgp32_kernel.h'\n  '/usr/local/cuda-8.0/include/cuda.h'\n  '/usr/local/cuda-8.0/include/curand_mtgp32.h'\n  '/usr/local/cuda-8.0/include/curand_philox4x32_x.h'\n  '/usr/local/cuda-8.0/include/curand_globals.h'\n  '/usr/local/cuda-8.0/include/curand_uniform.h'\n  '/usr/local/cuda-8.0/include/curand_normal.h'\n  '/usr/local/cuda-8.0/include/curand_normal_static.h'\n  '/usr/local/cuda-8.0/include/curand_lognormal.h'\n  '/usr/local/cuda-8.0/include/curand_poisson.h'\n  '/usr/local/cuda-8.0/include/curand_discrete2.h'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 461.388s, Critical Path: 393.27s\n\n```\n", "Thanks @tatatodd @Eidosper for helping me out. I wasn't able to install tensorflow from source and really couldn't wait, so now I have successfully started using the docker image.\n"]}, {"number": 4287, "title": "Added missing files to makefile", "body": "This fixes a build error with the makefile process.\n", "comments": ["@petewarden, thanks for your PR! By analyzing the annotation information on this pull request, we identified @martinwicke, @tensorflower-gardener and @vrv to be potential reviewers\n"]}, {"number": 4286, "title": "Create ADOPTERS.md", "body": "", "comments": []}, {"number": 4285, "title": "Force clean+fetch when re-running configure with different settings.", "body": "- Run bazel clean and bazel fetch in the configure script even when building\n  without GPU support to force clean+fetch if the user re-runs ./configure\n  with a different setting.\n- Print a more actionable error messsage if the user attempts to build with\n  --config=cuda but did not configure TensorFlow to build with GPU support.\n- Update the BUILD file in @local_config_cuda to use repository-local labels.\n\nFixes #4105\n", "comments": ["@davidzchen, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @vrv and @keveman to be potential reviewers\n", "Jenkins, test this please.\n", "+cc @damienmg \n\nThe error messages are somewhat mysterious and seem to be an issue with Bazel embedded tools:\n\n```\nERROR: error loading package 'bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/install/2ed8c6c819c55073ad03a9309500f9b2/_embedded_binaries/embedded_tools/src/main/protobuf': Extension file not found. Unable to load package for '//tools/build_rules:genproto.bzl': BUILD file not found on package path.\n```\n\n```\nERROR: package contains errors: bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/install/_embedded_binaries/embedded_tools/third_party/checker_framework_javacutil.\nERROR: error loading package 'bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/install/_embedded_binaries/embedded_tools/third_party/checker_framework_javacutil': Extension file not found. Unable to load package for '//tools/build_rules:java_rules_skylark.bzl': BUILD file not found on package path.\n```\n", "LGTM, not sure why you get error from Jenkins though.\n", "@tensorflow-jenkins test this please.\n", "@damienmg I have added the change to use `cc_toolchain_suite` for the `CROSSTOOL` file to this PR, though I do not completely understand how that rule works since it is undocumented. Does it implicitly look for a `CROSSTOOL` file in the current package?\n\n@vrv Which version of Bazel is installed on the CI machines?\n", "I believe we are on bazel 0.3.1\n", "Jenkins, retest this please.\n", "@davidzchen This appears to still be failing as of today:\n\n```\nERROR: package contains errors: bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/install/2ed8c6c819c55073ad03a9309500f9b2/_embedded_binaries/embedded_tools/src/main/protobuf.\nERROR: error loading package 'bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/install/2ed8c6c819c55073ad03a9309500f9b2/_embedded_binaries/embedded_tools/src/main/protobuf': Extension file not found. Unable to load package for '//tools/build_rules:genproto.bzl': BUILD file not found on package path.\n```\n", "After investigating this further, these errors appear to be a Bazel issue caused by files that are missing from Bazel's embedded tools workspace.\n\nI have opened bazelbuild/bazel#1787 for this bug.\n", "I think I may have been able to work around bazelbuild/bazel#1787, but now the build is failing due to changes introduced by ed87884e50e1a50f7dc7b36dc7a7ff225442bee0.\n", "Jenkins, test this please.\n", "I can't tell whether these errors are transient or not. There seem to be issues with network in general in the last couple of days.\n", "Rebased and all tests are now passing. :)\n", "@caisq we had to add this to jenkins job configs recently.\nDo we need to keep doing clean with each configure run?\nIs this a bug in bazel that is expected to be resolved?\n", "@gunan We currently need to run clean with each configure run to ensure that the `@local_config_cuda` repository is invalidated in case the configuration changes. IIUC, Bazel currently does not invalidate external repositories when environment variables change, but a future enhancement might be allowing repository rules to declare which environment variables it requires and invalidate if those variables are changed.\n", "We have reason to believe, it does not invalidate external repos in case their build rules, or their definitions in the workspace also changes.\nI will reach out to you with more details.\n", "Yes it is a known issue. I need to fix it. If local=True it should invalidate though.\n\nI also plan to indeed be able to declare invalidation on env variable. We have the internal machinery now.\n", "So the whole cache invalidation system doesn't extend to external repository definitions?\n\nIf so, that's a darn shame. Cache invalidation is the No. 1 problem Bazel exists to solve. I hope the Bazel team will choose to prioritize solving it maximally. Even if it's one of the two hardest problems in computer science :)\n"]}, {"number": 4284, "title": "Cherry-picking r0.10 fixes", "body": "", "comments": ["@caisq, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @keveman and @girving to be potential reviewers\n"]}, {"number": 4283, "title": "Breaking API change without deprecation warning", "body": "### Problem\n\nThe name_scope API has changed between last week and last night's latest changes in the 0.10 branch. The previous API was:\n\n```\nname_scope(*args, **kwds)\n```\n\nThe current API is:\n\n```\nname_scope(name)\n```\n\nThe API was changed in a breaking way, rather than the new API being added and the use of the old API triggering a warning.\n\nI was wondering what the TensorFlow gatekeepers' policy is on breaking API changes. Is there an official policy? Is the breaking change without a deprecation warning and phase-out intentional?\n", "comments": ["This is a deliberate change. Before 1.0, we have very few guarantees for the API itself. In fact, we're trying to make breaking changes before 1.0, so their frequency may increase in the coming months. \n\nPlease take a look at [our policy](https://www.tensorflow.org/versions/r0.10/resources/versions.html) regarding code. \n\nFor stored graphs, the guarantees are [stronger](https://www.tensorflow.org/versions/r0.10/resources/data_versions.html). \n\nAfter 1.0, there should not be breaking changes in minor releases, only for major ones (i.e., the next ones at 2.0).\n\nNote that regardless of version, development at head (or branch head) is inherently dangerous when it comes to stability due to bugs.\n", "So TF shouldn't be used in production until 1.0?  Is there documented rough schedule/timeline for 1.0?\n", "There's no problem with using TF in production (we use it plenty). But I _would_ advise against using TF _at head using continuous updates_ in a production environment.\n\nTo use TF in production, I would use a release (or, if you must, a specific hash), and update TF only in a controlled manner. In other words, treat it the same as other libraries you use in production: when you update the library to a new version, verify that everything works afterwards. \n\nEven updating more mature libraries like numpy to a new version is not something you can safely do in production. \n\nTF is no different in principle, although because of various factors (cuda, rapid development) people are much more likely to use TF at head, and so the pressure up update is much higher. \n", "`update TF only in a controlled manner.`\nSeems like the problem here is that there's no warnings or release notes when breaking changes occur. How should someone like me (external to Google, using TF) know which versions are safe and which ones are unsafe to depend upon? What strategy do you recommend, @martinwicke ?\n", "There are release notes when breaking changes occur: https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md, though it's clear it isn't always filled in with all changes :(.\n"]}, {"number": 4282, "title": "Unspecified dimension after tf.sparse_tensor_dense_matmul", "body": "### Environment info\n\nOperating System:\n\nProblem encountered on linux CPU build installed from `47501a5ebc62fcb8a3d7832722d39997696897dc`.  \n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n```\nimport tensorflow as tf\nindices = [[0, 0], [1, 2]]\nvalues = [1., 1.]\nshape = [4, 6]\ntest_sparse = tf.SparseTensor(indices, values, shape)\nv = tf.ones((6, 1))\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n\nmult = tf.sparse_tensor_dense_matmul(test_sparse, v)\nprint(mult)\n--> <tf.Tensor 'SparseTensorDenseMatMul_1/SparseTensorDenseMatMul:0' shape=(?, 1) dtype=float32>\n\n# but of course\nsess.run(mult).shape\n--> (4, 1)\n```\n\nMy question is: why is it that the shape of `mult` is partially unspecified before run time? It can be inferred from the shape of `test_sparse` and `v` that `mult` will have shape `(4, 1)`. \n\nThis is very annoying because it prevents me from initializing a Variable with the result of a sparse matmul as in the following:\n\n```\nmult_val = tf.Variable(tf.sparse_tensor_dense_matmul(test_sparse, v))\n--> ValueError: initial_value must have a shape specified: Tensor(\"SparseTensorDenseMatMul/SparseTensorDenseMatMul:0\", shape=(?, 1), dtype=float32)\n```\n\nOn the other hand, dense matmul has the desired behavior:\n\n```\ndense_mult = tf.matmul(tf.sparse_tensor_to_dense(test_sparse, v))\ndense_mult.get_shape()\n--> TensorShape([Dimension(4), Dimension(1)])\n```\n\nSo why does sparse matmul behave differently? Is this a bug? An implementation quirk? \n\nIn the meanwhile, how can I initialize a Variable with the result of a sparse matmul without having to densify the sparse matrix first?\n", "comments": ["OK, there is an easy solution to this problem. Simply construct the variable with\n\n```\nmult_val = tf.Variable(tf.sparse_tensor_dense_matmul(test_sparse, v), validate_shape=False)\n```\n\nI'm leaving this open though, because I still don't see why sparse_tensor_dense_matmul should return a tensor with unspecified shape.\n", "@rueberger nice that you found as solution to your immediate problem!\n@concretevitamin can correct me if what I say below is wrong, or if we have future plans to improve things.\n\nI think you can classify this as an \"implementation quirk\".  Let's start with the representation of sparse tensors.  Basically you can think of SparseTensor as a convenient wrapper around 3 underlying tensors, which represent the indices, values and shape independently:\nhttps://www.tensorflow.org/versions/r0.10/api_docs/python/sparse_ops.html#SparseTensor\n\nFor a regular ops on non-sparse tensors, the shape of the output is determined by the shapes of the input tensors, as you expect.\n\nLet's say that's all we're given, and we try to implement SparseTensor ourselves, on top of this framework.  Well the problem is that the shape of the SparseTensor is no longer fixed; since it is itself represented as a tensor.\n\nAnother way to look at at:\n- The shape of `[1,2,3]` is `[3]`, since we have a 1-dimensional vector with 3 elements.\n- The shape of `[4,6]` is `[2]`\n- The shape of the `shape = [4, 6]` variable in your example is `[2]`\n\nSo basically if we limit ourselves to looking at the shapes, and not at the values, we end up the behavior you've reported.  Hope this helps!\n", "Ah, of course, thanks for the explanation. Perhaps it would be possible to slightly modify tf.sparse_tensor_dense_matmul to have a defined shape when the `shape` argument is passed as an array? That would be a pretty easy change, no?\n", "Is this still current?", "Possibly. I just reported crashing behavior that seems to be related to the dimension of the tensors in sparse_tensor_dense_matmul (#7417). I'm on version 0.12.1.", "(I'm mainly referring to \"crash 2\" from that report, where `sparse_tensor_dense_matmul_op.cc` seems to get confused about the number of rows.  I'll let you know when I have a reproducible example).", "(example added.  Sorry for the triple comment!)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}]