[{"number": 40404, "title": "Restore multiple (different) variables from the same checkpoint tensor.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.15\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, if I use the `init_from_checkpoint()` with an `assignment_map`, I can't map one checkpoint tensor to multiple separate graph variables. Using a list only works for multiple partitions of the same variable. For my research use-case, I want to restore `n` matrices of the same shape, from a single checkpoint(ed) matrix of the same shape as well. Once initialisation is done however, these `n` matrices have no co-relation with one another. They will be trained and updated completely independent of each other. Hence I don't want the same variable in the graph.\r\n\r\n**Will this change the current api? How?**\r\nIt may change the semantics of assignment map, I am not sure.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to re-use large pretrained deep learning models, but play around with architectural changes, say for pruning or model compression.\r\n\r\n**Any Other info.**\r\nI am wondering if as a workaround, I can have multiple calls to `init_from_checkpoint()`, one for each `i in n`? And restore them one by one basically. Will the `i'th` call \"overwrite\" the `i-1'th` call?", "comments": ["TensorFlow v2 has upgraded many of the saving features, and allows for more robust model saving and reloading. See https://www.tensorflow.org/guide/saved_model for more information. You can also take a look at the pruning features available from the Model Optimization Toolkit: https://www.tensorflow.org/model_optimization\r\n\r\nIf you have feature requests for the TF v2 APIs, please post a new issue, and we can discuss there."]}, {"number": 40403, "title": "GPU-accelerated LSTMs/GRUs crash randomly with: [ InternalError: [_Derived_] Failed to call ThenRnnBackward with model config ]", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro, Build 19041\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5\r\n- GPU model and memory: NVidia Titan RTX, 24GB, RTX 2080 Ti, 11GB\r\n\r\n- nvidia driver version: 450.99\r\n\r\n**Describe the current behavior**\r\nBoth the Jupyter Notebook and extract Python script on the [Tensorflow Text Classification Tutorial ](https://www.tensorflow.org/tutorials/text/text_classification_rnn) crashes randomly when training locally on my GPU, with the following traceback:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InternalError:  [_Derived_]  Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 64, 64, 1, 2537, 64, 64]\r\n         [[{{node CudnnRNN}}]]\r\n         [[sequential/bidirectional/forward_lstm/StatefulPartitionedCall]]\r\n         [[gradient_tape/sequential/embedding/embedding_lookup/Reshape/_38]] [Op:__inference_train_function_6128]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function\r\n```\r\nI found two similar issues [#37942](https://github.com/tensorflow/tensorflow/issues/37942) and [#35950 ](https://github.com/tensorflow/tensorflow/issues/35950)\r\n\r\nMethods suggested in #37942 did not work and still crashes.\r\n\r\n**Describe the expected behavior**\r\nExample tutorial notebooks should run smoothly from top to bottom without random crashes.\r\n\r\n**Standalone code to reproduce the issue**\r\n[Github Gist here.](https://gist.github.com/leehanchung/8da991bf1264c19324920349171386bc)\r\n\r\nCode:\r\n```\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\ndef plot_graphs(history, metric):\r\n    plt.plot(history.history[metric])\r\n    plt.plot(history.history['val_'+metric], '')\r\n    plt.xlabel(\"Epochs\")\r\n    plt.ylabel(metric)\r\n    plt.legend([metric, 'val_'+metric])\r\n    plt.show()\r\n\r\n\r\ndataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\r\n                          as_supervised=True)\r\ntrain_dataset, test_dataset = dataset['train'], dataset['test']\r\n\r\nencoder = info.features['text'].encoder\r\nprint('Vocabulary size: {}'.format(encoder.vocab_size))\r\n\r\nsample_string = 'Hello TensorFlow.'\r\n\r\nencoded_string = encoder.encode(sample_string)\r\nprint('Encoded string is {}'.format(encoded_string))\r\n\r\noriginal_string = encoder.decode(encoded_string)\r\nprint('The original string: \"{}\"'.format(original_string))\r\n\r\nassert original_string == sample_string\r\n\r\nfor index in encoded_string:\r\n    print('{} ----> {}'.format(index, encoder.decode([index])))\r\n\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 64\r\n\r\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE)\r\ntrain_dataset = train_dataset.padded_batch(BATCH_SIZE)\r\ntest_dataset = test_dataset.padded_batch(BATCH_SIZE)\r\n\r\nfor example_batch, label_batch in train_dataset.take(20):\r\n    print(\"Batch shape:\", example_batch.shape)\r\n    print(\"label shape:\", label_batch.shape)\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Embedding(encoder.vocab_size, 64),\r\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\r\n    tf.keras.layers.Dense(64, activation='relu'),\r\n    tf.keras.layers.Dense(1)\r\n])\r\n\r\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n              optimizer=tf.keras.optimizers.Adam(1e-4),\r\n              metrics=['accuracy'])\r\n\r\nhistory = model.fit(train_dataset, epochs=10,\r\n                    validation_data=test_dataset, \r\n                    validation_steps=30)\r\ntest_loss, test_acc = model.evaluate(test_dataset)\r\n\r\nprint('Test Loss: {}'.format(test_loss))\r\nprint('Test Accuracy: {}'.format(test_acc))\r\n```\r\n\r\n**Other info / logs** \r\n```\r\nEpoch 1/10\r\n2020-06-11 23:48:47.036226: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-06-11 23:48:47.417459: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n 39/391 [=>............................] - ETA: 33s - loss: 0.6931 - accuracy: 0.50282020-06-11 23:48:52.108366: E tensorflow/stream_executor/dnn.cc:613] CUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1986): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), \r\ninput_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), \r\nreserve_space_data->size())'\r\n2020-06-11 23:48:52.109818: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 64, 64, 1, 1615, 64, 64] \r\nTraceback (most recent call last):\r\n  File \".\\lesson1.py\", line 59, in <module>\r\n    validation_steps=30)\r\n  File \"C:\\Users\\Han\\.virtualenvs\\tensorflow-in-practice-9XcfUv0Y\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"C:\\Users\\Han\\.virtualenvs\\tensorflow-in-practice-9XcfUv0Y\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 848, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"C:\\Users\\Han\\.virtualenvs\\tensorflow-in-practice-9XcfUv0Y\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\Han\\.virtualenvs\\tensorflow-in-practice-9XcfUv0Y\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 611, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"C:\\Users\\Han\\.virtualenvs\\tensorflow-in-practice-9XcfUv0Y\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2420, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access  File \"C:\\Users\\Han\\.virtualenvs\\tensorflow-in-practice-9XcfUv0Y\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1665, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"C:\\Users\\Han\\.virtualenvs\\tensorflow-in-practice-9XcfUv0Y\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1746, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"C:\\Users\\Han\\.virtualenvs\\tensorflow-in-practice-9XcfUv0Y\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 598, in call\r\n    ctx=ctx)\r\n  File \"C:\\Users\\Han\\.virtualenvs\\tensorflow-in-practice-9XcfUv0Y\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 64, 64, 1, 1615, 64, 64]\r\n         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n         [[StatefulPartitionedCall_1]]\r\n         [[gradient_tape/sequential/embedding/embedding_lookup/Reshape/_38]] [Op:__inference_train_function_6172]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function\r\n```", "comments": ["@leehanchung \r\n\r\nI tried in colab with TF version 2.2 and i am not seeing any issue.Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/98b63c4661740fa71e893cf5157c79d6/untitled12.ipynb).Thanks!", "Hi @ravikyram,\r\n\r\nYes the code runs on Colab.  But issue is specific to Windows 10 w/ GPU.", "Did more testing and fixed the problem.\r\n\r\nThis problem only shows up with `tf.keras.Bidirectional` wrapped around rnn/lstm/gru.\r\n\r\nFixed the problem by downgrading nvidia driver.\r\n\r\nCurrent software stack that works with `Bidirectional`:\r\n```\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro, Build 19041\r\n    TensorFlow installed from (source or binary): pip install tensorflow\r\n    TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n    Python version: 3.7.4\r\n    CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5\r\n    GPU model and memory: NVidia Titan RTX, 24GB, RTX 2080 Ti, 11GB\r\n    NVIDIA driver version: 431.86\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40403\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40403\">No</a>\n"]}, {"number": 40402, "title": "Usage and signature of Model.train_step() is unclear", "body": "## URL(s) with the issue:\r\n\r\n[Colab link](https://colab.research.google.com/drive/1A6gugIKUl3GCsGFlvmghFHQy5CAOd0RC?usp=sharing)\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThere seems to be some kind of restriction on the signature of Model.train_step(), which is undocumented.\r\n\r\n### Clear description\r\n\r\nThe issue comes up in TF 2.2.0, where the possibility to overwrite Model.train_step() was introduced.\r\n\r\nThe training is executed with Model.fit() with a generator as an input. The generator yields 4 tensors, which are combined in train_step() to produce a loss and gradients.\r\n\r\nThe logic fails, because tf.keras in the fit() function checks that the generator outputs at most 3 tensors (corresponding to x, y, and weights), so this mindset and the x, y, w signature is implicity forced onto the train_step() function.\r\n\r\nThis is not clear from the documentation, which only states, that train_step() has a single argument (data), which should be a 'A nested structure of Tensors.' [Link to documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#train_step)\r\n\r\nThe also seems to be a lack of official examples using this new API, which would help determine the sequence of methods one needs to call in order to use the new API (calling Model.compile() for instance).\r\n\r\n### Expected behaviour\r\n\r\nAccording to both the documentation and the release notes on TF 2.2.0, I was expecting to be able to use an arbitrary list of tensors as parameter to Model.train_step().\r\n\r\nI would expect that a restriction on Model.train_step()'s sole 'data' argument would be documented in the relevant documentation.\r\n\r\n## Disclaimer\r\n\r\nPlease note if I get the usage of Model.train_step() wrong. Also if there is an example out there using this new and very convenient API, feel free to direct me towards it.", "comments": ["Update: I solved it by issuing the following modifications:\r\n\r\n1) The data needs to be wrapped in a one-element tuple, eg. `yield ((tensor1, tensor2, ...),)`. This structure needs to be unpacked in the body of the Model.train_step() function.\r\n\r\n2) Added a line which builds the forward function. The line can either be a Model.predict() or a Model.build() call with the correct arguments.\r\n\r\nIt would be beneficial to somehow indicate in the documentation that explicit building of forward and backward passes are required for train_step() to work.", "Hi @csxeba, have you seen the [Customize what happens in Model.fit ](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit) guide?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 40401, "title": "Importing TensorFlow package in PyCharm", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: NVIDIA Geforce GTX 1050\r\n\r\n\r\n**Describe the problem**\r\n\r\nI installed tensorflow using `pip install tensorflow ` in cmd and tested in python its working or not using `import tensorflow as tf; print(tf.__version__)` successfully ran and got output 2.2.0. What means is I have installed tensorflow  correctly on my PC. But when I am trying to import tensorflow package in Pycharm using following Steps it doesn't work. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nSteps:\r\n1.Opened project.\r\n2. File -> Settings ->Project Interpreter (under Project: <Your project title>)\r\n![image](https://user-images.githubusercontent.com/46365760/84466587-51c31500-ac97-11ea-9749-9a230b58e8c5.png)\r\n3. Okay so there is no tensorflow package I need to add it now!\r\n4.When I hit + button to add package I found Tensorflow but when I import it is says `conda.exceptions.UnsatisfiableError:`\r\n![image](https://user-images.githubusercontent.com/46365760/84466734-b8e0c980-ac97-11ea-920d-af87263bafbc.png)\r\n5. Error Screenshot\r\n![image](https://user-images.githubusercontent.com/46365760/84466975-5b994800-ac98-11ea-8cbd-811f4d8999f8.png)\r\n\r\n\r\n**Any other info / logs**\r\n\r\nTried Solutions:\r\n1. Tensorflow works in Spyder too for me perfectly.\r\n2. Tried creating new interpreter also but no luck!\r\n\r\nOther Questions:\r\n1. Why in second screenshot tensorflow version 2.1.0 is shown at bottom right side(Specific version) as I haven't installed any other versions previously and no option of 2.2.0?\r\n2. Why tensorflow is located in Anaconda folder while I haven't installed tensorflow through Anaconda? \r\n3. Coping tensorflow package folder to pkgs folder i.e in my case from `C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\tensorflow` to `C:\\ProgramData\\Anaconda3\\pkgs` Pycharm will able to access tensorflow ?\r\n4. I referred a lot of your previous solved issues but why none have proper solution for 2.2.0?", "comments": ["Conda has poor compatibility with PyCharm. If you have `conda` installed in your system, you would have to start PyCharm manually from an activate conda environment, e.g. `base`.\r\n\r\nThere are two ways to solve this:\r\n1. Select `conda` when you create an virtual environment in PyCharm.\r\n2. Launch PyCharm from your command prompt/powershell *after* you activate an conda enviroment (`base` is fine).  And then select `venv` when you create an virtual environment in PyCharm\r\n\r\nFor example, I installed PyCharm for myself only, and I have to launch PyCharm by the following for #2 to work:\r\n```\r\ncd AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-C\\ch-0\\201.7846.77\\bin\r\n./pycharm.bat\r\n```\r\n\r\nHope this helps.\r\n\r\nMy spec is as follows:\r\n```\r\nWindows 10 Pro, Build 19041\r\nnvidia-smi CUDA Version: 11.0\r\nnvcc CUDA version: 10.1\r\n```", "> Conda has poor compatibility with PyCharm. If you have `conda` installed in your system, you would have to start PyCharm manually from an activate conda environment, e.g. `base`.\r\n> \r\n> There are two ways to solve this:\r\n> \r\n> 1. Select `conda` when you create an virtual environment in PyCharm.\r\n> 2. Launch PyCharm from your command prompt/powershell _after_ you activate an conda enviroment (`base` is fine).  And then select `venv` when you create an virtual environment in PyCharm\r\n> \r\n> For example, I installed PyCharm for myself only, and I have to launch PyCharm by the following for #2 to work:\r\n> \r\n> ```\r\n> cd AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-C\\ch-0\\201.7846.77\\bin\r\n> ./pycharm.bat\r\n> ```\r\n> \r\n> Hope this helps.\r\n> \r\n> My spec is as follows:\r\n> \r\n> ```\r\n> Windows 10 Pro, Build 19041\r\n> nvidia-smi CUDA Version: 11.0\r\n> nvcc CUDA version: 10.1\r\n> ```\r\n\r\n@leehanchung  Is there anything to do with environmental variables? I have added all envs while installation.", "@jforjay1 It's a known issue for PyCharm on Windows. PyCharm creates the venv using conda environment (conda is the bully). But when PyCharm activates the venv it doesnt activate conda first, thus the issue.\r\n\r\nSo the workaround, at least for me, is to launch PyCharm within an activated conda environment from command line.\r\n\r\nTry my workaround and see if that works for you.", "> @jforjay1 It's a known issue for PyCharm on Windows. PyCharm creates the venv using conda environment (conda is the bully). But when PyCharm activates the venv it doesnt activate conda first, thus the issue.\r\n> \r\n> So the workaround, at least for me, is to launch PyCharm within an activated conda environment from command line.\r\n> \r\n> Try my workaround and see if that works for you.\r\n\r\n@leehanchung  I did it. Thanks mate.\r\nThere was another environment as `myenv` already I added that environment and interpreter located inside it. The problem is how to locate environment of conda in Pycharm but I got it now. Thanks\u270c.", "@jforjay1\r\nIs this still an issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40401\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40401\">No</a>\n"]}, {"number": 40400, "title": "Build failure with master: logging_op_resolver", "body": "I am attempting to build the current development master and the built fails with fatal errors\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): github master\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): gpp-8\r\n- CUDA/cuDNN version: 10.2/8.0\r\n- GPU model and memory: RTX 2060 6gb\r\n\r\n**Describe the problem**\r\nAfter configuring and beginning the build, near the end I receive the following error:\r\n\r\n```\r\nERROR: /home/user/Development/Software/TensorFlow/tensorflow/tensorflow/lite/tools/optimize/calibration/BUILD:84:1: C++ compilation of rule '//tensorflow/lite/tools/optimize/calibration:logging_op_resolver' failed (Exit 1)\r\ntensorflow/lite/tools/optimize/calibration/logging_op_resolver.cc: In constructor 'tflite::optimize::calibration::LoggingOpResolver::LoggingOpResolver(const BuiltinOpsSet&, const CustomOpsSet&, const tflite::OpResolver&, tflite::optimize::calibration::KernelEvalFuncPtr)':\r\ntensorflow/lite/tools/optimize/calibration/logging_op_resolver.cc:78:10: error: 'FATAL' was not declared in this scope\r\n     DLOG(FATAL) << error_message;\r\n          ^~~~~\r\ntensorflow/lite/tools/optimize/calibration/logging_op_resolver.cc:78:10: note: suggested alternative:\r\nIn file included from ./tensorflow/core/platform/logging.h:27,\r\n                 from tensorflow/lite/tools/optimize/calibration/logging_op_resolver.cc:20:\r\n./tensorflow/core/platform/default/logging.h:39:11: note:   'tensorflow::FATAL'\r\n const int FATAL = 3;           // base_logging::FATAL;\r\n           ^~~~~\r\ntensorflow/lite/tools/optimize/calibration/logging_op_resolver.cc:78:5: error: 'DLOG' was not declared in this scope\r\n     DLOG(FATAL) << error_message;\r\n     ^~~~\r\ntensorflow/lite/tools/optimize/calibration/logging_op_resolver.cc:78:5: note: suggested alternative: 'LOG'\r\n     DLOG(FATAL) << error_message;\r\n     ^~~~\r\n     LOG\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/jeff/Development/Software/TensorFlow/tensorflow/tensorflow/python/tools/BUILD:99:1 C++ compilation of rule '//tensorflow/lite/tools/optimize/calibration:logging_op_resolver' failed (Exit 1)\r\nINFO: Elapsed time: 151.066s, Critical Path: 34.40s\r\nINFO: 3413 processes: 3413 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nconfigured to use Cuda 10.2, cuDNN 8.0, and compute capability 7.5\r\n\r\n\r\n", "comments": ["@JCruk \r\n\r\nCan you try with this [tested build configuration](https://www.tensorflow.org/install/source#gpu) and see if the problem if the issue still persists.Thanks!", "Here are the errors I received with a tested build configuration (master, Python3.8, GCC-7, cuDNN 7.6.5.32, CUDA 10.1, Bazel 2.0.0):\r\n\r\n```\r\nERROR: /home/user/Development/Software/TensorFlow/tensorflow/tensorflow/lite/tools/optimize/calibration/BUILD:84:1: C++ compilation of rule '//tensorflow/lite/tools/optimize/calibration:logging_op_resolver' failed (Exit 1)\r\ntensorflow/lite/tools/optimize/calibration/logging_op_resolver.cc: In constructor 'tflite::optimize::calibration::LoggingOpResolver::LoggingOpResolver(const BuiltinOpsSet&, const CustomOpsSet&, const tflite::OpResolver&, tflite::optimize::calibration::KernelEvalFuncPtr)':\r\ntensorflow/lite/tools/optimize/calibration/logging_op_resolver.cc:78:10: error: 'FATAL' was not declared in this scope\r\n     DLOG(FATAL) << error_message;\r\n          ^~~~~\r\ntensorflow/lite/tools/optimize/calibration/logging_op_resolver.cc:78:10: note: suggested alternative:\r\nIn file included from ./tensorflow/core/platform/logging.h:27:0,\r\n                 from tensorflow/lite/tools/optimize/calibration/logging_op_resolver.cc:20:\r\n./tensorflow/core/platform/default/logging.h:39:11: note:   'tensorflow::FATAL'\r\n const int FATAL = 3;           // base_logging::FATAL;\r\n           ^~~~~\r\ntensorflow/lite/tools/optimize/calibration/logging_op_resolver.cc:78:5: error: 'DLOG' was not declared in this scope\r\n     DLOG(FATAL) << error_message;\r\n     ^~~~\r\ntensorflow/lite/tools/optimize/calibration/logging_op_resolver.cc:78:5: note: suggested alternative: 'LOG'\r\n     DLOG(FATAL) << error_message;\r\n     ^~~~\r\n     LOG\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/jeff/Development/Software/TensorFlow/tensorflow/tensorflow/python/tools/BUILD:282:1 C++ compilation of rule '//tensorflow/lite/tools/optimize/calibration:logging_op_resolver' failed (Exit 1)\r\nINFO: Elapsed time: 39.973s, Critical Path: 11.20s\r\nINFO: 1616 processes: 1616 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "I was able to build successfully by using the bazel-2.0.0 binary, rather than letting bazelisk select the version.\r\n\r\nPerhaps coding the required version facilitate successful builds with bazelisk.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40400\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40400\">No</a>\n"]}, {"number": 40399, "title": "Support cuDNN NDHWC 3D convolution", "body": "This PR supports for the direct NDHWC 3D convolution which added in cuDNN v8: https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_8.html#rel-800-Preview\r\n\r\nFYI @nluehr ", "comments": []}, {"number": 40398, "title": "miss the ruy/prepare_packed_matrices.h", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux raspberrypi os (May 2020)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): trying to build\r\n- TensorFlow version: 2.2.0\r\n- Python version: 2.7.0\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): (Raspbian 8.3.0-6+rpi1) 8.3.0\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n\r\n**Describe the problem**\r\nHi, I'm trying to natively compile the tensorflow lite in Raspberry Pi following the official guide.\r\nIn the lastest step `./tensorflow/lite/tools/make/build_rpi_lib.h`, I got error like this:\r\n```\r\ntensorflow/lite/tools/make/downloads/ruy/ruy/prepare_packed_matrices.cc:16:10: fatal error: ruy/prepare_packed_matrices.h: No such file or directory\r\n #include \"ruy/prepare_packed_matrices.h\"\r\n```\r\nthis file is not in path `downloads/ruy/ruy` after downloading the dependencies.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nSee above.\r\n\r\n**Any other info / logs**\r\nI found the git commit log of download_dependencies.sh and got this file from the previous `RUY_URL` link. It compiled successfully. So I think it miss the prepare_packed_matrices.h.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40398\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40398\">No</a>\n"]}, {"number": 40397, "title": "Mixed precision does not improve the speed", "body": "I'm using TF 2.2 to write reinforcement learning algorithms. Because I use a relatively large model, I try to apply mixed precision to improve the training speed. My network consists of three ResNet blocks of depth `[16, 32, 32]`(features are downsampled using maxpooling layer with stride 2 before each block) following by 2 dense layers of size 512, and the input data is of shape `[512, 64, 64, 3]`. After applying the mixed_precision (following [this guide](https://www.tensorflow.org/guide/keras/mixed_precision)), I see the memory usage drops from 3787MB to 1229MB, but the training speed decreases from 20 training steps per second to 15 training steps per second. My code restrictly follows the [official guide](https://www.tensorflow.org/guide/keras/mixed_precision), but I cannot get the improvement on the speed. What might cause the problem?\r\n\r\nBy the way, my GPU is RTX 2080Ti.", "comments": ["@xlnwel \r\nPlease share simple stand alone code to replicate the issue faced or if possible share a colab gist for us to analyse the issue", "Hi, @Saduf2019 . Here's a colab file I create. Please check it\r\nhttps://colab.research.google.com/drive/1nBuYrmmH5kM0jbtIZdsuiG6uJbU6mpA7?usp=sharing", "@xlnwel \r\nI ran the code shared but face a different issue,the code continous to run for ever after mixed precision result.\r\nPlease find the [gist here](https://colab.research.google.com/gist/Saduf2019/c1a8afc6efce10617eef66d96cedb962/untitled231.ipynb)", "Maybe because you did not use GPU? Following \"Runtime -> set runtime type -> GPU\", you can set GPU. Running the gist you provided, I did not see any issue. Here's a screenshot of the result\r\n\r\n<img width=\"634\" alt=\"image\" src=\"https://user-images.githubusercontent.com/10184153/84834552-3aec3c00-b064-11ea-8ba2-887a21bc7de8.png\">\r\n\r\n\r\n", "I am able to replicate the issue faced, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/48cc78aedf6dbde9df80973aadb5d5cf/untitled235.ipynb).", "Hi, @Saduf2019, @gowthamkpr and @reedwm . Any idea why using `mixed_float16` takes longer time than using `float32`?", "I cannot reproduce with TF 2.4rc3. On a Titan V, I get 16.7 seconds with float16 and 43.2 seconds with float32, so I'm assuming this has been fixed."]}, {"number": 40396, "title": "Update losses.py", "body": "Small typo as mentioned in https://github.com/tensorflow/tensorflow/issues/39867", "comments": []}, {"number": 40395, "title": "Saving/Loading Subclassed Models TF2+/keras", "body": "**Update-template** I don't really see how to fill in this template.\r\nThis template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\nHello,\r\n\r\nI am having a lot of difficulties saving a subclassed model and reloading it. I have uploaded the current files I'm working on (specifically, policy.py) to https://github.com/ryanmaxwell96/trpo_fractal5NN. Now, I know you are supposed to be able to save things under the SavedModel format so that is what I've been trying to follow. But I'm still getting the error seen here:\r\n\r\n'''\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 411, in <module>\r\n    main(**vars(args))\r\n  File \"train.py\", line 367, in main\r\n    policy_model.save('policy_model')\r\n  File \"/home/ryan/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 975, in save\r\n    signatures, options)\r\n  File \"/home/ryan/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py\", line 115, in save_model\r\n    signatures, options)\r\n  File \"/home/ryan/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py\", line 74, in save\r\n    save_lib.save(model, filepath, signatures, options)\r\n  File \"/home/ryan/.local/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 883, in save\r\n    _ = _SaveableView(checkpoint_graph_view)\r\n  File \"/home/ryan/.local/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 164, in __init__\r\n    self.checkpoint_view.objects_ids_and_slot_variables())\r\n  File \"/home/ryan/.local/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py\", line 418, in objects_ids_and_slot_variables\r\n    object_names[obj] = _object_prefix_from_path(path)\r\n  File \"/home/ryan/.local/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py\", line 64, in _object_prefix_from_path\r\n    for trackable in path_to_root))\r\n  File \"/home/ryan/.local/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py\", line 64, in <genexpr>\r\n    for trackable in path_to_root))\r\n  File \"/home/ryan/.local/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py\", line 57, in _escape_local_name\r\n    return (name.replace(_ESCAPE_CHAR, _ESCAPE_CHAR + _ESCAPE_CHAR)\r\nAttributeError: 'NoneType' object has no attribute 'replace'\r\n'''\r\n\r\nIn line 90 of the policy.py file is where I'm trying to save the subclassed model \"self.trpo\" after having called train_on_batch and predict_on_batch. I have absolutely no idea what the error is telling me.\r\n\r\nAny help would be greatly appreciated!\r\n\r\nThanks,\r\n\r\nRyan", "comments": ["@ryanmaxwell96 \r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also, let us know Tensorflow version you are using.\r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n", "Here is the colab file with the issue I was referring to. https://colab.research.google.com/drive/1dD9QKj9-NzdCECBWKkkihS18Iqmlf5-o?usp=sharing\r\n\r\nFor the prompts it asks for, input the following in this order:\r\nCartPoleBulletEnv-v1\r\nn\r\n1.0", "Ahh bummer. It gave me the error the first time through, but now is giving me another error I haven't seen before.", "Ok so I think I may see a problem. I used the model.summary() command to get:\r\n\r\n\r\ntrpo summary\r\nModel: \"trpo\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n_________________________________________________________________\r\npolicy_nn (PolicyNN)         multiple                  4393      \r\n_________________________________________________________________\r\nlog_prob (LogProb)           multiple                  0         \r\n_________________________________________________________________\r\nkl_entropy (KLEntropy)       multiple                  0         \r\n_________________________________________________________________\r\nTotal params: 4,394\r\nTrainable params: 4,393\r\nNon-trainable params: 1\r\n_________________________________________________________________\r\nNone\r\n\r\n\r\nI have no idea why it says None at the end, but I'm guessing this is the problem. Any way to remove it?", "@ryanmaxwell96 \r\n\r\nRequest you to grant me the access for the colab link you have shared. Thanks!", "I think I gave it. Do you have access now?", "@ryanmaxwell96 \r\n\r\nI am not able to reproduce the issue.Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/7bf8b324c941de603121854dcdea118b/untitled36.ipynb)Please, help with the reproducible code .It helps us in localizing the issue faster.Thanks!", "Sorry for the round-about way of doing this, as I don't really know how to use colab very well. But you need to upload these (https://github.com/ryanmaxwell96/trpo_w_subclassing/upload) files to the colab as well as use !pip install pybullet to get the errors that I've been talking about.\r\n\r\n", "Also, don't forget this part:\r\n\r\nFor the prompts it asks for, input the following in this order:\r\nCartPoleBulletEnv-v1\r\nn\r\n1.0", "This should show the errors I've been getting:\r\nhttps://colab.research.google.com/drive/1dD9QKj9-NzdCECBWKkkihS18Iqmlf5-o#scrollTo=Ft_-1u1cnjW2", "@ryanmaxwell96 I am not able to reproduce this issue. Please share us a minimal reproducible code to reproduce this. Thanks!", "If you follow the steps that I listed: upload these (https://github.com/ryanmaxwell96/trpo_w_subclassing/upload) files to the colab, use !pip install pybullet, and then type these (CartPoleBulletEnv-v1, n, 1.0) in when prompted, you should get the errors I was referring to.\r\n\r\nI have shared the train colab file with you that should show the errors.", "@ryanmaxwell96 I tried to reproduce this issue but I am not able to. Please find my gist [here](https://colab.research.google.com/gist/gowthamkpr/9ecc1e702bb55c37d6e8c41158dee09b/train.ipynb)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40395\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40395\">No</a>\n"]}, {"number": 40394, "title": "Save/load subclassed model TF+2, keras", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@ryanmaxwell96,\r\nLooks like this is a duplicate of [#40395](https://github.com/tensorflow/tensorflow/issues/40395). Could you please close this issue since it is already being tracked there. Thanks!"]}, {"number": 40393, "title": "Relax stub include version checking.", "body": "Remove upper bound on version check for latest inc files.\r\n\r\nAttn: @sanjoy ", "comments": ["@nluehr Can you please check @sanjoy's comments and keep us posted. Thanks!", "@nluehr Can you please fix build failures ? Thanks!", "Build failures appear to be un-related to this PR. Rebased to a later point on master branch and builds seem to be OK."]}, {"number": 40392, "title": "[TF2.2] Loading a Saved Model from tensorflow_hub failed with `AttributeError: '_UserObject' object has no attribute 'summary'`", "body": "**System info:**\r\nPython: 3.6.9\r\nTensorflow: 2.2.0 CPU package from _pip_\r\n\r\n**The issue:**\r\nI got https://tfhub.dev/google/imagenet/resnet_v2_50/classification/4?tf-hub-format=compressed from tf-hub and then uncompressed in a new directory.\r\n\r\nwith following code:\r\n\r\n```\r\nprint(tf.__version__)\r\n\r\nresnet50v2_save_path = os.path.join('.', \"test2/imagenet_resnet_v2_50_feature_vector_4/\")\r\n\r\nloaded1 = tf.keras.models.load_model(resnet50v2_save_path)\r\nprint(loaded1.summary())\r\n```\r\n\r\nI get:\r\n```\r\n2.2.0\r\n2020-06-11 21:51:49.413961: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1995500000 Hz\r\n2020-06-11 21:51:49.414437: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x57d3130 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-11 21:51:49.414460: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"./test2.py\", line 43, in <module>\r\n    print(loaded1.summary())\r\nAttributeError: '_UserObject' object has no attribute 'summary'\r\n```\r\nHowever, if I create my own model and then save into .pb format and then load in the same way it works.\r\n\r\nWhy? Thx", "comments": ["@kinos9 \r\nCan you please refer to these issues with similar error:\r\n[link](https://github.com/tensorflow/tensorflow/issues/33870#issuecomment-576925084)\r\n[link1](https://stackoverflow.com/questions/60979586/how-do-i-convert-a-tensorflow-model-into-a-tensorrt-optimized-model-using-trt-tr) \r\n[link2](https://github.com/tensorflow/tensorflow/issues/26814#issuecomment-610067468)\r\nPlease share simple standalone code for us to replicate the issue in case the above issues are not helpful.", "I have already found and tried those solution without success.\r\nThis is what I do to get the error:\r\n\r\n```\r\nwget https://storage.googleapis.com/tfhub-modules/google/imagenet/resnet_v2_50/feature_vector/4.tar.gz\r\nmkdir test_pb\r\nmv 4.tar.gz test_pb\r\ncd test_pb\r\ntar -xvf 4.tar.gz\r\nrm 4.tar.gz\r\ncd ..\r\n./test.py\r\n```\r\n\r\nhttps://storage.googleapis.com/tfhub-modules/google/imagenet/resnet_v2_50/feature_vector/4.tar.gz is the feature vector pre-trained model from https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4 in tf-hub.\r\n\r\n`test.py` is the Python script, and this following is the standalone code:\r\n\r\n```\r\n#!/usr/bin/env python3\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport os\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\nresnet50v2_save_path = os.path.join('.', \"./test_pb/\")\r\n\r\nloaded1 = tf.keras.models.load_model(resnet50v2_save_path)\r\nprint(\"Load done\")\r\n\r\nprint(\"Signatures: \", loaded1.signatures)\r\n\r\nprint(\"Type: \", type(loaded1))\r\n\r\nprint(loaded1.summary())\r\n```\r\n\r\nthat give this output:\r\n\r\n```\r\n2.2.0\r\n2020-06-12 20:29:07.677555: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1995455000 Hz\r\n2020-06-12 20:29:07.678219: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x59eb130 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-12 20:29:07.678241: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nLoad done\r\nSignatures:  _SignatureMap({})\r\nType:  <class 'tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject'>\r\nTraceback (most recent call last):\r\n  File \"./test.py\", line 18, in <module>\r\n    print(loaded1.summary())\r\nAttributeError: '_UserObject' object has no attribute 'summary'\r\n```\r\n\r\nthat is the mentioned error.\r\n\r\nThx", "> @kinos9\r\n> Can you please refer to these issues with similar error:\r\n> [link](https://github.com/tensorflow/tensorflow/issues/33870#issuecomment-576925084)\r\n> [link1](https://stackoverflow.com/questions/60979586/how-do-i-convert-a-tensorflow-model-into-a-tensorrt-optimized-model-using-trt-tr)\r\n> [link2](https://github.com/tensorflow/tensorflow/issues/26814#issuecomment-610067468)\r\n> Please share simple standalone code for us to replicate the issue in case the above issues are not helpful.\r\n\r\nIf you run `print(loaded1.tensorflow_version)` it prints 1.14.0.\r\nMay .pb version is incompatible with 2.2.0 one? ", "Yes. Looks like it. Instead of downloading the model, can you load the model as a Keras Layer using `hub.KerasLayer()` and then use the model? Thanks!", "@kinos9 Can you please respond to the above comment so that we can take the discussion forward. Thanks!", "> Yes. Looks like it. Instead of downloading the model, can you load the model as a Keras Layer using `hub.KerasLayer()` and then use the model? Thanks!\r\n\r\nSorry, I miss this answer.\r\nYes of course and in that way work, but when I export the graph as protobuf for inference with C++ APIs it looks for a KerasLayer object.", "So, Did you try using `tf.keras.applications.ResNet50V2` instead of loading from a hub?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Hi, having the same issue, can this be reopened?", "please reopen this Issue", "I had this issue after converting a pytorch model to .onnx, to .pb\r\n\r\nHow top get something like a summary:\r\nhttps://github.com/onnx/onnx-tensorflow/issues/855#issuecomment-765028398\r\nHow to use the .pb file:\r\nhttps://github.com/onnx/onnx-tensorflow/issues/901#issuecomment-821686852"]}, {"number": 40391, "title": "Custom train loop inconsistent with Keras `fit` for vector variables", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab, MacOS 10.15.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary pip\r\n- TensorFlow version (use command below):2.2.0\r\n- Python version:3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:None\r\n- GPU model and memory:None\r\n\r\n**Describe the current behavior**\r\n\r\nTraining a linear model using a custom train loop and `tf.GradientTape` is inconsistent with `tf.keras.Model.fit` and the variables are updated in the wrong way. Scalar variables are updated correctly, vector variables get the wrong gradient.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe final results should be close in both cases.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnn=40\r\n_x= np.random.normal(size=(nn,3)).astype(np.float32)\r\n_y= .4*_x[:,0] + 0.2*_x[:,1] -0.3*_x[:,2]+1.5 + np.random.normal(size=(nn), scale=.1).astype(np.float32)\r\n\r\n_x = tf.convert_to_tensor(_x)\r\n_y = tf.convert_to_tensor(_y)\r\n\r\nmodelx=tf.keras.Sequential([\r\n    tf.keras.layers.Dense(1)\r\n])\r\n\r\nmodelx(_x)\r\n\r\nopt = tf.keras.optimizers.Adam(learning_rate=0.1)\r\n\r\n\r\nfor i in range(400):\r\n    with tf.GradientTape() as tape:\r\n\r\n        yhat = modelx(_x)\r\n        loss = tf.reduce_sum(tf.math.squared_difference(yhat,_y ))\r\n    grads = tape.gradient(loss, modelx.trainable_weights)\r\n\r\n    opt.apply_gradients(zip(grads, modelx.trainable_weights))\r\n\r\n\r\nmodelx.variables\r\n```\r\n\r\nColab to reproduce the bug:\r\n\r\nhttps://colab.research.google.com/drive/12YlqtjsekXqhEZiTnlTnjxz2s0RJCV0r?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried in colab with TF version 2.2, nightly version(`2.3.0-dev20200611`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/241892acf963a7ca7d2a195eb92662ff/untitled10.ipynb).Thanks!", "That is just a mistake in the your custom training loop. The problem is that `yhat` has shape `[batch_size, 1]`, while `_y` has shape `[batch_size]`. So subtracting them gives a result of `[batch_size, batch_size]`, which is definitely not what you wanted.\r\n\r\nIf you for example used\r\n```python\r\nloss = tf.reduce_sum(tf.math.squared_difference(yhat[:, 0],_y ))\r\n```\r\nwhich converts `yhat` first to vector of shape `[batch_size]`, it would work fine.\r\n\r\nAlso, the object losses perform this conversion automatically, so\r\n```python\r\nloss = tf.keras.losses.MeanSquaredError()(yhat,_y )\r\n```\r\nalso works (and this is what the `.fit` method does, I believe).", "Yes, indeed, now it works as expected.\r\nThank you!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40391\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40391\">No</a>\n"]}, {"number": 40390, "title": "Add dependencies package for gcs", "body": "@mihaimaruseac \r\nThis PR adds dependencies to build `google_cloud_cpp storage`\r\nhttps://github.com/googleapis/google-cloud-cpp/blob/b22e726162e3d5c5f7e177a3bf26982873399018/google/cloud/storage/BUILD#L81\r\nhttps://github.com/googleapis/google-cloud-cpp/blob/b22e726162e3d5c5f7e177a3bf26982873399018/google/cloud/storage/BUILD#L28\r\nWe already have `@nlohmann_json_lib` but this dependency is a single header so I think it won't cause any trouble.", "comments": ["Can we use the existing json dependency instead? There are 2 existing dependencies for JSON.", "We can but we will need a patch file since the dependency of gcs is a `http_file` while we have `http_archive`.\r\nhttps://github.com/googleapis/google-cloud-cpp/blob/b22e726162e3d5c5f7e177a3bf26982873399018/google/cloud/storage/BUILD#L26-L31", "Let's take this as it is for now then and revisit later"]}, {"number": 40389, "title": "use curl instead of com_github_curl_curl", "body": "@mihaimaruseac \r\nThis PR allow `google_cloud_cpp` use the existing `@curl` instead of download `@com_github_curl_curl` which is the same but different name because of BUILD file.\r\nhttps://github.com/googleapis/google-cloud-cpp/blob/b22e726162e3d5c5f7e177a3bf26982873399018/google/cloud/storage/BUILD#L80", "comments": ["I think I'll have to import this manually. Will take a few hours but I'll be back if there is a need for changes"]}, {"number": 40388, "title": "Protobuf not choosing the right libstdc++ while building with Bazel", "body": "Hi everyone,\r\n\r\nI'm having a bug which seems to be recurrent since 2016 and concerns the build of TensorFlow from the source using Bazel. Here are some system information and description of the problem.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Scientific Linux 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.2\r\n- Bazel version (if compiling from source): 3.2.0\r\n- GCC/Compiler version (if compiling from source): 8.2.0\r\n- CUDA/cuDNN version: 10.2/7\r\n- GPU model and memory: V100/16GB\r\n\r\n**Describe the current behavior**\r\nI'm trying to build the last version of TensorFlow (2.20) using Bazel 3.2.0. Everything goes fine ignoring some warings until the protoc execution of protobuf. Please find the error snippet hereafter :\r\n\r\n``\r\nERROR: /nobackup/anon/tensorflow/tensorflow/core/protobuf/BUILD:153:17: ProtoCompile tensorflow/core/protobuf/named_tensor.pb.h failed (Exit 1): protoc failed: error executing command \r\n  (cd /nobackup/anon/cache/bazel/_bazel_af261718/0773f41f5efd7b2afa75c2b5b53f9c82/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/com_google_protobuf/protoc '--cpp_out=bazel-out/k8-opt-exec-50AE0418/bin' -I. -Iexternal/com_google_protobuf/src -Ibazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src tensorflow/core/protobuf/named_tensor.proto)\r\nExecution platform: @local_execution_config_platform//:platform\r\nbazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)\r\nbazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)\r\nbazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)\r\nbazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /nobackup/anon/tensorflow/tensorflow/python/tools/BUILD:282:10 ProtoCompile tensorflow/core/framework/graph.pb.h failed (Exit 1): protoc failed: error executing command \r\n  (cd /nobackup/anon/cache/bazel/_bazel_af261718/0773f41f5efd7b2afa75c2b5b53f9c82/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/com_google_protobuf/protoc '--cpp_out=bazel-out/k8-opt-exec-50AE0418/bin' -I. -Iexternal/com_google_protobuf/src -Ibazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src tensorflow/core/framework/graph.proto)\r\nExecution platform: @local_execution_config_platform//:platform\r\n``\r\nAs we can see, it refers to /lib64 instead of refering to the lib specified in LD_LIBRARY_PATH. Actually, the `exec env -` will erase all the env vars passed to command ... I've tried several workarounds by adding a `--linkopt=\"-Wl, -rpath,/path/to/lib64\"` or even in the following issues: https://github.com/bazelbuild/bazel/issues/1358#issuecomment-232019644 and https://github.com/bazelbuild/bazel/issues/649#issuecomment-166710509 but it seems that the CROSSTOOL file doesn't exist anymore... These workarounds don't work for me and I don't know what to do anymore.\r\n\r\n**Describe the expected behavior**\r\nPassing correct lib64 folder to LD_LIBRARY_PATH should overcome the problem but since `exec env -` is still active the variable is erased.\r\n\r\nDo you have a solution for this problem?\r\nThanks", "comments": ["Problem solved, I was building on the master directly instead of checking out the r2.2 branch.\r\n\r\nHowever, the problem still exists with SWig.\r\n\r\nThe solution to address the issue is the following:\r\n\r\n- Navigating to `bazel-tensorflow/external/swig`\r\n- Modify the file `BUILD.bazel`\r\n- Add this add the end of the `cc_library` object:\r\n``\r\nlinkopts = [\"-Wl,-rpath,/path/to/custom/libstdc/path/\",\r\n                  \"-L/path/to/custom/libstdc/path\"\r\n]\r\n``\r\n\r\nThe issue can be closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40388\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40388\">No</a>\n", "Closing this issue since it's resolved. @boubasse Thanks for sharing solution."]}, {"number": 40387, "title": "tf_upgrade_v2 needs explicit encodings on Windows to upgrade utf-8", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary x64\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.8.2\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Tesla K80, 1080ti (11GB per gpu)\r\n\r\n**Describe the current behavior**\r\n\r\nRunning tf_upgrade_v2 yields encoding errors on Windows, even with defaults set to utf-8 (though no explicit encodings in each file -- as this isn't required in Python 3). \r\n\r\nAdding explicit encoding to `tensorflow\\tools\\compatibility\\ast_edits.py` as shown below resolved this for me -- if a default of utf-8 isn't desirable maybe a command line option?\r\n\r\n```\r\n    # Write to a temporary file, just in case we are doing an implace modify.\r\n    # pylint: disable=g-backslash-continuation\r\n    with open(in_filename, \"r\", encoding='utf-8') as in_file, \\\r\n        tempfile.NamedTemporaryFile(\"w\", delete=False, encoding='utf-8') as temp_file:\r\n      ret = self.process_opened_file(in_filename, in_file, out_filename,\r\n                                     temp_file)\r\n    # pylint: enable=g-backslash-continuation\r\n```\r\n\r\n", "comments": ["@sbuser \r\nPlease share simple stand alone code for us to replicate the issue faced or is possible share a colab gist for us to analyse the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40387\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40387\">No</a>\n"]}, {"number": 40386, "title": "clarification with TF datasets: more efficient to vectorize with unbatching (batch -> map -> unbatch) or just map?", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide/data\r\nhttps://www.tensorflow.org/guide/data_performance\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe documentation for vectorizing code says that vectorizing transformations is more efficient, as with batch -> map. However, it doesn't say anything about the overhead of unbatch; as a result, I have seen the process batch -> map -> unbatch to vectorize transformations before adding more methods (like cache -> shuffle) that you don't want to perform on the batched dataset.\r\n\r\nIt's not clear which is better, or whether it depends on hardware, dataset, batch size, etc.:\r\n1) Just performing dataset.map(my_transformation)\r\n2) Performing dataset.batch(batch_size).map(my_transformation).unbatch()\r\n\r\nWhen the whole dataset fits in memory, the map would be performed once (in front of cache), and so an inefficient implementation would only bear a one-time cost. However, if the dataset is large enough to require sharding (I haven't looked into this), wouldn't this extra cost be paid every time a new shard is loaded? \r\n\r\nI have seen (2) show up in numerous (non-official) tutorials, but at least in the few cases I have tested, it can cause a slowdown of up to 1.5x. It seems like a small change could clarify this, especially if the overhead in unbatch is probably always higher than the cost of non-vectorized transformations. I could also see it being that for certain cases, (2) is actually more efficient, but that's just a guess.", "comments": ["Thanks for reporting this @jshyatt2. \r\n\r\nI expect that the overhead of calling `unbatch` will almost always outweigh the overhead saved by vectorizing the map. It is better to vectorize at the final batch level, so that no `unbatch` is needed.\r\n\r\nI'll update https://www.tensorflow.org/guide/data_performance to follow the best practice of vectorizing at the final batch level.", "Good to know; I was afraid I'd have to worry about case-by-case optimizing. Thanks!", "@aaudiber Just to offer an additional comment with respect to unbatch (and specifically on https://www.tensorflow.org/guide/data_performance):\r\n\r\nIn https://www.tensorflow.org/guide/data_performance#pipelines_comparison, batch (with drop_remainder=True), followed by unbatch, is used to (as far as I can tell) get rid of the ragged batch at the end of the dataset.\r\n\r\nIn https://www.tensorflow.org/guide/data_performance#optimized (at least currently), it is used to vectorize a transformation, then unbatch, and in that case unbatch is actually called after prefetch."]}, {"number": 40385, "title": "ImportError: cannot import name 'naming' from 'tensorflow.python.autograph.core' for tf-agents", "body": "Hey, not sure if this is for here, for I should move it to tf-agents. I figured since the deepest part of the traceback was in tensorflow, it should be here. Please let me know if I'm wrong and I'll move to tf-agents.\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, using this example: https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux 5.6.15-arch1-1\r\n- TensorFlow installed from (source or binary): built from source\r\n- TensorFlow version (use command below): v1.12.1-33853-gc674577870 2.2.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): 10.1.0\r\n- CUDA/cuDNN version: 10.2 / 7.6.5\r\n- GPU model and memory: GTX 1080 Ti  11GB\r\n\r\n**Describe the current behavior**\r\nErrors out when importing anything from tf-agents.\r\n**Describe the expected behavior**\r\nShould run without error.\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nfrom tf_agents.agents.dqn import dqn_agent\r\n**Other info / logs** Full traceback of error:\r\n`  File \"/home/frodo/Desktop/gym_project/gym-open.py\", line 14, in <module>\r\n    from tf_agents.agents.dqn import dqn_agent\r\n  File \"/home/frodo/.local/lib/python3.8/site-packages/tf_agents/agents/__init__.py\", line 17, in <module>\r\n    from tf_agents.agents import tf_agent\r\n  File \"/home/frodo/.local/lib/python3.8/site-packages/tf_agents/agents/tf_agent.py\", line 26, in <module>\r\n    from tf_agents.specs import tensor_spec\r\n  File \"/home/frodo/.local/lib/python3.8/site-packages/tf_agents/specs/__init__.py\", line 20, in <module>\r\n    from tf_agents.specs.distribution_spec import DistributionSpec\r\n  File \"/home/frodo/.local/lib/python3.8/site-packages/tf_agents/specs/distribution_spec.py\", line 22, in <module>\r\n    import tensorflow_probability as tfp\r\n  File \"/home/frodo/.local/lib/python3.8/site-packages/tensorflow_probability/__init__.py\", line 76, in <module>\r\n    from tensorflow_probability.python import *  # pylint: disable=wildcard-import\r\n  File \"/home/frodo/.local/lib/python3.8/site-packages/tensorflow_probability/python/__init__.py\", line 24, in <module>\r\n    from tensorflow_probability.python import experimental\r\n  File \"/home/frodo/.local/lib/python3.8/site-packages/tensorflow_probability/python/experimental/__init__.py\", line 34, in <module>\r\n    from tensorflow_probability.python.experimental import auto_batching\r\n  File \"/home/frodo/.local/lib/python3.8/site-packages/tensorflow_probability/python/experimental/auto_batching/__init__.py\", line 24, in <module>\r\n    from tensorflow_probability.python.experimental.auto_batching import frontend\r\n  File \"/home/frodo/.local/lib/python3.8/site-packages/tensorflow_probability/python/experimental/auto_batching/frontend.py\", line 45, in <module>\r\n    from tensorflow.python.autograph.core import naming\r\nImportError: cannot import name 'naming' from 'tensorflow.python.autograph.core' (/home/frodo/.local/lib/python3.8/site-packages/tensorflow/python/autograph/core/__init__.py)`\r\n", "comments": ["@vonadz,\r\nI was able to run the code without any issues on TF v2.2. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/676b1a731029776de16245ba181bd330/40385.ipynb). \r\n\r\nCould you please try running the same code in a virtual environment and check if it works. Thanks!", "@amahendrakar thanks for replying.\r\nUsing `pip install tensorflow` for the virtual env makes the code run. But when installing the tensorflow package I built from source, it fails with the same error presented in the issue. I built it with all defaults, for CUDA support, following this guide: https://www.tensorflow.org/install/source, using r2.2.", "I have the exact same issue, Tensorflow 2.2.0 compiled from source following the official documentation.", "@vonadz I found something rather interesting, with the Tensorflow binary I compiled myself (version 2.2.0) I am able to use Tensorflow Probability (both self compile or installed from PIP) version 0.7, anything newer than that gives the same error:  \r\n~~~\r\ntensorflow.python.autograph.core import naming ImportError: cannot import name 'naming' from 'tensorflow.python.autograph.core' (/home/frodo/.local/lib/python3.8/site-packages/tensorflow/python/autograph/core/__init__.py)\r\n~~~", ">I found something rather interesting, with the Tensorflow binary I compiled myself (version 2.2.0) I am able to use Tensorflow Probability (both self compile or installed from PIP) version 0.7, anything newer than that gives the same error:\r\n> \r\n> ```\r\n> tensorflow.python.autograph.core import naming ImportError: cannot import name 'naming' from 'tensorflow.python.autograph.core' (/home/frodo/.local/lib/python3.8/site-packages/tensorflow/python/autograph/core/__init__.py)\r\n> ```\r\n\r\n@vonadz,\r\nCould you please check @cfabio's comment and let us know if it helps. Thanks!", "@cfabio yes it worked! Thanks.", "Marking the issue as closed, as it is resolved. Please feel free to re-open the issue if necessary. Thanks!", "I am not sure we can consider this resolved, since to \"make it work\" I had to downgrade to an older version of Tensorflow Probability.  \r\nMy impression is that the Tensorflow (not Probability) me and @vonadz compile from source is missing something, which is even weirder since we both followed the official documentation.  \r\nFor now I might be able to live with Tensorflow Probability version 0.7, but in the future I would like to be able to use the latest version.", "> I am not sure we can consider this resolved, since to \"make it work\" I had to downgrade to an older version of Tensorflow Probability.\r\n\r\n@cfabio,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template so that we can track the issue there. Thanks!", "I have the exactly same problem.", "Same problem when I try to import from tensorflow_probabilty.\r\nI'm using the official TensorFlow docker.\r\n[tf install reference](https://www.tensorflow.org/install/source)\r\n`docker run -it -p 8888:8888 tensorflow/tensorflow:nightly-jupyter`", "@yaowang1111, @Echo9k,\r\nPlease take a look at [#40584](https://github.com/tensorflow/tensorflow/issues/40584). The issue is already being tracked there. Thanks!"]}, {"number": 40384, "title": "Tensorflow is blocking CUDAmemcpyAsyn, and high CPU usage", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Linux Ubuntu 18.04):\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): c++ 2.2.0\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source):7.5.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: NVIDIA TitanV 12GB\r\n\r\n**Describe the current behavior**\r\nBackground:\r\nHi, I'm using tensorflow c++ to do offline inference. Since I want to do it in realtime, I dag in and found some optimisation tricks. Perhaps the most prominent thing is, if and do the inference by calling session->infer(). (if I understand this correctly) Tensorflow would do copy data from RAM to VRAM, GPU calculation, copy result back to RAM. I found that the GPU utilisation is low because the GPU is waiting the data being fed. So I'm trying to do async data tranfer and calculation. (This could double the performance of my model since the GPU is only used by 40%, without data transfer, it can take up to 90%)\r\n\r\nI found an example of creating GPU resident tensor in the source code of Tensorflow and I used it:\r\n\r\nTensorShape shape = TensorShape({this->batchSize, this->input_height, this->input_width, this->input_depth});\r\n    PlatformGpuId platform_gpu_id(0);\r\n\r\n    GPUMemAllocator *sub_allocatorA =\r\n        new GPUMemAllocator(\r\n            GpuIdUtil::ExecutorForPlatformGpuId(platform_gpu_id).ValueOrDie(),\r\n            platform_gpu_id, false /*use_unified_memory*/, {}, {});\r\n    GPUBFCAllocator *allocatorA = new GPUBFCAllocator(sub_allocatorA, shape.num_elements() * sizeof(uint8), \"GPU_0_bfc\");\r\n    this->BufferA = new Tensor(allocatorA, tensorflow::DT_UINT8, shape);\r\n\r\nand I feed data using:\r\n\r\ncudaMemcpyAsync(dst, p, this->batchSize * this->input_depth * this->input_height * this->input_width, cudaMemcpyHostToDevice, *CUDAstream);\r\n\r\nIssue 1:\r\nI implemented three threads, one is for feeding data, one is for inference, and one is for data fetching.\r\n\r\nFor some reason, session->run() would block the cudaMemcpyAsync() in which the cudaMemcpyAsync() is pending until session->run() ends.\r\nThis does not happen all the time, but for the majority of the time.\r\n\r\nIssue 2:\r\nAs I mentioned, I'm using GPU for inference, but tensorflow would occupy all my CPU (44 cores 100%) when it's inferring.\r\nI tried the code below, but it still creates roughly 100 threads and eat all the CPU. This is not happening with your official python wheels. (with the same saved model, there are still lots of threads created, but it is basically not using CPU). I can make sure the calculation is done by GPU, (I opened both htop and nvtop at the same time)\r\n\r\n    auto options = SessionOptions();\r\n    options.config.mutable_gpu_options()->set_allow_growth(true);\r\n    tensorflow::ConfigProto &config = options.config;\r\n    config.set_inter_op_parallelism_threads(10);\r\n    config.set_intra_op_parallelism_threads(10);\r\n    config.set_use_per_session_threads(false);  \r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nexpected 1:\r\n\r\nAsync memory transfer without being blocked.\r\n\r\nexpected 2:\r\n\r\nbasically same CPU usage with python when running model on GPU.\r\n\r\n**Standalone code to reproduce the issue**\r\nIf you need it, I can push them on to github\r\n\r\n**Other info / logs**\r\nNo other important info/logs, I'm working remotely using VNP&SSH during this hard time.\r\n\r\n\r\n", "comments": ["> If you need it, I can push them on to github\r\n\r\nThat would be great, please upload the code and post a link here if it isn't too much trouble.", "@sanjoy \r\nThanks for your reply,\r\nhere is the link to the repo:\r\nhttps://github.com/MinghaoCheng/tensorflowCC_infer_aync", "@sanjoy Hi,\r\nIs there any update?\r\nThanks", "I think I worked it out. It is mainly openCV to blame, not tensorflow.\r\nThanks a lot."]}, {"number": 40382, "title": "tf.image.adjust_jpeg_quality/tf.rawOpsEncodeJpegVariableQuality returns negative/inverted RGB image", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No** (only called low-level operators, like `EncodeJpegVariableQuality` from `tf.rawOps`)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Google Colaboratory => Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **1.X version available in Google Colaboratory**\r\n- TensorFlow version (use command below): **1.15.2**\r\n- Python version: **3.6.9**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **7.5.0**\r\n- CUDA/cuDNN version: **10.1 /10.1.243**\r\n- GPU model and memory: **Tesla K80 (11441MiB available)**\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.image.random_jpeg_quality` (using `tf.image.adjust_jpeg_quality`) returns an image with wrong RGB values (negative image of the one expected)\r\n\r\n**Describe the expected behavior**\r\n`tf.image.random_jpeg_quality` samples a different quality at runtime and returns properly formatted RGB image.\r\n\r\n**Standalone code to reproduce the issue**\r\n [Colab Notebook](https://colab.research.google.com/drive/1H1vhKRDwy2E6qLX6XmCtbqW6iv6DiMDj?usp=sharing)\r\n\r\nIn the standalone code one can see the results of using the following 3 scenarios to emulate jpeg compression given an input **float32** image:\r\n1. Directly call `tf.image.random_jpeg_quality` with two ints (max and min quality).\r\n2. Using custom code to call `tf.image.adjust_jpeg_quality` with tensors not int(s)\r\n3. Construct custom function based on `tf.image.adjust_jpeg_quality` but forcing the call to \"EncodeJpegVariableQuality\" operator (to ensure a different quality is obtained each run)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n**Important notes**\r\nI'm trying to use `tf.image.random_jpeg_quality` in my pipiline to emulate JPEG compression artifacts as another data augmentation transformation. This means that the input to the function is a float32 tensor which internally is converted to uint8, encoded, decoded again and converted back to float32.\r\n\r\nHowever, `tf.image.convert_image_dtype` (in `tf.image.adjust_jpeg_quality`) re-scales the float32 to the (0.0, 1.0) range, reason why I manually re-scale it to (0.0, 255.0) as there are other data augmentations down the line that need standard RGB range (and mean-max normalisation is applied at the end of the pipeline). \r\nIn scenario 3, I've also played around with the argument `saturate` of `tf.image.convert_image_dtype` hoping the problem was caused by an overflow while converting from float32 to uint8, but without success.\r\n\r\nThe \"normal\" and \"negative\" images (manually inverting them after seeing the RGB images outputted after re-scaling the output of the jpeg_quality functions).\r\nThe outputs in each case look as follows:\r\n**'Normal' RGB images (without inverting their values)**\r\n![image](https://user-images.githubusercontent.com/19503950/84380349-7d86c200-abe7-11ea-8bac-d0e6c09279f2.png)\r\n(original left,  scenario 1)\r\n![image](https://user-images.githubusercontent.com/19503950/84380718-203f4080-abe8-11ea-9dda-4f728adab232.png)\r\n(Scenario 2, scenario 3)\r\n\r\n\r\n**'Inverted/Negative' RGB images (after inverting their values)**\r\n![image](https://user-images.githubusercontent.com/19503950/84381052-b4a9a300-abe8-11ea-96df-1d54764b1c91.png)\r\n(original left,  scenario 1)\r\n![image](https://user-images.githubusercontent.com/19503950/84380978-93e14d80-abe8-11ea-9000-2af1e1e09800.png)\r\n(Scenario 2, scenario 3)\r\n\r\nAs you can see, inverting the RGB values gives a much more plausible image. However, after printing the normal + negative image(s) mean, I see that the average gray value is quite low at around 50 (after inversion). This is more likely the reason why the images look \"washed-out\".\r\n\r\nMoreover, looking at the mean of the float32 image before inputting it to the JPEG functions and after re-scaling and clipping to range, differences are huge:\r\nInput image has a mean (over 3 RGB channels) of ~100, output from jpeg functions is ~205.\r\nNext, I'll try to convert the float32 to uint8 before calling the functions and let you known if anything changes.\r\n\r\nPossibly related issues: #25882 (closed)\r\n\r\nBy all means, this could be an error on my end due to a bad usage of any of the aforementioned functions. I'm only reporting it here so no one else repeats such error or, we resolve the bug, if any.\r\n\r\nThanks in advance!\r\n\r\nRegards,\r\nFerran.", "comments": ["A quick update for anyone experiencing similar results. I've only tested on this new [colab notebook](https://colab.research.google.com/drive/19q3Zec0Mz-2uKqMGpLR74Nqmz5GA7n1d?usp=sharing) but passing a _uint8_ image to any of the JPEG functions and then converting to float32 seems to work just fine and the sampled `quality` is different everytime.\r\n\r\nOn top of that, when I pass a _float32_ numpy array with `feed_dict` and convert to _uint8_ before calling the jpeg functions and then cast back to _float32_ (hence avoiding re-scaling), the results are correct.\r\n\r\nThe last step is to test inside my training code which is much more complex and may give other problems. I'll let you know once that works and I'll close this issue.\r\n\r\nIndependently of that, I think that what breaks things is that my image was of type float32 but had values in the range (0.0, 255.0) and Tensorflow probably expected them in the range (0.0, 1.0) so it  applied a normalisation that broke everything (taking a look at  `tf.image.convert_image_dtype` source code would answer that).\r\n\r\n**UPDATE**\r\nFunny enough, the devs warned me but I did not listen... \ud83d\ude06 \r\n```\r\n# Note: We're ignoring float overflows. If your image dynamic range\r\n# exceeds float range, you're on your own.\r\n```\r\n\r\nI should not have played with unusual float32 range (although most other data augmentations transformations work fine with float32 in the range (0.0, 255.0)).\r\n\r\nHope this saves anyone hitting his/her head against the wall for +5hours!\r\nCheers.", "@fperezgamonal,\r\nThank you for the update. Is this still an issue? Please feel free to close the issue if resolved. Thanks!", "Tomorrow morning I'll be able to check it (an older training is underway now) and I'll immediately close this issue if I don't find any other problem.", "Hello,\r\n\r\nI've just verified that it works as expected with my code, producing different quality images each run given a maximum and minimum values.\r\n\r\nJust to summarise, the weird results **were caused by passing a `float32` image outside the usual `float32` range of (0.0, 1.0),** in (0.0, 255.0). As a consequence, `tf.image.convert_image_dtype`  yield an image with wrong RGB values.\r\n\r\nI solved this by casting the `float32` in the range (0.0, 255.0) to `uint8`, pass through jpeg functions and cast back to `float32` (same range as original). **Always using ``tf.cast()`` which does not change scale nor range.**\r\nIf one wants to use `float32` with the aforementioned jpeg functions, simply normalise your floats in (0.0, 1.0) and then scale back to the original range."]}, {"number": 40381, "title": "ValueError: tf.function-decorated function tried to create variables on non-first call.", "body": "Standalone code to reproduce the issue\r\nhttps://colab.research.google.com/drive/1gNzsl6mYg8_H2GRHpdxhk8VPGEOqREti?usp=sharing", "comments": ["@jifei \r\nPlease refer to these issues with similar error:\r\n[link](https://github.com/tensorflow/tensorflow/issues/27120)\r\n[link1](https://github.com/tensorflow/tensorflow/issues/34983)\r\n[link2](https://stackoverflow.com/questions/58352326/running-the-tensorflow-2-0-code-gives-valueerror-tf-function-decorated-functio)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40381\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40381\">No</a>\n"]}, {"number": 40380, "title": " keras.models.save_model does not respect include_optimizer option", "body": "**System information**\r\nTensorflow: v2.2.0-0-g2b96f3662b 2.2.0\r\n\r\n**Describe the current behavior**\r\nWhen passing `include_optimizer=False` to ` keras.models.save_model` the optimizers weights are included.\r\n\r\n**Describe the expected behavior**\r\nWhen passing `include_optimizer=False` to ` keras.models.save_model` the optimizers weights are **NOT** included.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1JocUwWRNmGQzE4mbL85YO-DAaVilJmlD?usp=sharing\r\n\r\n<img width=\"1621\" alt=\"Screenshot 2020-06-11 at 11 01 45\" src=\"https://user-images.githubusercontent.com/8607233/84366389-07786000-abd3-11ea-838f-29efb095041b.png\">\r\n\r\n", "comments": ["@philipp-eisen \r\n\r\nI have tried in colab with TF 2.2 , nightly version.PLease, find the gist [here](https://colab.research.google.com/gist/ravikyram/4aba249fb2cfdc244c0b9ca2eb32fb44/untitled6.ipynb).Is this the expected behavior?.Thanks!", "@ravikyram Thanks for checking this issue out.\r\n\r\nUnfortunately, this is not the the expected behavior. I added a workaround to the [original colab](https://colab.research.google.com/drive/1JocUwWRNmGQzE4mbL85YO-DAaVilJmlD?usp=sharing#scrollTo=laV6-irZJY9O) that produces the expected behavior.\r\n\r\nSo basically just setting the optimizer property to `None` does not remove the optimizer weights. (This is what happens in the implementation that is supposed to handle `include_optimizer=False`  [see here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/saved_model/save.py#L67))\r\n\r\n<img width=\"1645\" alt=\"Screenshot 2020-06-11 at 16 44 43\" src=\"https://user-images.githubusercontent.com/8607233/84400897-e7609500-ac02-11ea-8296-869f157fa123.png\">\r\n", "@fchollet I think this should not be stale. I think if anyone would want to seriously use saved models created from a tf.keras model in a production serving environment it should definitely be possible to export without optimizer weights. ", "Today, I found it too, when after a big model (e.g. BERT) trained, Adam's weights could be larger than model's weights ! (700MB of adams, 400MB of bert it self)\r\n\r\nA simple reproduce work: https://colab.research.google.com/drive/1TyFfOi71njrgO8ybWRB5AlHcRHfKOhlv?usp=sharing\r\n\r\nA little trick is replace model's optimizer before you save, if you really don't care the optimizer:\r\n\r\n```\r\nold_opt = model.optimizer\r\n# it WORKS, you have to assign a object which a sub-class of tf.keras.optimizers.Optimizer\r\nmodel.optimizer = tf.keras.optimizers.SGD()\r\nmodel.save(save_to, include_optimizer=False)\r\nmodel.optimizer = old_opt\r\n```\r\n\r\n```\r\nold_opt = model.optimizer\r\n# it NOT works!\r\nmodel.optimizer = None\r\nmodel.save(save_to, include_optimizer=False)\r\nmodel.optimizer = old_opt\r\n```", "I also encounter this issue which makes a fine-tuned bert model 1.2G.\r\nAs Philipp said, this issue makes tf.keras API not usable for production.", "We are also having this problem. We moved from TF 1.15 + estimator to TF 2.3 + Keras and our model variables went from 200MB to 600MB. The extra 400MB was all the momentum and velocity variables from the ADAM optimizer. Unfortunately 600MB exceeds [AI platform's model size limit](https://cloud.google.com/ai-platform/prediction/docs/quotas#model_size_limits) so we can't put it into production.\r\n\r\nUPDATE: @qhduan's hack worked for us. Kind of funny workaround.", "This has been fixed with commit https://github.com/tensorflow/tensorflow/commit/5931dc3cb388063c1512b16e1b90f579a012f91c\r\n\r\nThe optimizer slot variables in @philipp-eisen 's colab go away when running it with\r\n```\r\n!pip -q uninstall -y tensorflow\r\n!pip -q install 'tf-nightly==2.5.0.dev20210225'\r\n```", "Closing this issue now, as the fix has been verified!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40380\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40380\">No</a>\n"]}, {"number": 40379, "title": "AttributeError: 'Sequential' object has no attribute '_get_save_spec'", "body": "**System information**\r\n- OS Platform and Distribution Linux Ubuntu 18.04:\r\n- TensorFlow version 2.2.0:\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model).\r\n\r\n``---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-36-3e84dbd4022a> in <module>()\r\n      1 import tensorflow as tf\r\n----> 2 converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saving_utils.py in model_input_signature(model, keep_original_batch_size)\r\n     75     TensorSpecs. This list does not contain the `training` argument.\r\n     76   \"\"\"\r\n---> 77   input_specs = model._get_save_spec(dynamic_batch=not keep_original_batch_size)  # pylint: disable=protected-access\r\n     78   if input_specs is None:\r\n     79     return None\r\n\r\nAttributeError: 'Sequential' object has no attribute '_get_save_spec'\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I guess the error is coming from saving the given keras model object into the TF saved model directory during conversion. Probably, you need to make sure that your model is compatible with exporting to the saved model format.\r\n\r\nPlease refer #39001 I suspect that your code might be depending on keras but not on tf.keras.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40378, "title": "ValueError: You are trying to load a weight file containing 16 layers into a model with 0 layers.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@vijay2001 \r\nPlease share a simple stand alone code for us to replicate the issue faced and You may want to double check your weight file since it is saying the weight file contains 0 layers. \r\n\r\nwith respect to the error reported, please refer to these links:\r\n[link](https://github.com/keras-team/keras/issues/10417) [link1](https://github.com/tensorflow/tensorflow/issues/34016) [link2](https://stackoverflow.com/questions/51544666/keras-try-save-and-load-model-error-you-are-trying-to-load-a-weight-file-contain) [link3](https://www.google.com/search?rlz=1CANPDX_enIN885&biw=1410&bih=789&ei=O9ngXuzoMaHB3LUP19afsAk&q=You+are+trying+to+load+a+weight+file+containing+16+layers+into+a+model+with+0+layers&oq=You+are+trying+to+load+a+weight+file+containing+16+layers+into+a+model+with+0+layers&gs_lcp=CgZwc3ktYWIQAzICCABQir7ZJViKvtklYMLE2SVoAXAAeACAAbsBiAG7AZIBAzAuMZgBAKABAaABAqoBB2d3cy13aXqwAQA&sclient=psy-ab&ved=0ahUKEwis8uPVpvfpAhWhILcAHVfrB5YQ4dUDCAw&uact=5)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40377, "title": "[GPU delegate] Fix cannot locate symbol error", "body": "Fix cannot locate symbol \"_ZN6tflite3gpu2cl9Arguments11kArgsPrefixE\" error by defining Arguments::kArgsPrefix storage", "comments": []}, {"number": 40376, "title": "Included cudnn_backend.h when using cudnn 8", "body": "cudnn 8 requires cudnn_backend.h during tensorflow compilation. \r\nThis pull request adds it to the required headers when cudnn >= 8", "comments": ["Thanks for your help! Sorry for missing this. In the meantime, the same fix is already in review internally. It should be fixed soon."]}, {"number": 40375, "title": "ilsvrc:imagenet_accuracy_eval crossed compiled for Pixel4 fails on TF2.1.0's TFlite MobileNetV3 model", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 2.1.0\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\nPixel4\r\n\r\n**Describe the problem**\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n1. git clone https://github.com/tensorflow/tensorflow\r\n2. git checkout remotes/origin/r2.1\r\n3. conda install bazel==0.29.1\r\n4. sudo snap install android-studio --classic\r\n5. bazel build -c opt --config=android_arm64 //tensorflow/lite/tools/accuracy/ilsvrc:imagenet_accuracy_eval\r\n6. Copy the imagenet_accuracy_eval to Pixel4\r\n7. Download MobileNetV3 model and run TFLite converter with both TF1.5.0 and TF2.1.0. The two files obtained are attached.\r\n[mbnv3_uint8q_tf150.tflite.txt](https://github.com/tensorflow/tensorflow/files/4763102/mbnv3_uint8q_tf150.tflite.txt)\r\n[mbnv3_uint8q_tf210.tflite.txt](https://github.com/tensorflow/tensorflow/files/4763105/mbnv3_uint8q_tf210.tflite.txt)\r\n\r\n8. Copy both tflite files to Pixel4 and run them with the following commands:\r\nadb shell /data/local/tmp/yuan/imagenet_accuracy_eval --model_file=/data/local/tmp/yuan/mbnv3_uint8q_tf150.tflite --ground_truth_images_path=/data/local/tmp/ilsvrc_images --ground_truth_labels=/data/local/tmp/ilsvrc_validation_labels.txt --model_output_labels=/data/local/tmp/model_output_labels.txt --output_file_path=/data/local/tmp/mbv3_uint8q_tf150_output.txt --num_images=0\r\n\r\nThis will works for tf1.5 tflite model but it will crash for tf2.1.0 tflite model though I have used TF2.1 source code to compile imagenet_accuracy_eval:\r\n\r\nPixel4Host:~ yubei$ adb shell /data/local/tmp/yuan/imagenet_accuracy_eval_tf210 --model_file=/data/local/tmp/yuan/mbnv3_uint8q_tf210.tflite --ground_truth_images_path=/data/local/tmp/ilsvrc_images --ground_truth_labels=/data/local/tmp/yuan/ilsvrc_validation_labels.txt --model_output_labels=/data/local/tmp/model_output_labels.txt --output_file_path=/data/local/tmp/yuan/mbv3_uint8q_tf210_output.txt\r\nnative : imagenet_accuracy_eval.cc:213 Starting evaluation with: 4 threads.\r\nnative : imagenet_accuracy_eval.cc:117 Starting model evaluation: 100\r\nINFO: Initialized TensorFlow Lite runtime.\r\nAborted \r\n\r\nIt seems to me that this 2.1.0 executable doesn't support all functionalities in 2.1.0 TFLite models.\r\n", "comments": ["Any update on this issue? Many thanks.", "@luyuan2015 \r\nCould you please let us know if it is still an issue in TF v2.6.0 ? Could you please refer to the similar[ issue](https://stackoverflow.com/questions/62208454/tflite-cross-compiling-arm64-build-error) and let us know if it helps? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40375\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40375\">No</a>\n"]}, {"number": 40374, "title": "Use keras `ModelCheckpoint` failed in multi-worker distribute training env", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but the bug happens in official codes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): No\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: not relevant\r\n\r\n**Describe the current behavior**\r\nI'm writing some codes for training a keras model in multi-worker distribute strategy env. For some compatibility issues I don't use keras `model.fit` API to launch my training loop and write my customized training loop instead. In multi-gpu env with MirroredStrategy the keras `ModelCheckpoint` works fine, but when I switch to `MultiWorkerMirroredStrategy`, the `ModelCheckpoint` returns error at the beginning of the training job.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"my_training_script.py\", line 720, in custom_training_loop\r\n    callbacks._call_begin_hook(ModeKeys.TRAIN)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/callbacks.py\", line 253, in _call_begin_hook\r\n    self.on_train_begin()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/callbacks.py\", line 369, in on_train_begin\r\n    callback.on_train_begin(logs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/callbacks.py\", line 931, in on_train_begin\r\n    training_state.MultiWorkerTrainingState(self.model, self.filepath))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/distribute/multi_worker_training_state.py\", line 90, in __init__\r\n    if not multi_worker_util.should_save_checkpoint():\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/multi_worker_util.py\", line 243, in should_save_checkpoint\r\n    return dc_context.get_current_worker_context().should_checkpoint\r\nAttributeError: 'NoneType' object has no attribute 'should_checkpoint'\r\n```\r\n\r\nAfter a brief code search, I found the root cause of this error in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/distribute_coordinator_context.py#L26:\r\n\r\n```\r\ndef get_current_worker_context():\r\n  \"\"\"Returns the current task context.\"\"\"\r\n  try:\r\n    return _worker_context.current\r\n  except AttributeError:\r\n    return None\r\n```\r\nWhich means the worker context is probably not set before the callback hooks launch. It returns a `None` object and `should_save_checkpoint` fails. I guess if other keras callbacks depend on the multi worker context, they will also fail if the multi-worker context is not correctly set before the training job starts.\r\n\r\nSo my question is that, is this behaviour a bug ? If not, how can I correctly setup the multi-worker training env without using `model.compile` or `model.fit`. According to the docs, the distribute training can be run by `strategy.run(step_function, args=args, kwargs=kwargs)` and `strategy.run` function handles the context settings. In this case, should I call the keras callbacks in `strategy.scope`, since my callbacks are not executed in `step_function` ?\r\n", "comments": ["In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40374\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40374\">No</a>\n"]}]