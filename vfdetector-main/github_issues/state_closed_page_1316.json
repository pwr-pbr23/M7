[{"number": 13623, "title": "Why the parameters aren't stored on ps server for distributed tensorflow ?", "body": "Please see more detail on [stackoverflow](https://stackoverflow.com/questions/46667790/distributed-tensorflow-of-between-graph-replication).\r\n\r\nI don't start `ps` server, and parameters are stored on `worker` server. ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. Please refer to the answer of the StackOverflow question you linked. Thanks!"]}, {"number": 13622, "title": "CudnnLSTM returns all Ones(1) after the 10th sequence ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: v1.3.0-rc1-1486-g752dcb6 1.3.0\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: GTX1050Ti\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI tried to use CudnnLSTM to speed up the training, but found it only returns one after the 10th step, following code generate the output.\r\n\r\n### Source code / logs\r\n    import tensorflow as tf\r\n    from tensorflow.contrib.cudnn_rnn import CudnnLSTM\r\n    import numpy as np\r\n\r\n\r\n    np.set_printoptions(linewidth=240, edgeitems=6)\r\n    # Reset default graph\r\n    tf.reset_default_graph()\r\n\r\n    num_layer = 5\r\n    num_unit = 256\r\n    input_size = 400\r\n    seq_lenght = 20\r\n\r\n    with tf.device('/gpu:0'):\r\n        x = tf.random_uniform([seq_lenght, input_size], maxval=1, dtype=tf.float32)\r\n        x1 = tf.expand_dims(x, 1)\r\n        lstm = CudnnLSTM(num_layers=num_layer, num_units=num_unit, input_size=input_size,\r\n                     input_mode='linear_input',\r\n                     direction='unidirectional')\r\n\r\n        # CudnnLSTM parameter\r\n        lstm_para_size = lstm.params_size()\r\n        lstm_para = tf.Variable(tf.random_uniform([lstm_para_size]), validate_shape=False, name='lstm_para')\r\n\r\n        state_c = tf.Variable(tf.zeros(shape=[num_layer, 1, num_unit]), trainable=False)\r\n        state_h = tf.Variable(tf.zeros(shape=[num_layer, 1, num_unit]), trainable=False)\r\n\r\n        lstm_output, lstm_h, lstm_c = lstm(input_data=x1, input_h=state_h, input_c=state_c, params=lstm_para)\r\n\r\n    # Variable initializing op\r\n    init = tf.global_variables_initializer()\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n    cudnn_output = sess.run(lstm_output)\r\n    print(cudnn_output)\r\n\r\n###LSTM output\r\n[[[ 0.76159418  0.76159418  0.76159418  0.76159418  0.76159418  0.76159418 ...,  0.76159418  0.76159418  0.76159418  0.76159418  0.76159418  0.76159418]]\r\n\r\n [[ 0.96402758  0.96402758  0.96402758  0.96402758  0.96402758  0.96402758 ...,  0.96402758  0.96402758  0.96402758  0.96402758  0.96402758  0.96402758]]\r\n\r\n [[ 0.99505478  0.99505478  0.99505478  0.99505478  0.99505478  0.99505478 ...,  0.99505478  0.99505478  0.99505478  0.99505478  0.99505478  0.99505478]]\r\n\r\n [[ 0.99932933  0.99932933  0.99932933  0.99932933  0.99932933  0.99932933 ...,  0.99932933  0.99932933  0.99932933  0.99932933  0.99932933  0.99932933]]\r\n\r\n [[ 0.99990922  0.99990922  0.99990922  0.99990922  0.99990922  0.99990922 ...,  0.99990922  0.99990922  0.99990922  0.99990922  0.99990922  0.99990922]]\r\n\r\n [[ 0.99998772  0.99998772  0.99998772  0.99998772  0.99998772  0.99998772 ...,  0.99998772  0.99998772  0.99998772  0.99998772  0.99998772  0.99998772]]\r\n\r\n ..., \r\n [[ 1.          1.          1.          1.          1.          1.         ...,  1.          1.          1.          1.          1.          1.        ]]\r\n\r\n [[ 1.          1.          1.          1.          1.          1.         ...,  1.          1.          1.          1.          1.          1.        ]]\r\n\r\n [[ 1.          1.          1.          1.          1.          1.         ...,  1.          1.          1.          1.          1.          1.        ]]\r\n\r\n [[ 1.          1.          1.          1.          1.          1.         ...,  1.          1.          1.          1.          1.          1.        ]]\r\n\r\n [[ 1.          1.          1.          1.          1.          1.         ...,  1.          1.          1.          1.          1.          1.        ]]\r\n\r\n [[ 1.          1.          1.          1.          1.          1.         ...,  1.          1.          1.          1.          1.          1.        ]]]", "comments": ["@zheng-xq Can you comment on this, or suggest someone who can?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@protoget, could you take a look at this? Thanks!", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "We have introduced new CudnnLSTM layers after TF1.5 which manages the variables for you. Please try that out.\r\nAlso, note that the code doesn't show how train_ops are created.\r\n```py\r\n    lstm_para_size = lstm.params_size()\r\n    lstm_para = tf.Variable(tf.random_uniform([lstm_para_size]), validate_shape=False, name='lstm_para')\r\n\r\n    state_c = tf.Variable(tf.zeros(shape=[num_layer, 1, num_unit]), trainable=False)\r\n    state_h = tf.Variable(tf.zeros(shape=[num_layer, 1, num_unit]), trainable=False)\r\n\r\n    lstm_output, lstm_h, lstm_c = lstm(input_data=x1, input_h=state_h, input_c=state_c, params=lstm_para)\r\n```\r\n\r\nIf you're just doing inference, does \"1\" show up in one step or persists after one step? I suspect with your input it happens that the ground truth output is 1 for certain step.", "@protoget  I find it difficult to use new CudnnLSTM layer api in TF1.5, are there some sample use case for the new api?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@xingjinglu I think the new CudnnLSTM **layer** should make things way simpler.\r\nyou can check out unittest of the class to see examples.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/kernel_tests/cudnn_rnn_test.py#L118"]}, {"number": 13621, "title": "\u201cNone\u201d value for gradient of Tensorflow variables which is used in the network", "body": "I want to develop a custom Seq2Seq in tensorflow and I've created this part of network\r\n\r\n`wflat0 = tf.get_variable(\"wflat0\", shape=(1024, 1024), \r\ninitializer=LinearInitializer)\r\nbflat0 = tf.get_variable(\"bflat0\", shape=1024, initializer=BiasInitializer)\r\n\r\nl0flat = selu(tf.matmul(x, wflat0) + bflat0)\r\nx=tf.reshape(l0flat,[shape[0],shape[1],1024])\r\nx = tf.unstack(x, constant.Lstm_cell, 1)\r\nlstm_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(LSTMHiddenSize, forget_bias=1.0, dropout_keep_prob=0.9)\r\noutputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\r\n\r\ndecoder = tf.contrib.rnn.LayerNormBasicLSTMCell(LSTMHiddenSize, forget_bias=1.0, dropout_keep_prob=0.9)\r\n\r\ny = tf.unstack(tf.reshape(self.y_,[tf.shape(self.y_)[0],tf.shape(self.y_)[1],1]), constant.Lstm_cell, 1)\r\ndecoutputs, decstates = tf.contrib.rnn.static_rnn(decoder, y, dtype=tf.float32)\r\n\r\nwflat1 = tf.get_variable(\"wflat1\", shape=(LSTMHiddenSize, 128),\r\n                                     initializer=LinearInitializer)\r\nbflat1 = tf.get_variable(\"bflat1\", shape=128, initializer=BiasInitializer)\r\nshapes = tf.shape(decoutputs)\r\ndecoutputs=tf.transpose(decoutputs, perm=[1, 0, 2])\r\ndecoutputs=tf.reshape(decoutputs,(shapes[0]*shapes[1],shapes[2]))\r\nself.sss=decoutputs\r\nl1flat = selu(tf.matmul(decoutputs, wflat1) + bflat1)\r\nwflat2 = tf.get_variable(\"wflat2\", shape=(128, 1), initializer=LinearInitializer)\r\nbflat2 = tf.get_variable(\"bflat2\", shape=1, initializer=BiasInitializer)\r\nl2flat = tf.matmul(l1flat, wflat2) + bflat2\r\nl2flat = tf.reshape(l2flat,(constant.batchsize , constant.Lstm_cell))\r\nself.sigout=tf.nn.softmax(l2flat)\r\nself.out=l2flat\r\ncost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=self.out, labels=self.y_))`\r\n\r\nand this network create NAN output after first update iteration for large sequence and it just works for small sequence so I've decided to add gradient clipping and when I was developing that I've found that most of the variables have a none gradient as you can see in this [picture Imag](https://i.stack.imgur.com/XtkY0.png)e of computed Gradient how could it possible that some variables which take participate in loss function have a none gradient is this a bug?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13620, "title": "Erase Operation on tf.contrib.lookup.MutableHashTable", "body": "### System information\r\n- TensorFlow r1.3\r\n\r\n### Describe the problem\r\n\r\nThe `tf.contrib.lookup` package has `MutableHashTable` class which wraps the C++11 std::unordered_map class. However, the erase method of unordered_map is not exposed. \r\n\r\nIs there some existing way to erase keys from the lookup table? \r\n\r\nIf not I can create a patch with a new kernel/op on the MutableHashTable classes, and also add a method to the Python API.\r\n\r\nThanks", "comments": ["@ysuematsu Can you comment on this?", "Nagging Assignee @ysuematsu: It has been 154 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ysuematsu: It has been 169 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 13619, "title": "The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, str", "body": "When i feed the value, unfortunately , it is a tensor.\r\ni have already known that i should convert it to a array or other types to feed. But if i use sess.run() or .eval(). The speed makes me crazy, for the shape is [3600,14,25500].\r\n\r\nif other ways are available?\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13618, "title": "question in image_ops_impl.py", "body": "I am confused about the code line 827 in image_ops_impl.py .It should be \r\nvariance = (math_ops.reduce_mean(math_ops.square(image-image_mean))\r\n\r\nor some reason for the equation?\r\n", "comments": ["Looking at the code history, the current logic has been there since \"the beginning\".\r\n\r\nThe code in question is here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py#L827\r\n\r\nHere's the existing code:\r\n```\r\n  variance = (math_ops.reduce_mean(math_ops.square(image)) -\r\n              math_ops.square(image_mean))\r\n```\r\n\r\nHere's the proposed code:\r\n```\r\n  variance = (math_ops.reduce_mean(math_ops.square(image-image_mean))\r\n```\r\n\r\nI believe these compute the same logical value, but might have different properties wrt numerical stability.\r\n\r\n@rmlarsen might have some thoughts here on why we'd prefer one over the other.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 13617, "title": "Add contributing authors to 1.4 Release notes.", "body": "Thanks!", "comments": []}, {"number": 13616, "title": "Force tensor evaluation inside while-loop, scan and others.", "body": "Hello everyone,\r\n\r\nI had big plans for `tf.while_loop` until I discovered that it is impossible to re-evaluate tensor inside it. Let's dive into the the issue and potential useful feature:\r\n\r\n```python\r\nIn [13]: import tensorflow as tf\r\n    ...: import numpy as np\r\n    ...:\r\n    ...: def cond(i, _x, _sq):\r\n    ...:       return tf.less(i, 10)\r\n    ...:\r\n    ...: def gen_body(v):\r\n    ...:     def body(i, x, sq):\r\n    ...:         x = tf.Print(x, [x, sq], \"x and sq: \")\r\n    ...:         v_assign = v.assign(x + 1)\r\n    ...:         v_assign = tf.Print(v_assign, [v_assign], \"v_assign: \")\r\n    ...:         with tf.control_dependencies([v_assign]):\r\n    ...:             sq_neg = tf.negative(sq)\r\n    ...:         sq_neg = tf.Print(sq_neg, [i, sq_neg], message='i and sq_neg:')\r\n    ...:         return tf.add(i, 1), sq_neg, sq\r\n    ...:     return body\r\n    ...:\r\n\r\nIn [14]: sess = tf.InteractiveSession()\r\n\r\nIn [15]: i = tf.Variable(0)\r\n    ...: v = tf.Variable(2)\r\n    ...: sq = tf.square(v)\r\n    ...: l = tf.while_loop(cond, gen_body(v), (i, v, sq))\r\n    ...: sess.run(tf.global_variables_initializer())\r\n    ...: sess.run(l)\r\n    ...:\r\n2017-10-10 22:59:44.819271: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [2][4]\r\n2017-10-10 22:59:44.819405: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [3]\r\n2017-10-10 22:59:44.819466: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[0][-4]\r\n2017-10-10 22:59:44.819553: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]\r\n2017-10-10 22:59:44.819615: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]\r\n2017-10-10 22:59:44.819680: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[1][-4]\r\n2017-10-10 22:59:44.819827: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]\r\n2017-10-10 22:59:44.819885: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]\r\n2017-10-10 22:59:44.819932: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[2][-4]\r\n2017-10-10 22:59:44.820034: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]\r\n2017-10-10 22:59:44.820094: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]\r\n2017-10-10 22:59:44.820111: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[3][-4]\r\n2017-10-10 22:59:44.820162: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]\r\n2017-10-10 22:59:44.820250: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]\r\n2017-10-10 22:59:44.820265: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[4][-4]\r\n2017-10-10 22:59:44.820315: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]\r\n2017-10-10 22:59:44.820379: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]\r\n2017-10-10 22:59:44.820408: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[5][-4]\r\n2017-10-10 22:59:44.820428: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]\r\n2017-10-10 22:59:44.820438: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]\r\n2017-10-10 22:59:44.820446: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[6][-4]\r\n2017-10-10 22:59:44.820464: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]\r\n2017-10-10 22:59:44.820490: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]\r\n2017-10-10 22:59:44.820500: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[7][-4]\r\n2017-10-10 22:59:44.820519: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]\r\n2017-10-10 22:59:44.820532: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]\r\n2017-10-10 22:59:44.820542: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[8][-4]\r\n2017-10-10 22:59:44.820559: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]\r\n2017-10-10 22:59:44.820580: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]\r\n2017-10-10 22:59:44.820593: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[9][-4]\r\nOut[15]: (10, -4, 4)\r\n```\r\n\r\nI created `v` variable and tensor `sq` which equals to `v^2`. In fact, we don't have control over them, they are our input as `x` and `y` and we know that `y` depends on `x`. I would like to assign new value inside TensorFlow loop to `x` (equavalent to `v` at example) and evaluate fresh `y` (`sq` inside example) at each iteration of the loop. Meanwhile we can do other evaluations inside `while_loop`, but most important is that I need to update `x` and get updated `y`. Currently, assigning operation doesn't propagate updates down to the dependant nodes and it shouldn't, but when someone calls tensor depending on value which were updated via assign inside `while_loop`, I suppose the tensor node must detect this change and evaluate new tensor value again.\r\n\r\nThanks!", "comments": ["@skye Can you comment on this?", "Let me make sure I understand what you're requesting: you would like a tensor computed from variables to have a new updated value every time it's executed in a while loop.\r\n\r\nThis is a reasonable expectation. However, I don't think we'll change this behavior. Right now, only tensors defined inside the loop will be evaluated every loop iteration. All tensors defined outside a loop will be evaluated exactly once. So you would have to write something like:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef cond(i, _x):\r\n      return tf.less(i, 10)\r\n\r\ndef gen_body(v):\r\n    def body(i, x):\r\n        x = tf.Print(x, [x], \"x: \")\r\n        v_assign = v.assign(x + 1)\r\n        v_assign = tf.Print(v_assign, [v_assign], \"v_assign: \")\r\n        with tf.control_dependencies([v_assign]):\r\n          sq = tf.square(v)  # <---- put 'sq' definition inside loop\r\n          sq = tf.Print(sq, [sq], \"sq: \")\r\n          sq_neg = tf.negative(sq)\r\n        sq_neg = tf.Print(sq_neg, [i, sq_neg], message='i and sq_neg:')\r\n        return tf.add(i, 1), sq_neg\r\n    return body\r\n\r\nsess = tf.InteractiveSession()\r\n\r\nv = tf.Variable(2)\r\nl = tf.while_loop(cond, gen_body(v), (1, v))\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(l)\r\n``` \r\nNote that I moved the `sq = ...` inside the loop (and also made it depend on v_assign, to make sure we pick up the new assignment).\r\n\r\nDoes this help?", "@ebrevdo @mrry FYI", "I believe if you want a variable to be evaluated at each value of the loop, you should access it using variable.ref", "or perhaps if you use a ResourceVariable (get_variable(..., use_resource=True)) it may do the right thing.\r\n\r\nyou may need to also set parallel_iterations=1, but probably not.", "This is an optimization that we turned on because you don't want a Variable sitting on a parameter server elsewhere in the datacenter to be accessed at each iteration of an RNN.  you want to access it only once and cache it.  in this case you have an assign inside the while_loop and converting a Variable to a Tensor doesn't look to see if an assign has been performed just prior to this, to know it must refresh the value.  @mrry should we reset the caching identity tensor of a Variable after each assign?", "@skye also, wdyt?", "You're more familiar with these semantics than I am, but changing this might break (or at least regress) existing models, no?", "Not sure.  Depends how many users perform a tf.assign/tf.assign_{add,sub} inside a while_loop (including applying an optimiser) and read the new value inside the while loop -- and what behavior they expect; and may be getting silent errors or models not converging because of this issue.\r\n\r\nIf this already just \"works\" with ResourceVariable, then that's a sign that the Variable semantics are just broken.", "> should we reset the caching identity tensor of a Variable after each assign?\r\n\r\nI think this gets tricky because (from the perspective of a `tf.Variable` object) only some subset of the `Variable.assign()` (etc.) calls run in an any particular step, and so you can't use control dependencies to ensure that the identity is ordered after an assign. (Perhaps I am being too pessimistic and you have a more cunning solution in mind?)\r\n\r\n> If this already just \"works\" with ResourceVariable, then that's a sign that the Variable semantics are just broken.\r\n\r\n+1. I'd strongly encourage people to switch to ResourceVariable in new code, and it's more compatible with other recent features like `tf.data` and some of the `Estimator` variants. I'm not sure what it would take to switch the default, however.", "Thank you for your responses. Here is some clarifications to the task:\r\n\r\n* There are a variable _x_ and a tensor _y_ - the result of computation upon x. `y = tf.square(x)`. They created outside of the loop.\r\n* I pass implicitly to the while_loop both tensors - x and y\r\n* I assign new value to x inside loop (let's say `x.assign(x + 1)`) and expect y gets evaluated with new x value (`x + 1`) when it is used somewhere after assigning. So, the aim is to re-evaluate `y` at each iteration along with `x` assigning.\r\n\r\n**edited**\r\n\r\n@skye, so `sq` cannot be recreated inside `while_loop`, because we don't know what it does.", "I can be wrong, but in example below I never got different values for assign operation `b` and `a` variable outputs. I'm sure it *can* be a _coincidence_. Anyway, I got different values for `c`, and it is obvious because control_dependencies does **not** work in that case. I guess, that it would be useful to be able to construct an operation like `read_value()`, but for any tensor, so that whenever it is called it would re-evaluate that tensor.\r\n\r\n```python\r\nIn [91]: tf.reset_default_graph()\r\n    ...: a = tf.get_variable('a', initializer=2, use_resource=True)\r\n    ...: c = a + 1\r\n    ...: b = a.assign(a * 10)\r\n    ...: with tf.control_dependencies([b]):\r\n    ...:     c = c # <<<<<<<<<<<<<<< \r\n    ...:              # here should be `c.eval_tensor()` or `c.read_tensor()`, \r\n    ...:              # which constructs an operation being equivalent\r\n    ...:              # to read_value() for variables.\r\n    ...:              # ***Edited***: tf.identity() doesn't work here!\r\n    ...: sess = tf.InteractiveSession()\r\n    ...: sess.run(tf.global_variables_initializer())\r\n    ...: sess.run([a, b, c])\r\n...\r\nOut[91]: [20, 20, 3]   # <<< 3 \r\n...\r\nOut[92]: [20, 20, 21]   # <<< 21\r\n```", "@skye, @mrry, @ebrevdo I'm sorry for bombarding you with multiple messages, here is a proof that assigning inside loop is not a problem:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef cond(i, _x):\r\n      return tf.less(i, 10000)\r\n\r\ndef gen_body(x):\r\n    def body(i, xa_prev):\r\n        xa = x.assign(xa_prev + 1)\r\n        with tf.control_dependencies([xa]):\r\n            with tf.control_dependencies([tf.assert_equal(x, xa)]):\r\n                i = tf.add(i, 1)\r\n        return i, xa\r\n    return body\r\n\r\ntf.reset_default_graph()\r\ni = tf.get_variable('i', initializer=0, use_resource=True)\r\nv = tf.get_variable('v', initializer=0, use_resource=True)\r\nsess = tf.InteractiveSession()\r\nloop = tf.while_loop(cond, gen_body(v), [i, v])\r\nsess.run(tf.global_variables_initializer())\r\nsess.run([loop])\r\n\r\nOut[203]: [(10000, 10000)]\r\n```\r\n\r\nThe issue is that there is no way to recompute tensors which depend on it and were constructed outside of the `while_loop`. :)", "I faced the same problem,    \r\n```\r\ndef body(rawstate,x,v,coef):\r\n    rawstate.assign_add(tf.matmul(x[v],coef[v]))\r\n    v.assign_add(1)\r\n    return [rawstate,x,v,coef]\r\n```\r\n`calrawstate=tf.while_loop(cond,body,[rawstate,x,v,coef])`\r\nAnd I faced this error,   \r\n`AttributeError: 'Tensor' object has no attribute 'assign_add'`   \r\nHere `rawstate` and `v` are two variables created using `tf.get_variable()`   \r\n\r\nAlthough this problem is trivial, and can be done without using `assign` or `assign_add` inside `body`   \r\nbut sometimes it's needed in some case or in some custom model\r\nsuppose if we want to assign some specific value in `rawstate[0]` it's hard...", "@dchatterjee172, I think you have different issue. `rawstate` is a tensor, and by definition you can't assign value to it. If you expected to see same variable at each iteration, then it works a bit different. But it would be nice to pass variable to the loop as a reference.\r\n\r\n@ebrevdo what is `variable.ref`?", "After playing with this for a bit, I reached two conclusions:\r\n\r\n1. `ResourceVariable` has the same problem as `Variable`.\r\n2. `Variable.ref` has been hidden from view; but you can access it using a hidden API (wherein your code will break in the future) via `Variable._ref()` or `Variable._variable`.\r\n\r\nHere's code that does what I think you want it to do:\r\n\r\n```python\r\ndef cond(i, _x, _sq):\r\n  return i < 5\r\n\r\ndef gen_body(v):\r\n  def body(i, x, sq):\r\n    x = tf.Print(x, [x, sq], \"x and sq: \")\r\n    with tf.control_dependencies([v.assign(x + 1)]):\r\n      v_assign = v._ref()\r\n    v_assign = tf.Print(v_assign, [v_assign], \"v_assign: \")\r\n    with tf.control_dependencies([v_assign]):\r\n      sq_neg = tf.negative(sq)\r\n      sq_neg = tf.Print(sq_neg, [i, sq_neg], message='i and sq_neg:')\r\n      return tf.add(i, 1), sq_neg, sq\r\n  return body\r\n\r\nsess = tf.InteractiveSession()\r\n\r\n\r\ni = tf.get_variable(\"i\", initializer=0)\r\nv = tf.get_variable(\"v\", initializer=2)\r\nsq = tf.square(v)\r\nl = tf.while_loop(cond, gen_body(v), (i, v, sq))\r\nsess.run(tf.global_variables_initializer())\r\nsess.run((l, v))\r\n```\r\n\r\n@alextp any suggestions on how one would force accessing the updated value of a `ResourceVariable` within a calculation?  This has implications not only for while loops but also other distributed computations where one wants to access a new value from another machine after waiting for a bit.", "@alextp perhaps a stateful read operator is in order?", "It is possible to force a resource variable read to happen after an assignment, and indeed it will happen. @ebrevdo , why did resource variables not work? Do you have an example with them?\r\n\r\n(I do not fully understand why we're using variables at all here instead of the regular while loop variable mechanism)", "@alextp, @ebrevdo Hello, your snippet doesn't do what I need, I simplified it a bit below. Let me explain what it does.\r\nThere is function f and variable x. At each iteration step I increment x and **want** to re-evaluate f and take negative of it. **What's good:** if I take assign tensorflow operation and pass it to the next iteration (even if I'm not using it), I can successfully increment external variable x. **What's bad** the function - `f` evaluated only once, but _must be evaluated  with freshly assigned x as many times as number of iterations_. Check example below:\r\n\r\nHere `f(x) = x^2`, and `f_neg=-f(x)`:\r\n\r\n```python\r\ndef cond(i, _x_prev, _f_prev):\r\n  return i < 3\r\n\r\ndef gen_body(x, f):\r\n  def body(i, _x_prev, _f_prev):\r\n    x_assign = x.assign(x + 1)\r\n    # with tf.control_dependencies([x.assign(x + 1)]):\r\n    #   x_assign = x._ref()\r\n    with tf.control_dependencies([x_assign]):\r\n      f_neg = tf.negative(f)\r\n      i = tf.add(i, 1)\r\n      i = tf.Print(i, [i], message='>>> Iteration ')\r\n      i = tf.Print(i, [x], message='x = ')\r\n      i = tf.Print(i, [x_assign], message='x_assign = ')\r\n      i = tf.Print(i, [f], message='f = ')\r\n      i = tf.Print(i, [f_neg], message='f_neg = ')\r\n      return i, x_assign, f_neg\r\n  return body\r\n\r\ntf.reset_default_graph()\r\nsess = tf.InteractiveSession()\r\ni = tf.get_variable(\"i\", initializer=0)\r\nv = tf.get_variable(\"v\", initializer=0)\r\nfunc_v = tf.square(v)\r\nl = tf.while_loop(cond, gen_body(v, func_v), (i, v, func_v))\r\nsess.run(tf.global_variables_initializer())\r\nsess.run((l, v))\r\n```\r\n\r\nOutput. x incremented successfully from 0 to 3, but f remains non-updated! **I need updated f**.\r\n\r\n```\r\n... >>> Iteration [1]\r\n... x = [1]\r\n... x_assign = [1]\r\n... f = [0]\r\n... f_neg = [0]\r\n\r\n... >>> Iteration [2]\r\n... x = [2]\r\n... x_assign = [2]\r\n... f = [0]\r\n... f_neg = [0]\r\n\r\n... >>> Iteration [3]\r\n... x = [3]\r\n... x_assign = [3]\r\n... f = [0]\r\n... f_neg = [0]\r\n```\r\n\r\n```python\r\nIn [259]: tf.__version__\r\nOut[259]: '1.2.1'\r\n```\r\n\r\nI propose to add a method to the tensor structure which will be equivalent to `read_value()` for standard variable. It will form operation which re-evalutes the tensor when it is required. In example above that tensor is `f`.\r\n", "@alextp, \r\n> (I do not fully understand why we're using variables at all here instead of the regular while loop variable mechanism)\r\n\r\nThe task is **update the variable and the dependant on it function** which are defined outside of `while_loop`. In example above, the `f` is `func_v` tensor and `x` is `v` variable, both passed to `gen_body`.", "Right. If you want the function to be computed inside the while loop you need to call it inside the while loop. Something like\r\n\r\n```python\r\ndef cond(i, _x_prev, _f_prev):\r\n  return i < 3\r\n\r\ndef gen_body(x, f):\r\n  def body(i, _x_prev, _f_prev):\r\n    x_assign = x.assign(x + 1)\r\n    # with tf.control_dependencies([x.assign(x + 1)]):\r\n    #   x_assign = x._ref()\r\n    with tf.control_dependencies([x_assign]):\r\n      f_neg = tf.negative(f(x_assign))\r\n      i = tf.add(i, 1)\r\n      i = tf.Print(i, [i], message='>>> Iteration ')\r\n      i = tf.Print(i, [x], message='x = ')\r\n      i = tf.Print(i, [f(x_assign)], message='x_assign = ')\r\n      i = tf.Print(i, [f(v)], message='f = ')\r\n      i = tf.Print(i, [f_neg], message='f_neg = ')\r\n      return i, x_assign, f_neg\r\n  return body\r\n\r\ntf.reset_default_graph()\r\nsess = tf.InteractiveSession()\r\ni = tf.get_variable(\"i\", initializer=0)\r\nv = tf.get_variable(\"v\", initializer=0)\r\nfunc_v = lambda v: tf.square(v)\r\nl = tf.while_loop(cond, gen_body(v, func_v), (i, v, func_v(v)))\r\nsess.run(tf.global_variables_initializer())\r\nsess.run((l, v))\r\n```\r\n\r\nWhen you define func_v as a tensor outside the loop it has a fixed value which will stay fixed forever. If you want it to be recomputed you need to actually recompute it. ", "@alextp, that's right. In fact, it was a feature request - to be able to recompute the tensor and introduce property which generates this operation. I'm sorry if it wasn't clear from previous messages  That would be super useful in `while_loop`-like flow statements.\r\n\r\nThe solution which you proposed doesn't work for me, as I do not have control on input tensors `x` and `f`, and I do not know what's inside `f`.\r\n\r\nHow hard is it to implement re-computation operation for tensor?", "With the current graph semantics it's essentially impossible.\n\nOn Fri, Oct 13, 2017 at 7:51 AM, Artem Artemev <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp>, that's right. In fact, it was a\n> feature request - to be able to recompute the tensor and introduce property\n> which generates this operation. I'm sorry if it wasn't clear from previous\n> messages That would be super useful in while_loop-like flow statements.\n>\n> The solution which you proposed doesn't work for me, as I do have control\n> on input tensors x and f, and I do not know what's inside f.\n>\n> How hard is it to implement re-computation operation for tensor?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13616#issuecomment-336474085>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxcKI_k6pTejahsF7TvgFWFYvTgyCks5sr3j5gaJpZM4P0qPF>\n> .\n>\n\n\n\n-- \n - Alex\n", "@alextp. oh, that's bad. Okay, If I manage to pass f(x) as python function, hence at each iteration I construct new tensor `y = f(x)`, does it mean that the size of the graph will grow linearly with number of iterations? Can an unwise usage of `while_loop` lead to the graph \"overflow\", taking into account that graph size is limited?", "No, the size of the graph is constant even if you call f(x) in each\niteration, because tensorflow's while loop is symbolic and only creates the\nbody graph once.\n\nOn Fri, Oct 13, 2017 at 8:43 AM, Artem Artemev <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp>. oh, that's bad. Okay, I managed to\n> pass f(x) as python function and at each iteration I construct new tensor y\n> = f(x), does it mean that the size of the graph will grow linearly with\n> number of iterations? Can an unwise usage of while_loop lead to the graph\n> \"overflow\", taking into account that graph size is limited?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13616#issuecomment-336488570>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxXr6HKksKduSmuWFS3-emCehda_tks5sr4UrgaJpZM4P0qPF>\n> .\n>\n\n\n\n-- \n - Alex\n", "@alextp, last question: If I create c++ operation which will accept `x` variable and `f` tensor, will I be able to manipulate an execution of `f` depending on changes in `x`. In other words, will I be able to execute `f` tensor each time when I change `x`. Thanks!", "You need to explicitly include this C++ operation in the graph in all the\nplaces where you'd want it to run. There's no way to trigger it running\nonly when x is modified.\n\nOn Fri, Oct 13, 2017 at 1:35 PM, Artem Artemev <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp>, last question: If I create c++\n> operation, which will accept x variable and f tensor, will I be able to\n> manipulate execution of f depending on changes in x. In other words, will\n> I be able to execute f tensor each time when I change x. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13616#issuecomment-336560051>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxUGGup22k2UCOAs2An65SDeEfn6fks5sr8mYgaJpZM4P0qPF>\n> .\n>\n\n\n\n-- \n - Alex\n", "Thanks all for your responses!", "Reevaluation does not work outside of a loop; a tensor is only evaluated a\nsingle time within a single call to session.run.\n\nTry to have more than one update per call to session.run and you'll see\nthings breaking.\n\n(your example also relies on the lack of a memory model for legacy tf\nvariables but this is not relevant)\n\nOn Sat, Nov 3, 2018 at 11:32 AM Jonas Eschle <notifications@github.com>\nwrote:\n\n> A year later, did anything change on that? @alextp\n> <https://github.com/alextp>, is it still impossible? I find that it\n> somehow works and am confused about why it does/does not (I write here as\n> the previous discussion is basically the same thing as I try to accomplish):\n>\n> Basically, the re-evaluation works *outside* of a loop, that's confusing\n> for me\n>\n> Pseudo-code\n>\n> x = <Tensor depending on var>\n>\n> var_assign = tf.assign(var, value)\n> with tf.control_dependencies([var_assign])\n> \tsess.run(x)  # prints with the old value, of course\n>\n> but adding an identity before\n>\n>\n> var_assign = tf.assign(var, value)\n> with tf.control_dependencies([var_assign])\n> \tx_2 = tf.identity(x)\n> \tsess.run(x_2)  # prints with the new value!\n>\n> So in the second case, sess.run reevaluates the whole Tensor X. But this\n> does not work in a while loop (of course, we cannot have a sess.run). So\n> why does x_2 get's re-evaluated here but not in a while-loop? I did not\n> expect this way to work\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13616#issuecomment-435610487>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxfWbCkETnuov7i73C5uTctP_vaXXks5ureFJgaJpZM4P0qPF>\n> .\n>\n\n\n-- \n - Alex\n", "Ah, I see (so it's actually independent of a loop): one sess.run -> zero (if can be cached) or one evaluation of a tensor.\r\n\r\nThanks, @alextp! "]}, {"number": 13615, "title": "Importing tensorflow after import pytorch crashes inside tensorflow::port::TestCPUFeature", "body": "TF version:  https://github.com/tensorflow/tensorflow/commit/07bf1d3\r\nPyTorch version: '0.2.0_4' (whatever is installed by default yesterday)\r\n\r\nCrashing code\r\n```\r\nimport torch\r\nimport tensorflow\r\n```\r\nWork-around\r\n```\r\nimport tensorflow\r\nimport torch\r\n```\r\n\r\nStacktrace\r\n\r\n```\r\n#0  0x00007f2d2f5e7577 in void std::__once_call_impl<std::_Bind_simple<void (*())()> >() ()\r\n   from /home/yaroslav/anaconda3/envs/oct12/lib/python3.5/site-packages/torch/lib/libTHC.so.1\r\n#1  0x00007f2d546d5a99 in __pthread_once_slow (\r\n    once_control=0x7f2d0a470830 <tensorflow::port::(anonymous namespace)::cpuid_once_flag>, init_routine=0x7f2d2789e2a0 <__once_proxy>) at pthread_once.c:116\r\n#2  0x00007f2d09ae7faa in void std::call_once<void (&)()>(std::once_flag&, void (&)()) ()\r\n   from /home/yaroslav/anaconda3/envs/oct12/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007f2d09ae7fee in tensorflow::port::TestCPUFeature(tensorflow::port::CPUFeature) ()\r\n   from /home/yaroslav/anaconda3/envs/oct12/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007f2d09942895 in _GLOBAL__sub_I_cpu_feature_guard.cc ()\r\n   from /home/yaroslav/anaconda3/envs/oct12/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007f2d54de86ba in call_init (l=<optimized out>, argc=argc@entry=2, \r\n    argv=argv@entry=0x7ffe69b84308, env=env@entry=0x1ba0d00) at dl-init.c:72\r\n#6  0x00007f2d54de87cb in call_init (env=0x1ba0d00, argv=0x7ffe69b84308, \r\n    argc=2, l=<optimized out>) at dl-init.c:30\r\n#7  _dl_init (main_map=main_map@entry=0x284e6d0, argc=2, argv=0x7ffe69b84308, \r\n    env=0x1ba0d00) at dl-init.c:120\r\n#8  0x00007f2d54ded8e2 in dl_open_worker (a=a@entry=0x7ffe69b7e310)\r\n    at dl-open.c:575\r\n#9  0x00007f2d54de8564 in _dl_catch_error (\r\n    objname=objname@entry=0x7ffe69b7e300, \r\n    errstring=errstring@entry=0x7ffe69b7e308, \r\n    mallocedp=mallocedp@entry=0x7ffe69b7e2ff, \r\n    operate=operate@entry=0x7f2d54ded4d0 <dl_open_worker>, \r\n    args=args@entry=0x7ffe69b7e310) at dl-error.c:187\r\n#10 0x00007f2d54decda9 in _dl_open (\r\n    file=0x7f2d1ce72cc8 \"/home/yaroslav/anaconda3/envs/oct12/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\", \r\n    mode=-2147483646, \r\n    caller_dlopen=0x7f2d54a68553 <_PyImport_FindSharedFuncptr+115>, nsid=-2, \r\n    argc=<optimized out>, argv=<optimized out>, env=0x1ba0d00)\r\n    at dl-open.c:660\r\n#11 0x00007f2d544c3f09 in dlopen_doit (a=a@entry=0x7ffe69b7e540)\r\n    at dlopen.c:66\r\n#12 0x00007f2d54de8564 in _dl_catch_error (objname=0x1b20de0, \r\n    errstring=0x1b20de8, mallocedp=0x1b20dd8, \r\n    operate=0x7f2d544c3eb0 <dlopen_doit>, args=0x7ffe69b7e540)\r\n    at dl-error.c:187\r\n#13 0x00007f2d544c4571 in _dlerror_run (\r\n    operate=operate@entry=0x7f2d544c3eb0 <dlopen_doit>, \r\n    args=args@entry=0x7ffe69b7e540) at dlerror.c:163\r\n#14 0x00007f2d544c3fa1 in __dlopen (file=<optimized out>, \r\n    mode=<optimized out>) at dlopen.c:87\r\n#15 0x00007f2d54a68553 in _PyImport_FindSharedFuncptr (\r\n    prefix=0x7f2d54aeceda \"PyInit\", \r\n    shortname=0x7f2d1c9d04d0 \"_pywrap_tensorflow_internal\", \r\n---Type <return> to continue, or q <return> to quit--- \r\n    pathname=0x7f2d1ce72cc8 \"/home/yaroslav/anaconda3/envs/oct12/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\", fp=0x0)\r\n    at ./Python/dynload_shlib.c:95\r\n#16 0x00007f2d54a43ce7 in _PyImport_LoadDynamicModuleWithSpec (\r\n    spec=0x7f2d1c9b3b00, fp=0x0) at ./Python/importdl.c:124\r\n#17 0x00007f2d54a40aef in _imp_create_dynamic_impl (file=<optimized out>, \r\n    spec=0x7f2d1c9b3b00, module=<optimized out>) at Python/import.c:2031\r\n#18 _imp_create_dynamic (module=<optimized out>, args=<optimized out>)\r\n    at Python/clinic/import.c.h:282\r\n#19 0x00007f2d549a1209 in PyCFunction_Call (func=0x7f2d54f77ee8, \r\n    args=0x7f2d1c9b3c88, kwds=<optimized out>) at Objects/methodobject.c:109\r\n#20 0x00007f2d54a274fa in ext_do_call (nk=479935624, na=0, \r\n    flags=<optimized out>, pp_stack=0x7ffe69b7ea48, func=0x7f2d54f77ee8)\r\n    at Python/ceval.c:5084\r\n#21 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>)\r\n    at Python/ceval.c:3328\r\n#22 0x00007f2d54a2aa49 in _PyEval_EvalCodeWithName (_co=<optimized out>, \r\n    globals=<optimized out>, locals=<optimized out>, args=<optimized out>, \r\n    argcount=2, kws=0x7f2d53040760, kwcount=0, defs=0x0, defcount=0, \r\n    kwdefs=0x0, closure=0x0, name=0x7f2d54f657b0, qualname=0x7f2d54f657b0)\r\n    at Python/ceval.c:4071\r\n#23 0x00007f2d54a2894c in fast_function (nk=<optimized out>, na=2, \r\n    n=<optimized out>, pp_stack=0x7ffe69b7ec68, func=0x7f2d54f830d0)\r\n    at Python/ceval.c:4866\r\n#24 call_function (oparg=<optimized out>, pp_stack=0x7ffe69b7ec68)\r\n    at Python/ceval.c:4783\r\n#25 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>)\r\n    at Python/ceval.c:3289\r\n#26 0x00007f2d54a28ccc in fast_function (nk=<optimized out>, na=2, \r\n    n=<optimized out>, pp_stack=0x7ffe69b7ede8, func=0x7f2d54f2a7b8)\r\n    at Python/ceval.c:4856\r\n#27 call_function (oparg=<optimized out>, pp_stack=0x7ffe69b7ede8)\r\n    at Python/ceval.c:4783\r\n#28 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>)\r\n    at Python/ceval.c:3289\r\n#29 0x00007f2d54a28ccc in fast_function (nk=<optimized out>, na=1, \r\n    n=<optimized out>, pp_stack=0x7ffe69b7ef68, func=0x7f2d54f83b70)\r\n    at Python/ceval.c:4856\r\n#30 call_function (oparg=<optimized out>, pp_stack=0x7ffe69b7ef68)\r\n    at Python/ceval.c:4783\r\n#31 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>)\r\n    at Python/ceval.c:3289\r\n#32 0x00007f2d54a28ccc in fast_function (nk=<optimized out>, na=1, \r\n    n=<optimized out>, pp_stack=0x7ffe69b7f0e8, func=0x7f2d54f83d90)\r\n    at Python/ceval.c:4856\r\n#33 call_function (oparg=<optimized out>, pp_stack=0x7ffe69b7f0e8)\r\n    at Python/ceval.c:4783\r\n#34 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>)\r\n    at Python/ceval.c:3289\r\n#35 0x00007f2d54a28ccc in fast_function (nk=<optimized out>, na=1, \r\n    n=<optimized out>, pp_stack=0x7ffe69b7f268, func=0x7f2d54f83e18)\r\n    at Python/ceval.c:4856\r\n#36 call_function (oparg=<optimized out>, pp_stack=0x7ffe69b7f268)\r\n```", "comments": ["Looks like PyTorch is exporting some pthread symbols via RTLD_GLOBAL. We don't use RTLD_GLOBAL anymore, but we're still exposed to those symbols.\r\n\r\nOther than defensively using RTLD_DEEPBIND in pywrap_tensorflow.py (which does fix the crash for me, but would also break people who use LD_PRELOAD), I guess we just need to avoid using that symbol.", "Sorry for the issue, we'll fix it in the next pytorch release with a solution described here: https://github.com/pytorch/pytorch/issues/3059#issuecomment-335668027", "Thanks @soumith !", "Hi @soumith !\r\n\r\nI believe I am having similar issues. I'm not sure if I should open a new issue or continue here, please let me know!\r\n\r\nMy system is:\r\n\r\n- Ubuntu 16.04\r\n- TensorFlow 1.7.0 (pip)\r\n- PyTorch 0.4.0 (pip)\r\n- CUDA: 9.0.176\r\n- CuDNN: 7.0.5.15\r\n\r\nI get SIGABRT when running my TensorFlow program if I import `torch` before importing `tensorflow`. It doesn't happen if I do it the other way around.\r\n\r\nHow to reproduce the problem:\r\n\r\n1) Download the MNIST example: https://github.com/tensorflow/models/blob/master/tutorials/image/mnist/convolutional.py\r\n\r\n2) Add `import torch` before `import tensorflow as tf`.\r\n\r\n3) Execute. It will receive SIGABRT.\r\n\r\nHere's the backtrace:\r\n\r\n```\r\n(gdb) run convolutional.py \r\nThe program being debugged has been started already.\r\nStart it from the beginning? (y or n) y\r\nStarting program: /usr/bin/python3 convolutional.py\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x7ffff3dc7700 (LWP 217)]\r\n[New Thread 0x7ffff15c6700 (LWP 218)]\r\n[New Thread 0x7fffeedc5700 (LWP 219)]\r\n[New Thread 0x7fffec5c4700 (LWP 220)]\r\n[New Thread 0x7fffe9dc3700 (LWP 221)]\r\n[New Thread 0x7fffe95c2700 (LWP 222)]\r\n[New Thread 0x7fffe6dc1700 (LWP 223)]\r\n[Thread 0x7fffe6dc1700 (LWP 223) exited]\r\n[Thread 0x7fffe95c2700 (LWP 222) exited]\r\n[Thread 0x7fffe9dc3700 (LWP 221) exited]\r\n[Thread 0x7fffec5c4700 (LWP 220) exited]\r\n[Thread 0x7fffeedc5700 (LWP 219) exited]\r\n[Thread 0x7ffff15c6700 (LWP 218) exited]\r\n[Thread 0x7ffff3dc7700 (LWP 217) exited]\r\nExtracting data/train-images-idx3-ubyte.gz\r\nExtracting data/train-labels-idx1-ubyte.gz\r\nExtracting data/t10k-images-idx3-ubyte.gz\r\nExtracting data/t10k-labels-idx1-ubyte.gz\r\n2018-05-02 08:06:25.663691: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n[New Thread 0x7fffe6dc1700 (LWP 227)]\r\n[New Thread 0x7fffe95c2700 (LWP 228)]\r\n[New Thread 0x7fffe9dc3700 (LWP 229)]\r\n[New Thread 0x7fffec5c4700 (LWP 230)]\r\n[New Thread 0x7fff493ff700 (LWP 231)]\r\n[New Thread 0x7fff48bfe700 (LWP 232)]\r\n[New Thread 0x7fff483fd700 (LWP 233)]\r\n[New Thread 0x7fff47bfc700 (LWP 234)]\r\n[New Thread 0x7fff45400700 (LWP 235)]\r\n[New Thread 0x7fff44bff700 (LWP 236)]\r\n[New Thread 0x7fff3ffff700 (LWP 237)]\r\n[New Thread 0x7fff3f7fe700 (LWP 238)]\r\n2018-05-02 08:06:25.755908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-05-02 08:06:25.756207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.94GiB freeMemory: 3.26GiB\r\n2018-05-02 08:06:25.756248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-05-02 08:06:26.002388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-02 08:06:26.002425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 \r\n2018-05-02 08:06:26.002434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N \r\n2018-05-02 08:06:26.002678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2966 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n[New Thread 0x7ffefdbb8700 (LWP 239)]\r\n[New Thread 0x7ffef53b7700 (LWP 240)]\r\n[New Thread 0x7ffefd3b7700 (LWP 241)]\r\n[New Thread 0x7ffefcbb6700 (LWP 242)]\r\n[New Thread 0x7ffef7fff700 (LWP 243)]\r\n[New Thread 0x7ffef77fe700 (LWP 244)]\r\n[New Thread 0x7ffef6ffd700 (LWP 245)]\r\n[New Thread 0x7ffef67fc700 (LWP 246)]\r\n[New Thread 0x7ffef5ffb700 (LWP 247)]\r\n[New Thread 0x7ffef4bb6700 (LWP 248)]\r\nInitialized!\r\n2018-05-02 08:06:28.027051: E tensorflow/stream_executor/cuda/cuda_blas.cc:462] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2018-05-02 08:06:28.031781: E tensorflow/stream_executor/cuda/cuda_blas.cc:462] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2018-05-02 08:06:28.035610: E tensorflow/stream_executor/cuda/cuda_blas.cc:462] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2018-05-02 08:06:28.039622: E tensorflow/stream_executor/cuda/cuda_blas.cc:462] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2018-05-02 08:06:28.043484: E tensorflow/stream_executor/cuda/cuda_blas.cc:462] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2018-05-02 08:06:28.046642: E tensorflow/stream_executor/cuda/cuda_blas.cc:462] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2018-05-02 08:06:28.054981: E tensorflow/stream_executor/cuda/cuda_dnn.cc:403] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2018-05-02 08:06:28.055044: F tensorflow/core/kernels/conv_ops.cc:712] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \r\n\r\nThread 30 \"python3\" received signal SIGABRT, Aborted.\r\n[Switching to Thread 0x7ffef4bb6700 (LWP 248)]\r\n0x00007ffff7825428 in raise () from /lib/x86_64-linux-gnu/libc.so.6\r\n(gdb) bt\r\n#0  0x00007ffff7825428 in raise () from /lib/x86_64-linux-gnu/libc.so.6\r\n#1  0x00007ffff782702a in abort () from /lib/x86_64-linux-gnu/libc.so.6\r\n#2  0x00007fff849b8c34 in tensorflow::internal::LogMessageFatal::~LogMessageFatal() () from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fff846b166a in tensorflow::LaunchConv2DOp<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, bool, tensorflow::Tensor const&, tensorflow::Tensor const&, int, int, int, int, tensorflow::Padding const&, tensorflow::Tensor*, tensorflow::TensorFormat) () from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fff846b3ce3 in tensorflow::Conv2DOp<Eigen::GpuDevice, float>::Compute(tensorflow::OpKernelContext*) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fff7fc23289 in tensorflow::BaseGPUDevice::ComputeHelper(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#6  0x00007fff7fc23750 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#7  0x00007fff7fc5d365 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#8  0x00007fff7fc5db7a in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8> const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#9  0x00007fff7f8ce8ba in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#10 0x00007fff7f8cd962 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#11 0x00007fffee8e6c80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#12 0x00007ffff7bc16ba in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#13 0x00007ffff78f741d in clone () from /lib/x86_64-linux-gnu/libc.so.6\r\n```\r\n\r\n\r\nThanks!", "@carlosgalvezp this might be expected. TF is basically resolving some of the pytorch's statically linked cudnn symbols and some of the system libcudnn.so symbols, which are probably of different versions. We'll try to fix this next release."]}, {"number": 13614, "title": "SSD mobilenet trained model with custom data only recognize images in short distances", "body": " System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n     No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n     Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\n     binary\r\n- **TensorFlow version (use command below)**:\r\n     'v1.2.0-rc2-21-g12f033d', '1.2.0'\r\n- **Python version**: \r\n     2.7\r\n- **Bazel version (if compiling from source)**:\r\n     na\r\n- **CUDA/cuDNN version**:\r\n     CUDA Version 8.0.61\r\n     \r\n- **GPU model and memory**:\r\nname: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.683\r\npciBusID 0000:01:00.0\r\nTotal memory: 10.91GiB\r\nFree memory: 10.75GiB\r\n\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nI've trained a model with a custom dataset (Garfield images) with Tensorflow Object Detection API (ssd_mobilenet_v1 model) and referring it in the android sample application available on Tensorflow repository. The application can only detected the images in distances less or equal 20cm approximately.\r\n\r\nDo you have any clue about I can improve the model to perform recognitions in longer distances (about 30cm or more) ?\r\n\r\nI don't know with this limitation is related with input size I'm using (tested with images with 300x300 and 68x68) or any custom data augmentation is needed to improve that.\r\n\r\n\r\n### Source code / logs\r\n# SSD with Mobilenet v1, configured for Oxford-IIIT Pets Dataset.\r\n# Users should configure the fine_tune_checkpoint field in the train config as\r\n# well as the label_map_path and input_path fields in the train_input_reader and\r\n# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\r\n# should be configured.\r\n\r\nmodel {\r\n  ssd {\r\n    num_classes: 1\r\n    box_coder {\r\n      faster_rcnn_box_coder {\r\n        y_scale: 10.0\r\n        x_scale: 10.0\r\n        height_scale: 5.0\r\n        width_scale: 5.0\r\n      }\r\n    }\r\n    matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.5\r\n        unmatched_threshold: 0.5\r\n        ignore_thresholds: false\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n      }\r\n    }\r\n    similarity_calculator {\r\n      iou_similarity {\r\n      }\r\n    }\r\n    anchor_generator {\r\n      ssd_anchor_generator {\r\n        num_layers: 6\r\n        min_scale: 0.2\r\n        max_scale: 0.95\r\n        aspect_ratios: 1.0\r\n        aspect_ratios: 2.0\r\n        aspect_ratios: 0.5\r\n        aspect_ratios: 3.0\r\n        aspect_ratios: 0.3333\r\n      }\r\n    }\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 68\r\n        width: 68\r\n      }\r\n    }\r\n    box_predictor {\r\n      convolutional_box_predictor {\r\n        min_depth: 0\r\n        max_depth: 0\r\n        num_layers_before_predictor: 0\r\n        use_dropout: false\r\n        dropout_keep_probability: 0.8\r\n        kernel_size: 1\r\n        box_code_size: 4\r\n        apply_sigmoid_to_scores: false\r\n        conv_hyperparams {\r\n          activation: RELU_6,\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.00004\r\n            }\r\n          }\r\n          initializer {\r\n            truncated_normal_initializer {\r\n              stddev: 0.03\r\n              mean: 0.0\r\n            }\r\n          }\r\n          batch_norm {\r\n            train: true,\r\n            scale: true,\r\n            center: true,\r\n            decay: 0.9997,\r\n            epsilon: 0.001,\r\n          }\r\n        }\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'ssd_mobilenet_v1'\r\n      min_depth: 16\r\n      depth_multiplier: 1.0\r\n      conv_hyperparams {\r\n        activation: RELU_6,\r\n        regularizer {\r\n          l2_regularizer {\r\n            weight: 0.00004\r\n          }\r\n        }\r\n        initializer {\r\n          truncated_normal_initializer {\r\n            stddev: 0.03\r\n            mean: 0.0\r\n          }\r\n        }\r\n        batch_norm {\r\n          train: true,\r\n          scale: true,\r\n          center: true,\r\n          decay: 0.9997,\r\n          epsilon: 0.001,\r\n        }\r\n      }\r\n    }\r\n    loss {\r\n      classification_loss {\r\n        weighted_sigmoid {\r\n          anchorwise_output: true\r\n        }\r\n      }\r\n      localization_loss {\r\n        weighted_smooth_l1 {\r\n          anchorwise_output: true\r\n        }\r\n      }\r\n      hard_example_miner {\r\n        num_hard_examples: 3000\r\n        iou_threshold: 0.99\r\n        loss_type: CLASSIFICATION\r\n        max_negatives_per_positive: 3\r\n        min_negatives_per_image: 0\r\n      }\r\n      classification_weight: 1.0\r\n      localization_weight: 1.0\r\n    }\r\n    normalize_loss_by_num_matches: true\r\n    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 1e-8\r\n        iou_threshold: 0.6\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n  }\r\n}\t\r\ntrain_config: {\r\n  batch_size: 24\r\n  optimizer {\r\n    rms_prop_optimizer: {\r\n      learning_rate: {\r\n        exponential_decay_learning_rate {\r\n          initial_learning_rate: 0.004\r\n          decay_steps: 800720\r\n          decay_factor: 0.95\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.9\r\n      decay: 0.9\r\n      epsilon: 1.0\r\n    }\r\n  }\r\n  #fine_tune_checkpoint: \"/home/oliveira/tf_oda/checkpoints/ssd_mobilenet_v1_coco_11_06_2017/model.ckpt\"\r\n  from_detection_checkpoint: true\r\n  # Note: The below line limits the training process to 200K steps, which we\r\n  # empirically found to be sufficient enough to train the pets dataset. This\r\n  # effectively bypasses the learning rate schedule (the learning rate will\r\n  # never decay). Remove the below line to train indefinitely.\r\n  num_steps: 200000\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n  data_augmentation_options {\r\n    ssd_random_crop {\r\n    }\r\n  }\r\n}\r\n\r\ntrain_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"/data/tf_oda/garfield/dataset/pascal_train_garfield_68.record\"\r\n  }\r\n  label_map_path: \"/data/tf_oda/garfield/data/pascal_label_map_garfield.pbtxt\"\r\n}\r\n\r\neval_config: {\r\n  num_examples: 2000\r\n  # Note: The below line limits the evaluation process to 10 evaluations.\r\n  # Remove the below line to evaluate indefinitely.\r\n  max_evals: 10\r\n}\r\n\r\neval_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"/data/tf_oda/garfield/dataset/pascal_val_garfield_68.record\"\r\n  }\r\n  label_map_path: \"/data/tf_oda/garfield/data/pascal_label_map_garfield.pbtxt\"\r\n  shuffle: false\r\n  num_readers: 1\r\n}", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13613, "title": "Branch 171718021", "body": "", "comments": ["cc @alextp @case540 \r\nhttps://github.com/tensorflow/tensorflow/pull/13606 had a build issue, this PR should fix it."]}, {"number": 13612, "title": "Added support for Python3 Raspberry Pi CI builds", "body": "Updates the Docker and install scripts to use Python 3.4 in the cross-compilation toolchain, so we can set up nightly CI builds for Python3 wheels for the Raspberry Pi.", "comments": ["Looks like the sanity build is failing:\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/8928/", "The sanity build failures are related to the Skylark gymnastics I had to do to get the right Python3 includes in the CROSSTOOL template. I'm reverting those changes and trying something simpler now.", "Jenkins, test this please.", "Unrelated test failure: //tensorflow/python/kernel_tests:linalg_ops_test test failure (https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/7296/console)", "@petewarden Mind rebase and squash?", "@caisq could you file a bug to track linalg_ops_test failure?"]}, {"number": 13611, "title": "Configurable upper bound for MKL CPU allocator", "body": "Multiple ways to configure upper bound on the MKL cpu allocator\r\n- Default is 64 GB\r\n- Overridden by DRAM capacity, if available\r\n- Overridden by user configured limit if set through an environment variable.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Looks like there is something wrong with the commit author emails.\r\nCould you fix that?", "CLAs look good, thanks!\n\n<!-- ok -->", "@rmlarsen made the requested changes. can you take a look?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Thanks for the fixes @jbobba. Looks good to go, pending testing.", "Do we need this change also in RC1?\r\n@jbobba looks like not all the commits in the pull requests have the emails set up correctly, thus failing the CLA check.\r\nThe easiest way would be is to squash all of them and push them onto your branch.\r\nOr you need to fix their author information individually.", "@gunan I don't think this is critical to get in RC1. It wouldn't hurt.", "@gunan with regards to CLA, I did squash my first commits with the incorrect email and pushed new ones with the right email. I see a green cla label on the PR. Is this still an issue?", "Unfortunately, until the cla/google check on the PR is happy, it is potentially still a problem.\r\nJust to check, To retrigger CLA bot, could you copy paste this comment:\r\n`I signed it!`", "I signed it!", "Looks like the commit 8ad8b00 is the one not approved.\r\nCould you check if its authorship information matches the other commits?\r\n", "@gunan @rmlarsen created a new PR to get around the CLA issue. https://github.com/tensorflow/tensorflow/pull/13674.\r\n\r\nCan we move to that PR? I will close this one.", "@jbobba @gunan I approved the new PR."]}, {"number": 13610, "title": "Can we run Dataset API on GPU?", "body": "I am running binary TF 1.3.0 on Ubuntu 16.04.\r\n\r\nPython Version: 3.5.3\r\n\r\nI have Nvidia TITAN Xp installed.\r\n\r\nI use Dataset API to build input pipeline. What I want is to extract image features using CNN layers \r\nin the input pipeline. The code looks like this:\r\n\r\n```\r\n    def extract_feats(image):\r\n      with tf.device(\"/gpu:0\"):\r\n        _, end_points = vgg.vgg_16(tf.expand_dims(image, 0),\r\n                                   is_training=(mode == ModeKeys.TRAIN),\r\n                                   spatial_squeeze=False)\r\n        final_conv_layer = end_points['vgg_16/conv5/conv5_3']\r\n        feats = spatial_pyramid_pooling(final_conv_layer, [bin_size], mode='avg')\r\n      return tf.reshape(feats, shape=(bin_size * bin_size, tf.shape(final_conv_layer)[-1]))\r\n\r\n    features = features.map(extract_feats)\r\n```\r\n\r\nWhen running the code, my CPU usage is more than 1000% (I have an 6 cores/12 threads CPU), while the GPU usage is 0%. I suspect that the input pipeline built from Dataset API are forced to run on CPU. I tried to set `log_device_placement=True` and I can see that the operation is placed on GPU.\r\n\r\nSince I want to extract vectors with same length from variable-sized images using SPP pooling, I have to process these images one by one using `Dataset.map` before calling `Dataset.batch`. So I hope the operations inside `Dataset` could be run on GPU.\r\n", "comments": ["As you gathered, currently datasets execute all the computation on CPU.\r\n\r\n@rohan100jain is in the process of adding some functionality to be able to invoke TensorFlow functions on different devices, which will be a pre-requisite but not sufficient for executing parts of the computation in a dataset pipeline on GPU.\r\n\r\nSo, long story short: At this moment there is no concrete timeline for executing the `map` function of a dataset on GPU.\r\n\r\nFYI @mrry ", "Somewhat related question: I am curious how to effectively move data from the CPU to multiple GPU's using the Dataset API.\r\nDo we need a dataset and iterator per GPU? Or can a single iterator provide unique data to each tower of the GPU? I have a more detailed question with an example code snippet here at stackoverflow: https://stackoverflow.com/questions/46965098/how-does-one-move-data-to-multiple-gpu-towers-using-tensorflows-dataset-api", "@nirmalthacker  I posted an answer to your question [on Stack Overflow](https://stackoverflow.com/a/46966248/3574081).\r\n\r\n@tongda General cross-device pipelines are still some way off, but @rohan100jain has developed some nice support for staging data automatically to GPU memory, which covers one of the big use cases. In principle you could reuse some of the support for dispatching TensorFlow functions to remote devices in a host-side `Dataset.map()`, but the partitioning between devices would currently be manual.", "@mrry, Thanks! I've responded on Stackoverflow as well. In short, I'm trying to determine an approach with Datasets and Iterators that can provide the kind of throughput I'm seeing with the queue-based approach.", "I hope it's not too off-topic:\r\nIs there a possibility to use efficient GPU-Preloading with the image_tensor placeholder in a object detection frozen graph? \r\n\r\n(I'm searching since two days, and think that datasets might be a good answer, however I'm afraid that the feed_dict parameter to set data for placeholders does not support any preloaded data.)\r\n\r\n(I'm really sorry If I highjacked this thread - If this question is worth Stack Overflow I'll post it there)", "I have noticed that GPU-utilization is lower when I use Dataset API. Is it possible to reach 80-90[%] utilization with this API or does native python threads perform worse than queues?", "@agniszczotka It\u2019s definitely possible to reach higher utilization, so it would be good to understand what the problem is here. One possibility is that the `Dataset` pipeline can prefetch and preprocess data more aggressively than the older API, which in some configurations can lead to oversubscription of the CPU, and thereby delay the dispatch of work to the GPU. Please open a new issue with a description of the program you\u2019re running, and we\u2019ll take a look. ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "No new status to report, but this is an area that we'll start to focus on in the new year.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@mrry, Hi, I am recently building a multi-gpu model, does dataset API still do not support GPU well? I have been thinking if I should use dataset API or a queue-based approach, thanks!", "@Lancerchaing Both the (current version of the) `tf.data` API and the old queue-based approach place the entire input pipeline on the CPU, and the GPUs are exercised by parts of the graph that come after the input pipeline. Since the overheads associated with the `tf.data` implementation are smaller, I'd recommend using it over the queue-based approach. Take a look at the [TF benchmarks](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py) for an example of how to use `tf.data` efficiently in a multi-GPU setup.\r\n\r\nWe still plan to add support for running stages of a `tf.data` pipeline on GPU, as well as easier-to-use support for prefetching to GPU memory (currently implemented [here](https://github.com/tensorflow/tensorflow/blob/926fc13f7378d14fa7980963c4fe774e5922e336/tensorflow/contrib/data/python/ops/prefetching_ops.py#L30), but not yet exposed via the API), which should help to achieve even better performance in future.", "@mrry Hi thanks for the reply, I have looked into this benchmarking before and the other one here [https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10_estimator/cifar10_main.py](url) \r\nI am pretty new to tensorflow, and I need to change our existing project to a multi-gpu mode (instead of starting from scratch). Do you think below is a right approach with `tf.data` API ?\r\n\r\n\r\n```\r\ndataset = tf.data.TextLineDataset(filenames)\r\ndataset = dataset.map(...) #preprocessing data\r\ndataset = dataset.batch(batch_size)\r\niterator = dataset.make_initializable_iterator()\r\n\r\nnum_batches = len(filenames) // batch_size\r\npredictions = []\r\n\r\nfor i in range(num_batches):\r\n    batch = tf.split(dataset.next(), num_gpus) #splitting batch into smaller batches for each GPU\r\n    for i in range(num_gpus):\r\n        with tf.device('/gpu:%d' % i):\r\n            with tf.name_scope('%s_%d' % (Tower_name, i)) as scope:\r\n                predictions.append(inference(batch[i]))\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(predictions)\r\n```\r\n\r\nFurthermore, do I need a runner like queue_runner in dataset API to facilitate the input pipeline \uff08I hope I can to preprocess the next batch of image while GPU is doing computation\uff09? \r\nThanks :)", "I'm looking forward to GPU prefetching as currently using StagingArea to manually pipeline data gets a little messy.\r\n\r\nFor the single-machine use-case I'm looking at using a feedable iterator to switch between training data and test data - is there any chance this could be supported on GPU so that it could be combined with prefetching? Trying to share the same prefetch pipeline between two data sources, while possible, would require very careful coordination to ensure the right data comes out at the right time. It would also be difficult to implement with existing high-level API's.\r\n\r\nIn crude diagram form, this is the approach I have in mind:\r\n```\r\n    CPU                                    | GPU\r\n    train_data -> map -> prefetch<send> -> | -> prefetch<recv> -> string_handle\\\r\n                                           |                                     -> feedable_iterator\r\n    test_data  -> map -> prefetch<send> -> | -> prefetch<recv> -> string_handle/\r\n```\r\n\r\nedit: this is what \"sharing the same prefetch pipeline\" would look like using the same diagram convention:\r\n```\r\n        CPU                                                                      | GPU\r\n    train_data -> map -> string_handle\\                                          | \r\n                                       -> feedable_iterator -> prefetch<send> -> | -> prefetch<recv>\r\n    test_data  -> map -> string_handle/                                          |\r\n```", "@tongda  how did you manage that problem in the end ? Are you really running the VGG on the CPU for preprocessing ?", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "`dataset.` `prefetch_to_device` now in 1.7; [source](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/python/ops/prefetching_ops.py).", "@TimZaman It cannot be used with initializable iterators which limits its usability. Isn't it ?", "@calledbymountains I saw a commit to master that enables prefetch_to_device with initialisable iterators. I also think prefetch_to_device is in master, but not 1.7", "@mrry Is there any way to use `tf.contrib.data.prefetch_to_device` for mutl-gpu training? Thanks!", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Reassigning this to @rohan100jain, since he is currently looking into improvements to `prefetch_to_device()`, including for multi-GPU training and for chaining additional computation on the GPU.", "Nagging Assignee @rohan100jain: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 61 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi all,\r\n\r\nPlease take a look at MultiDeviceIterator https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/python/ops/prefetching_ops.py#L628\r\n\r\nCurrently its exposed via tf.contrib.data but this solves the prefetching to multiple GPU devices use case.\r\n\r\nSample API usage:\r\n\r\nhost_dataset_on_cpu = .....\r\nmulti_device_iterator = prefetching_ops.MultiDeviceIterator(host_dataset_on_cpu, devices=['/gpu:0', '/gpu:1'])\r\nelem_on_gpu_0, elem_on_gpu_1 = multi_device_iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(multi_device_iterator.initializer)\r\n  elem_0 = sess.run(elem_on_gpu_0)\r\n  elem_1 = sess.run(elem_on_gpu_1)\r\n\r\nMultiDeviceIterator deterministically prefetches elements to the different GPU's i.e. elements % 2 == 0 will go to GPU:0 and elements % 2 == 1 will go to GPU:1.\r\n\r\nClosing this issue for now but feel free to open new ones if you see issues.", "@rohan100jain that looks like a very powerful new iterator! I'm not sure if it's exactly addressing everything discussed in this issue. I think the OP was essentially asking whether they could add more dataset ops after prefetch_to_device.\r\n\r\nNow that all the docs make it fairly clear the Dataset API has been designed as an ETL process I would hazard a guess that you aren't planning to change that architecture to allow further \"T\" steps after the \"L\". Now that I look at it again, the OP's example doesn't really require Dataset.map anyway as they are all ops that can be run on GPU after the input pipeline has loaded the data onto there.\r\n\r\nAlso, I haven't tried it yet but the changes to `tf.estimator.train_and_evaluate` in the latest release sound like they do switching between train and test datasets on a single graph like I wanted to do.", "@rohan100jain Thanks for bringing this new iterator. Can I use it with Estimator? ", "@rohan100jain does `to_device` support operations after.  This would be great to have since image preprocessing can be greatly accelerated on gpu.\r\n"]}, {"number": 13609, "title": "Disable iterator_ops_test on Windows for 1.4 release", "body": "testRemoteIteratorUsingRemoteCallOpDirectSessionGPUCPU test is failing\r\nwith \"TypeError: only integer scalar arrays can be converted to a scalar\r\nindex\" on the Windows GPU Release bot. Disabling test.", "comments": ["Please also file a bug internally to track a fix for this."]}, {"number": 13608, "title": "Batch normalization dtype depends on inputs dtype", "body": "Make the batch normalization layer dtype be the same type as the inputs,\r\nbefore it was always float32", "comments": ["Can one of the admins verify this patch?", "ping @fchollet ", "Thank you for your patience.\r\n\r\nWeight `dtype` is already inferred from inputs dtype (unless explicitly specified in the layer constructor) in the base layer's `__call__`, and thus this PR is not warranted. See: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/base.py#L598\r\n\r\nAs a general feedback point for future PRs: when submitting PRs for such fixes, please include unit tests (a test that would fail at `master` but would pass with the proposed fix)."]}, {"number": 13606, "title": "Branch 171672655", "body": "", "comments": ["Closing in favor of https://github.com/tensorflow/tensorflow/pull/13613"]}, {"number": 13605, "title": "Fix typos", "body": "This PR fixes some typos: `overriden`, `explictly`, `corresonding`, `accross`, and `contructor`.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13604, "title": "Keras + tfdbg error: Dump root directory does not exist", "body": "# Problem\r\nI'm running the tf debugger and specifically am looking for nans and infs.\r\nI'm doing this from keras using the tf backend by setting the keras sess to tf.Session wrapped with the debugger. \r\n\r\n## MVCE\r\n```\r\n    import tensorflow as tf\r\n    from keras import Input\r\n    from keras import backend as keras_backend\r\n\r\n    inputs = Input((1,1))\r\n    nan_ = keras_backend.log(inputs * 0)\r\n    model = Model(inputs,nan_)\r\n    model.compile(loss='mse',optimizer='sgd')\r\n   \r\n    from tensorflow.python import debug as tf_debug\r\n    sess = tf_debug.LocalCLIDebugWrapperSession(tf.Session())\r\n    #Add filter for nans and infs\r\n    sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\r\n    keras_backend.set_session(sess)\r\n\r\n    import numpy as np\r\n    model.fit(np.zeros([1,1,1]), np.zeros([1,1,1])) \r\n```\r\nInside the debugger execute `run -f has_inf_or_nan`.\r\n\r\n## Traceback \r\n\r\nOSError: Dump root directory /tmp/tfdbg_dm3xvee9 does not exist. \r\n\r\n## Additional info\r\n\r\n#7615 references the same error a while ago, and gives similar traceback.\r\n\r\n## System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\nNo.\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\nLinux Fedora 4.11.5-200.fc25.x86_64\r\n\r\nTensorFlow installed from (source or binary): \r\nbinary\r\n\r\nTensorFlow version (use command below): \r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n\r\nPython version:\r\n3.6\r\n\r\nCUDA/cuDNN version:\r\nCuda 8.0, cuDNN v6\r\n\r\nGPU model and memory:\r\nQuadro M2000M, 4042MiB\r\n\r\n\r\n", "comments": ["Can you please follow [template](https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md) so that can have more information?", "I'm also getting this error on TF GPU 1.4.0, Python 3.6.3 x64 on Windows 10 x64", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@Joshuaalbert Your code snippet gave the error: \r\n\r\n> TypeError: Output tensors to a Model must be Keras tensors. Found: Tensor(\"Log:0\", shape=(?, 1, 1), dtype=float32)\r\n\r\nSo I slightly modified the code, by replacing keras_backend.log with a mundane Dense layer and tfdbg seems to work fine.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom keras import Input\r\nfrom keras import backend as keras_backend\r\nfrom keras.models import Model\r\nfrom keras.layers import Dense\r\n\r\ninputs = Input((1,1))\r\noutput = Dense(1)(inputs)\r\nmodel = Model(inputs, [output])\r\nmodel.compile(loss='mse', optimizer='sgd')\r\n\r\nfrom tensorflow.python import debug as tf_debug\r\nsess = tf_debug.LocalCLIDebugWrapperSession(tf.Session())\r\n#Add filter for nans and infs\r\nsess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\r\nkeras_backend.set_session(sess)\r\n\r\nimport numpy as np\r\nmodel.fit(np.zeros([1,1,1]), np.zeros([1,1,1]))\r\n```\r\n\r\nMy tensorflow version:  1.5.0-dev20171229 (I tried tensorflow 1.3.0 as well and it seems to work properly, too)\r\nMy keras version: 2.1.2.\r\n", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "No reply from @Joshuaalbert for 2+ weeks. I'm closing this PR. If someone experiences this issue again, feel free to re-open it."]}, {"number": 13603, "title": "SVD on GPU is slower than SVD on CPU", "body": "OS:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS release 7.4.1708\r\n- **TensorFlow installed from (source or binary)**: From source\r\n- **Python version**: 2.7.13\r\n- **Bazel version**: 0.6.1\r\n- **CUDA/cuDNN version**: CUDA 8.0/cuDNN 6.0.21\r\n- **GPU model and memory**: GeForce GTX 950M, memory 4GB\r\n\r\noutput of `tf_env_collect.sh`\r\n```\r\n\r\n== cat /etc/issue ===============================================\r\nLinux zhanghao 3.10.0-693.2.2.el7.x86_64 #1 SMP Tue Sep 12 22:26:13 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7 (Core)\"\r\nVERSION_ID=\"7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\r\nCopyright \u00a9 2015 Free Software Foundation, Inc.\r\n\u672c\u7a0b\u5e8f\u662f\u81ea\u7531\u8f6f\u4ef6\uff1b\u8bf7\u53c2\u770b\u6e90\u4ee3\u7801\u7684\u7248\u6743\u58f0\u660e\u3002\u672c\u8f6f\u4ef6\u6ca1\u6709\u4efb\u4f55\u62c5\u4fdd\uff1b\r\n\u5305\u62ec\u6ca1\u6709\u9002\u9500\u6027\u548c\u67d0\u4e00\u4e13\u7528\u76ee\u7684\u4e0b\u7684\u9002\u7528\u6027\u62c5\u4fdd\u3002\r\n\r\n== uname -a =====================================================\r\nLinux zhanghao 3.10.0-693.2.2.el7.x86_64 #1 SMP Tue Sep 12 22:26:13 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named tensorflow\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue Oct 10 16:36:08 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 950M    Off  | 00000000:0A:00.0 Off |                  N/A |\r\n| N/A   45C    P0    N/A /  N/A |      0MiB /  4044MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n\r\n== cat /etc/issue ===============================================\r\nLinux zhanghao 3.10.0-693.2.2.el7.x86_64 #1 SMP Tue Sep 12 22:26:13 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7 (Core)\"\r\nVERSION_ID=\"7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\r\nCopyright \u00a9 2015 Free Software Foundation, Inc.\r\n\u672c\u7a0b\u5e8f\u662f\u81ea\u7531\u8f6f\u4ef6\uff1b\u8bf7\u53c2\u770b\u6e90\u4ee3\u7801\u7684\u7248\u6743\u58f0\u660e\u3002\u672c\u8f6f\u4ef6\u6ca1\u6709\u4efb\u4f55\u62c5\u4fdd\uff1b\r\n\u5305\u62ec\u6ca1\u6709\u9002\u9500\u6027\u548c\u67d0\u4e00\u4e13\u7528\u76ee\u7684\u4e0b\u7684\u9002\u7528\u6027\u62c5\u4fdd\u3002\r\n\r\n== uname -a =====================================================\r\nLinux zhanghao 3.10.0-693.2.2.el7.x86_64 #1 SMP Tue Sep 12 22:26:13 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.1)\r\nprotobuf (3.4.0)\r\ntensorflow (1.4.0rc0)\r\ntensorflow-tensorboard (0.4.0rc1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.4.0-rc0\r\ntf.GIT_VERSION = v1.3.0-rc1-3111-g4196d6d\r\ntf.COMPILER_VERSION = v1.3.0-rc1-3111-g4196d6d\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/lib64/:/usr/local/cuda/lib64/stubs/:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/cuda/nvvm/lib64/:/usr/lib64/nvidia/:/opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64:/opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2017.4.196/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2017.4.196/linux/mpi/mic/lib:/opt/intel/compilers_and_libraries_2017.4.196/linux/ipp/lib/intel64:/opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2017.4.196/linux/mkl/lib/intel64_lin:/opt/intel/compilers_and_libraries_2017.4.196/linux/tbb/lib/intel64/gcc4.7:/opt/intel/debugger_2017/iga/lib:/opt/intel/debugger_2017/libipt/intel64/lib:/opt/intel/compilers_and_libraries_2017.4.196/linux/daal/lib/intel64_lin:/opt/intel/compilers_and_libraries_2017.4.196/linux/daal/../tbb/lib/intel64_lin/gcc4.4\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue Oct 10 16:36:37 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 950M    Off  | 00000000:0A:00.0 Off |                  N/A |\r\n| N/A   45C    P0    N/A /  N/A |      0MiB /  4044MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n```\r\n\r\noutput of `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n```\r\n('v1.3.0-rc1-3111-g4196d6d', '1.4.0-rc0')\r\n```\r\n\r\n### Describe the problem\r\n\r\nSVD on GPU is slower than SVD on CPU\r\n\r\n### Source code / logs\r\n\r\nfile main.py\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sys\r\n\r\nD = 1024\r\ndA = np.random.normal(size=(D,D))\r\n\r\ndev = \"/gpu:0\" if len(sys.argv)==1 else \"/cpu:0\"\r\n\r\nwith tf.device(dev):\r\n    A = tf.placeholder(shape=(D,D),dtype=tf.float32)\r\n    S, U, V = tf.svd(A)\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.log_device_placement = True\r\nconfig.graph_options.optimizer_options.global_jit_level=tf.OptimizerOptions.ON_1\r\nsess = tf.Session(config=config)\r\n\r\nfor _ in xrange(10):\r\n    dS, dU, dV = sess.run((S, U, V), feed_dict={A:dA})\r\n```\r\n\r\n## run on GPU\r\n`time python main.py`\r\n```\r\n2017-10-10 16:28:49.047703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-10-10 16:28:49.048176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce GTX 950M major: 5 minor: 0 memoryClockRate(GHz): 1.124\r\npciBusID: 0000:0a:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.91GiB\r\n2017-10-10 16:28:49.048205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0\r\n2017-10-10 16:28:49.064960: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0\r\n\r\nSvd: (Svd): /job:localhost/replica:0/task:0/device:GPU:0\r\n2017-10-10 16:28:49.067234: I tensorflow/core/common_runtime/placer.cc:874] Svd: (Svd)/job:localhost/replica:0/task:0/device:GPU:0\r\nPlaceholder: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\r\n2017-10-10 16:28:49.067302: I tensorflow/core/common_runtime/placer.cc:874] Placeholder: (Placeholder)/job:localhost/replica:0/task:0/device:GPU:0\r\n2017-10-10 16:28:49.074053: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x488e860\r\npython main.py  27.50s user 2.30s system 100% cpu 29.658 total\r\n```\r\n\r\n## run on CPU\r\n`time python main.py -`\r\n```\r\n2017-10-10 16:29:53.252138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-10-10 16:29:53.252572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce GTX 950M major: 5 minor: 0 memoryClockRate(GHz): 1.124\r\npciBusID: 0000:0a:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.91GiB\r\n2017-10-10 16:29:53.252600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0\r\n2017-10-10 16:29:53.269242: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0\r\n\r\nSvd: (Svd): /job:localhost/replica:0/task:0/device:CPU:0\r\n2017-10-10 16:29:53.271505: I tensorflow/core/common_runtime/placer.cc:874] Svd: (Svd)/job:localhost/replica:0/task:0/device:CPU:0\r\nPlaceholder: (Placeholder): /job:localhost/replica:0/task:0/device:CPU:0\r\n2017-10-10 16:29:53.271544: I tensorflow/core/common_runtime/placer.cc:874] Placeholder: (Placeholder)/job:localhost/replica:0/task:0/device:CPU:0\r\npython main.py -  34.33s user 10.68s system 621% cpu 7.241 total\r\n```", "comments": ["It seems during some time of GPU SVD, there is frequent data moving between Host and Device. I don't know whether it is the reason why GPU SVD is slow.", "Each computation of the SVD on the GPU requires one memcpy from Device to the Host: the status report if the algorithm was successful needs to be copied from the GPU to the CPU to be checked.\r\nThis can in principle be a bottleneck, but can't be avoided.\r\n\r\nBut for me, the logs indicate that the GPU version is indeed faster than the CPU if you compare the timings. See my comment in the pull request for more details.", "As another clear sample:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.client import timeline\r\n\r\noptions = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\nrun_metadata = tf.RunMetadata()\r\nsess = tf.Session()\r\n\r\nD = 1024\r\ndA = np.random.normal(size=(D,D))\r\nA = tf.placeholder(shape=(D,D),dtype=tf.float32)\r\n\r\nwith tf.device(\"/cpu:0\"):\r\n    C = tf.svd(A)\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    G = tf.svd(A)\r\n\r\n_ = sess.run(C+G, feed_dict={A:dA}, options=options, run_metadata=run_metadata)\r\n\r\nfetched_timeline = timeline.Timeline(run_metadata.step_stats)\r\nchrome_trace = fetched_timeline.generate_chrome_trace_format()\r\nwith open('timeline.json', 'w') as f:\r\n    f.write(chrome_trace)\r\n```\r\n\r\nrun it and get `timeline.json`, and open it via chrome then get it:\r\n\r\n![image](https://user-images.githubusercontent.com/11623447/31384419-8d85faae-ad84-11e7-91a2-7311bd228db8.png)\r\n\r\nthose memcpy are obviously called by SVD. there is nothing else in the program.\r\n\r\nThen zoom in, we can find there is many small memcpy here\r\n\r\n![image](https://user-images.githubusercontent.com/11623447/31384472-c5672c68-ad84-11e7-943f-e471cd2cf1ca.png)\r\n", "@hzhangxyz your benchmark is confusing because you have both CPU and GPU SVD in a single session run. It's better to isolate to a single op if the goal is to show that op kernel is slow.\r\n\r\nI have a benchmark in https://github.com/tensorflow/tensorflow/issues/13222#issuecomment-331642490 which isolates to just GPU SVD and rules out memory transfers.\r\n\r\nIn that example (n=1534, float32), TF CPU runs about 4.6 slower than corresponding version in MKL-enabled numpy, TF GPU version runs about  21x slower.\r\n\r\nin commit: https://github.com/tensorflow/tensorflow/commit/22a886b\r\n", "BTW, I updated [benchmark](https://github.com/yaroslavvb/stuff/blob/master/gpu_svd_bench.py) with PyTorch numbers. PyTorch and TF get similar performance on CPU, but very different on GPU (PyTorch ran 12x faster)", "@rmlarsen can you or someone look into why PyTorch is faster, and why the GPU op is slower than the CPU op?", "This really intrigues me now. I'll also have a look at PyTorch to see what algorithm they use.", "This is strange. Let me take a look.", "According to [these release notes](https://github.com/pytorch/pytorch/releases/tag/v0.1.12) PyTorch is using gesdd (via MAGMA?) to do SVD on the GPU whereas TF appears to be using gesvd (via cuSOLVER, based on https://github.com/tensorflow/tensorflow/issues/13222#issuecomment-331621523).", "Wow, this is a good point. According to this benchmark http://www.netlib.org/lapack/lug/node71.html, GESDD is way faster than GESVD. Sadly, GESDD is not implemented in cuSolver, so we would have to use Magma instead. \r\nIt appears that Magma is also currently under more active development (If I just look at the publications), and they have some other highly interesting algorithm implementations. Maybe replace cuSolver completely by Magma? Or add a compilation switch on which implementation should be used? The details would then need to be hidden in the cuSolver wrapper (which probably should be renamed then).", "it seems all kind of decomposition is not proper for gpu?\r\nas for qr, cpu is also faster than gpu", "Hello,\r\nI'm back from a long vacation and I'd like to add my piece to the discussion:\r\n\r\n- Yes, matrix decompositions are very often slower on the GPU than on the CPU. These are simply problems that are hard to parallelize on the GPU architecture. \r\n- Yes, Eigen without MKL (that's what TF uses on the CPU) is slower than numpy with MKL\r\n\r\nI would vote for keeping the GPU version of the SVD, because for some CPU-GPU configurations and problem sizes, it is indeed faster. That was the case for me when I proposed the Pull Request.\r\nIs it possible to declare the CPU version as the default one and only use the GPU version if explicitly requested? If I'm not wrong, TF uses always the GPU version if available and if nothing else is specified.", "Hi,\r\nI'm writing some code that needs to generate on the order of millions of small SVD decompositions. I believe since I updated to 1.6 tensorflow is now defaulting to CPU to calculate these? I'd love to be able to benchmark or verify, but I'm getting a \"no kernel found\" error if I try to hard-assign my GPU on the calculation.\r\n\r\nIt seems most of you are able to switch freely between the two, so I am genuinely perplexed. What complicates matters is I have recently upgraded to Windows 10 and updated my Nvidia drivers, so that might also be the case.\r\n\r\nSorry to bother with what amounts to a tech support question :/\r\n\r\nCheers!", "@PGadoury , If you want specify device for tensorflow, there is tutorial [here](https://www.tensorflow.org/programmers_guide/using_gpu), just add `with tf.device('/cpu:0'):` and so on. but first of all, you need to ensure your tensorflow compiled with gpu support.", "Thanks for getting back to me Hao,\r\n\r\nI have indeed specified gpu for computation. If I explicitly assign the GPU, I get the \"no kernel found\" error, but I'm seeing posts on this thread discussing the speed of GPU calculations, so I have to assume some release of tensorflow has SVD GPU support? Seeing as I need a ton of small SVD decompositions in parallel, I would imagine even a somewhat slower algorithm on the GPU would outperform more efficient CPU implementations...\r\n\r\nThanks again!", "NVIDIA has worked on (and are continuing to do so) improving the performance of their symmetric eigensolvers and SVD in CUDA 9, especially 9.1, which also adds batched interfaces. We will switch to using the faster versions and/or the batched interfaces as those versions of CUDA become supported by TensorFlow. ", "You mean `cusolverDn<t>gesvdjBatched`, don't you? I just saw this update in the documentation and it would be great if this would find it's way into TensorFlow as well.", "I guess svd and eigen operations are just slow in tf?", "As mentioned over and over, SVD and Eigenvalue Decomposition is very tricky to efficiently parallelize on the GPU. As rmlarsen mentioned above, NVIDIA continues the development of cuSolver, so we can hope for some performance improvements in the future. For now, the CPU version might be faster in most cases.\r\nAs always: Profiling! If the CPU version is faster than the GPU version, then use the CPU version.", "It can be painful to wrap each of the many occurrences of tf.svd and tf.qr with `with tf.devices(\":cpu\")`. Is there a nice way to do a program-wide flag for moving all SVD ops onto CPU?", "That would be an awesome feature. Some per-operation flag that specifies whether the GPU or CPU version should be selected by default. To my knowledge, if a GPU version is available, that one is always selected. @rmlarsen what do you think?", "You can already do this:\nhttps://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/training/device_setter.py#L97\nhttps://stackoverflow.com/questions/39666845/how-does-tf-train-replica-device-setter-work\n\nBrian Patton | Software Engineer | bjp@google.com | +1 (360) 903-9003\n\n\n\nOn Tue, Aug 28, 2018 at 1:50 PM Sebastian Weiss <notifications@github.com>\nwrote:\n\n> That would be an awesome feature. Some per-operation flag that specifies\n> whether the GPU or CPU version should be selected by default. To my\n> knowledge, if a GPU version is available, that one is always selected.\n> @rmlarsen <https://github.com/rmlarsen> what do you think?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13603#issuecomment-416677585>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AVJZIzNmRcBvpTL2iRVGaDjOWj_s-JKOks5uVYLwgaJpZM4Pzkeu>\n> .\n>\n", "GPU version is so slow right now I don't think it makes sense for anyone. Here are some numbers from a recent benchmark -- https://github.com/yaroslavvb/stuff/tree/master/linalg-benchmark\r\n\r\n```\r\n1534 x 1534 svd, in milliseconds\r\nnumpy default        min:   341.51, median:   342.65, mean:   408.34\r\nTF CPU               min:  1279.98, median:  1285.51, mean:  1292.32\r\nTF GPU               min:  6962.91, median:  7006.48, mean:  8967.89\r\nPyTorch CPU          min:  1048.54, median:  1226.51, mean:  1269.30\r\nPyTorch GPU          min:   506.14, median:   511.30, mean:   513.09\r\n```", "Your benchmark assumes a single large SVD operation as a use case. TF might well be more efficient to calculate a multitude of smaller SVD's in parallel...", "Has there been any progress on this? I am now using SVD for log-det computation (it's seems to be more numerically stable than other slog_det or det), but it's also considerably slower than ways of computing the log-det.\r\n\r\nSwitching to CPU and using a custom gradient helps (~ 15x speed-up), but would be great if it can be even faster.", "We've just recast SVD as the problem of minimizing an L_2-regularized linear autoencoder, making it far more amenable to hardware and software optimizations in TF.  With SGD, the approach is analogous to randomized SVD rather than GESDD or GESVD. Would love to see some experiments with the full array of dirty deep learning tricks!\r\n\r\nhttps://github.com/danielkunin/Regularized-Linear-Autoencoders\r\nhttps://twitter.com/jbloom22/status/1089675387163021313", "Using the example from #28518, it seems that there is further regression in TensorFlow version 1.13 compared to 1.12. I don't see any significant change in the GPU kernel for the SVD op so this is likely caused by upgrade to CUDA 10.0 in TF version 1.13 from 9.0.\r\n\r\n$ time docker run --runtime=nvidia -v ~/svd:/svd/ tensorflow/tensorflow:1.12.0-gpu python /svd/graph_v1.py\r\nreal\t0m27.925s\r\nuser\t0m0.036s\r\nsys\t0m0.040s\r\n\r\n$ time docker run --runtime=nvidia -v ~/svd:/svd/ tensorflow/tensorflow:1.13.1-gpu python /svd/graph_v1.py\r\nreal\t1m4.272s\r\nuser\t0m0.032s\r\nsys\t0m0.044s\r\n", "Did you also benchmark against 2.0a?", "Hello, I am having a different problem in that eigen decomposition is extremely slow in TF for both CPU and GPU. Here is a CPU example for a moderate sized matrix (400 x 400):\r\n```\r\nimport numpy as np\r\nimport scipy as sp\r\nimport tensorflow as tf\r\nfrom time import time\r\n\r\nA = np.random.randn(400, 400)\r\nA_tf = tf.constant(A)\r\n\r\ncur = time()\r\nd, v = sp.linalg.eig(A)\r\nprint(f'sp: {time() - cur:4.2f} s')\r\n\r\ncur = time()\r\nd, v = np.linalg.eig(A)\r\nprint(f'np: {time() - cur:4.2f} s')\r\n\r\ncur = time()\r\nd, v = tf.linalg.eig(A_tf)\r\nprint(f'tf: {time() - cur:4.2f} s')\r\n```\r\nWith output:\r\n```\r\nsp: 0.09 s\r\nnp: 0.08 s\r\ntf: 5.04 s\r\n```\r\nAny ideas?", "@hzhangxyz,\r\nSorry for the delayed response. \r\nThis performance issue seems to be resolved in the latest version of **`Tensorflow, 2.5`** because of subsequent changes and upgrades in the **`CUDA`** and **`CuDNN`** versions.\r\n\r\nWe have executed your code in [Google Colab with GPU Runtime](https://colab.research.google.com/gist/rmothukuru/6f92a74a1a97d7604123f075e9c8189b/gh_13603.ipynb) and observed that there is an **`improvement in performance`**. Can you please validate our observation so that we can close this issue? Thanks!", "I'd note that TF is still significantly slower than, for example, numpy (even without MKL), but at least GPU is faster than CPU now\r\n```\r\nPython version:  3.9.4 | packaged by conda-forge | (default, May 10 2021, 22:13:33) \r\n[GCC 9.3.0]\r\nCPU version:  model name\t: Intel(R) Xeon(R) Silver 4208 CPU @ 2.10GHz\r\nCPU logical cores: 32\r\nCPU physical cores: 16\r\nCPU physical sockets:  2\r\nMKL version b'Intel(R) oneAPI Math Kernel Library Version 2021.2-Product Build 20210312 for Intel(R) 64 architecture applications'\r\nScipy version:  1.6.3\r\nNumpy version:  1.19.5\r\nTF version:  v2.5.0-rc3-213-ga4dfb8d1a71 https://github.com/tensorflow/tensorflow/commit/a4dfb8d1a71\r\nTiming in ms for 1534 x 1534 SVD\r\nnumpy default        min:   447.89, median:   453.21, mean:   469.07\r\nnumpy gesvd          min:  1930.24, median:  1934.58, mean:  1935.91\r\nnumpy gesdd          min:   449.35, median:   450.08, mean:   450.22\r\nskipping torch\r\nTF CPU               min:  1702.51, median:  1707.78, mean:  1709.30\r\nTF GPU               min:  1068.35, median:  1069.48, mean:  1145.60\r\n```\r\n\r\nnumpy without MKL and pytorch:\r\n```\r\nPython version:  3.8.5 (default, Jan 27 2021, 15:41:15) \r\n[GCC 9.3.0]\r\nCPU version:  model name\t: Intel(R) Xeon(R) Silver 4208 CPU @ 2.10GHz\r\nCPU logical cores: 32\r\nCPU physical cores: 16\r\nCPU physical sockets:  2\r\nnot using MKL\r\nScipy version:  1.5.4\r\nNumpy version:  1.20.3\r\nTiming in ms for 1534 x 1534 SVD\r\nnumpy default        min:   948.72, median:   963.15, mean:   967.72\r\nnumpy gesvd          min:  7875.78, median:  7901.95, mean:  7936.07\r\nnumpy gesdd          min:  1023.72, median:  1032.82, mean:  1036.03\r\nPyTorch version 1.8.1+cu111\r\nPyTorch CPU          min:   530.46, median:   558.88, mean:   558.12\r\nPyTorch GPU          min:   246.01, median:   252.24, mean:   833.82\r\n```\r\n", "Thank you, I think this indeed resolves the problem!\n\nEvgeniy is right, it's still not perfect, but probably worth tracking in a\nseparate issue.\n\nOn Fri, 28 May 2021 at 14:26, Evgeniy Zheltonozhskiy <\n***@***.***> wrote:\n\n> I'd note that TF is still significantly slower than, for example, numpy\n> (even without MKL), but at least GPU is faster than CPU now\n>\n> [GCC 9.3.0]CPU version:  model name\t: Intel(R) Xeon(R) Silver 4208 CPU @ 2.10GHzCPU logical cores: 32CPU physical cores: 16CPU physical sockets:  2MKL version b'Intel(R) oneAPI Math Kernel Library Version 2021.2-Product Build 20210312 for Intel(R) 64 architecture applications'Scipy version:  1.6.3Numpy version:  1.19.5TF version:  v2.5.0-rc3-213-ga4dfb8d1a71 https://github.com/tensorflow/tensorflow/commit/a4dfb8d1a71Timing in ms for 1534 x 1534 SVDnumpy default        min:   447.89, median:   453.21, mean:   469.07numpy gesvd          min:  1930.24, median:  1934.58, mean:  1935.91numpy gesdd          min:   449.35, median:   450.08, mean:   450.22skipping torchTF CPU               min:  1702.51, median:  1707.78, mean:  1709.30TF GPU               min:  1068.35, median:  1069.48, mean:  1145.60```\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13603#issuecomment-850417720>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABK6V2OVP7XFKOPFXNPFUTLTP6KXHANCNFSM4D6OI6XA>\n> .\n>\n", "@rmothukuru  At least GPU is not slower than CPU for svd. Being slower than numpy(as @Randl said, I did not test) is another problem."]}, {"number": 13602, "title": "Calculate largest max rather than smallest max in quantized_add_op.cc", "body": "I think this is a simple error that just didn't get spotted", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 13601, "title": "java API had no bool tensorflow\uff0chow to add it to the session in java", "body": "i am  doing transer the facenet to android,the input is the img,and the phase_train is a bool data,but the java api had no bool to be feed to session,as i had down this:\r\n\r\n```\r\n    private String inputName;\r\n    private String phaseName;\r\n    private String outputName;\r\n    private int inputSize;\r\n\r\n    private int[] intValues;\r\n    private int[] valuedata;\r\n    private float[] outputs;\r\n\r\n bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n        for(int i = 0; i <intValues.length(); i++ )\r\n        {\r\n            final int val = intValues[i];\r\n            valuedata[i * 3 + 0] = ((val >> 16) & 0xFF);\r\n            valuedata[i * 3 + 1] = ((val >> 8) & 0xFF);\r\n            valuedata[i * 3 + 2] = (val & 0xFF);\r\n        }\r\ninferenceInterface.fillNodeInt(\r\n                inputName, new int[]{1, inputSize, inputSize, 3}, valuedata);\r\n```\r\nand then the c++ write :\r\nm_phase_tensor = tensorflow::Tensor(tensorflow::DT_BOOL, tensorflow::TensorShape());\r\nm_phase_tensor.scalar<bool>()() = false;\r\nhow to writen in java,can somebody help me!\r\n", "comments": ["The Java API supports boolean tensors but the `TensorFlowInferenceInterface` class does not do so yet. See: https://stackoverflow.com/questions/43001532/how-to-feed-boolean-placeholder-by-means-of-tensorflowinferenceinterface-java \r\n\r\nContributions are welcome to make the change suggested in that StackOverflow answer.\r\n\r\nFYI @andrewharp ", "I think this issue is resolved with PR #14059 merged. Maybe the issues could be closed?", "Yup, thanks."]}, {"number": 13600, "title": "LayerNormBasicLSTMCell causes `bias key not found in checkpoint` when layer_norm=False", "body": "When initializing `LayerNormBasicLSTMCell`, it has a parameter `layer_norm` which controls whether we want to enable layer norm or not. I assume `layer_norm=True` should be set during training and `layer_norm=False` for evaluation. However, if I use this in an Estimator, due to the following line\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/a2d9b3bf5f9e96bf459074d079b01e1c74b25afa/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L1331\r\n\r\nit will not initialize the `bias` term because `layer_norm=True`, resulting in a `NotFoundError` when loading the saved checkpoint with `layer_norm=False`.\r\n\r\nWhat should be the expected behavior of this? Should this cell applies the bias anyway regardless the `layer_norm`? If we do not use the bias during training, I see no points to use it during inference.\r\n\r\n", "comments": ["It should be an easy fix. Please let me know if you want me to contribute on this.", "@ebrevdo Can you comment on this?", "The original author should address the issue.", "@ebrevdo my apologies, but it looks like the original author was an intern and not on github, and you reviewed the change that introduced LayerNormBasicLSTMCell.\r\n\r\nDo you have recommendations for anyone else who might have the context to evaluate this issue?", "@lukaszkaiser can you recommend someone to review?  it's not clear to me that layer norm should be disabled at inference time.", "That's a hard question. I'd suggest maybe we ask Erich Elsen? Erich: do you know this?", "@lukaszkaiser My bad. Actually there is no need to disable `layer_norm` during inference. Closing the issue.", "For reference, @lukaszkaiser it looks like you meant me, but assigned Eric D. Nielsen instead :)  I just happened to randomly find this, sorry I didn't reply before."]}, {"number": 13599, "title": "Rewrite matrix_set_diag GPU kernel.", "body": "Base new implementation on matrix_diag_op.", "comments": ["Rasmus, I am now going to test this on windows. please feel free to throw this PR out if there should be a better way to implement this.", "Running windows GPU tests here:\r\nhttp://ci.tensorflow.org/view/Experimental/job/tf-win-gpu/3/\r\n\r\nHopefully, by tomorrow morning we will have a result.", "Still fails on windows. Not sure what is missing.", "As discussed, this is not the right direction to go.", "Instead, I recommend using the recent rewrite of matrix_band_part as a template. Getting rid of the Eigen generator mechanism makes the code more readable and much faster."]}, {"number": 13598, "title": "Fix discrepancy between docs and registered kernels for `tf.ones_like`/`tf.zeros_like`", "body": "This fix tries to address the discrepancy between docs and registered kernels for `tf.ones_like`. From the implementations the `OnesLike`/`ZerosLike` are registered with all POD types. However, in the documentation several data types are missing (`uint8`, `int8`, `uint16`, `int16`, `bool`).\r\n\r\nThis fix addresses the issue by adding missing types to documentation.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks @ebrevdo for the review. The PR has been updated. Please take a look.", "Jenkins, test this please."]}, {"number": 13597, "title": " module 'tensorflow.contrib.tfprof' has no attribute 'ProfileContext' ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tensorflow-gpu (1.3.0)\r\n- **Python version**:  3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: not relevant\r\n- **GPU model and memory**: not relevant\r\n- **Exact command to reproduce**: not relevant\r\n\r\n### Describe the problem\r\nProduction machine not hooked up to Internet. Used pip3 to install [tensorflow_gpu-1.3.0-cp35-cp35m-win_amd64.whl](https://pypi.python.org/pypi/tensorflow-gpu)  It looks the profile code may be missing from the wheel.\r\n\r\nWhen run tensorflow with the following code:\r\n```\r\nimport tensorflow as tf\r\nwith tf.contrib.tfprof.ProfileContext(FLAGS.log_dir + '\\\\test', trace_steps=[], dump_steps=[]) as pctx:\r\n```\r\nget error:\r\n`AttributeError: module 'tensorflow.contrib.tfprof' has no attribute 'ProfileContext'`\r\n```\r\nDirectory of C:\\Python35\\Lib\\site-packages\\tensorflow\\contrib\\tfprof\r\n08/25/2017  02:55 PM             4,132 model_analyzer.py\r\n08/25/2017  02:55 PM             1,301 tfprof_logger.py\r\n08/25/2017  02:55 PM             1,030 __init__.py\r\n09/11/2017  10:54 AM    <DIR>          __pycache__\r\n```\r\n### Source code / logs\r\nIt appears that the examples in the https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler Readme.md  may be obsolete.\r\n", "comments": ["The readme for 1.3, https://github.com/tensorflow/tensorflow/tree/r1.3/tensorflow/core/profiler, does not contain a reference to `ProfileContext`. It appears `ProfileContext` was added after TensorFlow 1.3, so you have to use the master branch to use it.", "Hi,\r\nI am having the same error using Tensorflow 1.3:\r\nAttributeError: module 'tensorflow.contrib.tfprof' has no attribute 'ProfileContext'\r\n\r\nwhen I use:\r\nwith tf.contrib.tfprof.ProfileContext('/tmp/train_dir', trace_steps=[], dump_steps=[]) as pctx:\r\n\r\nWhen I try to import the profiler_context using:\r\nfrom tensorflow.python.profiler import profile_context\r\n\r\nI get: ImportError: cannot import name 'profile_context'\r\n\r\nWhere can I find the way to import the profiler in Tensorflow 1.3 ?\r\n\r\nThank you!", "`ProfileContext` was added after TensorFlow 1.3, so you cannot use it in 1.3. To use it, wait for TensorFlow 1.4 to be released, or use a [nightly TensorFlow binary](https://github.com/tensorflow/tensorflow#installation)."]}, {"number": 13596, "title": "Revert \"Implementing ghost batch norm as defined in https://arxiv.org\u2026", "body": "@chrisying fyi\r\n\r\nReverting in case we want to change the parameter name and semantics.", "comments": []}, {"number": 13595, "title": "cifar-10-multi-gpu-train code isn't doing synchronization correctly?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**:  irrelevant\r\n- **GPU model and memory**: irrelevant\r\n- **Exact command to reproduce**: irrelevant\r\n\r\n### Describe the problem\r\nRecently I am trying to implement some new synchronization models, and during the research process I came across the cifar-10-multi-gpu-train code. It seems that on this line: \r\nhttps://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py#L196\r\nThe synchronization point only collects gradients from each worker and do averaging. It doesn't provide a **synchronization barrier** as tf.train.SyncReplicaOptimizer does. In this way there may be some stale gradient exists. E.g on worker_1 it computes to local_step = 104, while on worker_2 it only computes to local_step = 91\r\nPlease correct me if I am wrong here.\r\n", "comments": ["That example does not support distributed mode, so there is only one worker. Therefore, there is no need for synchronization. The line you pointed out averages gradient among the GPUs, not among multiple workers."]}, {"number": 13594, "title": "How to display Runtime Statistics in Tensorboard using Estimator API in a distributed environment", "body": "Hello,\r\n\r\nI am running in the the same issue than described in this stack overflow question: https://stackoverflow.com/questions/45719176/how-to-display-runtime-statistics-in-tensorboard-using-estimator-api-in-a-distri.. \r\nI know that GitHub is used for features requests and bugs but this question didn't get an answer and I am not the only one running in the problem.\r\n\r\n This is how the doc illustrates how to add  and save Runtime statistics:\r\n```\r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\nrun_metadata = tf.RunMetadata()\r\nsummary, _ = sess.run([merged, train_step],\r\n                                       feed_dict=feed_dict(True),\r\n                                       options=run_options,\r\n                                       run_metadata=run_metadata)\r\ntrain_writer.add_run_metadata(run_metadata, 'step%d' % i)\r\n train_writer.add_summary(summary, i)\r\n```\r\n\r\nGiven that there is no evident way to call `sess.run` in the training phase with the Estimator API, I am genuinely wondering how to write this kind of summary... is there a workaround?\r\n\r\nI was thinking about using a `SessionRunHook` to create something to pass to the `EstimatorSpec` but I am really not familiar with that..", "comments": ["@Threynaud Sorry, but we have limited resources for handling github issues, so we really do need to strictly require general questions to be handled on StackOverflow.\r\n\r\nClosing this out.  FYI @jhseu for the StackOverflow question.", "Can you reopen this and label it as documentation cause it is really a general request and not use case specific. \r\nFrom stackoverflow is not clear if`'ProfilerHook` is the recommended approach.", "Also I suppose that an newbie has high probably to be exposed to high level api like keras and estimators and probably he wants to check quickly performance information on the tensorboard instead of tfprof as a first approach to profiling. So it is better to document how to quickly enable runtime statistics in TB for high level api. /cc @fchollet"]}, {"number": 13593, "title": "Fixing the name of the disabled test.", "body": "", "comments": []}]