[{"number": 24536, "title": "Windows:  //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: Master\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: VirtualEnv and Pip\r\n- Bazel version (if compiling from source): 0.18 (Have tried 0.21)\r\n- GCC/Compiler version (if compiling from source): Visual Studio 2015\r\n- CUDA/cuDNN version: CUDA = 9, cuDNN = 7\r\n- GPU model and memory: GeForce GTX 950M 4GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nThe build process seems like it completes, but when it gets up to generating the python pip library - it fails at \"ImportError: DLL load failed: The specified module could not be found.\"\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nHere is the full dump\r\n```\r\n\r\n(venv) E:\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nERROR: Failed to query DisplayName of HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\Google Chrome\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nWARNING: E:/tensorflow/tensorflow/python/BUILD:3028:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: E:/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: E:/tensorflow/tensorflow/python/BUILD:98:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: E:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: E:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: E:/tensorflow/tensorflow/contrib/gan/BUILD:140:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: E:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:356:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: E:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:76:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: E:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:233:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: E:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: E:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: E:/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: E:/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: E:/tensorflow/tensorflow/python/keras/api/BUILD:45:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/benitch/_bazel_benitch/4ejpfwyr/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=E:/CUDA\r\n    SET CUDNN_INSTALL_PATH=E:/CUDA/cudNN\r\n    SET PATH=E:\\msys64\\usr\\bin;E:\\msys64\\bin;E:\\venv\\Scripts;E:\\CUDA\\bin;E:\\CUDA\\libnvvp;C:\\ProgramData\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn\\;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files\\nodejs\\;C:\\Program Files (x86)\\WinMerge;C:\\Users\\Benitch\\.dnx\\bin;C:\\Program Files\\Microsoft DNX\\Dnvm\\;C:\\Program Files\\Intel\\WiFi\\bin\\;C:\\Program Files\\Common Files\\Intel\\WirelessCommon\\;C:\\Program Files\\Git LFS;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Microsoft VS Code\\bin;C:\\Program Files\\dotnet\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;E:\\bazel;E:\\msys64\\usr\\bin;C:\\Users\\Benitch\\.dnx\\bin;C:\\Program Files\\Microsoft DNX\\Dnvm\\;C:\\Users\\Benitch\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\;C:\\Users\\Benitch\\AppData\\Local\\Programs\\Python\\Python36\\;C:\\Users\\Benitch\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\Benitch\\AppData\\Roaming\\npm\r\n    SET PYTHON_BIN_PATH=E:/venv/Scripts/python.exe\r\n    SET PYTHON_LIB_PATH=E:/venv/Lib/site-packages\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0\r\n    SET TF_CUDA_VERSION=9.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  E:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.exe  --apidir=bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api_v2/ --apiname=keras --apiversion=2  --package=tensorflow.python,tensorflow.python.keras --output_package=tensorflow.python.keras.api._v2 bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/activations/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/densenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/inception_resnet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/inception_v3/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/mobilenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/nasnet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/resnet50/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/vgg16/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/vgg19/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/applications/xception/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/backend/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/callbacks/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/constraints/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/boston_housing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/cifar10/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/cifar100/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/fashion_mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/imdb/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/datasets/reuters/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/estimator/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/experimental/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/initializers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/layers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/losses/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/metrics/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/models/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/optimizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/preprocessing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/preprocessing/image/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/preprocessing/sequence/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/preprocessing/text/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/regularizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/utils/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/wrappers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v2/keras/wrappers/scikit_learn/__init__.py\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Benitch\\AppData\\Local\\Temp\\Bazel.runfiles_05mo_66t\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"\\\\?\\C:\\Users\\Benitch\\AppData\\Local\\Temp\\Bazel.runfiles_05mo_66t\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"\\\\?\\C:\\Users\\Benitch\\AppData\\Local\\Temp\\Bazel.runfiles_05mo_66t\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"E:\\venv\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"E:\\venv\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Benitch\\AppData\\Local\\Temp\\Bazel.runfiles_05mo_66t\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\C:\\Users\\Benitch\\AppData\\Local\\Temp\\Bazel.runfiles_05mo_66t\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"\\\\?\\C:\\Users\\Benitch\\AppData\\Local\\Temp\\Bazel.runfiles_05mo_66t\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Benitch\\AppData\\Local\\Temp\\Bazel.runfiles_05mo_66t\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"\\\\?\\C:\\Users\\Benitch\\AppData\\Local\\Temp\\Bazel.runfiles_05mo_66t\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"\\\\?\\C:\\Users\\Benitch\\AppData\\Local\\Temp\\Bazel.runfiles_05mo_66t\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"E:\\venv\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"E:\\venv\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 4789.707s, Critical Path: 581.88s\r\nINFO: 4676 processes: 4676 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nAt the moment I am now trying to recompile without using venv; as the issue looks like it begins when msys64 is invoked - I have set my PATH variable to use the venv version of Python I have set up; is there something else I am maybe missing? \r\n\r\nedit: Editing my Path didn't help either, any help would be appreciated!", "comments": ["Hello,\r\nEverything on your template looks consistent except for the Bazel version. Try **_bazel 0.15.0_** and let us know. Bazel is generally behind the curve. Thanks.", "The author had already closed the issue on their own discretion.", "Thank you for the response, I became busy with work - I will try again with Bazel 0.15, however I think when I tried anything lower than 0.18 it told me to use 0.18 or higher - will keep you posted."]}, {"number": 24535, "title": "initial_sparsity should be float but instead its integer in hparams", "body": "\r\nHi,\r\nim trying to determine an initial sparsity level so i can continue pruning a model of which i already pruned and has a known initial sparsity level,\r\nbut it seems i cannot use the initial sparsity level because it is determine as an integer in  Hparams.\r\ni dont want to build a new directory and i would like to use the get_hparams function you kindly provided.\r\nif this issue could be resolved it will be great!\r\nthank you.", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 24534, "title": "The Windows GPU binary from googleapis.com cannot see the GPU device", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-1.12.0.zip\r\n- TensorFlow version: 1.12.0\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 9.0, cuDNN64_7\r\n- GPU model and memory: GeForce 1080 Ti\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nThe Windows GPU binary I downloaded from [googleapis.com](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-1.12.0.zip) seems unable to see the GPU device on my machine. I can only see the CPU device in the return value of `TF_SessionListDevices`:\r\n> $/job:localhost/replica:0/task:0/device:CPU:0 ($CPU)\r\n\r\nI tried 1.10.0, 1.11.0 and 1.12.0, and their RC releases, none of them worked.\r\n\r\nHowever I can confirm that the PIP installed tensorflow-gpu can see the GPU device and works well on the same graph:\r\n\r\n> 2018-12-24 02:46:37.352485: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n> 2018-12-24 02:46:37.687436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\n> name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\n> pciBusID: 0000:1f:00.0\r\n> totalMemory: 11.00GiB freeMemory: 9.10GiB\r\n> 2018-12-24 02:46:37.709856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n> \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hello @hillin , Please install with following procedure and let us know:\r\n\r\nTensorFlow supports only 64-bit Python 3.5 on Windows. Test Pip installation on Windows with this distribution of Python:   _https://www.python.org/downloads/release/python-352/_\r\n\r\nNOTE: TensorFlow requires MSVCP140.DLL, which may not be installed on your system. If, when you import tensorflow as tf, you see an error about No module named \"_pywrap_tensorflow\" and/or DLL load failed, check whether MSVCP140.DLL is in your %PATH% and, if not, you should install the Visual C++ 2015 redistributable (x64 version).\r\n\r\nTo install the GPU version of TensorFlow, enter the following command at a command prompt:\r\n> C:\\> pip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-win_amd64.whl", "Hi @msymp , thanks for the response.\r\n\r\nThe python/PIP version works fine on my side. I'm reporting an issue related to the C++ binary build (which I invokes with C#).", "The author has confirmed that they have a handle now and hence have closed the issue. Thanks."]}, {"number": 24533, "title": "The include path '%{computecpp_toolkit_path}' has an unrecognized %prefix%", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS on ODROID\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): source (building from source)\r\n- TensorFlow version: r1.12\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: n/a\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source):  7.3.0\r\n- CUDA/cuDNN version: n/a (ComputeCpp Info (CE 1.0.3))\r\n- GPU model and memory: (ARM Mali-T628)\r\n\r\n**Describe the problem**\r\nI am trying to build Tensorflow r1.12 with OpenCL support. I have ComputeCpp Info (CE 1.0.3). I am getting following error.\r\n\r\n**ERROR: /home/odroid/.cache/bazel/_bazel_odroid/0a12176dc649286ae7843d8ea212d4e2/external/local_config_sycl/crosstool/BUILD:12:1: in cc_toolchain rule @local_config_sycl//crosstool:cc-compiler-local: The include path '%{computecpp_toolkit_path}' has an unrecognized %prefix%\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_sycl//crosstool:cc-compiler-local' failed; build aborted**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n After ./configure with OpenCL and ComputeCpp, I execute following command\r\n\r\nbazel build --local_resources 2048,.5,1.0 -c opt --config=sycl //tensorflow/tools/pip_package:build_pip_package\r\n\r\nIt gives:\r\n\r\nStarting local Bazel server and connecting to it...\r\n...................................................\r\n**ERROR: /home/odroid/.cache/bazel/_bazel_odroid/0a12176dc649286ae7843d8ea212d4e2/external/local_config_sycl/crosstool/BUILD:12:1: in cc_toolchain rule @local_config_sycl//crosstool:cc-compiler-local: The include path '%{computecpp_toolkit_path}' has an unrecognized %prefix%\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted:** Analysis of target '@local_config_sycl//crosstool:cc-compiler-local' failed; build aborted\r\nINFO: Elapsed time: 8.349s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (67 packages loaded)\r\n    currently loading: tensorflow/python ... (3 packages)\r\n\r\nThanks for the help.", "comments": ["Hello @Kishwar, SYCL is not supported in TF 1.12. If you want to use ComputeCpp you will have to use a specific branch based on TF 1.9. The instructions are here: https://developer.codeplay.com/computecppce/latest/getting-started-with-tensorflow.\r\nAlso I think the issue mentioned here was a mistake.", "Hello @Kishwar , Please let us know if your query has been addressed by @Rbiessy\r\nThanks.", "Hello @msymp, I changed at one place in a file with path of \"computeCpp\". My compile crossed the failing point but now I have another failure. Currently I have stopped working on it. I will start again in next year.\r\n\r\nThanks.", "The author granted assent to close this issue, since they had other tasks to pursue."]}, {"number": 24532, "title": "[INTEL MKL] Fix for failure in conv_ops_test", "body": "Don't fuse conv ops when dtype is not DT_FLOAT. ", "comments": ["Pinging @tatianashp for review.", "@caisq can you merge this PR? Thanks", "@agramesh1 Thanks for the reminder. I'll kick off the tests and if they pass, merge the PR.", "@caisq @rthadur can you take a look at this PR. It has been stuck in \"ready to pull\" state for a week. Thanks", "Closing this PR as the changes were pushed part of #24617 "]}, {"number": 24531, "title": "RTX 2080TI Tensor Cores", "body": "Hello,\r\n\r\nI am wondering if there is a way to use tensor cores from rtx 2080ti in tensorflow. I tried to use float16 in dtypes but the performance is actually worse than when using float32. What could be the problem? Is there support for tensor cores in these new video cards?", "comments": ["> Hello,\r\n> \r\n> I am wondering if there is a way to use tensor cores from rtx 2080ti in tensorflow. I tried to use float16 in dtypes but the performance is actually worse than when using float32. What could be the problem? Is there support for tensor cores in these new video cards?\r\n\r\n\r\nAccording to NVIDIA TURING GPU ARCHITECTURE, NVIDIA Geforce RTX2080TI supports TensorCore.(see https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf)\r\nHowever, the TensorCore performance of Geforce game graphics is severely limited.The peak FP16 Tensor TFLOPS with FP32 Accumulate is only 43.6% of NVIDIA Quadro RTX6000.This is very abnormal, obviously an artificial limit.However, at least this generation of Geforce RTX gaming graphics hardware supports FP16 computing.There are many requirements for the use of TensorCore. Please refer to the NVIDIA Developer website for details.\r\nAccording to my test, the RTX2080TI uses FP16 with 10-40% improvement over FP32.(use tensorflow/benchmark).In addition, the use of CUDA 10.0 can also bring a small performance boost.\r\n\r\n\r\n", " But why CUDA 10.0 can bring a small performance boost? Does it makes rtx 2080ti to use tensor cores by default? I am asking these questions because i cannot figure out how to actually write a program in tensorflow that can use these tensor cores and improve the performance. I tried to run benchmarks and the value are as it is said, but i struggle to actually implement a network in tensorflow that can use them. ", "> But why CUDA 10.0 can bring a small performance boost? Does it makes rtx 2080ti to use tensor cores by default? I am asking these questions because i cannot figure out how to actually write a program in tensorflow that can use these tensor cores and improve the performance. I tried to run benchmarks and the value are as it is said, but i struggle to actually implement a network in tensorflow that can use them.\r\n\r\nMay require NVIDIA to provide a way to observe how Tensor Core works.", "Hello @MerceaOtniel , Please let us know if your query has been addressed by @rootkitchao\r\nThanks.", "\r\n> According to NVIDIA TURING GPU ARCHITECTURE, NVIDIA Geforce RTX2080TI supports TensorCore.(see https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf)\r\n> However, the TensorCore performance of Geforce game graphics is severely limited.The peak FP16 Tensor TFLOPS with FP32 Accumulate is only 43.6% of NVIDIA Quadro RTX6000.This is very abnormal, obviously an artificial limit.However, at least this generation of Geforce RTX gaming graphics hardware supports FP16 computing.There are many requirements for the use of TensorCore. Please refer to the NVIDIA Developer website for details.\r\n> According to my test, the RTX2080TI uses FP16 with 10-40% improvement over FP32.(use tensorflow/benchmark).In addition, the use of CUDA 10.0 can also bring a small performance boost.\r\n\r\nHi\r\n\r\nI doubt that Tensor Cores are artificially throttled in RTX 2080 Ti. If you look at Table 1 in the whitepaper you linked, it lists 14.2 TFLOPS for FP32 and 113.8 Tensor TFLOPS, so an 8-fold difference.\r\n\r\nHowever, cuBLAS requires the user to \"opt in\" to use Tensor Cores, according to https://devblogs.nvidia.com/programming-tensor-cores-cuda-9/\r\n\r\n", "Hi,\r\nI am trying to infer a model trained on fp16 on cpu. I don't see any speed ups in inference. Is this an expected behaviour ? or is there a limitation here with inference on CPUs"]}, {"number": 24530, "title": "I get problem when I run my tensorflow codes on multi gpu", "body": "I still have problem after I add\r\n\r\n```\r\nsess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\r\nsess_config.gpu_options.allow_growth = True\r\nwith tf.Session(config=sess_config) as sess:\r\n```\r\n\r\nI use Tensorflow-gpu 1.12.0.I run BIDAF model on multi gpu and I get the problem below.\r\n\r\n\r\n```\r\n\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:3b:00.0, compute capability: 6.1\r\nTraceback (most recent call last):\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _run_fn\r\n    self._extend_graph()\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1352, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'\r\n\r\n         [[{{node tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert}} = Assert[T=[DT_STRING,DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/device:GPU:0\"](tower_0/passage_encoding/bidirectional_rnn/fw/fw/All, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert/data_0, tower_0/passage_encoding/bidirectional_rnn/fw/fw/stack, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert/data_2, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Shape_1)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 408, in <module>\r\n    run()\r\n  File \"run.py\", line 399, in run\r\n    multi_gpu_train(args)\r\n  File \"run.py\", line 249, in multi_gpu_train\r\n    model = RCModel_ngpus(args, data)\r\n  File \"/home/home1/dmyan/codes/tensorflow/rc_model_multi_gpu.py\", line 80, in __init__\r\n    self.sess.run(tf.global_variables_initializer())\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'\r\n\r\n         [[node tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert (defined at /home/home1/dmyan/codes/tensorflow/layers/basic_rnn.py:51)  = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/device:GPU:0\"](tower_0/passage_encoding/bidirectional_rnn/fw/fw/All, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert/data_0, tower_0/passage_encoding/bidirectional_rnn/fw/fw/stack, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert/data_2, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Shape_1)]]\r\n\r\nCaused by op 'tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert', defined at:\r\n  File \"run.py\", line 408, in <module>\r\n    run()\r\n  File \"run.py\", line 399, in run\r\n    multi_gpu_train(args)\r\n  File \"run.py\", line 249, in multi_gpu_train\r\n    model = RCModel_ngpus(args, data)\r\n  File \"/home/home1/dmyan/codes/tensorflow/rc_model_multi_gpu.py\", line 74, in __init__\r\n    self._build_graph()\r\n  File \"/home/home1/dmyan/codes/tensorflow/rc_model_multi_gpu.py\", line 87, in _build_graph\r\n    self._encode()\r\n  File \"/home/home1/dmyan/codes/tensorflow/rc_model_multi_gpu.py\", line 102, in _encode\r\n    self.sep_p_encodes, _ = rnn('bi-lstm', self.p_emb, self.p_length, self.hidden_size)\r\n  File \"/home/home1/dmyan/codes/tensorflow/layers/basic_rnn.py\", line 51, in rnn\r\n    cell_bw, cell_fw, inputs, sequence_length=length, dtype=tf.float32\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 439, in bidirectional_dynamic_rnn\r\n    time_major=time_major, scope=fw_scope)\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 651, in dynamic_rnn\r\n    [_assert_has_shape(sequence_length, [batch_size])]):\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 646, in _assert_has_shape\r\n    packed_shape, \" but saw shape: \", x_shape])\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 189, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 159, in Assert\r\n    return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 52, in _assert\r\n    name=name)\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/home/home1/dmyan/.conda/envs/py_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'\r\n\r\n         [[node tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert (defined at /home/home1/dmyan/codes/tensorflow/layers/basic_rnn.py:51)  = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/device:GPU:0\"](tower_0/passage_encoding/bidirectional_rnn/fw/fw/All, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert/data_0, tower_0/passage_encoding/bidirectional_rnn/fw/fw/stack, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Assert/Assert/data_2, tower_0/passage_encoding/bidirectional_rnn/fw/fw/Shape_1)]]\r\n\r\n```\r\n\r\n", "comments": ["AssertOp's registered kernel is CPU, so you can not assign it on GPU.\r\n`REGISTER_KERNEL_BUILDER(Name(\"Assert\").Device(DEVICE_CPU), AssertOp);`\r\n", "Hello @demeiyan , please let us know if @lxl910915 comments has addressed you query sufficiently. Thanks.", "no,I can not run bi-lstm on multi gpu. And do not know how to run.", "Hello @demeiyan ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n[New Issue Template](https://github.com/tensorflow/tensorflow/issues/new)", "Thanks  msymp,\r\n\r\nI install a binary. The below is the platform I use:\r\n\r\ntensorflow version:1.12.0\r\nubuntu version : 16.04.5\r\n\r\nI will try again today and state the details.", "I don't set os.environ[\"CUDA_VISIBLE_DEVICES\"] . I solve the problem. Thanks !", "@demeiyan : I wonder how you solved the problem then? Remove os.environ[\"CUDA_VISIBLE_DEVICES\"] did not help for my case! \r\nThx"]}, {"number": 24529, "title": "bijectors can't be used in eager execution mode", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI can't use tfp.bijectors.Bijector in eager execution mode, because the back propagation is done with tf.GradientTape with tf.keras.Model. Whereas, tfp.bijectors.Bijector is not a derived class of tf.keras.Layer, so a list of trainable variable is hard to get when back propagation.\r\n\r\nWithout eager execution feature, creating bijectors is frustrating. Several bijectors such as ActNorm and Affinecoupling need initialization with the first batch of training data. This feature is hard to implement with graph barely.\r\n\r\nIf you make Bijector a derive class of tf.keras.Layer, all trainable variables are readily available with the built tf.keras.Model object. This will no doubt facilitate the building of custom bijectors.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo change to current api. To add the feature, you need to add derivation relationship and add build member function to add all trainable variables of every bijector to the list as described [in the document](https://tensorflow.google.cn/guide/eager#build_a_model).\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAll developer building flow-based generative model with tensorflow will be benefit. \r\n\r\n**Any Other info.**\r\n\r\nCurrently, workable Glow model (https://arxiv.org/abs/1807.03039) is only available in pytorch code.", "comments": ["Hello, _**Brian Patton**_ on Stack Overflow might have provided an answer to the exact query you have:\r\n_https://stackoverflow.com/questions/53338975/use-and-modify-variables-in-tensorflow-bijectors_\r\nPlease let us know if above Stack Overflow solution has addressed your issue, without currently needing a new feature. Thanks. ", "The author has not reverted back on our Stack Overflow suggestion, hence we assume it has been useful to them. This issue is closed.", "OK, thx!", "@breadbread1984 Did the stackoverflow post resolve the issue for you? I have the same problem amongst TF 1.14, TF 1.15 and TF 2.0, and I don't want to wrap the entire code inside a `tf.GradientTape`"]}, {"number": 24528, "title": "matrix_triangular_solve is much slower on GPUs than on CPUs", "body": "I'm running a fully connected mixture density network. What could be the possible reasons that matrix_triangular_solve routine takes much longer to compute on a GPU than on CPU (27ms vs 0.4ms)? I'm seeing similar behavior across different tensorflow verison (python 2.7), eg.\r\n\r\nCPU: Intel Haswell (E5-2695 v3)\r\nCUDA version: V9.0.176\r\ntenserflow version : 1.7\r\n\r\nGPU: Tesla P100-PCIE-16GB\r\nCUDA version: V10.0.130\r\ntenserflow version: 1.10\r\n\r\n![gpu_timeline](https://user-images.githubusercontent.com/5102663/50380990-de63b880-0648-11e9-82ea-a08051b83a55.png)\r\n![cpu_timeline](https://user-images.githubusercontent.com/5102663/50380991-de63b880-0648-11e9-97f4-6f59249b17e3.png)\r\n", "comments": ["The function involved is: \r\ngmm = tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(probs=out_p),\r\n                     components_distribution=tfd.MultivariateNormalTriL(loc=out_mu,\r\n                                                        scale_tril=sigma_mat))", "Hello @maoshenl, This is a known optimization problem for GPUs, with matrix solvers and operations. Your observations across TensorFlow versions are valid. Let us try two options:\r\n1. Please provide details about what platform you are using (**_operating system, architecture_**). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\nThe issue template (_https://github.com/tensorflow/tensorflow/issues/new_). Please provide all the information it asks.\r\n2. You could of course continue your mixture density network experiments with Intel Haswell CPU, and CUDA 9.0 and r1.7, since it gives good performance.\r\n\r\nHi @azaks2, can you kindly elaborate upon the current status of slow GPU performance for the above NVIDIA chipset with the r1.10 and CUDA 10.0; @maoshenl will provide further details on the platform by template. Thanks.\r\n\r\n", "Hi @msymp,  thanks!\r\n\r\nI'm using the official NVIDIA Tensorflow singularity container 18.10-py2, directly from their web site:\r\nhttps://ngc.nvidia.com/catalog/containers/nvidia%2Ftensorflow/tags\r\n\r\nAs for other information,\r\nOS:  CentOS Version 7\r\nGPU: NVIDIA Tesla P100 Pascal architecture, 16GB/GPU memory", "What are the dimensions of your matrix? How many such systems are you solving (batch size)? It is expected that triangular solve is slower on GPU for small matrices. I am in the process of adding a code path to call the batched solver in CUDA for large batches of small matrices.", "Hi There,\r\n\r\nWe are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24528\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24528\">No</a>\n"]}, {"number": 24527, "title": "build failure for tensorflow v1.12.0 ", "body": "Hello.  Having a problem with building tensorflow from source.  The most recent version I've been able to compile was 1.9.0, and that only after multiple attempts.  Here is the result from my recent attempt at building the build_pip_package target.  Any help here would be greatly appreciated.  Just let me know if I left out any valuable information, I'll be more than happy to provide it.  Thanks. \r\n\r\n**System information**\r\n\r\nDistributor ID:\tDebian\r\nDescription:\tDebian GNU/Linux 9.6 (stretch)\r\nRelease:\t9.6\r\nCodename:\tstretch\r\n\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v1.12.0 from git repos.\r\n- Python version: 3.5.3\r\n- Installed using virtualenv? pip? conda?: virtualenv or system\r\n- Bazel version (if compiling from source): v0.21.0\r\n- GCC/Compiler version (if compiling from source): gcc 4.8.4\r\n- CUDA/cuDNN version: none \r\n- GPU model and memory: none. (cpu only)\r\n**Describe the problem**\r\nUnable to compile pip package\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n./configure   (enter default settings)\r\n\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\n\r\nINFO: Invocation ID: b0ec9363-a8cc-4c61-9cbe-64b57af62a73\r\nERROR: /home/david/.cache/bazel/_bazel_david/1f9a24d051ca8bc02465362202ebba4c/external/protobuf_archive/BUILD:591:1: Traceback (most recent call last):\r\n\tFile \"/home/david/.cache/bazel/_bazel_david/1f9a24d051ca8bc02465362202ebba4c/external/protobuf_archive/BUILD\", line 591\r\n\t\tinternal_gen_well_known_protos_java(srcs = WELL_KNOWN_PROTOS)\r\n\tFile \"/home/david/.cache/bazel/_bazel_david/1f9a24d051ca8bc02465362202ebba4c/external/protobuf_archive/protobuf.bzl\", line 269, in internal_gen_well_known_protos_java\r\n\t\tLabel((\"%s//protobuf_java\" % REPOSITOR...))\r\n\tFile \"/home/david/.cache/bazel/_bazel_david/1f9a24d051ca8bc02465362202ebba4c/external/protobuf_archive/protobuf.bzl\", line 269, in Label\r\n\t\tREPOSITORY_NAME\r\nThe value 'REPOSITORY_NAME' has been removed in favor of 'repository_name()', please use the latter (https://docs.bazel.build/versions/master/skylark/lib/native.html#repository_name). You can temporarily allow the old name by using --incompatible_package_name_is_a_function=false\r\nERROR: /home/david/.cache/bazel/_bazel_david/1f9a24d051ca8bc02465362202ebba4c/external/protobuf_archive/BUILD:715:1: Target '@protobuf_archive//:use_fast_cpp_protos' contains an error and its package is in error and referenced by '@protobuf_archive//:protobuf_python'\r\nERROR: /home/david/tensorflow/tensorflow/python/BUILD:3546:1: Target '@protobuf_archive//:protobuf_python' contains an error and its package is in error and referenced by '//tensorflow/python:util'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 0.447s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (2 packages loaded, 15 targets configured)\r\n    currently loading: tensorflow/core\r\n\r\n", "comments": ["I was able to get past that error by passing \"--incompatible_package_name_is_a_function=false\" flag (as recommended in previous error messages).  The compilation proceeded quite a bit further, only to get the following.  \r\n\r\nERROR: /home/david/.cache/bazel/_bazel_david/1f9a24d051ca8bc02465362202ebba4c/external/protobuf_archive/BUILD:70:1: C++ compilation of rule '@protobuf_archive//:protobuf_lite' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer '-std=c++0x' -MD -MF ... (remaining 32 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nIn file included from /usr/include/c++/4.8/random:50:0,\r\n                 from /usr/include/c++/4.8/bits/stl_algo.h:65,\r\n                 from /usr/include/c++/4.8/algorithm:62,\r\n                 from external/protobuf_archive/src/google/protobuf/stubs/common.h:38,\r\n                 from external/protobuf_archive/src/google/protobuf/stubs/int128.h:33,\r\n                 from external/protobuf_archive/src/google/protobuf/stubs/int128.cc:31:\r\n/usr/include/c++/4.8/bits/random.h: In constructor 'std::student_t_distribution<_RealType>::student_t_distribution(const std::student_t_distribution<_RealType>::param_type&)':\r\n/usr/include/c++/4.8/bits/random.h:3391:39: internal compiler error: in cp_parser_lookup_name, at cp/parser.c:21173\r\n       : _M_param(__p), _M_nd(), _M_gd(__p.n() / 2, 2)\r\n                                       ^\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.\r\nThe bug is not reproducible, so it is likely a hardware or OS problem.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 392.793s, Critical Path: 61.91s\r\nINFO: 815 processes: 1 local, 814 processwrapper-sandbox.\r\nFAILED: Build did NOT complete successfully\r\n\r\n", "We build successfully using bazel 0.18.0 and gcc 5.4.", "I went back to gcc-4.8 because all the tested build configurations on the tensorflow install page use gcc-4.8. Previously, I used gcc-6 which resulted in a different error for certain header files included. (I even used the -cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" option for compilers after gcc-4).  gcc-5 doesn't seem to be available in the debian packages, but I'm unclear as to how that would help.  Do you know what it is about gcc-5.4 that successfully compiles?", "Hello @starkdg, In response to your original question, you can reliably build from r1.12 sources, with **_Python 3.5.3 and gcc 4.8.4 and bazel 0.15.0_**\r\nBazel is generally behind the curve to the latest TF distros.\r\n\r\n\r\n", "Thanks @msymp for the response.  I'm pretty sure I tried that already, but I just tried it again just to be sure.  While the compilation does advance much further, it ends with a a different error.  To be sure, the missing dependency information is gone, but  compilation ends in a seg fault much later. \r\n\r\nNot that it matters, but I tried to compile in a virtualenv created with pipenv.\r\npython 3.5.3\r\ngcc 4.8.4\r\nbazel 0.15.0\r\ntensorflow v1.12.0 source\r\nsystem: Linux debian 4.9.0-8-amd64 #1 SMP Debian 4.9.130-2 (2018-10-27) x86_64 GNU/Linux\r\n\r\nError log:\r\nERROR: /home/david/.cache/bazel/_bazel_david/1f9a24d051ca8bc02465362202ebba4c/external/llvm/BUILD.bazel:1118:1: C++ compilation of rule '@llvm//:code_gen' failed (Exit 1)\r\nIn file included from external/llvm/include/llvm/Support/Options.h:41:0,\r\n                 from external/llvm/include/llvm/IR/LLVMContext.h:21,\r\n                 from external/llvm/include/llvm/IR/Metadata.h:30,\r\n                 from external/llvm/include/llvm/IR/TrackingMDRef.h:17,\r\n                 from external/llvm/include/llvm/IR/DebugLoc.h:18,\r\n                 from external/llvm/include/llvm/IR/Instruction.h:22,\r\n                 from external/llvm/include/llvm/IR/BasicBlock.h:23,\r\n                 from external/llvm/include/llvm/IR/Function.h:28,\r\n                 from external/llvm/include/llvm/IR/CallSite.h:34,\r\n                 from external/llvm/include/llvm/Analysis/MemoryLocation.h:21,\r\n                 from external/llvm/include/llvm/Analysis/AliasAnalysis.h:44,\r\n                 from external/llvm/include/llvm/CodeGen/MachineInstr.h:24,\r\n                 from external/llvm/include/llvm/CodeGen/MachineBasicBlock.h:22,\r\n                 from external/llvm/lib/CodeGen/PHIEliminationUtils.h:13,\r\n                 from external/llvm/lib/CodeGen/PHIElimination.cpp:16:\r\nexternal/llvm/include/llvm/Support/CommandLine.h: In constructor 'llvm::cl::opt<DataType, ExternalStorage, ParserClass>::opt(const Mods& ...) [with Mods = {char [32], llvm::cl::initializer<bool>, llvm::cl::OptionHidden, llvm::cl::desc}; DataType = bool; bool ExternalStorage = false; ParserClass = llvm::cl::parser<bool>]':\r\nexternal/llvm/include/llvm/Support/CommandLine.h:1363:50: internal compiler error: Segmentation fault\r\n       : Option(Optional, NotHidden), Parser(*this) {\r\n                                                  ^\r\n\r\n", "Alright, I was finally able to build the r1.12.0 package under the following setup combination.  The usual error (\"internal compiler error: segfault\") still occured, but I found that if I just rerun the command, the build eventually completes successfully.  It took 3 or 4 tries before eventually succeeding.   Not a very satisfying or comforting resolution, but at least it's something.  \r\n\r\nNot that it matters, but I tried to compile in a virtualenv created with pipenv.\r\npython 3.5.3\r\ngcc 4.8.4\r\nbazel 0.15.0\r\ntensorflow v1.12.0 source\r\nsystem: Linux debian 4.9.0-8-amd64 #1 SMP Debian 4.9.130-2 (2018-10-27) x86_64 GNU/Linux\r\n\r\n", "Thank you @starkdg , for the detailed feedback from your experiments and closing the issue, especially info regarding the Debian Linux distros, which may be very helpful to the community."]}, {"number": 24526, "title": "Use rate instead of keep_prob calling dropout in keras", "body": "\r\nWhile running tf.keras I noticed the following warning:\r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\n```\r\n\r\nThis fix fixes the warning by replacing keep_prob with rate.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Nagging Reviewer @fchollet: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 24525, "title": "Duplicated Gradient issue during TF Lite conversion", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Yes\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.0-dev20181222\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): / \r\n- GCC/Compiler version (if compiling from source): / \r\n- CUDA/cuDNN version: 9.1 \r\n- GPU model and memory: 1080 Ti with 11Gib memory\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Code to reproduce the issue**\r\nI was trying to convert the model in https://github.com/MIT-HAN-LAB/ProxylessNAS to TFLite, however, when following the tutorial, I got errors below \r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom proxyless_nas_tensorflow import proxyless_cpu, proxyless_gpu, proxyless_mobile, proxyless_mobile_14\r\n\r\nnet = proxyless_cpu()\r\nnet.is_training = False\r\n\r\nconverter = tf.lite.TFLiteConverter.from_session(net.sess, [net.images, net.labels], [net.cross_entropy])\r\ntflite_model = converter.convert()\r\n'''\r\n~/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)\r\n    206       stderr = _try_convert_to_unicode(stderr)\r\n    207       raise ConverterError(\r\n--> 208           \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    209   finally:\r\n    210     # Must manually cleanup files.\r\n\r\nConverterError: TOCO failed. See console for info.\r\n2018-12-22 13:38:13.626047: F tensorflow/core/framework/function.cc:1626] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for ReadVariableOp\r\nAborted (core dumped)\r\n'''\r\n```\r\nAny advice would be appreciated!", "comments": ["I meet the same problem with you. Any idea?", "No yet. I have looked into the computation graph, but no idea where the duplication is. ", "same problem here...", "I turned to tf-nightly-1.13.0.dev20181027 and this problem disappeared.", "i have the same problem and tf-nightly-1.13.0.dev20181027 didn't work for me ", "Living hard times suffering the same issue here", "Hello all, to correctly use the latest TFLiteConverter for r1.12/r1.13 & higher nightly builds, you must use the  **_tf.contrib.lite.TFLiteConverter_** Python API instead of tf.lite.TFLiteConverter. Please try your conversions again and report back to us. Thanks.", "@msymp \r\nHi M Srikant~\r\n    I just tried the method you mentioned above\uff1a\r\n> tf.contrib.lite.TFLiteConverter Python API instead of tf.lite.TFLiteConverter\r\n\r\nBut it didn't work.\r\nMy tf version is **'1.13.0-dev20181226'**.\r\nHere is part of my code and the error log:\r\n\r\n**code:**\r\n\r\n\r\n`\r\nconverter =tf.contrib.lite.TFLiteConverter.from_frozen_graph('data/tflog/pb/feature_extracting_from_fixed_sequence.pb',\r\n                                                      ['adio_sequence'],['adio_sequence'])\r\nconverter.post_training_quantize = True\r\ntflite_quantized_model=converter.convert()\r\n`\r\n\r\n**error log:**\r\n\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-6-9437387566b3> in <module>\r\n      2                                                       ['adio_sequence'],['adio_sequence'])\r\n      3 converter.post_training_quantize = True\r\n----> 4 tflite_quantized_model=converter.convert()\r\n\r\n~/anaconda3/envs/try_new/lib/python3.6/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    497           input_tensors=self._input_tensors,\r\n    498           output_tensors=self._output_tensors,\r\n--> 499           **converter_kwargs)\r\n    500     else:\r\n    501       result = _toco_convert_graph_def(\r\n\r\n~/anaconda3/envs/try_new/lib/python3.6/site-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)\r\n    443   data = toco_convert_protos(model_flags.SerializeToString(),\r\n    444                              toco_flags.SerializeToString(),\r\n--> 445                              input_data.SerializeToString())\r\n    446   return data\r\n    447 \r\n\r\n~/anaconda3/envs/try_new/lib/python3.6/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)\r\n    206       stderr = _try_convert_to_unicode(stderr)\r\n    207       raise ConverterError(\r\n--> 208           \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    209   finally:\r\n    210     # Must manually cleanup files.\r\n\r\n**ConverterError: TOCO failed. See console for info.\r\n2018-12-27 11:01:58.639961: F tensorflow/core/framework/function.cc:1626] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for ReadVariableOp\r\nAborted (core dumped)**", "My problem was finally solved when switching to **TensorFlow 1.12.0**", "Great to hear!", "I have this same issue: trying to do a simple conversion from a saved_model, exported from an estimator, to .tflite. Tried with the command line tool first, and was getting a DType complaint for the placeholder. Changing dTypes did not help that. Now getting the duplicated gradient issue, using the latest from tf_nightly. ", "With TensorFlow 1.12.0, I get the original issue again. Which makes sense. That is, not using the nightly build. \r\n\r\nI tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 2\\n2018-12-30 15:40:54.450020: F tensorflow/contrib/lite/toco/import_tensorflow.cc:2137] Check failed: status.ok() Unexpected value forattribute 'T'. Expected 'DT_FLOAT'\\n\"\r\n"]}, {"number": 24524, "title": "AttributeError: module 'tensorflow.compat' has no attribute 'v1'", "body": "I ran cnn_mnist.py on my machine but got \"AttributeError: module 'tensorflow.compat' has no attribute 'v1'\". can anyone solve this problem?\r\n", "comments": ["You can run it by changing\r\ntrain_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\r\nto train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\nand eval_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\r\nto eval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n\r\nbasically the reverse than what was done in the last commit:\r\nhttps://github.com/tensorflow/tensorflow/commit/95e808ba44075dfe0b7db57bb49d2e64a1977a95\r\n\r\n", "Closing this issue since workaround has been provided and [PR](https://github.com/tensorflow/tensorflow/pull/24552) is in process. Thanks!", "I tried to upgrade tf to 1.13, and problems solved.", "I met a similar bug, that is ' AttributeError: module 'tensorflow._api.v1.compat' has no attribute 'v1' ', when I run inference.py of [FPN_tensorflow](https://github.com/DetectionTeamUCAS/FPN_Tensorflow), the source code is `summary_image_v1 = tf.compat.v1.summary.image` .\r\nThen I change the code to `summary_image_v1 = tf.summary.image`, the program can run correctly.", "because the tensorflow-plot version is higher. so \r\n pip install tensorflow-plot==0.3.0\r\nwill ok!", "Hello, \r\nI run into the \"same\" Problem. I have successfully installed tensorflow-gpu 1.12 on Ubuntu 18.04 with CUDA 9.0 and cudnn 7.1.2 in an anaconda environment (I followed this tutorial: https://medium.com/@redowan/no-bullshit-guide-on-installing-tensorflow-gpu-ubuntu-18-04-18-10-238924cc4a6a).\r\n\r\nThen i followed this tutorial for installing object detection models: https://medium.com/@teyou21/setup-tensorflow-for-object-detection-on-ubuntu-16-04-e2485b52e32a\r\n\r\nBut when i try to python `object_detection/builders/model_builder_test.py` i get following error: \r\n\r\n`Traceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_test.py\", line 23, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"/home/schaupp/models/research/object_detection/builders/model_builder.py\", line 35, in <module>\r\n    from object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res\r\n  File \"/home/schaupp/models/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py\", line 30, in <module>\r\n    from nets import inception_resnet_v2\r\n  File \"/home/schaupp/models/research/slim/nets/inception_resnet_v2.py\", line 375, in <module>\r\n    batch_norm_updates_collections=tf.compat.v1.GraphKeys.UPDATE_OPS,\r\nAttributeError: module 'tensorflow._api.v1.compat' has no attribute 'v1'\r\n`\r\nI installed exact the same ah few month ago on the same machine but this time it won't work.\r\nI hope someone can help me - i struggled many hours with this problem.", "> I met a similar bug, that is ' AttributeError: module 'tensorflow._api.v1.compat' has no attribute 'v1' ', when I run inference.py of [FPN_tensorflow](https://github.com/DetectionTeamUCAS/FPN_Tensorflow), the source code is `summary_image_v1 = tf.compat.v1.summary.image` .\r\n> Then I change the code to `summary_image_v1 = tf.summary.image`, the program can run correctly.\r\n\r\nThis worked for me . Thanks", "> I tried to upgrade tf to 1.13, and problems solved.\r\n\r\nMany thanks. It also worked for me."]}, {"number": 24523, "title": "Update bazel to 0.20.0 for tensorflow/tensorflow:custom-op", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): `docker run -i -t --rm tensorflow/tensorflow:custom-op `\r\n- TensorFlow version: `docker run -i -t --rm tensorflow/tensorflow:custom-op `\r\n- Python version: n/a\r\n- Installed using virtualenv? pip? conda?: docker\r\n- Bazel version (if compiling from source): 0.15.0 (`tensorflow/tensorflow:custom-op`)\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version:n/a\r\n- GPU model and memory:n/\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhile tensorflow has updated bazel to 0.20.0, The `tensorflow/tensorflow:custom-op` docker image still uses 0.15.0. This cause issues when trying to build a custom op that requires higher versions of bazel (e..g, 0.17.1+).\r\n\r\n\r\nPlease upgrade bazel in `tensorflow/tensorflow:custom-op` to 0.20.0.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n```\r\n$ docker run -i -t --rm tensorflow/tensorflow:custom-op \r\nroot@ubuntu:/# bazel version\r\nExtracting Bazel installation...\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nBuild label: 0.15.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jun 26 12:10:19 2018 (1530015019)\r\nBuild timestamp: 1530015019\r\nBuild timestamp as int: 1530015019\r\n```\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I pushed the first version of tensorflow/tensorflow:nightly-custom-op just now and am in the middle of setting up a nightly release. Please give it a try. Thanks!", "@yifeif I tried with `tensorflow/tensorflow:nightly-custom-op` to build custom ops and it works great. Thanks!", "Great! Thanks for letting us know."]}, {"number": 24522, "title": "Fix ValueError when passing tf.data.Dataset to model.fit", "body": "This fix tries to address the issue raised in #24520 where\r\npassing tf.data.Dataset to model.fit might result in:\r\n```\r\nValueError: Cannot take the length of Shape with unknown rank.\r\n```\r\n\r\nThis fix fixes the error by replacing len(x.shape) with x.shape.ndims\r\n\r\nThis fix fixes #24520.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @fchollet for the review. The PR has been updated with the `rank()` used.", "@yongtang Can you resolve the merge conflicts? Thanks!", "@yongtang any chance you'll be able to fix this soon (it would be really helpful to me on my current project)? Thanks so much for doing this!", "@yongtang gentle ping", "@yongtang Any update on above comment please?", "@yongtang Could you please check on line 756 , too? It raises similar error when one is training with class_weights and y is a tensor from tf.dataset. The error reads something like this:\r\n\r\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_utils.py\", line 756, in standardize_weights\r\n    if len(y.shape) > 2:\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 745, in __len__\r\n    raise ValueError(\"Cannot take the length of shape with unknown rank.\")", "@yongtang  any updates? As I said above, this is affecting in other places too. I am not able to train a model with class_weights and tf.dataset", "@dd1923 for what it's worth, I ran into the same error when using `tf.Dataset.from_generator`, and was able to fix the issue by passing in an argument for [`output_shapes`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#args_7). I didn't even have to specify the exact dimensions - using something like `tf.TensorShape([None, None, None])` worked in the case of using 3 dimensional images. While the `from_tensor_slices` method doesn't have have the same parameter, I wonder if there is an analogous way to set the shape (even if filled with `None`, but having the right dimensionality) that might resolve your issue until this PR is merged.\r\n\r\nSo, I have an imagenet generator outputting (image, label) of shape ((224, 224, 3), (1, )).\r\n\r\nOriginal code that leads to that same error being thrown:\r\n\r\n```\r\ndataset = tf.Dataset.from_generator(\r\n    generator, output_types=('float32', 'uint8')\r\n)\r\n```\r\n\r\nFixed code that prevents that error:\r\n\r\n```\r\ndataset = tf.Dataset.from_generator(\r\n    generator, output_types=('float32', 'uint8'),\r\n    output_shapes=(tf.TensorShape((None, None, None)), tf.TensorShape((1, )))\r\n)", "@sallamander that;s a good discovery. I gave up on tf.dataset lately. I switched to keras sequences and now everything just works. I think Tensorflow hastily declared support for tf.dataset in Keras APIs without first making sure that the underlying code is ready for that. It was a nightmare to get a few basic things work.", "@yongtang can you please resolve conflicts? Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "Is it possible to get this actually reopened and merged? The conflicts are relatively trivial of a fix (against the 1.14 branch), and this is still a bug in Tensorflow 1.14. (Unknown whether this error occurs for Tensorflow 2.0, our team has had no luck with getting 2.0 to work in our codebase).", "@jordanra-miso please open a new PR with this changes , We will help merge the PR. Thank you", "@rthadur My PR (#33895) has been made, with updated `shape.rank` usage to conform to @fchollet's comments."]}, {"number": 24521, "title": "TF Model Lite Benchmark tool still failed to build on Windows", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 Pro 64bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:nightly build\r\n- Python version:3.5\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):0.20.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\nThanks for @jdduke #24475 patch.\r\nUnfortunately TF Model Lite Benchmark tool still failed to build on Windows.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel --output_base $(Build.BinariesDirectory) build -c opt //tensorflow/lite/tools/benchmark:benchmark_model\r\n\r\n**Any other info / logs**\r\n```\r\n2018-12-22T17:16:35.9502732Z .\\tensorflow/core/util/stats_calculator.h(41): error C2039: 'max': is not a member of 'std'\r\n2018-12-22T17:16:35.9502783Z C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\map(15): note: see declaration of 'std'\r\n2018-12-22T17:16:35.9502890Z .\\tensorflow/core/util/stats_calculator.h(35): note: while compiling class template member function 'void tensorflow::Stat<int64_t,double>::UpdateStat(ValueType)'\r\n2018-12-22T17:16:35.9503047Z         with\r\n2018-12-22T17:16:35.9503198Z         [\r\n2018-12-22T17:16:35.9503238Z             ValueType=int64_t\r\n2018-12-22T17:16:35.9503347Z         ]\r\n2018-12-22T17:16:35.9503394Z .\\tensorflow/core/util/stats_calculator.h(150): note: see reference to function template instantiation 'void tensorflow::Stat<int64_t,double>::UpdateStat(ValueType)' being compiled\r\n2018-12-22T17:16:35.9503433Z         with\r\n2018-12-22T17:16:35.9503520Z         [\r\n2018-12-22T17:16:35.9503557Z             ValueType=int64_t\r\n2018-12-22T17:16:35.9503592Z         ]\r\n2018-12-22T17:16:35.9503638Z .\\tensorflow/core/util/stats_calculator.h(159): note: see reference to class template instantiation 'tensorflow::Stat<int64_t,double>' being compiled\r\n2018-12-22T17:16:35.9503730Z .\\tensorflow/core/util/stats_calculator.h(41): error C2660: 'tensorflow::Stat<int64_t,double>::max': function does not take 2 arguments\r\n2018-12-22T17:16:35.9503782Z .\\tensorflow/core/util/stats_calculator.h(42): error C2039: 'min': is not a member of 'std'\r\n2018-12-22T17:16:35.9503825Z C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\map(15): note: see declaration of 'std'\r\n2018-12-22T17:16:35.9503927Z .\\tensorflow/core/util/stats_calculator.h(42): error C2660: 'tensorflow::Stat<int64_t,double>::min': function does not take 2 arguments\r\n2018-12-22T17:16:35.9777999Z Target //tensorflow/lite/tools/benchmark:benchmark_model failed to build\r\n2018-12-22T17:16:35.9810313Z Use --verbose_failures to see the command lines of failed build steps.\r\n2018-12-22T17:16:35.9833736Z INFO: Elapsed time: 2.003s, Critical Path: 0.59s\r\n```\r\nFor further information visit our [CI environment (see build_tflite_win Job)](https://dev.azure.com/mlops/build-tflite/_build/results?buildId=14)\r\n", "comments": ["I guess /tensorflow/core/util/stats_calculator.h should include algorithm header.\r\nWould you like to check [std::max specification](http://www.cplusplus.com/reference/algorithm/max/) ?", "@jdduke, Could you please take a look at this build issue in TF Lite. Thanks", "@karimnosseir will be helping with this issue, thanks for the report and apologies for the trouble! I suspect the fix is pretty trivial."]}, {"number": 24520, "title": "\"ValueError: Cannot take the length of Shape with unknown rank\". error when passing tf.data.Dataset tensors to model.fit", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \"18.04.1 LTS (Bionic Beaver)\"\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf.VERSION = 1.12.0\r\n- Python version: python3.6\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: cuda9.0 with cuDNN 7.4.1\r\n- GPU model and memory: GTX 1080 with 8 GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI am trying to pass the tfrecords read through tf.data.Dataset api into the model.fit .\r\nSince the images could be of different sizes, I am storing the image shapes into tfrecords itself which are \r\nlater on read and applied to the img data using  `tf.reshape` . But the tensorflow.keras is unable to determine the shape of this image data at this stage and throws the error.\r\n\r\n```\r\ndef _parse_function(proto):\r\n    keys_to_features = {\"im_path\": tf.FixedLenSequenceFeature([], tf.string, allow_missing=True),\r\n                        \"im_shape\": tf.FixedLenSequenceFeature([], tf.int64, allow_missing=True),\r\n                        \"im_arr\": tf.FixedLenSequenceFeature([], tf.string, allow_missing=True),\r\n                        \"label\": tf.FixedLenSequenceFeature([], tf.int64, allow_missing=True),\r\n                        }\r\n\r\n    parsed_features = tf.parse_single_example(serialized=proto, features=keys_to_features)\r\n    parsed_features['im_arr'] = parsed_features['im_arr'][0]\r\n    parsed_features['label'] = parsed_features['label'][0]\r\n    parsed_features['im_arr'] = tf.decode_raw(parsed_features['im_arr'], tf.uint8)\r\n    parsed_features['im_arr'] = tf.reshape(parsed_features['im_arr'], parsed_features['im_shape'])\r\n\r\n    return parsed_features['im_arr'], parsed_features['label']\r\n```\r\n\r\n The error thrown is as follows : \r\n\r\n\tTraceback (most recent call last):\r\n\t  File \"issue/IssueScript.py\", line 75, in <module>\r\n\t    verbose=1)\r\n\t  File \"opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1536, in fit\r\n\t    validation_split=validation_split)\r\n\t  File \"opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 992, in _standardize_user_data\r\n\t    class_weight, batch_size)\r\n\t  File \"opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1117, in _standardize_weights\r\n\t    exception_prefix='input')\r\n\t  File \"opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 284, in standardize_input_data\r\n\t    data = [standardize_single_array(x) for x in data]\r\n\t  File \"opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 284, in <listcomp>\r\n\t    data = [standardize_single_array(x) for x in data]\r\n\t  File \"opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 218, in standardize_single_array\r\n\t    if x.shape is not None and len(x.shape) == 1:\r\n\t  File \"opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 579, in __len__\r\n\t    raise ValueError(\"Cannot take the length of Shape with unknown rank.\")\r\n\tValueError: Cannot take the length of Shape with unknown rank.\r\n\r\n\r\nSo as a debugging step, I removed the length check present in the `standardize_single_array` function by changing the check as (note the `False and` part which bypasses the length check)\r\n\r\n```def standardize_single_array(x):\r\n  if x is None:\r\n    return None\r\n  if False and (x.shape is not None and len(x.shape) == 1):\r\n    if tensor_util.is_tensor(x):\r\n      return array_ops.expand_dims(x, axis=1)\r\n    else:\r\n      return np.expand_dims(x, 1)\r\n  return x\r\n```\r\nThen I get the following error \r\n\r\n\tTraceback (most recent call last):\r\n\t  File \"issue/IssueScript.py\", line 75, in <module>\r\n\t    verbose=1)\r\n\t  File \"opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1536, in fit\r\n\t    validation_split=validation_split)\r\n\t  File \"opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 992, in _standardize_user_data\r\n\t    class_weight, batch_size)\r\n\t  File \"opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1154, in _standardize_weights\r\n\t    exception_prefix='target')\r\n\t  File \"opt/github/example-classification/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 323, in standardize_input_data\r\n\t    'with shape ' + str(data_shape))\r\n\tValueError: Error when checking target: expected activation_4 to have 2 dimensions, but got array with shape (None,)\r\n\r\n\r\nI did the same with the above error. I removed the check present at line 323 by commenting out the length check as follows.\r\n```\r\n        \"\"\"\r\n        if len(data_shape) != len(shape):\r\n          raise ValueError('Error when checking ' + exception_prefix +\r\n                           ': expected ' + names[i] + ' to have ' +\r\n                           str(len(shape)) + ' dimensions, but got array '\r\n                           'with shape ' + str(data_shape))\r\n        \"\"\"\r\n```\r\nNow the training proceeds smoothly without error. I believe there is issue with tf.reshape when tensors are supplied as a shape to the function.\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nCode : https://github.com/dineshdharme/tensorflow-issue1\r\nJust run : `python3 issue/IssueScript.py`\r\n\r\nI have also added a tfrecords generating script `tfrecords_utils.py` which you can call by\r\nTo generate tfrecords file using the image data present in the data folder : \r\n`python3 issue/tfrecords_utils.py`\r\n\r\n", "comments": ["@dineshdharme Added PR #24522 for the fix.", "Even fixing the issues #24522  in my local code i still see the same problem. My code snippet is as:\r\n\r\n```\r\ndef get_image_arr(path, width, height, img_norm=\"divide\"):\r\n  try:\r\n    img = cv2.imread(path, 1)\r\n    if img_norm == \"sub_and_divide\":\r\n      img = np.float32(cv2.resize(img, (width, height))) / 127.5 - 1\r\n    elif img_norm == \"sub_mean\":\r\n      img = cv2.resize(img, (width, height))\r\n      img = img.astype(np.float32)\r\n      img[:,:,0] -= 103.939\r\n      img[:,:,1] -= 116.779\r\n      img[:,:,2] -= 123.68\r\n    elif img_norm == \"divide\":\r\n      img = cv2.resize(img, (width, height))\r\n      img = img.astype(np.float32)\r\n      img = img/255.0\r\n    return img\r\n  \r\n  except Exception as e:\r\n    print (e)\r\n    \r\n\r\ndef get_groundtruth_arr(path, width, height, n_classes=1):\r\n  try:\r\n    gt_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\r\n    gt_img = gt_img.astype(np.float32)\r\n    gt_img = cv2.resize(gt_img, (width, height))\r\n    gt_img = gt_img/255.0\r\n    gt_img_exp = np.expand_dims(gt_img, axis=2)\r\n    \r\n    return gt_img_exp\r\n      \r\n  except Exception as e:\r\n    print (e)\r\n\r\ndef get_x_and_y(im, gt):\r\n  X = get_image_arr(im, input_width, input_height)\r\n  Y = get_groundtruth_arr(gt, output_width, output_height)\r\n  print(X.ndim)\r\n  return X, Y\r\n\r\nimages_path = train_images_path\r\ngt_path = train_gt_path\r\nimages = glob.glob( images_path + \"*.jpg\" ) + glob.glob( images_path + \"*.png\" ) +  glob.glob( images_path + \"*.jpeg\" )\r\nimages.sort()\r\nground_truth  = glob.glob( gt_path + \"*.jpg\" ) + glob.glob( gt_path + \"*.png\" ) +  glob.glob( gt_path + \"*.jpeg\" )\r\nground_truth.sort()\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((images, ground_truth))\r\n\r\ndataset = dataset.map(lambda img, gt_img: tuple(tf.py_func(\r\n                      get_x_and_y, [img, gt_img], [tf.float32, tf.float32])))\r\n\r\ndataset = dataset.shuffle(dataset_size(train_images_path)).batch(BATCH_SZ)\r\n\r\n```\r\n\r\nThe error i get is following: \r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-99-deead2a4c31f> in <module>()\r\n----> 1 model.fit(dataset, epochs=1, steps_per_epoch=per_epoch_steps, validation_data=val_dataset, validation_steps=steps_validation, verbose=1, callbacks=[tbCallBack])\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    920           steps_per_epoch=steps_per_epoch,\r\n    921           validation_steps=validation_steps,\r\n--> 922           steps_name='steps_per_epoch')\r\n    923 \r\n    924   def evaluate(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, steps_name, **kwargs)\r\n    143 \r\n    144   # Prepare input data.\r\n--> 145   ins = _prepare_feed_values(model, inputs, targets, sample_weights, mode)\r\n    146   if not is_dataset:\r\n    147     num_samples_or_steps = _get_num_samples_or_steps(ins, batch_size,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py in _prepare_feed_values(model, inputs, targets, sample_weights, mode)\r\n    424     inputs, targets, sample_weights = model._standardize_user_data(\r\n    425         inputs,\r\n--> 426         extract_tensors_from_dataset=True)\r\n    427 \r\n    428   inputs = training_utils.ModelInputs(inputs).as_list()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n   2456           feed_input_shapes,\r\n   2457           check_batch_axis=False,  # Don't enforce the batch size.\r\n-> 2458           exception_prefix='input')\r\n   2459 \r\n   2460     if y is not None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n    303       data = [\r\n    304           x.values if x.__class__.__name__ == 'DataFrame' else x for x in data\r\n--> 305       ]\r\n    306   else:\r\n    307     data = data.values if data.__class__.__name__ == 'DataFrame' else data\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py in <listcomp>(.0)\r\n    303       data = [\r\n    304           x.values if x.__class__.__name__ == 'DataFrame' else x for x in data\r\n--> 305       ]\r\n    306   else:\r\n    307     data = data.values if data.__class__.__name__ == 'DataFrame' else data\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py in standardize_single_array(x, expected_shape)\r\n    234 \r\n    235   #if (x.shape is not None\r\n--> 236   if tensor_util.is_tensor(x):\r\n    237       x_shape_ndims = x.shape.ndims if x.shape is not None else None\r\n    238   else:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py in __len__(self)\r\n    746     \"\"\"Returns the rank of this shape, or raises ValueError if unspecified.\"\"\"\r\n    747     if self._dims is None:\r\n--> 748       raise ValueError(\"Cannot take the length of shape with unknown rank.\")\r\n    749     return len(self._dims)\r\n    750 \r\n\r\nValueError: Cannot take the length of shape with unknown rank.\r\n```", "@milinddeore for what it's worth, I ran into the same error when using `tf.Dataset.from_generator`, and was able to fix the issue by passing in an argument for [`output_shapes`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#args_7). I didn't even have to specify the exact dimensions - using something like `tf.TensorShape([None, None, None])` worked in the case of using 3 dimensional images. While the `from_tensor_slices` method doesn't have have the same parameter, I wonder if there is an analogous way to set the shape (even if filled with `None`, but having the right dimensionality) that might resolve your issue until this PR is merged.\r\n\r\nSo, I have an imagenet generator outputting (image, label) of shape ((224, 224, 3), (1, )).\r\n\r\nOriginal code that leads to that same error being thrown:\r\n\r\n```\r\ndataset = tf.Dataset.from_generator(\r\n    generator, output_types=('float32', 'uint8')\r\n)\r\n```\r\n\r\nFixed code that prevents that error:\r\n\r\n```\r\ndataset = tf.Dataset.from_generator(\r\n    generator, output_types=('float32', 'uint8'),\r\n    output_shapes=(tf.TensorShape((None, None, None)), tf.TensorShape((1, )))\r\n)", "I have also encountered this issue when passing tf.data to tf.keras fit method.\r\nUnlike @sallamander though, I used `tf.data.Dataset.from_tensor_slices` instead of `tf.Dataset.from_generator`.\r\n\r\nThe shape can then be defined inside the mapping function.\r\n```\r\ndef preprocessing(img_path):\r\n    train_img = load_img(img_path) # Unrelated implementation thus not shown\r\n    train_img = tf.reshape(train_img, shape=(input_width, input_height, input_channel))\r\n    return train_img\r\n\r\ntrain_data = tf.data.Dataset.from_tensor_slices(img_paths)\r\ntrain_data = train_data.shuffle(len(img_paths)).map(preprocessing)\r\n```", "I have the same error like so:\r\n\r\n`  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.1.1\\helpers\\pydev\\pydevd.py\", line 1758, in <module>\r\n    main()\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.1.1\\helpers\\pydev\\pydevd.py\", line 1752, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.1.1\\helpers\\pydev\\pydevd.py\", line 1147, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.1.1\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"C:/Users/hep01/PycharmProjects/DWIDL/3DUnet1.1_with_tfdata_pipeline.py\", line 138, in <module>\r\n    model.fit(data_train,epochs=1,callbacks=[checkpointer])\r\n  File \"C:\\Users\\hep01\\PycharmProjects\\DWIDL\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 780, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"C:\\Users\\hep01\\PycharmProjects\\DWIDL\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 174, in model_iteration\r\n    ins = _prepare_feed_values(model, inputs, targets, sample_weights, mode)\r\n  File \"C:\\Users\\hep01\\PycharmProjects\\DWIDL\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 501, in _prepare_feed_values\r\n    extract_tensors_from_dataset=True)\r\n  File \"C:\\Users\\hep01\\PycharmProjects\\DWIDL\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 2651, in _standardize_user_data\r\n    exception_prefix='input')\r\n  File \"C:\\Users\\hep01\\PycharmProjects\\DWIDL\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\", line 334, in standardize_input_data\r\n    standardize_single_array(x, shape) for (x, shape) in zip(data, shapes)\r\n  File \"C:\\Users\\hep01\\PycharmProjects\\DWIDL\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\", line 334, in <listcomp>\r\n    standardize_single_array(x, shape) for (x, shape) in zip(data, shapes)\r\n  File \"C:\\Users\\hep01\\PycharmProjects\\DWIDL\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\", line 265, in standardize_single_array\r\n    if (x.shape is not None and len(x.shape) == 1 and\r\n  File \"C:\\Users\\hep01\\PycharmProjects\\DWIDL\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 825, in __len__\r\n    raise ValueError(\"Cannot take the length of shape with unknown rank.\")\r\nValueError: Cannot take the length of shape with unknown rank.\r\n`\r\nInterestingly, the error does not exist when I use CPU to tun the training. It appears after I installed a GPU with toolbox such as CUDA and cuDNN. Hope that info helps for debugging this.\r\n\r\n", "Same situation here. Installing the  nightly build of today (July 24th, 2019) with:\r\n```\r\npip install tf-nightly-2.0-preview\r\n```\r\nin colaboratory.\r\n\r\nIf I use a strategy, in this case to access a TPU, I get the error:\r\n```\r\nValueError: Cannot take the length of shape with unknown rank.\r\n```\r\nHowever, if I just avoid the use of an strategy, thinks work smoothly.\r\n\r\nI am using a dataset.map which includes a numpy_function that calls a python object that is in charge of the training.\r\n```python\r\nclass DummyTrainer:\r\n    def __init__(self, file_path, samples):\r\n        self.file_path = file_path\r\n        self.samples = samples\r\n        self.index_array = np.random.permutation(self.samples)\r\n\r\n    def _calc_image(self, index):\r\n        filename= os.path.join(self.file_path, str(i)+'.png')\r\n        if os.path.isfile(filename):\r\n            img_raw = tf.io.read_file(filename)\r\n            img = tf.io.decode_png(img_raw, channels=1)\r\n            img = tf.dtypes.cast(img, tf.float32) / 256.0\r\n            return img\r\n        else:\r\n            return None\r\n\r\n    def _dataset_function(self, index):\r\n        return tuple(tf.numpy_function(\r\n            self._calc_image, [index],\r\n            [tf.float32, tf.uint8]))\r\n\r\n    def get_trainer_dataset(self):\r\n        # Reading the images from the numpy array of indexes\r\n        dataset = Dataset.from_tensor_slices(\r\n            self.index_array[:self.trainer_samples])\r\n        return dataset.map(self._dataset_function).shuffle(1)\r\n\r\ntrainer = DummyTrainer('data', 256)\r\n\r\nmodel.fit(trainer.get_trainer_dataset(), ...)\r\n```\r\nMy first guess would be something related with serialization.", "same problem with tf1.14", "Also having this problem with tf1.14\r\n\r\nIn my case, I encountered the problem when using a `tf.data.Dataset`, with which I had mapped a python function using a combination of `tf.data.Dataset.map()` and `tf.py_func`. I was able to sidestep the issue by specifying the shape of the tensor returned from `tf.py_func`, as below:\r\n\r\n```python\r\n        def mappable_fn(x):\r\n            result_tensors = tf.py_func(func=my_py_func,\r\n                                        inp=[my_py_func_args],\r\n                                        Tout=[my_py_func_output_types])\r\n            result_tensor.set_shape(the_shape_i_know_ahead_of_time)\r\n            return (result_tensor)\r\n\r\n```\r\n\r\nObviously this will only work if you know the shapes of the tensor(s) ahead of time. ", "Thanks for flagging this. Looks like we don't check the shape correctly in several different places -- fix in progress.", "Seeing the same problem.\r\n\r\nIn TF1.14, it seems to help to activate eager execution by adding `tf.enable_eager_execution()` at the top.", "Also encountering this issue on TF r2.0.\r\n\r\nUsing a tf.data.Dataset with a map function, where the map function is a tf.py_function;\r\n\r\nThis issue persists even after I set the Tensor shape manually by calling  `.set_shape(tf.TensorShape(some_shape))` on the tensor inside the py_function being called on map function. (When I check my dataset's elements, it appears that there are valid shapes for the tensors)\r\n\r\nI get the \r\n\r\n> ValueError: Cannot take the length of Shape with unknown rank.\r\n\r\nissue while using a MirroredStrategy.\r\n\r\nAfter removing the MirroredStrategy, I encounter an error similar to [this](https://github.com/tensorflow/tensorflow/issues/35358#issuecomment-571946651):\r\n\r\n> ValueError: as_list() is not defined on an unknown TensorShape.\r\n\r\n@adriancaruana 's answer helped to temporarily resolve the issue", "Same problem with dataset created with `tf.data.TFRecordDataset`", "Until this gets fixed and released to GA can we have a workaround please? There are so many papers pushing their bespoke data to TPUs. If they can do it it means there is an alternative approach available. Some insights in this would be welcomed.", "I have been getting the same errors for a model that I trained without any issues the last summer. The conda environment I used got lost during a macos update so I dig up the environment and here are the versions of python and tensorflow that works:\r\n\r\npython=3.7.3\r\ntensorflow=2.0.0a0\r\n\r\nHope this will provide a temporary solution in case you are having issues because of the same reasons as I do.", "I encountered the same problem when using the **_from_tensor_slices_** in the tf.keras model.fit method. I use `dataset.as_numpy_iterator() `as a workaround.\r\n\r\nUse like:\r\n\r\n```\r\ndataset = tf.data.Dataset.from_tensor_slices(....).map(...)\r\nmodel.fit(dataset.as_numpy_iterator())\r\n```\r\n\r\nInstead of:\r\n\r\n```\r\ndataset = tf.data.Dataset.from_tensor_slices(....).map(...)\r\nmodel.fit(dataset)\r\n\r\n```\r\n", "I encountered the same problem with tf.data.Dataset. When I checked `mydata.element_spec`. some elements had `TensorSpec(shape=<unknown>)`. I used padded_batch instead of batch ahd I added the wanted shapes to padded_shapes with the same structure as 'mydata.element_spec'.\r\nFor example:\r\n```\r\npadded_shapes = ({'input_mask': [128],\r\n                  'input_type_ids': [128],\r\n                  'input_word_ids': [128]},\r\n                  [])\r\ntrain_data = train_data.padded_batch(32, padded_shapes=padded_shapes, drop_remainder=True)\r\n```\r\n\r\n", "I encountered this too", "There are two separate issues here. @rodyt, I think you're encountering an issue with datasets with unknown shape + distribution strategy, which we've fixed (#34469) at head. For other people, a temporary workaround while we work on a fix is using `set_shape` if you know the output shapes, as @adriancaruana pointed out.", "Update: Based on talking to @robieta, keras expects its inputs to have at least known rank (even if dimensions are unknown). So, if the dataset has components with unknown rank, keras will not work. \r\n\r\nIn some cases, tf.data is not able to statically infer the rank of its outputs (e.g. if you use a py_func), so you have to manually use `set_shape` to tell the dataset what shapes its outputs are, as @adriancaruana suggested in https://github.com/tensorflow/tensorflow/issues/24520#issuecomment-532958834. Note that you don't have to know the shape fully, you just need to know the number of dimensions. So, you could do something like:\r\n\r\n```\r\ndef map_fn(x):\r\n  result_tensor = ...\r\n  result_tensor.set_shape([None for _ in range(rank)])\r\n  return result_tensor\r\n```\r\n\r\nDoes this resolve the issue for all who've encountered it?\r\n\r\nOn the keras side, we should surface a more informative error. @karmel , can you reassign this to someone on the keras team to surface a more informative error message when the input shapes are of unknown rank? ", "@rachellim I encountered exactly the same thing as @adriancaruana . His proposed solution works for me with `tf.__version__: 2.1.0`. Be sure to apply `set_shape()` to the tensor _returned_ by `tf.py_function()`, and not within `tf.py_function()`.", "I'm currently using tf.__version__: 2.1.0 docker image from TensorFlow docker hub.\r\n\r\nI encountered this issue when I used tf.data.TFRecordDataset() as an argument of model.fit().\r\nAs @birkanatici mentioned, using dataset.as_numpy_iterator() helped me to resolve the error, but it's just workaround.", "@dartlune -- please see my comment above (https://github.com/tensorflow/tensorflow/issues/24520#issuecomment-577325475). Does the workaround of using `set_shape` help? ", "@rachellim Oh, I thought that only tensor(non-scalar) should be applied `set_shape`.\r\nAfter I apply `set_shape` to a scalar, the error is gone!\r\nThank you \ud83d\udc4d", "Hello All,\r\n\r\nI am currently working with Tensorflow 2.1\r\n\r\nI am trying to implement MirroredStrategy to an Image captions generator. I am getting the above-said error when I try to start training.\r\n\r\nCode snippet\r\n`\r\n\r\n       with strategy.scope():\r\n\r\n\t\tdef map_func(img_name, cap):\r\n\t\timg_tensor = np.load(img_name.decode('utf-8')+'.npy')\r\n\t\timg_tensor.set_shape(299,299)\r\n\t\treturn img_tensor, cap\r\n\r\n\t   dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\r\n\r\n\t   dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]))\r\n\r\n\t   dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n\t   dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\n\t   train_dist_dataset = strategy.experimental_distribute_dataset(dataset)`\r\n\r\nError\r\n`\r\nTraceback (most recent call last):\r\n  File \"MirrorTrainer_Image2Smilex.py\", line 88, in <module>\r\n    train_dist_dataset = strategy.experimental_distribute_dataset(dataset)\r\n  File \"/home/kohulan/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 677, in experimental_distribute_dataset\r\n    return self._extended._experimental_distribute_dataset(dataset)  # pylint: disable=protected-access\r\n  File \"/home/kohulan/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 580, in _experimental_distribute_dataset\r\n    split_batch_by=self._num_replicas_in_sync)\r\n  File \"/home/kohulan/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 89, in get_distributed_dataset\r\n    input_context=input_context)\r\n  File \"/home/kohulan/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 509, in __init__\r\n    dataset = distribute._RebatchDataset(dataset, split_batch_by)\r\n  File \"/home/kohulan/.local/lib/python3.6/site-packages/tensorflow_core/python/data/experimental/ops/distribute.py\", line 110, in __init__\r\n    rebatch, dataset_ops.get_structure(input_dataset))\r\n  File \"/home/kohulan/.local/lib/python3.6/site-packages/tensorflow_core/python/data/util/nest.py\", line 245, in map_structure\r\n    structure[0], [func(*x) for x in entries])\r\n  File \"/home/kohulan/.local/lib/python3.6/site-packages/tensorflow_core/python/data/util/nest.py\", line 245, in <listcomp>\r\n    structure[0], [func(*x) for x in entries])\r\n  File \"/home/kohulan/.local/lib/python3.6/site-packages/tensorflow_core/python/data/experimental/ops/distribute.py\", line 105, in rebatch\r\n    batch_size = recalculate_batch_size(type_spec._to_legacy_output_shapes())\r\n  File \"/home/kohulan/.local/lib/python3.6/site-packages/tensorflow_core/python/data/experimental/ops/distribute.py\", line 90, in recalculate_batch_size\r\n    if len(output_shapes) < 1:\r\n  File \"/home/kohulan/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 822, in __len__\r\n    raise ValueError(\"Cannot take the length of shape with unknown rank.\")\r\nValueError: Cannot take the length of shape with unknown rank.\r\n`\r\n\r\nAny idea to come across this problem? Or even a different distributed training implementation for Image Captioning will be much helpful", "@Nixon59-lab, you're encountering a different issue (https://github.com/tensorflow/tensorflow/issues/34469) that has since been fixed at head. You might want to try the tf nightly build. This will also be fixed in 2.2.", "@rachellim Yes, this works for me, **TF 2.1**:\r\n\r\n```python\r\ndef load_image(file, label):\r\n    nifti = np.asarray(nibabel.load(file.numpy().decode('utf-8')).get_fdata()).astype(np.float32)\r\n\r\n    xs, ys, zs = np.where(nifti != 0)\r\n    nifti = nifti[min(xs):max(xs) + 1, min(ys):max(ys) + 1, min(zs):max(zs) + 1]\r\n    nifti = nifti[0:100, 0:100, 0:100]\r\n    nifti = np.reshape(nifti, (100, 100, 100, 1))\r\n    return nifti, label\r\n\r\n\r\n@tf.autograph.experimental.do_not_convert\r\ndef load_image_wrapper(file, label):\r\n    result_tensors = tf.py_function(load_image, [file, label], [tf.float64, tf.float64])\r\n    result_tensors[0].set_shape([100, 100, 100, 1])\r\n    result_tensors[1].set_shape([None])\r\n    return result_tensors\r\n\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((train, labels))\r\ndataset = dataset.map(load_image_wrapper, num_parallel_calls=24)\r\ndataset = dataset.repeat(50)\r\ndataset = dataset.batch(12, drop_remainder=True)\r\ndataset = dataset.prefetch(buffer_size=6)\r\n```\r\nHowever, the performance is quite bad, comparable to the old Keras `fit_generator()`, which defeats the whole purpose of `tf.data`.\r\nDoes anyone know how to increase the performance of the dataset?\r\nCan an iterator be used with Keras' `model.fit()`?\r\n\r\nI've tried all of this:\r\n```python\r\ndataset = tf.data.Dataset.from_tensor_slices((train, labels))\r\ndataset = dataset.map(load_image_wrapper, num_parallel_calls=12)\r\ndataset = dataset.repeat(50)\r\ndataset = dataset.prefetch(buffer_size=2)\r\ndataset = dataset.apply(tf.data.experimental.prefetch_to_device('/device:GPU:0', 1))\r\ndataset = dataset.batch(12, drop_remainder=True)\r\n```", "I have figured it out.\r\n**With keras, to improve performance:**\r\n\r\n```python\r\ndef load_image(file, label):\r\n    nifti = np.asarray(nibabel.load(file.numpy().decode('utf-8')).get_fdata()).astype(np.float32)\r\n\r\n    xs, ys, zs = np.where(nifti != 0)\r\n    nifti = nifti[min(xs):max(xs) + 1, min(ys):max(ys) + 1, min(zs):max(zs) + 1]\r\n    nifti = nifti[0:100, 0:100, 0:100]\r\n    nifti = np.reshape(nifti, (100, 100, 100, 1))\r\n    return nifti, label\r\n\r\n\r\n@tf.autograph.experimental.do_not_convert\r\ndef load_image_wrapper(file, label):\r\n    return tf.py_function(load_image, [file, label], [tf.float64, tf.float64])\r\n\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((train, labels))\r\ndataset = dataset.map(load_image_wrapper, num_parallel_calls=32)\r\ndataset = dataset.prefetch(buffer_size=1)\r\ndataset = dataset.apply(tf.data.experimental.prefetch_to_device('/device:GPU:0', 1))\r\n# Now my dataset size is 522, and for batch, I'm creating a single batch of the entire dataset.\r\ndataset = dataset.batch(522, drop_remainder=True).repeat()\r\n\r\n# Initialise iterator\r\niterator = iter(dataset)\r\n\r\n# get x and y\r\nbatch_image, batch_label = iterator.get_next()\r\n\r\n# Over here, supply model.fit with x & y, and THEN supply your batch size here.\r\nmodel.fit(batch_image, batch_label, epochs=100, batch_size=12)\r\n```\r\n\r\nWhy does this work? Is this even the **correct implementation**?\r\n", "`set_shape([])` for scalar tensors worked me too.", "> I encountered the same problem when using the **_from_tensor_slices_** in the tf.keras model.fit method. I use `dataset.as_numpy_iterator() `as a workaround.\r\n> \r\n> Use like:\r\n> \r\n> ```\r\n> dataset = tf.data.Dataset.from_tensor_slices(....).map(...)\r\n> model.fit(dataset.as_numpy_iterator())\r\n> ```\r\n> \r\n> Instead of:\r\n> \r\n> ```\r\n> dataset = tf.data.Dataset.from_tensor_slices(....).map(...)\r\n> model.fit(dataset)\r\n> ```\r\n\r\nI tried this but now it is very slow. At first it took milliseconds for one step. It is now taking 17s per step. Any way around the speed?", "@eaaarmah - instead of using `as_numpy_iterator()`, use `set_shape` in your map function to give your dataset a known shape (or at least, known rank). Please read all the comments above in this thread for examples.", "@dominthomas, two things:\r\n\r\n(1) i would recommend using `tf.data.experimental.AUTOTUNE` as the parallelism setting on your `map` function, e.g.\r\n\r\n`dataset = dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)`\r\n\r\nThis dynamically picks the optimal parallelism setting at runtime for best performance -- reducing the need to set the parallelism manually. Let me know how this works for you. \r\n\r\nIf you want to understand how to debug input pipeline performance, the TF team recently released the TF profiler with TF 2.2. It can help you understand which parts of the input pipeline are slow. We can chat about that in a separate thread - the team is also working on releasing a guide to debugging input pipeline performance using the TF profiler.\r\n\r\nTutorial: https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras\r\nGuide: https://www.tensorflow.org/guide/profiler\r\n\r\n(2) The second code snippet you shared \"works\" (doesn't raise an error or require `set_shape`) because you're passing the outputs of the iterator directly to keras. The `set_shape` workaround is only necessary when you're passing the dataset directly to keras; it has to do with how keras handles dataset shapes.", "I have to agree with @dominthomas here compared to using a Sequence with multiprocessing on all 24 cores, this is way, way way slower (20 hours versus 4 using multiprocessing for an epoch for me).  I get no parallelism using TF.dataset I assume b/c I am using a pyfunc, b/c I couldn't figure out how to put all my work into TF.  My Sequence functions involve using a pandas index file, and reading out from a binary numpy file based on the pandas index, maybe there is a way to do this in pure dataset notation, but right now the documentation is so opaque I can't figure it out.", "> I have figured it out.\r\n> **With keras, to improve performance:**\r\n> \r\n> ```python\r\n> def load_image(file, label):\r\n>     nifti = np.asarray(nibabel.load(file.numpy().decode('utf-8')).get_fdata()).astype(np.float32)\r\n> \r\n>     xs, ys, zs = np.where(nifti != 0)\r\n>     nifti = nifti[min(xs):max(xs) + 1, min(ys):max(ys) + 1, min(zs):max(zs) + 1]\r\n>     nifti = nifti[0:100, 0:100, 0:100]\r\n>     nifti = np.reshape(nifti, (100, 100, 100, 1))\r\n>     return nifti, label\r\n> \r\n> \r\n> @tf.autograph.experimental.do_not_convert\r\n> def load_image_wrapper(file, label):\r\n>     return tf.py_function(load_image, [file, label], [tf.float64, tf.float64])\r\n> \r\n> \r\n> dataset = tf.data.Dataset.from_tensor_slices((train, labels))\r\n> dataset = dataset.map(load_image_wrapper, num_parallel_calls=32)\r\n> dataset = dataset.prefetch(buffer_size=1)\r\n> dataset = dataset.apply(tf.data.experimental.prefetch_to_device('/device:GPU:0', 1))\r\n> # Now my dataset size is 522, and for batch, I'm creating a single batch of the entire dataset.\r\n> dataset = dataset.batch(522, drop_remainder=True).repeat()\r\n> \r\n> # Initialise iterator\r\n> iterator = iter(dataset)\r\n> \r\n> # get x and y\r\n> batch_image, batch_label = iterator.get_next()\r\n> \r\n> # Over here, supply model.fit with x & y, and THEN supply your batch size here.\r\n> model.fit(batch_image, batch_label, epochs=100, batch_size=12)\r\n> ```\r\n> \r\n> Why does this work? Is this even the **correct implementation**?\r\n\r\n@dominthomas have you had any success using cache with the working code that you have posted?", "@nickkimer I've tried caching and it didn't enhance the performance, I suspect the bottleneck is in the load_image function, as there is a computation there to remove background, just for some context, these are MRI images; so it's removing  slices in all 3 dimensional planes that doesn't contain tissue.\r\n\r\nI will test out the TF profiler @rachellim mentioned, which could prove my suspicion, but even then, with a 32 core cpu, this shouldn't be an issue.\r\n\r\n@rachellim Thank you for the reply, however, when I use `tf.data.experimental.AUTOTUNE`, performance is around 30% lower. Explicitly stating to use 32 cores works best in my scenario.", "@dominthomas  - thanks for the info. i'm interested in understanding why AUTOTUNE isn't performing well for you. can you start a separate github issue about the performance issues you're facing? ", "Ok. I managed to get a solution working using only the Protobuf api available in tensorflow. This approach avoid serializing numpy data to bytes and passing the raw bytes to the TFRecord. This approach forces the use of a special py_function that cannot be serialized and moved on the graph when sending data to the TPU or in a distributed environment. \r\n\r\nThe code is available at https://gist.github.com/vicpara/5c23c78d0f3105af53798272e628d2ad .\r\n\r\nAs the map function that does the tensor manipulation of the feature dictionary produced by the `TFRecordDataset` is not required anymore, the above approach also avoids the additional `set_shape` operation outside the `@py_function` scope in an additional map operation.\r\n\r\nLastly, i feel i managed to get a decent solution that at least solves my problem to a satisfactory level. It's easy to add more features of different kinds after getting to understand how is TF using protobuf to serialize these features which is not the easiest thing ever. Having run into all these issues i would like to mention the following:\r\n\r\n* Just by looking at this issue alone one can find that the support given here is quite disconsiderate. This API goes 90% to do something and then just drops the ball. The errors are obscure, the documentation arcane and difficult to decipher and the proper examples almost lacking. Within a year and a half there has been little guidance that came from the TF devs to help address all these issues raised here.\r\n\r\n* (Tensorflow Dataset)[https://github.com/tensorflow/datasets] goes a long way to fix so many issues around the current API when it comes to making the `serialization` and `deserialization` experience smooth. They almost completely built their own mechanism barely using anything provided in this one. It does introduce an additional functionality around publishing a dataset to the cloud but they do fill in a lot of issues around the existing protobuf protocol.\r\n\r\n* Tensors as they currently stand contain all the information needed to get fully serialized/deserialized in the backend. You have there the shape, the types and the values. For the typical standard use it should be straight forward.  \r\nWhy do put your API users to such pain of digging in multiple source repos to fix your half baked solutions ?\r\n\r\n* The dev support for tensorflow related issues is incredibly slow and mostly not helpful. It pretty much says that we have an error because we have an error and don't do that to not get the error.\r\n\r\n* The documentation lacks supporting examples, the examples are trivial and the error hide so much of the magic happening inside the graph.\r\n\r\nWhat kind of community do you want around Tensorflow? Is Tensorflow just for the academics? You seem to cater mostly to the academics and presume all datasets are and should be floating freely in the cloud. Are the academics going to sell out your TPU time?\r\n\r\nYour declared intentions in the promotional videos of TF relating to the industry adoption don't correspond to the level of support you show here nor to the maturity, documentation and completeness of the APIs. \r\n\r\nIf you feel the industry people should move to PyTorch  and forget about using TF with ease I'd appreciate to find out about this sooner rather than later.\r\n", "@dineshdharme Thanks for the issue!\r\n\r\nApologies for the delay, `tf.keras`'s built-in training loops just went through a major rewrite in order to support custom training steps out-of-the-box. Many fixes were blocked on this rewrite.\r\n\r\nThis is now fixed at head, here's an example of passing `Tensor`s of unknown rank to `Model.fit`:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef my_numpy_fn(x, y):\r\n  return -x, -y\r\n\r\nfeatures = np.arange(10).astype(np.float32)\r\nlabels = 2 * features\r\nds = tf.data.Dataset.from_tensor_slices((features, labels))\r\n# Do a transformation that loses rank information.\r\nds = ds.map(\r\n    lambda x, y: tf.numpy_function(\r\n        my_numpy_fn, inp=[x, y], Tout=[tf.float32, tf.float32]\r\n        )\r\n    ).batch(2)\r\n\r\nassert iter(ds).output_shapes[0] == tf.TensorShape(None)\r\n\r\n# Model works with Tensors of unknown rank.\r\n# Note that if your Model uses layers like `Dense`, etc. that\r\n# only work with ceratin ranks, you should still use `x.set_shape`\r\n# before passing the data to that layer, to give the Model a hint\r\n# about the rank.\r\nclass MyModel(tf.keras.Model):\r\n  def call(self, x):\r\n    return 2 * x\r\n\r\nmodel.compile('sgd', 'mse')\r\nmodel.fit(ds)\r\n```\r\n\r\nFor more info on the rewrite, please check out the [2.2 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24520\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24520\">No</a>\n", "I was getting this error with the multi-input, generator code below,\r\n\r\n ValueError: Cannot take the length of shape with unknown rank.\r\n\r\nupgrading to v2.2 resolved the issue thanks\r\n\r\n```\r\ndef gen_obs():\r\n    for i in itertools.count(1):\r\n        action = env.action_space.sample()\r\n        #print(action)\r\n        obs, rew,dn,inf = env.step(action)\r\n        print(obs, rew,dn,inf)\r\n        #yield ([obs[0],obs[1],obs[2],obs[3], rew, dn], [10.0])\r\n        yield {\"reference_pa\":[obs[0]], \"perception_pa\":[obs[1]]}, [10.0]\r\n      \r\n\r\nds_counter = tf.data.Dataset.from_generator(gen_obs, output_types=({\"reference_pa\":tf.float32, \"perception_pa\":tf.float32}, tf.float32))\r\n\r\nhistory = model.fit(ds_counter, epochs=20, verbose=True)\r\n\r\n```", "Same here, upgrade to TF2.2 solved the problem.", "I have same problem, here was my code:\r\n`def distort_simclr(image):`\r\n `   image = tf.cast(image, tf.float32)`\r\n `   v1 = color_distortion(image / 255.)`\r\n  `  v2 = color_distortion(image / 255.)`\r\n   ` return v1, v2`\r\n`training_set = tf.data.Dataset.from_generator(path, output_types=(tf.float32, tf.float32), output_shapes = ([2,224,224,3],[2,2]))`\r\n`training_set = training_set.map(distort_simclr, num_parallel_calls=tf.data.experimental.AUTOTUNE)`\r\n\r\nI find this error:\r\n`TypeError: tf__distort_simclr() takes 1 positional argument but 2 were given`", "@aynesss , that's a different problem. Namely, your generator dataset has two components `(output_types = (tf.float32, tf.float32))`, but the function you supply to map only takes one argument. In this case, assuming your generator produces an (image, label) tuple, you probably want to rewrite your `distort_simclr` function to take two arguments (`def distort_simclr(image, label): ...`).\r\n\r\nSee the documentation of [`dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) for more details (\"The input signature of map_func is determined by the structure of each element in this dataset.\")", "Had the exact same error in tf 2.3.0\r\nset_shape solved the error with model.fit ", "Hello i'm running the same problem with tf 2.5.0.\r\nin my case the training set and the validation set are saved on disk as a tfrecord file, they are read and passed to the fit method and I get the error.\r\n\r\n```\r\n#reading training and validation dataset\r\ndef read_tfrecord(example):\r\n  tfrecord_format = (\r\n      {\r\n          \"x\": tf.io.FixedLenFeature([], tf.string),\r\n          \"y\": tf.io.FixedLenFeature([], tf.string),\r\n      }\r\n  )\r\n  example = tf.io.parse_single_example(example, tfrecord_format)\r\n  x = tf.io.parse_tensor(example['x'], out_type=tf.float32)\r\n  y = tf.io.parse_tensor(example['y'], out_type=tf.double)\r\n    \r\n\r\n  return x,y\r\n\r\nbatch_size = 32\r\n\r\nfilename = \"training.tfrecord\"\r\ntraining_dataset = tf.data.TFRecordDataset(filename).map(read_tfrecord)\r\ntraining_dataset = training_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\r\n\r\nfilename = \"validation.tfrecord\"\r\nvalidation_dataset = tf.data.TFRecordDataset(filename).map(read_tfrecord)\r\nvalidation_dataset = validation_dataset.batch(batch_size)\r\n\r\n\r\nmodel.fit(training_dataset,\r\n          validation_data=validation_dataset,\r\n          epochs=35,\r\n          verbose=1)\r\n\r\n```\r\n\r\nusing set_shape on read_tfrecord method  as suggested before, I get the error\r\n```\r\ndef read_tfrecord(example):\r\n  tfrecord_format = (\r\n      {\r\n          \"x\": tf.io.FixedLenFeature([], tf.string),\r\n          \"y\": tf.io.FixedLenFeature([], tf.string),\r\n      }\r\n  )\r\n  example = tf.io.parse_single_example(example, tfrecord_format)\r\n  x = tf.io.parse_tensor(example['x'], out_type=tf.float32)\r\n  y = tf.io.parse_tensor(example['y'], out_type=tf.double)\r\n    \r\n  x = x.set_shape((None, None, None))\r\n  y = y.set_shape((None, None)\r\n\r\n  return x,y\r\n```\r\n`\r\nInvalidArgumentError: Length for attr 'output_shapes' of 0 must be at least minimum 1`\r\n\r\nAny suggestion?"]}, {"number": 24519, "title": "Unexpected behaviour of tf.map_fn()", "body": "- OS Platform and Distribution: Windows 10 \r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.10.0\r\n- Python version: 3.6.6\r\n\r\nI am using K.map_fn() in my custom loss function where I am passing both y_true and y_pred of shape (None, None) as elems argument of this function. But when the function specified in map_fn is called, elements obtained in that function are of different shapes. And that's the problem.\r\n\r\nHere's the example:\r\n\r\nMy custom loss function:\r\n```\r\ndef negative_avg_log_error(y_true, y_pred):\r\n\r\n    def sum_of_log_probabilities(true_and_pred):\r\n        y_true, y_pred = true_and_pred\r\n        print(K.int_shape(y_true))\r\n        print(K.int_shape(y_pred))\r\n        start_index = int(y_true[0])\r\n        end_index = int(y_true[1])\r\n        start_probability = y_pred[start_index]\r\n        end_probability = y_pred[end_index]\r\n        return K.log(start_probability) + K.log(end_probability)\r\n\r\n    print(K.int_shape(y_true))\r\n    print(K.int_shape(y_pred))\r\n    batch_probability_sum = K.map_fn(lambda x: sum_of_log_probabilities(x), elems=[y_true, y_pred], dtype='float32')\r\n    return -K.mean(batch_probability_sum, axis=1)\r\n```\r\n\r\nCode from which this loss function is called:\r\n\r\n`model.compile(loss=negative_avg_log_error, optimizer='adadelta', metrics='loss')`\r\n\r\n\r\nError log:\r\n\r\n```\r\n(None, None)\r\n(None, None)\r\n(None,)\r\n(None, 1)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-15-807b6d06a435> in <module>()\r\n----> 1 model.compile(loss=negative_avg_log_error, optimizer='adadelta', metrics='loss')\r\n\r\nC:\\Python36\\lib\\site-packages\\keras\\engine\\training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\r\n    331                 with K.name_scope(self.output_names[i] + '_loss'):\r\n    332                     output_loss = weighted_loss(y_true, y_pred,\r\n--> 333                                                 sample_weight, mask)\r\n    334                 if len(self.outputs) > 1:\r\n    335                     self.metrics_tensors.append(output_loss)\r\n\r\nC:\\Python36\\lib\\site-packages\\keras\\engine\\training_utils.py in weighted(y_true, y_pred, weights, mask)\r\n    401         \"\"\"\r\n    402         # score_array has ndim >= 2\r\n--> 403         score_array = fn(y_true, y_pred)\r\n    404         if mask is not None:\r\n    405             # Cast the mask to floatX to avoid float64 upcasting in Theano\r\n\r\nE:\\Deep Learning Material\\IMP for Project\\model\\scripts\\loss_function.py in negative_avg_log_error(y_true, y_pred)\r\n     16     print(K.int_shape(y_true))\r\n     17     print(K.int_shape(y_pred))\r\n---> 18     batch_probability_sum = K.map_fn(lambda x: sum_of_log_probabilities(x), elems=[y_true, y_pred], dtype='float32')\r\n     19     return -K.mean(batch_probability_sum, axis=1)\r\n\r\nC:\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in map_fn(fn, elems, name, dtype)\r\n   4229         Tensor with dtype `dtype`.\r\n   4230     \"\"\"\r\n-> 4231     return tf.map_fn(fn, elems, name=name, dtype=dtype)\r\n   4232 \r\n   4233 \r\n\r\nC:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py in map_fn(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)\r\n    457         back_prop=back_prop,\r\n    458         swap_memory=swap_memory,\r\n--> 459         maximum_iterations=n)\r\n    460     results_flat = [r.stack() for r in r_a]\r\n    461 \r\n\r\nC:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\r\n   3230       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\r\n   3231     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\r\n-> 3232                                     return_same_structure)\r\n   3233     if maximum_iterations is not None:\r\n   3234       return result[1]\r\n\r\nC:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants, return_same_structure)\r\n   2950       with ops.get_default_graph()._mutation_lock():  # pylint: disable=protected-access\r\n   2951         original_body_result, exit_vars = self._BuildLoop(\r\n-> 2952             pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2953     finally:\r\n   2954       self.Exit()\r\n\r\nC:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2885         flat_sequence=vars_for_body_with_tensor_arrays)\r\n   2886     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n-> 2887     body_result = body(*packed_vars_for_body)\r\n   2888     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n   2889     if not nest.is_sequence(body_result):\r\n\r\nC:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in <lambda>(i, lv)\r\n   3199         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\r\n   3200             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\r\n-> 3201         body = lambda i, lv: (i + 1, orig_body(*lv))\r\n   3202 \r\n   3203     if context.executing_eagerly():\r\n\r\nC:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py in compute(i, tas)\r\n    446       \"\"\"\r\n    447       packed_values = input_pack([elem_ta.read(i) for elem_ta in elems_ta])\r\n--> 448       packed_fn_values = fn(packed_values)\r\n    449       nest.assert_same_structure(dtype or elems, packed_fn_values)\r\n    450       flat_fn_values = output_flatten(packed_fn_values)\r\n\r\nE:\\Deep Learning Material\\IMP for Project\\model\\scripts\\loss_function.py in <lambda>(x)\r\n     16     print(K.int_shape(y_true))\r\n     17     print(K.int_shape(y_pred))\r\n---> 18     batch_probability_sum = K.map_fn(lambda x: sum_of_log_probabilities(x), elems=[y_true, y_pred], dtype='float32')\r\n     19     return -K.mean(batch_probability_sum, axis=1)\r\n\r\nE:\\Deep Learning Material\\IMP for Project\\model\\scripts\\loss_function.py in sum_of_log_probabilities(true_and_pred)\r\n      8         print(K.int_shape(y_true))\r\n      9         print(K.int_shape(y_pred))\r\n---> 10         start_index = int(y_true[0])\r\n     11         end_index = int(y_true[1])\r\n     12         start_probability = y_pred[start_index]\r\n\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'Tensor'\r\n```\r\n\r\n\r\nI am currently focusing on:\r\nIn negative_avg_log_error() => K.int_shape(y_true)=(None, None) and K.int_shape(y_pred)=(None, None)\r\nIn sum_of_log_probabilities() => K.int_shape(y_true)=(None,) and K.int_shape(y_pred)=(None, 1)\r\n\r\nBut I think that the shape of both these tensors in sum_of_log_probabilities() should be (None, ).\r\n\r\nNeed help as understanding this will help me solve this error.\r\n", "comments": ["@ParikhKadam I am not able to reproduce with tf-nightly, can you try with tf-nightly or share your full minimal reproducible code that exposed the issue?", "@yongtang Thank you for the response.. This is already the minimal code which should be able to reproduce this unexpected behaviour. If you want, I can upload my project on Github and share it with you.\r\n\r\nAlso, can you share your piece of code so that I can check if there are any implementation differences. I am in a hurry as I have to submit my project on Monday and everything relies on this loss function which is getting me wrong output.\r\n\r\nI have also shared the same issue on stack, subreddits and keras-github with no response. Thank you..", "@ParikhKadam You are using 1.10.0 which is quite old. Can you try with tf-nightly or 1.12.0?", "@yongtang Sure.. I will try that and update here.\r\n\r\nBy the way, will I also need to change the GPU version? Its because, I have already set up my computer with CUDA 9. And by installing another GPU version, I might also need to change CUDA version which is very time consuming.", "@yongtang Updated tensorflow and still the problem persists.\r\n\r\nBy the way, updated tensorflow-gpu too and it needed the same version of CUDA. Saved me some time of reinstalling things.. But the problem still persists..\r\n\r\nNeed help..", "@ParikhKadam It would be helpful if you could provide the full example code that could reproduce the error.", "@yongtang Sure... here is the file containing model architecture and the error:\r\n\r\nhttps://github.com/ParikhKadam/bidaf-keras/blob/master/graph.ipynb", "@yongtang Any updates? I have tried running an example code snippet and it worked as expected. That means, there is no problem with the tensorflow version or my environment. It's something different. Here it is..\r\n\r\n```\r\nimport numpy as np \r\nfrom keras import backend as K \r\n\r\nA = np.random.random((32,6))\r\n# B = np.moveaxis( A, 3, 1)\r\n# C = np.transpose( A, (0,3,1,2))\r\nB = np.random.random((32, 6))\r\n\r\nprint(A.shape)\r\nprint(B.shape)\r\n\r\ndef mul(x):\r\n    A, B = x\r\n    print(K.int_shape(A))\r\n    print(K.int_shape(B))\r\n    return A*B\r\n\r\nA_t = K.variable(A)\r\nB_t = K.variable(B)\r\n# C_t = K.permute_dimensions(A_t, (0,3,1,2))\r\n\r\nC = K.map_fn(mul, (A_t, B_t), dtype='float32')\r\n\r\ny = K.eval(C)\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n(32, 6)\r\n(32, 6)\r\n(6,)\r\n(6,)\r\n```\r\n\r\nAnd w.r.t to this snippet, looking at my real code, the iterated elements should have the shape (None, ). But I don't know why one of it is having shape (None, 1).", "Tried shuffling the inputs to know if the way I am passing the inputs is affecting this! And no, it's not.. The problem only occurs with `y_pred` and `y_true` works fine.\r\n\r\nAnd I don't know why as both are of same shapes..", "@yongtang Something sparked in my mind which I had forgotten. My model has two output tensors of shape (batch_size, num_of_words). So what would be the value/shape/structure of y_pred in this case?", "Updated my code accordingly:\r\n```\r\n\r\nfrom keras import backend as K\r\n\r\n\r\ndef negative_avg_log_error(y_true, y_pred):\r\n\r\n    def sum_of_log_probabilities(true_and_pred):\r\n        y_true, y_pred_start, y_pred_end = true_and_pred\r\n\r\n        print(K.int_shape(y_true))\r\n        print(K.int_shape(y_pred_start))\r\n        print(K.int_shape(y_pred_end))\r\n        # ignore the below part, just a check for shapes\r\n        start_index = int(y_true[0])\r\n        end_index = int(y_true[1])\r\n        start_probability = y_pred[start_index]\r\n        end_probability = y_pred[end_index]\r\n        return K.log(start_probability) + K.log(end_probability)\r\n\r\n    print(K.int_shape(y_true))\r\n    print(K.int_shape(y_pred[0]))\r\n    print(K.int_shape(y_pred[1]))\r\n    batch_probability_sum = K.map_fn(sum_of_log_probabilities, (y_true, y_pred[0], y_pred[1]), dtype='float32')\r\n    return -K.mean(batch_probability_sum, axis=1)\r\n```\r\n\r\nAnd the outputs of the model are of shape (None, None). Here's its code snippet and output.\r\n```\r\n\r\nspan_begin_probabilities = SpanBegin(name='span_begin')([merged_context, modeled_passage])\r\nspan_end_probabilities = SpanEnd(name='span_end')([encoded_passage, merged_context, modeled_passage, span_begin_probabilities])\r\n\r\nmodel = Model([question_input, passage_input], [span_begin_probabilities, span_end_probabilities])\r\n```\r\n\r\nLast part of the output of model.summary():\r\n\r\n```\r\n__________________________________________________________________________________________________\r\nspan_begin (SpanBegin)          (None, None)         0           merged_context[0][0]             \r\n                                                                 bidirectional_decoder_0[0][0]    \r\n__________________________________________________________________________________________________\r\nspan_end (SpanEnd)              (None, None)         0           bidirectional_encoder[1][0]      \r\n                                                                 merged_context[0][0]             \r\n                                                                 bidirectional_decoder_0[0][0]    \r\n                                                                 span_begin[0][0]                 \r\n==================================================================================================\r\n```\r\n\r\nOutput of model.compile():\r\n\r\n```\r\n(None, None)\r\n(None, 1)\r\n(None, 1)\r\n(None,)\r\n(1,)\r\n(1,)\r\n```\r\n\r\nThe problem now is, the shape of y_pred[0] and y_pred[1] should be (None, None) instead of (None, 1).\r\n\r\nI think we are close to finding a solution. Please help...\r\n\r\nBy the way, just a thing I noticed -- int_shape() doesn't throw an error when called on a list instead of a tensor object. Why?", "@ParikhKadam When I run the code (https://github.com/ParikhKadam/bidaf-keras/blob/master/graph.ipynb) I had the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"graph.py\", line 13, in <module>\r\n    glove = Magnitude('data/magnitude/glove.6B.300d.magnitude')\r\n  File \"/usr/local/lib/python3.6/dist-packages/pymagnitude/__init__.py\", line 352, in __init__\r\n    self.path, log=self.log, _local=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pymagnitude/__init__.py\", line 2173, in download_model\r\n    remote_file_path)\r\nRuntimeError: The path to the Magnitude file at 'data/magnitude/glove.6B.300d.magnitude' could not be found. Also failed to find a valid remote model at the following URL: http://magnitude.plasticity.ai/data/magnitude/glove.6B.300d.magnitude\r\n```", "@yongtang Yes bro.. I need to make this module portable but it's my first time to deploy an open source project so I don't know how.. I will consider this as an issue in the project a d start working out on it. Will update soon..  And then you can continue working out to solve this issue here..\r\n\r\nThank you..", "@yongtang I have commented out some lines of the notebook as they were only wrote by me for the purpose of some practice. Now, we only have the code of model in graph.py. I will also remove the unnecessary comments after some time. As this is my first time creating an open source project and that too being the first project in Deep Learning, it's somewhat necessary for me..", "@ParikhKadam I tried to run the code but with the following error:\r\n```\r\n# python3\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from keras.layers import Input, TimeDistributed, LSTM, Bidirectional\r\nUsing TensorFlow backend.\r\n\r\n>>> from keras.models import Model\r\n>>> from layers import Highway, Similarity, C2QAttention, Q2CAttention, MergedContext, SpanBegin, SpanEnd\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'layers'\r\n>>> \r\n```\r\n\r\nCan you create a `graph.py` file with the error you intend to show, and makes sure a command line:\r\n```\r\npython graph.py\r\n```\r\ncould complete and expose the error you discussed?", "That can't be possible.. Maybe you haven't cloned the whole project. This file has its own dependencies which are already included in the project.\r\n\r\nThe module 'layers' is also a part of the project which is here:\r\nhttps://github.com/ParikhKadam/bidaf-keras/tree/master/layers", "@yongtang Bro.. As I said above, the issue was because of my model returning more than one outputs. And I tried to access them using a single loss function. As I google for a while, I came to know that I can combine both outputs using tf.stack() and use the returned tensor of this function as final output of my model. But I am still facing issues with shapes in the loss function.\r\n\r\nI had two output tensors x and y of shape (None, None) which is combined using z=tf.stack([x,y], axis=0). Now, I have a single output tensor z of shape (2, None, None). I pass this tensor z to loss function and am trying to get the original output tensors x and y of shape (None, None) using slicing.\r\n\r\nHere's the code snippet, the output and the error:\r\n\r\nmodel.summary():\r\n\r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\npassage_input (InputLayer)      (None, None, 600)    0                                            \r\n__________________________________________________________________________________________________\r\nquestion_input (InputLayer)     (None, None, 600)    0                                            \r\n__________________________________________________________________________________________________\r\nhighway_1_ptd (TimeDistributed) (None, None, 600)    0           passage_input[0][0]              \r\n__________________________________________________________________________________________________\r\nhighway_1_qtd (TimeDistributed) (None, None, 600)    0           question_input[0][0]             \r\n__________________________________________________________________________________________________\r\nbidirectional_encoder (Bidirect (None, None, 1200)   5764800     highway_1_qtd[0][0]              \r\n                                                                 highway_1_ptd[0][0]              \r\n__________________________________________________________________________________________________\r\nsimilarity_layer (Similarity)   (None, None, None)   3601        bidirectional_encoder[1][0]      \r\n                                                                 bidirectional_encoder[0][0]      \r\n__________________________________________________________________________________________________\r\ncontext_to_query_attention (C2Q (None, None, 1200)   0           similarity_layer[0][0]           \r\n                                                                 bidirectional_encoder[1][0]      \r\n__________________________________________________________________________________________________\r\nquery_to_context_attention (Q2C (None, None, 1200)   0           similarity_layer[0][0]           \r\n                                                                 bidirectional_encoder[0][0]      \r\n__________________________________________________________________________________________________\r\nmerged_context (MergedContext)  (None, None, 4800)   0           bidirectional_encoder[1][0]      \r\n                                                                 context_to_query_attention[0][0] \r\n                                                                 query_to_context_attention[0][0] \r\n__________________________________________________________________________________________________\r\nbidirectional_decoder_0 (Bidire (None, None, 1200)   25924800    merged_context[0][0]             \r\n__________________________________________________________________________________________________\r\nspan_begin (SpanBegin)          (None, None)         0           merged_context[0][0]             \r\n                                                                 bidirectional_decoder_0[0][0]    \r\n__________________________________________________________________________________________________\r\nspan_end (SpanEnd)              (None, None)         0           bidirectional_encoder[1][0]      \r\n                                                                 merged_context[0][0]             \r\n                                                                 bidirectional_decoder_0[0][0]    \r\n                                                                 span_begin[0][0]                 \r\n__________________________________________________________________________________________________\r\ncombine_outputs (CombineOutputs (2, None, None)      0           span_begin[0][0]                 \r\n                                                                 span_end[0][0]                   \r\n==================================================================================================\r\nTotal params: 31,693,201\r\nTrainable params: 31,693,201\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n```\r\n\r\nloss function:\r\n\r\n```\r\nfrom keras import backend as K\r\n\r\n\r\ndef negative_avg_log_error(y_true, y_pred):\r\n\r\n    def sum_of_log_probabilities(true_and_pred):\r\n        y_true, y_pred_start, y_pred_end = true_and_pred\r\n\r\n        print(K.int_shape(y_true))\r\n        print(K.int_shape(y_pred_start))\r\n        print(K.int_shape(y_pred_end))\r\n        start_index = int(y_true[0])\r\n        end_index = int(y_true[1])\r\n        start_probability = y_pred_start[start_index]\r\n        end_probability = y_pred_end[end_index]\r\n        return K.log(start_probability) + K.log(end_probability)\r\n\r\n    y_true = K.squeeze(y_true, axis=0)\r\n    y_pred_start = y_pred[0]\r\n    y_pred_end = y_pred[1]\r\n    print(type(y_pred))\r\n    print(K.int_shape(y_true))\r\n    print(K.int_shape(y_pred))\r\n    print(K.int_shape(y_pred_start))\r\n    print(K.int_shape(y_pred_end))\r\n    batch_probability_sum = K.map_fn(sum_of_log_probabilities, (y_true, y_pred_start, y_pred_end), dtype='float32')\r\n    return -K.mean(batch_probability_sum, axis=0)\r\n```\r\n\r\nmodel.compile(loss=negative_avg_log_error, optimizer='adadelta', metrics='loss'):\r\n\r\n```\r\n<class 'tensorflow.python.framework.ops.Tensor'>\r\n(None, None)\r\n(2, None, None)\r\n(None, None, 1)\r\n(None, None, 1)\r\n(None,)\r\n(None, 1)\r\n(None, 1)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-21-807b6d06a435> in <module>()\r\n----> 1 model.compile(loss=negative_avg_log_error, optimizer='adadelta', metrics='loss')\r\n\r\nC:\\Python36\\lib\\site-packages\\keras\\engine\\training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\r\n    340                 with K.name_scope(self.output_names[i] + '_loss'):\r\n    341                     output_loss = weighted_loss(y_true, y_pred,\r\n--> 342                                                 sample_weight, mask)\r\n    343                 if len(self.outputs) > 1:\r\n    344                     self.metrics_tensors.append(output_loss)\r\n\r\nC:\\Python36\\lib\\site-packages\\keras\\engine\\training_utils.py in weighted(y_true, y_pred, weights, mask)\r\n    402         \"\"\"\r\n    403         # score_array has ndim >= 2\r\n--> 404         score_array = fn(y_true, y_pred)\r\n    405         if mask is not None:\r\n    406             # Cast the mask to floatX to avoid float64 upcasting in Theano\r\n\r\nE:\\Deep Learning Material\\IMP for Project\\model\\scripts\\loss_function.py in negative_avg_log_error(y_true, y_pred)\r\n     24     print(K.int_shape(y_pred_start))\r\n     25     print(K.int_shape(y_pred_end))\r\n---> 26     batch_probability_sum = K.map_fn(sum_of_log_probabilities, (y_true, y_pred_start, y_pred_end), dtype='float32')\r\n     27     return -K.mean(batch_probability_sum, axis=0)\r\n\r\nC:\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in map_fn(fn, elems, name, dtype)\r\n   4319         Tensor with dtype `dtype`.\r\n   4320     \"\"\"\r\n-> 4321     return tf.map_fn(fn, elems, name=name, dtype=dtype)\r\n   4322 \r\n   4323 \r\n\r\nC:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py in map_fn(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)\r\n    457         back_prop=back_prop,\r\n    458         swap_memory=swap_memory,\r\n--> 459         maximum_iterations=n)\r\n    460     results_flat = [r.stack() for r in r_a]\r\n    461 \r\n\r\nC:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\r\n   3230       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\r\n   3231     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\r\n-> 3232                                     return_same_structure)\r\n   3233     if maximum_iterations is not None:\r\n   3234       return result[1]\r\n\r\nC:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants, return_same_structure)\r\n   2950       with ops.get_default_graph()._mutation_lock():  # pylint: disable=protected-access\r\n   2951         original_body_result, exit_vars = self._BuildLoop(\r\n-> 2952             pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2953     finally:\r\n   2954       self.Exit()\r\n\r\nC:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2885         flat_sequence=vars_for_body_with_tensor_arrays)\r\n   2886     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n-> 2887     body_result = body(*packed_vars_for_body)\r\n   2888     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n   2889     if not nest.is_sequence(body_result):\r\n\r\nC:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in <lambda>(i, lv)\r\n   3199         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\r\n   3200             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\r\n-> 3201         body = lambda i, lv: (i + 1, orig_body(*lv))\r\n   3202 \r\n   3203     if context.executing_eagerly():\r\n\r\nC:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py in compute(i, tas)\r\n    446       \"\"\"\r\n    447       packed_values = input_pack([elem_ta.read(i) for elem_ta in elems_ta])\r\n--> 448       packed_fn_values = fn(packed_values)\r\n    449       nest.assert_same_structure(dtype or elems, packed_fn_values)\r\n    450       flat_fn_values = output_flatten(packed_fn_values)\r\n\r\nE:\\Deep Learning Material\\IMP for Project\\model\\scripts\\loss_function.py in sum_of_log_probabilities(true_and_pred)\r\n     10         print(K.int_shape(y_pred_start))\r\n     11         print(K.int_shape(y_pred_end))\r\n---> 12         start_index = int(y_true[0])\r\n     13         end_index = int(y_true[1])\r\n     14         start_probability = y_pred_start[start_index]\r\n\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'Tensor'\r\n```\r\n\r\nThe problem:\r\n\r\n```\r\ny_pred_start = y_pred[0]\r\ny_pred_end = y_pred[1]\r\n\r\nprint(K.int_shape(y_pred)) -> (2, None, None)\r\nprint(K.int_shape(y_pred_start)) -> (None, None, 1) instead of (None, None)\r\nprint(K.int_shape(y_pred_end)) -> (None, None, 1) instead of (None, None)\r\n```\r\n\r\nWhere do this extra '1' come from after slicing?", "Please help... @yongtang ", "An update.. There's no problem with slicing or tf.map_fn(). There seems to be a problem with y_pred.\r\n\r\nReason:\r\nOn iterating, tf.map_fn() returned elements of (None, 1) and slicing too returns this extra 1 at the end which is (None, None, 1). And this happens only with y_pred and not with y_true.\r\n\r\nQuestion:\r\nSo, what's actually wrong with y_pred?", "@bstriner Sir, I have read many issue threads on loss function in Keras and seen you provide a solution there.. Maybe you can help me here.. ", "You can mark it as solved.. Took time but done.. Thank you..."]}, {"number": 24518, "title": "Keras crossentropy with logits as loss function", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13rc0 (Windows GPU build)\r\n- Are you willing to contribute it (Yes/No): **Yes**\r\n\r\n**Current behavior/state.**\r\nIn the keras api for tensorflow there does not seem to be a function for crossentropy with logits, (without sigmoid or softmax activations). This is quite confusing for those coming from the estimators (like me). I see that a `from_logits` argument is being added in the current release, but it needs to be used with a proxy function or a lambda.\r\n\r\n```\r\n...\r\ny_pred = keras.layers.Dense(10)(x)\r\n\r\nmodel = keras.Model(inputs=x, outputs=y_pred)\r\nmodel.compile(\"adam\", lambda x, y: keras.losses.sparse_categorical_crossentropy(x, y, from_logits=True))\r\n```\r\n\r\n**Will this change the current api? How?**\r\nInstead of introducing argument, I think it's better to have a new loss function like \r\n```\r\nmodel.compile(\"adam\", keras.losses.sparse_categorical_crossentropy_with_logits)\r\n```\r\n**or**\r\nloss name as string:\r\n```\r\nmodel.compile(\"adam\", \"sparse_categorical_crossentropy_with_logits\")\r\n```\r\n\r\n**Who will benefit with this feature?**\r\n- Using logits to calculate loss is actually efficient than activation then loss, so tf should encourage using crossentropy with logits, as it did in estimator API.\r\n- Those who switch from estimators to Keras API won't have any confusion.\r\n", "comments": ["@fchollet, \r\nplease take a look at this feature request in keras to add crossentropy with logits as loss function", "I recently went through the code for crossentropy loss functions in r2.0 branch.\r\n\r\n- It seems that even when we apply softmax or sigmoid activation and then loss, the activation is undone, ([see here](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/backend.py#L3926)) to recover logits for loss calculation. This answers the \"efficiency\" part of my question.\r\n- When coming from Estimator API, if we need to use logits directly, we can use losses class API, such as [CategoricalCrossentropy](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses/CategoricalCrossentropy), which takes from_logits as parameter, so need for proxy function is eliminated:\r\n```\r\n...\r\ny_pred = keras.layers.Dense(10)(x)\r\n\r\nmodel = keras.Model(inputs=x, outputs=y_pred)\r\nmodel.compile(\"adam\", keras.losses.SparseCategoricalCrossentropy(from_logits=True))\r\n```\r\nwhich is neater than above lambda solution.\r\n\r\nThus, with the current loss implementations and API, there is no need for this issue. So, I am closing the issue myself."]}, {"number": 24517, "title": "BUG: code throws exceptions when using COND_V2", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):archlinux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):v1.12.0-rc2-0-g748435b8ef 1.12.0-rc2\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):n/a\r\n- CUDA/cuDNN version:n/a\r\n- GPU model and memory:n/a\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport os\r\nos.environ['TF_ENABLE_COND_V2'] = '1'\r\nimport tensorflow as tf\r\na = tf.placeholder(tf.float32, shape=[3])\r\nu = tf.distributions.Normal(a, scale=1., validate_args=True)\r\n```\r\n\r\n**Other info / logs**\r\nIt fails with the following message. However the code works well without cond_v2.\r\n```\r\nTraceback (most recent call last):                                                                                                                                                                 \r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 933, in convert                                                                                    \r\n    x = ops.convert_to_tensor_or_indexed_slices(x)                                                                                                                                                 \r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1269, in convert_to_tensor_or_indexed_slices                                                        \r\n    value=value, dtype=dtype, name=name, as_ref=False)                                                                                                                                             \r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1307, in internal_convert_to_tensor_or_indexed_slices                                               \r\n    value, dtype=dtype, name=name, as_ref=as_ref)                                                                                                                                                  \r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1146, in internal_convert_to_tensor                                                                 \r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)                                                                                                                            \r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 6168, in _operation_conversion_error                                                                \r\n    name, as_ref))                                                                                                                                                                                 \r\nTypeError: Can't convert Operation 'NoOp' to Tensor (target dtype=None, name=None, as_ref=False)\r\n                                                                                                                                                                                                   \r\nDuring handling of the above exception, another exception occurred:\r\n                                                                                                                                                                                                   \r\nTraceback (most recent call last):\r\n  File \"a.py\", line 8, in <module>                                                                                                                                                                 \r\n    u = tf.distributions.Normal(a, scale=1., validate_args=True)\r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/ops/distributions/normal.py\", line 140, in __init__                                                                         \r\n    validate_args else []):\r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 204, in assert_positive                                                                             \r\n    return assert_less(zero, x, data=data, summarize=summarize)\r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 569, in assert_less                                                                                 \r\n    return control_flow_ops.Assert(condition, data, summarize=summarize)\r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 189, in wrapped                                                                                \r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 167, in Assert                                                                               \r\n    guarded_assert = cond(condition, no_op, true_assert, name=\"AssertGuard\")\r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func                                                                                 \r\n    return func(*args, **kwargs)\r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2040, in cond                                                                                \r\n    return cond_v2_impl.cond_v2(pred, true_fn, false_fn, name)\r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/ops/cond_v2_impl.py\", line 70, in cond_v2                                                                                   \r\n    true_name, true_fn, [], {})\r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 947, in func_graph_from_py_func                                                                    \r\n    func_outputs = nest.map_structure(convert, func_outputs)\r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 381, in map_structure                                                                                   \r\n    structure[0], [func(*x) for x in entries])\r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 381, in <listcomp>                                                                                      \r\n    structure[0], [func(*x) for x in entries])\r\n  File \"/home/wyx/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 939, in convert\r\n    (str(python_func), type(x)))\r\nTypeError: To be compatible with tf.contrib.eager.defun, Python functions must return zero or more Tensors; in compilation of <function no_op at 0x7f64b06a7ea0>, found return value of type <class\r\n'tensorflow.python.framework.ops.Operation'>, which is not a Tensor.\r\n```\r\n\r\ncc @skye, who seems to work on cond_v2 in https://github.com/tensorflow/tensorflow/issues/15874#issuecomment-436833266.", "comments": ["I was able to run your code snippet successfully using TF 1.12. Can you try using https://colab.sandbox.google.com/notebooks/welcome.ipynb to execute your code and confirm?", "![1226-11 07 10](https://user-images.githubusercontent.com/1381301/50454757-6cb49480-08fe-11e9-96f2-6c34d0542594.png)\r\nYou may be executing my code in the wrong way (e.g. import tensorflow before setting envvar).", "Yes you are right.", "Hi, thanks for reporting this! Just to let you know, I'm working on some other control flow bugs at the moment so I haven't had a chance to take a look at this yet, but this is on my radar.", "I am unable to repro this at head so it's possible this has been fixed since. Please retry and reopen if this is still an issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24517\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24517\">No</a>\n"]}, {"number": 24516, "title": "Tensorflow freezes on first Epoch then exits without exception", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10 x64\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9.0/7.2.0\r\n- GPU model and memory: 980M 4GB\r\n\r\n**I am trying to train a Siamese network.  When I get to the first epoch the program simply freezes for several minutes then exits with no diagnostic output. I'd built too large of a model before so I know it is allocating memory with CUDA. My assumption is this is an installation issue as it seems to be some sort of critical failure, but without additional diagnostic output i am unsure what to look into. **\r\n\r\n** Steps **\r\n1. Build Model\r\n2. Build Generator for fit_generator\r\n3. Execute Fit Generator for training\r\n\r\n**Any other info / logs**\r\n```\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom tqdm import tqdm\r\nimport numpy.random as rng\r\nfrom whalegenerator import WhaleGenerator\r\n\r\nimport tensorflow as tf\r\nfrom keras import backend as K\r\nfrom keras.regularizers import l2\r\nfrom keras.optimizers import SGD,Adam\r\nfrom keras.losses import binary_crossentropy\r\nfrom keras.models import Model, Sequential, save_model, load_model\r\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\r\nfrom keras.layers import Input, Conv2D, Lambda, Subtract, Dense, Flatten,MaxPooling2D\r\n\r\nProcessImages = True\r\nTrainPath = '.\\\\processed\\\\train'\r\nProcessedPath = '.\\\\processed'\r\nTrainTruthPath = '.\\\\train.csv'\r\n\r\ndef buildModel():\r\n    #We are building a saimese network, whaling problem should use comparison\r\n    input_shape = (256,256,1)\r\n    left = Input(input_shape)\r\n    right = Input(input_shape)\r\n\r\n    convnet = Sequential()\r\n    convnet.add(Conv2D(64,(9,9),activation='relu',input_shape=input_shape,kernel_initializer='random_normal',kernel_regularizer=l2(2E-4)))\r\n    convnet.add(MaxPooling2D())\r\n    convnet.add(Conv2D(128,(7,7),activation='relu',kernel_regularizer=l2(2E-4),kernel_initializer='random_normal',bias_initializer='random_normal'))\r\n    convnet.add(MaxPooling2D())\r\n    convnet.add(Conv2D(256,(5,5),activation='relu',kernel_initializer='random_normal',kernel_regularizer=l2(2e-4),bias_initializer='random_normal'))\r\n    convnet.add(MaxPooling2D())\r\n    convnet.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='random_normal',kernel_regularizer=l2(2e-4),bias_initializer='random_normal'))\r\n    convnet.add(Flatten())\r\n    convnet.add(Dense(2048,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer='random_normal',bias_initializer='random_normal'))\r\n\r\n    encodedL = convnet(left)\r\n    encodedR = convnet(right)\r\n\r\n    subtract = Subtract()([encodedL,encodedR])\r\n    diff = Lambda(lambda x: K.abs(x))(subtract)\r\n    prediction = Dense(1,activation='sigmoid',bias_initializer='random_normal')(diff)\r\n    siamese_net = Model(inputs=[left,right],outputs=prediction)\r\n    run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\r\n    siamese_net.compile(loss=\"binary_crossentropy\",optimizer=Adam(6E-5),options=run_opts,metrics=['accuracy'])\r\n    return siamese_net \r\n\r\nmodel = buildModel()\r\nprint(model.count_params())\r\nbatch_size = 32\r\ngenerator = WhaleGenerator(TrainPath,TrainTruthPath,batch_size,(256,256))\r\n\r\ntry:\r\n        result = model.fit_generator(generator=generator,epochs=100,steps_per_epoch=int(np.floor(len(generator)/batch_size)),max_queue_size=50,verbose=2,callbacks=[\r\n                ModelCheckpoint('.\\\\models\\\\whale_256.h5',save_best_only=True,monitor='accuracy')\r\n                ])\r\nexcept ValueError as e:\r\n        print(e)\r\nexcept Exception as e:\r\n        print(e)\r\nelse:\r\n        print('unkown error')\r\n\r\nprint('finished!')\r\n\r\n\r\n\r\n```\r\n\r\n```\r\nimport os\r\nimport cv2\r\nimport random\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom keras.utils import Sequence\r\n\r\nclass WhaleGenerator(Sequence):\r\n\r\n    #Number of items in the group\r\n    def __len__(self):\r\n        return int(np.floor(len(self.imageList)/2))\r\n\r\n    def __getitem__(self,index):\r\n        X = [np.empty((self.batch_size, *self.dim, 1)) for i in range(2)]\r\n        Y = np.empty((self.batch_size), dtype=float)\r\n\r\n        # Generate data\r\n        matches = 0\r\n        matchKeys = list(self.hasMatches.items())\r\n        for i in range(self.batch_size):\r\n            if self.batch_size - i < self.min_match and matches < self.min_match:\r\n                id1 = random.choice(matchKeys)[0]\r\n                images1 = self.imgGroups[id1]\r\n                X[0][i,] = images1[random.randint(0,len(images1)-1)]\r\n                X[1][i,] = images1[random.randint(0,len(images1)-1)]\r\n                Y[i] = 1.0\r\n                matches += 1\r\n            else:\r\n                item1 = self.imageList[random.randint(0,len(self.imageList)-1)]\r\n                id1 = item1[\"Id\"]\r\n                X[0][i,] = item1[\"Image\"]\r\n                images1 = self.imgGroups[id1]\r\n                if id1 != \"new_whale\" and len(images1) > 1 and random.random() <= 0.5: \r\n                    X[1][i,] = images1[random.randint(0,len(images1)-1)]\r\n                    Y[i] = 1.0\r\n                    matches += 1.0\r\n                else:\r\n                    item2 = self.imageList[random.randint(0,len(self.imageList)-1)]\r\n                    id2 = item2[\"Id\"]\r\n                    X[1][i,] = item2[\"Image\"]\r\n                    if id1 != \"new_whale\" and id1 == id2:\r\n                        Y[i] = 1.0\r\n                        matches += 1.0\r\n                    else:\r\n                        Y[i] = 0.0\r\n\r\n        return X, Y\r\n\r\n    def __init__(self, image_path, csv, batch_size, dim):\r\n\r\n        self.min_match = 3\r\n        self.imgGroups = { }\r\n        self.imageList = [ ]\r\n        self.hasMatches = { }\r\n        self.dim = dim\r\n        self.df = pd.read_csv(csv)\r\n        self.batch_size = batch_size\r\n\r\n        for i, row in self.df.iterrows():\r\n\r\n            _id = row[\"Id\"]\r\n            img = cv2.imread(os.path.join(image_path,row[\"Image\"]))\r\n\r\n            if img is None:\r\n                continue\r\n\r\n            img = img[:,:,0]\r\n            img = img.reshape((*self.dim,1))\r\n            self.imageList.append({ \"Id\": _id, \"Image\": img })\r\n            if not _id in self.imgGroups:\r\n                self.imgGroups[_id] = [img]\r\n            else:\r\n                if _id != \"new_whale\":\r\n                    self.hasMatches[_id] = True\r\n\r\n                self.imgGroups[_id].append(img)\r\n```\r\n\r\nWhat console output i do get\r\n```\r\nUsing TensorFlow backend.\r\n83297857\r\nEpoch 1/100\r\n2018-12-21 22:05:42.560621: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-12-21 22:05:43.369992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: GeForce GTX 980M major: 5 minor: 2 memoryClockRate(GHz): 1.1265\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.32GiB\r\n2018-12-21 22:05:43.381093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n```\r\n", "comments": ["I reinstalled with no-cache and it began working. Not sure why, but it's working now.", "@ChaseLewis Thanks! I think it was resolved. Please open new ticket if you see similar issue. Thanks!"]}, {"number": 24515, "title": "tf.count_nonzero not working on TPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not Tested\r\n- TensorFlow installed from (source or binary): Not sure (google colab)\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.2\r\n- GPU model and memory: None (TPU)\r\n\r\n**Describe the current behavior**\r\naccording to the documentation [here](https://cloud.google.com/tpu/docs/tensorflow-ops), tf.count_nonzero is available in TPU, but it returned compilation error when used in TPU.\r\n\r\n```\r\nCompilation failure: While rewriting computation to not contain X64 element types, XLA encountered an HLO for which this rewriting is not implemented: %reduce.124.120 = s64[] reduce(s64[1,4]{1,0} %convert.124.113, s64[] %convert.124.115), dimensions={0,1}, to_apply=%total_example_Sum-reduction119, sharding={maximal device=0}, metadata={op_type=\"Sum\" op_name=\"total_example/Sum\"}\r\n\tTPU compilation failed\r\n```\r\n\r\n**Describe the expected behavior**\r\ntf.count_nonzero is working for TPU\r\n\r\n**Code to reproduce the issue**\r\n[Codes](https://colab.research.google.com/drive/14sja1ttnwgxUlIXqCxuAhSh0L9uSa1te) are available in google colab\r\n```\r\ntf.reset_default_graph()\r\n\r\na = np.random.normal(0,10, [8,4]).astype(np.float32)\r\nb = np.random.normal(0,10, [8,4]).astype(np.float32)\r\n\r\nx = tf.convert_to_tensor(a)\r\ny = tf.convert_to_tensor(b)\r\n\r\n\r\ndef ops(x,y):\r\n    mask =tf.greater(x,y)\r\n    return tf.count_nonzero(mask, name='total_example')\r\n                     \r\ntpu_ops = tf.contrib.tpu.batch_parallel(ops,inputs=[x,y],num_shards=8)\r\n                     \r\nwith tf.Session(tpu_address) as sess:\r\n    sess.run(tf.contrib.tpu.initialize_system())\r\n    out = sess.run(tpu_ops)\r\n    sess.run(tf.contrib.tpu.shutdown_system())\r\n    \r\nprint(np.array(a>b).astype(np.uint8))\r\nprint(out)\r\n```\r\n\r\n**Other info / logs**\r\nThe problem seems to be the XLA compilation, casting to tf.int64 is not working in the current tf.count_nonzero chain.\r\nI made a code that similar but convert to float32 instead of int64, and it's working.\r\n```\r\ndef count_nonzero(x, axis=None, name='nonzero'):\r\n\r\n    count = tf.reduce_sum(\r\n        tf.cast(\r\n            tf.not_equal(x,tf.zeros([],dtype=x.dtype)), \r\n            dtype=tf.float32, # <-- the original tf.count_nonzero function cast to tf.int64\r\n            name='casting'\r\n        ), \r\n        axis=axis,\r\n        name=name\r\n    )\r\n    return count\r\n    #return tf.cast(count, tf.int64)\r\n\r\ndef ops(x,y):\r\n    mask =tf.greater(x,y)\r\n    return count_nonzero(mask, name='total_example')\r\n\r\ntpu_ops = tf.contrib.tpu.batch_parallel(ops,inputs=[x,y],num_shards=8)\r\n```\r\n\r\nFurthermore, simple casting from bool to int64 is working, so i guess this is related to the XLA optimization\r\n\r\n```\r\ndef ops(x,y):\r\n    mask =tf.greater(x,y)\r\n    return tf.cast(mask, tf.int64)\r\n\r\ntpu_ops = tf.contrib.tpu.batch_parallel(ops,inputs=[x,y],num_shards=8)\r\n```\r\n\r\nAll [codes](https://colab.research.google.com/drive/14sja1ttnwgxUlIXqCxuAhSh0L9uSa1te) are available on google colab.", "comments": ["Is this still valid in TF 2.x?", "@kurnianggoro I ran your code with `TF1.15.5` and I don't see any error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/75bb5440e96788ccd15b5b69948cc387/count_nonzero-tpu.ipynb).\r\n\r\nPlease note that there is no support for `TF1.x` anymore. Please feel free to upgrade to `TF2.x` and create a new issue if you face any issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24515\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24515\">No</a>\n", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "@jvishnuvardhan thanks a lot. I already check the gist. There should not be any problem now.  @tensorflowbutler Thank you, everything is okay now. And yes, I am moving to TF 2."]}, {"number": 24514, "title": "lite: tiny typo-like update to ResizeBilinear opt op", "body": "this is an replacement for #24501, which originally based branch r1.12 and pinged too many people when changed base to master.", "comments": []}, {"number": 24513, "title": "tfdbg memory leak", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\nSystem information\r\n\r\nWindows 7, 64\r\nTensorFlow installed from source\r\nTensorFlow version:1.8\r\nPython version:3.6\r\nInstalled using pip\r\nDescribe the problem\r\n\r\n        def get_tfdbg_watch_fn(self):\r\n        assert(len(self.reg_list)!=0),\"reg_list can not be the empty\"\r\n        regex = \"|\"\r\n        regex = regex.join(self.reg_list)\r\n        raw_regex = \"%r\" % regex\r\n        def watch_fn(fetches, feeds):\r\n            del fetches, feeds\r\n            return tfdbg.WatchOptions(\r\n                debug_ops=[\"DebugIdentity\"],\r\n                node_name_regex_whitelist=regex,\r\n                tolerate_debug_op_creation_failures=False)\r\n        return  watch_fn\r\n\r\n    def run(self,output,feed_fit):\r\n        if self.debug_session is None and self.dump_dirs_count()==0:\r\n            watch_fn = self.get_tfdbg_watch_fn()\r\n            self.debug_session = tfdbg.DumpingDebugWrapperSession(sess=self.sess, session_root=self.session_root, watch_fn=watch_fn,\r\n                                                    log_usage=False)\r\n        if self.debug_session is not None and self.debug_session.run_call_count>=self.dump_dirs_count() :\r\n            self.debug_session.run(output,feed_fit)\r\n            #self.sess.run(output,feed_fit)\r\n            del output\r\n            del feed_fit\r\n        return\r\nhi\r\ni run the tfdbg to get the intermediate tensor of the graph,the tfdbg  works,but when i run the self.debug_session.run more times i find the memory usage is grwoing biger and biger and it finally cause the programe aborted. i use the objgraph to check the python obj used,It dosen't grow after the second run,So i guess maybe there are some c obj leak in tfdbg.\r\nthks", "comments": ["@liuchangready,\r\n\r\nWe see you were using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to latest stable version and let us know if the issue still persists in newer versions.we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24513\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24513\">No</a>\n"]}, {"number": 24512, "title": "fixed function call of incompatible API", "body": "The old function call will throw an error.", "comments": ["Nagging Reviewer : You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 46 days with no activity and the `awaiting review` label has been applied.", "@isaprykin  Hi, Could you PTAL and approve.", "@ziyin-dl what error does it throw?", "@isaprykin It would throw an `AttributeError` complaining the function does not exist. Apparently it has been moved from `tf.compat.v1.estimator` to `tf.estimator`.", "> @isaprykin It would throw an `AttributeError` complaining the function does not exist. Apparently it has been moved from `tf.compat.v1.estimator` to `tf.estimator`.\r\n\r\nWhat version are you using?  I don't think I see that, because I can find that symbol.  It shouldn't be a problem with the nightly, say.\r\n>> import tensorflow as tf\r\n>> tf.compat.v1.estimator.inputs.numpy_input_fn\r\n<function [redacted...].tensorflow_estimator.python.estimator.inputs.numpy_io.numpy_input_fn>", "It seems like the rename to compat.v1 did not make it into 1.13 while the reference to it in examples/tutorials/layers/cnn_mnist.py did: \r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/inputs/numpy_input_fn\r\nhttps://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/examples/tutorials/layers/cnn_mnist.py#L137\r\n\r\nSorry for the confusion - this example should work out of the box in 1.14."]}, {"number": 24511, "title": "tf.compat.v1.estimator.inputs AP changed to tf.estimator.inputs", "body": ". The old function call will throw an error.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it.\n\nOn Fri, Dec 21, 2018, 13:27 googlebot <notifications@github.com wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here (e.g. I\n> signed it!) and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google.com/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/24511#issuecomment-449501934>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AH6E0CP5wIF9yTizCOIt2fTEla7ROmFbks5u7VJTgaJpZM4ZfHBj>\n> .\n>\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Alright, I switched to the email address I used to sign the CLA. Now it is\nsigned.\n\nOn Fri, Dec 21, 2018, 13:31 googlebot <notifications@github.com wrote:\n\n> We found a Contributor License Agreement for you (the sender of this pull\n> request), but were unable to find agreements for all the commit author(s)\n> or Co-authors. If you authored these, maybe you used a different email\n> address in the git commits than was used to sign the CLA (login here\n> <https://cla.developers.google.com/> to double check)? If these were\n> authored by someone else, then they will need to sign a CLA as well, and\n> confirm that they're okay with these being contributed to Google.\n> In order to pass this check, please resolve this problem and have the pull\n> request author add another comment and the bot will run again. If the bot\n> doesn't comment, it means it doesn't think anything has changed.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/24511#issuecomment-449502832>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AH6E0GoKiUjuy5xfBQyL4w8IGpXbW2Agks5u7VNCgaJpZM4ZfHBj>\n> .\n>\n"]}, {"number": 24510, "title": "pip install tensorflow - as described on tensorflow.org/install failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Windows 10 x64\r\n- Python version: 3.7.1 (via Chocolatey)\r\n\r\n\r\n**Describe the problem**\r\n\r\nTrying to install tensorflow fails when following the instuctions on tensorflow.org/install\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Starting with a clean Windows 10 x64 machine\r\n2. choco install python   (in Powershell, which installed 3.7.1)\r\n3. python -m pip install --upgrade pip\r\n4. pip install tensorflow    (as described at tensorflow.org/install)\r\n5. failed with:\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n\r\n**Any other info**\r\n\r\nGoogle returned this stack overflow with lots of accepts:\r\nhttps://stackoverflow.com/questions/38896424/tensorflow-not-found-using-pip\r\n\r\nIt suggested the following, which worked:\r\npython -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl\r\n\r\n**PowerShell output**\r\n\r\nPS C:\\WINDOWS\\system32> pip install tensorflow\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\nYou are using pip version 10.0.1, however version 18.1 is available.\r\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\r\nPS C:\\WINDOWS\\system32> python -m pip install --upgrade pip\r\nCollecting pip\r\n  Downloading https://files.pythonhosted.org/packages/c2/d7/90f34cb0d83a6c5631cf71dfe64cc1054598c843a92b400e55675cc2ac37/pip-18.1-py2.py3-none-any.whl (1.3MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3MB ...\r\nInstalling collected packages: pip\r\n  Found existing installation: pip 10.0.1\r\n    Uninstalling pip-10.0.1:\r\n      Successfully uninstalled pip-10.0.1\r\nSuccessfully installed pip-18.1\r\nPS C:\\WINDOWS\\system32> pip install tensorflow\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow", "comments": ["Follow the instructions mentioned here [https://www.tensorflow.org/install/source_windows](url). ", "Thanks for the suggestion. I was already able to get it working by reading the Stack Overflow notes - I just thought I'd raise the issue here that the basic instructions didn't work in this scenario, raising the barrier to entry for people less familiar with python, and leading quite a lot of people to seek answers on Stack Overflow. Not sure if this is something that can be fixed.", "TensorFlow does not support Python 3.7 yet. I am glad you were able to make it work seeking out to the community. Closing since its resolved. Thanks!"]}, {"number": 24509, "title": "Error message after importing Tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Windows 7, 64\r\n- \r\n- TensorFlow installed from source\r\n- TensorFlow version:1.12\r\n- Python version:3.6\r\n- Installed using pip\r\n- \r\n\r\n**Describe the problem**\r\n\r\nRan pip install tensorflow on Anaconda prompt, then wrote the following code on Jupiter notebook:\r\n\r\nimport tensorflow as tf\r\n\r\na = tf.Variable(1, name=\"a\")\r\nb = tf.Variable(2, name=\"b\")\r\nf = a + b\r\n\r\ninit = tf.global_variables_initializer()\r\nwith tf.Session() as s:\r\n    init.run()\r\n    print( f.eval() )\r\n\r\n got the following error:\r\n\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nPlease help!\r\n", "comments": ["Error code:\r\n\r\n![grafik](https://user-images.githubusercontent.com/42918639/50351733-5b0c6f00-0543-11e9-953e-10c78e5af8fc.png)\r\n\r\n![grafik](https://user-images.githubusercontent.com/42918639/50351992-21883380-0544-11e9-9928-376bb648b29e.png)\r\n\r\n![grafik](https://user-images.githubusercontent.com/42918639/50352020-42e91f80-0544-11e9-8707-7c370c2394be.png)\r\n\r\n", "The code is running in my system. Check your Anaconda and TensorFlow installation parts. ", "My Anaconda runs well. The installing of Tensorflow went well. Anybody has a applicable idea what i can do? ", "Facing same problem with tensorflow CPU version", "A friend told me there is an issue with Anaconda and Tensorflow. The\nsolution is to create a virtual environment.\n\n\nAm Fr., 28. Dez. 2018, 01:35 hat AksBrijSandy <notifications@github.com>\ngeschrieben:\n\n> Facing same problem with tensorflow CPU version\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24509#issuecomment-450262076>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Ao7i7wHGtkXhsp0Ubqu_sjRRfEcIgjP7ks5u9WdDgaJpZM4ZeL30>\n> .\n>\n", "@julesverneat Thanks for sharing your workaround. Closing this issue since activating virtual environment solves the issue for anaconda users."]}, {"number": 24508, "title": "Why stop gets backpropagated to the mean from the", "body": "I have noticed there is a request[Add a note that stop_gradient in moments does not change the gradient](https://github.com/tensorflow/tensorflow/pull/19609), but I wonder why stop gets backpropagated to the mean from the variance calculation.\r\nIn paper[Batch normalization layer (Ioffe and Szegedy, 2014)](https://arxiv.org/abs/1502.03167) Section 3, I have derived the formula:\r\n![](http://latex.codecogs.com/gif.latex?\\\\frac{\\\\partial\\L}{\\\\partial\\\\mu_B}=(\\\\sum_{i=1}^m\\\\frac{\\\\partial\\L}{\\\\partial\\hat\\{x_i}}\\\\cdot\\\\frac{\\hat\\{x_i}}{\\\\partial\\\\mu_B}+\\\\frac{\\\\partial\\L}{\\\\partial\\\\sigma_B^2}\\\\cdot\\\\frac{\\\\partial\\\\sigma_B^2}{\\\\partial\\\\mu_B})=(\\\\sum_{i=1}^m\\\\frac{\\\\partial\\L}{\\\\partial\\\\hat\\{x_i}}\\\\cdot\\\\frac{-1}{\\\\sqrt{\\\\sigma_B^2+\\\\epsilon}})+\\\\frac{\\\\partial\\L}{\\\\partial\\\\sigma_B^2}\\\\cdot\\\\frac{\\\\sum_{i=1}^m-2(x_i-\\\\mu_B)}{m})\r\nGradients of mean are from two parts, one is gradients of variance, so I have some doubts about this.\r\nBesides, I have mentioned ![](http://latex.codecogs.com/gif.latex?\\\\frac{\\\\sum_{i=1}^m-2(x_i-\\\\mu_B)}{m}) may be zero, or little, but I can't prove this, is this the answer for my doubt?", "comments": ["Nagging Reviewer @annarev: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 59 days with no activity and the `awaiting review` label has been applied."]}, {"number": 24507, "title": "Estimator inputs are not quantized using create_training_graph/create_eval_graph ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04 Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No \r\n- CUDA/cuDNN version: No \r\n- GPU model and memory: No\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nEstimator inputs are not quantized using create_training_graph/create_eval_graph as this layer has no activation.\r\n\r\nFrom https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py\r\n\r\nThough\r\n```\r\n# Activations that are supported by the quantization rewrite.\r\n_ACTIVATION_TYPES = {'Relu', 'Relu6', 'Identity'}\r\n```\r\nThe valid activations are only Relu*\r\n```\r\n_RELU_TYPES = {'Relu', 'Relu6'}\r\n...\r\n_PASS_THROUGH_OP = {'Reshape', 'Identity', 'BatchToSpaceND', 'SpaceToBatchND'}\r\n_VALID_ACTIVATION_OP = {'Relu', 'Relu6'}\r\n```\r\nThus, having something as\r\n```\r\ninput = tf.identity(input, name='q_input')\r\n...\r\ndoes not produce the expected result.\r\n\r\nThe alternative is using a manual layer as FakeQuantOp, but in this case max/min should be provided.\r\nIt would be required that create_training_graph/create_eval_graph automatically quantize the input as any other activation.\r\n\r\nThanks\r\n", "comments": ["Currently the tooling doesn't quantize inputs by design. This is because the rewriter targets image models with the expectation that inputs during inference are values from 0 to 255. You can then map these values to the float values used as inputs during training, for instance in the range [0.0f, 1.0f], using the mean_value and std_value flags.\r\nmean_value = the uint8 value that maps to floating point 0.\r\nstd_value = (uint8 range) / (float input range).\r\n\r\nWe understand that the current rewriter tool is brittle and we are actively working on more robust tooling to replace it.", "Hi @suharshs could it be possible to get a bit more information on that new tool?\r\nI have just send you an email to the mail account you have on your profile, subject referring this github issue.\r\n\r\nThank you very much for your time"]}]