[{"number": 32433, "title": "Fix major Adamax gpu bug.", "body": "PiperOrigin-RevId: 268469299", "comments": []}, {"number": 32432, "title": "Enable direct dilated convolution via CUDNN", "body": "For the dilated convolution, TF will perform space-to-batch transforms and then call the non-atrous convolution kernels.\r\n\r\nThis PR removes the transforms and directly call the atrous convolution kernels from CUDNN. \r\n\r\nfyi. @nluehr ", "comments": ["@houtoms do you want to add some test cases around this ?", "@tatianashp who is the best reviewer for this?", "@rthadur Yes, thx for reminder. I added some tests.", "@houtoms thank you ", "Hi @rthadur, it seems I cannot access the logs for failed tests and get `Uh oh! Something went wrong.` when I click the `Details`. Could you please help?", "@houtoms i ran tests again , please check back later", "@rthadur I still cannot access the logs.", "@houtoms i don't  see failures related to your code , let's wait for @aaroey to review , thanks for your patience!", "@reedwm seems you have experience with convolutional.py/nn_ops.py, could you help to take a look? Thanks.", "I redesigned the code to use the implementation selector. PTAL.", "@ezhulenev for layout optimizer\r\n", "@houtoms  Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Since the performance didn't materialize, let's close this PR."]}, {"number": 32431, "title": "[r2.0-CherrPick]:Add Ftrl cuda kernels.", "body": "PiperOrigin-RevId: 268335477", "comments": []}, {"number": 32430, "title": "Add Ftrl cuda kernels. Fix major Adamax cuda bug.", "body": "PiperOrigin-RevId: 268335477", "comments": []}, {"number": 32429, "title": "TensorBoard: gracefully handle deleted event files", "body": "as far as i can tell this is exactly the same as this issue https://github.com/tensorflow/tensorflow/issues/3267\r\n\r\nif i delete files from the logdir while tensorboard is running i get things like\r\n\r\n```\r\nE0911 11:27:19.441699 139989077399296 plugin_event_multiplexer.py:226] Unable to reload accumulator 'srresnet_voc_2x': [Errno 2] No such file or directory: b'/home/maksim/data/tensorboard/srresnet_voc_2x/events.out.tfevents.1568215513.maksim-desktop.105092.0'\r\nE0911 11:27:24.447706 139989077399296 plugin_event_multiplexer.py:226] Unable to reload accumulator 'srresnet_voc_2x': [Errno 2] No such file or directory: b'/home/maksim/data/tensorboard/srresnet_voc_2x/events.out.tfevents.1568215513.maksim-desktop.105092.0'\r\nE0911 11:27:29.453577 139989077399296 plugin_event_multiplexer.py:226] Unable to reload accumulator 'srresnet_voc_2x': [Errno 2] No such file or directory: b'/home/maksim/data/tensorboard/srresnet_voc_2x/events.out.tfevents.1568215513.maksim-desktop.105092.0'\r\nE0911 11:27:34.459517 139989077399296 plugin_event_multiplexer.py:226] Unable to reload accumulator 'srresnet_voc_2x': [Errno 2] No such file or directory: b'/home/maksim/data/tensorboard/srresnet_voc_2x/events.out.tfevents.1568215513.maksim-desktop.105092.0'\r\n```\r\n\r\ni'm using `tb-nightly==1.15.0a20190911` through pytorch.\r\n\r\ni'm not sure when reaping is supposed to happen e.g. as in https://github.com/tensorflow/tensorflow/issues/3267#issuecomment-292911229\r\nor how to manually force\r\n\r\n```\r\nWARNING:tensorflow:Deleting accumulator 'run1/test'\r\nWARNING:tensorflow:Deleting accumulator 'run1'\r\nWARNING:tensorflow:Deleting accumulator 'run2/test'\r\nWARNING:tensorflow:Deleting accumulator 'run2'\r\n```", "comments": []}, {"number": 32428, "title": "How to convert speech_recognition_frozen_graph.pb to FP32 or FP16?", "body": "How to convert speech_recognition_frozen_graph.pb to FP32 or FP16?\r\nCan you list convert command line for Intel openvino?\r\n\r\n", "comments": ["Perhaps Intel Developer Zone Forum for Intel Distribution of OpenVINO Toolkit can be a good platform to raise this question.\r\nSee https://software.intel.com/en-us/forums/computer-vision", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32428\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32428\">No</a>\n"]}, {"number": 32427, "title": "Exception in tflite with missing operators that model uses", "body": "**System information**\r\n- OS Platform and Distribution : Ubuntu 18.04.2 LTS\r\n- TensorFlow installed from binary\r\n- TensorFlow version tf-1.0.0 (or github SHA if from source):\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CAST, CONCATENATION, CONV_2D, EXPAND_DIMS, FULLY_CONNECTED, MAX_POOL_2D, RESIZE_BILINEAR, SOFTMAX. Here is a list of operators for which you will need custom implementations: DecodeJpeg.\r\n\r\nlink to a GraphDef or the model.\r\nhttps://github.com/RaghavPrabhu/Deep-Learning/tree/master/dogs_breed_classification\r\n", "comments": ["Note that TF 1.0.0 is extremely old and not supported", "As noted in the error message, DecodeJpeg (https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/decode-jpeg) has not been added to TFLite yet. Consider adding this as a [custom op](https://www.tensorflow.org/lite/guide/ops_custom) and perhaps then contributing it as a builtin operator.", "We don't have any immediate plans to add this as a builtin-op. However, you should be able to convert your model *after* the decode operation, and use platform-specific functionality to load the jpeg into memory and pass as an input into the tensor."]}, {"number": 32426, "title": "Diagnose", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Closing as nothing has been filled in in the template"]}, {"number": 32425, "title": "undefined reference to `tensorflow::str_util::EndsWith", "body": "On Kubuntu 18.08 \r\nTensorFlow 1.13 installed from source.\r\nPython 3.6\r\nBazel 19.2\r\nGCC 7.4.0\r\n \r\nIn the past, i have a tested program in ubuntu 16.04 and it run fine. I make the migration to ubuntu 18.04, recompile tensorflow for c++ and obtain the libraries libtensorflow_cc.so and libtensorflow_framework.so.\r\nWhen i tried to compile my program using \"cmake .. && make\"  see the following messages:\r\n\r\ntensorflowlabelimageclassification.cpp:(.text+0x2bff): undefined reference to `tensorflow::str_util::EndsWith(std::basic_string_view<char, std::char_traits<char> >, std::basic_string_view<char, std::char_traits<char> >)'\r\ntensorflowlabelimageclassification.cpp:(.text+0x2d3c): undefined reference to `tensorflow::str_util::EndsWith(std::basic_string_view<char, std::char_traits<char> >, std::basic_string_view<char, std::char_traits<char> >)'\r\ntensorflowlabelimageclassification.cpp:(.text+0x2ee3): undefined reference to `tensorflow::str_util::EndsWith(std::basic_string_view<char, std::char_traits<char> >, std::basic_string_view<char, std::char_traits<char> >)'\r\n\r\nAny help?\r\nThanks.\r\nDibet\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@dibet, The TensorFlow team does not officially support cmake, sorry. Please try out with Bazel.", "Hi,\r\nOk. But tensorflow is compiled with Bazel without trouble. \r\nWhen i run \r\n      nm -Ca libtensorflow_cc.so \r\n      nm -Ca libtensorflow_framework.so \r\ni only see this function   \r\n      U tensorflow::str_util::EndsWith(absl::string_view, absl::string_view)\r\n      T tensorflow::str_util::EndsWith(absl::string_view, absl::string_view)\r\n\r\nAnd into the str_util.h see only the following declaration:\r\n      bool EndsWith(StringPiece text, StringPiece suffix);\r\n\r\n\r\nAs i see, my functions library don't return bool as a result.\r\n\r\nThanks.\r\nDibet\r\n\r\n\r\n", "Hi, i just write to inform how i resolve my problem.\r\nIn the past (ubuntu 16.04 and gcc 5) i was compiling using  -std=c++17.\r\nNow, on ubuntu 18.04 and gcc 7 i need to use  -std=c++14 \r\nI don't know the reason but i fix it in that way.\r\nThanks.\r\n", "Thanks for sharing your resolution. Closing since its resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32425\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32425\">No</a>\n"]}, {"number": 32424, "title": "No rule to make target '/home/ai002/tensorflow_Cpp/add0/tensorflow_cpp2/tensorflow-1.11.0/tensorflow/contrib/makefile/gen/proto/tensorflow/core/util/test_log.pb.cc', needed by '/home/ai002/tensorflow_Cpp/add0/tensorflow_cpp2/tensorflow-1.11.0/tensorflow/contrib/makefile/gen/obj/tensorflow/contrib/boosted_trees/proto/learner.pb.o'.  Stop.", "body": "tensorflow 1.11.0\r\nbazel 0.18.0\r\nWhen i run sudo ./tensorflow/contrib/makefile/build_all_linux.sh...... help me ! thank you hardly!!!\r\nNo rule to make target '/home/ai002/tensorflow_Cpp/add0/tensorflow_cpp2/tensorflow-1.11.0/tensorflow/contrib/makefile/gen/proto/tensorflow/core/util/test_log.pb.cc', needed by '/home/ai002/tensorflow_Cpp/add0/tensorflow_cpp2/tensorflow-1.11.0/tensorflow/contrib/makefile/gen/obj/tensorflow/contrib/boosted_trees/proto/learner.pb.o'.  Stop.", "comments": ["@BlackDeal ,\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32424\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32424\">No</a>\n"]}, {"number": 32423, "title": "Fix few typos in the annotations and docs", "body": "fix some typos:\r\n'a error' change into 'an error'\r\n'an unique' change into 'a unique'\r\n'a http' change into 'an http'\r\n'a hdfs' change into 'an hdfs'", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32423) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32423) for more info**.\n\n<!-- ok -->", "Sorry the MLIR code in *third_party* cannot be changed, this is a read-only mirror from https://github.com/tensorflow/mlir ; please open a PR there.", "thank you @joker-eph , @dengziming can you please exclude those file ", "> thank you @joker-eph , @dengziming can you please exclude those file\r\n@joker-eph \r\nsorry for my late, I have excluded those files, thank you"]}, {"number": 32422, "title": "[lite/micro] quantized dense layer is not supported.", "body": "The following error is thrown while running a quantized tf lite model converted from a tensorflow model using tf lite **micro**.\r\n\r\n```\r\nQuantized FullyConnected expects output data type uint8 or int16\r\nNode FULLY_CONNECTED (number 1) failed to invoke with status 1\r\n```\r\n\r\n# Environment information\r\n- The master branch is used to compile lite/micro.\r\n- OS: Ubuntu 16.04\r\n- Gcc version: 5.4.0\r\n\r\n# tensorflow version\r\n1.13.1\r\n\r\nThe script to convert the tensorflow model to a tflite model is as follows:\r\n\r\n```python\r\n#!/usr/bin/env python\r\n\r\nfrom __future__ import print_function, absolute_import, division\r\n\r\nimport argparse\r\nimport os\r\nimport warnings\r\n\r\nimport numpy as np\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\nwarnings.simplefilter(action='ignore', category=FutureWarning)\r\n\r\nimport tensorflow as tf\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\n\r\n\r\ndef test():\r\n    # first build a graph\r\n    g = tf.Graph()\r\n\r\n    with g.as_default():\r\n        in_shape = [1, 10, 9, 3]\r\n        x = tf.placeholder(tf.float32, shape=in_shape, name=\"input\")\r\n\r\n        w = np.random.rand(3, 3)\r\n        b = np.random.rand(9)\r\n\r\n        layer = tf.layers.conv2d(inputs=x,\r\n                                 filters=9,\r\n                                 kernel_size=w.shape,\r\n                                 name=\"layer1\",\r\n                                 kernel_initializer=tf.constant_initializer(w),\r\n                                 bias_initializer=tf.constant_initializer(b))\r\n        relu = tf.nn.relu(layer)\r\n\r\n        relu = tf.reshape(relu, (1, -1))\r\n\r\n        w2 = np.random.rand(relu.shape[-1])\r\n        b2 = np.random.rand(9)\r\n        y = tf.layers.dense(inputs=relu,\r\n                            units=9,\r\n                            kernel_initializer=tf.constant_initializer(w2),\r\n                            bias_initializer=tf.constant_initializer(b2),\r\n                            name=\"output\")\r\n\r\n    with tf.Session(graph=g) as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        converter = tf.lite.TFLiteConverter.from_session(sess,\r\n                                                         input_tensors=[x],\r\n                                                         output_tensors=[y])\r\n\r\n        print(\"before conversion\")\r\n        in_data = np.arange(270).reshape(x.shape)\r\n        out = sess.run(y, feed_dict={x: in_data})\r\n        print(out)\r\n\r\n        converter.post_training_quantize = True  # for 1.13.1\r\n\r\n        tflite_model = converter.convert()\r\n        model_filename = \"xxx.tflite\"\r\n        open(model_filename, \"wb\").write(tflite_model)\r\n\r\n        cc_src = \"xxx.h\"\r\n        os.system(\"xxd -i {} > {}\".format(model_filename, cc_src))\r\n\r\n    # after conversion\r\n    interpreter = tf.lite.Interpreter(model_path=model_filename)\r\n    interpreter.allocate_tensors()\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n    input_shape = input_details[0]['shape']\r\n    interpreter.set_tensor(input_details[0]['index'],\r\n                           in_data.astype(np.float32))\r\n    interpreter.invoke()\r\n    output_data = interpreter.get_tensor(output_details[0]['index'])\r\n    print(\"after conversion\")\r\n    print(output_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    print(tf.__version__)\r\n    print(tf.__git_version__)\r\n    print(tf.__compiler_version__)\r\n    np.random.seed(777)\r\n    test()\r\n```\r\n\r\nOutput of the above script is:\r\n\r\n```\r\n1.13.1\r\nb'v1.13.1-0-g6612da8951'\r\n4.8.5\r\nbefore conversion\r\n[[327913.94 327228.56 327169.47 327523.12 326526.3  327757.97 328415.53\r\n  326741.97 327555.22]]\r\nafter conversion\r\n[[327600.28 326899.12 326834.88 327196.44 326194.53 327430.94 328063.88\r\n  326408.12 327227.72]]\r\n```", "comments": ["It looks like you're generating a model which uses hybrid quantization.  Micro currently only supports full-quanitzed or full-float models.  If you want to model to work today, the best way forward is to avoid quantizing the model in your conversion step.  When we finish up landing the int8 quantized version of CONV2D, you will be able to run a fully quantized model.  In order to fully quantize your model, you will need to provide a representative dataset.", "@njeffrie \r\ncould you tell me how to use `full-quantized` only? I don't care whether it is full-quantized or hybrid-quantized."]}, {"number": 32421, "title": "Do I have to compile a TF.keras model that's part of a compiled model in order for its added losses to be taken into account during training?", "body": "I have already tried to get an answer [here](https://stackoverflow.com/questions/57851525) to this question because I cannot figure it out based on the documentation:\r\n\r\nI have a VAE tf.keras model and it consists of an encoder and decoder model. I want to regularize the encoder and the decoder with different custom terms (using add_loss()) while training VAE (using VAE.fit()).\r\n\r\nIn pseudocode:\r\n\r\n```\r\nencoder = model(input, encoding)\r\ndecoder = model(encoding, output)\r\nencoder.add_loss(custom_loss_1)\r\ndecoder.add_loss(custom_loss_2)\r\nvae = model(input, output)\r\nvae.compile(optimizer, standard_keras_loss_function)\r\nvae.fit(args)\r\n```\r\n\r\nNote that I can't add the custom losses to the wrapper model (vae) because I want them to be backpropagated only in their respective submodels (encoder's loss shouldnt affect decoder's gradients and vice versa). I also can't fit encoder and decoder separately, VAEs benefit from jointly training the encoder and decoder.\r\n\r\nI'm facing two challenges:\r\n\r\nAlthough I see no error during training, I have no easy way to heck if encoder and decoder losses are taken into account when fitting bar wrapper model. Do I need to compile encoder and decoder for the added custom losses to be taken into account?\r\n\r\nI don't want the 'standard_keras_loss' (binary crossentropy) to be backpropagated beyond the decoder, it should only affect the decoder's gradients. However, vae.compile() method requires me to add at least one loss to the VAE model. Thus, the only way to prevent this loss from backpropagating to encoder is to try and add a stop_gradient lambda layer after the encoder but this would disconnect my encoder from my decoder (in fact I'm getting disconnected graph error wherever I try to add the stop_gradient layer).\r\n\r\nIs there a TF.keras way to do this without having to write custom training loop? I'm trying to keep a clean code.", "comments": ["@kristofgiber Can you please take a look at this [answer](https://stackoverflow.com/questions/54400971/keras-clean-implementation-for-multiple-outputs-and-custom-loss-functions) and let me know if it helps. Thanks!", "Closing this issue as it has been inactive for more than 5 days. Please add additional comments and we can open the issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32421\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32421\">No</a>\n"]}, {"number": 32420, "title": "TF2.0 - Multiple calls to Keras .fit and .evaluate makes RAM explode and is 25x slower", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0-dev20190909\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n(Everything is done on the CPU)\r\nConsecutive calls to either `.fit()` or `.evaluate()` increase the RAM used, even if calling with the same data. The calls takes approximately 10 times longer than with TF1.x\r\n\r\n**Describe the expected behavior**\r\nI expect the RAM usage to remain constant just like in TF1.x\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nfrom memory_profiler import profile\r\nfrom time import time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential([tf.keras.layers.Dense(100, activation=tf.nn.softmax)])\r\nmodel.compile(loss='mse', optimizer='sgd')\r\n\r\n@profile\r\ndef eval(x, y):\r\n    model.evaluate(x, y)\r\n\r\nx = np.random.normal(size=(1,100))\r\ny = np.random.normal(size=(1,100))\r\n\r\nfor i in range(100000):\r\n    print('iteration', i)\r\n    tic = time()\r\n    eval(x, y)\r\n    print('timeit', time() - tic)\r\n```\r\n\r\nUsing TF2.0. 229MB RAM used and evaluate completed in 99ms\r\n```\r\niteration 20\r\nTrain on 1 samples\r\n1/1 [==============================] - 0s 10ms/sample - loss: 1.0597\r\nFilename: reproduce_keras_oom.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     9    228.8 MiB    228.8 MiB   @profile\r\n    10                             def eval(x, y):\r\n    11    229.1 MiB      0.2 MiB       model.evaluate(x, y)\r\n\r\ntimeit 0.09978580474853516\r\n```\r\n\r\nUsing TF2.0 1508MB RAM used after calling `.evaluate()` 3312 times.\r\n```\r\niteration 3312\r\n1/1 [==============================] - 0s 4ms/sample - loss: 1.0205\r\nFilename: reproduce_keras_oom.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     9   1508.3 MiB   1508.3 MiB   @profile\r\n    10                             def eval(x, y):\r\n    11   1508.7 MiB      0.4 MiB       model.evaluate(x, y)\r\n\r\ntimeit 0.09004998207092285\r\n```\r\n\r\nUsing TF1.x, the RAM used is not increasing over consecutive calls of `.evaluate()`. RAM stays at 176MB indefinitely (iteration 5100 below). **Also note that it is 25 times faster!**\r\n```\r\niteration 5100\r\n1/1 [==============================] - 0s 1ms/sample - loss: 1.2716\r\nFilename: reproduce_keras_oom.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     9    176.0 MiB    176.0 MiB   @profile\r\n    10                             def eval(x, y):\r\n    11    176.0 MiB      0.0 MiB       model.evaluate(x, y)\r\n\r\ntimeit 0.004405021667480469\r\n```\r\n\r\nI just discovered that wrapping `(x, y)` into a `tf.data.Dataset` does not have this issue!\r\nModified code:\r\n```python\r\nfrom memory_profiler import profile\r\nfrom time import time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential([tf.keras.layers.Dense(100, activation=tf.nn.softmax)])\r\nmodel.compile(loss='mse', optimizer='sgd')\r\n\r\n@profile\r\ndef eval(dataset):\r\n    model.evaluate(dataset)\r\n\r\nx = np.random.normal(size=(1,100))\r\ny = np.random.normal(size=(1,100))\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((x, y))\r\ndataset = dataset.batch(1)\r\n\r\nfor i in range(100000):\r\n    print('iteration', i)\r\n    tic = time()\r\n    eval(dataset)\r\n    print('timeit', time() - tic)\r\n```\r\n\r\nUsing `tf.data.Dataset`, there is no exploding RAM. 217 MB RAM used after calling `.evaluate()` 8154 times. It is also only 2.5 times slower than TF1.x\r\n```\r\niteration 8154\r\n1/1 [==============================] - 0s 3ms/step - loss: 0.9972\r\nFilename: reproduce_keras_oom.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     9    217.6 MiB    217.6 MiB   @profile\r\n    10                             def eval(dataset):\r\n    11    217.6 MiB      0.0 MiB       model.evaluate(dataset)\r\n\r\ntimeit 0.010456085205078125\r\n```\r\n\r\n", "comments": ["@grananqvist \r\n\r\nI tried reproducing the issue. I am attaching the [gist](https://colab.sandbox.google.com/gist/ravikyram/211ae1084a4b922d38cf36bd122ffcad/untitled180.ipynb) for your reference. Is this the expected behavior?.Thanks!", "From what I see in the notebook, using profile to measure the RAM consumption does not work in jupyter", "@grananqvist Sorry for the delay in response. Can you please try `tf-nightly` and compare that with `TF1.15.0rc3`. I see similar runtime (~0.004) with `tf-nightly` and `TF1.15.0rc3`. Please let us know what you think. Thanks!", "@grananqvist Did you had time to review my comments? Thanks!", "Sorry I have not had the time to test this yet.", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32420\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32420\">No</a>\n", "Would using `tf.function` help in this situation?"]}, {"number": 32419, "title": "Masking in BERT", "body": "In the original paper of BERT it is said:\r\n\r\n> Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine-tuning, as the [MASK] symbol never appears during the fine-tuning stage.\r\n\r\nLet's consider a sentence \"I am a Liverpool fan\" which with 40% masking will be transformed into \"I [MASK] a [MASK] fan\". When predicting the first [MASK], will it be predicted by a phrase \"I [MASK] a fan\", excluding the second [MASK] or \"I [MASK] a [MASK] fan\", by a full sentence?\r\n\r\nAnd what is the purpose of replacing 10% of masked tokens with themselves? Does it mean they will not be predicted? Or we will predict them, having themselves in the context (like predicting the first [MASK] by \"I am a [MASK] fan\"?\r\n\r\nWill be very grateful for any help!", "comments": ["Hi,\r\n\r\nI will try to answer your questions. Keep in mind, however that a) my knowledge of BERT only comes from the same paper you read (plus the associated source code and my personal experience re-implementing it for TF 2.0) and b) other design choices could be achieved through an alternative implementation.\r\n\r\nThat being said:\r\n\r\n> When predicting the first [MASK], will it be predicted by a phrase \"I [MASK] a fan\", excluding the second [MASK] or \"I [MASK] a [MASK] fan\", by a full sentence?\r\n\r\nSecond option :-) Since the training is conducted on a single forward-pass, the prediction of the first masked token will be based on the encoding of the sentence with both masked tokens. In other words, the sentence \"I [MASK] a [MASK] fan\" will be fed to the network, embedded, processed by the stack of Transformer encoder blocks, and in the end produce an encoding matrix of shape (n_tokens, model_dim). Then, the masked-tokens-prediction pre-training output layer will further process this matrix and produce predicted tokens probability distributions for each and every tokens marked as masked (both the actual [MASK] ones, and those randomly shifted or not shifted at all yet set to be predicted).\r\n\r\n> And what is the purpose of replacing 10% of masked tokens with themselves? Does it mean they will not be predicted?\r\n\r\nTo start with the practical point : the masked tokens replaced with themselves are indeed predicted (and made part of the MLM loss). In other words, what happens is that the MLM loss is based both on the ability of the model to a) predict masked tokens based on context, b) rebuild input non-masked tokens based on their encoding (the case you are asking about), and c) put such tokens in doubt based on their context (the case of tokens replaced by random ones).\r\n\r\nThis point is arguable, but the idea is that we want to ensure, at the same time, that the encoding retains information on the tokens (and does not just focus on [MASK] ones) - this is the \"I can predict a token identical to the input\" part - AND incorporates contextual elements - this is the \"I can predict the right token instead of the random input one\" part. In a sense, you would hope that the model is both able to trust and doubt the input information, and produce encodings of non-masked tokens that retain and embed the valuable information needed for downward tasks (including tasks that would not imply any masked tokens).\r\n\r\nI hope this is somehow clearer than the few lines in the paper and can help you! To be honest I struggled a lot with understanding the rationale behind the design (independently from the fact that it is relatively easy to implement).", "@pandrey-fr geniously! Thanks a lot, the whole picture has become apparent to me :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32419\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32419\">No</a>\n", "@Aktsvigun That is nice to read! You are welcome :)"]}, {"number": 32418, "title": "[INTEL MKL] Add primitive cache for mkl concat", "body": "Add support for primitive reuse in mkl concat with all the inputs as TF tensors.\r\n\r\nFiles modified\r\ntensorflow/core/kernels/mkl_concat_op.cc\r\n\r\nPlease review", "comments": ["@penpornk  Thank you for your valuable review comments. I will address the review comments and submit the patch ASAP", "@penpornk Sorry for the delay. Addressed all the review comments, please review it"]}, {"number": 32417, "title": "[lite/micro] Override operator delete in memory planner", "body": "Fix #32416 .\r\nOverride operator delete.", "comments": ["@wangtz \r\n`LinearMemoryPlanner` is never used so it does not cause link time error.\r\nI'll fix it in the next patch.", "@csukuangfj Can you please resolve conflicts? Thanks!", "@gbaned \r\nDone.", "It takes nearly two months to merge such a small fix !!!\r\n\r\nIt's still not merged!", "@gbaned Hi Dero, could you help merge the PR?\r\nThanks!", "@csukuangfj Can you please check build failures? Thanks!", "@gbaned \r\nThe error log from <https://source.cloud.google.com/results/invocations/117384e4-7ee8-41a2-a56b-c09ba03c8320/targets/%2F%2Ftensorflow%2Ftools%2Fci_build%2Fbuilds:gen_android_out/log>\r\nhas nothing to do with my commit.\r\n```\r\n[0 / 80] [Prepa] BazelWorkspaceStatusAction stable-status.txt\r\nERROR: /tmpfs/tmp/bazel/external/jsoncpp_git/BUILD.bazel:5:1: C++ compilation of rule '@jsoncpp_git//:jsoncpp' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 29 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\ngcc: error: unrecognized command line option '-std=c++14'\r\nTarget //tensorflow/core/common_runtime/eager:execute failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/core/BUILD:242:1 C++ compilation of rule '@snappy//:snappy' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 27 argument(s) skipped)\r\n\r\n```", "@csukuangfj Could you please address Ubuntu Sanity errors? Thanks!", "> ERROR: error loading package 'tensorflow/compiler/tests': Unable to load file '//tensorflow:tensorflow.google.bzl': file doesn't exist\r\n\r\nfrom https://source.cloud.google.com/results/invocations/6070c286-565c-49ca-bdf3-df7db18f76e2/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_sanity_out;shard=1;run=1;attempt=3/log\r\n\r\nis not caused by this commit.\r\n\r\n@gbaned \r\n\r\nI only added 4 lines and have tested it on my computer. Which line can cause the CI's failure?", "@wangtz Could you PTAL. Thanks!", "Nit: Please don't include issue number in title, include it in the description of the PR. And give the PR a descriptive title", "@csukuangfj  Could you please address above internal error and resolve conflicts? Thanks!", "@gbaned \r\nyou've mentioned your two colleagues but neither of them replies.  I do NOT know\r\nwhy the file `compatibility.h` is missing and I guess there must be some reasons for\r\nits removal.\r\n\r\n**Three** months have passed and such a small change is still not merged. I can think of the\r\nhigh communication cost in such a large project.\r\n\r\nI will resolve the conflicts tomorrow.", "Thanks for your patience, sorry this has taken so long, we're trying to improve the process so it doesn't take so long in the future. Feel free to email me directly at petewarden@google.com if you find yourself stuck with a long-running PR like this again.", "Changes have been submitted internally , waiting for auto-merge to happen. Thank you "]}, {"number": 32416, "title": "[lite/micro] missing delete() in GreedyMemoryPlanner", "body": "https://github.com/tensorflow/tensorflow/blob/8c0df1fa0b0490d8b1e54d7b019e2b2242ad6718/tensorflow/lite/experimental/micro/memory_planner/greedy_memory_planner.h#L43\r\n\r\ndoes not override `void operator delete(void *p)` which results in link time error.", "comments": ["@csukuangfj Is this resolved or still an issue? Thanks!", "@jvishnuvardhan \r\nwhen this pullrequest https://github.com/tensorflow/tensorflow/pull/32417 is merged,\r\nthis issue should be closed by GitHub automatically; but more than 3 months have passed,\r\nit is still not merged.\r\n\r\n", "@csukuangfj I see the reviewer approved the PR. So it will be merged soon. Thanks!", "@jvishnuvardhan \r\n\r\nThings are not always that easy like you thought. You can see that the pullrequest\r\nhas been approved for multiple times, but nothing happens when  the`ready to pull`\r\nlabel is added.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32416\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32416\">No</a>\n"]}, {"number": 32415, "title": "TensorFlow Lite for micro controllers support BatchNorm?", "body": "I'm looking on this file that should list supported operations for micro controllers:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/kernels/all_ops_resolver.cc\r\n\r\nI don't see an explicit support in batch norm layer.\r\n\r\nDoes TF Lite for micro controllers support batch normalization?\r\ngenerally, does it support basic architectures like resnet and mobilenet v2?", "comments": ["For mobilenet v2 TF Lite support, this thread can be useful https://github.com/tensorflow/tensorflow/issues/28161", "The TFLite conversion tools support folding the batch norm ops into the the convolutions.  The resulting tflite models are supported on tflite for micros.  The micro_vision example in the code base utilizes a mobilenet-derived architecture, for example.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32415\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32415\">No</a>\n"]}, {"number": 32414, "title": "fix lite quantization_spec docment error", "body": "", "comments": []}, {"number": 32413, "title": "How to split neural network into different GPU devices ?", "body": "Hi, I am working on a Video Segmentation project. Due to the limit of GPU memory (GTX 1080TI),  OOM error has occurred. I do know this error can be avoided if I squeeze my model or just use other GPUs with higher memory. But, does Tensorflow support splitting one neural network into different GPU devices? i.e. , I have 2 GPUs(A and B) and the model definitation is shown as following:\r\n```\r\n    input = keras.layers.Input((28, 28))\r\n    f = keras.layers.Flatten()(input)\r\n    #I want GPU A finish this part\r\n    part1 = keras.layers.Dense(128, activation=tf.nn.relu)(f)    \r\n     #After part1 is finished, GPU B will finish this part\r\n    part2 = keras.layers.Dense(10, activation=tf.nn.softmax)(part1)   \r\n```\r\nI want  GPU A finish `part1 = keras.layers.Dense(128, activation=tf.nn.relu)(f)  ` and GPU B finish `part2 = keras.layers.Dense(10, activation=tf.nn.softmax)(part1) `.", "comments": ["@Liang-yc, Did you ever try the distributed training with Keras. Please refer the [Tensorflow doc](https://www.tensorflow.org/beta/tutorials/distribute/keras). And also please take a look at [here](https://www.tensorflow.org/beta/guide/using_gpu#limiting_gpu_memory_growth). Thanks!", "> @Liang-yc, Did you ever try the distributed training with Keras. Please refer the [Tensorflow doc](https://www.tensorflow.org/beta/tutorials/distribute/keras). And also please take a look at [here](https://www.tensorflow.org/beta/guide/using_gpu#limiting_gpu_memory_growth). Thanks!\r\n\r\nThanks for your reply. I added some codes in my project as following:\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    tf.config.experimental.set_virtual_device_configuration(\r\n        gpus[0],\r\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),\r\n         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    print(e)\r\n```\r\nAnd it doesn't work no matter how large `memory_limit` is; the OOM error occurred again.  My Tensorflow version is 1.14, and it only test on my machine with GTX 1660. ", "@Liang-yc,In order to expedite the trouble-shooting process, please provide a complete standalone code to reproduce the issue reported here. Thanks!\r\n", "OK, you can use the following code: \r\n```\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport os\r\nimport sys\r\nimport shutil\r\nimport timeit\r\nimport tensorflow as tf\r\nimport numpy\r\nfrom  six.moves import xrange\r\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\r\nfrom keras.layers.noise import AlphaDropout\r\nfrom tensorflow.contrib.nn.python.ops.alpha_dropout import alpha_dropout\r\nimport mnist_data\r\n\r\nFLAGS = None\r\nIMAGE_SIZE = 28\r\nNUM_CHANNELS = 1\r\nPIXEL_DEPTH = 255\r\nNUM_LABELS = 10\r\nSEED =0\r\nBATCH_SIZE = 100\r\nMAX_STEPS=200001\r\nWEIGHT_INIT=tf.keras.initializers.lecun_uniform()\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    tf.config.experimental.set_virtual_device_configuration(\r\n        gpus[0],\r\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10000),\r\n         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10000)])\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    print(e)\r\n\r\ndef error_rate(predictions, labels):\r\n    \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\r\n    results=numpy.argmax(predictions, 1)\r\n    return 100.0 - (\r\n        100.0 *\r\n        numpy.sum(numpy.argmax(predictions, 1) == labels) /\r\n        predictions.shape[0])\r\n\r\ndef PReLU(_x, scope=None):\r\n    \"\"\"parametric ReLU activation\"\"\"\r\n    with tf.variable_scope(name_or_scope=scope, default_name=\"prelu\"):\r\n        _alpha = tf.get_variable(\"prelu\", shape=_x.get_shape()[-1],\r\n                                 dtype=_x.dtype, initializer=tf.constant_initializer(0.1))\r\n        return tf.maximum(0.0, _x) + _alpha * tf.minimum(0.0, _x)\r\n\r\ndef main(_):\r\n  numpy.random.seed(1280)\r\n  # Import data\r\n  mnist = read_data_sets(FLAGS.data_dir, reshape=False,validation_size=0, source_url='http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/')\r\n  with tf.Graph().as_default():\r\n      data = tf.placeholder(\r\n          tf.float32,\r\n          shape=(None, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS), name='Input')\r\n\r\n      labels = tf.placeholder(tf.int64, shape=(None,), name='Label')\r\n\r\n      is_train=tf.placeholder(tf.bool)\r\n      keep_prob = tf.placeholder(tf.float32)\r\n      with tf.name_scope('conv'):\r\n          conv_weights=tf.get_variable(\"w\", shape=[3, 3, NUM_CHANNELS, 5120],dtype=tf.float32,\r\n                                       # initializer=tf.keras.initializers.he_normal())\r\n                          initializer=tf.contrib.layers.xavier_initializer())\r\n          conv_biases=tf.get_variable(\"b\", shape=[5120],dtype=tf.float32,\r\n                                      # initializer=tf.keras.initializers.he_normal())\r\n                          initializer=tf.contrib.layers.xavier_initializer())\r\n          conv = tf.nn.conv2d(data,\r\n                              conv_weights,\r\n                              strides=[1, 1, 1, 1],\r\n                              padding='SAME')\r\n          conv=tf.nn.bias_add(conv, conv_biases)\r\n          convbn=tf.layers.batch_normalization(conv,training=is_train)\r\n          relu=tf.nn.selu(convbn,name='conv')\r\n\r\n      with tf.name_scope('conv11'):\r\n          conv_weights=tf.get_variable(\"w11\", shape=[3, 3, 5120, 64],dtype=tf.float32,\r\n                                       # initializer=tf.keras.initializers.he_normal())\r\n                          initializer=tf.contrib.layers.xavier_initializer())\r\n          conv_biases=tf.get_variable(\"b11\", shape=[64],dtype=tf.float32,\r\n                                      # initializer=tf.keras.initializers.he_normal())\r\n                          initializer=tf.contrib.layers.xavier_initializer())\r\n          conv = tf.nn.conv2d(relu,\r\n                              conv_weights,\r\n                              strides=[1, 1, 1, 1],\r\n                              padding='SAME')\r\n          conv=tf.nn.bias_add(conv, conv_biases)\r\n          convbn=tf.layers.batch_normalization(conv,training=is_train)\r\n          relu=tf.nn.selu(convbn,name='conv')\r\n\r\n\r\n      for i in range(1,3):\r\n          with tf.name_scope('block'+str(i)):\r\n              conv1_weights = tf.get_variable(\"b\"+str(i)+\"w1\", shape=[3, 3, 64, 64], dtype=tf.float32,\r\n                                              # initializer=tf.keras.initializers.he_normal())\r\n                                              initializer=tf.contrib.layers.xavier_initializer())\r\n              conv1_biases = tf.get_variable(\"b\"+str(i)+\"b1\", shape=[64], dtype=tf.float32,\r\n                                             # initializer=tf.keras.initializers.he_normal())\r\n                                             initializer=tf.contrib.layers.xavier_initializer())\r\n              conv1 = tf.nn.conv2d(relu,\r\n                                   conv1_weights,\r\n                                   strides=[1, 2, 2, 1],\r\n                                   padding='SAME')\r\n              conv1 = tf.nn.bias_add(conv1, conv1_biases)\r\n              convbn1 = tf.layers.batch_normalization(conv1, training=is_train)\r\n              relu1=tf.nn.relu(convbn1)\r\n              dropout = tf.nn.dropout(relu1, keep_prob)\r\n              conv2_weights = tf.get_variable(\"b\"+str(i)+\"w2\", shape=[3, 3, 64, 64], dtype=tf.float32,\r\n                                              # initializer=tf.keras.initializers.he_normal())\r\n                                              initializer=tf.contrib.layers.xavier_initializer())\r\n              conv2_biases = tf.get_variable(\"b\"+str(i)+\"b2\", shape=[64], dtype=tf.float32,\r\n                                             # initializer=tf.keras.initializers.he_normal())\r\n                                             initializer=tf.contrib.layers.xavier_initializer())\r\n              conv2 = tf.nn.conv2d(dropout,\r\n                                   conv2_weights,\r\n                                   strides=[1, 1, 1, 1],\r\n                                   padding='SAME')\r\n              conv2 = tf.nn.bias_add(conv2, conv2_biases)\r\n              conv2 = tf.add(conv2, relu1)\r\n              convbn2 = tf.layers.batch_normalization(conv2, training=is_train)\r\n              # relu = tf.nn.selu(convbn2, name=\"b\" + str(i) + \"c2\")\r\n              relu=tf.nn.relu(convbn2)\r\n      with tf.name_scope('conv5'):\r\n          conv_weights=tf.get_variable(\"c5w\", shape=[7, 7, 64, 64],dtype=tf.float32,\r\n                          initializer=tf.contrib.layers.xavier_initializer())\r\n          conv_biases=tf.get_variable(\"c5b\", shape=[64],dtype=tf.float32,\r\n                          initializer=tf.contrib.layers.xavier_initializer())\r\n                                      # initializer=tf.keras.initializers.he_normal())\r\n          conv = tf.nn.conv2d(relu,\r\n                              conv_weights,\r\n                              [1,1,1,1],\r\n                              padding='VALID')\r\n          conv=tf.nn.bias_add(conv, conv_biases)\r\n          convbn=tf.layers.batch_normalization(conv,training=is_train)\r\n          # relu=PReLU(convbn,scope=\"conv5\")\r\n          # relu = tf.nn.selu(convbn, name=\"conv5\")\r\n          relu=tf.nn.relu(convbn)\r\n          pool_shape = relu.get_shape().as_list()\r\n          center_out = tf.reshape(\r\n              relu,\r\n              [-1, pool_shape[1] * pool_shape[2] * pool_shape[3]])\r\n\r\n      with tf.name_scope('fc3'):\r\n          fc3_weights = tf.get_variable(\"fc3w\", shape=[64, NUM_LABELS], dtype=tf.float32,\r\n                                        initializer=tf.contrib.layers.xavier_initializer())\r\n                                        # initializer=tf.keras.initializers.he_normal()   )\r\n          fc3_biases = tf.get_variable(\"fc3b\", shape=[NUM_LABELS], dtype=tf.float32,\r\n                                       initializer=tf.contrib.layers.xavier_initializer())\r\n                                        # initializer = tf.keras.initializers.he_normal())\r\n          fc3_out = tf.add(tf.matmul(center_out, fc3_weights), fc3_biases, name=\"Output\")\r\n\r\n      with tf.name_scope('Train'):\r\n          loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n              # labels=labels, logits=logits))\r\n              labels=labels, logits=fc3_out))\r\n          extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n          with tf.control_dependencies(extra_update_ops):\r\n              optimizer = tf.train.AdamOptimizer(3e-4).minimize(loss)\r\n              # optimizer = tf.train.GradientDescentOptimizer(1e-3).minimize(loss)\r\n\r\n      saver = tf.train.Saver()\r\n      config = tf.ConfigProto()\r\n      config.gpu_options.allow_growth = True\r\n      sess = tf.Session(config=config)\r\n      sess.run(tf.global_variables_initializer())\r\n\r\n      with tf.Session() as sess:\r\n          # Run all the initializers to prepare the trainable parameters.\r\n          tf.global_variables_initializer().run()\r\n          print('Initialized!')\r\n          # Loop through training steps.\r\n          start_epoch = 0\r\n          checkpoint = tf.train.latest_checkpoint(FLAGS.model_dir)\r\n          # if checkpoint:\r\n          #     saver.restore(sess, checkpoint)\r\n          #     print(\"## restore from the checkpoint {0}\".format(checkpoint))\r\n          #     start_epoch += int(checkpoint.split('-')[-1])\r\n          print('start training')\r\n          for step in range(start_epoch, MAX_STEPS):\r\n              batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\r\n              loss_value,_=sess.run([loss, optimizer],\r\n                              feed_dict = {data: batch_xs,\r\n                                           labels: batch_ys,\r\n                                           # is_train:True,\r\n                                           keep_prob: 0.8,\r\n                                           is_train: True})\r\n\r\n\r\n              # Save Model\r\n              if step %1000 == 0:\r\n                  print('Step: %d: loss: %.6f.' % (step, loss_value))\r\n                  sys.stdout.flush()\r\n                  checkpoint_file = os.path.join(FLAGS.model_dir, 'model.ckpt')\r\n                  saver.save(sess, checkpoint_file, global_step=step)\r\n                  size = mnist.test.images.shape[0]\r\n                  predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)\r\n                  av_time = 0\r\n\r\n                  for begin in xrange(0, size, BATCH_SIZE):\r\n                      end = begin + BATCH_SIZE\r\n                      batch_xs, batch_ys = mnist.test.next_batch(BATCH_SIZE)\r\n                      start_time = timeit.default_timer()\r\n                      predictions[begin:end, :] = sess.run(\r\n                          fc3_out,\r\n                          feed_dict={data: batch_xs,\r\n                                     labels: batch_ys,\r\n                                     # is_train:False,\r\n                                     keep_prob: 1.0,\r\n                                     is_train: False})\r\n                      elapsed = timeit.default_timer() - start_time\r\n                      av_time = av_time + elapsed\r\n\r\n                  print(\"Number of test samples: \", size)\r\n                  print(\"Average run time per sample: \", av_time / size)\r\n                  test_error = error_rate(predictions, mnist.test.labels)\r\n                  print('Test error: %.1f%%' % test_error)\r\n          print('saving trained model')\r\n          saver.save(sess, './trained_model/model.ckpt')\r\n          print('start testing')\r\n\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n  parser = argparse.ArgumentParser()\r\n  parser.add_argument('--data_dir', type=str, default='./data/fashion',\r\n                      help='Directory for storing input data')\r\n  parser.add_argument('--model_dir', type=str, default='./model_bn9',\r\n                      help='Directory for storing trained model')\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n\r\n```", "Here is the error message:\r\n```\r\nC:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\python.exe E:/workplace/classifier/fashion_mnist_bn_9.py\r\nUsing TensorFlow backend.\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0916 20:16:27.749476  8564 deprecation_wrapper.py:119] From E:/workplace/classifier/fashion_mnist_bn_9.py:27: The name tf.keras.initializers.lecun_uniform is deprecated. Please use tf.compat.v1.keras.initializers.lecun_uniform instead.\r\n\r\n2019-09-16 20:16:27.756805: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll\r\n2019-09-16 20:16:27.861427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:01:00.0\r\n2019-09-16 20:16:27.861592: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-09-16 20:16:27.862039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-09-16 20:16:27.862432: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-09-16 20:16:27.864225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:01:00.0\r\n2019-09-16 20:16:27.864380: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-09-16 20:16:27.864823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n1 Physical GPU, 2 Logical GPUs\r\nExtracting ./data/fashion\\train-images-idx3-ubyte.gz\r\n2019-09-16 20:16:28.484962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-09-16 20:16:28.485078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-09-16 20:16:28.485148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-09-16 20:16:28.486104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10000 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-09-16 20:16:28.487081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10000 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nW0916 20:16:28.478023  8564 deprecation_wrapper.py:119] From E:/workplace/classifier/fashion_mnist_bn_9.py:239: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\r\n\r\nW0916 20:16:28.478023  8564 deprecation.py:323] From E:/workplace/classifier/fashion_mnist_bn_9.py:59: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nW0916 20:16:28.478023  8564 deprecation.py:323] From C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nW0916 20:16:28.478023  8564 deprecation.py:323] From C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting ./data/fashion\\train-labels-idx1-ubyte.gz\r\nW0916 20:16:28.712376  8564 deprecation.py:323] From C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nExtracting ./data/fashion\\t10k-images-idx3-ubyte.gz\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nW0916 20:16:28.743613  8564 deprecation.py:323] From C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nExtracting ./data/fashion\\t10k-labels-idx1-ubyte.gz\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nW0916 20:16:28.923557  8564 deprecation_wrapper.py:119] From E:/workplace/classifier/fashion_mnist_bn_9.py:61: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nW0916 20:16:28.923557  8564 deprecation_wrapper.py:119] From E:/workplace/classifier/fashion_mnist_bn_9.py:70: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\r\n\r\nW0916 20:16:28.939182  8564 deprecation.py:323] From E:/workplace/classifier/fashion_mnist_bn_9.py:81: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\r\nW0916 20:16:29.080882  8564 deprecation.py:506] From E:/workplace/classifier/fashion_mnist_bn_9.py:115: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\nW0916 20:16:29.269706  8564 deprecation_wrapper.py:119] From E:/workplace/classifier/fashion_mnist_bn_9.py:164: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\r\n\r\nW0916 20:16:29.270704  8564 deprecation_wrapper.py:119] From E:/workplace/classifier/fashion_mnist_bn_9.py:164: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\r\n\r\nW0916 20:16:29.270704  8564 deprecation_wrapper.py:119] From E:/workplace/classifier/fashion_mnist_bn_9.py:166: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\r\n\r\nW0916 20:16:29.813624  8564 deprecation_wrapper.py:119] From E:/workplace/classifier/fashion_mnist_bn_9.py:169: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\r\n\r\nW0916 20:16:29.876081  8564 deprecation_wrapper.py:119] From E:/workplace/classifier/fashion_mnist_bn_9.py:170: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\r\n\r\n2019-09-16 20:16:29.880953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:01:00.0\r\n2019-09-16 20:16:29.881109: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-09-16 20:16:29.881545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-09-16 20:16:29.881662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-09-16 20:16:29.881767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-09-16 20:16:29.881833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-09-16 20:16:29.882269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10000 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-09-16 20:16:30.128099: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 9.77G (10485760000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:30.128257: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 8.79G (9437184000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:30.128395: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 7.91G (8493465600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:30.128533: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 7.12G (7644119040 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:30.128668: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 6.41G (6879707136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:30.128802: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.77G (6191736320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:30.128939: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.19G (5572562432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:30.327942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:01:00.0\r\n2019-09-16 20:16:30.328099: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-09-16 20:16:30.328544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-09-16 20:16:30.328656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-09-16 20:16:30.328761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-09-16 20:16:30.328827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-09-16 20:16:30.329393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10000 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-09-16 20:16:30.329784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10000 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nInitialized!\r\nstart training\r\n2019-09-16 20:16:32.933092: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:32.933240: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.77GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-09-16 20:16:32.933480: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:32.933611: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.77GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-09-16 20:16:33.855820: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:33.855970: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.43GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-09-16 20:16:33.856211: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:33.856342: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.43GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-09-16 20:16:33.856575: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:33.856711: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.86GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-09-16 20:16:33.856951: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:33.857084: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.86GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-09-16 20:16:33.971223: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:33.971386: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:43.972015: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:43.972666: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:43.973801: W tensorflow/core/common_runtime/bfc_allocator.cc:314] Allocator (GPU_0_bfc) ran out of memory trying to allocate 19.14MiB (rounded to 20070400).  Current allocation summary follows.\r\n2019-09-16 20:16:43.974501: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (256): \tTotal Chunks: 208, Chunks in use: 208. 52.0KiB allocated for chunks. 52.0KiB in use in bin. 34.9KiB client-requested in use in bin.\r\n2019-09-16 20:16:43.975236: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (512): \tTotal Chunks: 2, Chunks in use: 1. 1.0KiB allocated for chunks. 512B in use in bin. 400B client-requested in use in bin.\r\n2019-09-16 20:16:43.976517: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1024): \tTotal Chunks: 2, Chunks in use: 1. 2.3KiB allocated for chunks. 1.3KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2019-09-16 20:16:43.977449: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2048): \tTotal Chunks: 4, Chunks in use: 4. 10.0KiB allocated for chunks. 10.0KiB in use in bin. 10.0KiB client-requested in use in bin.\r\n2019-09-16 20:16:43.978480: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4096): \tTotal Chunks: 1, Chunks in use: 1. 4.0KiB allocated for chunks. 4.0KiB in use in bin. 3.9KiB client-requested in use in bin.\r\n2019-09-16 20:16:43.979791: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:16:43.980877: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16384): \tTotal Chunks: 24, Chunks in use: 24. 485.0KiB allocated for chunks. 485.0KiB in use in bin. 485.0KiB client-requested in use in bin.\r\n2019-09-16 20:16:43.982238: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (32768): \tTotal Chunks: 1, Chunks in use: 1. 39.5KiB allocated for chunks. 39.5KiB in use in bin. 25.0KiB client-requested in use in bin.\r\n2019-09-16 20:16:43.982666: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:16:43.982909: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (131072): \tTotal Chunks: 20, Chunks in use: 20. 2.95MiB allocated for chunks. 2.95MiB in use in bin. 2.95MiB client-requested in use in bin.\r\n2019-09-16 20:16:43.983070: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (262144): \tTotal Chunks: 1, Chunks in use: 1. 306.3KiB allocated for chunks. 306.3KiB in use in bin. 306.3KiB client-requested in use in bin.\r\n2019-09-16 20:16:43.983230: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (524288): \tTotal Chunks: 4, Chunks in use: 4. 3.06MiB allocated for chunks. 3.06MiB in use in bin. 3.06MiB client-requested in use in bin.\r\n2019-09-16 20:16:43.983390: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1048576): \tTotal Chunks: 8, Chunks in use: 6. 9.57MiB allocated for chunks. 7.18MiB in use in bin. 7.18MiB client-requested in use in bin.\r\n2019-09-16 20:16:43.983553: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:16:43.983702: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4194304): \tTotal Chunks: 8, Chunks in use: 6. 38.28MiB allocated for chunks. 28.71MiB in use in bin. 28.71MiB client-requested in use in bin.\r\n2019-09-16 20:16:43.984237: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8388608): \tTotal Chunks: 4, Chunks in use: 4. 45.00MiB allocated for chunks. 45.00MiB in use in bin. 45.00MiB client-requested in use in bin.\r\n2019-09-16 20:16:43.984399: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16777216): \tTotal Chunks: 2, Chunks in use: 2. 38.28MiB allocated for chunks. 38.28MiB in use in bin. 38.28MiB client-requested in use in bin.\r\n2019-09-16 20:16:43.984563: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:16:43.984711: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:16:43.984901: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:16:43.985053: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (268435456): \tTotal Chunks: 3, Chunks in use: 3. 4.54GiB allocated for chunks. 4.54GiB in use in bin. 4.49GiB client-requested in use in bin.\r\n2019-09-16 20:16:43.985212: I tensorflow/core/common_runtime/bfc_allocator.cc:780] Bin for 19.14MiB was 16.00MiB, Chunk State: \r\n2019-09-16 20:16:43.985302: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 5015306240\r\n2019-09-16 20:16:43.985391: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0000000509E00000 next 77 of size 256\r\n2019-09-16 20:16:43.985485: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0000000509E00100 next 78 of size 256\r\n2019-09-16 20:16:43.985577: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0000000509E00200 next 183 of size 11796480\r\n2019-09-16 20:16:43.985704: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A940200 next 184 of size 256\r\n2019-09-16 20:16:43.986478: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A940300 next 158 of size 256\r\n2019-09-16 20:16:43.986674: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A940400 next 159 of size 184320\r\n2019-09-16 20:16:43.986781: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96D400 next 156 of size 256\r\n2019-09-16 20:16:43.986879: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96D500 next 157 of size 256\r\n2019-09-16 20:16:43.986973: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96D600 next 177 of size 2560\r\n2019-09-16 20:16:43.987071: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E000 next 178 of size 256\r\n2019-09-16 20:16:43.987165: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E100 next 174 of size 256\r\n2019-09-16 20:16:43.987260: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E200 next 175 of size 256\r\n2019-09-16 20:16:43.987356: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E300 next 173 of size 256\r\n2019-09-16 20:16:43.987450: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E400 next 151 of size 256\r\n2019-09-16 20:16:43.987544: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E500 next 152 of size 802816\r\n2019-09-16 20:16:43.987639: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32500 next 135 of size 256\r\n2019-09-16 20:16:43.987735: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32600 next 136 of size 256\r\n2019-09-16 20:16:43.988158: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32700 next 162 of size 256\r\n2019-09-16 20:16:43.988295: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32800 next 163 of size 256\r\n2019-09-16 20:16:43.988452: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32900 next 161 of size 256\r\n2019-09-16 20:16:43.988620: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32A00 next 166 of size 256\r\n2019-09-16 20:16:43.988749: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32B00 next 167 of size 256\r\n2019-09-16 20:16:43.988877: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32C00 next 160 of size 256\r\n2019-09-16 20:16:43.988988: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32D00 next 145 of size 256\r\n2019-09-16 20:16:43.989133: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32E00 next 146 of size 256\r\n2019-09-16 20:16:43.989247: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32F00 next 154 of size 256\r\n2019-09-16 20:16:43.989420: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33000 next 155 of size 256\r\n2019-09-16 20:16:43.989560: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33100 next 153 of size 256\r\n2019-09-16 20:16:43.989712: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33200 next 150 of size 256\r\n2019-09-16 20:16:43.989848: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33300 next 149 of size 256\r\n2019-09-16 20:16:43.989995: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33400 next 148 of size 256\r\n2019-09-16 20:16:43.990159: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33500 next 164 of size 256\r\n2019-09-16 20:16:43.990291: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33600 next 144 of size 256\r\n2019-09-16 20:16:43.990381: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33700 next 143 of size 256\r\n2019-09-16 20:16:43.990472: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33800 next 83 of size 256\r\n2019-09-16 20:16:43.990562: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33900 next 84 of size 256\r\n2019-09-16 20:16:43.990651: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33A00 next 141 of size 256\r\n2019-09-16 20:16:43.990745: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33B00 next 142 of size 256\r\n2019-09-16 20:16:43.990835: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33C00 next 82 of size 256\r\n2019-09-16 20:16:43.990925: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33D00 next 140 of size 256\r\n2019-09-16 20:16:43.991017: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33E00 next 90 of size 256\r\n2019-09-16 20:16:43.991107: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33F00 next 91 of size 256\r\n2019-09-16 20:16:43.991198: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34000 next 139 of size 256\r\n2019-09-16 20:16:43.991291: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34100 next 97 of size 256\r\n2019-09-16 20:16:43.991383: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34200 next 98 of size 256\r\n2019-09-16 20:16:43.991473: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34300 next 137 of size 256\r\n2019-09-16 20:16:43.991563: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34400 next 134 of size 256\r\n2019-09-16 20:16:43.991655: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34500 next 133 of size 256\r\n2019-09-16 20:16:43.992146: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34600 next 106 of size 20480\r\n2019-09-16 20:16:43.992241: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA39600 next 107 of size 20480\r\n2019-09-16 20:16:43.992334: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA3E600 next 110 of size 20480\r\n2019-09-16 20:16:43.992425: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA43600 next 111 of size 20480\r\n2019-09-16 20:16:43.992517: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA48600 next 165 of size 20480\r\n2019-09-16 20:16:43.992608: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA4D600 next 130 of size 256\r\n2019-09-16 20:16:43.992697: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA4D700 next 131 of size 256\r\n2019-09-16 20:16:43.992789: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA4D800 next 129 of size 147456\r\n2019-09-16 20:16:43.992880: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA71800 next 124 of size 256\r\n2019-09-16 20:16:43.992969: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA71900 next 125 of size 147456\r\n2019-09-16 20:16:43.993065: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95900 next 118 of size 256\r\n2019-09-16 20:16:43.993158: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95A00 next 119 of size 256\r\n2019-09-16 20:16:43.993249: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95B00 next 117 of size 256\r\n2019-09-16 20:16:43.993340: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95C00 next 138 of size 256\r\n2019-09-16 20:16:43.993431: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95D00 next 121 of size 256\r\n2019-09-16 20:16:43.993521: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95E00 next 122 of size 147456\r\n2019-09-16 20:16:43.993614: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAB9E00 next 115 of size 256\r\n2019-09-16 20:16:43.993707: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAB9F00 next 116 of size 147456\r\n2019-09-16 20:16:43.994235: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADDF00 next 114 of size 256\r\n2019-09-16 20:16:43.994385: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE000 next 108 of size 256\r\n2019-09-16 20:16:43.994530: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE100 next 109 of size 256\r\n2019-09-16 20:16:43.994646: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE200 next 113 of size 256\r\n2019-09-16 20:16:43.994813: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE300 next 102 of size 256\r\n2019-09-16 20:16:43.994950: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE400 next 103 of size 256\r\n2019-09-16 20:16:43.995105: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE500 next 128 of size 256\r\n2019-09-16 20:16:43.995261: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE600 next 147 of size 256\r\n2019-09-16 20:16:43.995398: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE700 next 100 of size 20480\r\n2019-09-16 20:16:43.995564: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE3700 next 101 of size 256\r\n2019-09-16 20:16:43.995724: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE3800 next 95 of size 256\r\n2019-09-16 20:16:43.995899: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE3900 next 96 of size 1280\r\n2019-09-16 20:16:43.996046: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE3E00 next 93 of size 256\r\n2019-09-16 20:16:43.996204: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE3F00 next 94 of size 2560\r\n2019-09-16 20:16:43.996365: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE4900 next 92 of size 184320\r\n2019-09-16 20:16:43.996512: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AB11900 next 123 of size 2560\r\n2019-09-16 20:16:43.996676: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AB12300 next 126 of size 11796480\r\n2019-09-16 20:16:43.996839: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B652300 next 127 of size 184320\r\n2019-09-16 20:16:43.996967: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B67F300 next 87 of size 2560\r\n2019-09-16 20:16:43.997057: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B67FD00 next 88 of size 256\r\n2019-09-16 20:16:43.997147: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B67FE00 next 86 of size 256\r\n2019-09-16 20:16:43.997237: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B67FF00 next 104 of size 256\r\n2019-09-16 20:16:43.997327: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B680000 next 105 of size 256\r\n2019-09-16 20:16:43.997417: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B680100 next 85 of size 256\r\n2019-09-16 20:16:43.997508: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B680200 next 89 of size 802816\r\n2019-09-16 20:16:43.997599: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B744200 next 79 of size 256\r\n2019-09-16 20:16:43.997689: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B744300 next 182 of size 256\r\n2019-09-16 20:16:43.997781: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B744400 next 81 of size 802816\r\n2019-09-16 20:16:43.997872: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B808400 next 180 of size 256\r\n2019-09-16 20:16:43.997963: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B808500 next 181 of size 256\r\n2019-09-16 20:16:43.998052: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B808600 next 179 of size 256\r\n2019-09-16 20:16:43.998143: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B808700 next 99 of size 802816\r\n2019-09-16 20:16:43.998234: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CC700 next 80 of size 256\r\n2019-09-16 20:16:43.998325: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CC800 next 132 of size 256\r\n2019-09-16 20:16:43.998417: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CC900 next 176 of size 256\r\n2019-09-16 20:16:43.998505: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CCA00 next 120 of size 256\r\n2019-09-16 20:16:43.998594: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CCB00 next 172 of size 256\r\n2019-09-16 20:16:43.998687: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CCC00 next 170 of size 11796480\r\n2019-09-16 20:16:43.999253: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40CC00 next 171 of size 256\r\n2019-09-16 20:16:43.999379: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40CD00 next 169 of size 256\r\n2019-09-16 20:16:43.999468: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40CE00 next 112 of size 256\r\n2019-09-16 20:16:43.999557: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40CF00 next 168 of size 256\r\n2019-09-16 20:16:43.999646: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40D000 next 76 of size 256\r\n2019-09-16 20:16:43.999738: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40D100 next 75 of size 256\r\n2019-09-16 20:16:43.999829: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40D200 next 74 of size 11796480\r\n2019-09-16 20:16:43.999923: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D200 next 73 of size 256\r\n2019-09-16 20:16:44.000014: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D300 next 72 of size 256\r\n2019-09-16 20:16:44.000104: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D400 next 71 of size 256\r\n2019-09-16 20:16:44.000194: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D500 next 70 of size 256\r\n2019-09-16 20:16:44.000284: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D600 next 69 of size 256\r\n2019-09-16 20:16:44.000378: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D700 next 68 of size 256\r\n2019-09-16 20:16:44.000468: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D800 next 67 of size 256\r\n2019-09-16 20:16:44.000559: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D900 next 66 of size 184320\r\n2019-09-16 20:16:44.000649: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7A900 next 65 of size 256\r\n2019-09-16 20:16:44.001172: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7AA00 next 64 of size 20480\r\n2019-09-16 20:16:44.001265: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FA00 next 63 of size 256\r\n2019-09-16 20:16:44.001355: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FB00 next 62 of size 256\r\n2019-09-16 20:16:44.001458: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FC00 next 61 of size 256\r\n2019-09-16 20:16:44.001548: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FD00 next 60 of size 256\r\n2019-09-16 20:16:44.001638: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FE00 next 59 of size 256\r\n2019-09-16 20:16:44.001733: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FF00 next 58 of size 20480\r\n2019-09-16 20:16:44.001856: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF84F00 next 57 of size 147456\r\n2019-09-16 20:16:44.001948: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA8F00 next 56 of size 256\r\n2019-09-16 20:16:44.002068: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9000 next 55 of size 256\r\n2019-09-16 20:16:44.002161: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9100 next 54 of size 256\r\n2019-09-16 20:16:44.002255: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9200 next 53 of size 256\r\n2019-09-16 20:16:44.002346: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9300 next 52 of size 256\r\n2019-09-16 20:16:44.002437: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9400 next 51 of size 256\r\n2019-09-16 20:16:44.002699: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9500 next 50 of size 20480\r\n2019-09-16 20:16:44.003400: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAE500 next 49 of size 256\r\n2019-09-16 20:16:44.003570: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAE600 next 48 of size 256\r\n2019-09-16 20:16:44.003682: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAE700 next 47 of size 256\r\n2019-09-16 20:16:44.003774: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAE800 next 46 of size 256\r\n2019-09-16 20:16:44.003874: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAE900 next 45 of size 256\r\n2019-09-16 20:16:44.003968: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAEA00 next 44 of size 256\r\n2019-09-16 20:16:44.004060: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAEB00 next 43 of size 20480\r\n2019-09-16 20:16:44.004153: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFB3B00 next 42 of size 256\r\n2019-09-16 20:16:44.004244: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFB3C00 next 41 of size 147456\r\n2019-09-16 20:16:44.004341: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFD7C00 next 40 of size 256\r\n2019-09-16 20:16:44.004433: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFD7D00 next 39 of size 20480\r\n2019-09-16 20:16:44.004527: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFDCD00 next 38 of size 256\r\n2019-09-16 20:16:44.004619: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFDCE00 next 37 of size 256\r\n2019-09-16 20:16:44.004710: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFDCF00 next 36 of size 256\r\n2019-09-16 20:16:44.005160: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFDD000 next 35 of size 256\r\n2019-09-16 20:16:44.005254: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFDD100 next 34 of size 20480\r\n2019-09-16 20:16:44.005347: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFE2100 next 33 of size 20480\r\n2019-09-16 20:16:44.005440: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFE7100 next 32 of size 147456\r\n2019-09-16 20:16:44.005532: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D00B100 next 31 of size 256\r\n2019-09-16 20:16:44.005622: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D00B200 next 30 of size 147456\r\n2019-09-16 20:16:44.005717: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D02F200 next 29 of size 256\r\n2019-09-16 20:16:44.005809: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D02F300 next 28 of size 256\r\n2019-09-16 20:16:44.005901: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D02F400 next 27 of size 256\r\n2019-09-16 20:16:44.005992: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D02F500 next 26 of size 20480\r\n2019-09-16 20:16:44.006084: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D034500 next 25 of size 20480\r\n2019-09-16 20:16:44.006174: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D039500 next 24 of size 147456\r\n2019-09-16 20:16:44.006266: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D05D500 next 23 of size 147456\r\n2019-09-16 20:16:44.006357: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081500 next 22 of size 256\r\n2019-09-16 20:16:44.006446: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081600 next 21 of size 256\r\n2019-09-16 20:16:44.006535: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081700 next 20 of size 256\r\n2019-09-16 20:16:44.006629: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081800 next 19 of size 256\r\n2019-09-16 20:16:44.007119: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081900 next 18 of size 256\r\n2019-09-16 20:16:44.007299: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081A00 next 17 of size 256\r\n2019-09-16 20:16:44.007407: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081B00 next 16 of size 256\r\n2019-09-16 20:16:44.007572: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081C00 next 15 of size 147456\r\n2019-09-16 20:16:44.007720: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0A5C00 next 14 of size 256\r\n2019-09-16 20:16:44.007853: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0A5D00 next 13 of size 147456\r\n2019-09-16 20:16:44.007989: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0C9D00 next 12 of size 256\r\n2019-09-16 20:16:44.008127: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0C9E00 next 11 of size 256\r\n2019-09-16 20:16:44.008292: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0C9F00 next 10 of size 20480\r\n2019-09-16 20:16:44.008431: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0CEF00 next 9 of size 147456\r\n2019-09-16 20:16:44.008606: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0F2F00 next 8 of size 256\r\n2019-09-16 20:16:44.008720: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0F3000 next 7 of size 256\r\n2019-09-16 20:16:44.008809: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0F3100 next 6 of size 147456\r\n2019-09-16 20:16:44.008900: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D117100 next 5 of size 147456\r\n2019-09-16 20:16:44.008991: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D13B100 next 4 of size 147456\r\n2019-09-16 20:16:44.009081: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D15F100 next 3 of size 256\r\n2019-09-16 20:16:44.009170: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D15F200 next 2 of size 256\r\n2019-09-16 20:16:44.009259: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D15F300 next 1 of size 20480\r\n2019-09-16 20:16:44.009350: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164300 next 185 of size 256\r\n2019-09-16 20:16:44.009440: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164400 next 186 of size 256\r\n2019-09-16 20:16:44.009532: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164500 next 187 of size 256\r\n2019-09-16 20:16:44.009622: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164600 next 188 of size 256\r\n2019-09-16 20:16:44.009715: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164700 next 189 of size 256\r\n2019-09-16 20:16:44.010096: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164800 next 190 of size 256\r\n2019-09-16 20:16:44.010188: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164900 next 191 of size 256\r\n2019-09-16 20:16:44.010279: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164A00 next 192 of size 256\r\n2019-09-16 20:16:44.010369: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164B00 next 193 of size 256\r\n2019-09-16 20:16:44.010459: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164C00 next 194 of size 256\r\n2019-09-16 20:16:44.010550: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164D00 next 195 of size 256\r\n2019-09-16 20:16:44.010640: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164E00 next 196 of size 256\r\n2019-09-16 20:16:44.010734: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164F00 next 197 of size 256\r\n2019-09-16 20:16:44.010824: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165000 next 198 of size 256\r\n2019-09-16 20:16:44.010914: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165100 next 199 of size 256\r\n2019-09-16 20:16:44.011004: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165200 next 200 of size 256\r\n2019-09-16 20:16:44.011095: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165300 next 201 of size 256\r\n2019-09-16 20:16:44.011185: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165400 next 202 of size 256\r\n2019-09-16 20:16:44.011277: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165500 next 203 of size 256\r\n2019-09-16 20:16:44.011368: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165600 next 204 of size 256\r\n2019-09-16 20:16:44.011461: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165700 next 205 of size 256\r\n2019-09-16 20:16:44.011550: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165800 next 206 of size 256\r\n2019-09-16 20:16:44.011639: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165900 next 207 of size 256\r\n2019-09-16 20:16:44.012162: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165A00 next 208 of size 256\r\n2019-09-16 20:16:44.012304: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165B00 next 209 of size 256\r\n2019-09-16 20:16:44.012443: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165C00 next 210 of size 256\r\n2019-09-16 20:16:44.012593: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165D00 next 211 of size 256\r\n2019-09-16 20:16:44.012746: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165E00 next 212 of size 256\r\n2019-09-16 20:16:44.012871: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165F00 next 213 of size 256\r\n2019-09-16 20:16:44.013035: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166000 next 214 of size 256\r\n2019-09-16 20:16:44.013193: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166100 next 215 of size 256\r\n2019-09-16 20:16:44.013349: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166200 next 216 of size 256\r\n2019-09-16 20:16:44.013469: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166300 next 217 of size 256\r\n2019-09-16 20:16:44.013647: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166400 next 218 of size 256\r\n2019-09-16 20:16:44.013810: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166500 next 219 of size 256\r\n2019-09-16 20:16:44.013997: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166600 next 220 of size 256\r\n2019-09-16 20:16:44.014166: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166700 next 221 of size 256\r\n2019-09-16 20:16:44.014288: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166800 next 222 of size 256\r\n2019-09-16 20:16:44.014382: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166900 next 223 of size 256\r\n2019-09-16 20:16:44.014476: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166A00 next 224 of size 256\r\n2019-09-16 20:16:44.014567: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166B00 next 225 of size 256\r\n2019-09-16 20:16:44.014660: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166C00 next 226 of size 256\r\n2019-09-16 20:16:44.014752: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166D00 next 227 of size 256\r\n2019-09-16 20:16:44.014843: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166E00 next 228 of size 256\r\n2019-09-16 20:16:44.014934: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166F00 next 229 of size 256\r\n2019-09-16 20:16:44.015024: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167000 next 230 of size 256\r\n2019-09-16 20:16:44.015115: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167100 next 231 of size 256\r\n2019-09-16 20:16:44.015206: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167200 next 232 of size 256\r\n2019-09-16 20:16:44.015297: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167300 next 233 of size 256\r\n2019-09-16 20:16:44.015388: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167400 next 234 of size 256\r\n2019-09-16 20:16:44.015479: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167500 next 235 of size 256\r\n2019-09-16 20:16:44.015570: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167600 next 236 of size 256\r\n2019-09-16 20:16:44.015664: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167700 next 237 of size 256\r\n2019-09-16 20:16:44.015757: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167800 next 238 of size 256\r\n2019-09-16 20:16:44.015847: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167900 next 239 of size 256\r\n2019-09-16 20:16:44.015937: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167A00 next 240 of size 256\r\n2019-09-16 20:16:44.016031: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167B00 next 241 of size 256\r\n2019-09-16 20:16:44.016122: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167C00 next 242 of size 256\r\n2019-09-16 20:16:44.016212: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167D00 next 243 of size 256\r\n2019-09-16 20:16:44.016303: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167E00 next 244 of size 256\r\n2019-09-16 20:16:44.016394: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167F00 next 245 of size 20480\r\n2019-09-16 20:16:44.017118: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D16CF00 next 246 of size 20480\r\n2019-09-16 20:16:44.017211: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D171F00 next 247 of size 20480\r\n2019-09-16 20:16:44.017303: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D176F00 next 248 of size 20480\r\n2019-09-16 20:16:44.017395: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 000000050D17BF00 next 249 of size 1024\r\n2019-09-16 20:16:44.017486: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D17C300 next 250 of size 313600\r\n2019-09-16 20:16:44.017579: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 000000050D1C8C00 next 252 of size 512\r\n2019-09-16 20:16:44.017675: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C8E00 next 253 of size 256\r\n2019-09-16 20:16:44.017767: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C8F00 next 290 of size 256\r\n2019-09-16 20:16:44.017858: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9000 next 291 of size 256\r\n2019-09-16 20:16:44.017973: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9100 next 257 of size 512\r\n2019-09-16 20:16:44.018076: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9300 next 258 of size 256\r\n2019-09-16 20:16:44.018164: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9400 next 259 of size 256\r\n2019-09-16 20:16:44.018252: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9500 next 283 of size 256\r\n2019-09-16 20:16:44.018341: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9600 next 261 of size 256\r\n2019-09-16 20:16:44.018430: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9700 next 262 of size 256\r\n2019-09-16 20:16:44.018518: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9800 next 263 of size 256\r\n2019-09-16 20:16:44.018606: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9900 next 277 of size 256\r\n2019-09-16 20:16:44.019082: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9A00 next 265 of size 256\r\n2019-09-16 20:16:44.019309: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9B00 next 266 of size 256\r\n2019-09-16 20:16:44.019401: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9C00 next 267 of size 1605632000\r\n2019-09-16 20:16:44.019496: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000056CD09C00 next 268 of size 1605632000\r\n2019-09-16 20:16:44.019590: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC849C00 next 274 of size 256\r\n2019-09-16 20:16:44.019805: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC849D00 next 275 of size 256\r\n2019-09-16 20:16:44.019894: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC849E00 next 270 of size 40448\r\n2019-09-16 20:16:44.020079: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC853C00 next 271 of size 20480\r\n2019-09-16 20:16:44.020171: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC858C00 next 272 of size 20480\r\n2019-09-16 20:16:44.020262: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC85DC00 next 273 of size 20070400\r\n2019-09-16 20:16:44.020356: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CDB81C00 next 269 of size 20070400\r\n2019-09-16 20:16:44.020451: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CEEA5C00 next 276 of size 5017600\r\n2019-09-16 20:16:44.020543: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CF36EC00 next 264 of size 5017600\r\n2019-09-16 20:16:44.020642: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 00000005CF837C00 next 278 of size 5017600\r\n2019-09-16 20:16:44.021122: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CFD00C00 next 260 of size 5017600\r\n2019-09-16 20:16:44.021219: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 00000005D01C9C00 next 279 of size 5017600\r\n2019-09-16 20:16:44.021314: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D0692C00 next 280 of size 5017600\r\n2019-09-16 20:16:44.021407: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D0B5BC00 next 281 of size 5017600\r\n2019-09-16 20:16:44.021501: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1024C00 next 282 of size 5017600\r\n2019-09-16 20:16:44.021595: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D14EDC00 next 284 of size 1254400\r\n2019-09-16 20:16:44.021691: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1620000 next 285 of size 1254400\r\n2019-09-16 20:16:44.021785: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1752400 next 251 of size 4096\r\n2019-09-16 20:16:44.021877: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 00000005D1753400 next 286 of size 1250304\r\n2019-09-16 20:16:44.021983: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1884800 next 254 of size 1254400\r\n2019-09-16 20:16:44.022075: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 00000005D19B6C00 next 255 of size 1254400\r\n2019-09-16 20:16:44.022168: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1AE9000 next 287 of size 1254400\r\n2019-09-16 20:16:44.022260: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1C1B400 next 288 of size 1254400\r\n2019-09-16 20:16:44.022353: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1D4D800 next 289 of size 1254400\r\n2019-09-16 20:16:44.022446: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1E7FC00 next 292 of size 25600\r\n2019-09-16 20:16:44.022540: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1E86000 next 18446744073709551615 of size 1659314176\r\n2019-09-16 20:16:44.022653: I tensorflow/core/common_runtime/bfc_allocator.cc:809]      Summary of in-use Chunks by size: \r\n2019-09-16 20:16:44.023205: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 208 Chunks of size 256 totalling 52.0KiB\r\n2019-09-16 20:16:44.023371: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 512 totalling 512B\r\n2019-09-16 20:16:44.023505: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 1280 totalling 1.3KiB\r\n2019-09-16 20:16:44.023662: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 2560 totalling 10.0KiB\r\n2019-09-16 20:16:44.023794: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 4096 totalling 4.0KiB\r\n2019-09-16 20:16:44.023889: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 23 Chunks of size 20480 totalling 460.0KiB\r\n2019-09-16 20:16:44.024025: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 25600 totalling 25.0KiB\r\n2019-09-16 20:16:44.024190: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 40448 totalling 39.5KiB\r\n2019-09-16 20:16:44.024333: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 16 Chunks of size 147456 totalling 2.25MiB\r\n2019-09-16 20:16:44.024446: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 184320 totalling 720.0KiB\r\n2019-09-16 20:16:44.024535: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 313600 totalling 306.3KiB\r\n2019-09-16 20:16:44.024624: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 802816 totalling 3.06MiB\r\n2019-09-16 20:16:44.024711: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 6 Chunks of size 1254400 totalling 7.18MiB\r\n2019-09-16 20:16:44.024798: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 6 Chunks of size 5017600 totalling 28.71MiB\r\n2019-09-16 20:16:44.024886: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 11796480 totalling 45.00MiB\r\n2019-09-16 20:16:44.024974: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 20070400 totalling 38.28MiB\r\n2019-09-16 20:16:44.025063: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 1605632000 totalling 2.99GiB\r\n2019-09-16 20:16:44.025154: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 1659314176 totalling 1.54GiB\r\n2019-09-16 20:16:44.025245: I tensorflow/core/common_runtime/bfc_allocator.cc:816] Sum Total of in-use chunks: 4.66GiB\r\n2019-09-16 20:16:44.025329: I tensorflow/core/common_runtime/bfc_allocator.cc:818] total_region_allocated_bytes_: 5015306240 memory_limit_: 10485760000 available bytes: 5470453760 curr_region_allocation_bytes_: 20971520000\r\n2019-09-16 20:16:44.025483: I tensorflow/core/common_runtime/bfc_allocator.cc:824] Stats: \r\nLimit:                 10485760000\r\nInUse:                  5002764800\r\nMaxInUse:               5002764800\r\nNumAllocs:                     583\r\nMaxAllocSize:           2250966016\r\n\r\n2019-09-16 20:16:44.025679: W tensorflow/core/common_runtime/bfc_allocator.cc:319] ***************************************************************************************************x\r\n2019-09-16 20:16:44.026141: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at constant_op.cc:172 : Resource exhausted: OOM when allocating tensor with shape[100,64,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n2019-09-16 20:16:44.026472: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:44.026607: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:54.027508: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:54.027869: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:54.028193: W tensorflow/core/common_runtime/bfc_allocator.cc:314] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.20MiB (rounded to 1254400).  Current allocation summary follows.\r\n2019-09-16 20:16:54.028576: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (256): \tTotal Chunks: 208, Chunks in use: 208. 52.0KiB allocated for chunks. 52.0KiB in use in bin. 34.9KiB client-requested in use in bin.\r\n2019-09-16 20:16:54.028974: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (512): \tTotal Chunks: 2, Chunks in use: 1. 1.0KiB allocated for chunks. 512B in use in bin. 400B client-requested in use in bin.\r\n2019-09-16 20:16:54.029354: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1024): \tTotal Chunks: 2, Chunks in use: 1. 2.3KiB allocated for chunks. 1.3KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2019-09-16 20:16:54.029742: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2048): \tTotal Chunks: 4, Chunks in use: 4. 10.0KiB allocated for chunks. 10.0KiB in use in bin. 10.0KiB client-requested in use in bin.\r\n2019-09-16 20:16:54.030136: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4096): \tTotal Chunks: 1, Chunks in use: 1. 4.0KiB allocated for chunks. 4.0KiB in use in bin. 3.9KiB client-requested in use in bin.\r\n2019-09-16 20:16:54.030523: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:16:54.030885: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16384): \tTotal Chunks: 25, Chunks in use: 25. 510.0KiB allocated for chunks. 510.0KiB in use in bin. 510.0KiB client-requested in use in bin.\r\n2019-09-16 20:16:54.031282: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (32768): \tTotal Chunks: 1, Chunks in use: 1. 39.5KiB allocated for chunks. 39.5KiB in use in bin. 25.0KiB client-requested in use in bin.\r\n2019-09-16 20:16:54.032770: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:16:54.033480: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (131072): \tTotal Chunks: 20, Chunks in use: 20. 2.95MiB allocated for chunks. 2.95MiB in use in bin. 2.95MiB client-requested in use in bin.\r\n2019-09-16 20:16:54.033881: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (262144): \tTotal Chunks: 1, Chunks in use: 1. 306.3KiB allocated for chunks. 306.3KiB in use in bin. 306.3KiB client-requested in use in bin.\r\n2019-09-16 20:16:54.034279: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (524288): \tTotal Chunks: 4, Chunks in use: 4. 3.06MiB allocated for chunks. 3.06MiB in use in bin. 3.06MiB client-requested in use in bin.\r\n2019-09-16 20:16:54.034667: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1048576): \tTotal Chunks: 8, Chunks in use: 7. 9.54MiB allocated for chunks. 8.37MiB in use in bin. 8.37MiB client-requested in use in bin.\r\n2019-09-16 20:16:54.035055: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:16:54.036156: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4194304): \tTotal Chunks: 8, Chunks in use: 8. 38.28MiB allocated for chunks. 38.28MiB in use in bin. 38.28MiB client-requested in use in bin.\r\n2019-09-16 20:16:54.036570: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8388608): \tTotal Chunks: 4, Chunks in use: 4. 45.00MiB allocated for chunks. 45.00MiB in use in bin. 45.00MiB client-requested in use in bin.\r\n2019-09-16 20:16:54.037264: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16777216): \tTotal Chunks: 2, Chunks in use: 2. 38.28MiB allocated for chunks. 38.28MiB in use in bin. 38.28MiB client-requested in use in bin.\r\n2019-09-16 20:16:54.037927: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:16:54.038302: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:16:54.038676: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:16:54.039052: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (268435456): \tTotal Chunks: 3, Chunks in use: 3. 4.54GiB allocated for chunks. 4.54GiB in use in bin. 4.49GiB client-requested in use in bin.\r\n2019-09-16 20:16:54.040017: I tensorflow/core/common_runtime/bfc_allocator.cc:780] Bin for 1.20MiB was 1.00MiB, Chunk State: \r\n2019-09-16 20:16:54.040257: I tensorflow/core/common_runtime/bfc_allocator.cc:786]   Size: 1.17MiB | Requested Size: 4B | in_use: 0 | bin_num: 12, prev:   Size: 25.0KiB | Requested Size: 25.0KiB | in_use: 1 | bin_num: -1, next:   Size: 1.20MiB | Requested Size: 1.20MiB | in_use: 1 | bin_num: -1\r\n2019-09-16 20:16:54.040755: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 5015306240\r\n2019-09-16 20:16:54.040958: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0000000509E00000 next 77 of size 256\r\n2019-09-16 20:16:54.041179: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0000000509E00100 next 78 of size 256\r\n2019-09-16 20:16:54.041835: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0000000509E00200 next 183 of size 11796480\r\n2019-09-16 20:16:54.042074: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A940200 next 184 of size 256\r\n2019-09-16 20:16:54.042512: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A940300 next 158 of size 256\r\n2019-09-16 20:16:54.042745: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A940400 next 159 of size 184320\r\n2019-09-16 20:16:54.042976: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96D400 next 156 of size 256\r\n2019-09-16 20:16:54.043206: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96D500 next 157 of size 256\r\n2019-09-16 20:16:54.043433: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96D600 next 177 of size 2560\r\n2019-09-16 20:16:54.043662: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E000 next 178 of size 256\r\n2019-09-16 20:16:54.043888: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E100 next 174 of size 256\r\n2019-09-16 20:16:54.044113: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E200 next 175 of size 256\r\n2019-09-16 20:16:54.044856: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E300 next 173 of size 256\r\n2019-09-16 20:16:54.045302: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E400 next 151 of size 256\r\n2019-09-16 20:16:54.045588: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E500 next 152 of size 802816\r\n2019-09-16 20:16:54.045901: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32500 next 135 of size 256\r\n2019-09-16 20:16:54.046218: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32600 next 136 of size 256\r\n2019-09-16 20:16:54.046673: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32700 next 162 of size 256\r\n2019-09-16 20:16:54.046946: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32800 next 163 of size 256\r\n2019-09-16 20:16:54.047170: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32900 next 161 of size 256\r\n2019-09-16 20:16:54.047394: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32A00 next 166 of size 256\r\n2019-09-16 20:16:54.047617: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32B00 next 167 of size 256\r\n2019-09-16 20:16:54.047841: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32C00 next 160 of size 256\r\n2019-09-16 20:16:54.048075: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32D00 next 145 of size 256\r\n2019-09-16 20:16:54.048308: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32E00 next 146 of size 256\r\n2019-09-16 20:16:54.048535: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32F00 next 154 of size 256\r\n2019-09-16 20:16:54.048760: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33000 next 155 of size 256\r\n2019-09-16 20:16:54.048986: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33100 next 153 of size 256\r\n2019-09-16 20:16:54.049198: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33200 next 150 of size 256\r\n2019-09-16 20:16:54.049291: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33300 next 149 of size 256\r\n2019-09-16 20:16:54.049384: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33400 next 148 of size 256\r\n2019-09-16 20:16:54.049477: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33500 next 164 of size 256\r\n2019-09-16 20:16:54.049567: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33600 next 144 of size 256\r\n2019-09-16 20:16:54.049658: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33700 next 143 of size 256\r\n2019-09-16 20:16:54.049749: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33800 next 83 of size 256\r\n2019-09-16 20:16:54.049841: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33900 next 84 of size 256\r\n2019-09-16 20:16:54.049931: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33A00 next 141 of size 256\r\n2019-09-16 20:16:54.050035: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33B00 next 142 of size 256\r\n2019-09-16 20:16:54.050126: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33C00 next 82 of size 256\r\n2019-09-16 20:16:54.050228: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33D00 next 140 of size 256\r\n2019-09-16 20:16:54.050386: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33E00 next 90 of size 256\r\n2019-09-16 20:16:54.050476: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33F00 next 91 of size 256\r\n2019-09-16 20:16:54.050569: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34000 next 139 of size 256\r\n2019-09-16 20:16:54.050661: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34100 next 97 of size 256\r\n2019-09-16 20:16:54.050766: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34200 next 98 of size 256\r\n2019-09-16 20:16:54.050860: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34300 next 137 of size 256\r\n2019-09-16 20:16:54.050954: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34400 next 134 of size 256\r\n2019-09-16 20:16:54.051048: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34500 next 133 of size 256\r\n2019-09-16 20:16:54.051142: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34600 next 106 of size 20480\r\n2019-09-16 20:16:54.052645: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA39600 next 107 of size 20480\r\n2019-09-16 20:16:54.052740: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA3E600 next 110 of size 20480\r\n2019-09-16 20:16:54.052832: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA43600 next 111 of size 20480\r\n2019-09-16 20:16:54.052923: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA48600 next 165 of size 20480\r\n2019-09-16 20:16:54.053059: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA4D600 next 130 of size 256\r\n2019-09-16 20:16:54.053153: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA4D700 next 131 of size 256\r\n2019-09-16 20:16:54.053244: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA4D800 next 129 of size 147456\r\n2019-09-16 20:16:54.053337: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA71800 next 124 of size 256\r\n2019-09-16 20:16:54.053431: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA71900 next 125 of size 147456\r\n2019-09-16 20:16:54.053525: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95900 next 118 of size 256\r\n2019-09-16 20:16:54.053618: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95A00 next 119 of size 256\r\n2019-09-16 20:16:54.053711: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95B00 next 117 of size 256\r\n2019-09-16 20:16:54.053804: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95C00 next 138 of size 256\r\n2019-09-16 20:16:54.053896: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95D00 next 121 of size 256\r\n2019-09-16 20:16:54.053988: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95E00 next 122 of size 147456\r\n2019-09-16 20:16:54.054083: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAB9E00 next 115 of size 256\r\n2019-09-16 20:16:54.054176: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAB9F00 next 116 of size 147456\r\n2019-09-16 20:16:54.054737: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADDF00 next 114 of size 256\r\n2019-09-16 20:16:54.054861: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE000 next 108 of size 256\r\n2019-09-16 20:16:54.055000: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE100 next 109 of size 256\r\n2019-09-16 20:16:54.055143: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE200 next 113 of size 256\r\n2019-09-16 20:16:54.055297: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE300 next 102 of size 256\r\n2019-09-16 20:16:54.055437: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE400 next 103 of size 256\r\n2019-09-16 20:16:54.055577: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE500 next 128 of size 256\r\n2019-09-16 20:16:54.055726: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE600 next 147 of size 256\r\n2019-09-16 20:16:54.055845: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE700 next 100 of size 20480\r\n2019-09-16 20:16:54.055935: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE3700 next 101 of size 256\r\n2019-09-16 20:16:54.056023: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE3800 next 95 of size 256\r\n2019-09-16 20:16:54.056110: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE3900 next 96 of size 1280\r\n2019-09-16 20:16:54.056322: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE3E00 next 93 of size 256\r\n2019-09-16 20:16:54.056415: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE3F00 next 94 of size 2560\r\n2019-09-16 20:16:54.056505: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE4900 next 92 of size 184320\r\n2019-09-16 20:16:54.056597: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AB11900 next 123 of size 2560\r\n2019-09-16 20:16:54.056688: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AB12300 next 126 of size 11796480\r\n2019-09-16 20:16:54.056782: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B652300 next 127 of size 184320\r\n2019-09-16 20:16:54.056875: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B67F300 next 87 of size 2560\r\n2019-09-16 20:16:54.056965: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B67FD00 next 88 of size 256\r\n2019-09-16 20:16:54.057056: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B67FE00 next 86 of size 256\r\n2019-09-16 20:16:54.057144: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B67FF00 next 104 of size 256\r\n2019-09-16 20:16:54.057234: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B680000 next 105 of size 256\r\n2019-09-16 20:16:54.057322: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B680100 next 85 of size 256\r\n2019-09-16 20:16:54.057411: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B680200 next 89 of size 802816\r\n2019-09-16 20:16:54.057501: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B744200 next 79 of size 256\r\n2019-09-16 20:16:54.057588: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B744300 next 182 of size 256\r\n2019-09-16 20:16:54.057677: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B744400 next 81 of size 802816\r\n2019-09-16 20:16:54.057767: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B808400 next 180 of size 256\r\n2019-09-16 20:16:54.057855: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B808500 next 181 of size 256\r\n2019-09-16 20:16:54.057943: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B808600 next 179 of size 256\r\n2019-09-16 20:16:54.058035: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B808700 next 99 of size 802816\r\n2019-09-16 20:16:54.058126: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CC700 next 80 of size 256\r\n2019-09-16 20:16:54.058703: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CC800 next 132 of size 256\r\n2019-09-16 20:16:54.058836: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CC900 next 176 of size 256\r\n2019-09-16 20:16:54.058993: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CCA00 next 120 of size 256\r\n2019-09-16 20:16:54.059140: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CCB00 next 172 of size 256\r\n2019-09-16 20:16:54.059284: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CCC00 next 170 of size 11796480\r\n2019-09-16 20:16:54.059464: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40CC00 next 171 of size 256\r\n2019-09-16 20:16:54.059573: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40CD00 next 169 of size 256\r\n2019-09-16 20:16:54.059662: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40CE00 next 112 of size 256\r\n2019-09-16 20:16:54.059750: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40CF00 next 168 of size 256\r\n2019-09-16 20:16:54.059838: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40D000 next 76 of size 256\r\n2019-09-16 20:16:54.059925: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40D100 next 75 of size 256\r\n2019-09-16 20:16:54.060012: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40D200 next 74 of size 11796480\r\n2019-09-16 20:16:54.060103: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D200 next 73 of size 256\r\n2019-09-16 20:16:54.060384: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D300 next 72 of size 256\r\n2019-09-16 20:16:54.060475: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D400 next 71 of size 256\r\n2019-09-16 20:16:54.060563: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D500 next 70 of size 256\r\n2019-09-16 20:16:54.060653: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D600 next 69 of size 256\r\n2019-09-16 20:16:54.060742: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D700 next 68 of size 256\r\n2019-09-16 20:16:54.060832: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D800 next 67 of size 256\r\n2019-09-16 20:16:54.060921: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D900 next 66 of size 184320\r\n2019-09-16 20:16:54.061012: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7A900 next 65 of size 256\r\n2019-09-16 20:16:54.061102: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7AA00 next 64 of size 20480\r\n2019-09-16 20:16:54.061194: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FA00 next 63 of size 256\r\n2019-09-16 20:16:54.061284: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FB00 next 62 of size 256\r\n2019-09-16 20:16:54.061373: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FC00 next 61 of size 256\r\n2019-09-16 20:16:54.061461: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FD00 next 60 of size 256\r\n2019-09-16 20:16:54.061548: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FE00 next 59 of size 256\r\n2019-09-16 20:16:54.061635: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FF00 next 58 of size 20480\r\n2019-09-16 20:16:54.061725: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF84F00 next 57 of size 147456\r\n2019-09-16 20:16:54.061828: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA8F00 next 56 of size 256\r\n2019-09-16 20:16:54.061916: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9000 next 55 of size 256\r\n2019-09-16 20:16:54.062005: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9100 next 54 of size 256\r\n2019-09-16 20:16:54.062098: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9200 next 53 of size 256\r\n2019-09-16 20:16:54.062652: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9300 next 52 of size 256\r\n2019-09-16 20:16:54.062745: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9400 next 51 of size 256\r\n2019-09-16 20:16:54.062834: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9500 next 50 of size 20480\r\n2019-09-16 20:16:54.062925: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAE500 next 49 of size 256\r\n2019-09-16 20:16:54.063015: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAE600 next 48 of size 256\r\n2019-09-16 20:16:54.063104: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAE700 next 47 of size 256\r\n2019-09-16 20:16:54.063194: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAE800 next 46 of size 256\r\n2019-09-16 20:16:54.063284: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAE900 next 45 of size 256\r\n2019-09-16 20:16:54.063372: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAEA00 next 44 of size 256\r\n2019-09-16 20:16:54.063462: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAEB00 next 43 of size 20480\r\n2019-09-16 20:16:54.063552: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFB3B00 next 42 of size 256\r\n2019-09-16 20:16:54.063641: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFB3C00 next 41 of size 147456\r\n2019-09-16 20:16:54.063734: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFD7C00 next 40 of size 256\r\n2019-09-16 20:16:54.063825: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFD7D00 next 39 of size 20480\r\n2019-09-16 20:16:54.063917: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFDCD00 next 38 of size 256\r\n2019-09-16 20:16:54.064007: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFDCE00 next 37 of size 256\r\n2019-09-16 20:16:54.064096: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFDCF00 next 36 of size 256\r\n2019-09-16 20:16:54.064564: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFDD000 next 35 of size 256\r\n2019-09-16 20:16:54.064654: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFDD100 next 34 of size 20480\r\n2019-09-16 20:16:54.064743: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFE2100 next 33 of size 20480\r\n2019-09-16 20:16:54.064832: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFE7100 next 32 of size 147456\r\n2019-09-16 20:16:54.064921: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D00B100 next 31 of size 256\r\n2019-09-16 20:16:54.065008: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D00B200 next 30 of size 147456\r\n2019-09-16 20:16:54.065097: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D02F200 next 29 of size 256\r\n2019-09-16 20:16:54.065185: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D02F300 next 28 of size 256\r\n2019-09-16 20:16:54.065308: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D02F400 next 27 of size 256\r\n2019-09-16 20:16:54.065397: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D02F500 next 26 of size 20480\r\n2019-09-16 20:16:54.065487: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D034500 next 25 of size 20480\r\n2019-09-16 20:16:54.065578: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D039500 next 24 of size 147456\r\n2019-09-16 20:16:54.065670: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D05D500 next 23 of size 147456\r\n2019-09-16 20:16:54.065761: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081500 next 22 of size 256\r\n2019-09-16 20:16:54.065851: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081600 next 21 of size 256\r\n2019-09-16 20:16:54.065940: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081700 next 20 of size 256\r\n2019-09-16 20:16:54.066031: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081800 next 19 of size 256\r\n2019-09-16 20:16:54.066122: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081900 next 18 of size 256\r\n2019-09-16 20:16:54.066617: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081A00 next 17 of size 256\r\n2019-09-16 20:16:54.066761: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081B00 next 16 of size 256\r\n2019-09-16 20:16:54.066911: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081C00 next 15 of size 147456\r\n2019-09-16 20:16:54.067043: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0A5C00 next 14 of size 256\r\n2019-09-16 20:16:54.067176: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0A5D00 next 13 of size 147456\r\n2019-09-16 20:16:54.067266: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0C9D00 next 12 of size 256\r\n2019-09-16 20:16:54.067353: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0C9E00 next 11 of size 256\r\n2019-09-16 20:16:54.067441: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0C9F00 next 10 of size 20480\r\n2019-09-16 20:16:54.067530: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0CEF00 next 9 of size 147456\r\n2019-09-16 20:16:54.067620: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0F2F00 next 8 of size 256\r\n2019-09-16 20:16:54.067707: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0F3000 next 7 of size 256\r\n2019-09-16 20:16:54.067794: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0F3100 next 6 of size 147456\r\n2019-09-16 20:16:54.067885: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D117100 next 5 of size 147456\r\n2019-09-16 20:16:54.067975: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D13B100 next 4 of size 147456\r\n2019-09-16 20:16:54.068066: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D15F100 next 3 of size 256\r\n2019-09-16 20:16:54.068155: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D15F200 next 2 of size 256\r\n2019-09-16 20:16:54.068584: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D15F300 next 1 of size 20480\r\n2019-09-16 20:16:54.068678: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164300 next 185 of size 256\r\n2019-09-16 20:16:54.068769: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164400 next 186 of size 256\r\n2019-09-16 20:16:54.068862: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164500 next 187 of size 256\r\n2019-09-16 20:16:54.068956: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164600 next 188 of size 256\r\n2019-09-16 20:16:54.069047: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164700 next 189 of size 256\r\n2019-09-16 20:16:54.069139: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164800 next 190 of size 256\r\n2019-09-16 20:16:54.069384: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164900 next 191 of size 256\r\n2019-09-16 20:16:54.069499: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164A00 next 192 of size 256\r\n2019-09-16 20:16:54.069591: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164B00 next 193 of size 256\r\n2019-09-16 20:16:54.069682: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164C00 next 194 of size 256\r\n2019-09-16 20:16:54.069847: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164D00 next 195 of size 256\r\n2019-09-16 20:16:54.070088: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164E00 next 196 of size 256\r\n2019-09-16 20:16:54.070611: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164F00 next 197 of size 256\r\n2019-09-16 20:16:54.070787: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165000 next 198 of size 256\r\n2019-09-16 20:16:54.070917: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165100 next 199 of size 256\r\n2019-09-16 20:16:54.071049: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165200 next 200 of size 256\r\n2019-09-16 20:16:54.071186: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165300 next 201 of size 256\r\n2019-09-16 20:16:54.071347: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165400 next 202 of size 256\r\n2019-09-16 20:16:54.071543: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165500 next 203 of size 256\r\n2019-09-16 20:16:54.071700: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165600 next 204 of size 256\r\n2019-09-16 20:16:54.071882: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165700 next 205 of size 256\r\n2019-09-16 20:16:54.072014: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165800 next 206 of size 256\r\n2019-09-16 20:16:54.072195: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165900 next 207 of size 256\r\n2019-09-16 20:16:54.072372: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165A00 next 208 of size 256\r\n2019-09-16 20:16:54.072536: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165B00 next 209 of size 256\r\n2019-09-16 20:16:54.072635: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165C00 next 210 of size 256\r\n2019-09-16 20:16:54.072725: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165D00 next 211 of size 256\r\n2019-09-16 20:16:54.072815: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165E00 next 212 of size 256\r\n2019-09-16 20:16:54.072905: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165F00 next 213 of size 256\r\n2019-09-16 20:16:54.072995: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166000 next 214 of size 256\r\n2019-09-16 20:16:54.073240: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166100 next 215 of size 256\r\n2019-09-16 20:16:54.073332: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166200 next 216 of size 256\r\n2019-09-16 20:16:54.073421: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166300 next 217 of size 256\r\n2019-09-16 20:16:54.073510: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166400 next 218 of size 256\r\n2019-09-16 20:16:54.073598: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166500 next 219 of size 256\r\n2019-09-16 20:16:54.073686: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166600 next 220 of size 256\r\n2019-09-16 20:16:54.073775: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166700 next 221 of size 256\r\n2019-09-16 20:16:54.073865: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166800 next 222 of size 256\r\n2019-09-16 20:16:54.073957: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166900 next 223 of size 256\r\n2019-09-16 20:16:54.074051: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166A00 next 224 of size 256\r\n2019-09-16 20:16:54.074145: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166B00 next 225 of size 256\r\n2019-09-16 20:16:54.074478: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166C00 next 226 of size 256\r\n2019-09-16 20:16:54.074571: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166D00 next 227 of size 256\r\n2019-09-16 20:16:54.074662: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166E00 next 228 of size 256\r\n2019-09-16 20:16:54.074754: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166F00 next 229 of size 256\r\n2019-09-16 20:16:54.074844: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167000 next 230 of size 256\r\n2019-09-16 20:16:54.074934: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167100 next 231 of size 256\r\n2019-09-16 20:16:54.075022: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167200 next 232 of size 256\r\n2019-09-16 20:16:54.075112: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167300 next 233 of size 256\r\n2019-09-16 20:16:54.075202: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167400 next 234 of size 256\r\n2019-09-16 20:16:54.075291: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167500 next 235 of size 256\r\n2019-09-16 20:16:54.075380: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167600 next 236 of size 256\r\n2019-09-16 20:16:54.075468: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167700 next 237 of size 256\r\n2019-09-16 20:16:54.075557: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167800 next 238 of size 256\r\n2019-09-16 20:16:54.075646: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167900 next 239 of size 256\r\n2019-09-16 20:16:54.075737: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167A00 next 240 of size 256\r\n2019-09-16 20:16:54.075826: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167B00 next 241 of size 256\r\n2019-09-16 20:16:54.075916: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167C00 next 242 of size 256\r\n2019-09-16 20:16:54.076007: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167D00 next 243 of size 256\r\n2019-09-16 20:16:54.076099: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167E00 next 244 of size 256\r\n2019-09-16 20:16:54.076703: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167F00 next 245 of size 20480\r\n2019-09-16 20:16:54.076876: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D16CF00 next 246 of size 20480\r\n2019-09-16 20:16:54.077041: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D171F00 next 247 of size 20480\r\n2019-09-16 20:16:54.077154: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D176F00 next 248 of size 20480\r\n2019-09-16 20:16:54.077289: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 000000050D17BF00 next 249 of size 1024\r\n2019-09-16 20:16:54.077457: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D17C300 next 250 of size 313600\r\n2019-09-16 20:16:54.077634: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 000000050D1C8C00 next 252 of size 512\r\n2019-09-16 20:16:54.077776: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C8E00 next 253 of size 256\r\n2019-09-16 20:16:54.077935: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C8F00 next 290 of size 256\r\n2019-09-16 20:16:54.078087: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9000 next 291 of size 256\r\n2019-09-16 20:16:54.078238: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9100 next 257 of size 512\r\n2019-09-16 20:16:54.078328: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9300 next 258 of size 256\r\n2019-09-16 20:16:54.078416: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9400 next 259 of size 256\r\n2019-09-16 20:16:54.078504: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9500 next 283 of size 256\r\n2019-09-16 20:16:54.078591: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9600 next 261 of size 256\r\n2019-09-16 20:16:54.078679: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9700 next 262 of size 256\r\n2019-09-16 20:16:54.078766: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9800 next 263 of size 256\r\n2019-09-16 20:16:54.078854: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9900 next 277 of size 256\r\n2019-09-16 20:16:54.078942: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9A00 next 265 of size 256\r\n2019-09-16 20:16:54.079031: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9B00 next 266 of size 256\r\n2019-09-16 20:16:54.079367: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9C00 next 267 of size 1605632000\r\n2019-09-16 20:16:54.079464: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000056CD09C00 next 268 of size 1605632000\r\n2019-09-16 20:16:54.079559: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC849C00 next 274 of size 256\r\n2019-09-16 20:16:54.079649: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC849D00 next 275 of size 256\r\n2019-09-16 20:16:54.079739: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC849E00 next 270 of size 40448\r\n2019-09-16 20:16:54.079830: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC853C00 next 271 of size 20480\r\n2019-09-16 20:16:54.079921: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC858C00 next 272 of size 20480\r\n2019-09-16 20:16:54.080014: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC85DC00 next 273 of size 20070400\r\n2019-09-16 20:16:54.080110: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CDB81C00 next 269 of size 20070400\r\n2019-09-16 20:16:54.080206: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CEEA5C00 next 276 of size 5017600\r\n2019-09-16 20:16:54.080301: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CF36EC00 next 264 of size 5017600\r\n2019-09-16 20:16:54.080394: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CF837C00 next 278 of size 5017600\r\n2019-09-16 20:16:54.080485: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CFD00C00 next 260 of size 5017600\r\n2019-09-16 20:16:54.080575: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D01C9C00 next 279 of size 5017600\r\n2019-09-16 20:16:54.080666: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D0692C00 next 280 of size 5017600\r\n2019-09-16 20:16:54.080756: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D0B5BC00 next 281 of size 5017600\r\n2019-09-16 20:16:54.080848: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1024C00 next 282 of size 5017600\r\n2019-09-16 20:16:54.080939: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D14EDC00 next 284 of size 1254400\r\n2019-09-16 20:16:54.081030: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1620000 next 285 of size 1254400\r\n2019-09-16 20:16:54.081577: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1752400 next 251 of size 4096\r\n2019-09-16 20:16:54.081747: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1753400 next 256 of size 25600\r\n2019-09-16 20:16:54.081865: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 00000005D1759800 next 286 of size 1224704\r\n2019-09-16 20:16:54.082018: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1884800 next 254 of size 1254400\r\n2019-09-16 20:16:54.082168: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D19B6C00 next 255 of size 1254400\r\n2019-09-16 20:16:54.082259: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1AE9000 next 287 of size 1254400\r\n2019-09-16 20:16:54.082349: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1C1B400 next 288 of size 1254400\r\n2019-09-16 20:16:54.082441: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1D4D800 next 289 of size 1254400\r\n2019-09-16 20:16:54.082532: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1E7FC00 next 292 of size 25600\r\n2019-09-16 20:16:54.082623: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1E86000 next 18446744073709551615 of size 1659314176\r\n2019-09-16 20:16:54.082727: I tensorflow/core/common_runtime/bfc_allocator.cc:809]      Summary of in-use Chunks by size: \r\n2019-09-16 20:16:54.082816: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 208 Chunks of size 256 totalling 52.0KiB\r\n2019-09-16 20:16:54.082903: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 512 totalling 512B\r\n2019-09-16 20:16:54.082986: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 1280 totalling 1.3KiB\r\n2019-09-16 20:16:54.083071: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 2560 totalling 10.0KiB\r\n2019-09-16 20:16:54.083421: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 4096 totalling 4.0KiB\r\n2019-09-16 20:16:54.083509: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 23 Chunks of size 20480 totalling 460.0KiB\r\n2019-09-16 20:16:54.083598: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 25600 totalling 50.0KiB\r\n2019-09-16 20:16:54.083685: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 40448 totalling 39.5KiB\r\n2019-09-16 20:16:54.083773: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 16 Chunks of size 147456 totalling 2.25MiB\r\n2019-09-16 20:16:54.083862: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 184320 totalling 720.0KiB\r\n2019-09-16 20:16:54.083949: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 313600 totalling 306.3KiB\r\n2019-09-16 20:16:54.084035: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 802816 totalling 3.06MiB\r\n2019-09-16 20:16:54.084122: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 7 Chunks of size 1254400 totalling 8.37MiB\r\n2019-09-16 20:16:54.084208: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 8 Chunks of size 5017600 totalling 38.28MiB\r\n2019-09-16 20:16:54.084294: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 11796480 totalling 45.00MiB\r\n2019-09-16 20:16:54.084418: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 20070400 totalling 38.28MiB\r\n2019-09-16 20:16:54.084505: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 1605632000 totalling 2.99GiB\r\n2019-09-16 20:16:54.084593: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 1659314176 totalling 1.54GiB\r\n2019-09-16 20:16:54.084680: I tensorflow/core/common_runtime/bfc_allocator.cc:816] Sum Total of in-use chunks: 4.67GiB\r\n2019-09-16 20:16:54.084764: I tensorflow/core/common_runtime/bfc_allocator.cc:818] total_region_allocated_bytes_: 5015306240 memory_limit_: 10485760000 available bytes: 5470453760 curr_region_allocation_bytes_: 20971520000\r\n2019-09-16 20:16:54.084920: I tensorflow/core/common_runtime/bfc_allocator.cc:824] Stats: \r\nLimit:                 10485760000\r\nInUse:                  5014080000\r\nMaxInUse:               5014080000\r\nNumAllocs:                     587\r\nMaxAllocSize:           2250966016\r\n\r\n2019-09-16 20:16:54.085118: W tensorflow/core/common_runtime/bfc_allocator.cc:319] ***************************************************************************************************x\r\n2019-09-16 20:16:54.085769: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at constant_op.cc:172 : Resource exhausted: OOM when allocating tensor with shape[100,64,7,7] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n2019-09-16 20:16:54.086319: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:16:54.086506: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:17:04.087051: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:17:04.087286: E tensorflow/stream_executor/cuda/cuda_driver.cc:828] failed to allocate 5.09G (5470453760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-09-16 20:17:04.087508: W tensorflow/core/common_runtime/bfc_allocator.cc:314] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.50GiB (rounded to 1605632000).  Current allocation summary follows.\r\n2019-09-16 20:17:04.087772: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (256): \tTotal Chunks: 208, Chunks in use: 207. 52.0KiB allocated for chunks. 51.8KiB in use in bin. 34.9KiB client-requested in use in bin.\r\n2019-09-16 20:17:04.088032: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (512): \tTotal Chunks: 2, Chunks in use: 1. 1.0KiB allocated for chunks. 512B in use in bin. 400B client-requested in use in bin.\r\n2019-09-16 20:17:04.088290: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1024): \tTotal Chunks: 2, Chunks in use: 1. 2.3KiB allocated for chunks. 1.3KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2019-09-16 20:17:04.088568: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2048): \tTotal Chunks: 4, Chunks in use: 4. 10.0KiB allocated for chunks. 10.0KiB in use in bin. 10.0KiB client-requested in use in bin.\r\n2019-09-16 20:17:04.088849: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4096): \tTotal Chunks: 1, Chunks in use: 1. 4.0KiB allocated for chunks. 4.0KiB in use in bin. 3.9KiB client-requested in use in bin.\r\n2019-09-16 20:17:04.089129: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:17:04.089391: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16384): \tTotal Chunks: 25, Chunks in use: 25. 510.0KiB allocated for chunks. 510.0KiB in use in bin. 510.0KiB client-requested in use in bin.\r\n2019-09-16 20:17:04.089688: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (32768): \tTotal Chunks: 1, Chunks in use: 1. 39.5KiB allocated for chunks. 39.5KiB in use in bin. 25.0KiB client-requested in use in bin.\r\n2019-09-16 20:17:04.089978: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:17:04.091082: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (131072): \tTotal Chunks: 20, Chunks in use: 20. 2.95MiB allocated for chunks. 2.95MiB in use in bin. 2.95MiB client-requested in use in bin.\r\n2019-09-16 20:17:04.091375: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (262144): \tTotal Chunks: 1, Chunks in use: 1. 306.3KiB allocated for chunks. 306.3KiB in use in bin. 306.3KiB client-requested in use in bin.\r\n2019-09-16 20:17:04.091664: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (524288): \tTotal Chunks: 4, Chunks in use: 4. 3.06MiB allocated for chunks. 3.06MiB in use in bin. 3.06MiB client-requested in use in bin.\r\n2019-09-16 20:17:04.091967: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (1048576): \tTotal Chunks: 8, Chunks in use: 7. 9.54MiB allocated for chunks. 8.37MiB in use in bin. 8.37MiB client-requested in use in bin.\r\n2019-09-16 20:17:04.092253: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:17:04.092525: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (4194304): \tTotal Chunks: 8, Chunks in use: 8. 38.28MiB allocated for chunks. 38.28MiB in use in bin. 38.28MiB client-requested in use in bin.\r\n2019-09-16 20:17:04.092817: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (8388608): \tTotal Chunks: 4, Chunks in use: 4. 45.00MiB allocated for chunks. 45.00MiB in use in bin. 45.00MiB client-requested in use in bin.\r\n2019-09-16 20:17:04.093622: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (16777216): \tTotal Chunks: 2, Chunks in use: 2. 38.28MiB allocated for chunks. 38.28MiB in use in bin. 38.28MiB client-requested in use in bin.\r\n2019-09-16 20:17:04.094067: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:17:04.094540: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:17:04.094909: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-09-16 20:17:04.095316: I tensorflow/core/common_runtime/bfc_allocator.cc:764] Bin (268435456): \tTotal Chunks: 3, Chunks in use: 3. 4.54GiB allocated for chunks. 4.54GiB in use in bin. 4.49GiB client-requested in use in bin.\r\n2019-09-16 20:17:04.095799: I tensorflow/core/common_runtime/bfc_allocator.cc:780] Bin for 1.50GiB was 256.00MiB, Chunk State: \r\n2019-09-16 20:17:04.096151: I tensorflow/core/common_runtime/bfc_allocator.cc:793] Next region of size 5015306240\r\n2019-09-16 20:17:04.096348: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0000000509E00000 next 77 of size 256\r\n2019-09-16 20:17:04.096635: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0000000509E00100 next 78 of size 256\r\n2019-09-16 20:17:04.096922: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 0000000509E00200 next 183 of size 11796480\r\n2019-09-16 20:17:04.097233: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A940200 next 184 of size 256\r\n2019-09-16 20:17:04.097490: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A940300 next 158 of size 256\r\n2019-09-16 20:17:04.097799: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A940400 next 159 of size 184320\r\n2019-09-16 20:17:04.098110: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96D400 next 156 of size 256\r\n2019-09-16 20:17:04.098314: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96D500 next 157 of size 256\r\n2019-09-16 20:17:04.098586: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96D600 next 177 of size 2560\r\n2019-09-16 20:17:04.098861: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E000 next 178 of size 256\r\n2019-09-16 20:17:04.099106: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E100 next 174 of size 256\r\n2019-09-16 20:17:04.099384: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E200 next 175 of size 256\r\n2019-09-16 20:17:04.099661: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E300 next 173 of size 256\r\n2019-09-16 20:17:04.099922: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E400 next 151 of size 256\r\n2019-09-16 20:17:04.100201: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050A96E500 next 152 of size 802816\r\n2019-09-16 20:17:04.100358: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32500 next 135 of size 256\r\n2019-09-16 20:17:04.100448: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32600 next 136 of size 256\r\n2019-09-16 20:17:04.100537: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32700 next 162 of size 256\r\n2019-09-16 20:17:04.100629: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32800 next 163 of size 256\r\n2019-09-16 20:17:04.100726: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32900 next 161 of size 256\r\n2019-09-16 20:17:04.100824: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32A00 next 166 of size 256\r\n2019-09-16 20:17:04.100953: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32B00 next 167 of size 256\r\n2019-09-16 20:17:04.101066: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32C00 next 160 of size 256\r\n2019-09-16 20:17:04.101160: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32D00 next 145 of size 256\r\n2019-09-16 20:17:04.101290: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32E00 next 146 of size 256\r\n2019-09-16 20:17:04.101383: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA32F00 next 154 of size 256\r\n2019-09-16 20:17:04.101541: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33000 next 155 of size 256\r\n2019-09-16 20:17:04.101674: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33100 next 153 of size 256\r\n2019-09-16 20:17:04.101769: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33200 next 150 of size 256\r\n2019-09-16 20:17:04.101862: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33300 next 149 of size 256\r\n2019-09-16 20:17:04.102423: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33400 next 148 of size 256\r\n2019-09-16 20:17:04.102764: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33500 next 164 of size 256\r\n2019-09-16 20:17:04.102925: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33600 next 144 of size 256\r\n2019-09-16 20:17:04.103028: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33700 next 143 of size 256\r\n2019-09-16 20:17:04.103155: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33800 next 83 of size 256\r\n2019-09-16 20:17:04.103519: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33900 next 84 of size 256\r\n2019-09-16 20:17:04.103663: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33A00 next 141 of size 256\r\n2019-09-16 20:17:04.103754: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33B00 next 142 of size 256\r\n2019-09-16 20:17:04.103845: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33C00 next 82 of size 256\r\n2019-09-16 20:17:04.103937: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33D00 next 140 of size 256\r\n2019-09-16 20:17:04.104199: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33E00 next 90 of size 256\r\n2019-09-16 20:17:04.104289: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA33F00 next 91 of size 256\r\n2019-09-16 20:17:04.104377: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34000 next 139 of size 256\r\n2019-09-16 20:17:04.104468: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34100 next 97 of size 256\r\n2019-09-16 20:17:04.104559: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34200 next 98 of size 256\r\n2019-09-16 20:17:04.104649: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34300 next 137 of size 256\r\n2019-09-16 20:17:04.104739: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34400 next 134 of size 256\r\n2019-09-16 20:17:04.104829: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34500 next 133 of size 256\r\n2019-09-16 20:17:04.104922: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA34600 next 106 of size 20480\r\n2019-09-16 20:17:04.105016: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA39600 next 107 of size 20480\r\n2019-09-16 20:17:04.105111: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA3E600 next 110 of size 20480\r\n2019-09-16 20:17:04.105205: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA43600 next 111 of size 20480\r\n2019-09-16 20:17:04.105297: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA48600 next 165 of size 20480\r\n2019-09-16 20:17:04.105388: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA4D600 next 130 of size 256\r\n2019-09-16 20:17:04.105476: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA4D700 next 131 of size 256\r\n2019-09-16 20:17:04.105567: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA4D800 next 129 of size 147456\r\n2019-09-16 20:17:04.105660: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA71800 next 124 of size 256\r\n2019-09-16 20:17:04.105752: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA71900 next 125 of size 147456\r\n2019-09-16 20:17:04.105844: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95900 next 118 of size 256\r\n2019-09-16 20:17:04.105941: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95A00 next 119 of size 256\r\n2019-09-16 20:17:04.106522: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95B00 next 117 of size 256\r\n2019-09-16 20:17:04.106651: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95C00 next 138 of size 256\r\n2019-09-16 20:17:04.106811: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95D00 next 121 of size 256\r\n2019-09-16 20:17:04.106936: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AA95E00 next 122 of size 147456\r\n2019-09-16 20:17:04.107028: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAB9E00 next 115 of size 256\r\n2019-09-16 20:17:04.107118: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAB9F00 next 116 of size 147456\r\n2019-09-16 20:17:04.107209: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADDF00 next 114 of size 256\r\n2019-09-16 20:17:04.107299: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE000 next 108 of size 256\r\n2019-09-16 20:17:04.107544: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE100 next 109 of size 256\r\n2019-09-16 20:17:04.107636: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE200 next 113 of size 256\r\n2019-09-16 20:17:04.107725: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE300 next 102 of size 256\r\n2019-09-16 20:17:04.107816: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE400 next 103 of size 256\r\n2019-09-16 20:17:04.107907: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE500 next 128 of size 256\r\n2019-09-16 20:17:04.107996: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE600 next 147 of size 256\r\n2019-09-16 20:17:04.108086: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AADE700 next 100 of size 20480\r\n2019-09-16 20:17:04.108177: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE3700 next 101 of size 256\r\n2019-09-16 20:17:04.108268: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE3800 next 95 of size 256\r\n2019-09-16 20:17:04.108359: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE3900 next 96 of size 1280\r\n2019-09-16 20:17:04.108451: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE3E00 next 93 of size 256\r\n2019-09-16 20:17:04.108542: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE3F00 next 94 of size 2560\r\n2019-09-16 20:17:04.108633: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AAE4900 next 92 of size 184320\r\n2019-09-16 20:17:04.108723: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AB11900 next 123 of size 2560\r\n2019-09-16 20:17:04.108813: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050AB12300 next 126 of size 11796480\r\n2019-09-16 20:17:04.108905: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B652300 next 127 of size 184320\r\n2019-09-16 20:17:04.109389: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B67F300 next 87 of size 2560\r\n2019-09-16 20:17:04.109480: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B67FD00 next 88 of size 256\r\n2019-09-16 20:17:04.109567: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B67FE00 next 86 of size 256\r\n2019-09-16 20:17:04.109655: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B67FF00 next 104 of size 256\r\n2019-09-16 20:17:04.109742: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B680000 next 105 of size 256\r\n2019-09-16 20:17:04.109831: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B680100 next 85 of size 256\r\n2019-09-16 20:17:04.109921: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B680200 next 89 of size 802816\r\n2019-09-16 20:17:04.110011: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B744200 next 79 of size 256\r\n2019-09-16 20:17:04.110101: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B744300 next 182 of size 256\r\n2019-09-16 20:17:04.110189: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B744400 next 81 of size 802816\r\n2019-09-16 20:17:04.110281: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B808400 next 180 of size 256\r\n2019-09-16 20:17:04.110372: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B808500 next 181 of size 256\r\n2019-09-16 20:17:04.110463: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B808600 next 179 of size 256\r\n2019-09-16 20:17:04.110552: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B808700 next 99 of size 802816\r\n2019-09-16 20:17:04.110644: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CC700 next 80 of size 256\r\n2019-09-16 20:17:04.110733: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CC800 next 132 of size 256\r\n2019-09-16 20:17:04.110822: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CC900 next 176 of size 256\r\n2019-09-16 20:17:04.110915: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CCA00 next 120 of size 256\r\n2019-09-16 20:17:04.111456: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CCB00 next 172 of size 256\r\n2019-09-16 20:17:04.111613: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050B8CCC00 next 170 of size 11796480\r\n2019-09-16 20:17:04.111810: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40CC00 next 171 of size 256\r\n2019-09-16 20:17:04.111945: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40CD00 next 169 of size 256\r\n2019-09-16 20:17:04.112035: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40CE00 next 112 of size 256\r\n2019-09-16 20:17:04.112124: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40CF00 next 168 of size 256\r\n2019-09-16 20:17:04.112213: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40D000 next 76 of size 256\r\n2019-09-16 20:17:04.112302: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40D100 next 75 of size 256\r\n2019-09-16 20:17:04.112389: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050C40D200 next 74 of size 11796480\r\n2019-09-16 20:17:04.112481: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D200 next 73 of size 256\r\n2019-09-16 20:17:04.112571: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D300 next 72 of size 256\r\n2019-09-16 20:17:04.112660: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D400 next 71 of size 256\r\n2019-09-16 20:17:04.112750: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D500 next 70 of size 256\r\n2019-09-16 20:17:04.112841: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D600 next 69 of size 256\r\n2019-09-16 20:17:04.112932: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D700 next 68 of size 256\r\n2019-09-16 20:17:04.113316: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D800 next 67 of size 256\r\n2019-09-16 20:17:04.113408: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF4D900 next 66 of size 184320\r\n2019-09-16 20:17:04.113500: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7A900 next 65 of size 256\r\n2019-09-16 20:17:04.113590: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7AA00 next 64 of size 20480\r\n2019-09-16 20:17:04.113683: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FA00 next 63 of size 256\r\n2019-09-16 20:17:04.113773: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FB00 next 62 of size 256\r\n2019-09-16 20:17:04.113863: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FC00 next 61 of size 256\r\n2019-09-16 20:17:04.113954: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FD00 next 60 of size 256\r\n2019-09-16 20:17:04.114041: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FE00 next 59 of size 256\r\n2019-09-16 20:17:04.114128: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF7FF00 next 58 of size 20480\r\n2019-09-16 20:17:04.114216: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CF84F00 next 57 of size 147456\r\n2019-09-16 20:17:04.114306: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA8F00 next 56 of size 256\r\n2019-09-16 20:17:04.114394: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9000 next 55 of size 256\r\n2019-09-16 20:17:04.114481: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9100 next 54 of size 256\r\n2019-09-16 20:17:04.114570: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9200 next 53 of size 256\r\n2019-09-16 20:17:04.114657: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9300 next 52 of size 256\r\n2019-09-16 20:17:04.114745: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9400 next 51 of size 256\r\n2019-09-16 20:17:04.114835: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFA9500 next 50 of size 20480\r\n2019-09-16 20:17:04.114927: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAE500 next 49 of size 256\r\n2019-09-16 20:17:04.115485: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAE600 next 48 of size 256\r\n2019-09-16 20:17:04.115578: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAE700 next 47 of size 256\r\n2019-09-16 20:17:04.115668: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAE800 next 46 of size 256\r\n2019-09-16 20:17:04.115758: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAE900 next 45 of size 256\r\n2019-09-16 20:17:04.115848: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAEA00 next 44 of size 256\r\n2019-09-16 20:17:04.115939: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFAEB00 next 43 of size 20480\r\n2019-09-16 20:17:04.116070: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFB3B00 next 42 of size 256\r\n2019-09-16 20:17:04.116160: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFB3C00 next 41 of size 147456\r\n2019-09-16 20:17:04.116253: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFD7C00 next 40 of size 256\r\n2019-09-16 20:17:04.116343: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFD7D00 next 39 of size 20480\r\n2019-09-16 20:17:04.116435: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFDCD00 next 38 of size 256\r\n2019-09-16 20:17:04.116523: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFDCE00 next 37 of size 256\r\n2019-09-16 20:17:04.116610: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFDCF00 next 36 of size 256\r\n2019-09-16 20:17:04.116698: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFDD000 next 35 of size 256\r\n2019-09-16 20:17:04.116785: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFDD100 next 34 of size 20480\r\n2019-09-16 20:17:04.116875: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFE2100 next 33 of size 20480\r\n2019-09-16 20:17:04.117394: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050CFE7100 next 32 of size 147456\r\n2019-09-16 20:17:04.117536: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D00B100 next 31 of size 256\r\n2019-09-16 20:17:04.117658: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D00B200 next 30 of size 147456\r\n2019-09-16 20:17:04.117749: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D02F200 next 29 of size 256\r\n2019-09-16 20:17:04.117837: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D02F300 next 28 of size 256\r\n2019-09-16 20:17:04.117928: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D02F400 next 27 of size 256\r\n2019-09-16 20:17:04.118018: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D02F500 next 26 of size 20480\r\n2019-09-16 20:17:04.118109: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D034500 next 25 of size 20480\r\n2019-09-16 20:17:04.118200: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D039500 next 24 of size 147456\r\n2019-09-16 20:17:04.118291: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D05D500 next 23 of size 147456\r\n2019-09-16 20:17:04.118383: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081500 next 22 of size 256\r\n2019-09-16 20:17:04.118473: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081600 next 21 of size 256\r\n2019-09-16 20:17:04.118564: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081700 next 20 of size 256\r\n2019-09-16 20:17:04.118654: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081800 next 19 of size 256\r\n2019-09-16 20:17:04.118744: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081900 next 18 of size 256\r\n2019-09-16 20:17:04.118834: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081A00 next 17 of size 256\r\n2019-09-16 20:17:04.118923: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081B00 next 16 of size 256\r\n2019-09-16 20:17:04.119380: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D081C00 next 15 of size 147456\r\n2019-09-16 20:17:04.119472: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0A5C00 next 14 of size 256\r\n2019-09-16 20:17:04.119560: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0A5D00 next 13 of size 147456\r\n2019-09-16 20:17:04.119649: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0C9D00 next 12 of size 256\r\n2019-09-16 20:17:04.119783: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0C9E00 next 11 of size 256\r\n2019-09-16 20:17:04.119872: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0C9F00 next 10 of size 20480\r\n2019-09-16 20:17:04.119997: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0CEF00 next 9 of size 147456\r\n2019-09-16 20:17:04.120087: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0F2F00 next 8 of size 256\r\n2019-09-16 20:17:04.120175: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0F3000 next 7 of size 256\r\n2019-09-16 20:17:04.120264: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D0F3100 next 6 of size 147456\r\n2019-09-16 20:17:04.120354: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D117100 next 5 of size 147456\r\n2019-09-16 20:17:04.120445: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D13B100 next 4 of size 147456\r\n2019-09-16 20:17:04.120537: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D15F100 next 3 of size 256\r\n2019-09-16 20:17:04.120626: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D15F200 next 2 of size 256\r\n2019-09-16 20:17:04.120714: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D15F300 next 1 of size 20480\r\n2019-09-16 20:17:04.120807: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164300 next 185 of size 256\r\n2019-09-16 20:17:04.120901: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164400 next 186 of size 256\r\n2019-09-16 20:17:04.121380: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164500 next 187 of size 256\r\n2019-09-16 20:17:04.121476: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164600 next 188 of size 256\r\n2019-09-16 20:17:04.121566: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164700 next 189 of size 256\r\n2019-09-16 20:17:04.121655: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164800 next 190 of size 256\r\n2019-09-16 20:17:04.121756: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164900 next 191 of size 256\r\n2019-09-16 20:17:04.121878: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164A00 next 192 of size 256\r\n2019-09-16 20:17:04.121967: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164B00 next 193 of size 256\r\n2019-09-16 20:17:04.122055: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164C00 next 194 of size 256\r\n2019-09-16 20:17:04.122144: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164D00 next 195 of size 256\r\n2019-09-16 20:17:04.122233: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164E00 next 196 of size 256\r\n2019-09-16 20:17:04.122322: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D164F00 next 197 of size 256\r\n2019-09-16 20:17:04.122411: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165000 next 198 of size 256\r\n2019-09-16 20:17:04.122499: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165100 next 199 of size 256\r\n2019-09-16 20:17:04.122587: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165200 next 200 of size 256\r\n2019-09-16 20:17:04.122677: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165300 next 201 of size 256\r\n2019-09-16 20:17:04.122766: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165400 next 202 of size 256\r\n2019-09-16 20:17:04.122857: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165500 next 203 of size 256\r\n2019-09-16 20:17:04.123341: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165600 next 204 of size 256\r\n2019-09-16 20:17:04.123523: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165700 next 205 of size 256\r\n2019-09-16 20:17:04.123653: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165800 next 206 of size 256\r\n2019-09-16 20:17:04.123801: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165900 next 207 of size 256\r\n2019-09-16 20:17:04.123947: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165A00 next 208 of size 256\r\n2019-09-16 20:17:04.124037: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165B00 next 209 of size 256\r\n2019-09-16 20:17:04.124127: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165C00 next 210 of size 256\r\n2019-09-16 20:17:04.124217: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165D00 next 211 of size 256\r\n2019-09-16 20:17:04.124306: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165E00 next 212 of size 256\r\n2019-09-16 20:17:04.124397: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D165F00 next 213 of size 256\r\n2019-09-16 20:17:04.124487: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166000 next 214 of size 256\r\n2019-09-16 20:17:04.124579: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166100 next 215 of size 256\r\n2019-09-16 20:17:04.124670: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166200 next 216 of size 256\r\n2019-09-16 20:17:04.124761: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166300 next 217 of size 256\r\n2019-09-16 20:17:04.124849: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166400 next 218 of size 256\r\n2019-09-16 20:17:04.124938: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166500 next 219 of size 256\r\n2019-09-16 20:17:04.125027: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166600 next 220 of size 256\r\n2019-09-16 20:17:04.125115: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166700 next 221 of size 256\r\n2019-09-16 20:17:04.125202: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166800 next 222 of size 256\r\n2019-09-16 20:17:04.125290: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166900 next 223 of size 256\r\n2019-09-16 20:17:04.125377: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166A00 next 224 of size 256\r\n2019-09-16 20:17:04.125465: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166B00 next 225 of size 256\r\n2019-09-16 20:17:04.125553: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166C00 next 226 of size 256\r\n2019-09-16 20:17:04.125646: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166D00 next 227 of size 256\r\n2019-09-16 20:17:04.125738: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166E00 next 228 of size 256\r\n2019-09-16 20:17:04.125831: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D166F00 next 229 of size 256\r\n2019-09-16 20:17:04.126439: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167000 next 230 of size 256\r\n2019-09-16 20:17:04.126533: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167100 next 231 of size 256\r\n2019-09-16 20:17:04.126624: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167200 next 232 of size 256\r\n2019-09-16 20:17:04.126716: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167300 next 233 of size 256\r\n2019-09-16 20:17:04.126810: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167400 next 234 of size 256\r\n2019-09-16 20:17:04.127053: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167500 next 235 of size 256\r\n2019-09-16 20:17:04.127186: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167600 next 236 of size 256\r\n2019-09-16 20:17:04.127333: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167700 next 237 of size 256\r\n2019-09-16 20:17:04.127422: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167800 next 238 of size 256\r\n2019-09-16 20:17:04.127510: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167900 next 239 of size 256\r\n2019-09-16 20:17:04.127599: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167A00 next 240 of size 256\r\n2019-09-16 20:17:04.127687: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167B00 next 241 of size 256\r\n2019-09-16 20:17:04.127775: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167C00 next 242 of size 256\r\n2019-09-16 20:17:04.127864: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167D00 next 243 of size 256\r\n2019-09-16 20:17:04.127952: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167E00 next 244 of size 256\r\n2019-09-16 20:17:04.128040: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D167F00 next 245 of size 20480\r\n2019-09-16 20:17:04.128129: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D16CF00 next 246 of size 20480\r\n2019-09-16 20:17:04.128220: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D171F00 next 247 of size 20480\r\n2019-09-16 20:17:04.128313: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D176F00 next 248 of size 20480\r\n2019-09-16 20:17:04.128404: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 000000050D17BF00 next 249 of size 1024\r\n2019-09-16 20:17:04.128496: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D17C300 next 250 of size 313600\r\n2019-09-16 20:17:04.128590: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C8C00 next 252 of size 512\r\n2019-09-16 20:17:04.128681: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C8E00 next 253 of size 256\r\n2019-09-16 20:17:04.128773: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C8F00 next 290 of size 256\r\n2019-09-16 20:17:04.128866: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9000 next 291 of size 256\r\n2019-09-16 20:17:04.129417: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 000000050D1C9100 next 257 of size 512\r\n2019-09-16 20:17:04.129511: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9300 next 258 of size 256\r\n2019-09-16 20:17:04.129601: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9400 next 259 of size 256\r\n2019-09-16 20:17:04.129689: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9500 next 283 of size 256\r\n2019-09-16 20:17:04.129776: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9600 next 261 of size 256\r\n2019-09-16 20:17:04.129866: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9700 next 262 of size 256\r\n2019-09-16 20:17:04.129953: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9800 next 263 of size 256\r\n2019-09-16 20:17:04.130041: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9900 next 277 of size 256\r\n2019-09-16 20:17:04.130129: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9A00 next 265 of size 256\r\n2019-09-16 20:17:04.130217: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 000000050D1C9B00 next 266 of size 256\r\n2019-09-16 20:17:04.130340: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000050D1C9C00 next 267 of size 1605632000\r\n2019-09-16 20:17:04.130433: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 000000056CD09C00 next 268 of size 1605632000\r\n2019-09-16 20:17:04.130528: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC849C00 next 274 of size 256\r\n2019-09-16 20:17:04.130619: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC849D00 next 275 of size 256\r\n2019-09-16 20:17:04.130710: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC849E00 next 270 of size 40448\r\n2019-09-16 20:17:04.130802: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC853C00 next 271 of size 20480\r\n2019-09-16 20:17:04.131334: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC858C00 next 272 of size 20480\r\n2019-09-16 20:17:04.131470: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CC85DC00 next 273 of size 20070400\r\n2019-09-16 20:17:04.131565: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CDB81C00 next 269 of size 20070400\r\n2019-09-16 20:17:04.131660: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CEEA5C00 next 276 of size 5017600\r\n2019-09-16 20:17:04.131753: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CF36EC00 next 264 of size 5017600\r\n2019-09-16 20:17:04.131846: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CF837C00 next 278 of size 5017600\r\n2019-09-16 20:17:04.132083: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005CFD00C00 next 260 of size 5017600\r\n2019-09-16 20:17:04.132178: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D01C9C00 next 279 of size 5017600\r\n2019-09-16 20:17:04.132269: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D0692C00 next 280 of size 5017600\r\n2019-09-16 20:17:04.132360: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D0B5BC00 next 281 of size 5017600\r\n2019-09-16 20:17:04.132451: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1024C00 next 282 of size 5017600\r\n2019-09-16 20:17:04.132542: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D14EDC00 next 284 of size 1254400\r\n2019-09-16 20:17:04.132633: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1620000 next 285 of size 1254400\r\n2019-09-16 20:17:04.132724: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1752400 next 251 of size 4096\r\n2019-09-16 20:17:04.132817: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1753400 next 256 of size 25600\r\n2019-09-16 20:17:04.132936: I tensorflow/core/common_runtime/bfc_allocator.cc:800] Free  at 00000005D1759800 next 286 of size 1224704\r\n2019-09-16 20:17:04.133075: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1884800 next 254 of size 1254400\r\n2019-09-16 20:17:04.133169: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D19B6C00 next 255 of size 1254400\r\n2019-09-16 20:17:04.133263: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1AE9000 next 287 of size 1254400\r\n2019-09-16 20:17:04.133358: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1C1B400 next 288 of size 1254400\r\n2019-09-16 20:17:04.133455: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1D4D800 next 289 of size 1254400\r\n2019-09-16 20:17:04.133553: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1E7FC00 next 292 of size 25600\r\n2019-09-16 20:17:04.133649: I tensorflow/core/common_runtime/bfc_allocator.cc:800] InUse at 00000005D1E86000 next 18446744073709551615 of size 1659314176\r\n2019-09-16 20:17:04.133756: I tensorflow/core/common_runtime/bfc_allocator.cc:809]      Summary of in-use Chunks by size: \r\n2019-09-16 20:17:04.133854: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 207 Chunks of size 256 totalling 51.8KiB\r\n2019-09-16 20:17:04.134492: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 512 totalling 512B\r\n2019-09-16 20:17:04.134580: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 1280 totalling 1.3KiB\r\n2019-09-16 20:17:04.134667: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 2560 totalling 10.0KiB\r\n2019-09-16 20:17:04.134754: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 4096 totalling 4.0KiB\r\n2019-09-16 20:17:04.134840: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 23 Chunks of size 20480 totalling 460.0KiB\r\n2019-09-16 20:17:04.135087: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 25600 totalling 50.0KiB\r\n2019-09-16 20:17:04.135177: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 40448 totalling 39.5KiB\r\n2019-09-16 20:17:04.135265: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 16 Chunks of size 147456 totalling 2.25MiB\r\n2019-09-16 20:17:04.135353: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 184320 totalling 720.0KiB\r\n2019-09-16 20:17:04.135441: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 313600 totalling 306.3KiB\r\n2019-09-16 20:17:04.135529: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 802816 totalling 3.06MiB\r\n2019-09-16 20:17:04.135617: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 7 Chunks of size 1254400 totalling 8.37MiB\r\n2019-09-16 20:17:04.135708: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 8 Chunks of size 5017600 totalling 38.28MiB\r\n2019-09-16 20:17:04.135798: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 4 Chunks of size 11796480 totalling 45.00MiB\r\n2019-09-16 20:17:04.136059: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 20070400 totalling 38.28MiB\r\n2019-09-16 20:17:04.136189: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 1605632000 totalling 2.99GiB\r\n2019-09-16 20:17:04.136280: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 1659314176 totalling 1.54GiB\r\n2019-09-16 20:17:04.136372: I tensorflow/core/common_runtime/bfc_allocator.cc:816] Sum Total of in-use chunks: 4.67GiB\r\n2019-09-16 20:17:04.136458: I tensorflow/core/common_runtime/bfc_allocator.cc:818] total_region_allocated_bytes_: 5015306240 memory_limit_: 10485760000 available bytes: 5470453760 curr_region_allocation_bytes_: 20971520000\r\n2019-09-16 20:17:04.136750: I tensorflow/core/common_runtime/bfc_allocator.cc:824] Stats: \r\nLimit:                 10485760000\r\nInUse:                  5014079744\r\nMaxInUse:               5014080000\r\nNumAllocs:                     588\r\nMaxAllocSize:           2250966016\r\n\r\n2019-09-16 20:17:04.137348: W tensorflow/core/common_runtime/bfc_allocator.cc:319] ***************************************************************************************************x\r\n2019-09-16 20:17:04.137580: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at transpose_op.cc:199 : Resource exhausted: OOM when allocating tensor with shape[100,28,28,5120] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\r\n  (0) Resource exhausted: OOM when allocating tensor with shape[100,64,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node Train/gradients/zeros_30}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[Train/Mean/_19]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n  (1) Resource exhausted: OOM when allocating tensor with shape[100,64,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node Train/gradients/zeros_30}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"E:/workplace/classifier/fashion_mnist_bn_9.py\", line 239, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\absl\\app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"E:/workplace/classifier/fashion_mnist_bn_9.py\", line 194, in main\r\n    is_train: True})\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\r\n  (0) Resource exhausted: OOM when allocating tensor with shape[100,64,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[node Train/gradients/zeros_30 (defined at E:/workplace/classifier/fashion_mnist_bn_9.py:166) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[Train/Mean/_19]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n  (1) Resource exhausted: OOM when allocating tensor with shape[100,64,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[node Train/gradients/zeros_30 (defined at E:/workplace/classifier/fashion_mnist_bn_9.py:166) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nOriginal stack trace for 'Train/gradients/zeros_30':\r\n  File \"E:/workplace/classifier/fashion_mnist_bn_9.py\", line 239, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\absl\\app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"E:/workplace/classifier/fashion_mnist_bn_9.py\", line 166, in main\r\n    optimizer = tf.train.AdamOptimizer(3e-4).minimize(loss)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 403, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 512, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 158, in gradients\r\n    unconnected_gradients)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py\", line 722, in _GradientsHelper\r\n    out_grads[i] = control_flow_ops.ZerosLikeOutsideLoop(op, i)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 1357, in ZerosLikeOutsideLoop\r\n    return array_ops.zeros(zeros_shape, dtype=val.dtype)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1883, in zeros\r\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 4167, in fill\r\n    \"Fill\", dims=dims, value=value, name=name)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\31401\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n\r\nProcess finished with exit code 1\r\n\r\n```", "I tried on colab with tensorflow 1.14.0. Please take a look at gist [here](https://colab.sandbox.google.com/gist/gadagashwini/429a257794d9c423d988904c4dfe7f8e/untitled155.ipynb). Thanks!", "> I tried on colab with tensorflow 1.14.0. Please take a look at gist [here](https://colab.sandbox.google.com/gist/gadagashwini/429a257794d9c423d988904c4dfe7f8e/untitled155.ipynb). Thanks!\r\n\r\n In my experiment, I only have GTX1660 with less than 6GB GPU momery. In order to reproduce OOM error, I rapidly increased the channel number (5120) in first `conv` layer so that the OOM error can occur. On this premise\uff0cI added  `tf.config.experimental.VirtualDeviceConfiguration` function but found that it was useless. So I uploaded the code and error message.\r\n\r\nIt seems like Google Colab's GPU memory is larger than mine, which means that the OOM error will not occur in current configuration. It can not support the conclusion that `tf.config.experimental.VirtualDeviceConfiguration` function can increase the available GPU memory.\r\nYou can reproduce the error in 4 steps:\r\n\r\n1. Comment out the following codes:\r\n```\r\n# gpus = tf.config.experimental.list_physical_devices('GPU')\r\n# if gpus:\r\n#   try:\r\n#     tf.config.experimental.set_virtual_device_configuration(\r\n#         gpus[0],\r\n#         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100000),\r\n#          tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100000)])\r\n#     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n#     print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\r\n#   except RuntimeError as e:\r\n#     print(e)\r\n```\r\n\r\n2.  Increase the channel numbers in `conv` layer and `conv11` layer:\r\n```\r\n      with tf.name_scope('conv'):\r\n          conv_weights=tf.get_variable(\"w\", shape=[3, 3, NUM_CHANNELS, 10000],dtype=tf.float32,\r\n                                       # initializer=tf.keras.initializers.he_normal())\r\n                          initializer=tf.contrib.layers.xavier_initializer())\r\n          conv_biases=tf.get_variable(\"b\", shape=[10000],dtype=tf.float32,\r\n                                      # initializer=tf.keras.initializers.he_normal())\r\n                          initializer=tf.contrib.layers.xavier_initializer())\r\n          conv = tf.nn.conv2d(data,\r\n                              conv_weights,\r\n                              strides=[1, 1, 1, 1],\r\n                              padding='SAME')\r\n          conv=tf.nn.bias_add(conv, conv_biases)\r\n          convbn=tf.layers.batch_normalization(conv,training=is_train)\r\n          relu=tf.nn.selu(convbn,name='conv')\r\n\r\n      with tf.name_scope('conv11'):\r\n          conv_weights=tf.get_variable(\"w11\", shape=[3, 3, 10000, 64],dtype=tf.float32,\r\n                                       # initializer=tf.keras.initializers.he_normal())\r\n                          initializer=tf.contrib.layers.xavier_initializer())\r\n          conv_biases=tf.get_variable(\"b11\", shape=[64],dtype=tf.float32,\r\n                                      # initializer=tf.keras.initializers.he_normal())\r\n                          initializer=tf.contrib.layers.xavier_initializer())\r\n          conv = tf.nn.conv2d(relu,\r\n                              conv_weights,\r\n                              strides=[1, 1, 1, 1],\r\n                              padding='SAME')\r\n          conv=tf.nn.bias_add(conv, conv_biases)\r\n          convbn=tf.layers.batch_normalization(conv,training=is_train)\r\n          relu=tf.nn.selu(convbn,name='conv')\r\n```\r\n\r\n3.  Run this script and you will see the OOM error message. (I have run this code andseen the OOM error in Colab)\r\n\r\n4.  Restore the code in Step 1 and run the script again. You will see the OOM error again. (changing the `memory_limit` does not work.)\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    tf.config.experimental.set_virtual_device_configuration(\r\n        gpus[0],\r\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100000),\r\n         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=100000)])\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    print(e)\r\n```\r\nThere are 2 reasons to explain for this situation: \r\n\r\n1. I misuse the `tf.config.experimental.VirtualDeviceConfiguration` API. \r\n\r\n2. `tf.config.experimental.VirtualDeviceConfiguration` is still in progresss.\r\n\r\nAll in all,thanks for your reply and our patience.", "@ Liang-yc \r\nIs this still an issue", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32413\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32413\">No</a>\n"]}, {"number": 32412, "title": "[TF 2.0.0-rc0] Keras Model subclassing `dynamic=True` throws Error. Possible Regression bug of rc0", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n    - Mac OSX 10.13.6, intel i7, MacBook Pro (Retina, 13-inch, Early 2015), Intel Iris Graphics 6100 1536 MB, \r\n    - Google Colab (Both CPU and GPU)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary (pip)\r\n- TensorFlow version (use command below): 2.0.0-rc0\r\n` pip install tensorflow==2.0.0-rc0`\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: Intel Iris Graphics 6100 1536 MB (Stock Intel i7 CPU bundled)\r\n\r\n**Describe the current behavior**\r\nIn 2.0.0-rc0, While using Model Subclassing of tf,keras.Model and passing `dynamic=True`. Then when I call model.fit it throws `AttributeError: 'NoneType' object has no attribute 'dtype'`\r\n\r\n**Describe the expected behavior**\r\nmodel.fit should not throw any error. Note this was working fine in `2.0.0-beta1`\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport os\r\n\r\nprint(tf.__version__)\r\n\r\n\r\nclass ConvBn2D(tf.keras.Model):\r\n    def __init__(self, c_out, kernel_size=3):\r\n        super().__init__()\r\n        self.conv = tf.keras.layers.Conv2D(filters=c_out, kernel_size=kernel_size,\r\n                                           strides=1, padding=\"SAME\",\r\n                                           use_bias=False)\r\n        self.bn = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-7)\r\n\r\n    def call(self, inputs):\r\n        res = tf.nn.relu(self.bn(self.conv(inputs)))\r\n        return res\r\n\r\n\r\nclass FNet(tf.keras.Model):\r\n    def __init__(self, start_kernels=64, weight=0.125, **kwargs):\r\n        super().__init__(**kwargs)\r\n        c = start_kernels\r\n        self.max_pool = tf.keras.layers.MaxPooling2D()\r\n        self.init_conv_bn = ConvBn2D(c, kernel_size=3)\r\n        self.c0 = ConvBn2D(c, kernel_size=3)\r\n\r\n        self.c1 = ConvBn2D(c * 2, kernel_size=3)\r\n        self.c2 = ConvBn2D(c * 2, kernel_size=3)\r\n\r\n        self.c3 = ConvBn2D(c * 2, kernel_size=3)\r\n        self.c4 = ConvBn2D(c * 2, kernel_size=3)\r\n\r\n        self.pool = tf.keras.layers.GlobalMaxPool2D()\r\n        self.linear = tf.keras.layers.Dense(10, use_bias=False)\r\n        self.weight = weight\r\n\r\n    def call(self, x):\r\n        h = self.max_pool(self.c0(self.init_conv_bn(x)))\r\n        h = self.max_pool(self.c2(self.c1(h)))\r\n        h = self.max_pool(self.c4(self.c3(h)))\r\n        h = self.pool(h)\r\n        h = self.linear(h) * self.weight\r\n        return h\r\n\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\r\ntrain = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\ntrain = train.map(lambda x,y:(tf.cast(x,tf.float32),tf.cast(y,tf.int64))).map(lambda x,y:(x/255.0,y)).batch(512)\r\n\r\n# model = FNet(start_kernels=8)\r\n\r\nmodel = FNet(start_kernels=8,dynamic=True)\r\n\r\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),\r\n              loss=loss)\r\n\r\ncallbacks=[]\r\nmodel.fit(train, epochs=2, callbacks=callbacks,verbose=1)\r\n```\r\n\r\n\r\n**Other info / logs**\r\nRun code in `2.0.0-beta1` it works. in rc0 fails.\r\n\r\n[Colab Notebook with same issue](https://colab.research.google.com/drive/1dZaIwlgKRk_8FSKkdyQlwXsT3auqV6VX)\r\n", "comments": ["I have tried on colab with TF version 2.0.0-rc0 and was able to reproduce the issue.However i am not seeing any issue with TF 2.0.0-beta1 .Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/c1083c26816bb17ae69785f8d5a111cc/untitled174.ipynb). Thanks!", "this issue is no longer seen with 2.2.0-rc0. \r\n@faizanahemad could you please check and close if the issue if it is resolved. ", "Closing this now, @faizanahemad please feel free to re-open if you still continue to see the issue. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32412\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32412\">No</a>\n"]}, {"number": 32411, "title": "Fix simple typo: activiations -> activations", "body": "", "comments": []}, {"number": 32409, "title": "keras.backend.learning_phase() broken when sample_weights are used", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n\r\n- TensorFlow version (use command below):\r\n1.14.0\r\n\r\n- Python version:\r\n3.7\r\n\r\n**Describe the current behavior**\r\n\r\nThere are multiple issues:\r\n\r\n- *Issue 1* keras.backend.learning_phase() does not always return a scalar value (as stated by the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/backend/learning_phase) but it returns multiple values.\r\n- *Issue 2* Even when though it returns multiple values, the shape says it is a 0d tensor indicating it is a scalar\r\n- *Issue 3* Reduce does not do anything even though the tensor has multiple values.\r\n\r\n**Describe the expected behavior**\r\n\r\n- *Issue 1* Documentation is fixed to reflect the correct behavior\r\n- *Issue 2* The right shape is set for the tensor\r\n- *Issue 3* `reduce_any` should reduce all values when no axis is given irrespective of the shape.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\nBATCH_SIZE = 4\r\n\r\nclass SimpleModel(keras.Model):\r\n\r\n  def __init__(self):\r\n    super(SimpleModel, self).__init__()\r\n    self._layer = keras.layers.Dense(1)\r\n\r\n  def call(self, inputs):\r\n    learning_phase = keras.backend.learning_phase()\r\n    inputs = tf.Print(inputs, [tf.constant(learning_phase.shape.ndims)],\r\n                      \"Looking at the ndims says this is a salar: \",\r\n                      summarize=BATCH_SIZE)\r\n    inputs = tf.Print(inputs, [learning_phase], \"But this is not a scalar: \",\r\n                      summarize=BATCH_SIZE)\r\n    inputs = tf.Print(inputs,\r\n                      [tf.reduce_any(learning_phase)],\r\n                      \"Even reduce_any does not create a scalar: \", summarize=BATCH_SIZE)\r\n    return self._layer(inputs)\r\n\r\n\r\ndef get_dataset(batch_size=BATCH_SIZE):\r\n  num_batches = 16\r\n  num_features = 5\r\n  inputs = np.random.random((num_batches, num_features))\r\n  labels = np.random.random((num_batches))\r\n  sample_weights = (np.random.random((num_batches)) > 0.5).astype(np.float32)\r\n  dataset = tf.data.Dataset.from_tensor_slices((inputs, labels, sample_weights))\r\n  return dataset.batch(batch_size).take(1)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  tf.logging.set_verbosity(tf.logging.ERROR)\r\n  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\n  print(\"-------------\")\r\n  print(tf.__version__)\r\n  print(\"-------------\")\r\n\r\n  model = SimpleModel()\r\n  optimizer = tf.keras.optimizers.SGD(learning_rate=0.005)\r\n  loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n  model.compile(optimizer, loss)\r\n  model.fit(get_dataset(), verbose=0)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n-------------\r\n1.14.0\r\n-------------\r\nLooking at the ndims says this is a salar: [0]\r\nBut this is not a scalar: [0 1 1 0]\r\nEven reduce_any does not create a scalar: [0 1 1 0]\r\n```", "comments": ["@pavanky Can you try with `TF1.15.0rc0` and let us know whether the issue persists? Thanks!", "@jvishnuvardhan Issue 1 no longer persists, but I am not sure how I can reproduce Issue 2 and Issue 3 without this Issue 1 being broken. If we can find a way to repro the remaining 2 issues, I can create a new ticket and close this one.", "@pavanky Issue2 and 3 depends on Issue1 right? If issue1 is no longer persists, then Issue2 and 3 are no longer valid i guess. Please let us know what you think. You could close the issue and open a new issue when you can reproduce issue 2 and 3. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32409\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32409\">No</a>\n"]}, {"number": 32408, "title": "The name \"resnet50\" is used 2 times in the model.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **tensorflow-gpu 1.14.0**\r\n- Python version: **3.7.3**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: \r\n\r\n---\r\n\r\nwhen i try to use more than one `keras.application.ResNet50` in my model, there shows a `ValueError: The name \"resnet50\" is used 2 times in the model. All layer names should be unique.` and i checked tensorflow's source code, the model name is unchangable.\r\n\r\nhere is my test code:\r\n\r\n```python\r\nfrom tensorflow.compat.v1 import enable_eager_execution\r\nfrom tensorflow.keras.layers import Concatenate, Dense, Input\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.applications import ResNet50\r\n\r\nimport os\r\n\r\nenable_eager_execution()\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\r\n\r\nres_net = ResNet50(include_top=False, input_shape=(224, 224, 3))\r\nres_net2 = ResNet50(include_top=False, input_shape=(224, 224, 3))\r\n\r\nx = Input(shape=(224, 224, 3), name='input')\r\ny = res_net(x)\r\nz = res_net2(x)\r\nz = Dense(1, name='output')(Concatenate(axis=-1)([y, z]))\r\n\r\nmodel = Model([x], [z])\r\n```\r\n\r\nand here is the logs:\r\n\r\n```text\r\nTraceback (most recent call last):\r\n  File \"draft.py\", line 19, in <module>\r\n    model = Model([x], [z])\r\n  File \"/home/xiefangyuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 129, in __init__\r\n    super(Model, self).__init__(*args, **kwargs)\r\n  File \"/home/xiefangyuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 162, in __init__\r\n    self._init_graph_network(*args, **kwargs)\r\n  File \"/home/xiefangyuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/xiefangyuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 315, in _init_graph_network\r\n    self.inputs, self.outputs)\r\n  File \"/home/xiefangyuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1862, in _map_graph_network\r\n    str(all_names.count(name)) + ' times in the model. '\r\nValueError: The name \"resnet50\" is used 2 times in the model. All layer names should be unique.\r\n```\r\n\r\nand hee is tensorflow's source code at line 274 in `keras_applications\\resnet50.py`:\r\n\r\n```pyton\r\n# Create model.\r\nmodel = models.Model(inputs, x, name='resnet50')\r\n```\r\n\r\ni think sometimes we need to change the model's name.", "comments": ["i think i just found the solution.\r\n\r\n```python\r\nfrom tensorflow.compat.v1 import enable_eager_execution\r\nfrom tensorflow.keras.layers import Concatenate, Dense, Input\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.applications import ResNet50\r\n\r\nimport os\r\n\r\nenable_eager_execution()\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\r\n\r\nres_net = ResNet50(include_top=False, input_shape=(224, 224, 3), pooling='avg')\r\n\r\nres_net = Model(inputs=res_net.input, outputs=res_net.get_layer('activation_39').output, name='resnet50_1')\r\nres_net2 = Model(inputs=res_net.input, outputs=res_net.get_layer('activation_39').output,  name='resnet50_2')\r\n\r\nx = Input(shape=(224, 224, 3), name='input')\r\ny = res_net(x)\r\nz = res_net2(x)\r\nz = Dense(1, name='output')(Concatenate(axis=-1)([y, z]))\r\n\r\nmodel = Model([x], [z])\r\n```\r\n\r\ni create new models and use the specific inputs layers and outputs layers. i can also change the name of my models."]}, {"number": 32407, "title": "Adding NMSv4 GPU implementation", "body": "This PR adds GPU implementation of NonMaxSuppresionOpV4.", "comments": ["@samikama Is there any plan to add CombinedNonMaxSuppression GPU version?", "@qinyao-he check #34852 "]}, {"number": 32406, "title": "Improve TensorRT version checks", "body": "TensorRT is not forward compatible.", "comments": []}, {"number": 32405, "title": "Failed to build on Ubuntu 18.04 with Ryzen CPU", "body": "<em>Error when build from source on Ubuntu 18.04 tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from source 1.14\r\n- TensorFlow version: 1.14\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 7.4\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: RTX 2060\r\n- CPU: Ryzen 5 2600\r\n\r\n**Describe the problem**\r\nI follow tutorial https://www.tensorflow.org/install/source, but It always throw error like this\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nbazel build --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" -- define=tensorflow_mkldnn_contraction_kernel=0 --verbose_failures  //tensorflow/tools/pip_package:build_pip_package\r\nor \r\nbazel build --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\nor \r\nbazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n**Any other info / logs**\r\nERROR: /home/tuan/Downloads/tensorflow-1.14.0/tensorflow/core/BUILD:3079:1: C++ compilation of rule '//tensorflow/core:core_cpu_base' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/tuan/.cache/bazel/_bazel_tuan/5cac32750bec0be81932199159ef575e/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    PATH=/home/tuan/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=7.5 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n    TF_NEED_TENSORRT=1 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/_objs/core_cpu_base/graph_constructor.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/_objs/core_cpu_base/graph_constructor.pic.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/k8-opt/genfiles -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/genfiles/external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote bazel-out/k8-opt/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote bazel-out/k8-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote bazel-out/k8-opt/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote bazel-out/k8-opt/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote bazel-out/k8-opt/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem bazel-out/k8-opt/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem bazel-out/k8-opt/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '-D_GLIBCXX_USE_CXX11_ABI=0' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -msse3 -pthread '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -c tensorflow/core/graph/graph_constructor.cc -o bazel-out/k8-opt/bin/tensorflow/core/_objs/core_cpu_base/graph_constructor.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nIn file included from external/protobuf_archive/src/google/protobuf/arena_impl.h:40:0,\r\n                 from external/protobuf_archive/src/google/protobuf/arena.h:51,\r\n                 from bazel-out/k8-opt/genfiles/tensorflow/core/framework/graph.pb.h:24,\r\n                 from ./tensorflow/core/graph/graph_constructor.h:19,\r\n                 from tensorflow/core/graph/graph_constructor.cc:16:\r\nexternal/protobuf_archive/src/google/protobuf/map.h: In instantiation of 'void google::protobuf::Map<Key, T>::InnerMap::iterator_base<KeyValueType>::SearchFrom(google::protobuf::Map<Key, T>::size_type) [with KeyValueType = google::protobuf::Map<unsigned int, std::basic_string<char> >::KeyValuePair; Key = unsigned int; T = std::basic_string<char>; google::protobuf::Map<Key, T>::size_type = long unsigned int]':\r\nexternal/protobuf_archive/src/google/protobuf/map.h:400:19:   required from 'google::protobuf::Map<Key, T>::InnerMap::iterator_base<KeyValueType>::iterator_base(const google::protobuf::Map<Key, T>::InnerMap*) [with KeyValueType = google::protobuf::Map<unsigned int, std::basic_string<char> >::KeyValuePair; Key = unsigned int; T = std::basic_string<char>]'\r\nexternal/protobuf_archive/src/google/protobuf/map.h:516:31:   required from 'google::protobuf::Map<Key, T>::InnerMap::iterator google::protobuf::Map<Key, T>::InnerMap::begin() [with Key = unsigned int; T = std::basic_string<char>; google::protobuf::Map<Key, T>::InnerMap::iterator = google::protobuf::Map<unsigned int, std::basic_string<char> >::InnerMap::iterator_base<google::protobuf::Map<unsigned int, std::basic_string<char> >::KeyValuePair>]'\r\nexternal/protobuf_archive/src/google/protobuf/map.h:1037:29:   required from 'google::protobuf::Map<Key, T>::iterator google::protobuf::Map<Key, T>::begin() [with Key = unsigned int; T = std::basic_string<char>]'\r\nexternal/protobuf_archive/src/google/protobuf/map.h:1147:29:   required from 'void google::protobuf::Map<Key, T>::clear() [with Key = unsigned int; T = std::basic_string<char>]'\r\nexternal/protobuf_archive/src/google/protobuf/map_field_inl.h:188:3:   required from 'void google::protobuf::internal::MapField<Derived, Key, T, key_wire_type, value_wire_type, default_enum_value>::Clear() [with Derived = tensorflow::DeviceStepStats_ThreadNamesEntry_DoNotUse; Key = unsigned int; T = std::basic_string<char>; google::protobuf::internal::WireFormatLite::FieldType kKeyFieldType = (google::protobuf::internal::WireFormatLite::FieldType)13; google::protobuf::internal::WireFormatLite::FieldType kValueFieldType = (google::protobuf::internal::WireFormatLite::FieldType)9; int default_enum_value = 0]'\r\nbazel-out/k8-opt/genfiles/tensorflow/core/framework/step_stats.pb.h:2329:23:   required from here\r\nexternal/protobuf_archive/src/google/protobuf/map.h:423:9: internal compiler error: Segmentation fault\r\n         GOOGLE_DCHECK(m_->index_of_first_non_null_ == m_->num_buckets_ ||\r\n         ^\r\nPlease submit a full bug report,\r\n```", "comments": ["I build tensorflow 1.14, 2.0 on Intel CPU and Nvidia GPU does not have any problem", "Can you try adding `-march=native`, as mentioned [here](https://github.com/yaroslavvb/tensorflow-community-wheels/issues/40)?", "> Can you try adding `-march=native`, as mentioned [here](https://github.com/yaroslavvb/tensorflow-community-wheels/issues/40)?\r\n\r\nthank you for your reply. But It still have error. I have to use prebuild package", "@babytut \r\nIs this still an issue", "yes, I still can not build it.", "I also change to build Tensorflow 2.2.0 from source. But it still have same error. Does anyone use AMD like me have same error ?", "FYI, I had similar problems with threadripper. I \"solved\" this by using fewer build threads with `--jobs=N` flag. Try to run `prime95` in \"torture\" mode. If it fails (which it does on my system), your system is not stable enough to build something as large as TF. Usually this is due to memory compatibility problems.", "Confirmed: replacing memory with ECC memory allows me to build 2.4.0 RC3 with parallelism of 64 on my machine without any issues.", "Thank you for your help. I will compile TF as you suggest.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32405\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32405\">No</a>\n"]}, {"number": 32404, "title": "Successfully opened dynamic library libcuda.so.1", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32404\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32404\">No</a>\n"]}, {"number": 32403, "title": "[r1.15Cherrypick]: Cherry-pick #30893 to r1.15", "body": "fix #32401 \r\n\r\n@mihaimaruseac ", "comments": ["@mihaimaruseac I would have preferred if the original commits have been kept from #30893. After all you already had them in the master and cherry-picking would be better than creating a new commit with a different author.", "Sorry for this. I always use the GitHub interface when creating cherry-picks.\r\n\r\nThe rest is just dependent on how you created the PR/cherry-pick.", "@mihaimaruseac I didn't create the cherry-pick, @ppwwyyxx did, I expected you would verify it is a cherry-pick before merging.", "Yes, we verify if commit content shows up but we don't verify author as authors of cherry-picks could be different than authors of original commit for all kinds of valid reasons.\r\n\r\nSorry this caused issues, I could have handled this better.", "IIRC I did a `git cherry-pick` and had to deal with some merge conflicts:\r\n```\r\n\u2570\u2500$git cherry-pick -m 1 5d6158e0d4a736a8ad2fc98b717fed519e4080f0\r\nerror: could not apply 5d6158e0d4... Merge pull request #30893 from samikama:GPUNMSFixes\r\nhint: after resolving the conflicts, mark the corrected paths\r\nhint: with 'git add <paths>' or 'git rm <paths>'\r\nhint: and commit the result with 'git commit'\r\n```\r\nafter resolving them I did a `git commit` following the instruction which probably created the commit with me being the author. Sorry for the mistake."]}]