[{"number": 37216, "title": "Tf 2.1, cause system crash and reboot", "body": "**System information** \r\n* POP_OS 19.10, Kernel: 5.3.0-7629-generic\r\n* Tensorflow: installed using `pip`, `__version__` 2.1.0\r\n* Nvidia_driver: 440.44, Cuda : 10.2 \r\n* GPU: Nvidia GTX 1080Ti , Memory : 11GB\r\n\r\n**Describe the current behavior**\r\n\r\nHi \r\nJust tried to run an example on `TF2.1`, every time right after starting training the system will crash immediately and will reboot (because of crash reboot there is no useful log available on `/var/log/`)\r\n\r\nSo decide to change my code to a simpler version and using the cifar10 dataset to check that, again system crash happend. this time I switched to `TF1.13.1` and run the same code and no problem, system work properly and at last, print the result.\r\n\r\neven tried on other versions `TF1.14`, `1.15` there is no problem (all versions are GPU support)\r\n\r\n\r\n**Standalone code to reproduce the issue** \r\n[sample code](https://github.com/pykeras/bugReport/blob/master/sample.py)\r\n", "comments": ["@pykeras \r\nI have tried on colab with TF version 2.1.0  and i am not seeing any issue. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/2f99c48131c5e08cf99534f0515105ac/untitled692.ipynb). Thanks!", "@ravikyram \r\n\r\nThanks for responde\r\n\r\nI tried different things on my OS, found that the problem is `BatchNormalization()` layer, every time when I remove these layers from codes (even the sample I shared or my other codes), there is no problem at all\r\n\r\nbut right after adding the BatchNorm layers, the system will crash\r\n\r\n", "Update: \r\n\r\nJust tried another thing, using another OS with same hardware solved the problem\r\nso I don't know what cause the problem, but sure this is not only about TF2.1 \r\n\r\nBecause of this I think should close the issue, Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37216\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37216\">No</a>\n", "excuse me\r\nThis may be related to [https://github.com/tensorflow/tensorflow/issues/36072](https://github.com/tensorflow/tensorflow/issues/36072)", "My system didn't crash but hanged after a few batches of first epoch.. At first i felt it is related to GPU temperature like someone mentioned above. But just updating the driver resolved it.. Thanks to the one who mentioned it.. Saved my life..!!!!"]}, {"number": 37215, "title": "TFLite Micro: SUB Op Support", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: Linux 5.5.5 Arch\r\n- TensorFlow installed from: source\r\n- Tensorflow version: 41c6bf7c6215bea9bfb9bf0a9b63f2084e6f3058\r\n- Target platform: OpenMV CAM (STM32H7)\r\n\r\n**Describe the problem**\r\n\r\nHello, I trained a simple dogs/cats classification model through transfer learning and then converted it to be run on the [OpenMV CAM](https://openmv.io/): [converted_model.mv1025128.tflite.zip](https://github.com/tensorflow/tensorflow/files/4272695/converted_model.mv1025128.tflite.zip).\r\n\r\nIt fails to load on the MCU inside the `InitializeRuntimeTensor` function when the SUB tensor is processed (you can see on [netron](https://lutzroeder.github.io/netron/) where SUB is needed).\r\n\r\nI found out the SUB op is not supported on tf lite micro, but, since I am quite new to ML, I would like to know if there is a specific reason to this missing support, if it can be *easily* added or if you can suggest a workaround.\r\n\r\nThank you\r\n", "comments": ["Hi,\r\nThere's a PR: https://github.com/tensorflow/tensorflow/pull/34894 by @giuseros. I think it covers your need? @njeffrie\r\nCheers!\r\nFredrik ", "Thank you, it's definitely good to know!\r\nOne thing I noticed though is that the problem arises while the SUB tensor is initialized as if the model file is not parsed properly. I do not know if there is an issue with the FlatBuffer generated schema, but what I see is that the `flatbuffer_tensor.shape()` (`tensorflow/lite/micro/memory_helpers.cc:82`) call returns a NULL pointer when the SUB tensor is processed.\r\nShould I open a different issue?", "@giuseros , do you have input on this?", "@iotwithit,\r\nSorry for the delayed response. **`SUB Operation`** seems to be added for [TF Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/all_ops_resolver.cc). Can you please confirm the same so that we can close this issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 37214, "title": "Fix np.squeeze call with dimensionality check", "body": "Calling `np.squeeze` is required only if `preds.shape[-1]` is equal to one.. Otherwise, the prediction can be returned as is. Added a conditional to check for this condition.", "comments": []}, {"number": 37213, "title": "Fix plot_model for .pdf files", "body": "If `plot_model` is asked for a .pdf file in Jupyter via the command `plot_model(model, to_file='model.pdf')`, a value error is raised:\r\n\r\n```\r\nValueError: Cannot embed the 'pdf' image format\r\n```\r\n\r\nThis is presumably because Jupyter Notebook does not support .pdf format. This PR fixes this issue by adding an additional check for the `extension` argument.", "comments": ["Also just found out that this issue was addressed in an identical fashion in the original Keras-team/keras repo. "]}, {"number": 37212, "title": "[MLIR][XLA] Buffer Assignment", "body": "In this PR, we have provided our first draft for generic XLA buffer assignment. This generic BufferAssignment class automatically analyzes the values and their aliases (also in other blocks) and returns the positions of allocations and deallocations. To find these positions, the algorithm uses the block Dominator and Post-Dominator analyses. In our proposed algorithm, we have considered aliasing, liveness analysis, nested regions, branches, conditional branches, critical edges, and independency to custom block terminators. This implementation doesn't support block loops. However, we have considered this in our design. For this purpose, we only need to have a loop analysis to allocate and deallocate some special cases outside of these loops. \r\n\r\nThis is the sample usage of BufferAssignment class:\r\n```c++\r\nBufferAssignment assignments(funcOp);\r\nauto positions = assignments.computeAllocAndDeallocPositions(value);\r\nallocBuilder.setInsertionPoint(positions.getAllocPosition());\r\n<create alloc>\r\ndeallocBuilder.setInsertionPointAfter(positions.getDeallocPosition());\r\n<create dealloc>\r\n```\r\nOr alternatively:\r\n```c++\r\nBufferAssignment assignments(funcOp);\r\nauto positions = assignments.computeAllocAndDeallocPositions(value);\r\npositions.insertAlloc<AllocOp>(...);\r\npositions.insertDealloc<DeallocOp>(...);\r\n```\r\n\r\nPlease note that this class should be used during the legalization (i.e. HLO-To-LHLO).", "comments": ["Refactored BufferAssignment analysis into two distinct parts:\r\n- BufferAssignmentLegalizer helper class that can be used during legalization.\r\n- BufferAssignmentPass that moves alloc and dealloc nodes (if any) into the\r\n  right positions. If there are not associated dealloc nodes for a given\r\n  alloc node, this pass inserts a DeallocOp in the right place automatically.", "Could you provide an example how the use of this would look like for hlo->lhlo? It is not clear to me how this composes.", "The intended use of this buffer assignment is that the dialect-expert creates an instance of BufferAssignmentLegalizer at the beginning of the pass (i.e. in the runOnFunction()) and passes this instance to all conversion-pattern classes using OwningRewritePatternList instance.\r\nThese conversion pattern classes could inherit from BufferAssignmentOpConversionPattern that has a protected BufferAssignmentLegalizer member. This will avoid creating BufferAssignmentLegalizer multiple times. In each conversion pattern class that the dialect-expert needs to allocate a buffer, they do the following:\r\n\r\n```c++\r\nBufferAssignmentOpConversionPattern<HloOpTy>::bufferAssignment->computeAllocPosition(result);\r\nauto alloc = allocPosition.insertAlloc<AllocOp>(result, loc, memref_type); // we are going to remove result from the list of arguments in the next commit since it's unnecessary.\r\n```\r\n\r\nLater on, --buffer-assignment flag must be passed to tf-opt to move Alloc and Dealloc operations in their proper positions. If there is no Dealloc for a buffer, it will be automatically inserted.", "@dfki-ehna Can you please address Ubuntu Sanity errors? Thanks!", "Lacking my comment being addressed, I issued a revert here: https://github.com/tensorflow/tensorflow/pull/38291", "This unfortunately leaves this PR to a terminal state. @dfki-ehna would you mind to open a new one? Thanks!", "@joker-eph Oh, I see \ud83d\udd22 Maybe we can add a test dialect including a legalization step in order to simulate lowering from one dialect to another one (or even the same) with buffers?", "Why do we need a test dialect? I may be missing something but the description of the pass is:\r\n\r\n```\r\n /// The actual buffer assignment pass that moves alloc and dealloc nodes into\r\n /// the right positions. \r\n```\r\n\r\nI was expecting this to be able to run on something like LHLO?\r\n", "@joker-eph The test dialect will be needed if we want to have this pass in MLIR core, since there is no  LHLO there. I think @dfki-mako was assuming this.", "@joker-eph I have already thought about this transfer and future features at the same time. It might be useful to have a small test dialect that contains additional features that we might want to think about and include in the general BA pass.", "@joker-eph Please note that this is not a strict requirement at the moment. We can include several Linalg-based tests in the MLIR core version and apply BA to them.", "I am a bit lost (but it is quite late here, my brain may be slow...): I was looking at this PR in TensorFlow, in a disconnected way from Core, and I see code that does not seem to have any test here.\r\nRegardless of plans about upstreaming, I expect that the development in MLIR-TensorFlow follows roughly the same set of good practices as upstream, starting with complete coverage of new code with lit tests. Unless I'm mistaken there is a non-tested pass in this PR?", "Yes and no. We had to redesign several parts and split some features (regarding legalization and use) into other PRs. No, because the test pass basically computes all required information to move allocs and deallocs. Yes, because the actual movement part is not covered.", "@joker-eph you are right with respect to this PR. It has never been our intention to push an untested pass. We address the missing-tests issue and add a complete test case to test the whole BA pass including movement of allocs and deallocs.", ">  We address the missing-tests issue and add a complete test case to test the whole BA pass including movement of allocs and deallocs.\r\n\r\nIs there already a PR up for review?"]}, {"number": 37211, "title": " error: the value of \u2018j\u2019 is not usable in a constant expression", "body": "\r\n**System information**\r\n- OS Platform and Distribution:  \r\n    16.04.6 LTS (GNU/Linux 4.15.0-60-generic x86_64)\r\n\r\n- Mobile device\r\n    not a mobile device\r\n- TensorFlow installed from (source or binary):\r\n    using pip command\r\n- TensorFlow version:\r\n    2.1.0\r\n- Python version:\r\n    3.5.2.\r\n- Installed using virtualenv? pip? conda?:\r\n  using viertualenv\r\n- Bazel version (if compiling from source):\r\n  no\r\n- GCC/Compiler version (if compiling from source):\r\n   5.4.0\r\n- CUDA/cuDNN version:\r\n   no cuda\r\n- GPU model and memory:\r\n    GeForce GT 750M\r\n\r\n\r\n**Describe the problem**\r\n\r\nTensorflow doesn't compile:\r\n\r\n```\r\ng++ -std=c++11 -shared -fPIC -I/home/nickai/evolution/deep-neuroevolution/env/lib/python3.5/site-packages/tensorflow_core/include -I/home/nickai/evolution/deep-neuroevolution/env/lib/python3.5/site-packages/tensorflow_core/include/external/nsync/public -L/home/nickai/evolution/deep-neuroevolution/env/lib/python3.5/site-packages/tensorflow_core -D_GLIBCXX_USE_CXX11_ABI=0 -O2 -DGOOGLE_CUDA=1 .//*.cpp .//ops/*.cpp -ltensorflow_framework -o gym_tensorflow.so\r\nIn file included from /home/nickai/evolution/deep-neuroevolution/env/lib/python3.5/site-packages/tensorflow_core/include/unsupported/Eigen/CXX11/Tensor:101:0,\r\n                 from /home/nickai/evolution/deep-neuroevolution/env/lib/python3.5/site-packages/tensorflow_core/include/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /home/nickai/evolution/deep-neuroevolution/env/lib/python3.5/site-packages/tensorflow_core/include/tensorflow/core/framework/numeric_types.h:20,\r\n                 from /home/nickai/evolution/deep-neuroevolution/env/lib/python3.5/site-packages/tensorflow_core/include/tensorflow/core/framework/allocator.h:26,\r\n                 from /home/nickai/evolution/deep-neuroevolution/env/lib/python3.5/site-packages/tensorflow_core/include/tensorflow/core/framework/op_kernel.h:25,\r\n                 from .//ops/indexedmatmul.cpp:8:\r\n/home/nickai/evolution/deep-neuroevolution/env/lib/python3.5/site-packages/tensorflow_core/include/unsupported/Eigen/CXX11/src/Tensor/TensorBlockV2.h: In static member function \u2018static IndexType Eigen::internal::TensorBlockIOV2<Scalar, IndexType, NumDims, Layout>::Copy(const Eigen::internal::TensorBlockIOV2<Scalar, IndexType, NumDims, Layout>::Dst&, const Eigen::internal::TensorBlockIOV2<Scalar, IndexType, NumDims, Layout>::Src&, const DimensionsMap&)\u2019:\r\n/home/nickai/evolution/deep-neuroevolution/env/lib/python3.5/site-packages/tensorflow_core/include/unsupported/Eigen/CXX11/src/Tensor/TensorBlockV2.h:998:33: error: the value of \u2018j\u2019 is not usable in a constant expression\r\n       if (++it[j].count < it[j].size) {                                \\\r\n                                 ^\r\n/home/nickai/evolution/deep-neuroevolution/env/lib/python3.5/site-packages/tensorflow_core/include/unsupported/Eigen/CXX11/src/Tensor/TensorBlockV2.h:1011:7: note: in expansion of macro \u2018COPY_INNER_DIM\u2019\r\n       COPY_INNER_DIM(LinCopy::Linear);\r\n       ^\r\n\r\n```\r\nThis is because you are using some old version of Eigen:\r\n```\r\n(env) nickai@lap:~/evolution/deep-neuroevolution/gpu_implementation/gym_tensorflow$ ls -l /home/nickai/evolution/deep-neuroevolution/env/lib/python3.5/site-packages/tensorflow_core/include/unsupported/Eigen/CXX11/src/Tensor/TensorBlockV2.h\r\n-rw-rw-r-- 1 nickai nickai 48966 mar  1 15:10 /home/nickai/evolution/deep-neuroevolution/env/lib/python3.5/site-packages/tensorflow_core/include/unsupported/Eigen/CXX11/src/Tensor/TensorBlockV2.h\r\n(env) nickai@lap:~/evolution/deep-neuroevolution/gpu_implementation/gym_tensorflow$ \r\n```\r\nI wonder, why is an \"unsupported\" tree of the Eigen source code appearing inside tensorflow build directory at all?\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI installed tensorflow using this command:\r\n\r\n    pip install --upgrade tensorflow\r\n\r\nThen I tried to build an app that depends on Tensorflow \r\nhttps://github.com/uber-research/deep-neuroevolution/tree/master/gpu_implementation/gym_tensorflow\r\nby exeucting \r\n\r\n    make\r\n\r\nBut since Tensorflow has some bad code inside its headers (i.e. using old and unsupported Eigen library) my the app I am trying to builid doesn't build.\r\n\r\nThis is possibly the same issue as reported here: https://github.com/tensorflow/tensorflow/issues/29927\r\n\r\nI suspect that to fix this, you have to update Eigen library.", "comments": ["@nuliknol, Can you provide standalone code to reproduce the reported issue. Thanks", "@nuliknol, Any update ?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37211\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37211\">No</a>\n", "I've got the same error when I compile tf-1.15 into my app.\r\nThe solution is quite simple: put the tf headers at the top of other headers.", "> I've got the same error when I compile tf-1.15 into my app.\r\n> The solution is quite simple: put the tf headers at the top of other headers.\r\n\r\nit works on tf 2.3.1 when building custom operator"]}, {"number": 37210, "title": "tf.split T value inconsistency", "body": "tf.split raise ValueError when it is used before tf.identity to copy splitted tensors. Error seems related to split num parameter when it is set to None and occurs when split operation outputs more then 2 tensors. \r\n\r\nCode for the model:\r\n        inp = tf.keras.layers.Input(shape=(102,df.shape[1]),dtype = 'float32')\r\n        x0, x1, x2 = tf.split(inp,[66,22,14],axis=1,num=3)\r\n        x00 = tf.identity(x0)\r\n        x11 = tf.identity(x1)\r\n        x22 = tf.identity(x2)\r\n\r\nhandled by the code:\r\n\r\n        inp = tf.keras.layers.Input(shape=(102,df.shape[1]),dtype = 'float64')\r\n        inp_ = tf.identity(inp)\r\n        x0, x1, x2 = tf.split(inp,[66,22,14],axis=1,num=3) \r\n        x00, x11, x22 = tf.split(inp_,[66,22,14],axis=1,num=3)\r\n\r\nError raised:\r\n\r\nValueError: Inconsistent values for attr 'T' DT_DOUBLE vs. DT_FLOAT while building NodeDef 'split' using Op<name=SplitV; signature=value:T, size_splits:Tlen, split_dim:int32 -> output:num_split*T; attr=num_split:int,min=1; attr=T:type; attr=Tlen:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\n\r\nLooks like code is incomplete(`name 'df' is not defined`). Request you to provide colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!\r\n", "@alperyyildiz \r\n\r\nAny update on this issue please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 37209, "title": "tensorflow.keras saving weights?", "body": "I have a model that is run on a time series train set trying to predict stock prices. I never save or load the model or the weights but every time i run it with the save hyperparameters it keeps decreasing in accuracy. I don't understand why, is tensorflow.keras saving the weights or something?\r\n", "comments": ["@oskar347567 \r\nplease provide us with simple stand alone code to replicate the issue faced by you on our local, along with the Tensorflow version on which issue is faced.\r\nAlso share the error logs for us to help you resolve the issue.", "@oskar347567 \r\nplease update on the above comment.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37209\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37209\">No</a>\n"]}, {"number": 37208, "title": "Adding generic Cortex m4f target to TFL micro", "body": "All existing make targets generate binaries for dedicated hardware (chip, board) and make assumptions on the way, the debugging log is written. Porting an existing example application to a new hardware with the same core (e.g. Cortex M4F) requires the replacement of the pin/package/HAL layers. Such layers can be quite large, depending on the vendor's product portfolio and must be added to the TFL micro build. If the user needs only the microlite library (for use in a different build environment), he wants to generate the library without modifications of the sources.\r\n\r\nThis pull request provides a generic make target for Cortex-M4F without being specific on the debug log functions and without having to add any pin/package/HAL layers. The user can integrate the microlite library into his project and register a callback function for the debugging log. The big advantage is that the microlite library can be used off-the-shelf without doing customizations to a specific board hardware.\r\n\r\nThe microlite library for the generic Cortex M4F device is generated by the command\r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=cortex-m4f-generic microlite\r\n\r\nThe output is\r\n\r\ntensorflow/lite/micro/tools/make/gen/cortex-m4f-generic_cortex-m4/lib/libtensorflow-microlite.a\r\n\r\nIn his build system, the user must register the callback function before actually using the debug log, e.g.:\r\n\r\nvoid debug_log_printf(const char* s)\r\n{\r\n\tprintf(s);\r\n}\r\n\r\nvoid setup(void)\r\n{\r\n    DebugLog_register_callback(debug_log_printf);\r\n    DebugLog(\"Make setup\\r\\n\");\r\n    ...\r\n}\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37208) for more info**.\n\n<!-- need_sender_cla -->", "@ml-0 thank you for your contribution, please sign CLA.", "The functionality of this PR is provided in #41860."]}, {"number": 37207, "title": "No module named 'tensorflow.python.tools' in Visual Studio 2017 v15.9.20", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  No.\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Windows 10 Pro\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): Installed from pip\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWithin Visual Studio 2017 I am unable to run a python file if I have 'import tensorflow as tf' in the code. If I run [python -c \"import tensorflow as tf; print(tf.__version__);\" it works\r\n**Describe the expected behavior**\r\nI should be able to run the file\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nJust have a .py file with \"import tensorflow as tf\" and run from within Visual Studio with the python package installed\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nI have python Visual Studio extension version 15.9.18254.1 installed\r\n\r\n#34722 is very close to this issue. In that ticket they had a poorly named file. Not sure what VS is doing. This is most likely something within VS but not sure. Maybe an import?\r\n\r\n#9778 mentions a Windows error but it is quite a few years old", "comments": ["#36441 is also similar but the user just switched computers at work and solved his issue.", "Running the same file from the command line works just fine. It must have something to do with how visual studio is running the file.", "@ehennis Try running on any of the Python IDLE like Jupyter or spyder. Let us know how it progresses. Thanks", "I spun up a Juypter notebook and it ran without issues.", "@ehennis, Looks like running inside Visual studio 2017 missing some Tensorflow dependencies or it might installed outside the environment. \r\nAre you happy to close this issue. Thanks! ", "I would rather not close it since I still can't use it within Visual Studio. Another odd thing is that even if I call it through a StartProcess class within Visual Studio it doesn't work. There must be something with VS running that is messing with paths and whatnot.\r\n\r\nIs there anything else I can try and help debug it? If I can get some time (3 year old daughter is a bit much) I want to create a virtual environment and try that. I am not hopeful because I have tried different Anaconda environments and get the same errors.", "@ehennis, Try the following instructions to install Tensorflow in virtual environmnet.\r\n\r\n```\r\n#Install tensorflow using pip virtual env \r\npip install virtualenv\r\nvirtualenv tf_2.1   # tf_2.1 is virtual env name\r\nsource tf_2.1/bin/activate\r\ntf_2.1 $ pip install tensorflow==2.1\r\ntf_2.1 $ python\r\n>>import tensorflow as tf\r\n>>tf.__version__\r\n2.1.0\r\n```\r\n", "@ehennis, is this still an issue?", "Sorry for the delay. My day job got crazy. Anyway, after creating the virtual environment and use that python.exe I was able to get my code working in Visual Studio.\r\n\r\nDoes that mean my normal install is back or is this just something with virtualenv?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37207\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37207\">No</a>\n"]}, {"number": 37206, "title": "iOS Object Detection with Custom .tflite file: \"Failed to allocate memory for input tensors.\"", "body": "I am trying to run the sample iOS with a custom .tflite file ([download](https://github.com/tensorflow/tensorflow/files/4271813/custom_tflite_file.zip)) where only one object exists.\r\n\r\nWhen the app loads, it crashes with the error `Failed to create the interpreter with error: Failed to allocate memory for input tensors.`.\r\n\r\nHere is a comparison of both .tflite files (left: original, right: custom): \r\n<img width=\"982\" alt=\"attributes\" src=\"https://user-images.githubusercontent.com/12123471/75626712-3477a880-5bca-11ea-9d23-a2d3c34029dd.png\">\r\n\r\nAs shown in the image, input tensors exist. What is the reason for this error?\r\n\r\n\r\n", "comments": ["Hi @MHX792. Which sample app were you trying to run? Can you provide a way to reproduce the error?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 37205, "title": "Unable to download the TensorFlow using pip in Windows 10", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Microsoft Windows 10 Home Single Language |  10.0.18363 N/A Build 18363 |  x64-based PC | 16GB | 4GB 1050Ti Nvidia GC\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: NA\r\n- TensorFlow installed from (source or binary): https://www.tensorflow.org/install/pip?lang=python3\r\n- TensorFlow version:  2.1\r\n- Python version:  3.8.2\r\n- Installed using virtualenv? pip? conda?: virtualenv and pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: 1050Ti Nvidia \r\n\r\n\r\n\r\n**Describe the problem**\r\nI have tried to both Virtualenv and pip but won't able to install. What should I do?\r\n\r\n\" ERROR: Could not find a version that satisfies the requirement TensorFlow (from versions: none)\r\nERROR: No matching distribution found for TensorFlow\"\r\n\r\n**Provide the exact sequence of commands/steps that you executed before running into the problem**\r\n\r\npython3 --version   # 3.8.2\r\npip3 --version           # 20.0.2\r\nvirtualenv --version  #  20.0.7\r\n\r\npip3 install -U pip virtualenv\r\n\r\nvirtualenv --system-site-packages -p python3 ./venv\r\n.\\venv\\Scripts\\activate\r\npip install --upgrade pip\r\n\r\npip list  # show packages installed within the virtual environment\r\n\r\npip install --upgrade tensorflow\r\n\r\nthen the error is coming \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@imoisharma,\r\nTensorflow is only supported till Python 3.7 as of now. For more information, please check [this](https://github.com/tensorflow/tensorflow/issues/33374#issuecomment-571074915) comment from a similar issue. Thanks!", "Any updates regarding this issue? Thanks!", "I understand that it is only limited to Python 3.7, however, which subversion of 3.7? Do we have a confirmed number or should it \"simply be any Python 3.7\"?\r\n\r\nI'm encountering the same issues with Python 3.7.6 no matter what operating system I attempt to install on, resulting in the exact same error.\r\n\r\nThe documentation doesn't specify any specific 3.7 installation however it would be very beneficial to know if we need to be so specific in our versions. I can't imagine any breaking changes between any 3.7.x and 3.7.x+ however, clearly something has gone wrong here.", "Any version of python 3.5, 3.6 or 3.7 should work, provided it is a 64 bits, your CPU is 64 bits and supports AVX and you have the latest MSVC redistributable installed. Oh, also, ensure `pip` is at latest version, to be able to install manylinux2010 packages\r\n\r\nFor all other cases, you will have to build from source.", "Python 3.7.6\r\nPip 20.0.2\r\nWindows 10\r\nGot it working by downloading the matching (version and environment) wheel from https://pypi.org/project/tensorflow/#files and installing manually from that.\r\n\r\nIn my example, I needed Tensorflow 2.1 and downloaded the tensorflow-2.1.0-cp37-cp37m-win_amd64.whl as I have cp37 (CPython 3.7), win_amd64 (Windows 64 bit)\r\n\r\nFor those who come here in the future and may not be used to installing pip wheels manually (only having ever used pip install)\r\n\r\nDownload the necessary file. Tensorflow's naming convention seems to be tensorflow-<version>-<python version>-<os & architecture>, although I'm not sure what the cp37m means here, it should not be necessary for you to find the right file.\r\nOpen a terminal and navigate to your download directory.\r\npip install <tensorflow whl filename>\r\n\r\nIn my case, on windows, it looked like this:\r\n\r\npip install C:/Users/username/Downloads/tensorflow-2.1.0-cp37-cp37m-win_amd64.whl\r\n\r\nThis did the trick for me, however I was not able to reproduce this with pipenv install, so for now, virtual environments are out of the question, albeit right now, not even necessary for me anyways.\r\n\r\n\r\nAlso, Thank you @mihaimaruseac for the info, much appreciated!\r\n", "`cp37m` is `cpython 3.7 manylinux` tag.\r\n\r\nDon't know exactly why you needed to download the file and then install it like this, but good to know it works.\r\n\r\nClosing the issue as it is resolved", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37205\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37205\">No</a>\n"]}, {"number": 37204, "title": "android mask rcnn", "body": "Please provide the code get output mask r-cnn on android. Thanks", "comments": ["@hoangtubatu123, Please take a look at [Tensorflow lite doc](https://www.tensorflow.org/lite/models/segmentation/overview). Thanks!", "@hoangtubatu123, Did you get a chance to go through the above given . Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Can anybody provide a reference to implementing a custom mask rcnn model in Java?\r\n\r\nThanks in advance"]}, {"number": 37203, "title": "Cannot import tensorflow 1.15", "body": "\r\n**System information**\r\n- OS Platform and Distribution (Windows 10 pro):\r\n-Lenovo ideapad 520-15iKB\r\n- TensorFlow installed from (\"pip install tensorflow==1.15\"):\r\n- TensorFlow version: 1.15 (cpu-only)\r\n- Python version: 3.6.0\r\n- Installed using pip\r\n- GPU model and memory: GeForce 940MX 4GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\ni installed tensoflow == 1.15 and i can't import\r\n\r\n`Python 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n2020-03-01 03:36:32.916332: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\r\n2020-03-01 03:36:32.921892: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 99, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\core\\framework\\graph_pb2.py\", line 7, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: No se encontr\u00f3 el proceso especificado.`\r\n\r\ni don't know what is going on, please help me.\r\n", "comments": ["@WallbangJocrod \r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you. Just to verify did you follow the instructions to install from pip using [Tensorflow website](https://www.tensorflow.org/install/pip).Thanks", "Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37203\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37203\">No</a>\n"]}, {"number": 37202, "title": "Print out the user number in Tensorflow", "body": "Dear everyone; \r\n\r\nwhen users increase. I want to print out that user number and how to value the performance of Tensorflow?\r\n\r\nPlease give me some of your idea!\r\nMany thank!", "comments": ["@ccu1tn,\r\nCan you elaborate your issue with code snippet or any use case. Thanks!", "Any updates regarding this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 37201, "title": "Restructure Keras Scikit-Learn wrappers to better implement Scikit-Learn API", "body": "This is a modification of #32533. I am opening a new PR because that one seems stalled and I made a lot of changes/improvements (but keeping the same idea).\r\n\r\nA quick summary:\r\nThe existing `scikit-learn` wrappers for Keras models are not compatible with many `scikit-learn` functions. Additionally, they require that dataset dimensions be determined _before_ calling the `fit` method, which is unlike the `scikit-learn` estimators and makes it hard to build dynamically adaptable pipelines.\r\n\r\nWhat this PR does:\r\nThis PR does _not_ change any API.\r\nBy moving the storage of parameters from `self.sk_params` to `self.__dict__`, compatibility with a lot of the `scikit-learn` functionalities are improved. Additionally, I gave the model building function the ability to request the data that will be fitted (to determine dimensions) as well as any other attributes of the wrapper instance. Finally, I enabled copying/pickling of wrapped models as well as the ability to wrap instances of `Model`, which should allow for greater flexibility in incorporating into an exsiting `Keras` workflow.\r\n\r\nHow is it tested:\r\nAll existing tests are working and were unchanged. This confirms that there were no API changes. New tests were added for all of the new functionality as well as some of the most common `scikit-learn` operations that were previously broken.\r\nI would like to credit @daviddiazvico , the author of the original PR: I borrowed a lot of the tests he had written as well as the original idea for fixing these issues. I will make him a co-author of the final commit if this gets approved.", "comments": ["Thanks for your effort @adriangb ! I'm sorry I couldn't write back before.\r\n\r\nOf course, I don't have any problem with you updating the other PR, but I'm not sure if I have to do something to allow you to edit it. I've added you as a collaborator to my tf fork.\r\n\r\nIn any case, this is a nice improvement for tf.keras in my opinion, but I don't know if the development team is interested. My [PR](https://github.com/tensorflow/tensorflow/pull/32533) has been open for a while and [the same changes](https://github.com/keras-team/keras/pull/12833) were proposed in the keras repo about a year ago now.", "I'm hoping that maybe the reviewers were just a bit busy?\r\n\r\nI feel like the wrappers are used quite often, especially in beginner tutorials. I think it's important that the initial experience be seamless.\r\n\r\nIn addition to your PR, this resolves several issues: #33204, #36074, #34689 and #36137. There are/will be more, like the [comment](url) I posted on your PR.\r\n\r\nI'm hoping we can at least get some feedback from @fchollet or @pavithrasv regarding interest in these changes.", "I was able to add built-in support for all of the multi-output modes that Scikit-Learn supports, as well as a framework to easily support multi-input models. This means this would close #34689 as well.\r\n\r\nBecause of the great modularity of the Functional API, only a limited number of multi-output cases can be automatically supported (those that scikit-learn itself supports with 1-1 mapping of model outputs to `y` columns) and no multi-input models can be supported out of the box. If a user wants to use a more complex model with the wrappers, they would have to subclass and manually define the desired behavior. As I show in the tests, all that is needed to implement multi-input is to define a behvior for `_pre_process_X` (for example, columns 0-3 are one input, 4 is another input). All of this was done with no changes to the API.\r\n\r\nAlthough the original problem statement ('fix compatibility') was quite large, I do feel that this PR has grown very large. I would personally prefer to split it into smaller PRs (even if that means more work for me), but I will leave that up to the reviewers.\r\n\r\n@gbaned , is there a timeline for this review process? It would be nice to at least get tests running so that I can see if there are issues.", "I realized that we actually need to re-implement not only an R^2 score, but also a classifier accuracy score: Keras does not use the `sample_weight` parameter for metrics, but Scikit-Learn does use it for scoring. That and the fact that Keras metrics don't support the Scikit-Learn style multi-output concept, so all of those scores are already being handled manually.\r\n\r\nThis prompted me to think:  does it make sense to make scikit-learn an optional dependency that is only imported within this module? I can see pros and cons, just playing devil's advocate here.", "> Approving to run the presubmits.\r\n> \r\n> Can you add tests to make sure that saving and loading works with the wrapper?\r\n\r\nThank you for kicking off those tests!\r\n\r\nThere are several tests for pickling/unpickling of Functional API models with and without Callbacks, etc. I think the only thing that is missing is a test for subclassed models. I'll add that in the next few days.", "Quite a few errors:\r\n\r\n- Pylint: my fault, I was using 4 spaces. Fixing.\r\n- Scipy import error: for some CI builds, the scikit-learn version was pinned to a 2016 version that now has some broken functionality that these new changes were relying on. I bumped the version.\r\n- Python 2 errors: I'm ignoring these.\r\n- API changes: it's complaining about changes to internal parameters that should never have been public in the first place. I now appended an `_` to them to fix this for the future. It also doesn't like that I capitalized `X` to match scikit-learn.\r\n- Windows builds: I have no idea why these are failing. Any advice would be much appreciated.\r\n\r\nAlso, I added the test suggested by @k-w-w (it's called `SerializeCustomLayers` fyi).\r\n\r\nI guess another approval is needed for tests to run again, it'd be nice to fix the windows build errors before that though.", "Thanks for fixing the bugs! I'm pretty sure the windows tests are unrelated. Running the tests again", "@fchollet can you review the API changes made to the Scikit Learn wrapper? ", "@k-w-w fyi the tests only passed just now because I changed the tag in the `BUILD` from `small` to `large` to reflect that they used to take ~10s but with all of the new test cases take ~5m, but I now realize that that filtered them out from CI, so I'm going to change it to `medium` which seems to do the trick. I'm still working on running the CI on my end, compiling takes several hours on my HW. Anyway, I will fix any issues eventually.", "@k-w-w I think everything should be working, passing the tests under the CPU Docker container. I also tested on Python 3.6 & did the sanity check. Could you kick off CI tests again so we can confirm?", "Sure thing, thanks!", "Hmm it seems that there are still SciPy/Scikit-Learn version issues. The `Ubuntu CPU` container still has a scipy/scikit-learn version incompatibility, and the windows container does not seem to have scikit-learn at all. I don't get any issues when running the testing Docker container locally.\r\n\r\nI can try to bump to `scikit-learn==0.22.2` and add `scikit-learn` to the Windows container, but to be honest, it's a bit of a shotgun approach, since I can't run those builds locally.\r\n\r\nJust to be clear, scikit-learn would only be required for testing, not building.\r\n\r\nThe good news is that the Python 2 and 3 builds on Mac OS are passing (the only failure is `api_compatibility_test`), so I'm fairly confident that once we fix these version issues the other builds will pass as well!", "Thank you for re-running @av8ramit. I saved the logs this time.\r\n\r\nJust to confirm the issue, the [failed test on `Ubuntu CPU`](https://source.cloud.google.com/results/invocations/a35d40fa-4cc0-4161-a226-77a1dfdb9fa8/targets/%2F%2Ftensorflow%2Fpython%2Fkeras%2Fwrappers:scikit_learn_test/log) fails on a scikit-learn line that was removed in this [2017 commit](https://github.com/scikit-learn/scikit-learn/commit/95aa2952e1544a5a2e0f14d366bae1bfd8e9195a). The [`install_pip_packages.sh`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/install/install_pip_packages.sh) script you linked has `scikit-learn==0.18.1` ([released in 2016](https://github.com/scikit-learn/scikit-learn/releases/tag/0.18.1)). The latest versions of scikit-learn are `0.20.4` and `0.22.2.post1` for Py2 and Py3 respectively.\r\n\r\nThe test passed on [MacOS Python2](https://source.cloud.google.com/results/invocations/01679038-c190-4230-a4f8-478653f0bd39/targets;query=wrapper;collapsed=/%2F%2Ftensorflow%2Fpython%2Fkeras%2Fwrappers:scikit_learn_test/tests;failed=true;notrun=true) and [MacOS Python3](https://source.cloud.google.com/results/invocations/b6b24f79-efb9-4238-871e-e0f724eff514/targets;query=wrapper;collapsed=/%2F%2Ftensorflow%2Fpython%2Fkeras%2Fwrappers:scikit_learn_test).\r\n\r\nIf you or @k-w-w can fix this internally that would be much appreciated. I'll remove the edits I made to `common_win.bat` and `common.sh` when I squash commits.", "Hey @adriangb we discussed offline and we are going to wait for general approval for the change and then we can rebuild the containers with the requested changes. ", "That sounds good to me, thank you for your help.\r\n\r\nI think the main review needed is from @fchollet right?", "@fchollet, thank you for looking at this!\r\n\r\nI think that could work really well. But I'd like to list out pros and cons to consider\r\n\r\nPros:\r\n- More flexibility. `scikit-learn` can become required for the wrapper, which eliminates a lot of code duplication for this wrapper and removes the need for `scikit-learn` from TF CI.\r\n- Relieves maintenance burden from Keras/TF team.\r\n\r\nCons:\r\n- Improvements in Keras might take a long time to trickle over.\r\n- When issues do crop up in the future, wrapper maintainers (i.e. me) might have to open an issue with Keras/TF to get support in fixing things.\r\n\r\nOverall, I think the pros outweigh the cons. \r\n\r\nI will start working on getting a separate repo with CI/publishing working. In the meantime, if you could do a brief review of this PR as it currently stands, that would be super useful to make sure the initial release is as good as possible.", "Are there any preferences as far as:\r\n- Naming of the repo/package or any project this should live under. I came up with `sklearn-keras-wrap`\r\n- Documentation style or content. I planned on copying the [existing docs](https://www.tensorflow.org/api_docs/python/tf/keras/wrappers/scikit_learn) but just keeping everying in the README.md for simplicity.\r\n- Python versions: I think it would be easier to make this package >=Python3.5\r\n- Linting: I would switch to flake8/black\r\n- Testing: I would switch to pytest.\r\n\r\nI played around with a packaging a bit, everything seems to work as far as CI/testing/releasing. It would have `scikit-learn>=0.21.0` and `tensorflow>=2.1.0` as dependencies.", "A quick update: the package is now fully operational. I settled on the name SciKeras.\r\nPyPi: https://pypi.org/project/scikeras/\r\nGitHub: https://github.com/adriangb/scikeras\r\n\r\nSome important updates since this PR:\r\n1. Inherited BaseWrapper from sklearn.base.BaseEstimator.\r\n2. Implemented tags interface.\r\n3. Use OneHotEncoder/LabelEncoder instead of manual numpy work.\r\n4. Fix model serialization bugs.\r\n5. A lot of cleanup.\r\n\r\nWith all of this, estimators created with these wrappers now pass all of scikit-learn's estimator checks, except those that require setting a random state. As far as I understand, it is not possible to easily set a random seed in tf.", "Hi @gbaned, just checking if there are any updates on this proposal/PR? Thanks!", "> Hi @gbaned, just checking if there are any updates on this proposal/PR? Thanks!\r\n\r\nHi @adriangb, It is waiting for approval. Thanks!", "Thanks for the update. It's great to see that you've already released the new package. We can recommend that people start using it instead of `keras.wrappers.scikit_learn`.\r\n\r\n> With all of this, estimators created with these wrappers now pass all of scikit-learn's estimator checks, except those that require setting a random state. As far as I understand, it is not possible to easily set a random seed in tf.\r\n\r\nThis should be fixable: https://www.tensorflow.org/api_docs/python/tf/random/set_seed\r\n\r\nWhat do you want us to do with the current PR? Should we close it?\r\n", "> We can recommend that people start using it instead of `keras.wrappers.scikit_learn`.\r\n\r\nThat sounds good.\r\n\r\n> This should be fixable: https://www.tensorflow.org/api_docs/python/tf/random/set_seed\r\n\r\nWill take a look, thank you.\r\n\r\n> What do you want us to do with the current PR? Should we close it?\r\n\r\nI think let's keep it open for a bit longer. Dask is looking to adopt SciKeras as a wrapper ([here](https://github.com/dask/dask-ml/issues/696)), so as they do their testing I expect there to be a couple of issues that crop up in the next couple of weeks that I may need input from the TF team on. Unless the TF team is willing to check the SciKeras repo if they are tagged.", "> I think let's keep it open for a bit longer. Dask is looking to adopt SciKeras as a wrapper (here), so as they do their testing I expect there to be a couple of issues that crop up in the next couple of weeks that I may need input from the TF team on. Unless the TF team is willing to check the SciKeras repo if they are tagged.\r\n\r\nOk, sounds good! Please reach out if you need anything from us (over email preferably, so we don't miss it). We'll start recommending your library as soon as it starts getting traction then \ud83d\udc4d ", "@adriangb  Any update on this PR? Please. Thanks!", "@adriangb Any update on this PR? Please. Thanks!", "Hi @gbaned, as per Fran\u00e7ois' comment above, the plan is to _not_ merge this PR and instead move this part of tf.keras to an external package. I had asked to keep this PR open for communication and help with bringup of the external package, but I've since established communication with Fran\u00e7ois directly and the external package is making good progress, so I think we can close this PR \ud83d\ude04"]}, {"number": 37200, "title": "NotImplementedError: Cannot convert a symbolic Tensor (up_sampling2d_5_target:0) to a numpy array", "body": "> import numpy as np\r\n> import tensorflow as tf\r\n> print(np.__version__)\r\n> print(keras.__version__)\r\n> print(tf.__version__)\r\n> \r\n> 1.17.3\r\n> 2.3.1\r\n> 2.1.0\r\n\r\n```\r\n\r\nimport keras.backend as K\r\nfrom keras.optimizers import Adam\r\nfrom keras.losses import binary_crossentropy\r\n\r\n## intersection over union\r\ndef IoU(y_true, y_pred, eps=1e-6):\r\n    #print(y_true)\r\n    if np.max(y_true) == 0.0:\r\n        return IoU(1-y_true, 1-y_pred) ## empty image; calc IoU of zeros\r\n\r\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\r\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - intersection\r\n    return -K.mean( (intersection + eps) / (union + eps), axis=0)\r\n```\r\n\r\nHere the np.max(y_true) bring the problem. \r\n\r\n> NotImplementedError: Cannot convert a symbolic Tensor (up_sampling2d_5_target:0) to a numpy array", "comments": ["@SlowMonk, Can you provide the complete standalone code to replicate the issue. Thanks!", "https://www.kaggle.com/hmendonca/u-net-model-with-submission \r\nits in17. it work in this kernel but not my local ", "@SlowMonk, Is this similar issue [#37177](https://github.com/tensorflow/tensorflow/issues/37177)? ", "@SlowMonk, Can you provide complete code to analyze this issue and also let us know if its duplicate of [#37177](https://github.com/tensorflow/tensorflow/issues/37177) . Thanks", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37200\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37200\">No</a>\n", "@SlowMonk, I had the same issue but it was resolved by updating TensorFlow to 2.2.0rc2 version."]}, {"number": 37199, "title": "Default type for complex sparse tensor is complex128", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): pip\r\n\r\n**Describe the current behavior**\r\nWhen I instantiate a complex sparse tensor, the default type is complex128, whereas with a float sparse tensor it is float32.\r\n\r\n**Describe the expected behavior**\r\nThe default type for complex sparse tensor should probably be complex64, since we can't set the type in the tensor creation.\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\na = tf.sparse.SparseTensor(indices=[[0, 0],], values=[1.+ 0j], dense_shape=[2, 2])\r\nprint(a.dtype)\r\n```\r\nThis gives `tf.complex128`.\r\n\r\n**Other info / logs** \r\nThis can be circumvented using `numpy` to fix the type of the values but it isn't ideal imho.\r\n", "comments": ["@zaccharieramzi \r\nThe default datatype for complex sparse tensor is `complex128` ,If you need to change complex data type you can typecast to other datatype and use it. For  Float  `dtype = float32` is default and for complex `dtype = complex128` is default. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/82d7e1aa86981ffb81d1a928a957e711/untitled694.ipynb). Thanks!", "Yes of course, there are many ways to circumvent this issue.\r\nWhat I am saying is that it is an unexpected behaviour (in one case you have a double in the other not), and in that regard it should probably be changed.", "This is consistent with the behavior in numpy:\r\n\r\n```\r\n>>> np.array([1.+ 0j]).dtype\r\ndtype('complex128')\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37199\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37199\">No</a>\n"]}, {"number": 37198, "title": "Docker Tensorflow GPU 5x times slower than TF CPU", "body": "I installed \"tensorflow/tensorflow:nightly-gpu-py3\" (at this moment 9 days old) docker image on my Ubuntu 18.04 with nvidia-driver-435, 8gb RAM, i5-8265U and mx110 video card. \r\n\r\nI executed the following code https://github.com/vadimen/algorithms/blob/master/code2.py . It's running mobileNet SSD from tensorflow examples. It's processing a video.\r\n\r\nThe code is writing in console the time(FPS) it need for inference. Executed on CPU on local machine it's 5x times faster than on GPU. \r\n\r\nOutput is as follows:\r\n\r\n2020-02-29 20:09:18.544335: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-02-29 20:09:18.552675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-29 20:09:18.553295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce MX110 computeCapability: 5.0\r\ncoreClock: 1.006GHz coreCount: 2 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-02-29 20:09:18.554196: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-02-29 20:09:18.576049: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-29 20:09:18.587918: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-02-29 20:09:18.590462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-02-29 20:09:18.613041: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-02-29 20:09:18.616945: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-02-29 20:09:18.661350: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-29 20:09:18.661573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-29 20:09:18.662435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-29 20:09:18.663145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-02-29 20:09:20.104462: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-02-29 20:09:20.130370: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1800000000 Hz\r\n2020-02-29 20:09:20.130765: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4bf61c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-02-29 20:09:20.130803: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-02-29 20:09:20.168110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-29 20:09:20.168450: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4b85270 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-02-29 20:09:20.168467: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX110, Compute Capability 5.0\r\n2020-02-29 20:09:20.169053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-29 20:09:20.169287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce MX110 computeCapability: 5.0\r\ncoreClock: 1.006GHz coreCount: 2 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-02-29 20:09:20.169336: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-02-29 20:09:20.169350: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-29 20:09:20.169364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-02-29 20:09:20.169379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-02-29 20:09:20.169394: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-02-29 20:09:20.169414: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-02-29 20:09:20.169432: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-29 20:09:20.169502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-29 20:09:20.169752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-29 20:09:20.170089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-02-29 20:09:20.170405: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-02-29 20:09:20.171326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-29 20:09:20.171339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 \r\n2020-02-29 20:09:20.171345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N \r\n2020-02-29 20:09:20.171411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-29 20:09:20.171657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-29 20:09:20.171889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1024 MB memory) -> physical GPU (device: 0, name: GeForce MX110, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n{'detection_classes': TensorShape([None, 100]), 'num_detections': TensorShape([None]), 'detection_boxes': TensorShape([None, 100, 4]), 'detection_scores': TensorShape([None, 100])}\r\n2020-02-29 20:10:47.947871: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-29 20:10:49.042241: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-29 20:10:49.351873: W tensorflow/core/common_runtime/bfc_allocator.cc:245] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-02-29 20:10:49.389878: W tensorflow/core/common_runtime/bfc_allocator.cc:245] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-02-29 20:10:49.427350: W tensorflow/core/common_runtime/bfc_allocator.cc:245] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-02-29 20:10:49.463571: W tensorflow/core/common_runtime/bfc_allocator.cc:245] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-02-29 20:10:49.502225: W tensorflow/core/common_runtime/bfc_allocator.cc:245] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-02-29 20:10:49.539195: W tensorflow/core/common_runtime/bfc_allocator.cc:245] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-02-29 20:10:49.577960: W tensorflow/core/common_runtime/bfc_allocator.cc:245] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.15GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-02-29 20:10:49.627041: W tensorflow/core/common_runtime/bfc_allocator.cc:245] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.15GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-02-29 20:10:49.706667: W tensorflow/core/common_runtime/bfc_allocator.cc:245] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.14GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-02-29 20:10:49.761623: W tensorflow/core/common_runtime/bfc_allocator.cc:245] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n\r\nI think it's because of not enough memory, my mx110 card has 2GB of memory but output says it's all used.\r\n\r\nI tried to limit memory with this but it's still using everything.\r\n\r\n`gpus = tf.config.experimental.list_physical_devices('GPU')\r\n\r\ntf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])`", "comments": ["Can you try using [TensorBoard](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) to gather a profile and see if something sticks out?\r\n\r\nThe memory usage is likely a red herring:\r\n\r\n * By default TF allocates all of the memory on the GPU on startup.  You can [change this](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth).\r\n * Even if we tell TF to allocate GPU memory incrementally, autotuning in TF will usually try to allocate large amounts of scratch memory so that it can run faster convolutions.", "@vadimen,\r\nSorry for the delayed response. Can you please confirm if the issue still persists? If so, can you please respond to the [above comment](https://github.com/tensorflow/tensorflow/issues/37198#issuecomment-593663663)? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 37197, "title": "Correct phrasing on CuDNN conditions", "body": "As-is, the condition means \"inputs are not masked\" AND \"inputs are not strictly right-padded\", which isn't the actual condition, and is thereby misleading.", "comments": []}, {"number": 37196, "title": "Keras saves invalid JSON files containing NaN", "body": "**Describe the current behavior**\r\n\r\nJSON saved by Keras contains `NaN` which is invalid according to [RFC 7159](https://tools.ietf.org/html/rfc7159):\r\n\r\n> \"Numeric values that cannot be represented in the grammar below (such as Infinity and NaN) are not permitted.\"\r\n\r\n**Describe the expected behavior**\r\n\r\nKeras saves correct JSON format.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```\r\nimport tensorflow as tf\r\ni = tf.keras.layers.Input((600,600,3))\r\no = tf.keras.layers.Conv2D(16, (3, 3), padding='same', name='conv0',\r\n       kernel_regularizer=tf.keras.regularizers.l2(1e-2),\r\n       bias_regularizer=tf.keras.regularizers.l2(None),\r\n       kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\r\n       bias_initializer=tf.keras.initializers.constant(0.0))(i)\r\nmodel = tf.keras.models.Model(i, o)\r\nwith open('repro.json', 'w') as json_file:\r\n    json_file.write(model.to_json())\r\n```\r\n\r\n```\r\n~ node\r\n> JSON.parse(require('fs').readFileSync('repro.json', 'utf-8'))\r\nUncaught SyntaxError: Unexpected token N in JSON at position 861 \r\n```\r\n\r\n[repro.zip](https://github.com/tensorflow/tensorflow/files/4270871/repro.zip)\r\n\r\nlutzroeder/netron#435\r\n", "comments": ["I was able to replicate the issue Tf 2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/d029a1da140e81b7b510335a0acf23db/untitled.ipynb). Thanks!", "@lutzroeder @gadagashwini I am willing to take a closer look, fix it and send a PR.", "@lutzroeder @gadagashwini \r\n\r\nThis bug is caused by `tf.keras.regularizers.l2` which will convert `None` to JSON string when `model.to_json` is called. \r\n`None` will convert to NumPy float first, it will be a `np.nan`. \r\nThen `np.nan` will be converted to Python float, it will be `float('nan')`. \r\nFinally, `float('nan')` will be converted to a JSON string, python's built-in JSON library allow dump `float('nan')` to string as `NaN` (https://docs.python.org/3.6/library/json.html#infinite-and-nan-number-values).  \r\n\r\nA gist for this can be found [here](https://colab.research.google.com/drive/1bOOODHoS9T1_gvFNA8PhqsSQigsj35__).\r\n\r\n@lutzroeder Use `tf.keras.regularizers.l2(0)` instead of `tf.keras.regularizers.l2(None)` can fix you problem. \r\n\r\nI will submit a PR later to make sure `tf.keras.regularizers.l2` don't accept None.\r\n", "Does this same issue apply to `tf.keras.layers.BatchNormalization()`?\r\nI'm using the Pix2Pix example code and having a similar problem with NaN values in my one set of weights and in my model.json when created with tfjs.", "Hi @clkruse, If your arguments contain None which not supposed to be. It's likely to be the same issue. ", "@clkruse Can you submit an issue for your problem, so I can check and fix it.", "@howl-anderson I have an issue out for my version of the problem in #38698. \r\n\r\nNote: the error only seems to occur when batchnorm is used with a batch size of 1. ", "@clkruse I will take a close look to see if I can fix it.", "@lutzroeder can you confirm if your issue is fixed with the above commit by @howl-anderson , if so please close the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37196\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37196\">No</a>\n"]}, {"number": 37195, "title": " ValueError: Dimension 0 in both shapes must be equal", "body": "**System information**\r\n- OS Platform and Distribution: Google AI Platform\r\n- TensorFlow version: v2.0\r\n- Python version: v3.*\r\n- Machine type: 16 vCPUs, 30 GB RAM\r\n- GPU: 1x NVIDIA Tesla K80\r\n\r\nI am training a model to caption images and have mostly used this tutorial: https://www.tensorflow.org/tutorials/text/image_captioning#download_and_prepare_the_ms-coco_dataset\r\n\r\nThe only thing that is different from this notebook is that I am using a different dataset.\r\n\r\nWhen I try and train the model I get the error in the first epoch:\r\n\r\n`ValueError: Dimension 0 in both shapes must be equal, but are 10 and 48. Shapes are [10,1] and [48,1]`\r\n\r\nI have tried this with many different batch sizes and the error is always in the format:\r\n\r\n`ValueError: Dimension 0 in both shapes must be equal, but are 10 and {BATHCH_SIZE}. Shapes are [10,1] and [{BATHCH_SIZE},1]`\r\n\r\nI also found a similar issue on github: https://github.com/tensorflow/tensorflow/issues/27862\r\nIt is suggested to change the batch size to 48 but that didn't work for me.\r\n\r\nCan anyone help out?\r\n\r\nThanks", "comments": ["Just from the looks of your error message, I\u2019m wondering if setting the batch size to 10 would solve the issue. I know this isn\u2019t a fundamental fix or an answer to your question, but can you give it a try?", "yes, I tried that too. But still got the error:\r\n\r\n`ValueError: Dimension 0 in both shapes must be equal, but are 4 and 10. Shapes are [4,1] and [10,1]`\r\n\r\nI haven't tried batch size 4 but I assume there is a deeper issue here\r\n", "@anthonyatp \r\n\r\nWill it be possible to share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "So, I managed to solve the issue by limiting my dataset to a round number (it was 14,586 and I limited to 14,000).\r\n\r\nIs this something that should always be done with datasets? Or is there another reason why I couldn't train the model initially?\r\n\r\nApologies if this is naivety on my behalf. I can put together a notebook to share if required", "@anthonyatp Please share the gist so that we can figure out where the issue was. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37195\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37195\">No</a>\n"]}, {"number": 37194, "title": "Cant run exe file from convert tensorflow to exe", "body": "i have problem for run exe file it been after i deploy script to exe i got application file and run it i got this message in command  i try many things try with command try with auto-py-to-exe it dont solve\r\n\r\n![75507237-1b78c700-5a13-11ea-8705-92635f011c90](https://user-images.githubusercontent.com/49858640/75612200-f2ac1b00-5b53-11ea-981e-27aebdbddd33.png)\r\n\r\nthis time die T-T\r\nPlease help me\r\ni use: Python 3.6.9 :: Anaconda, Inc\r\n   tensorflow-gpu==1.9.0\r\n   PyInstaller==4.0.dev0+a1f92c6a08\r\n   Eel==0.11.0", "comments": ["That's a pyinstaller issue, not a tensorflow one... \r\nI created this PR for pyinstaller:\r\nhttps://github.com/pyinstaller/pyinstaller/pull/4704\r\n\r\nShould resolve this issue... ", "@TOPMANZA,\r\nCould you please check @Hfusi0n's comment and let us know if it works? Thanks!", "sorry i'm late and sorry for tag \r\nlook like i have problem like https://github.com/pyinstaller/pyinstaller/issues/3709 \r\ni cant find how to resolve this issue\r\nlast comment he tell OS changes right?\r\ni using windows10", "@TOPMANZA,\r\nSorry for the delayed response. \r\n\r\nAccording to the [comment](https://github.com/pyinstaller/pyinstaller/issues/3709#issuecomment-593185502) you mentioned, the build platform should be greater than the running platform.\r\nEg. If you have built the exe file on Windows 7, you can run it on Windows 7 and above.", "> @TOPMANZA,\r\n> Sorry for the delayed response.\r\n> \r\n> According to the [comment](https://github.com/pyinstaller/pyinstaller/issues/3709#issuecomment-593185502) you mentioned, the build platform should be greater than the running platform.\r\n> Eg. If you have built the exe file on Windows 7, you can run it on Windows 7 and above.\r\n\r\nQuoting Legorooj comment... \r\n> @Overdrivr win8>win7 will work; windows is the same platform. The other way round may not due to underlying OS changes.\r\n\r\nYou can try and use the hooks created in the PR from my comment. See if using those hook files can resolve your issues. \r\n\r\n", "@TOPMANZA, \r\nAny updates regarding this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 37193, "title": "\"Cannot take the length of shape with unknown rank\" error", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 18.04.3\r\n- TensorFlow installed from (source or\r\nbinary): pip\r\n- TensorFlow version (use command below): tensorflow-addons==0.8.2\r\ntensorflow-estimator==2.1.0\r\ntensorflow-gpu==2.1.0\r\ntensorflow-probability==0.9.0\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version:  10.1.243 / 7.6.5\r\n- GPU model and memory: Tesla V100-SXM2 32 GB\r\n\r\n**Describe the current behavior**\r\nI\u2019m trying to switch from keras to tensorflow.keras (actually only to add TensorBoard callback to model.fit_generator and to be able to profile performance). \r\nNow I am getting \r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-70f0d0a1e94e> in <module>\r\n\u2026\r\n~/Visual_Z2/ImageNet.py in _doLearning(self, epochCount, learningCallback, otherParams, initialEpochNum)\r\n #                             self.model.fit_generator\r\n    430                                  validation_steps=testImageCount // self.batchSize,\r\n    431                                  workers=2,\r\n--> 432                                  verbose=2, callbacks=[tensorBoardCallback])\r\n    433             #, summaryCallback])\r\n    434             # Without make_one_shot_iterator - error fused convolution not supported\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    322               'in a future version' if date is None else ('after %s' % date),\r\n    323               instructions)\r\n--> 324       return func(*args, **kwargs)\r\n    325     return tf_decorator.make_decorator(\r\n    326         func, new_func, 'deprecated',\r\n\u2026\r\nValueError: in converted code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py:677 map_fn\r\n        batch_size=None)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py:2469 _standardize_tensors\r\n        exception_prefix='target')\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py:529 standardize_input_data\r\n\u2026\r\nValueError: Cannot take the length of shape with unknown rank.\r\n\r\n```\r\n**Describe the expected behavior**\r\nEverything worked on keras with tensorflow as backend.\r\n\r\n**Other info / logs**\r\nI read this can happen because of PyFunction, I really have it, but adding reshape/image.set_shape don't fix the error.\r\nI build input dataset like this\r\n\r\n```\r\ndef _loadTestImage(imageNum, label):\r\n    imageData = self.imageDataset.getImage(imageNum, 'net', 'test')       # Returns numpy array (227, 227, 3) of floats\r\n    return (imageData, keras.utils.to_categorical(label, num_classes=classCount))\r\n\r\ndef _tfLoadTestImage(imageNum, label):\r\n    image, label = tf.py_function(_loadTestImage, [imageNum, label], [tf.float32, tf.int32]) \r\n    # Adding image.set_shape(crop_size + (3, ))\r\n    # or tf.reshape(image, shape=crop_size + (3, ))\r\n    # here doesn\u2019t help\r\n    return image, label\r\n\r\nnumDs = tf.data.Dataset.from_tensor_slices(imageNums)       # Shuffled indices of images [15, 22, 3, \u2026] \r\nlabel_ds = tf.data.Dataset.from_tensor_slices(self.imageNumLabels)\r\nds = tf.data.Dataset.zip((numDs, label_ds))\r\nds = ds.repeat()\r\ntfTestDataset = ds.shuffle(buffer_size=max(epochImageCount, 2000))\r\ntfTestDataset = tfTestDataset.map(_tfLoadTestImage, num_parallel_calls=4)\r\ntfTestDataset = tfTestDataset.batch(self.batchSize)\r\n```\r\n\r\nThe problem looks identical to https://github.com/tensorflow/tensorflow/issues/24520#issuecomment-589119295 so I tried nightly build today (2.2.0-dev20200229) and still getting error, only \r\n\r\n```\r\nValueError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:499 train_function  *\r\n        outputs = self.distribute_strategy.experimental_run_v2(\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:856 experimental_run_v2  **\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2112 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2470 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:471 train_step  **\r\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:348 update_state\r\n        self._build(y_pred, y_true)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:267 _build\r\n        self._metrics, y_true, y_pred)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1118 map_structure_up_to\r\n        **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1214 map_structure_with_tuple_paths_up_to\r\n        *flat_value_lists)]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1213 <listcomp>\r\n        results = [func(*args, **kwargs) for args in zip(flat_path_list,\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1116 <lambda>\r\n        lambda _, *values: func(*values),  # Discards the path arg.\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:377 _get_metric_objects\r\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:377 <listcomp>\r\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:398 _get_metric_object\r\n        y_t_rank = len(y_t.shape.as_list())\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py:1173 as_list\r\n        raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\n\r\n    ValueError: as_list() is not defined on an unknown TensorShape.\r\n```", "comments": ["@MikhailMashukov, Thanks for reporting this issue.\r\nPlease provide the complete standalone code to reproduce the reported issue.\r\nThanks", "@MikhailMashukov, Can you update the issue with standalone code. Thanks", "Oh, I'm afraid I don't know how to do this in reasonable amount of time. Needs endless iterations of cutting code and retesting whether it still reproduces.\r\n\r\nMaybe some additional logging instead?", "It would be helpful if we get the sample standalone code to analyze this issue. Thanks! ", "@MikhailMashukov, `Any update ", "No, I will write here if I have what to write", "@MikhailMashukov, Please update the code snippet. ", "@MikhailMashukov, \r\nplease update  on the above comment", "I still have this issue with TF2.1 and you can find my workaround [here](https://github.com/tensorflow/tensorflow/issues/31373#issuecomment-573663963)", "> I still have this issue with TF2.1 and you can find my workaround [here](https://github.com/tensorflow/tensorflow/issues/31373#issuecomment-573663963)\r\n\r\nThank you Ismael! \r\nAnd I have already forgot what I tried to do and have moved to PyTorch... Hope it will help to other people.", "@MikhailMashukov\r\nplease confirm if we may move this to closed status.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "an normal LSTM with None shape still cannot train? is this will be fix?\r\n\r\n```\r\nif (x.shape is not None and len(x.shape) == 1 and\r\n    /usr/local/var/pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:822 __len__\r\n        raise ValueError(\"Cannot take the length of shape with unknown rank.\")\r\n    ValueError: Cannot take the length of shape with unknown rank.\r\n```", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37193\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37193\">No</a>\n", "You can solve this issue by adding \"set_shape()\" under map_fn like this:\r\n```\r\ndef map_fn(x, label):\r\n    x = tf.py_function(your_fn, inp=[x], Tout=(tf.float32))\r\n    x.set_shape([shape])\r\n    return x, label\r\ndataset = dataset.map(map_fn).batch(batch_size)\r\n```\r\nThe better performerance is setting map after batch, so you can do this:\r\n```\r\ndef map_fn(x, label):\r\n    x = tf.py_function(your_fn, inp=[x], Tout=(tf.float32))\r\n    x.set_shape([batch_size, shape])\r\n    return x, label\r\ndataset = dataset.batch(batch_size).map(map_fn)\r\n```\r\n"]}, {"number": 37192, "title": "Specifying about representative_datasets in TfLiteConverter", "body": "Fixes #37111 . @mihaimaruseac , Please review.", "comments": ["It's probably not fixed.", "> It's probably not fixed.\r\n\r\n@tigert1998 , Please suggest the required changes that you want me to change. ", "@ashutosh1919 Can you please check reviewer comments and keep us posted. Thanks!", "@mihaimaruseac and @suharshs , I have changed the description as per your comments. Please review it.", "@suharshs , Please review this PR."]}, {"number": 37191, "title": "Something is wrong with positional encoding,sin and cos should be concatenated.", "body": "def positional_encoding(position, d_model):\r\n  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\r\n                          np.arange(d_model)[np.newaxis, :],\r\n                          d_model)\r\n  \r\n  # \u5c06 sin \u5e94\u7528\u4e8e\u6570\u7ec4\u4e2d\u7684\u5076\u6570\u7d22\u5f15\uff08indices\uff09\uff1b2i\r\n  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\r\n  \r\n  # \u5c06 cos \u5e94\u7528\u4e8e\u6570\u7ec4\u4e2d\u7684\u5947\u6570\u7d22\u5f15\uff1b2i+1\r\n  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\r\n  angle_rads = np.concatenate([angle_rads[:, 0::2] ,angle_rads[:, 1::2]],axis = -1)\r\n  pos_encoding = angle_rads[np.newaxis, ...]\r\n    \r\n  return tf.cast(pos_encoding, dtype=tf.float32)", "comments": ["@lyj1998 \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\n\r\nPlease, share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Please, fill issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@lyj1998 \r\n\r\nAny update on this issue please. Thanks!", "sorry ,maybe i was wrong", "@lyj1998 \r\nPlease, close this thread if the issue got resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37191\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37191\">No</a>\n"]}, {"number": 37190, "title": "Re-instate PR #36578: Go: NewTensor & Value performance improvement ", "body": "See https://github.com/tensorflow/tensorflow/pull/36578. \r\n\r\nHave fixed issues with pointer arithmetic checks introduced with Go 1.14 that caused original PR to be reverted", "comments": ["Ah, rolled back again. I give up. I believe in these changes, but dealing with this codebase with its missing generated files and mysterious extra background testing processes is just too painfull"]}, {"number": 37189, "title": "Can you guys work on making Tensorflow easier to install?", "body": "Hi.  I spent 2 weeks trying to get Tensorflow up and running on my GPU.  It worked... for that one particular repo.  Now I'm trying another project, and apparently that particular version of Tensorflow gives me the error \"Tensorflow has no attribute 'dimension'\".  \r\n\r\nSo I tried to open a new environment, re-install tensorflow-gpu, and I'm getting a \"core-dump\" error.\r\n\r\nWith other packages, I can just type \"pip install\" and it works.  With tensorflow, every single time I have to use or install it I have to spend multiple hours debugging it.  Please, instead of adding new features, work on making Tensorflow able to be installed and used, without errors, by someone without a doctorate in computer science.", "comments": ["Hi @Tylersuard can you please fill out the form that goes along with the issues so that we may better assist you? What OS and version of TF are you trying to install?", "https://www.reddit.com/r/tensorflow/comments/czo5x7/why_is_tensorflow_so_hard_to_install/\r\nhttps://www.quora.com/Why-is-TensorFlow-so-hard-to-install\r\n\r\nIt's not the OS or the version of my particular machine.  It's that Tensorflow is incompatible with EVERYTHING, unless I do a huge amount of error solving to get it to work.  The first link above is from a person who says that in their 6 years of experience he has never run into a library that's as difficult to install as Tensorflow.  And I have to agree.\r\n\r\nIt's particularly frustrating to try to install tensorflow-GPU: when last I tried, it was not compatible with the latest NVIDIA drivers, so I had to uninstall them as well as CUDNN in order to get tensorflow-gpu to work on my machine.\r\n\r\nThough I've often been told not to use Conda to install tensorflow, recently I'm finding that is the only way to actually get Tensorflow to work.\r\n\r\nEvery other package I've ever used that is not Tensorflow, I just do pip install and it works and it's fine.  With Tensorflow, I have to solve around 10+ errors after every install, and often in between uses.\r\n\r\nTensorflow was made by Google.  Google is well-known for its simple software and great interfaces.  Tensorflow has nothing to do with either of those qualities.\r\n\r\nI am from now on going to try to use Pytorch, not because of its quality, but because I can actually install it.", "In my experience, the ease of installation of TensorFlow has gotten better with every new release. The current versions are just as easy as `pip install tensorflow` (Which defaults to GPU enabled option now). I believe the challenge you face is the interdependency between the platform, graphics drivers, CUDA, CuDNN, and Tensorflow. In that case, I also agree the overall pipeline of TF installation has never been smooth for me. \r\n\r\nBelow is my experience on Ubuntu OS (16.04, 18.04) with Cuda 8.x, 9.x, 10.x, CuDNN 5.x onwards and TF 0.9 till 2.0. The main problems:\r\n1) Corruption of Graphics drivers causing UI to not work on restart.\r\n2) Causing login loops\r\n3) GPU used for UI and ML\r\n4) Version mismatches between cuda, drivers, and TF\r\n5) Getting CUDA and CuDNN to communicate with each other\r\n6) Difference between installation using DEB packages vs Nvidia run files.\r\n \r\nand a few more. \r\n\r\nI think, for this to be smoother Nvidia and TF team have to work together. As I said before once the right version of drivers and CUDA are in place, I have never faced any issue with installing any version of TensorFlow including nightly packages, or local package building.\r\n", "@Tylersuard,\r\nCould you please share the exact sequence of commands you executed before running into the problem. If possible share the error logs too, so that we can look into this issue and fix it.\r\n\r\nYou can check out [this](https://www.tensorflow.org/install/pip) guide to install TensorFlow through pip packages. Or, if you want to build TensorFlow from source you can follow [the official guide](https://www.tensorflow.org/install/source) depending on the OS you are using. Thanks!", "@amahendrakar I'm sorry, I can't give you any exact sequence of commands.  This post is just me venting my frustration at using Tensorflow for over a year, and it being by far the most difficult package to install each and every time.  \r\n\r\nTensorflow doesn't work with the most recent version of Python.  Tensorflow doesn't work with the most recent version of CUDNN.  Tensorflow doesn't work with the most recent version of CUDA.\r\n\r\nI've resorted to using Colab for everything because I don't have to wade through hours of error-solving just to open a program.\r\n\r\nThis isn't asking you to solve a specific error.  This is me asking your team to solve ALL the errors, and please, please, for the love of god, PLEASE make Tensorflow easy to install so that I and others don't have to keep tearing our hair out.", "Unfortunately this is a huge project with too many dependencies, so making sure everything works as soon as new dependencies versions are released is a Sisyphean task with little recognition. It is what it is.\r\n\r\nFortunately, this is a community project, anyone from the open source can provide PRs to upgrade build tools/code so that TF would work with the newer versions.\r\n\r\nThere is the alternative of splitting TF into many smaller packages each one being much easier to install. But this has several people upset because then they can no longer just do a `pip install tensorflow` and `import tensorflow as tf` and have everything working.", "While the OP mentioned having TF support the latest versions of CuDNN, CUDA, etc; I do not think that alone is the problem. Assuming you have specific versions of these dependencies that are supported, **is there a foolproof way of installing packages that will work reliably?** Yes, I have gone through the steps mentioned in the TF installation page and yes the latest version **mostly** works. However, I still face difficulties, where the exact same method with the same setup will not work in two different ubuntu machines. The most troublesome part is when a certain step leads to issues like login loop or Cuda not being detected. During my attempts, I have used the Nvidia website, various blog posts, TF installation guides, askubuntu answers, FastAI advice (and some combination therein) to make things work. It would be much easier if we can have a more robust installation procedure so we can worry about building DL algorithms with TF rather than setting up TF.\r\n\r\nDon't get me wrong! TF is an amazing package without which things would have been far gloomier. It is just that \"why\" feeling when you end up spending hours or days setting up TF rather than training your models already!", "We try to ensure that the official builds are working in as many cases as possible. If you encounter a problem with official install guides, please open a PR and we will fix issues.\r\n\r\nUnfortunately, the build system has grown fast and in a haphazard way. We need to clean it up and make it better but this needs head count and time. Will happen eventually.", "@SivamPillai Thank you for backing me up on this.  I feel exactly the same way.\r\n\r\n@mihaimaruseac At this point I would settle for a Windows-type installation wizard, anything to just get the software to work.  It could even include matching Nvidia drivers and Python versions with Tensorflow or something like that.  It's just frustrating to me that everything else by Google is amazing and easy to use, except Tensorflow, which is the worst program I have ever had to work with.  If Google wants to maintain its dominance in the AI field, it needs to make Tensorflow easier to install and use.  Not everyone has multiple degrees in computer software and hardware, and you are losing devs because of Tensorflow's impossible installation.\r\n\r\nUsing the software is not much better, it should be very telling that Tensorflow's interface is so difficult that someone had to invent a library called Keras so that actual humans can use it.\r\n\r\nPlease stop improving Tensorflow's performance, and improve its usability.", "@mihaimaruseac I am not setting up TF at the moment in any machine. But will share specific issues when I come across those. @Tylersuard sure, it was just resonating so chipped in. I don't think making things so rigid like a windows software package will help either. As you know most of us want to make use of the latest improvements or algorithms implemented in TF. Windows like rigid packaging will most likely lack the ability to update on-the-go. \r\n\r\nI think  the way to go is modularizing and minimizing dependencies. I am sure TF folks are working on this but would love to see if a separate project on the usability is setup as mentioned by @Tylersuard so as to have a roadmap. We can chip in where possible for this project.", "Closing this issue for now. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37189\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37189\">No</a>\n", "> https://www.reddit.com/r/tensorflow/comments/czo5x7/why_is_tensorflow_so_hard_to_install/\r\n> https://www.quora.com/Why-is-TensorFlow-so-hard-to-install\r\n> \r\n> It's not the OS or the version of my particular machine. It's that Tensorflow is incompatible with EVERYTHING, unless I do a huge amount of error solving to get it to work. The first link above is from a person who says that in their 6 years of experience he has never run into a library that's as difficult to install as Tensorflow. And I have to agree.\r\n> \r\n> It's particularly frustrating to try to install tensorflow-GPU: when last I tried, it was not compatible with the latest NVIDIA drivers, so I had to uninstall them as well as CUDNN in order to get tensorflow-gpu to work on my machine.\r\n> \r\n> Though I've often been told not to use Conda to install tensorflow, recently I'm finding that is the only way to actually get Tensorflow to work.\r\n> \r\n> Every other package I've ever used that is not Tensorflow, I just do pip install and it works and it's fine. With Tensorflow, I have to solve around 10+ errors after every install, and often in between uses.\r\n> \r\n> Tensorflow was made by Google. Google is well-known for its simple software and great interfaces. Tensorflow has nothing to do with either of those qualities.\r\n> \r\n>\r\n\r\n\r\n\r\n> Hi. I spent 2 weeks trying to get Tensorflow up and running on my GPU. It worked... for that one particular repo. Now I'm trying another project, and apparently that particular version of Tensorflow gives me the error \"Tensorflow has no attribute 'dimension'\".\r\n> \r\n> So I tried to open a new environment, re-install tensorflow-gpu, and I'm getting a \"core-dump\" error.\r\n> \r\n> With other packages, I can just type \"pip install\" and it works. With tensorflow, every single time I have to use or install it I have to spend multiple hours debugging it. Please, instead of adding new features, work on making Tensorflow able to be installed and used, without errors, by someone without a doctorate in computer science.\r\n\r\nJust use pytorch, i never have issues with that. "]}, {"number": 37188, "title": "Corrected the default value for name param in Nadam", "body": "Fixes #37179 . @mihaimaruseac , Please review this one.", "comments": ["@ashutosh1919  Can you please check reviewer comments and keep us posted. Thanks!", "@mihaimaruseac , Closing this one. As per your suggestion, I have added changes of this PR in #37192. "]}, {"number": 37187, "title": "guarantees for the logs argument in keras Callbacks", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback\r\nhttps://www.tensorflow.org/guide/keras/custom_callback\r\n\r\n## Description of issue (what needs changing):\r\nThe documentation for the keras `Callback` base class contains the following generic statement about the `logs` parameter passed to its methods:\r\n```\r\nThe logs dictionary that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch.\r\n```\r\nand\r\n```\r\nThe logs dict contains the loss value, and all the metrics at the end of a batch or epoch. Example includes the loss and mean absolute error.\r\n```\r\non the Keras custom callbacks page.\r\n\r\nSince python passes objects by reference, the question becomes whether write-access to this logs parameter is allowed and supported. An example use case would be to provide a custom callback that populates the `logs` dictionary with some additional information that than would automatically be displayed in the progress bar and tensorboard, and recorded in history and CSV callbacks. \r\n\r\nTherefore, I think the documentation should clearly state whether \r\n1) Write-access to the `logs` dict is forbidden (in which case it might be worthwhile to pass a non-writeable dict-like type)\r\n2) Write-access to `logs` is allowed, and will not have any side-effects on any other Callback (i.e. each Callback gets an independent copy)\r\n3) The `logs` dict is writable, and changes to it are visible to any further Callback. This would also require to specify in which order the callbacks are processed.\r\n ", "comments": ["I'm interested in clarifying this as well. Since I currently assign a new metric into the logs dictionary, and the metric ends up being reported in tensorboard, but not on the terminal even with `verbose=1`.\r\n\r\nIt would nice to be able to add whatever metric we want into the tensorboard.\r\n\r\nAlthough strangely enough, even though my custom metric appears in tensorboard, it's not possible to use the filter entry to filter to that scalar metric.", "+1\r\n\r\nVery confusing that updates to the logs dict inside a `keras.Callback` do not make it to the terminal.", "Seems this issue is still not solved. Here we are redirected to #44940, and there we are redirected back here. I have the same issue. Can someone please clarify how the callbacks \"logs\" parameters behave?\r\n\r\nI cannot migrate from TF 2.0 to TF 2.2 and this is one of the issues.", "I came across this issue while trying to pass a custom metric to `ModelCheckpoint` callback from a custom callback. `logs` dictionary can be used for this purpose, however `_supports_tf_logs` flag of the `ModelCheckpoint` instance needed to be set to `False`, otherwise the `logs` dictionary was not updated and passed to it. Additionally, what I observed is that the callbacks are executed in the order they are defined and `logs` dictionary is updated in that order as well.\r\n\r\nSample code:\r\n```\r\n# Define the custom callback for the metric\r\nclass CustomMetricCallback(tf.keras.callbacks.Callback):\r\n    def __init__(self, logs={}):\r\n        super().__init__()\r\n\r\n    def on_epoch_end(self, epoch, logs={}):\r\n        logs['customer_metric_name'] = ... # compute the metric value\r\n```\r\n```\r\n# Initialize the checkpoint callback with the custom metric\r\nweights_path = 'checkpoint-{epoch:02d}-{customer_metric_name:.4f}.h5'\r\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(weights_path, monitor='customer_metric_name')\r\ncheckpoint._supports_tf_logs = False\r\n```\r\n```\r\n# Define the model callbacks\r\ncallbacks = [\r\n    CustomMetricCallback(), # should be listed before the other callbacks that will use its logs dictionary\r\n    checkpoint\r\n]\r\nmodel.fit(train_gen, validation_data=val_gen, callbacks=callbacks,)\r\n```\r\n\r\nTested with tensorflow version 2.4.1.", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! "]}]