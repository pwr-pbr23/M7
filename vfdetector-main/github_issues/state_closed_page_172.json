[{"number": 49615, "title": "test build issue ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49615\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49615\">No</a>\n"]}, {"number": 49614, "title": "Documentation of ResourceVariable is hard to find.  (keras)", "body": "Please provide a link to the documentation entry, for example:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/a954d375fec881b2b050088f87754b3d0995924a/tensorflow/python/keras/engine/base_layer.py#L573\r\n\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nModel.add_weight describes use_resource with a single sentence in docstring.\r\n\r\nIt would be nice if we can add a link to some documentation of ResourceVariable describing what it does.\r\n\r\nOtherwise, it is difficult to contextualize the implications of this argument.\r\n\r\nFor example, is this a tfv1 vs tfv2 issue?\r\nWhat behavior changes to the variables are expected?\r\n\r\nAdding to the difficulty to explore the context, is that searching for `ResourceVariable` on tf document website no documentation of ResourceVariable shows up as top results, but we get a ton of 'resource variable' and 'variable' documentation. That's probably a separate issue.\r\n\r\n", "comments": ["@rainwoodman ,\r\n\r\nWe see that the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue.\r\n\r\nThanks!", "Sorry. I followed the github link \"Reference in a new issue\" from the source file.\r\n\r\nThis link doesn't prefill the bug with a template. I updated the bug.\r\n\r\n\r\n", "@rainwoodman ,\r\n\r\nCan you please take a look at this [link](https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_resource_variables) which you are looking for.It helps.\r\n\r\nThanks!", "Thanks for the link. The description only explains enabling v2 feature that's compatible with v1, and it helped a bit.\r\n\r\nI found that ResourceVariable actually has a really nice docstring:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/resource_variable_ops.py#L1461\r\n\r\nBut I don't think this docstring is discoverable from the documentation site.\r\n\r\nDo you know why(or whether) this docstring is excluded and if there is a way to link to the docstring in the documentation that mentions 'resource variable\"?\r\n", "Hi, For more details on the `Variable` behavior in Tensorflow 2, you can refer to [this](https://github.com/tensorflow/community/blob/master/rfcs/20180817-variables-20.md) documentation. \r\nAlso, refer [this](https://stackoverflow.com/questions/40817665/) post for details on `ResourceVariable` which explains `ResourceVariable` as the replacement for `Variable`.", "Those are some good resources @sachinprasadhs, @rainwoodman, and @tilakrayal.\r\n\r\nIf you want to help make these more visible I think this is/should be the central location for explaining this on the site:\r\n\r\nhttps://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables\r\n\r\nIf anyone wants to help I would approve any changes that:\r\n\r\n- add links from that section in to the RFC or other helpful materials, or\r\n- link to that section from anywhere that mentions reference-variables or resources-variables."]}, {"number": 49613, "title": "spell correction", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": []}, {"number": 49612, "title": "Video classification TensorFlow Lite model(i3d) with 5D input on android ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Vivo Y91i\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: v2.5.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): NIL\r\n- GCC/Compiler version (if compiling from source): NIL\r\n- CUDA/cuDNN version: CUDA 11.0\r\n- GPU model and memory: 11 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nNeed to run a sign language video classification model(i3d) on android, that takes 5 dimension ( 1 3 64 224 224),https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android , but failes. But runs on PC with python3.7\r\n```bash\r\nimport random\r\nimport re\r\nimport os\r\nimport tempfile\r\nimport ssl\r\nimport cv2\r\nimport scipy.special\r\nimport numpy as np\r\nimport pickle as pkl\r\nimport math\r\nimport copy\r\nimport tensorflow as tf\r\n# Some modules to display an animation using imageio.\r\nimport imageio\r\nfrom IPython import display\r\n\r\nfrom urllib import request  # requires python3\r\n# Utilities to open video files using CV2\r\ndef crop_center_square(frame):\r\n  y, x = frame.shape[0:2]\r\n  min_dim = min(y, x)\r\n  start_x = (x // 2) - (min_dim // 2)\r\n  start_y = (y // 2) - (min_dim // 2)\r\n  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\r\n\r\ndef load_label():\r\n  with open(\"labels.txt\", 'r') as f:\r\n    labels = [line.strip() for line in f.readlines()]\r\n    return labels\r\n\r\n# load the label\r\nword_data = load_label(\r\n    \r\n  )\r\n# load the tensorflow lite run time\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=\"onnx_lite64.tflite\")\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# resize\r\nresize = (224,224)\r\npath = \"test.mp4\"\r\n# read the video\r\ncap = cv2.VideoCapture(path)\r\n\r\n# Get the Default resolutions\r\nframe_width = int(cap.get(3))\r\nframe_height = int(cap.get(4))\r\n\r\n# Define the codec and filename.\r\nout = cv2.VideoWriter('output.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 5, (frame_width,frame_height))\r\nframes = []\r\n\r\nin_frames = 65\r\ncounter = 1\r\n\r\n\r\ntry:\r\n  while True:\r\n   \r\n    ret, frame_old = cap.read()\r\n    if not ret:\r\n      break\r\n    frame_new = copy.deepcopy(frame_old)\r\n    frame = crop_center_square(frame_old)\r\n\r\n    frame = cv2.resize(frame, resize)\r\n    frame = frame[:, :, [2, 1, 0]] \r\n    \r\n    frames.append(frame)\r\n    # check the counter is not zero and append the frames\r\n    if counter!=0:\r\n\r\n      if counter % in_frames == 0:\r\n      \r\n\r\n        del frames[:]\r\n\r\n    else:\r\n\r\n     \r\n      frames.append(frame)\r\n     \r\n     \r\n    \r\n\r\n    \r\n    res = np.array(frames) / 255.0\r\n    print(res.shape)\r\n    value = res.shape[0] \r\n    counter+=1\r\n   \r\n  \r\n\r\n    # Remove the frame with zero clip\r\n    if value !=0 and value == 64:\r\n      #print(res.shape)\r\n      model_input = tf.constant(res, dtype=tf.float32)[tf.newaxis, ...]\r\n      print(model_input.shape)\r\n      inp = tf.transpose(model_input, perm=[0, 4, 1, 2,3])\r\n      print(inp.shape)\r\n      interpreter.set_tensor(input_details[0]['index'], inp)\r\n      interpreter.invoke()\r\n      outputs = interpreter.get_tensor(output_details[0]['index'])\r\n      topk=1\r\n      result = []\r\n      num_clips =1\r\n      num_detections = 2000\r\n      for i in range(num_detections):\r\n        res = outputs[0][i]\r\n        result.append(res)\r\n      #print(result)\r\n      res = []\r\n      res.append(result)\r\n      \r\n      raw_scores = np.array(res)\r\n\r\n      prob_scores = scipy.special.softmax(raw_scores, axis=1)\r\n      prob_sorted = np.sort(prob_scores, axis=1)[:, ::-1]\r\n      pred_sorted = np.argsort(prob_scores, axis=1)[:, ::-1]\r\n     \r\n\r\n\r\n\r\n      word_topk = None\r\n      for k in range(topk):\r\n          for i, p in enumerate(pred_sorted[:, k]):\r\n              if str(word_data[p])!=\"want\":\r\n                  print(str(word_data[p]))\r\n              \r\n                  word_topk= word_data[p]\r\n      prob_topk = prob_sorted[:, :topk].transpose()\r\n      print(\"Predicted signs:\")\r\n      print(word_topk)\r\n      print(type(prob_topk))\r\n\r\n\r\n      cv2.putText(frame_old, str(word_topk),(30,50),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,0),2,cv2.LINE_AA)\r\n      out.write(frame_old)\r\n      cv2.imshow(\"Result\",frame_old)\r\n\r\n  # All the results have been drawn on the frame, so it's time to display it.\r\n    cv2.imshow('rgb', frame_new)\r\n\r\n\r\n    if cv2.waitKey(1) == ord('q'):\r\n      break\r\n\r\n\r\n  #word_data = load_label()\r\n\r\nfinally:\r\n  cap.release()\r\n\r\n```\r\n## Error on Android studio\r\n![java](https://user-images.githubusercontent.com/48623612/119530465-a5dbe200-bda0-11eb-9a1e-5331b514fbd4.png)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nThe model was converted from PyTorch > TF .> TFLite. The code throws error while running on android studio.\r\n\r\n![input](https://user-images.githubusercontent.com/48623612/119531294-68c41f80-bda1-11eb-8ae2-faaee6fc89e7.png)\r\n![output](https://user-images.githubusercontent.com/48623612/119531305-6b267980-bda1-11eb-9852-1c597d11c502.png)\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["Please consider using the fixed input tensor shapes and FlexAddV2 can be gone if there are only fixed shapes in the input tensors of the add op.", "But how can we input (1 3 64 224 224) instead of (1 224 224 3) in the Java", "For the JavaScript question, could you upload a new post about that in order to keep each issue focused?", "![119530465-a5dbe200-bda0-11eb-9a1e-5331b514fbd4](https://user-images.githubusercontent.com/48623612/119594686-e156ca80-bdf9-11eb-9d6f-3331143de423.png)\r\nThis is the error. I think it is due to the input size. In opencv we can concat frames and convert it to a numpy array to get the size as (1 64 224 224 3) then transpose it to (1 3 64 224 224). No idea on how it can be done with Java.", "> For the JavaScript question, could you upload a new post about that in order to keep each issue focused?\r\n\r\nsorry for the typo, it's java", "Any updates", "It would be better to find equivalent numpy library in Java to transpose your data in Java or modify your model to add a transpose op to handle the request in the model itsel.f", "I just tried many ways. The possibile solution is bitmat to bytebuffer conversion. After many trial and error. It runs but the labels are not accurate", "![Uploading IMG_20210527_223254.jpg\u2026]()\r\n", "I think the 5 D input is the issue. How can we put the float values in the bytebuffer without messing up the input", "![IMG_20210527_223254](https://user-images.githubusercontent.com/48623612/119868578-d789b000-bf3c-11eb-92cb-3070ad7f35f0.jpg)\r\n", "Has anyone successfully run  5D video tensors as input in android java", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49612\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49612\">No</a>\n"]}, {"number": 49611, "title": "[QST] tensorflow 2.4.2 release date [security patches]", "body": "Synopsys Black Duck is flagging tensorflow 2.4.0 with security vulnerabilities\r\n\r\nlooking at the security advisories - it looks like these would be patched with 2.4.2 - Is there an estimated date of when 2.4.2 would be released?\r\n\r\nApologies if this is already discussed elsewhere and I just wasn't able to find it", "comments": ["Thanks for your issue. We can expect a patch release for TF 2.4 in next week. Will update this thread as we know more.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@ymodak do you have any further updates? - didn't want the bot to close this :)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "TF 2.4.2 has been released https://github.com/tensorflow/tensorflow/releases/tag/v2.4.2\r\nThanks!"]}, {"number": 49610, "title": "\"AutoGraph could not transform function  and will run it as-is\" for an Example from a Guide ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro 19041.985\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.8\r\n- Bazel version (if compiling from source): none\r\n- GCC/Compiler version (if compiling from source):none\r\n- CUDA/cuDNN version:none\r\n- GPU model and memory:none\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nRun code \r\n```\r\ndef train_one_step():\r\n  pass\r\n\r\n@tf.function\r\ndef train(num_steps):\r\n  print(\"Tracing with num_steps = \", num_steps)\r\n  tf.print(\"Executing with num_steps = \", num_steps)\r\n  for _ in tf.range(num_steps):\r\n    train_one_step()\r\n\r\ntrain(num_steps=10)\r\n```\r\nfrom the guide [https://www.tensorflow.org/guide/function](https://www.tensorflow.org/guide/function)\r\nand received a warning (see an attachment) \r\n[warning.log](https://github.com/tensorflow/tensorflow/files/6540126/warning.log)\r\n\r\n**Describe the expected behavior**\r\nan example from the guide should work \r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing): no\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49610\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49610\">No</a>\n", "It is an issue of PyCharm python console."]}, {"number": 49609, "title": "Fixing math_ops.reduce_variance for dispatch failure to ragged", "body": "Fixes #49606 \r\n\r\ncc @edloper , Thanks for help in providing solution.", "comments": ["@edloper , I have added test case. Can you please review?", "@edloper , can you please approve again? I forgot to add `axis`", "@edloper, @rohan100jain, @gbaned - This PR was approved 12 days ago but hasn't merged yet. Is there anything pending or failing for this?"]}, {"number": 49608, "title": "[MLIR][DISC] pattern conversion from tf2mhlo: ConvertPadOpDynamic and ConvertGatherNdOpDynamic", "body": "We are porting our MLIR-based dynamic shape compiler to tf community (From OP def, Patttern, to Optimization pass, etc).\r\nThis is the first PR about tf2mhlo pattern conversion, which including ConvertPadOpStaticDynamic and ConvertGatherNdOpStaticDynamic.\r\nThe rest pattern conversions we will add:\r\n- ConvertConvOpxxx\r\n- ConvertGatherV2Opxxx\r\n- ConvertRangeOpxxx\r\n- ConvertSigmoidOpxxx\r\n- ConvertSliceOpxxx\r\n- ConvertSplitOpxxx\r\n- ConvertSqueezeOpxxx\r\n- ConvertStridedSliceOpxxx\r\n- ConvertTileOpxxx\r\n- ConvertUnpackOpxxx\r\n- ConvertPrintOp\r\n- ConvertSigmoidGradOpxxx\r\n- ConvertSignOpxxx\r\n\r\nPrevious discussions\uff1a[RFC](https://groups.google.com/a/tensorflow.org/g/mlir/c/_X48poNcbDI/m/jCC8BWIICQAJ), [discussion_1](https://llvm.discourse.group/t/updates-on-mlir-based-dynamic-shape-compiler/2384), [Recording of meeting](https://drive.google.com/file/d/1_uEISlV5MUWdG9faKAdKlCWnPtGjRC-D/view).\r\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49608) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49608) for more info**.\n\n<!-- need_author_cla -->", "We see internal test failures, looking like:\r\n\r\n```\r\ngoogle3.third_party.tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: <unknown>:0: error: loc(\"depthwise/SpaceToBatchND\"): 'mhlo.dynamic_pad' op can't be translated to XLA HLO\r\n<unknown>:0: note: loc(\"depthwise/SpaceToBatchND\"): see current operation: %7 = \"mhlo.dynamic_pad\"(%arg0, %1, %5, %6, %0) : (tensor<4x6x6x48xf32>, tensor<f32>, tensor<4xi64>, tensor<4xi64>, tensor<4xi64>) -> tensor<4x6x6x48xf32>\r\n\r\n\t [[{{node depthwise/SpaceToBatchND}}]]\r\n  (1) Unknown: <unknown>:0: error: loc(\"depthwise/SpaceToBatchND\"): 'mhlo.dynamic_pad' op can't be translated to XLA HLO\r\n<unknown>:0: note: loc(\"depthwise/SpaceToBatchND\"): see current operation: %7 = \"mhlo.dynamic_pad\"(%arg0, %1, %5, %6, %0) : (tensor<4x6x6x48xf32>, tensor<f32>, tensor<4xi64>, tensor<4xi64>, tensor<4xi64>) -> tensor<4x6x6x48xf32>\r\n\r\n\t [[{{node depthwise/SpaceToBatchND}}]]\r\n\t [[depthwise/BatchToSpaceND/_9]]\r\n0 successful operations.\r\n```\r\n\r\nI can get more information out of these tests tomorrow I think, let me know!", "> We see internal test failures, looking like:\r\n> \r\n> ```\r\n> google3.third_party.tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n>   (0) Unknown: <unknown>:0: error: loc(\"depthwise/SpaceToBatchND\"): 'mhlo.dynamic_pad' op can't be translated to XLA HLO\r\n> <unknown>:0: note: loc(\"depthwise/SpaceToBatchND\"): see current operation: %7 = \"mhlo.dynamic_pad\"(%arg0, %1, %5, %6, %0) : (tensor<4x6x6x48xf32>, tensor<f32>, tensor<4xi64>, tensor<4xi64>, tensor<4xi64>) -> tensor<4x6x6x48xf32>\r\n> \r\n> \t [[{{node depthwise/SpaceToBatchND}}]]\r\n>   (1) Unknown: <unknown>:0: error: loc(\"depthwise/SpaceToBatchND\"): 'mhlo.dynamic_pad' op can't be translated to XLA HLO\r\n> <unknown>:0: note: loc(\"depthwise/SpaceToBatchND\"): see current operation: %7 = \"mhlo.dynamic_pad\"(%arg0, %1, %5, %6, %0) : (tensor<4x6x6x48xf32>, tensor<f32>, tensor<4xi64>, tensor<4xi64>, tensor<4xi64>) -> tensor<4x6x6x48xf32>\r\n> \r\n> \t [[{{node depthwise/SpaceToBatchND}}]]\r\n> \t [[depthwise/BatchToSpaceND/_9]]\r\n> 0 successful operations.\r\n> ```\r\n> \r\n> I can get more information out of these tests tomorrow I think, let me know!\r\n\r\nFixed. We expected if paddings is a variable, tf.pad should become mhlo.dynamic_pad. But as we haven't implemented downstream logic, this transform breaks the test. ", "This was merged"]}, {"number": 49607, "title": "New Compatibility Problem of Stateful Random Binomial Operators", "body": "hi,\r\nWhen tf is upgraded from 1.15 to 2.4.1, the implementation of the StatefulRandomBinomial operator is different in the two versions, and the verification rules for the input count and probs shape are changed. So what's the reason for this change? What are the differences between the two versions? In addition, the StatefulRandomBinomial operator is not found in API 1.15. Is this operator not supported in API 1.15?", "comments": ["Are there any relevant conclusions?", "Hi feiyouliang,\r\n\r\nCan you please clarify what you mean by \"implementation is different\"? Can you provide a piece of code to show the difference you were describing?", "Hi @feiyouliang , the shape restrictions were relaxed by https://github.com/tensorflow/tensorflow/commit/5396e7a3cd91b5f7895fa3dbe9809400e032d68a to allow broadcasting.", "@JW1992 The tf2.4.1 version is incompatible with tf1.5.1. For example, the input parameter shape is [3] and the value is [2, 2, 3], and the input parameter count is [1] and the value is [2]. The shape of the input parameter probs is [1] and the value is [0.5]. This example can work in version 1.15, but does not meet the verification requirements of version 2.4.1.", "Hi!  If I understand correctly, I believe this also works in TF2.  Using the example from rng.binomial:\r\n\r\n```python\r\ncounts = [1.]\r\n# Probability of success.\r\nprobs = [0.5]\r\n \r\nrng = tf.random.Generator.from_seed(seed=234)\r\nbinomial_samples = rng.binomial(shape=[1], counts=counts, probs=probs)\r\nprint(binomial_samples)\r\n```\r\n\r\nQuick note: if this code used `counts = [1]`, it will (correctly) complain about lacking an integer kernel, so it requires the usage of the floating point value: `counts = [1.]`.\r\n\r\nIf that doesn't help explain, could you pass along an example which fails -- but that you expect to work?  Thanks!", "Hi @feiyouliang, you are right that the semantics changed from broadcasting `counts` and `probs` from the *leftmost* dimension of `shape` to the *rightmost*. But we don't guarantee backward compatibility between TF1 and TF2.\r\n\r\nBTW your example happen to work in both TF1 and TF2 because the leftmost and the rightmost dimension of shape `[3]` are the same thing."]}, {"number": 49606, "title": "reduce_variance gives error in case of RaggedTensor when axis=0", "body": "As part of #37014 , `reduce_variance` was added for ragged tensors. Although it is working fine for `axis=1`, it gives error for same input if we change to `axis=0`. \r\n\r\nI am attaching the [gist](https://colab.research.google.com/gist/ashutosh1919/b0591ddb485107187b982a08276712be/untitled550.ipynb).\r\n\r\nBy observing the stack trace, what I think the issue is:\r\n(1) when I explicitly pass input to `ragged_math_ops.reduce_variance`, it gives correct result. So, the problem is not with `reduce_variance` function.\r\n(2) When I call `tf.math.reduce_variance`, it internally call `GlobalDispatcherOp` which iterates over all the dispatchers to see where the op is supported. \r\n(3) When the op runs for `BinaryRaggedElementwiseDispatcher`, it fails for some reason and that is why execution fails. It didn't reach till the dispatcher associated for `reduce_variance`.\r\n(4) That's why the problem I think is with dispatcher module.\r\n\r\n@mihaimaruseac, @edloper - I am not able to exactly find why the error occures in dispatcher. Please help me by pointing to specific direction and will contribute the fix.", "comments": ["I don't see any errors (or anything involving ragged tensors or reduce_variance) in the attached gist.  Did you link the right one?", "I was able to reproduce this problem with:\r\n\r\n```\r\nimport tensorflow as tf\r\nx = tf.ragged.constant([[1., 2, 3], [4, 5], [6, 7, 8, 9]])\r\nprint(tf.math.reduce_variance(x, axis=1))  # succeeds\r\nprint(tf.math.reduce_variance(x, axis=0))  # fails\r\n```\r\n\r\nThe problem arises because the current dispatch mechanism is reactive, not proactive.  In particular, the current dispatch mechanism is a fallback that gets used when the \"normal\" implementation raises an error.  This design was used to ensure that dispatch didn't add overhead to existing operations, but we have plans to change it to a proactive mechanism, which checks the types of arguments before running the operation.\r\n\r\nSo the long term fix is to update the dispatch to be proactive, and not a fallback mechanism.\r\n\r\nBut until we've finished that update, a shorter-term solution is to update reduce_variance to call convert_to_tensor on its input.  Almost all TensorFlow ops call convert_to_tensor on their inputs before processing them.  This ensures that we can pass in non-tensor values, such as numpy arrays or python lists.  The fact that (almost) all TensorFlow ops call convert_to_tensor is what makes the dispatch fallback mechanism work, since this will fail for types such as RaggedTensor.\r\n\r\nIf we look at some of the other reduce operations in math_ops.py, such as reduce_logsumexp, they do call convert_to_tensor on their input.  I believe that the following change should make dispatch work correctly (though I haven't actually tested it yet):\r\n\r\n```\r\n  with ops.name_scope(name):\r\n    input_tensor = ops.convert_to_tensor(input_tensor)       # NEW\r\n    means = reduce_mean(input_tensor, axis=axis, keepdims=True)\r\n```\r\n\r\nThis change will also have the side benefit that input_tensor won't get converted to a tensor twice.  (In the current code, if input_tensor is a python list or other non-tensor value, then it will get converted twice: once when we call reduce_mean, and again in the expression `input_tensor - means`).\r\n\r\nI'd be happy to make that fix if you like; or if you'd prefer to contribute the fix, that would be fine too.", "@edloper , apologies. I have updated [gist](https://colab.research.google.com/gist/ashutosh1919/b0591ddb485107187b982a08276712be/untitled550.ipynb)", "The issue will move to closed status once the PR is merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49606\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49606\">No</a>\n"]}, {"number": 49605, "title": "Prediction difference in the converted TFlite model and trained checkpoint from TFOD API", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution : Android (Model trained linux Pop OS)\r\n- TensorFlow installation : pip\r\n- TensorFlow library : 2.4.1\r\n\r\n**Descriptions**\r\nWe have been training model to predict corners using SSD MobileNet V2 320x320 FPNlite architecture model. We used TFOD API code files to train and convert the checkpoint into TFlite format . We were able to convert checkpoint (ckpt 55) into TFlite format without any errors. Analyzing prediction from TFlite model and checkpoint for the same image, we found there is significate difference in predicted coordinates and score values. We can't identify any issues when converting the model and the model is not quantized. \r\n\r\nFollowing are link includes to trained checkpoints, converted tflite model and prediction images.\r\n **link** : https://drive.google.com/drive/folders/1ZNJgMaUXWZTKCDQoB1gPFufwhypX6ZHB?usp=sharing\r\n\r\nTflite prediction -  \r\nimage : prediction_from_tflite.jpg\r\nscores : [0.90501845 0.3708766  0.09824225 0.05691937 0.05246159 0.04990548\r\n 0.04931968 0.03742141 0.03397185 0.03386968]\r\n\r\nCheckpoint prediction \r\nimage : prediction_from_checkpoint.jpg\r\nscores : [0.98141104 0.9789326  0.7256872  0.49804866 0.309637   0.23567227\r\n 0.19505009 0.05802718 0.056795   0.04440427]\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Hey @Sudhan97 a few questions to clarify the issue:\r\n\r\n1. Can you try the non-FPNLite `SSD MobileNet v2 320x320` from the [detection zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)? This will help me understand if the error is in the conversion of the model or inference side.\r\n2. Could you show the commands you used to export & convert the model to TFLite?", "Hi @srjoglekar246, I also used non-FPNlite model previously there was no similar issues. Using FPNlite model  we were able to get more accurate predictions (using checkpoint) compared to non-FPNlite model. I'll update you on code used for export and conversion.", "@srjoglekar246 \r\nFor the exporting we modified exporter_lib_v2 script (under object-detection : https://github.com/tensorflow/models/blob/master/research/object_detection/exporter_lib_v2.py ) to export inference graph from specific checkpoint(using checkpoint number). In the following link, we have included the files used for exporting and converting.\r\n**link :** https://drive.google.com/drive/folders/1e1UGrUOWkvfzAyNWJ5g8l0iH7w7sWkfi?usp=sharing\r\n\r\nexporter_lib_v2.py - modified to export inference graph from specific checkpoint.\r\nexport_saved_model_from_ckpt.py - script used to export into saved model using exporter_lib_v2 script\r\nconversion_script.py - script used to convert into tflite model", "Hi @srjoglekar246, is there any update on this issue?", "Hey @Sudhan97 , the procedure to generate a TFLite-friendly SavedModel is a little different, especially considering quantization & accelerator support. See [this g3doc](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md). Note that the script to generate intermediate SavedModel is different (`export_tflite_graph_tf2.py`)", "@srjoglekar246, we followed instructions, generated intermediate saveModel using `export_tflite_graph_tf2.py` and converted into TFlite model enabling custom_ops as previously mentioned. \r\nAgain it show the similar behaviour, where there is significant difference between prediction score from trained checkpoint and converted TFlite model for the same image. The model is not quantized. \r\n\r\nPrediction score from trained checkpoint\r\nscores : [`0.98141104, 0.9789326, 0.7256872, 0.49804866`, 0.309637` 0.23567227,\r\n0.19505009, 0.05802718, 0.056795, 0.04440427]\r\n\r\nPrediction score from converted TFLite model\r\nscore : [`0.90501964, 0.37087825, 0.09824187, 0.05691952,` 0.05246329, 0.04990584,\r\n 0.04931888, 0.03742117, 0.03397211, 0.03386891]\r\n\r\nIn the following link we have included the saved model generated using `export_tflite_graph_tf2.py script` and converted TFlite model. \r\n**Link for saved models and converted TFlite model**\r\nhttps://drive.google.com/drive/folders/1ZsEHi8wXuberGjggYCZJp2Fuwqcd3gqL?usp=sharing\r\n", "Can you try enabling regular NMS in TFLite using [this flag](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_tf2.py#L122) in `export_tflite_graph_tf2.py`? This performs a slower (but more accurate) version of NonMaxSuppression in the SSD graph.", "@srjoglekar246 , we tried enbling regular NMS but it didn't chnage the results. We get the similar predictions and scores as previous converted models.", "Hmm, it seems like the model is unsuitable somehow, since you are doing all the steps right :-/. Can you try using `SSD MobileNet V1 FPN 640x640` or `CenterNet MobileNetV2 FPN 512x512` from the same zoo? (See [CenterNet conversion Colab](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/centernet_on_device.ipynb)). There might be some discrepenacy in output of the TF/TFLite kernels in that SSD FPNLite model you are trying.", "@srjoglekar246, We trained model using `SSD MobileNet V1 FPN 640x640` reducing the image input size 320x320. We experience similar issue with this architecture also. We are getting more accurate prediction and score from checkpoint but the **converted TFLite model** has low accuracy and score.  Following is the prediction score from checkpoint and converted TFLite model. \r\n\r\nPrediction score from **trained checkpoint**\r\nscores : [`0.9993659,  0.99498355, 0.98933035, 0.80486923,` 0.42113468,  0.10038441,\r\n 0.08877391, 0.08859652, 0.02857885, 0.02726579]\r\n\r\nPrediction score from converted **tflite model**\r\nscore : [ `0.78976953, 0.28661278, 0.23196483, 0.152545,`   0.13510752, 0.11414424,\r\n 0.0949465,  0.08693931, 0.07839099, 0.07618982, 0.07174137, 0.05871615]\r\n\r\nWe have shared the link for exported saved model and converted model.\r\n**link :**https://drive.google.com/drive/folders/1zG4aFIRF6Rujmp-mrHKxEYPvw4XKUBe-?usp=sharing", "I took a look at the code, and I think the [TF2 exporting library](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_lib_tf2.py) doesn't handle the FPN portion of the feature extractor correctly. (IIUC, a portion of the graph is missing)\r\n\r\nThere are two ideas I have. I don't have the cycles to fully debug this, but we can push with your help.\r\n\r\n1) The CenterNet MobileNetV2 FPN 512x512 download doesn't have this issue. You can try using it with a higher resolution than 320x320 if possible, and it should perform well (especially with NNAPI/GPU acceleration). At the very least, we can verify that the SavedModel & the TFLite model have the same outputs.\r\n\r\n2) The [Model Maker](https://www.tensorflow.org/lite/tutorials/model_maker_object_detection) folks recently added support for training EfficientDets with custom data, can you check if that works better for you?\r\n\r\nSorry about the confusion, there seem to be some [FPN-specific](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph_lib.py#L281) portions of the TF1 exporting script that weren't correctly ported to the TF2 version.", "Thanks I'll check this out and let you know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@srjoglekar246 Hi, we trained model using **CenterNet MobileNet V2 FPN 512x512** as you suggested and we didn't find accuracy difference between predictions from checkpoint and converted TFlite model (prediction taken using Python API for tflite) `but when the  same model used in android application we don't get the accurate prediction (even for the same image).` We follow the similar approach, implemented in sample android applicaton https://github.com/tensorflow/examples/blob/fa33ab053877a0acbee1b78c5e3120554b94e5a5/lite/examples/object_detection/android/lib_interpreter/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java#L186\r\n\r\nDoesn't this **CenterNet MobileNet V2 FPN architecture** require different approach because the sample android has used SSD Mobilenet architecture? \r\nFYI we use the float model.\r\n\r\n**Converted tflite model link : **https://drive.google.com/drive/folders/1EAdII2BMzA2fdyJnTawldbCFakgqJc7z?usp=sharing \r\n", "Yup. As you correctly guessed, you need to make a few changes based on your model. Assuming you are not exporting the CenterNet model with keypoint output (since you only need bounding boxes), you simply need to disable input preprocessing that happens in the inference code. @lintian06 / @lu-wang-g  can you provide a pointer to how to disable preprocessing to support CenterNet?", "@srjoglekar246 I may not have the full context, but why the input processing need to be disabled? Is there a custom implementation? \r\n\r\nAs far as I understand, the converted model should be able to fit the tflite object detection reference app. But there are a few places need to be checked:\r\n1. Have you added metadata to your model. See the instruction [here](https://www.tensorflow.org/lite/convert/metadata_writer_tutorial#object_detectors). Note that you need to pack the label file into the model. The label file in the [asset](https://github.com/tensorflow/examples/tree/fa33ab053877a0acbee1b78c5e3120554b94e5a5/lite/examples/object_detection/android/app/src/main/assets) folder is for demonstration purpose only.\r\n2. Have you updated the [model and label file information](https://github.com/tensorflow/examples/blob/fa33ab053877a0acbee1b78c5e3120554b94e5a5/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/DetectorActivity.java#L53-L57) accordingly?\r\n3. Try [building with Task Library](https://github.com/tensorflow/examples/tree/fa33ab053877a0acbee1b78c5e3120554b94e5a5/lite/examples/object_detection/android#switch-between-inference-solutions-task-library-vs-tflite-interpreter) , a high-level API compared to Interpreter. It encapsulates everything from init, to pre/post processing. So as long as you have the correct model file [here](https://github.com/tensorflow/examples/blob/fa33ab053877a0acbee1b78c5e3120554b94e5a5/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/DetectorActivity.java#L55), it will handle the rest for you.", "The CenterNet graph obtained from ODAPI does the pre-processing [within the model](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_lib_tf2.py#L287), so the model takes as input a [float tensor with raw pixel values](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_lib_tf2.py#L268). IIUC we will need some changes so that we don't pre-process the data before invoking the model.", "Does the model require an input image of 320x320 (read from Netron) or any random shape? ", "Shape is fixed (320x320)", "Then we still need to convert the camera output to 320x320 RGB, right?", "Aah yes, I just meant disabling the normalization etc in the pre-processing. My bad :-)", "I see. That makes sense!\r\n\r\nTo disable normalization in lib_interpreter, just update the value for [MEAN and STD](https://github.com/tensorflow/examples/blob/fa33ab053877a0acbee1b78c5e3120554b94e5a5/lite/examples/object_detection/android/lib_interpreter/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java#L62-L63) as follows:\r\n```\r\nprivate static final float IMAGE_MEAN = 0.0f;\r\nprivate static final float IMAGE_STD = 1.0f;\r\n```\r\n\r\nTo disable normalization in lib_task_api, set mean and std to the following value when [creating the metadata](https://www.tensorflow.org/lite/convert/metadata_writer_tutorial#object_detectors):\r\n```\r\n_INPUT_NORM_MEAN = 0.0\r\n_INPUT_NORM_STD = 1.0\r\n``` ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49605\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49605\">No</a>\n"]}, {"number": 49604, "title": "Support full [b]float16 in embedding_lookup_sparse", "body": "- Adds float16 support for SparseSegment* ops.\r\n- Removes a forced cast to float32 in `embedding_lookup_sparse` and instead outputs the same type as the input. The inner computations are still done in float32 to avoid numerical issues.\r\n- This improves performance and makes the operation consistent with all other operations that output the same type as the input.\r\n\r\ncc @nluehr @reedwm ", "comments": ["@penpornk I noticed that `SparseSegmentSumGrad` was added since I submitted this PR, so we should add float16 for that op too.\r\nI can rebase and force push to this PR, or I can submit a separate PR. Let me know which you'd prefer.", "Gentle ping", "@benbarsdell Sorry for the late reply! Please submit another PR."]}, {"number": 49602, "title": "Update variable name within Anomaly Detection example", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:https://www.tensorflow.org/tutorials/generative/autoencoder?hl=en\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\nWithin Anomaly Detection section, within Build the Model, some variables names are misleading with regard to data that is handled when using the encoder and decoder after training.\r\nAs a matter of fact, data here are ECG time series and no longer images like in previous example with mnist and fashion_mnist\r\n\r\n### Clear description\r\n\r\nencoded_imgs = autoencoder.encoder(normal_test_data).numpy()\r\ndecoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\r\n\r\nshould be replaced by something like:\r\nencoded_data = autoencoder.encoder(normal_test_data).numpy()\r\ndecoded_data = autoencoder.decoder(encoded_data).numpy()\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@cl3m3nt \r\nThank you for your update, we will review the changes and submit a pr accordingly.", "@cl3m3nt \r\nA pr has been created for this issue, it will be closed once the pr is merged #1905", "@Saduf2019 @cl3m3nt Thank you, will review.", "https://github.com/tensorflow/docs/pull/1905 \u2705 \r\n", "@8bitmp3 \r\nThank you for the update, shall i move this issue to closed status.", "And thank you @Saduf2019 @cl3m3nt Feel free to go ahead if you are happy with the PR \ud83d\udc4d "]}, {"number": 49600, "title": "`tf.keras.layers.LayerNormalization` may produce CPU->GPU Memcpy error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 11.2 / 8.1.1\r\n- GPU model and memory: Quadro T1000 (computeCapability: 7.5)\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.keras.layers.LayerNormalization` may produce an error when epsilon is too high (in my case it was 1e-3 but when setting it to 1e-7 it works as expected):\r\n```\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal:  cuDNN launch failure : input shape ([1,256,128,1])\r\n         [[node GenieDelay/label/layer_norm/FusedBatchNormV3 (defined at Workspace\\CompanyTransformer\\company_transformer\\src\\company_transformer\\genie\\delay\\modeling.py:53) ]]\r\n         [[div_no_nan_3/ReadVariableOp/_566]]\r\n  (1) Internal:  cuDNN launch failure : input shape ([1,256,128,1])\r\n         [[node GenieDelay/label/layer_norm/FusedBatchNormV3 (defined at Workspace\\CompanyTransformer\\company_transformer\\src\\company_transformer\\genie\\delay\\modeling.py:53) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_21841]\r\nFunction call stack:\r\ntrain_function -> train_function\r\n2021-05-24 19:06:14.571178: I tensorflow/stream_executor/stream.cc:1404] [stream=000001C31D860660,impl=000001C331B8A6D0] did not wait for [stream=000001C31D860D20,impl=000001C331B8A670]\r\n2021-05-24 19:06:14.574031: F tensorflow/core/common_runtime/gpu/gpu_util.cc:340] CPU->GPU Memcpy failed\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nNot quite sure what should be expected here because I'm not quite sure why copying a float32[] from CPU to GPU fails but I guess if it's due to some precision (why so?) cropping should be applied. If it's due to NaN then maybe a proper logging would be fine (note that in that case, `tf.keras.callbacks.TerminateOnNaN()` was not hit, but I guess this is called only as a callback at then end of a batch but here it seems to occur in the middle of the calculation). Otherwise, I can't say why copying the buffer fails.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing): Haven't investigated in the code right now but I can dig if need be.\r\n\r\n**Standalone code to reproduce the issue**\r\nUnfortunately I cannot provide the data as they're confidential but I don't think they matter that much actually. I use a `HuggingFace's` BERT-like Transformer (just the architecture, I adapated it to run on custom financial data) which I stack with other embeddings and standalone features, and append a head. The input vector size of this head is roughly ~300: the 256 first dimensions come from a pooled-CLS of a Transformer so having values between [-1,1] as they're output from `tanh`, there are roughly 10 features that are unscaled but positive, and the rest are embeddings gotten from layers three layers `tf.keras.layers.Embedding` with output dimension 16 (all weights are initialized with `tf.keras.initializers.TruncatedNormal(stddev=0.02)`). The error comes in the head and here the code stub (without data, I'll try to produce fake data that makes it fail if ever needed):\r\n```Python\r\nclass DelayHead(tf.keras.layers.Layer):\r\n    def __init__(self,\r\n                 number_heads: int,\r\n                 head_hidden_size: int,\r\n                 initializer_range: float,\r\n                 head_layer_norm_eps: float,\r\n                 head_dropout_prob: float,\r\n                 **kwargs):\r\n        super().__init__(**kwargs)\r\n\r\n        self.number_heads = number_heads\r\n        self.denses = []\r\n        self.layer_norms = []\r\n        for _ in range(self.number_heads):\r\n            self.denses.append(\r\n                tf.keras.layers.Dense(\r\n                    units=head_hidden_size,\r\n                    kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\r\n                    name='dense'\r\n                )\r\n            )\r\n            self.layer_norms.append(\r\n                tf.keras.layers.LayerNormalization(\r\n                    epsilon=head_layer_norm_eps,\r\n                    name='layer_norm'\r\n                )\r\n            )\r\n        self.dropout = tf.keras.layers.Dropout(\r\n            rate=head_dropout_prob\r\n        )\r\n        self.labeling = tf.keras.layers.Dense(\r\n            units=1,\r\n            activation='linear',\r\n            name='label'\r\n        )\r\n\r\n    def call(self, hidden_state: tf.Tensor) -> tf.Tensor:\r\n        for i in range(self.number_heads):\r\n            hidden_state = self.denses[i](hidden_state)\r\n            hidden_state = self.layer_norms[i](hidden_state)\r\n        hidden_state = self.dropout(hidden_state)\r\n        return self.labeling(hidden_state)\r\n```\r\n\r\n**Other info / logs** \r\n\r\nLogs can be slightly different from one run to another, but it boils down to a CPU->GPU memcpy error in the end. Here are two different logs I got:\r\n```\r\n2021-05-24 20:42:11.957505: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2021-05-24 20:42:11.957815: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on \r\nWindows\r\n2021-05-24 20:42:11.958364: W .\\tensorflow/stream_executor/stream.h:2048] attempting to perform DNN operation using StreamExecutor without DNN support\r\nTraceback (most recent call last):\r\n  File \"company_transformer\\src\\company_transformer\\genie\\delay\\task.py\", line 150, in <module>\r\n    main(\r\n  File \"C:\\Workspace\\CompanyTransformer\\company_transformer\\src\\company_transformer\\utils.py\", line 86, in decorated\r\n    return func(*args, **kwargs)\r\n  File \"company_transformer\\src\\company_transformer\\genie\\delay\\task.py\", line 131, in main\r\n    model.fit(\r\n  File \"C:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"C:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 888, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"C:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2942, in __call__\r\n    return graph_function._call_flat(\r\n  File \"C:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1918, in _call_flat\r\n  File \"C:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 555, in call\r\n    outputs = execute.execute(\r\n  File \"C:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal:  cuDNN launch failure : input shape ([1,256,128,1])\r\n         [[node GenieDelay/label/layer_norm/FusedBatchNormV3 (defined at Workspace\\CompanyTransformer\\company_transformer\\src\\company_transformer\\genie\\delay\\modeling.py:53) ]] \r\n         [[gradient_tape/GenieDelay/company_transformer/company_transformer_embeddings/RaggedToTensor/strided_slice/_626]]\r\n  (1) Internal:  cuDNN launch failure : input shape ([1,256,128,1])\r\n         [[node GenieDelay/label/layer_norm/FusedBatchNormV3 (defined at Workspace\\CompanyTransformer\\company_transformer\\src\\company_transformer\\genie\\delay\\modeling.py:53) ]] \r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_21841]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n\r\n2021-05-24 20:42:12.384440: I tensorflow/stream_executor/stream.cc:1404] [stream=0000016D1BA41AA0,impl=0000016D2FC1F860] did not wait for [stream=0000016D1BA40540,impl=0000016D2FC1F830]\r\n2021-05-24 20:42:12.385247: F tensorflow/core/common_runtime/gpu/gpu_util.cc:340] CPU->GPU Memcpy failed\r\n```\r\nand:\r\n```\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal:  cuDNN launch failure : input shape ([1,256,128,1])\r\n         [[node GenieDelay/label/layer_norm/FusedBatchNormV3 (defined at Workspace\\CompanyTransformer\\company_transformer\\src\\company_transformer\\genie\\delay\\modeling.py:53) ]]\r\n         [[div_no_nan_3/ReadVariableOp/_566]]\r\n  (1) Internal:  cuDNN launch failure : input shape ([1,256,128,1])\r\n         [[node GenieDelay/label/layer_norm/FusedBatchNormV3 (defined at Workspace\\CompanyTransformer\\company_transformer\\src\\company_transformer\\genie\\delay\\modeling.py:53) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_21841]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n\r\n2021-05-24 19:06:14.571178: I tensorflow/stream_executor/stream.cc:1404] [stream=000001C31D860660,impl=000001C331B8A6D0] did not wait for [stream=000001C31D860D20,impl=000001C331B8A670]\r\n2021-05-24 19:06:14.574031: F tensorflow/core/common_runtime/gpu/gpu_util.cc:340] CPU->GPU Memcpy failed\r\n```", "comments": ["@vbod ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.\r\n", "Please try limiting GPU memory growth using any of the methods listed [here](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and tensorflow v2.4 is compatible with cuda11.0.\r\nPlease take a look at configurations [here](https://www.tensorflow.org/install/source_windows#gpu) and check if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49600\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49600\">No</a>\n"]}, {"number": 49599, "title": "Fix a BUILD file for fusion ops to use Cudnn Frontend ", "body": "This fixes an issue when using the Cudnn Frontend for fusion ops. Specifically, the `StreamExecutor::GetFusedConvolveExecutionPlans` requires GOOGLE_CUDA to enable the Cudnn Frontend API for fusion ops. So, we need to set it when CUDA config is detected in the BUILD file.\r\n\r\ncc. @nluehr ", "comments": ["Done. PTAL. @timshen91 "]}, {"number": 49598, "title": "[MLIR][DISC] legalize tensor_load inserted during hlo-to-lhlo conversion", "body": "This PR implements logic for lowering memref.tensor_load ops that are\r\ninserted during `mhlo-legalize-to-lmhlo`", "comments": ["> I'd suggest breaking up this PR into smaller PRs and introduce the dynamic ops one or two at a time. That'll help ensure a more thorough code review.\r\n\r\nThanks. I'll take a try.", "@sanjoy Hi, I split the original PR into small PRs and sent the first PR. Could you take a look again?", "LGTM otherwise.", "> Only very minor comments.\r\n\r\nI have refined the code according to the comments. Could you have a look again?", "@sanjoy Could you refresh the page? I have changed both in the last commit.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/49598/files#diff-2aece6a81b6b40b767c54d4762d4da714b85bd2ee4f99ec52cf96349ea0f2cc7R240", "> @sanjoy Could you refresh the page? I have changed both in the last commit.\r\n> \r\n> https://github.com/tensorflow/tensorflow/pull/49598/files#diff-2aece6a81b6b40b767c54d4762d4da714b85bd2ee4f99ec52cf96349ea0f2cc7R240\r\n\r\nI believe in that commit you changed tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/lhlo_legalize_to_affine.cc, LMK if I'm misunderstanding.", "> > @sanjoy Could you refresh the page? I have changed both in the last commit.\r\n> > [#49598 (files)](https://github.com/tensorflow/tensorflow/pull/49598/files#diff-2aece6a81b6b40b767c54d4762d4da714b85bd2ee4f99ec52cf96349ea0f2cc7R240)\r\n> \r\n> I believe in that commit you changed tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/lhlo_legalize_to_affine.cc, LMK if I'm misunderstanding.\r\n\r\nSorry for that! you are right. I changed the wrong place. I pushed a new commit now.", "@sanjoy Could you take a look again?", "@sanjoy Thanks! I have changed it. ", "@wyzero  Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned Hi, I fix the error. Could you help to test again?", "@joker-eph Hi, I emit an error when failure. Could you help to take a look again?", "@gbaned Hi, It seems that the error in `Ubuntu Sanity errors` is not related to the part of code I changed. Is there anything I need to do (e.g. rebase to latest master)?   Or simply re-run the test again is enough?\r\n\r\n```\r\nFAIL: Found 10 errors\r\ntensorflow/python/lib/io/file_io_test.py:673: [E1123(unexpected-keyword-arg), FileIoTest.testFileSeekableWithZip] Unexpected keyword argument 'allow_pickle' in function call\r\ntensorflow/python/keras/datasets/mnist.py:76: [E1123(unexpected-keyword-arg), load_data] Unexpected keyword argument 'allow_pickle' in function call\r\ntensorflow/python/keras/datasets/reuters.py:114: [E1123(unexpected-keyword-arg), load_data] Unexpected keyword argument 'allow_pickle' in function call\r\ntensorflow/python/keras/datasets/boston_housing.py:62: [E1123(unexpected-keyword-arg), load_data] Unexpected keyword argument 'allow_pickle' in function call\r\ntensorflow/python/keras/datasets/imdb.py:108: [E1123(unexpected-keyword-arg), load_data] Unexpected keyword argument 'allow_pickle' in function call\r\ntensorflow/python/tools/saved_model_cli.py:663: [E1123(unexpected-keyword-arg), load_inputs_from_input_arg_string] Unexpected keyword argument 'allow_pickle' in function call\r\ntensorflow/python/training/tracking/python_state_test.py:131: [E1123(unexpected-keyword-arg), _NumpyWrapper.deserialize] Unexpected keyword argument 'allow_pickle' in function call\r\ntensorflow/python/kernel_tests/random/multinomial_op_big_test.py:44: [E1123(unexpected-keyword-arg), MultinomialTest.testLargeDynamicRange] Unexpected keyword argument 'return_counts' in function call\r\ntensorflow/python/kernel_tests/random/multinomial_op_big_test.py:62: [E1123(unexpected-keyword-arg), MultinomialTest.testLargeDynamicRange2] Unexpected keyword argument 'return_counts' in function call\r\ntensorflow/python/kernel_tests/random/multinomial_op_big_test.py:85: [E1123(unexpected-keyword-arg), MultinomialTest.testLargeDynamicRange3] Unexpected keyword argument 'return_counts' in function call\r\n```", "> @gbaned Hi, It seems that the error in Ubuntu Sanity errors is not related to the part of code I changed. Is there anything I need to do (e.g. rebase to latest master)? Or simply re-run the test again is enough?\r\n\r\nIt happens, either way is fine. Ignoring is fine as well: we re-run all of this when we try to integrate internally and we are used to re-trigger these builds when they fail for unrelated reasons.", "@wyzero  Can you please address Ubuntu Sanity errors? Thanks!", "> @wyzero Can you please address Ubuntu Sanity errors? Thanks!\r\n\r\nAs I said above, It seems that the error is not related to the part of code I changed. I will try to rebase to the latest master to see if this can work.\r\n\r\n```\r\nFAIL: Found 10 errors\r\ntensorflow/python/lib/io/file_io_test.py:673: [E1123(unexpected-keyword-arg), FileIoTest.testFileSeekableWithZip] Unexpected keyword argument 'allow_pickle' in function call\r\ntensorflow/python/keras/datasets/mnist.py:76: [E1123(unexpected-keyword-arg), load_data] Unexpected keyword argument 'allow_pickle' in function call\r\ntensorflow/python/keras/datasets/reuters.py:114: [E1123(unexpected-keyword-arg), load_data] Unexpected keyword argument 'allow_pickle' in function call\r\ntensorflow/python/keras/datasets/boston_housing.py:62: [E1123(unexpected-keyword-arg), load_data] Unexpected keyword argument 'allow_pickle' in function call\r\ntensorflow/python/keras/datasets/imdb.py:108: [E1123(unexpected-keyword-arg), load_data] Unexpected keyword argument 'allow_pickle' in function call\r\ntensorflow/python/tools/saved_model_cli.py:663: [E1123(unexpected-keyword-arg), load_inputs_from_input_arg_string] Unexpected keyword argument 'allow_pickle' in function call\r\ntensorflow/python/training/tracking/python_state_test.py:131: [E1123(unexpected-keyword-arg), _NumpyWrapper.deserialize] Unexpected keyword argument 'allow_pickle' in function call\r\ntensorflow/python/kernel_tests/random/multinomial_op_big_test.py:44: [E1123(unexpected-keyword-arg), MultinomialTest.testLargeDynamicRange] Unexpected keyword argument 'return_counts' in function call\r\ntensorflow/python/kernel_tests/random/multinomial_op_big_test.py:62: [E1123(unexpected-keyword-arg), MultinomialTest.testLargeDynamicRange2] Unexpected keyword argument 'return_counts' in function call\r\ntensorflow/python/kernel_tests/random/multinomial_op_big_test.py:85: [E1123(unexpected-keyword-arg), MultinomialTest.testLargeDynamicRange3] Unexpected keyword argument 'return_counts' in function call\r\n```", "@gbaned Could you test again?", "@gbaned  It's still the same error. It seems that pylint check failed. However I don't change any python files at all. Is anyone else meet the same problem? \r\n\r\n```\r\n==== Summary of sanity check results ====\r\n1. do_configure_test: Run ./configure\r\n  PASS\r\n2. do_pylint: Python 3 pylint\r\n  FAIL\r\n3. do_buildifier: buildifier check\r\n  PASS\r\n4. do_bazel_nobuild: bazel nobuild\r\n  PASS\r\n5. do_bazel_deps_query: bazel query\r\n  PASS\r\n6. do_pip_package_licenses_check: pip: license check for external dependencies\r\n  PASS\r\n7. do_lib_package_licenses_check: C library: license check for external dependencies\r\n  PASS\r\n8. do_java_package_licenses_check: Java Native Library: license check for external dependencies\r\n  PASS\r\n9. do_pip_smoke_test: Pip Smoke Test: Checking py_test dependencies exist in pip package\r\n  PASS\r\n10. do_check_load_py_test: Check load py_test: Check that BUILD files with py_test target properly load py_test\r\n  PASS\r\n11. do_code_link_check: Code Link Check: Check there are no broken links\r\n  PASS\r\n12. do_check_file_name_test: Check file names for cases\r\n  PASS\r\n13. do_pip_no_cuda_deps_check_ubuntu: Check Ubuntu gpu pip package does not depend on cuda shared libraries\r\n  PASS\r\n14. do_pip_no_cuda_deps_check_windows: Check Windows gpu pip package does not depend on cuda shared libraries\r\n  PASS\r\n```", "@gbaned @joker-eph @sanjoy Could you trigger the test again?", "@joker-eph @sanjoy Hi, Could you approve this PR again?", "> @joker-eph @sanjoy Hi, Could you approve this PR again?\r\n\r\nIt does not need a re-approval, it is in the integration pipeline since the original approvals, it didn't go through by itself because the BUILD file is missing some dependencies that I had to fix manually (I don't know why Bazel is OK with this but it failed internally)", "@joker-eph Thanks! Is there anything I can do to accelerate this process? It would be nice if we can finish this PR quickly since we still have a lot of new PRs in the queue.", "> @joker-eph Thanks! Is there anything I can do to accelerate this process? It would be nice if we can finish this PR quickly since we still have a lot of new PRs in the queue.\r\n\r\nNo: I had to rebase multiple times to work around issues with the various bots...\r\n(also Google was closed last Friday, and Monday was a holiday in the US, many people to a day or day two extra to make it almost a week off!)"]}, {"number": 49597, "title": " Due to the upgrade tensorflow2.5.0,I cannot load weights which tf2.4.0 canload,maybe the reson is  the h5py from 2.10.0 to 3.1.0,I want to use tf2.5.0 to load the  pretrain model,how can I do?", "body": "ValueError: in user code:\r\n\r\n    E:\\git_projects\\segmentation\\deeplabv31\\subpixel.py:91 call  *\r\n        return self._phase_shift(super(Subpixel, self).call(inputs))\r\n    D:\\software\\soft\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py:273 call  **\r\n        outputs.set_shape(out_shape)\r\n    D:\\software\\soft\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:777 set_shape\r\n        raise ValueError(str(e))\r\n\r\n    ValueError: Dimension 1 in both shapes must be equal, but are 150 and 1200. Shapes are [?,150,150,128] and [?,1200,1200,2].\r\n", "comments": ["@successAI ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.\r\n\r\nAlso please take a look at the links with similar error.[link1](https://stackoverflow.com/questions/41835789/tensorflow-valueerror-dimension-0-in-both-shapes-must-be-equal),[link2](https://stackoverflow.com/questions/41800643/valueerror-dimensions-must-be-equal-but-are-1-and-3-for-conv2d-op-conv2d/41880215),[link3](https://github.com/matterport/Mask_RCNN/issues/604).It helps.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49597\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49597\">No</a>\n"]}, {"number": 49596, "title": "Fix multiple issues in EditDistance", "body": "PiperOrigin-RevId: 372033948\nChange-Id: Ieb957c29894af05bdfeb1a0402fced808dfcfd7b", "comments": []}, {"number": 49595, "title": "Fix multiple issues in EditDistance", "body": "PiperOrigin-RevId: 372033948\nChange-Id: Ieb957c29894af05bdfeb1a0402fced808dfcfd7b", "comments": []}, {"number": 49594, "title": "Fix multiple issues in EditDistance", "body": "PiperOrigin-RevId: 372033948\nChange-Id: Ieb957c29894af05bdfeb1a0402fced808dfcfd7b", "comments": []}, {"number": 49593, "title": "Fix multiple issues in EditDistance", "body": "PiperOrigin-RevId: 372033948\nChange-Id: Ieb957c29894af05bdfeb1a0402fced808dfcfd7b", "comments": []}, {"number": 49592, "title": "Prevent check fail in FFT", "body": "PiperOrigin-RevId: 372031044\nChange-Id: I50994e3e8a5d1342d01bde80256f6bf2730ca299", "comments": []}, {"number": 49591, "title": "Prevent check fail in FFT", "body": "PiperOrigin-RevId: 372031044\nChange-Id: I50994e3e8a5d1342d01bde80256f6bf2730ca299", "comments": []}, {"number": 49590, "title": "Prevent check fail in FFT", "body": "PiperOrigin-RevId: 372031044\nChange-Id: I50994e3e8a5d1342d01bde80256f6bf2730ca299", "comments": []}, {"number": 49589, "title": "Prevent check fail in FFT", "body": "PiperOrigin-RevId: 372031044\nChange-Id: I50994e3e8a5d1342d01bde80256f6bf2730ca299", "comments": []}, {"number": 49588, "title": "Fix a check fail in Fast Fourier implementation", "body": "PiperOrigin-RevId: 372026629\nChange-Id: Id05c3362aa575271bc3e06b16316c9037085fc11", "comments": []}, {"number": 49587, "title": "Fix a check fail in Fast Fourier implementation", "body": "PiperOrigin-RevId: 372026629\nChange-Id: Id05c3362aa575271bc3e06b16316c9037085fc11", "comments": []}, {"number": 49586, "title": "Fix a check fail in Fast Fourier implementation", "body": "PiperOrigin-RevId: 372026629\nChange-Id: Id05c3362aa575271bc3e06b16316c9037085fc11", "comments": []}, {"number": 49585, "title": "Fix a check fail in Fast Fourier implementation", "body": "PiperOrigin-RevId: 372026629\nChange-Id: Id05c3362aa575271bc3e06b16316c9037085fc11", "comments": []}, {"number": 49584, "title": "Fix a check fail", "body": "PiperOrigin-RevId: 372011072\nChange-Id: I1062cfaed0aa16884e9a16312483794d188db76f", "comments": []}]