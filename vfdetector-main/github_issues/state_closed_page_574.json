[{"number": 36466, "title": "Go: fix handling empty tags-set for loading saved model", "body": "This PR fixes a crash in `LoadSavedModel` for empty tag-set.\r\n\r\nThe panic itself (`panic: runtime error: index out of range`) comes from https://github.com/tensorflow/tensorflow/blob/25251f057c476df16451599aa34a6283b9e45209/tensorflow/go/saved_model.go#L69\r\n", "comments": ["Is there anything I can do to help to get the PR reviewed/merged?", "Sorry, slipped off my radar somehow.", "No problem and thanks for the help \ud83d\ude04"]}, {"number": 36464, "title": "If-Condition with TF-Bools looses track of namescopes", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (see below)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tested on tf-gpu v2.0.0 and tf v2.1.0 (CPU as I do not have CUDA 10.1 running)\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.4.2\r\n- GPU model and memory: tested on Nvidia Quadro P2000\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen inside a function with graph mode activated, using an `if` statement with a tensorflow bool leads possibly to a new namescope. I assume this, because the observation one can make is that `tf.summary`s inside this if condition are placed inside different name scopes depending on whether the condition is a python bool (so static w.r.t. to the tensorflow graph) or a tensorflow tensor (resulting in a dynamic `tf.cond` through autograph). This happens for both v2.0 and v2.1, afaict. Of course, in eager mode this problem is non-existent.\r\n\r\n**Describe the expected behavior**\r\nThe final summary scope should be independent of whether a autographed condition was used or a statically compiled one. Attached is a minimal working example producing a tensorboard output show in the image. The two lines are produced by the same `tf.summary.scalar` line inside the `tf.name_scope('monitoring')`. In this case it would be easy enough to move the name_scope creation inside the if-scope, however my use cases do not allow this kind of workaround (summaries are written inside keras layer calls which add their own respective namescope).\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport os.path as osp\r\n\r\nimport tensorflow as tf\r\n\r\n\r\n@tf.function\r\ndef func_with_summaries(step, write_summary):\r\n    with tf.name_scope(\"monitoring\"):\r\n        if write_summary:\r\n            tf.summary.scalar(\"step\", step, tf.cast(step, dtype=tf.int64))\r\n\r\n\r\nbrok_writer = tf.summary.create_file_writer(\r\n    osp.join(\"/tmp\", \"summaries_conds_scopes\", \"broken\")\r\n)\r\nwriter = tf.summary.create_file_writer(\r\n    osp.join(\"/tmp\", \"summaries_conds_scopes\", \"correct\")\r\n)\r\nst = tf.Variable(0.0, name=\"step\", dtype=tf.float32)\r\nfor i in range(10):\r\n    st.assign(i)\r\n    with writer.as_default():\r\n        func_with_summaries(st, i % 3 == 0)\r\n    with brok_writer.as_default():\r\n        func_with_summaries(st, tf.constant(i) % 3 == 0)\r\n```\r\n**Other info / logs**\r\n![image](https://user-images.githubusercontent.com/6333870/73756641-3172cf00-4768-11ea-9ec8-31f003c0c049.png)\r\n", "comments": ["I just found a workaround, but it is extremely hacky and I hope the underlying issue will still be fixed in the future.\r\n\r\n```diff\r\n+with tf.name_scope(\"\") as root_scope:\r\n+    pass\r\n\r\n\r\n @tf.function\r\n def func_with_summaries(step, write_summary):\r\n-    with tf.name_scope(\"monitoring\"):\r\n+    with tf.name_scope(\"monitoring\") as scope:\r\n         if write_summary:\r\n+            with tf.name_scope(root_scope):\r\n+                with tf.name_scope(scope):\r\n                     tf.summary.scalar(\"step\", step, tf.cast(step, dtype=tf.int64))\r\n```", "@demmerichs Looks like this was resolved. Can you please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/2d3f8f5ea449a7032f265e08b67b5929/untitled.ipynb). I ran your code with `TF2.4rc3`. \r\n\r\nCan you please verify and close the issue if this was already resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36464\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36464\">No</a>\n"]}, {"number": 36463, "title": "tf.keras.Sequential ignores the outer name scope(s) when built proactively", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.2 (19C57)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.5 (v3.6.5:f59c0932b4)\r\n\r\n**Describe the current behavior**\r\n\r\nWhen the `build` method of a `tf.keras.Sequential` instance is (explicitly) called within one (or several nested) `tf.name_scope` context manager(s), the variable (weight) names of the layers wrapped into the `Sequential` don't get the respective name scope prefixes.\r\n\r\n**Describe the expected behavior**\r\n\r\nAfter `build`ing a `Sequential`, all the `tf.name_scope`s surrounding the `build` call must be reflected as prefixes of the `Sequential`'s variable (weight) names. This is what currently happens in `tensorflow==1.15.2`.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nseq = tf.keras.Sequential(layers=[\r\n    tf.keras.layers.Dense(units=10, name=\"d1\"),\r\n    tf.keras.layers.Dense(units=20, name=\"d2\"),\r\n])\r\n\r\nwith tf.name_scope(\"a\"):\r\n    with tf.name_scope(\"b\"):\r\n        seq.build(input_shape=[32, 784])\r\n\r\nfor w in seq.weights:\r\n    print(w.name)\r\n```\r\n\r\nthe output with `tensorflow==2.1.0`:\r\n\r\n```\r\nd1/kernel:0\r\nd1/bias:0\r\nd2/kernel:0\r\nd2/bias:0\r\n```\r\n\r\nthe output with `tensorflow==1.15.2`:\r\n\r\n```\r\na/b/d1/kernel:0\r\na/b/d1/bias:0\r\na/b/d2/kernel:0\r\na/b/d2/bias:0\r\n```", "comments": ["I have tried on colab with TF version 2.1.0-rc2,2.2.0-dev20200204 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/de7f77d612ed52c22e0eb969c126e08c/untitled620.ipynb). Thanks!", "If you enable eager execution in TF 1.15.2 you will see that it prints same output at TF 2.1\r\n```python\r\nd1/kernel:0\r\nd1/bias:0\r\nd2/kernel:0\r\nd2/bias:0\r\n```\r\nReason being eager execution is enabled by default in TF 2.X.\r\nScope name is added if not executing eagerly.", "@ymodak Thank you for the answer.\r\n\r\nI can confirm what you've written. I can also add that, even in TF 2.1, if the `build` call nested in a pair of `tf.name_scope` context managers happens within a `@tf.function`-decorated function,\r\n\r\n```python\r\n@tf.function\r\ndef func():\r\n    with tf.name_scope(\"a\"):\r\n        with tf.name_scope(\"b\"):\r\n            seq.build(input_shape=[32, 784])\r\n\r\nfunc()\r\n\r\nfor w in seq.weights:\r\n    print(w.name)\r\n```\r\n\r\n the names defined by the `tf.name_scope`s show up in the resulting variable (weight) names:\r\n\r\n```\r\na/b/d1/kernel:0\r\na/b/d1/bias:0\r\na/b/d2/kernel:0\r\na/b/d2/bias:0\r\n```\r\n\r\nOn the other hand, I cannot agree that it is normal that `Sequential` is ignoring the `tf.name_scope`s in the eager mode. For two reasons:\r\n\r\n1. The [documentation](https://www.tensorflow.org/api_docs/python/tf/name_scope) of `tf.name_scope` doesn't say that it doesn't work in the eager mode.\r\n\r\n2. More importantly, an instance of `tf.keras.layers.Dense` built by explicitly calling its `built` method within one or more `tf.name_scope` context managers respects the outer name scopes. So the same code running in the eager mode, but with a `Dense` instead of a `Sequential`:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nd = tf.keras.layers.Dense(units=10, name=\"d1\")\r\n\r\nwith tf.name_scope(\"a\"):\r\n    with tf.name_scope(\"b\"):\r\n        d.build(input_shape=[32, 784])\r\n\r\nfor w in d.weights:\r\n    print(w.name)\r\n```\r\n\r\nproduces the following output:\r\n\r\n```\r\na/b/kernel:0\r\na/b/bias:0\r\n```\r\n\r\nIt seems that the outer name scope-related behaviours of `Sequential` and `Dense` are not consistent in the eager mode. This makes me thinking that this is a problem of `Sequential` that may still be isolated and fixed.", "@jaingaurav any update on this one?", "Was able to reproduce issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/9410c27b705aef776028261d057c54ce/35650.ipynb). Thanks!", "Able to reproduce the issue in TF nightly `2.7.0-dev20210923`. Please find the [gist here](https://colab.research.google.com/gist/sanatmpa1/4c8e40108fa4ff4c91ca38e38c8a7afe/35650.ipynb).", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36463\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36463\">No</a>\n"]}, {"number": 36462, "title": "Autograph is incompatible with typeguard", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no gpu\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom typeguard import typechecked\r\n@tf.function(autograph=True)\r\n@typechecked\r\ndef add(a: tf.Tensor, b: tf.Tensor) -> tf.Tensor:\r\n    return a + b\r\n\r\nprint(add(tf.ones(2), tf.zeros(2)))\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n<ipython-input-5-5567d1a6d381> in <module>()\r\n      4     return a + b\r\n      5 \r\n----> 6 print(add(tf.ones(2), tf.zeros(2)))\r\n\r\n8 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    613       # This is the first call of __call__, so we have to initialize.\r\n    614       initializers = []\r\n--> 615       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    616     finally:\r\n    617       # At this point we know that the initialization is complete (or less\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    495     self._concrete_stateful_fn = (\r\n    496         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 497             *args, **kwds))\r\n    498 \r\n    499     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2387       args, kwargs = None, None\r\n   2388     with self._lock:\r\n-> 2389       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2390     return graph_function\r\n   2391 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2701 \r\n   2702       self._function_cache.missed.add(call_context_key)\r\n-> 2703       graph_function = self._create_graph_function(args, kwargs)\r\n   2704       self._function_cache.primary[cache_key] = graph_function\r\n   2705       return graph_function, args, kwargs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2591             arg_names=arg_names,\r\n   2592             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2593             capture_by_value=self._capture_by_value),\r\n   2594         self._function_attributes,\r\n   2595         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    976                                           converted_func)\r\n    977 \r\n--> 978       func_outputs = python_func(*func_args, **func_kwargs)\r\n    979 \r\n    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    437         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    438         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    440     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    441 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nNameError: in converted code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/typeguard/__init__.py:3 wrapper  *\r\n        check_argument_types(memo)\r\n    /usr/local/lib/python3.6/dist-packages/typeguard/__init__.py:663 check_argument_types  *\r\n        for argname, expected_type in memo.type_hints.items():\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/operators/control_flow.py:339 for_stmt\r\n        return _py_for_stmt(iter_, extra_test, body, get_state, set_state, init_vars)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/operators/control_flow.py:348 _py_for_stmt\r\n        if extra_test is not None and not extra_test(*state):\r\n    /tmp/tmpnaoua4_l.py:96 extra_test\r\n        return ag__.not_(do_return)\r\n\r\n    NameError: free variable 'do_return' referenced before assignment in enclosing scope\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n```\r\ntf.Tensor([1. 1.], shape=(2,), dtype=float32)\r\n```\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom typeguard import typechecked\r\n\r\n@tf.function(autograph=True)\r\n@typechecked\r\ndef add(a: tf.Tensor, b: tf.Tensor) -> tf.Tensor:\r\n    return a + b\r\n\r\nprint(add(tf.ones(2), tf.zeros(2)))\r\n```\r\n\r\nSee the colab notebook: https://colab.research.google.com/drive/1RTg-ysyIdKME4fbJQ1D5Dxs-11YfBV5e\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@gabrieldemarmiesse \r\n\r\nCan you try running the code in latest -tf-nightly ( `!pip install tf-nightly`). The issue seemed to be fixed, kindly find the [gist of colab](https://colab.sandbox.google.com/gist/ravikyram/3f87aa19ac8da80ccb090f842407294a/untitled5.ipynb) for the same.Thanks!\r\n", "@ravikyram \r\n\r\nThis issue does not seem to be fixed with the current nightly 2.2.0-dev20200218. I get the same error as before. I tested locally and in google colab (your notebook) with tf-nightly and the error is the same. \r\n\r\nSee my minimal notebook with tensorflow uninstalled and tf-nightly installed instead:\r\nhttps://colab.research.google.com/drive/1qkIblkiFhd-jPmC6Ki4F7xemPorRbf_-\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36462\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36462\">No</a>\n", "I'm not sure if the PR fixed it, but we can check with the next nightly.", "I checked the repro colab with the new tf-nightly, and it appears that the fix worked.", "I confirm! thanks it's awesome!"]}, {"number": 36461, "title": "Missning signaturedef while converting model from .h5 to tensorflow saved format model ", "body": "hi,i am saving the finetuned model as :\r\nmodel.save_pretrained('directory')\r\nIt saves a config.json and .h5 model in the directory.\r\nBut,when i load the same model using:\r\nloaded_model = TFDistilBertModel.from_pretrained(\"directory\")\r\nand then,save it in tf format using:\r\ntf.saved_model.save(loaded_model, \"./tmp/db/1\")\r\nThis tfsaved format model conatins:variables,assets and saved_model.pb file.But,it doesn't contain a signature_def so that I can serve the model using tf_serving.\r\nPlease help,i am totally stuck", "comments": ["@divyag11,\r\nTo see signature_def run this command,\r\n`saved_model_cli show --dir {export_path} --all`\r\nThanks!", "I know the command to see signature_def .My ISSUE is :\r\nwhen i run: saved_model_cli show --dir {export_path} --all\r\nit shows as shown below ,but does not have any i/p or o/p signature.Is there any way t explicitly pass signature to model in tf.saved_model.save.LET me know about it how to pass signatures explicitly\r\n```MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['__saved_model_init_op']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['__saved_model_init_op'] tensor_info:\r\n        dtype: DT_INVALID\r\n        shape: unknown_rank\r\n        name: NoOp\r\n  Method name is:", "@divyag11 What version of tensorflow are you using. Also can you please provide a minimal reproducible case for us to reproduce the issue. Thanks!", "i am using tf 2.1.\r\nI just want you to tell how to explicitly set signature for tensofrlow format:\r\neg:\r\ntf.save_model.save(model_name,directory_name,signatures = )\r\nhow to set the signatures in above format?", "@divyag11 You can define your own signatures in the form of a dictionary as shown [here](https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/saved_model/save.py#L448)", "@divyag11 I am gonna close this issue as its not related to bug/performance, build/install, feature request related issue. If you have more questions please post it in stack overflow as there is a bigger community to respond. "]}, {"number": 36460, "title": "saving trained tensor flow data on google colab", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): stock example\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac catalina\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version: python 3.0 on colab\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: colab\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI ran a walkthrough using python notebook 3 on google colab and every time I run the model, I have to train with the data and then run the prediction. but I'm curious is there anyway to save your data on google colab, so I don't have to  train the model every time when I need to do the prediction.\r\n**Describe the expected behavior**\r\nI want it to save my trained data.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@power9799, Save your model and load when ever you want to evaluate or predict. Save the model in .h5 or .pb format in your google drive. Please take a look at the [guide](https://www.tensorflow.org/tutorials/keras/save_and_load) to Save and load model in Tensorflow. Thanks!", "oh \r\n\r\n> @power9799, Save your model and load when ever you want to evaluate or predict. Save the model in .h5 or .pb format in your google drive. Please take a look at the [guide](https://www.tensorflow.org/tutorials/keras/save_and_load) to Save and load model in Tensorflow. Thanks!\r\n", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@power9799, Closing this issue since it is resolved. Please feel free to open if still issue persists. Thanks"]}, {"number": 36459, "title": "High RAM Usage for TF Runtime?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 LTS\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tensorflow-gpu==2.0.0 or tensorflow==2.1.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: CUDA Toolkit 10.1.243 / cuDNN 7.6.5.32\r\n- GPU model and memory: 8 x GeForce RTX 2080 Ti, 11019MiB\r\n- System memory: 32GB\r\n\r\n**Describe the current behavior**\r\n\r\nEven the smallest 'computation' leads to very high RAM usages of the system memory (not GPU memory). As shown in the following, a simple single-float-Variable initialization leads to more than 2GB RAM increase. The fewer graphics cards are visible for tensorflow, the less RAM is used after all. \r\n\r\n**Describe the expected behavior**\r\n\r\nThe expected behavior is way less memory usage and also a constant amount, independently of the number of GPUs visible to tensorflow. More than 2GB per instance is not viable for my (/our) situation, since multiple users are working on the machine. \r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport os, psutil\r\np = psutil.Process(os.getpid())\r\n\r\nprint(\"Memory Usage (before import):\", p.memory_info().rss/1024/1024, \"MB\")\r\n\r\nimport tensorflow as tf\r\nnum_visible_gpus = 8 # OPTIONAL\r\ngpu_devs = tf.config.experimental.list_physical_devices(\"GPU\") # OPTIONAL\r\ntf.config.experimental.set_visible_devices(gpu_devs[:num_visible_gpus], \"GPU\") # OPTIONAL\r\n\r\nprint(\"Memory Usage (after  import):\", p.memory_info().rss/1024/1024, \"MB\")\r\n\r\ntf.Variable(42.0) # Do some pseudowork...\r\n\r\nprint(\"Memory Usage (after var-def):\", p.memory_info().rss/1024/1024, \"MB\")\r\n```\r\n\r\nOutput:\r\n> Memory Usage (before import): 47.62109375 MB\r\n> Memory Usage (after  import): 340.15625 MB\r\n> Memory Usage (after var-def): 2485.1796875 MB\r\n\r\nFewer GPUs result in less memory usage (after var-def):\r\n\r\n> num_visible_gpus=8 => 2626.039063 MB\r\n> num_visible_gpus=7 => 2488.765626 MB\r\n> num_visible_gpus=6 => 2267.289063 MB\r\n> num_visible_gpus=5 => 2173.917968 MB\r\n> num_visible_gpus=4 => 2011.917968 MB\r\n> num_visible_gpus=3 => 1809.957031 MB\r\n> num_visible_gpus=2 => 1681.316406 MB\r\n> num_visible_gpus=1 => 1539.554688 MB\r\n> num_visible_gpus=0 => 1201.152343 MB\r\n\r\nThese results were obtained in a jupyter-notebook environment. When run as a py-file, the usages are slightly (by ~200MB) lower (e.g. num_visible_gpus=8 => 2347.7148437 MB). \r\n", "comments": ["I tried on colab, it took around 1GB of RAM with Tf 2.1 and 2.0.\r\nPlease take a look at the gist [here](https://colab.research.google.com/gist/gadagashwini/c0fb1823be9b1feb7bcd78705a20b5ba/untitled375.ipynb). Thanks!", "This is because Colab does not have the same setup as I have. Colab only utilizes a single GPU:\r\n`tf.config.experimental.list_physical_devices(\"GPU\")`\r\noutputs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n\r\nThis aggrees somehow with my observations when I turn only one of the eight GPUs visible (num_visible_gpus=1 => 1539.554688 MB) or zero (num_visible_gpus=0 => 1201.152343 MB). \r\n\r\nSo does this mean, that Tensorflow simply aquires this much of memory in such a minimal scenario?\r\nAlso: Is it not possible to lower the usage while more GPUs are visible?", "@yetanotheryeti Please go through this [doc ](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and let me know if it helps. Thanks!", "Sorry, this does not help, since it is not the GPU-memory, but the system memory, which is the bottleneck for us. \r\n\r\nNevertheless, I tried setting the memory growth to true and it surprisingly lowers the usage a little bit  (about 200 MB). Still, ~2.4 GB usage is too much for our usecase. \r\n\r\nWhat I'm interested in is if this is reproducable for other systems with more than one GPU. \r\nIf the answer to this problem is simply: \"Well tensorflow simply needs to allocate this much memory, because ..., so you can not do anything about it.\" then I'd be happy. \r\nHowever, I am still unsure if the high usage is a \"bug\" or a \"mishandling\" on my side or simply how it is. ", "@sanjoy Can you PTAL? Thanks!", "@sanjoy \r\nHello, are there any updates? This is a somewhat blocking issue for our team.\r\nThank you!", "I believe RSS includes all of the CUDA libraries TensorFlow loads which could explain the large memory footprint you're seeing.  Given that, I would expect multiple TF gpu processes to share the actual physical memory for these loaded shared objects.", "please update if possible, facing the same issue. too much CPU memory used for pinned memory", "I think this is working as intended (so I'm closing the issue).  The CUDA libraries are provided by NVIDIA and we can't do much to reduce their size.  On the other hand, I would expect RSS to be shared with other processes so the motivating concern\r\n\r\n> More than 2GB per instance is not viable for my (/our) situation, since multiple users are working on the machine.\r\n\r\ndoes not apply.\r\n\r\n@yetanotheryeti, please reopen the issue if you disagree.\r\n\r\n> too much CPU memory used for pinned memory\r\n\r\n@billythegod I believe you have a different issue, this issue is not about pinned host memory.  Please open a separate issue with a reproducer.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36459\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36459\">No</a>\n", "> On the other hand, I would expect RSS to be shared with other processes\r\n\r\nTo clarify: *each* process has the described RSS-footprint (such as ~2GB), so it is not shared, even among processes of the same user. Sorry, I did not clearly state this before.\r\n\r\n> The CUDA libraries are provided by NVIDIA and we can't do much to reduce their size.\r\n\r\nOk, this is what I fairly expected (similar to as I stated before: \"Well tensorflow (or CUDA) simply needs to allocate this much memory, because ..., so you can not do anything about it.\"). \r\n\r\nThis leaves us with either figuring out if and how the CUDA libraries can be shared between processes, or finding another strategy for our team. ", "> To clarify: each process has the described RSS-footprint (such as ~2GB), so it is not shared, even among processes of the same user. Sorry, I did not clearly state this before.\r\n\r\nAre you sure this ~2GB number is not misleading?  I believe that a page loaded in memory will be accounted in the RSS count for all processes that have that page in their page table.  So the sum of RSS footprints for all running processes can exceed the total amount of RAM in the system.", "> Are you sure this ~2GB number is not misleading?\r\n\r\nI am quite sure. We had multiple OOMs, leading to process kills by the OOM killer or even system crashes. Also, according to (h)top the total system memory consumption is rising per process as well:\r\n\r\nFour python instances with tensorflow loaded. Each instance has its own GPU assigned via 'set_visible_devices':\r\n<img src=\"https://user-images.githubusercontent.com/5852533/75334278-d4aa9600-5887-11ea-931d-1750e4f6b722.png\" alt=\"drawing\" width=\"500\"/>\r\n\r\nAfter initializing 'tf.Variable(42.)' for each instance: \r\n<img src=\"https://user-images.githubusercontent.com/5852533/75334299-df652b00-5887-11ea-85ba-6aa928819b45.png\" alt=\"drawing\" width=\"500\"/>", "I have experienced the same observation and have been wondering why the rss is higher when using tensorflow working with the GPU vs CPU only. For a same running program, I have observed these rss percentages (on a 16GB ram computer):\r\n\r\n- CPU only : 6.7%\r\n- GPU : 13,7%\r\n- GPU (and allow_growth set to True) : 11.6%\r\n", "https://github.com/tensorflow/tensorflow/issues/31312#issuecomment-691583440", "run 6 small models(.pb about 44MB ), inference with 6 multiprocessing in python3.7 and tensorflow1.13.1.\r\nIn windows, use 4G memory, but in Ubuntu use 12G memory, don't know why.", "> run 6 small models(.pb about 44MB ), inference with 6 multiprocessing in python3.7 and tensorflow1.13.1.\r\n> In windows, use 4G memory, but in Ubuntu use 12G memory, don't know why.\r\n\r\nI noticed the same behavior in linux vs windows", "It seems like whatever computation we're doing, the RAM (and CPU) overhead are used for maintaining the GPUs to run, so it seems like for small models we need to deliberately turn off some GPU to prevent excess memory usage. If that's not the case, I also noticed in my case that using tf.Dataset API .cache() for large dataset will just cache all the data and run out of memory eventually so I disable .cache() and everything works fine for me. ", "> **System information**\r\n> \r\n> * Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 LTS\r\n> * TensorFlow installed from (source or binary): pip\r\n> * TensorFlow version (use command below): tensorflow-gpu==2.0.0 or tensorflow==2.1.0\r\n> * Python version: 3.6.8\r\n> * CUDA/cuDNN version: CUDA Toolkit 10.1.243 / cuDNN 7.6.5.32\r\n> * GPU model and memory: 8 x GeForce RTX 2080 Ti, 11019MiB\r\n> * System memory: 32GB\r\n> \r\n> **Describe the current behavior**\r\n> \r\n> Even the smallest 'computation' leads to very high RAM usages of the system memory (not GPU memory). As shown in the following, a simple single-float-Variable initialization leads to more than 2GB RAM increase. The fewer graphics cards are visible for tensorflow, the less RAM is used after all.\r\n> \r\n> **Describe the expected behavior**\r\n> \r\n> The expected behavior is way less memory usage and also a constant amount, independently of the number of GPUs visible to tensorflow. More than 2GB per instance is not viable for my (/our) situation, since multiple users are working on the machine.\r\n> \r\n> **Code to reproduce the issue**\r\n> \r\n> ```\r\n> import os, psutil\r\n> p = psutil.Process(os.getpid())\r\n> \r\n> print(\"Memory Usage (before import):\", p.memory_info().rss/1024/1024, \"MB\")\r\n> \r\n> import tensorflow as tf\r\n> num_visible_gpus = 8 # OPTIONAL\r\n> gpu_devs = tf.config.experimental.list_physical_devices(\"GPU\") # OPTIONAL\r\n> tf.config.experimental.set_visible_devices(gpu_devs[:num_visible_gpus], \"GPU\") # OPTIONAL\r\n> \r\n> print(\"Memory Usage (after  import):\", p.memory_info().rss/1024/1024, \"MB\")\r\n> \r\n> tf.Variable(42.0) # Do some pseudowork...\r\n> \r\n> print(\"Memory Usage (after var-def):\", p.memory_info().rss/1024/1024, \"MB\")\r\n> ```\r\n> \r\n> Output:\r\n> \r\n> > Memory Usage (before import): 47.62109375 MB\r\n> > Memory Usage (after  import): 340.15625 MB\r\n> > Memory Usage (after var-def): 2485.1796875 MB\r\n> \r\n> Lesser GPUs result in less memory usage (after var-def):\r\n> \r\n> > num_visible_gpus=8 => 2626.039063 MB\r\n> > num_visible_gpus=7 => 2488.765626 MB\r\n> > num_visible_gpus=6 => 2267.289063 MB\r\n> > num_visible_gpus=5 => 2173.917968 MB\r\n> > num_visible_gpus=4 => 2011.917968 MB\r\n> > num_visible_gpus=3 => 1809.957031 MB\r\n> > num_visible_gpus=2 => 1681.316406 MB\r\n> > num_visible_gpus=1 => 1539.554688 MB\r\n> > num_visible_gpus=0 => 1201.152343 MB\r\n> \r\n> These results were obtained in a jupyter-notebook environment. When run as a py-file, the usages are slightly (by ~200MB) lower (e.g. num_visible_gpus=8 => 2347.7148437 MB).\r\n\r\nHi yetanotheryeti, I have ran into a similar situation where the CPU Memory (RAM) spikes when tf gpu is imported and it occupies almost around 70% of the total memory. I tested with tf-cpu and the memory usage is around 5%. Did you get a fix for this?", "I'm in the same boat. Training my models on Windows+CPU is no problem and takes 13 GB RAM. On Linux+GPU it's easily above my installed 32 GB (probably aiming for ~37 GB as the factor between the two systems stays around x2.8 with smaller datasets) and gets terminated (oom, exit code 139 (interrupted by signal 11: SIGSEGV)).\r\nI've tried most of the ideas from [#31312](https://github.com/tensorflow/tensorflow/issues/31312) but the problem still persists - with or without validation data. Both systems are on TF 2.1, same code and files.", "> \r\n> \r\n> I'm in the same boat. Training my models on Windows+CPU is no problem and takes 13 GB RAM. On Linux+GPU it's easily above my installed 32 GB (probably aiming for ~37 GB as the factor between the two systems stays around x2.8 with smaller datasets) and gets terminated (oom, exit code 139 (interrupted by signal 11: SIGSEGV)).\r\n> I've tried most of the ideas from [#31312](https://github.com/tensorflow/tensorflow/issues/31312) but the problem still persists - with or without validation data. Both systems are on TF 2.1, same code and files.\r\n\r\nTry to run a simple model, like Keras example for MNIST dataset.\r\nhttps://keras.io/examples/vision/mnist_convnet/\r\nTell us if you still facing the same problem with this simple one\r\nBtw, i'm using TF-GPU 2.3.0, Cuda 10.1 and  Cudnn 7.6.5.\r\nI can train models on my GTX 1660 with no problem.", "> \r\n> Try to run a simple model, like Keras example for MNIST dataset.\r\n> https://keras.io/examples/vision/mnist_convnet/\r\n> Tell us if you still facing the same problem with this simple one\r\n> Btw, i'm using TF-GPU 2.3.0, Cuda 10.1 and Cudnn 7.6.5.\r\n> I can train models on my GTX 1660 with no problem.\r\n\r\nAre you on Windows or Linux? I haven't managed to run the MNIST example yet but was able to start a short test run of my code on the CPU under Linux. It has the same multiplier as the GPU so my problem seems to be on Linux only and independent of CPU/GPU. In RAM (not VRAM) numbers:\r\nWin/CPU: ~4.2 GB used\r\nLinux/CPU: ~11.8 GB\r\n\r\nI've seen the same problem with/without memory_growth = True, validation_data, run_eagerly=True and garbage collector in callbacks.", "> \r\n> \r\n> > Try to run a simple model, like Keras example for MNIST dataset.\r\n> > https://keras.io/examples/vision/mnist_convnet/\r\n> > Tell us if you still facing the same problem with this simple one\r\n> > Btw, i'm using TF-GPU 2.3.0, Cuda 10.1 and Cudnn 7.6.5.\r\n> > I can train models on my GTX 1660 with no problem.\r\n> \r\n> Are you on Windows or Linux? I haven't managed to run the MNIST example yet but was able to start a short test run of my code on the CPU under Linux. It has the same multiplier as the GPU so my problem seems to be on Linux only and independent of CPU/GPU. In RAM (not VRAM) numbers:\r\n> Win/CPU: ~4.2 GB used\r\n> Linux/CPU: ~11.8 GB\r\n> \r\n> I've seen the same problem with/without memory_growth = True, validation_data, run_eagerly=True and garbage collector in callbacks.\r\n\r\nGotcha.\r\nSo you could try to use another tensorflow version, instead of 2.1 you could install 2.3 / 2.4.", "any updates here? I'm also having trouble with CPU memory usage. I'm using linux.", "@yetanotheryeti By any chance you found a solution for this issue ? If yes, please let us know. We are also facing same issue on tensorflow 2.6.", "@kattakarthik Unfortunately no. However, it seems to be a CUDA specific \"issue\". In Pytorch, for example, a similar amount of RAM is allocated for this minimal code example (in my case 2500 MB):\r\n\r\n```\r\nimport torch\r\ntorch.tensor([1.]).cuda()\r\n```\r\n\r\nCompare to [CPU RAM Usage with CUDA is Large (2+GB)](https://discuss.pytorch.org/t/cpu-ram-usage-with-cuda-is-large-2-gb/117668)."]}, {"number": 36458, "title": "Update cohen's Kappa to allow for batch dimension", "body": "Not sure if we're still able to make changes to TF1, but I have to use it for AWS Sagemaker unfortunately.\r\n\r\nI see in the Cohen's Kappa function, line 3857, we squeeze 2-dimensional label tensors (num, 1) down to 1-dim (num,). Unfortunately, in my case my label tensor is (batch_size, num), and then this function squeezes that down unnecessarily.\r\n\r\nMy current workaround is to call the function as follows:\r\n\r\n```\r\nkappa = tf.contrib.metrics.cohen_kappa(labels=tf.reshape(labels, [-1]),\r\n                                           predictions_idx=tf.reshape(predictions, [-1]),\r\n                                           num_classes=N_CLASSES,\r\n                                           name='kappa_op')\r\n```\r\n\r\nWhich seems to work fine.", "comments": ["~~We should still get this on master and then cherry-pick against `r1.15`~~\r\n\r\nOh, this is touching only `contrib/` paths, so it cannot go on master.\r\n\r\nPlease add some unit tests and then ping me to trigger presubmit tests against the `r1.15` branch", "@loodvn Any update please ? Thanks!", "Hey there! Sorry, I've been struggling to get the CI scripts to work on my machine (to check my unit tests), so this PR hasn't been getting the attention it deserves. I'll try get it working ASAP, sorry!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Hey all - I'm sorry I left this for so long! I still didn't manage to get the CI scripts to work (both using the Docker image and by building locally) and I think it's best to rather close this PR as I don't see myself polishing it any time soon.\r\n\r\nThanks for your time though, and I will definitely put more thought and time commitment into a proper TF PR at some stage! I appreciate it. Hope you're keeping safe :)"]}, {"number": 36457, "title": "Predicting on custom model fails", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): bunary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1 / 7.6.4.38-1\r\n- GPU model and memory: RTX 2080 Ti 11GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n* While I was trying to train some model (keras subclassed model) and export that model for tensorflow serving, I encountered this error.\r\n* So I tried to mimc same error with different but smaller code as below.\r\n* error is that ... \r\n  * using tf.saved_model.save() with model that outputs image tensor (not scalar - it works) not working in tf 2.1, but works in tf 2.0\r\n  * debug trace shows this error is related to distribute training, but I'm not using it.. or maybe I'm misleading it.\r\n  * in official package's `training_v2_utils.py` and `_aggregate_predict_results(strategy, batch_outs, model)` function, something seems wrong on `model.output`\r\n\r\n**Describe the expected behavior**\r\n* should work on tf 2.1 too\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\n\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\n# https://www.tensorflow.org/tutorials/generative/dcgan\r\n\r\ndef set_gpu_memory_growth():\r\n    gpus = tf.config.list_physical_devices('GPU')\r\n    if gpus:\r\n        try:\r\n            # Currently, memory growth needs to be the same across GPUs\r\n            for gpu in gpus:\r\n                tf.config.experimental.set_memory_growth(gpu, True)\r\n            logical_gpus = tf.config.list_logical_devices('GPU')\r\n            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n        except RuntimeError as e:\r\n            # Memory growth must be set before GPUs have been initialized\r\n            print(e)\r\n    return\r\n\r\n\r\ndef generate_and_save_images(model, epoch, test_input):\r\n    # Notice `training` is set to False.\r\n    # This is so all layers run in inference mode (batchnorm).\r\n    predictions = model(test_input, training=False)\r\n\r\n    fig = plt.figure(figsize=(4, 4))\r\n\r\n    for i in range(predictions.shape[0]):\r\n        plt.subplot(4, 4, i + 1)\r\n        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\r\n        plt.axis('off')\r\n\r\n    plt.savefig('./results/dcgan/image_at_epoch_{:04d}.png'.format(epoch))\r\n    plt.close(fig)\r\n    return\r\n\r\n\r\nclass Dense(tf.keras.layers.Layer):\r\n    def __init__(self, units, **kwargs):\r\n        super(Dense, self).__init__(**kwargs)\r\n        self.units = units\r\n\r\n    def build(self, input_shape):\r\n        fin = np.prod(input_shape[1:])\r\n        weight_shape = [fin, self.units]\r\n\r\n        w_init = tf.random.normal(shape=weight_shape, mean=0.0, stddev=0.01)\r\n        self.w = tf.Variable(w_init, name='w', trainable=True)\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        x = tf.keras.layers.Flatten()(inputs)\r\n        x = tf.matmul(x, self.w)\r\n        return x\r\n\r\n    def get_config(self):\r\n        config = super(Dense, self).get_config()\r\n        config.update({'units': self.units})\r\n        return config\r\n\r\n\r\nclass LeakyReLU(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        super(LeakyReLU, self).__init__(**kwargs)\r\n        self.alpha = 0.2\r\n\r\n        self.act = tf.keras.layers.LeakyReLU(alpha=self.alpha)\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        x = self.act(inputs)\r\n        return x\r\n\r\n    def get_config(self):\r\n        config = super(LeakyReLU, self).get_config()\r\n        config.update({'alpha': self.alpha})\r\n        return config\r\n\r\n\r\nclass Generator(tf.keras.Model):\r\n    def __init__(self, kernel, **kwargs):\r\n        super(Generator, self).__init__(**kwargs)\r\n\r\n        self.kernel = kernel\r\n\r\n        self.dense0 = Dense(units=7 * 7 * 256)\r\n        self.bn0 = tf.keras.layers.BatchNormalization()\r\n        self.lrelu0 = LeakyReLU()\r\n\r\n        self.convt1 = tf.keras.layers.Conv2DTranspose(128, self.kernel, strides=(1, 1), padding='same', use_bias=False)\r\n        self.bn1 = tf.keras.layers.BatchNormalization()\r\n        self.lrelu1 = LeakyReLU()\r\n\r\n        self.convt2 = tf.keras.layers.Conv2DTranspose(64, self.kernel, strides=(2, 2), padding='same', use_bias=False)\r\n        self.bn2 = tf.keras.layers.BatchNormalization()\r\n        self.lrelu2 = LeakyReLU()\r\n\r\n        self.convt3 = tf.keras.layers.Conv2DTranspose(1, self.kernel, strides=(2, 2), padding='same', use_bias=False,\r\n                                                      activation='tanh')\r\n\r\n    @tf.function\r\n    def call(self, inputs, training=None, mask=None):\r\n        z = inputs\r\n\r\n        x = self.dense0(z)\r\n        x = self.bn0(x, training=training)\r\n        x = self.lrelu0(x)\r\n\r\n        x = tf.reshape(x, shape=[-1, 7, 7, 256])\r\n\r\n        x = self.convt1(x)\r\n        x = self.bn1(x, training=training)\r\n        x = self.lrelu1(x)\r\n\r\n        x = self.convt2(x)\r\n        x = self.bn2(x, training=training)\r\n        x = self.lrelu2(x)\r\n\r\n        x = self.convt3(x)\r\n        return x\r\n\r\n    def get_config(self):\r\n        config = super(Generator, self).get_config()\r\n        config.update({'kernel': self.kernel})\r\n        return config\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        print('[Generator] - compute_output_shape() input_shape: {}'.format(input_shape))\r\n        return input_shape[0], 28, 28, 1\r\n\r\n    @tf.function\r\n    def serve(self, z):\r\n        x = self.dense0(z)\r\n        x = self.bn0(x, training=False)\r\n        x = self.lrelu0(x)\r\n\r\n        x = tf.reshape(x, shape=[-1, 7, 7, 256])\r\n\r\n        x = self.convt1(x)\r\n        x = self.bn1(x, training=False)\r\n        x = self.lrelu1(x)\r\n\r\n        x = self.convt2(x)\r\n        x = self.bn2(x, training=False)\r\n        x = self.lrelu2(x)\r\n\r\n        x = self.convt3(x)\r\n        return x\r\n\r\n\r\ndef make_discriminator_model():\r\n    model = tf.keras.Sequential()\r\n    model.add(tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\r\n    model.add(tf.keras.layers.LeakyReLU())\r\n    model.add(tf.keras.layers.Dropout(0.3))\r\n\r\n    model.add(tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\r\n    model.add(tf.keras.layers.LeakyReLU())\r\n    model.add(tf.keras.layers.Dropout(0.3))\r\n\r\n    model.add(tf.keras.layers.Flatten())\r\n    model.add(tf.keras.layers.Dense(1))\r\n    return model\r\n\r\n\r\ndef load_mnist(batch_size):\r\n    (train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\r\n    train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\r\n    train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\r\n\r\n    # Batch and shuffle the data\r\n    train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(60000).batch(batch_size)\r\n    return train_dataset\r\n\r\n\r\ndef discriminator_loss(real_output, fake_output, cross_entropy):\r\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\r\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\r\n    total_loss = real_loss + fake_loss\r\n    return total_loss\r\n\r\n\r\ndef generator_loss(fake_output, cross_entropy):\r\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\r\n\r\n\r\n# Notice the use of `tf.function`\r\n# This annotation causes the function to be \"compiled\".\r\n@tf.function\r\ndef train_step(images, batch_size, noise_dim, generator, discriminator,\r\n               cross_entropy, generator_optimizer, discriminator_optimizer):\r\n    noise = tf.random.normal([batch_size, noise_dim])\r\n\r\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n        generated_images = generator(noise, training=True)\r\n\r\n        real_output = discriminator(images, training=True)\r\n        fake_output = discriminator(generated_images, training=True)\r\n\r\n        gen_loss = generator_loss(fake_output, cross_entropy)\r\n        disc_loss = discriminator_loss(real_output, fake_output, cross_entropy)\r\n\r\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\r\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\r\n\r\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\r\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\r\n\r\n\r\ndef train_loop():\r\n    batch_size = 256\r\n    epochs = 30\r\n    noise_dim = 100\r\n    num_examples_to_generate = 16\r\n\r\n    dataset = load_mnist(batch_size)\r\n\r\n    # create models\r\n    generator = Generator(kernel=5)\r\n    discriminator = make_discriminator_model()\r\n\r\n    # loss\r\n    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n\r\n    # optimizer\r\n    generator_optimizer = tf.keras.optimizers.Adam(1e-4)\r\n    discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\r\n\r\n    checkpoint_dir = './models/dcgan'\r\n    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\r\n                                     discriminator_optimizer=discriminator_optimizer,\r\n                                     generator=generator,\r\n                                     discriminator=discriminator)\r\n    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=2)\r\n\r\n    # We will reuse this seed overtime (so it's easier)\r\n    # to visualize progress in the animated GIF)\r\n    seed = tf.random.normal([num_examples_to_generate, noise_dim])\r\n\r\n    for epoch in range(epochs):\r\n        start = time.time()\r\n\r\n        for image_batch in dataset:\r\n            train_step(image_batch, batch_size, noise_dim, generator, discriminator,\r\n                       cross_entropy, generator_optimizer, discriminator_optimizer)\r\n\r\n        # Save the model every 10 epochs\r\n        if (epoch + 1) % 10 == 0:\r\n            # Produce images for the GIF as we go\r\n            generate_and_save_images(generator, epoch + 1, seed)\r\n            manager.save(checkpoint_number=epoch + 1)\r\n\r\n        print('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))\r\n\r\n    # Generate after the final epoch\r\n    generate_and_save_images(generator, epochs, seed)\r\n    return\r\n\r\n\r\ndef export_model():\r\n    # restore generator model only\r\n    noise_dim = 100\r\n    generator = Generator(kernel=5, dynamic=True)\r\n    test_x = tf.random.normal([1, noise_dim])\r\n    _ = generator(test_x, training=False)\r\n\r\n    checkpoint_dir = './models/dcgan'\r\n    checkpoint = tf.train.Checkpoint(generator=generator)\r\n    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=2)\r\n    checkpoint.restore(manager.latest_checkpoint).expect_partial()\r\n    if manager.latest_checkpoint:\r\n        print('Restored from {}'.format(manager.latest_checkpoint))\r\n        _ = generator.predict(test_x)\r\n    else:\r\n        raise ValueError()\r\n\r\n    # export generator model\r\n    export_dir = './models/dcgan/1'\r\n    tf.saved_model.save(\r\n        generator,\r\n        export_dir,\r\n        signatures=generator.serve.get_concrete_function(\r\n            z=tf.TensorSpec(shape=[None, noise_dim], dtype=tf.float32)\r\n        )\r\n    )\r\n    return\r\n\r\n\r\ndef main():\r\n    set_gpu_memory_growth()\r\n\r\n    train_loop()\r\n\r\n    export_model()\r\n    return\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n/home/moono/anaconda3/envs/tf-2.1/bin/python /mnt/data_ssd/git-repos/tensorflow-serving-2.x/custom_training_dcgan.py\r\n2020-02-04 17:07:07.806587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2020-02-04 17:07:07.807381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\r\n2020-02-04 17:07:08.226664: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-02-04 17:07:08.252985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-04 17:07:08.253267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.575GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-02-04 17:07:08.253285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-02-04 17:07:08.253303: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-04 17:07:08.254301: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-02-04 17:07:08.254520: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-02-04 17:07:08.255463: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-02-04 17:07:08.255982: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-02-04 17:07:08.256001: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-04 17:07:08.256046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-04 17:07:08.256327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-04 17:07:08.256572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-02-04 17:07:08.256776: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-02-04 17:07:08.277755: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz\r\n2020-02-04 17:07:08.278023: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a541a20050 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-02-04 17:07:08.278034: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-02-04 17:07:08.343779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-04 17:07:08.344115: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a54215a3e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-02-04 17:07:08.344127: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-02-04 17:07:08.344226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-04 17:07:08.344839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.575GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-02-04 17:07:08.344861: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-02-04 17:07:08.344868: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-04 17:07:08.344878: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-02-04 17:07:08.344884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-02-04 17:07:08.344891: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-02-04 17:07:08.344898: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-02-04 17:07:08.344903: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-04 17:07:08.344935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-04 17:07:08.345190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-04 17:07:08.345429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-02-04 17:07:08.345473: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-02-04 17:07:08.509396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-04 17:07:08.509447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-02-04 17:07:08.509452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-02-04 17:07:08.509580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-04 17:07:08.509854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-02-04 17:07:08.510101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9565 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n1 Physical GPUs, 1 Logical GPUs\r\n2020-02-04 17:07:10.728256: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-04 17:07:10.847845: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\nTime for epoch 1 is 7.408124208450317 sec\r\nTime for epoch 2 is 4.247774362564087 sec\r\nTime for epoch 3 is 4.300286054611206 sec\r\nTime for epoch 4 is 4.301387786865234 sec\r\nTime for epoch 5 is 4.302702188491821 sec\r\nTime for epoch 6 is 4.3085198402404785 sec\r\nTime for epoch 7 is 4.314511060714722 sec\r\nTime for epoch 8 is 4.3180766105651855 sec\r\nTime for epoch 9 is 4.276162147521973 sec\r\nTime for epoch 10 is 4.594261884689331 sec\r\nTime for epoch 11 is 4.290403127670288 sec\r\nTime for epoch 12 is 4.332635164260864 sec\r\nTime for epoch 13 is 4.34122896194458 sec\r\nTime for epoch 14 is 4.342823505401611 sec\r\nTime for epoch 15 is 4.30718469619751 sec\r\nTime for epoch 16 is 4.350044012069702 sec\r\nTime for epoch 17 is 4.3491082191467285 sec\r\nTime for epoch 18 is 4.352354049682617 sec\r\nTime for epoch 19 is 4.318714618682861 sec\r\nTime for epoch 20 is 4.648451089859009 sec\r\nTime for epoch 21 is 4.324911117553711 sec\r\nTime for epoch 22 is 4.33571720123291 sec\r\nTime for epoch 23 is 4.367572069168091 sec\r\nTime for epoch 24 is 4.363953351974487 sec\r\nTime for epoch 25 is 4.342392683029175 sec\r\nTime for epoch 26 is 4.374950647354126 sec\r\nTime for epoch 27 is 4.375419616699219 sec\r\nTime for epoch 28 is 4.3764965534210205 sec\r\nTime for epoch 29 is 4.358669757843018 sec\r\nTime for epoch 30 is 4.596313714981079 sec\r\nRestored from ./models/dcgan/ckpt-30\r\n[Generator] - compute_output_shape() input_shape: (None, 100)\r\nTraceback (most recent call last):\r\n  File \"/mnt/data_ssd/git-repos/tensorflow-serving-2.x/custom_training_dcgan.py\", line 322, in <module>\r\n    main()\r\n  File \"/mnt/data_ssd/git-repos/tensorflow-serving-2.x/custom_training_dcgan.py\", line 317, in main\r\n    export_model()\r\n  File \"/mnt/data_ssd/git-repos/tensorflow-serving-2.x/custom_training_dcgan.py\", line 296, in export_model\r\n    _ = generator.predict(test_x)\r\n  File \"/home/moono/anaconda3/envs/tf-2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1013, in predict\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/moono/anaconda3/envs/tf-2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 498, in predict\r\n    workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\r\n  File \"/home/moono/anaconda3/envs/tf-2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 475, in _model_iteration\r\n    total_epochs=1)\r\n  File \"/home/moono/anaconda3/envs/tf-2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 168, in run_one_epoch\r\n    strategy, batch_outs, model)\r\n  File \"/home/moono/anaconda3/envs/tf-2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 264, in _aggregate_predict_results\r\n    nest.flatten(nested_outs))\r\n  File \"/home/moono/anaconda3/envs/tf-2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py\", line 1199, in concat_along_batch_dimension\r\n    if isinstance(outputs[0], sparse_tensor.SparseTensor):\r\nIndexError: list index out of range\r\n\r\nProcess finished with exit code 1", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/3117af743b141fd4fa629dc130ab6143/36457.ipynb). Thanks!", "Forgot to mention one important thing.\r\nTo reproduce this issue, one needs to set `dynamic=True` before export the model.\r\nAs in sample code I provided,\r\n\r\n```python\r\ndef export_model():\r\n    # restore generator model only\r\n    noise_dim = 100\r\n    generator = Generator(kernel=5, dynamic=True)\r\n```", "When dynamic is set to True, it means the model is only able to run in eager mode (and thus incompatible with graphs). The SavedModel format tries to serialize the model in a graph, so this is fundamentally incompatible.\r\n\r\nThe error being raised here is actually related to `generator.predict(test_x)`.  (try calling this right after the generator and test_x are created). I've attached the error logs below. @qlzh727  Can you take a look and see if it's an error with the `_aggregate_predict_results`? If there's no issue, how can the error be improved?\r\n\r\n```\r\n<ipython-input-4-4f5b3ec7f8fa> in export_model()\r\n    265     test_x = tf.random.normal([1, noise_dim])\r\n    266     _ = generator(test_x, training=False)\r\n--> 267     _ = generator.predict(test_x)\r\n    268     checkpoint_dir = './models/dcgan'\r\n    269     checkpoint = tf.train.Checkpoint(generator=generator)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n   1011         max_queue_size=max_queue_size,\r\n   1012         workers=workers,\r\n-> 1013         use_multiprocessing=use_multiprocessing)\r\n   1014 \r\n   1015   def reset_metrics(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in predict(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    496         model, ModeKeys.PREDICT, x=x, batch_size=batch_size, verbose=verbose,\r\n    497         steps=steps, callbacks=callbacks, max_queue_size=max_queue_size,\r\n--> 498         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\r\n    499 \r\n    500 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in _model_iteration(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    473               mode=mode,\r\n    474               training_context=training_context,\r\n--> 475               total_epochs=1)\r\n    476           cbks.make_logs(model, epoch_logs, result, mode)\r\n    477 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    166       else:\r\n    167         batch_outs = training_v2_utils._aggregate_predict_results(\r\n--> 168             strategy, batch_outs, model)\r\n    169 \r\n    170       if step == 0:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in _aggregate_predict_results(strategy, batch_outs, model)\r\n    262     nested_outs = batch_outs[i * num_replicas:i * num_replicas + num_replicas]\r\n    263     per_output_result = dist_utils.concat_along_batch_dimension(\r\n--> 264         nest.flatten(nested_outs))\r\n    265 \r\n    266     if need_batch_index_gather:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py in concat_along_batch_dimension(outputs)\r\n   1197 def concat_along_batch_dimension(outputs):\r\n   1198   \"\"\"Concats prediction outputs along the batch dimension.\"\"\"\r\n-> 1199   if isinstance(outputs[0], sparse_tensor.SparseTensor):\r\n   1200     return sparse_ops.sparse_concat_v2(axis=0, sp_inputs=outputs)\r\n   1201   if isinstance(outputs[0], ragged_tensor.RaggedTensor):\r\n\r\nIndexError: list index out of range\r\n```\r\n", "Digging more into details, I think the issue is caused by how we build model input/output on a dynamic model. It is using the result of model.compute_output_shape to build a symbolic output, and incorrectly use nest.map_structure, which in this case expand a 4D shape into 4 outputs. The number of wrong outputs cause the error down the stream.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/af38219818b1b677277018468c089593965d8c24/tensorflow/python/keras/engine/base_layer.py#L2219", "The walk around so far is to update your compute_output_shape method with the wrapper of tf.TensorShape(), so that the nest lib will not try to expand it. We will fix the issue on our side in the meantime.\r\n\r\n```python\r\n  def compute_output_shape(self, input_shape):\r\n    print('[Generator] - compute_output_shape() input_shape: {}'.format(input_shape))\r\n    return tf.TensorShape([input_shape[0], 28, 28, 1])\r\n```", "Thank you for the reply!!\r\nI'll try the workaround. And hope to see the fixes in next release."]}, {"number": 36456, "title": "Unable to evaluate FusedBatchNormV3 operation without GPU", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- Mobile device if the issue happens on mobile device: n/a\r\n- TensorFlow installed from: binary (installed by pip)\r\n- TensorFlow version: v1.15.0-rc3-22-g590d6ee 1.15.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: CUDA:10.0, cuDNN:7.6\r\n- GPU model and memory: GeForce GTX 1080 Ti\r\n\r\n**Describe the current behavior**\r\nI would like to evaluate the outputs of each operation of our network.\r\nI succeeded to get the outputs by running the following code with GPU (Using `tensorflow-gpu`), but without GPU (Using `tensorflow-cpu`), I got an error as follows.\r\n```\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: Node 'BatchNorm/FusedBatchNormV3' (type: '_FusedConv2D', num of outputs: 1) does not have output 1\r\n```\r\nThere are different behavior about `BatchNorm/FusedBatchNormV3` between w/ GPU and w/o GPU.\r\nI think `FusedBatchNormV3`'s num of outputs should be 6 but that error shows num of outputs is 1.\r\n\r\n**Describe the expected behavior**\r\nI expected to be enabled to evaluate the outputs of `BatchNorm/FusedBatchNormV3` only using CPU.\r\n\r\n**Code to reproduce the issue**\r\nAt first, create a test script (`test.py`) as following.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nbatch_size=1\r\nimage_size = (64,64)\r\n\r\ndef main():\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        shape = (batch_size, image_size[0], image_size[1], 3)\r\n        images_placeholder = tf.compat.v1.placeholder(\r\n            tf.float32,\r\n            shape=shape,\r\n            name=\"images_placeholder\")\r\n\r\n        conv = tf.layers.conv2d(images_placeholder, filters=32, kernel_size=3, padding='SAME', use_bias=False)\r\n        batch_normed = tf.contrib.layers.batch_norm(conv,is_training=False)\r\n        softmax = tf.nn.softmax(batch_normed)\r\n        output = tf.identity(softmax, name=\"output\")\r\n\r\n        init_op = tf.global_variables_initializer()\r\n\r\n    session_config = tf.ConfigProto()\r\n    sess = tf.Session(graph=graph, config=session_config)\r\n    sess.run(init_op)\r\n\r\n    images = np.expand_dims(np.zeros((image_size[0],image_size[1], 3), dtype=float), axis=0)\r\n    feed_dict = {\r\n        images_placeholder: images,\r\n    }\r\n    \r\n    all_ops = graph.get_operations()\r\n    all_outputs = []\r\n    index = 0\r\n    for op in all_ops:\r\n        for op_output in op.outputs:\r\n            print(op_output)\r\n            val = sess.run(op_output.name, feed_dict=feed_dict)\r\n            all_outputs.append({'val': val, 'name': op_output.name})\r\n            index += 1\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\nAfter that, run commands as following.\r\n```\r\n$ pip install tensorflow-cpu==1.15.0\r\n$ python test.py\r\n``` \r\n\r\n**Other info / logs**\r\n* [**Success**] Run `test.py` with GPU (`tensorflow-gpu==1.15.0`)\r\n```\r\n$ python test.py \r\nWARNING:tensorflow:From test.py:16: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.keras.layers.Conv2D` instead.\r\nWARNING:tensorflow:From /home/hadusam/tensorflow-gpu/lib/python3.5/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.__call__` method instead.\r\nWARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nWARNING:tensorflow:From test.py:21: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\r\n\r\nWARNING:tensorflow:From test.py:23: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\r\n\r\nWARNING:tensorflow:From test.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2020-02-04 17:12:56.452491: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2020-02-04 17:12:56.458872: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3599980000 Hz\r\n2020-02-04 17:12:56.459272: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6728e70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-02-04 17:12:56.459299: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-02-04 17:12:56.462463: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-02-04 17:12:56.676188: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x67d26e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-02-04 17:12:56.676219: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-02-04 17:12:56.677202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:af:00.0\r\n2020-02-04 17:12:56.677442: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-02-04 17:12:56.678571: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-02-04 17:12:56.679662: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-02-04 17:12:56.679943: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-02-04 17:12:56.681246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-02-04 17:12:56.682741: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-02-04 17:12:56.685936: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-04 17:12:56.688818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-02-04 17:12:56.688898: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-02-04 17:12:56.689938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-04 17:12:56.689984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2020-02-04 17:12:56.690014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2020-02-04 17:12:56.691698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:af:00.0, compute capability: 6.1)\r\nTensor(\"images_placeholder:0\", shape=(1, 64, 64, 3), dtype=float32)\r\nTensor(\"conv2d/kernel/Initializer/random_uniform/shape:0\", shape=(4,), dtype=int32)\r\nTensor(\"conv2d/kernel/Initializer/random_uniform/min:0\", shape=(), dtype=float32)\r\nTensor(\"conv2d/kernel/Initializer/random_uniform/max:0\", shape=(), dtype=float32)\r\nTensor(\"conv2d/kernel/Initializer/random_uniform/RandomUniform:0\", shape=(3, 3, 3, 32), dtype=float32)\r\nTensor(\"conv2d/kernel/Initializer/random_uniform/sub:0\", shape=(), dtype=float32)\r\nTensor(\"conv2d/kernel/Initializer/random_uniform/mul:0\", shape=(3, 3, 3, 32), dtype=float32)\r\nTensor(\"conv2d/kernel/Initializer/random_uniform:0\", shape=(3, 3, 3, 32), dtype=float32)\r\nTensor(\"conv2d/kernel:0\", shape=(3, 3, 3, 32), dtype=float32_ref)\r\nTensor(\"conv2d/kernel/Assign:0\", shape=(3, 3, 3, 32), dtype=float32_ref)\r\nTensor(\"conv2d/kernel/read:0\", shape=(3, 3, 3, 32), dtype=float32)\r\nTensor(\"conv2d/dilation_rate:0\", shape=(2,), dtype=int32)\r\nTensor(\"conv2d/Conv2D:0\", shape=(1, 64, 64, 32), dtype=float32)\r\n2020-02-04 17:12:57.365891: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-04 17:12:58.204387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\nTensor(\"BatchNorm/Const:0\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/beta/Initializer/zeros:0\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/beta:0\", shape=(32,), dtype=float32_ref)\r\nTensor(\"BatchNorm/beta/Assign:0\", shape=(32,), dtype=float32_ref)\r\nTensor(\"BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/moving_mean/Initializer/zeros:0\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/moving_mean:0\", shape=(32,), dtype=float32_ref)\r\nTensor(\"BatchNorm/moving_mean/Assign:0\", shape=(32,), dtype=float32_ref)\r\nTensor(\"BatchNorm/moving_mean/read:0\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/moving_variance/Initializer/ones:0\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/moving_variance:0\", shape=(32,), dtype=float32_ref)\r\nTensor(\"BatchNorm/moving_variance/Assign:0\", shape=(32,), dtype=float32_ref)\r\nTensor(\"BatchNorm/moving_variance/read:0\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/FusedBatchNormV3:0\", shape=(1, 64, 64, 32), dtype=float32)\r\nTensor(\"BatchNorm/FusedBatchNormV3:1\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/FusedBatchNormV3:2\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/FusedBatchNormV3:3\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/FusedBatchNormV3:4\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/FusedBatchNormV3:5\", dtype=float32)\r\nTensor(\"BatchNorm/Const_1:0\", shape=(), dtype=float32)\r\nTensor(\"Softmax:0\", shape=(1, 64, 64, 32), dtype=float32)\r\nTensor(\"output:0\", shape=(1, 64, 64, 32), dtype=float32)\r\n```\r\n\r\n\r\n* [**Error**] Run `test.py` without GPU (`tensorflow-cpu==1.15.0`)\r\n```\r\n$ python test.py\r\nWARNING:tensorflow:From test.py:16: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.keras.layers.Conv2D` instead.\r\nWARNING:tensorflow:From /home/hadusam/tensorflow-cpu/lib/python3.5/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.__call__` method instead.\r\nWARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nWARNING:tensorflow:From test.py:21: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\r\n\r\nWARNING:tensorflow:From test.py:23: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\r\n\r\nWARNING:tensorflow:From test.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2020-02-04 17:26:55.513752: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2020-02-04 17:26:55.518222: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3599980000 Hz\r\n2020-02-04 17:26:55.518610: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x52ffee0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-02-04 17:26:55.518636: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTensor(\"images_placeholder:0\", shape=(1, 64, 64, 3), dtype=float32)\r\nTensor(\"conv2d/kernel/Initializer/random_uniform/shape:0\", shape=(4,), dtype=int32)\r\nTensor(\"conv2d/kernel/Initializer/random_uniform/min:0\", shape=(), dtype=float32)\r\nTensor(\"conv2d/kernel/Initializer/random_uniform/max:0\", shape=(), dtype=float32)\r\nTensor(\"conv2d/kernel/Initializer/random_uniform/RandomUniform:0\", shape=(3, 3, 3, 32), dtype=float32)\r\nTensor(\"conv2d/kernel/Initializer/random_uniform/sub:0\", shape=(), dtype=float32)\r\nTensor(\"conv2d/kernel/Initializer/random_uniform/mul:0\", shape=(3, 3, 3, 32), dtype=float32)\r\nTensor(\"conv2d/kernel/Initializer/random_uniform:0\", shape=(3, 3, 3, 32), dtype=float32)\r\nTensor(\"conv2d/kernel:0\", shape=(3, 3, 3, 32), dtype=float32_ref)\r\nTensor(\"conv2d/kernel/Assign:0\", shape=(3, 3, 3, 32), dtype=float32_ref)\r\nTensor(\"conv2d/kernel/read:0\", shape=(3, 3, 3, 32), dtype=float32)\r\nTensor(\"conv2d/dilation_rate:0\", shape=(2,), dtype=int32)\r\nTensor(\"conv2d/Conv2D:0\", shape=(1, 64, 64, 32), dtype=float32)\r\nTensor(\"BatchNorm/Const:0\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/beta/Initializer/zeros:0\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/beta:0\", shape=(32,), dtype=float32_ref)\r\nTensor(\"BatchNorm/beta/Assign:0\", shape=(32,), dtype=float32_ref)\r\nTensor(\"BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/moving_mean/Initializer/zeros:0\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/moving_mean:0\", shape=(32,), dtype=float32_ref)\r\nTensor(\"BatchNorm/moving_mean/Assign:0\", shape=(32,), dtype=float32_ref)\r\nTensor(\"BatchNorm/moving_mean/read:0\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/moving_variance/Initializer/ones:0\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/moving_variance:0\", shape=(32,), dtype=float32_ref)\r\nTensor(\"BatchNorm/moving_variance/Assign:0\", shape=(32,), dtype=float32_ref)\r\nTensor(\"BatchNorm/moving_variance/read:0\", shape=(32,), dtype=float32)\r\nTensor(\"BatchNorm/FusedBatchNormV3:0\", shape=(1, 64, 64, 32), dtype=float32)\r\nTensor(\"BatchNorm/FusedBatchNormV3:1\", shape=(32,), dtype=float32)\r\nTraceback (most recent call last):\r\n  File \"/home/hadusam/tensorflow-cpu/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\r\n    return fn(*args)\r\n  File \"/home/hadusam/tensorflow-cpu/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"/home/hadusam/tensorflow-cpu/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: Node 'BatchNorm/FusedBatchNormV3' (type: '_FusedConv2D', num of outputs: 1) does not have output 1\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 43, in <module>\r\n    main()\r\n  File \"test.py\", line 38, in main\r\n    val = sess.run(op_output.name, feed_dict=feed_dict)\r\n  File \"/home/hadusam/tensorflow-cpu/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\", line 956, in run\r\n    run_metadata_ptr)\r\n  File \"/home/hadusam/tensorflow-cpu/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/hadusam/tensorflow-cpu/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\r\n    run_metadata)\r\n  File \"/home/hadusam/tensorflow-cpu/lib/python3.5/site-packages/tensorflow_core/python/client/session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: Node 'BatchNorm/FusedBatchNormV3' (type: '_FusedConv2D', num of outputs: 1) does not have output 1\r\n```\r\n", "comments": ["Works without issues with [TF-GPU](https://colab.sandbox.google.com/gist/amahendrakar/3d36e90ad8cd5f4be8c6b34caa50ec5f/36456_gpu.ipynb), was able to reproduce the issue with [TF-CPU](https://colab.sandbox.google.com/gist/amahendrakar/6358db5cb3b3f9415daf484149bb3a52/36456_cpu.ipynb). Please find the attached Gist. Thanks!", "`tf.contrib.layers.batch_norm` is deprecated. Can you please try using [`tf.keras.layers.BatchNormalization`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/layers/BatchNormalization) instead? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36456\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36456\">No</a>\n"]}, {"number": 36455, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError: input and filter must have same depth", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **HPC**:\r\n- **TensorFlow installed from conda**:\r\n- **TensorFlow version 1.15**:\r\n- **Python version 3.6.10:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nFolloing is the model graph summary\r\n![image](https://user-images.githubusercontent.com/13520957/73723907-711ac600-472a-11ea-8762-21a4787e2193.png)\r\nAnd the error code is \r\n![image](https://user-images.githubusercontent.com/13520957/73723959-8b54a400-472a-11ea-96d0-38cd9e792ce3.png)\r\n\r\nI assume that the layer 'block1_conv1' should automatically generate output with depth 64. \r\nHowever, from the error message it looks like it generates an output with depth 1 only. What is wrong here?\r\n", "comments": ["@Amit507017, Please provide the minimal standalone code to replicate the reported issue. Thanks! ", "@Amit507017, Could you provide the standalone code to analyze the issue. Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36454, "title": "even load work?", "body": "\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/migrate#saved_models_compatibility\r\n\r\n## Description of issue (what needs changing):\r\nThe sentence below is unclear in what it means. I think there might be an extra word.\r\n\r\n\"TensorFlow 2.0 saved_models even load work in TensorFlow 1.x if all the ops are supported.\"\r\n\r\n", "comments": ["Thanks. This is fixed now."]}, {"number": 36452, "title": "Broken outbound links from index pages for older TF API", "body": "## URL(s) with the issue:\r\n\r\nNumerous links of the older TF APIs, listing only a few index pages as examples here:\r\n\r\n- Links under \"Classes\" and \"Functions\" within https://github.com/tensorflow/docs/blob/r1.11/site/en/api_docs/python/tf.md\r\n- Links under \"Classes\" and \"Functions\" within https://github.com/tensorflow/docs/blob/r1.11/site/en/api_docs/python/tfdbg.md\r\n- Links within https://github.com/tensorflow/docs/blob/r1.5/site/en/api_docs/python/index.md\r\n- \"JAVA\" link within https://github.com/tensorflow/docs/blob/r1.7/site/en/api_docs/index.md\r\n\r\n## Description of issue (what needs changing):\r\n\r\nA large amount of links on index pages (e.g. for functions and classes) are currently broken (404). This appears to affect only the older TF versions (multiple versions affected), whose index pages are Markdown files within Github repositories.\r\n\r\n### Clear description\r\n\r\nOutbound links from these indexing pages are currently broken. Some might be fixed by adding a .md after the current URL to point to the correct Markdown available in the repositories, but not all of them can be fixed through this way.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n- Incorrect.\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue?\r\n- Currently no plan.\r\n", "comments": ["This issue persists in latest TF versions as well.Please, find below links as well.Thanks!\r\n\r\nhttps://github.com/tensorflow/docs/blob/r1.15/site/en/api_docs/python/tf.md\r\n\r\nhttps://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/tf.md\r\n\r\nhttps://github.com/tensorflow/docs/blob/r1.15/site/en/api_docs/python/tfdbg.md\r\n\r\nhttps://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/tfdbg.md\r\n\r\nhttps://github.com/tensorflow/docs/blob/r1.15/site/en/api_docs/python/index.md\r\n\r\nhttps://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/index.md\r\n\r\n\r\n", "Acknowledged.\r\n\r\nThis is my fault. I'll make sure it doesn't happen going forward, but IDK how many of the old snap-shots I can fix.", "Hey, @MarkDaoust  I'll happy to work in this issue.", "Hi @lichili233 ! 1.x version are not supported any more. For the 2.x version , You can find working links here. \r\n[python/index.md](https://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/index.md)  (Updated till 2.0 )\r\n[api_docs/index.md](https://github.com/tensorflow/docs/blob/r2.3/site/en/api_docs/index.md)  (Updated till 2.3)\r\n[python/tf.md](https://github.com/tensorflow/docs/blob/r2.4/site/en/api_docs/python/tf.md) (updated till 2.4)\r\n[debugger](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/debugger.md)", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 36451, "title": "Differentiating an extremely complicated weightless transform", "body": "I'm considering inserting a weightless transform between CNN layers in a neural network. The transform is very complicated - details below. I'm aware that custom transformations are supported via e.g. Keras'  `Lambda` layer  - however, I don't know if TF is capable of handling something of this complexity. The code I have needs to be converted for compatibility, but I rather it not be a wasted effort.\r\n\r\nCan TensorFlow's autodifferentiation handle [`synsq_cwt_fwd`](https://github.com/OverLordGoldDragon/ssqueezepy/blob/master/ssqueezepy/synsq_cwt.py#L12)?\r\n\r\n\r\n<hr>\r\n\r\n**Continuous Synchrosqueezing Transform** - full dev-stage code in the [ssqueezepy](https://github.com/OverLordGoldDragon/ssqueezepy) repository. Highlights (simplified):\r\n\r\n```python\r\nx = np.random.randn(2000)\r\nlen(Wx.squeeze().shape) == 2\r\n\r\npsihfn = lambda w: np.exp(2 * PI * 1j * .1 * w) * (\r\n    np.abs(w - 1) < .999) * np.exp(-1. / (1 - ((w - 1) * (np.abs(w - 1) < .999)) ** 2))\r\n\r\nfor i in range(200):\r\n    psih = psihfn(5 * np.arange(500))\r\n    Wx[i] = np.fft.ifftshift(np.fft.ifft(psih * np.fft.fft(x)))\r\n```\r\n```python\r\nWx.dtype == 'complex128'\r\nu = np.unwrap(np.angle(Wx)).T\r\nw = np.array([np.diff(u), u[-1] - u[0]]).T\r\n```\r\n 1. `psihfn` involves (a) complex exponential; (b) inverse asymptotic real exponential; (c) boolean mask\r\n 2. `Wx` involves (a) `fft` of `ifftshift` of `ifft` of `psih * fft(x)`, (b) computed _iteratively_, but _not recurrently_ (future values don't depend on past)\r\n 3. `w`  (second blob) involves (a) `Wx` computed in 1 & 2, a `complex128` 2D array; (b) applying `dif(unwrap(angle(Wx)).T)`\r\n\r\nThe complex components may be eliminated entirely, including in 1(a). The main subject of attention is perhaps the FFT nest; I don't know how numpy computes FFT, as it encapsulates an involved op. There's also a question of _performance_ - if it takes hours on a small array, it's not worth it.", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36451\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36451\">No</a>\n"]}, {"number": 36450, "title": "git is required by several tutorial examples", "body": "i was trying out the docker releases of tensorflow and several \"tensorflow-tutorials\" failed on me since they uses pip sources with git\r\n\r\neg  `!pip install git+https://github.com/tensorflow/docs`\r\n\r\nhttps://github.com/tensorflow/docs/blob/d9d0c5f4dadc423861ecfafaa2e795419d9898b7/site/en/tutorials/keras/overfit_and_underfit.ipynb#L183\r\n\r\nwill fail with \r\n\r\n```\r\nERROR: Error [Errno 2] No such file or directory: 'git': 'git' while executing command git clone -q https://github.com/tensorflow/docs /tmp/pip-req-build-ox49wr7p\r\nERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?\r\n```\r\n\r\n\r\nit seems like your Dockerfile config should be low-friction for new users. \u00a0having a tutorial fail like this isn't great. \u00a0yes, you could remove the git tutorials (not awesome either) or simply include git in the Dockerfile.\r\n\r\n\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36450) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36450) for more info**.\n\n<!-- ok -->", "Thanks for the fix. Can you follow the directions in dockerfiles/README.md to regenerate the rest of the dockerfiles, please?", "@carchrae  Can you please check reviewer comments and keep us posted. Thanks!", "sorry for the delay, generated files now committed."]}, {"number": 36449, "title": "Unclear documentation for implementing custom TensorFlow Keras optimizers", "body": "## URL(s) with the issue:\r\n\r\n[`tf.keras.optimizers.Optimizer`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer), specifically the section [Write a customized optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#write_a_customized_optimizer_2).\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe instructions for creating a custom optimizer seem to be inconsistent with how `tf.keras.optimizers.Optimizer` subclasses are defined in TensorFlow and other projects.\r\n\r\n### Clear description\r\n\r\nThis originated as a [question on Stack Overflow](https://stackoverflow.com/q/58772846/1917160), which is reproduced below.\r\n\r\nSuppose I want to write a custom optimizer class that conforms to the `tf.keras` API (using TensorFlow version>=2.0). I am confused about the documented way to do this versus what's done in implementations.\r\n\r\nThe documentation for `tf.keras.optimizers.Optimizer` states,\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L217-L224\r\n\r\nHowever, the current `tf.keras.optimizers.Optimizer` implementation does not define a `resource_apply_dense` method, but it *does* define a private-looking [`_resource_apply_dense` method stub](https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L916-L928). Similarly, there are no `resource_apply_sparse` or `create_slots` methods, but there are a [`_resource_apply_sparse` method stub](https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L958-L977) and a [`_create_slots` method call](https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L434).\r\n\r\nIn official `tf.keras.optimizers.Optimizer` subclasses (using `tf.keras.optimizers.Adam` as an example), there are [`_resource_apply_dense`](https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/adam.py#L192-L227), [`_resource_apply_sparse`](https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/adam.py#L229-L267), and [`_create_slots`](https://github.com/tensorflow/tensorflow/blob/7b1283ecf14e5f057b1a5c321a46db907ea713fc/tensorflow/python/keras/optimizer_v2/adam.py#L150-L159) methods, and there are no such methods without the leading underscore.\r\n\r\nThere are similar leading-underscore methods in slightly-less-official `tf.keras.optimizers.Optimizer` subclasses (e.g., `tfa.optimizers.MovingAverage` from TensorFlow Addons: [`_resource_apply_dense`](https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/average_wrapper.py#L73-L76), [`_resource_apply_sparse`](https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/average_wrapper.py#L78-L82), [`_create_slots`](https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/moving_average.py#L92-L95)).\r\n\r\nAnother confounding point for me is that some of the TensorFlow Addons optimizers *also* override the `apply_gradients` method (e.g., [`tfa.optimizers.MovingAverage`](https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/average_wrapper.py#L55-L57)), whereas the `tf.keras.optimizers` optimizers do not.\r\n\r\nMoreover, I noticed that the `apply_gradients` method of `tf.keras.optimizers.Optimizer` method calls `_create_slots`, but the base `tf.keras.optimizers.Optimizer` class does not have a `_create_slots` method. So, it seems that a `_create_slots` method *must* be defined in an optimizer subclass if that subclass does not override `apply_gradients`.\r\n\r\n#### Questions\r\n\r\nWhat is the correct way to subclass a `tf.keras.optimizers.Optimizer`? Specifically,\r\n\r\n1. Does the `tf.keras.optimizers.Optimizer` documentation listed at the top simply mean to override the leading-underscore versions of the methods they mention (e.g., `_resource_apply_dense` instead of `resource_apply_dense`)? If so, are there any API guarantees about these private-looking methods not changing their behavior in future versions of TensorFlow? What are the signatures of these methods?\r\n2. When would one override `apply_gradients` in addition to the `_apply_resource_[dense|sparse]` methods?", "comments": ["@artemmavrin Thanks for the report! This has been fixed nightly [here](https://github.com/tensorflow/tensorflow/commit/9f7ded2d44a8bf8c3c47cc2a475a5ca6bab6b765)\r\n\r\nLet us know if you have other questions."]}, {"number": 36448, "title": "Categorical encoding in `docs/csv.ipynb` returning inconsistent results.", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/load_data/csv\r\n\r\nwhich is available on GitHub at\r\n\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/csv.ipynb\r\n\r\n## Description of issue:\r\n\r\nThis is about the output of the last cell in the section _Categorical data_, namely the output of `print(categorical_layer(example_batch).numpy()[0])`. If we remove the index `[0]`, we're supposed to get the one-hot encoding of the categorical features, i.e., a 5-by-20 matrix, where 5 is the batch size and 20 is the total dimensionality of all categorical features.\r\n\r\nIf, for the sake of reproducibility, we also set `shuffle=False` in the call to `tf.data.experimental.make_csv_dataset()` at the very top of the notebook, the matrix we then get is:\r\n\r\n```\r\n[[0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\r\n [0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\r\n [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\r\n [0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\r\n [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]]\r\n```\r\n\r\nThis does not match up with the input categorical features for that batch, namely\r\n\r\n```\r\nsex: [b'male' b'female' b'female' b'female' b'male']\r\nclass: [b'Third' b'First' b'Third' b'First' b'Third']\r\ndeck: [b'unknown' b'C' b'unknown' b'C' b'unknown']\r\nembark_town: [b'Southampton' b'Cherbourg' b'Southampton' b'Southampton' b'Queenstown']\r\nalone: [b'n' b'n' b'y' b'n' b'y']\r\n```\r\n\r\nFor example, `[b'male' b'female' b'female' b'female' b'male']` does not match up with the first two columns of the matrix.", "comments": ["Hi @laghaout,  the categorical features are plotted alphabetically.\r\nWhen you print the `categorical_layer(example_batch)` you will get the alphabetically sorted one-hot encoding.\r\n\r\nFor this the first 2 columns is the 'alone' column, then next 3 columns is for 'Class' column, and so on.\r\n\r\nHope this helps you. :) \r\n", "@laghaout As mentioned above, the columns are sorted alphabetically under the hood. let me know if the explanation helps. Thanks!", "Yes, it's clear now, thanks.\r\n\r\nIf I were to give feedback to TF, however, I'd say that this sorting \"under the hood\" can be a source of confusion since the developers will normally have a certain ordering of their features in mind. Overwriting that ordering makes it difficult to read the stacked one-hot encodings or embeddings in general. If anything, it would be better to enforce that the user provides ordered dictionaries rather than doing this sorting behind the scenes.\r\n\r\nI'll close this current issue, but if you agree with my comment above, shall I open a new issue to address it?", "Yes. I am closing this issue but you can go ahead and open a new issue related to docs."]}, {"number": 36447, "title": "Saving to file a model within TPUStrategy", "body": "On Tensorflow 2.0 and 2.1, trying to save to file a TPU trained model or a model that was even created within the scope of a `tf.distribute.experimental.TPUStrategy` yields error below. Despite throwing an `UnimplementedException` the code does create a folder on disk with some content. \r\n\r\nThe reproducible code can be found in a collab notebook here:\r\nhttps://colab.research.google.com/drive/1DOkwNlzMLsg0wQZe41eq88XhEUqR9Amn\r\n\r\n**The exception and it's stack**\r\n```python\r\n---------------------------------------------------------------------------\r\nUnimplementedError                        Traceback (most recent call last)\r\n<ipython-input-7-b3a563716de1> in <module>()\r\n     48         print(\"1.4 \",err)\r\n     49 \r\n---> 50     model.save(\"model-new.1\")\r\n     51 \r\n     52     model.compile(\r\n\r\n14 frames\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n   1006     \"\"\"\r\n   1007     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n-> 1008                     signatures, options)\r\n   1009 \r\n   1010   def save_weights(self, filepath, overwrite=True, save_format=None):\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    113   else:\r\n    114     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n--> 115                           signatures, options)\r\n    116 \r\n    117 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)\r\n     76     # we use the default replica context here.\r\n     77     with distribution_strategy_context._get_default_replica_context():  # pylint: disable=protected-access\r\n---> 78       save_lib.save(model, filepath, signatures, options)\r\n     79 \r\n     80   if not include_optimizer:\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n    914   # SavedModel proto itself.\r\n    915   utils_impl.get_or_create_variables_dir(export_dir)\r\n--> 916   object_saver.save(utils_impl.get_variables_path(export_dir))\r\n    917   builder_impl.copy_assets_to_destination_dir(asset_info.asset_filename_map,\r\n    918                                               export_dir)\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/training/tracking/util.py in save(self, file_prefix, checkpoint_number, session)\r\n   1166     file_io.recursive_create_dir(os.path.dirname(file_prefix))\r\n   1167     save_path, new_feed_additions = self._save_cached_when_graph_building(\r\n-> 1168         file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)\r\n   1169     if new_feed_additions:\r\n   1170       feed_dict.update(new_feed_additions)\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/training/tracking/util.py in _save_cached_when_graph_building(self, file_prefix, object_graph_tensor)\r\n   1114         or context.executing_eagerly() or ops.inside_function()):\r\n   1115       saver = functional_saver.MultiDeviceSaver(named_saveable_objects)\r\n-> 1116       save_op = saver.save(file_prefix)\r\n   1117       with ops.device(\"/cpu:0\"):\r\n   1118         with ops.control_dependencies([save_op]):\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/training/saving/functional_saver.py in save(self, file_prefix)\r\n    228         # _SingleDeviceSaver will use the CPU device when necessary, but initial\r\n    229         # read operations should be placed on the SaveableObject's device.\r\n--> 230         sharded_saves.append(saver.save(shard_prefix))\r\n    231 \r\n    232     with ops.control_dependencies(sharded_saves):\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/training/saving/functional_saver.py in save(self, file_prefix)\r\n     67       for spec in saveable.specs:\r\n     68         tensor_names.append(spec.name)\r\n---> 69         tensors.append(spec.tensor)\r\n     70         tensor_slices.append(spec.slice_spec)\r\n     71     with ops.device(\"cpu:0\"):\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/training/saving/saveable_object.py in tensor(self)\r\n     50   @property\r\n     51   def tensor(self):\r\n---> 52     return self._tensor() if callable(self._tensor) else self._tensor\r\n     53 \r\n     54 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/training/saving/saveable_object_util.py in f()\r\n     89         def f():\r\n     90           with ops.device(v.device):\r\n---> 91             x = v.read_value()\r\n     92             # To allow variables placed on non-CPU devices to be checkpointed,\r\n     93             # we copy them to CPU on the same machine first.\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py in read_value(self)\r\n    633     \"\"\"\r\n    634     with ops.name_scope(\"Read\"):\r\n--> 635       value = self._read_variable_op()\r\n    636     # Return an identity so it can get placed on whatever device the context\r\n    637     # specifies instead of the device where the variable is.\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py in _read_variable_op(self)\r\n    611     variable_accessed(self)\r\n    612     result = gen_resource_variable_ops.read_variable_op(self._handle,\r\n--> 613                                                         self._dtype)\r\n    614     _maybe_set_handle_data(self._dtype, self._handle, result)\r\n    615 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/gen_resource_variable_ops.py in read_variable_op(resource, dtype, name)\r\n    477         pass  # Add nodes to the TensorFlow graph.\r\n    478     except _core._NotOkStatusException as e:\r\n--> 479       _ops.raise_from_not_ok_status(e, name)\r\n    480   # Add nodes to the TensorFlow graph.\r\n    481   dtype = _execute.make_type(dtype, \"dtype\")\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6604   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6605   # pylint: disable=protected-access\r\n-> 6606   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6607   # pylint: enable=protected-access\r\n   6608 \r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nUnimplementedError: File system scheme '[local]' not implemented (file: 'model-new.1/variables/variables_temp_17ffcf98334348fd8ef1e339869f0bfc')\r\n\tEncountered when executing an operation using EagerExecutor. This error cancels all future operations and poisons their output tensors. [Op:ReadVariableOp]\r\n```\r\n\r\n\r\n**System information**\r\nThe error is reproduced in Collab so I'm skipping `tf_env_collect.sh` output.\r\n", "comments": ["You can/should use  a cloud storage bucket path. See https://cloud.google.com/tpu/docs/troubleshooting#cannot_use_local_filesystem", "@vicpara Please let us know if the above comment helps resolve issue.", "Yes it does. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36447\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36447\">No</a>\n", "Hi, I'm using a TPU on Kaggle, and I have no write permission to the GS cloud storage bucket, only the local filesystem. Is there a way around that, so I can save to the local filesystem?\r\n\r\nThanks.", "@rafwaf You can save the model with legacy h5 format to local storage (tested on Colab, TF2.7).\r\n\r\nUse `model.save(\"/path/to/model.h5\")` instead of `model.save(\"/path/to/savedModel\")`.\r\nhttps://www.tensorflow.org/guide/keras/save_and_serialize#whole-model_saving_loading\r\n\r\nWith TPU backend, you will need a GS bucket to save the model in SavedModel format.\r\n\r\nP.S. Please note that some layers are not supported in h5 format, like such: https://github.com/tensorflow/tfjs/issues/3656#issuecomment-664604710"]}, {"number": 36446, "title": "tf.keras model.fit(): enormous difference between train loss and val loss on the same data", "body": "Tensorflow version 2.1\r\n\r\nSee the colab notebook to reproduce the issue: https://drive.google.com/file/d/1Fvc6G_9v5mek015cai7qYT6HoY-fLkzk/view?usp=sharing\r\n\r\nWhen the training loss goes down the val_loss does not change, although this is exactly the same data.\r\n\r\nTrain on 2 samples, validate on 2 samples\r\nEpoch 1/30\r\n2/2 [==============================] - 3s 2s/sample - loss: 0.4630 - val_loss: 302.4763\r\nEpoch 2/30\r\n2/2 [==============================] - 1s 457ms/sample - loss: 0.8565 - val_loss: 496.9578\r\nEpoch 3/30\r\n2/2 [==============================] - 1s 457ms/sample - loss: 0.7886 - val_loss: 1050.9148\r\nEpoch 4/30\r\n2/2 [==============================] - 1s 450ms/sample - loss: 0.1080 - val_loss: 744.4895\r\nEpoch 5/30\r\n2/2 [==============================] - 1s 474ms/sample - loss: 0.1144 - val_loss: 1353.2678\r\nEpoch 6/30\r\n2/2 [==============================] - 1s 465ms/sample - loss: 0.0402 - val_loss: 3237.9683\r\nEpoch 7/30\r\n2/2 [==============================] - 1s 465ms/sample - loss: 0.0635 - val_loss: 3946.7822\r\nEpoch 8/30\r\n2/2 [==============================] - 1s 470ms/sample - loss: 0.0355 - val_loss: 4054.5461\r\nEpoch 9/30\r\n2/2 [==============================] - 1s 462ms/sample - loss: 0.0345 - val_loss: 4991.5400\r\n\r\nHow is this possible?\r\nThe code is pretty straightforward:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nbase_model = tf.keras.applications.ResNet50V2(input_shape=(180, 320, 3), weights=None, include_top=False)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\r\noutput = tf.keras.layers.Dense(8)(x)\r\nmodel = tf.keras.models.Model(inputs=base_model.input, outputs=output)\r\nmodel.compile(optimizer='adam', loss='mse')\r\ndata = np.random.rand(2, 180, 320, 3)\r\nlabels = np.random.rand(2, 8)\r\n\r\nmodel.fit(data, labels, validation_data=(data,labels), batch_size=2, epochs=30)\r\n```\r\n\r\nThere are known issues with keras and batch normalization (see for example https://github.com/keras-team/keras/issues/6977). This is probably related, but I don't see directly how. What do I have to change to make this working as expected? Is this something in the included package https://github.com/qubvel/classification_models or where to solve it?\r\n\r\nEdit: the behaviour of batch normalization is changed as of TF 2.0, so the other issues might not be related, see https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization", "comments": ["@dmus \r\n\r\nCan you please provide the access to colab link to reproduce the issue in our environment.Thanks!", "Updated colab notebook link", "@dmus \r\n\r\nI have tried in colab with TF version 2.2.0-dev20200205 and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/053d84cd47d7653c362ac6d7bc19ab04/untitled623.ipynb). Is this the expected behavior? Thanks!", "@ravikyram Thanks for having a look. I think indeed there is no bug, but this is the expected behaviour of BatchNormalization layers. It takes time before the batch statistics are similar to the statistics used in a batch, explaining the big difference between train and val loss.\r\nEventually this can be speed up by lowering the momentum property of the BatchNormalization, default is 0.99", "I'm facing the same problem. Transfer learning with Mobilenetv2, all layers from mobilenet are set as _not trainable_ (eg layer.trainable=False), rest of the network is similar to what is in issue description. Same data (tfrecord dataset) are passed in as train data and also as validation data. Training stats looks good. loss goes quickly to zero, accuracy goes quickly to 1.\r\n\r\nSo far, all as it should be. Validation stats on the other hand are broken. Validation loss goes up and validation accuracy doesn't change. Stopping training after it reaches good performance on training data and then run prediction on same data gives same crappy result as validation pass during traing (or as model.eval()). This behavior makes transfer learning unusable. One cannot train model long after training converged for a chance that training and validation stats will eventually become similar. If it really still is BatchNormalization, it should be fixed and not hidden under carpet\r\n\r\nSame model (as much as I could make it same) implemented with pytorch behaves as one would expect. Both training and validation stats are pretty much the same. No problems with prediction/evaluation of the model. Number of required epochs is reasonable (and similar to tensorflow training stats). Data fed into keras model and pytorch model were processed in almost exactly same way.\r\n\r\nI guess I'll have to run my own training loop for now and update only layers added on top of mobilenet (and hope it will help). Is there some workaround except _just let it run wor 500 more epochs_?\r\n\r\n@dmus how is this not a problem? How are you training your models?\r\n\r\nedit:\r\ntraining progression\r\n3/3 [==============================] - 2s 643ms/step - loss: 3.8230 - sparse_categorical_accuracy: 0.0857 - val_loss: 3.2082 - val_sparse_categorical_accuracy: 0.0857\r\nEpoch 2/1000\r\n3/3 [==============================] - 1s 266ms/step - loss: 0.7137 - sparse_categorical_accuracy: 0.6857 - val_loss: 4.1043 - val_sparse_categorical_accuracy: 0.1143\r\nEpoch 3/1000\r\n3/3 [==============================] - 1s 261ms/step - loss: 0.2898 - sparse_categorical_accuracy: 0.9143 - val_loss: 5.7666 - val_sparse_categorical_accuracy: 0.1143\r\nEpoch 4/1000\r\n3/3 [==============================] - 1s 258ms/step - loss: 0.0327 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.8659 - val_sparse_categorical_accuracy: 0.1143\r\nEpoch 5/1000\r\n3/3 [==============================] - 1s 259ms/step - loss: 0.0124 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.7208 - val_sparse_categorical_accuracy: 0.1143\r\nEpoch 6/1000\r\n3/3 [==============================] - 1s 252ms/step - loss: 0.0068 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.2006 - val_sparse_categorical_accuracy: 0.1143\r\nEpoch 7/1000\r\n3/3 [==============================] - 1s 259ms/step - loss: 0.0044 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.5022 - val_sparse_categorical_accuracy: 0.1143\r\nEpoch 8/1000\r\n3/3 [==============================] - 1s 256ms/step - loss: 0.0028 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.6324 - val_sparse_categorical_accuracy: 0.1143\r\nEpoch 9/1000\r\n3/3 [==============================] - 1s 259ms/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.7268 - val_sparse_categorical_accuracy: 0.1143\r\nEpoch 10/1000\r\n3/3 [==============================] - 1s 254ms/step - loss: 0.0013 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.8189 - val_sparse_categorical_accuracy: 0.1143", "I've noticed similar behavior as @ccg1415 with a  pre-trained ResNet, the validation loss skyrocket through the roof", "Closing this issue for now since the issue author is aligned. However feel free to post a new issue if still have problems. Thanks you."]}, {"number": 36445, "title": "TensorFlow lite on armv6 (Raspberry Zero) crashes wit SIGILL", "body": "sudo apt-get update\r\nsudo apt-get install crossbuild-essential-armhf\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd ./tensorflow\r\ngit checkout r2.0\r\n./tensorflow/lite/tools/make/download_dependencies.sh\r\n\r\nCC_PREFIX=arm-linux-gnueabihf- make -j 8 -f ./tensorflow/lite/tools/make/Makefile TARGET=rpi TARGET_ARCH=armv6l\r\n\r\nTry running on Raspberry Pi Zero the example minimal - it crashes with Invalid Instruction\r\n/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6l/bin/minimal\r\n\r\n", "comments": ["Could you try it again with TARGET_ARCH=armv6 ?\r\nAccording to tensorflow/lite/tools/make/targets/rpi_makefile.inc, \"armv6l\" won't work.", "In file included from tensorflow/lite/core/api/error_reporter.cc:15:0:\r\n./tensorflow/lite/core/api/error_reporter.h: In destructor \u2018virtual tflite::ErrorReporter::~ErrorReporter()\u2019:\r\n./tensorflow/lite/core/api/error_reporter.h:37:28: sorry, unimplemented: Thumb-1 hard-float VFP ABI\r\n   virtual ~ErrorReporter() {}", "Are you seeing #36661 ? Please refer it.\r\nIt seems that SIGILL issue was gone.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36445\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36445\">No</a>\n", "There must be additional setup because build fails with:\r\nsorry, unimplemented: Thumb-1 hard-float VFP ABI\r\nOS: Ubuntu 18.04 LTS\r\n", "I've tested both 2.0 branch and 2.1 but I can't find any build issue on cross build.\r\n\r\n@bvarbanov did you try clean build?\r\n\r\nCC_PREFIX=arm-linux-gnueabihf- make -j 8 -f ./tensorflow/lite/tools/make/Makefile TARGET=rpi TARGET_ARCH=armv6l clean\r\nCC_PREFIX=arm-linux-gnueabihf- make -j 8 -f ./tensorflow/lite/tools/make/Makefile TARGET=rpi TARGET_ARCH=armv6l", "Try now with armv6 (no trailing l)\r\n\r\nCC_PREFIX=arm-linux-gnueabihf- make -j 8 -f ./tensorflow/lite/tools/make/Makefile TARGET=rpi TARGET_ARCH=armv6\r\n\r\nI get compile errors: \r\n/usr/arm-linux-gnueabihf/include/stdlib.h:361:1: sorry, unimplemented: Thumb-1 hard-float VFP ABI\r\n", "@bvarbanov you should use armv6l. What's the result of using armv6l?", "Oh i was wrong. You're right.\r\nYou should use armv6. Now I can reproduce the issue. Let me dig more.", "there is a tricky part with raspberry pi zero - the similar bug has been reported to WebRTC and abseil teams... Nobody has done anything so far about it - but it's a result of incorrectly generated floating point instructions...\r\n\r\nI can reproduce some of the crashes using cross-compiled program like thins:\r\nint main()\r\n{\r\n  long long a = 1;\r\n  auto b = static_cast<double>(a); // <--- vmov instruction here goes boom\r\n  return 0;\r\n}\r\n\r\nI have this crash also using cross-compiling with clang:\r\n\r\nmy toolchain cmake file has following options targeting raspberry zero (cpu arm1176jzf-s)\r\n\r\nSET(triple \"arm-linux-gnueabihf\")\r\nadd_compile_options(-target ${triple})\r\nadd_compile_options(-march=armv6zk)\r\nadd_compile_options(-mfpu=vfp)\r\nadd_compile_options(-mcpu=arm1176jzf-s)\r\nadd_compile_options(-mfloat-abi=hard)\r\nadd_compile_options(-Wall)\r\nadd_compile_options(-fPIC)\r\n", "Could you try this?\r\n\r\nhttps://github.com/terryheo/tensorflow/commit/9e4de36c24ecb84af1ac77cb9d6c46f5d885313b\r\n", "Build is fixed. On raspberry pi zero - resulting executable is still crashing \r\n  \r\npi@pi4:~/prob $ ls\r\nbenchmark_model  benchmark_model_performance_options  minimal\r\npi@pi4:~/prob $ ./minimal\r\nSegmentation fault\r\npi@pi4:~/prob $ ./benchmark_model\r\nSegmentation fault\r\npi@pi4:~/prob $ ./benchmark_model_performance_options\r\nIllegal instruction\r\npi@pi4:~/prob $\r\npi@pi4:~/prob $ gdb -q ./minimal\r\nReading symbols from ./minimal...done.\r\n(gdb) start\r\nTemporary breakpoint 1 at 0x5a2c\r\nStarting program: /home/pi/prob/minimal\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/arm-linux-gnueabihf/libthread_db.so.1\".\r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n0x004557f4 in std::pair<int, int> tflite::ops::builtin::tile::(anonymous namespace)::TileOneDimension<int, int>(TfLiteIntArray const&, int const*, int const*, int*, int) ()\r\n\r\n", "I have if fixed:\r\n\r\nFor rasbperry pi zero:\r\nsudo apt-get install crossbuild-essential-armel\r\n\r\nModify the rpi_makefile.inc for rasbperry pi zero to have proper toolchain prefix:\r\nTARGET_TOOLCHAIN_PREFIX := arm-linux-gnueabi-\r\n\r\n", "Thanks for the information. Let me fix the script.", "Fix was merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36445\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36445\">No</a>\n"]}, {"number": 36444, "title": "Removed non-existant `arm_cmplx_mag_squared_q10p6.c` from `micro_spee\u2026", "body": "`arm_cmplx_mag_squared_q10p6.c` does not exist in CMSIS, error reported #35889.\r\n\r\nThe inclusion of this file in ` tensorflow/lite/micro/examples/micro_speech/CMSIS/Makefile.inc`\r\ncauses the example build `make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project` to fail. \r\n\r\nBuilds successfully without file. Not tested on actual hardware. ", "comments": ["Remove the header file as well -- https://github.com/tensorflow/tensorflow/blob/5b0fb79512c177f54b30c51ea10a77b37a17083a/tensorflow/lite/micro/examples/micro_speech/CMSIS/Makefile.inc#L21", "@petewarden could look into this and approve this change if the fix looks good? This would fix another issue: https://github.com/tensorflow/tensorflow/issues/35889", "@alxhoff Can you please check @Dasch0's comments and keep us posted. Thanks!", "@alxhoff  Any update on this PR? Please. Thanks!", "Sorry for the delay, was at a conference last week. I have removed the line mentioned by @Dasch0 in the original PR. Everything builds for me."]}, {"number": 36443, "title": "[TFLite int16] Requantize node for the case: 16-bit activations and 8-bit weights", "body": "This PR is one of steps to extend 8-bit quantization to support symmetric 16-bit activations.\r\n\r\nEach activation is of type int16 and symmetric around zero. The weight tensor precision remains at 8-bit signed values. The bias is set to int64 precision.\r\n\r\nIn this PR we enable 16-bit version of Requantize Operator.", "comments": []}, {"number": 36442, "title": "eager", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": []}, {"number": 36441, "title": "ModuleNotFoundError: No module named 'tensorflow_core.python'", "body": "**System information**\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7.4\r\n\r\n\r\nTensorflow is installed on the computer. When I try to import (import tensorflow as tf) I get this error\r\n\r\nModuleNotFoundError: No module named 'tensorflow_core.python'\r\n\r\nWhat's causing this error? I've tried downgrading TF to below version 2 and still received the error.", "comments": ["Please fill in the issue template. Also, please post output of `pip freeze` using ` ``` ` around computer generated messages (so the formatting is preserved)", "This is the output of pip freeze\r\n\r\n```\r\nabsl-py==0.9.0\r\nastor==0.8.1\r\nattrs==19.3.0\r\nbackcall==0.1.0\r\nbeautifulsoup4==4.8.2\r\nbleach==3.1.0\r\nblis==0.4.1\r\nboto==2.49.0\r\nboto3==1.11.9\r\nbotocore==1.14.9\r\nbs4==0.0.1\r\nbz2file==0.98\r\ncachetools==4.0.0\r\ncatalogue==1.0.0\r\ncertifi==2019.11.28\r\ncffi==1.13.2\r\nchardet==3.0.4\r\ncolorama==0.4.3\r\nconda==4.8.2\r\nconda-package-handling==1.6.0\r\ncryptography==2.8\r\ncymem==2.0.3\r\nCython==0.29.14\r\ndecorator==4.4.1\r\ndefusedxml==0.6.0\r\ndocutils==0.15.2\r\nentrypoints==0.3\r\ngast==0.2.2\r\ngensim==3.8.1\r\ngoogle-auth==1.11.0\r\ngoogle-auth-oauthlib==0.4.1\r\ngoogle-pasta==0.1.8\r\ngrpcio==1.26.0\r\nh5py==2.10.0\r\nidna==2.8\r\nimportlib-metadata==1.4.0\r\nipykernel==5.1.3\r\nipython==7.11.1\r\nipython-genutils==0.2.0\r\nipywidgets==7.5.1\r\njedi==0.15.2\r\nJinja2==2.10.3\r\njmespath==0.9.4\r\njoblib==0.14.1\r\njsonschema==3.2.0\r\njupyter==1.0.0\r\njupyter-client==5.3.4\r\njupyter-console==6.1.0\r\njupyter-core==4.6.1\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.0\r\nlxml==4.4.2\r\nMarkdown==3.1.1\r\nMarkupSafe==1.1.1\r\nmenuinst==1.4.16\r\nmistune==0.8.4\r\nmkl-service==2.3.0\r\nmore-itertools==8.1.0\r\nmurmurhash==1.0.2\r\nnbconvert==5.6.1\r\nnbformat==5.0.4\r\nnltk==3.4.5\r\nnotebook==6.0.3\r\nnumpy==1.17.5\r\noauthlib==3.1.0\r\nopt-einsum==3.1.0\r\npandas==0.25.3\r\npandocfilters==1.4.2\r\nparso==0.5.2\r\npickleshare==0.7.5\r\nplac==1.1.3\r\npreshed==3.0.2\r\nprometheus-client==0.7.1\r\nprompt-toolkit==3.0.2\r\nprotobuf==3.11.2\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\npycosat==0.6.3\r\npycparser==2.19\r\nPygments==2.5.2\r\npyOpenSSL==19.1.0\r\npyrsistent==0.15.7\r\nPySocks==1.7.1\r\npython-dateutil==2.8.1\r\npytz==2019.3\r\npywin32==225\r\npywinpty==0.5.7\r\npyzmq==18.1.1\r\nqtconsole==4.6.0\r\nrequests==2.22.0\r\nrequests-oauthlib==1.3.0\r\nrsa==4.0\r\nruamel-yaml==0.15.80\r\ns3transfer==0.3.2\r\nscikit-learn==0.22.1\r\nscipy==1.4.1\r\nSend2Trash==1.5.0\r\nsix==1.14.0\r\nsklearn==0.0\r\nsmart-open==1.9.0\r\nsoupsieve==1.9.5\r\nspacy==2.2.3\r\nsrsly==1.0.1\r\ntensorflow==2.1.0\r\ntensorflow-estimator==2.1.0\r\ntensorflow-gpu-estimator==2.1.0\r\ntensorflow-hub==0.7.0\r\ntermcolor==1.1.0\r\nterminado==0.8.3\r\ntestpath==0.4.4\r\nthinc==7.3.1\r\ntorch==1.3.1\r\ntornado==6.0.3\r\ntqdm==4.41.1\r\ntraitlets==4.3.3\r\nurllib3==1.25.7\r\nwasabi==0.6.0\r\nwcwidth==0.1.8\r\nwebencodings==0.5.1\r\nWerkzeug==0.16.0\r\nwidgetsnbextension==3.5.1\r\nwin-inet-pton==1.1.0\r\nwincertstore==0.2\r\nwrapt==1.11.2\r\nzipp==2.0.1\r\n```", "Where can I find issue template", "The issue template is the stuff you see already there when you fill in the issue for the first time. You can also see it at https://github.com/tensorflow/tensorflow/tree/master/.github/ISSUE_TEMPLATE\r\n\r\nYou forgot to add ` ``` ` before and after the `pip freeze` dump above. I went ahead and added it for you, but please do use proper markdown in the future.", "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\nTensorFlow installed from (source or binary): conda install?\r\nTensorFlow version (use command below): 2.1.0\r\nPython version: 3.7.4\r\nBazel version (if compiling from source): no\r\nGCC/Compiler version (if compiling from source): no\r\nCUDA/cuDNN version: no\r\nGPU model and memory: intel 620, 3.9 GB\r\nYou can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\" 2. TF 2.0: python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n\r\nDescribe the current behavior: No module named 'tensorflow_core.python' \r\n\r\nDescribe the expected behavior: imports without error\r\n\r\nCode to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem.: import tensorflow\r\n\r\nOther info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "Can you try the following?\r\n\r\n```\r\nimport tensorflow_core as tfc\r\nprint(tfc.__path__)\r\n```", "```Traceback (most recent call last):\r\n  File \"c:\\users\\myname\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-12-9f0e6ea0081c>\", line 1, in <module>\r\n    import tensorflow_core as tfc\r\n  File \"C:\\Users\\myname\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\myname\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\myname\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"c:\\users\\myname\\appdata\\local\\continuum\\miniconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named 'tensorflow_core.python'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\myname\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2034, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'ModuleNotFoundError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\myname\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"c:\\users\\myname\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"c:\\users\\myname\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"c:\\users\\myname\\appdata\\local\\continuum\\miniconda3\\lib\\inspect.py\", line 1502, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"c:\\users\\myname\\appdata\\local\\continuum\\miniconda3\\lib\\inspect.py\", line 1460, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"c:\\users\\myname\\appdata\\local\\continuum\\miniconda3\\lib\\inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"c:\\users\\myname\\appdata\\local\\continuum\\miniconda3\\lib\\inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"C:\\Users\\myname\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\myname\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"c:\\users\\myname\\appdata\\local\\continuum\\miniconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"C:\\Users\\myname\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\myname\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\myname\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"c:\\users\\myname\\appdata\\local\\continuum\\miniconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named 'tensorflow_core.python'```\r\n", "I get the same error as before with trying to import tf when I try what you posted", "Facing this same issue", "@mshmsh1234 please reformat your dump of error message to start and end with ` ``` ` so the formatting is preserved. As it is now, it is unreadable.", "I was trying to package a python app, which uses tensorflow, using PyInstaller. That's where I faced this same issue. I'm using tensorflow-cpu==1.15. Running the python file using terminal works just fine. Not sure if this an issue with Tensorflow or with PyInstaller.\r\n\r\n```(packer) C:\\Users\\harsh\\Desktop\\dtoxd-demo\\dist\\server>server.exe\r\nTraceback (most recent call last):\r\n  File \"server.py\", line 2, in <module>\r\n  File \"c:\\users\\harsh\\anaconda3\\envs\\packer\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 623, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"site-packages\\tensorflow\\__init__.py\", line 99, in <module>\r\n  File \"c:\\users\\harsh\\anaconda3\\envs\\packer\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 623, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"site-packages\\tensorflow_core\\__init__.py\", line 28, in <module>\r\n  File \"site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n  File \"site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n  File \"importlib\\__init__.py\", line 126, in import_module\r\nImportError: No module named 'tensorflow_core.python'\r\n[14776] Failed to execute script server```\r\n", "I think you should downgrade tensorflow 1.14", "> I think you should downgrade tensorflow 1.14\r\n\r\nThis is a terrible advice. First, because TF 1.14 is no longer receiving new features and new security updates. Second, because it just hides the error under the rug.", "Downgrading doesn't help to make TF usable/the error go away, regardless", "Cant package tensorflow=1.15 with Pyinstaller because of the same error.\r\nIs there any update on this?\r\nPS: I am using pyinstaller to package the dependencies.\r\n\r\n<img width=\"642\" alt=\"Screenshot 2020-02-11 at 9 26 18 PM\" src=\"https://user-images.githubusercontent.com/12194719/74253532-32ca6b80-4d15-11ea-8ad6-f734430659eb.png\">\r\n\r\n", "@mshmsh1234 You said you tried to downgrade below TF 2.0 but your `pip freeze` output still lists TF 2.1.\r\n\r\nProbably your environment is in a bad state. Can you please make sure you clean it? Using `virtualenv` is recommended.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36441\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36441\">No</a>\n", "Please also say how the issue got fixed so other people searching for their issue and reaching here can use the same fix if their issue is similar.", "> Please also say how the issue got fixed so other people searching for their issue and reaching here can use the same fix if their issue is similar.\r\n\r\nI switched computers at work and this issue stop occurring on my new computer. Unfortunately can't offer a solution for others to replicate.", "@aayusharora  I had to switch to TF1.14 and that ended up creating a lot more problems because Ragged tensors were introduced in TF1.15. I then had to convert that model to be compatible to TF1.14", "[As said](https://github.com/tensorflow/tensorflow/issues/36441#issuecomment-583480775), switching to 1.14 was a terrible advice.", "I understand that very well. But sometimes, terrible option is better than no option at all. ", "So it means TF 1.15 can not packaged by pyinstaller right now? i face the same problem, too.", "I'm running into the same issue with TF 2.2.0rc3\r\nAre there any fixes for this?", "Locking conversation as original issue has been solved.\r\n\r\nPlease make sure `pip list` has `tensorflow`, `tensorboard` and `tensorflow_estimator` at the same (major,minor) version and no nightly packages installed. If that does not happen, please uninstall all packages and install again / install in a new virtualenv.\r\n\r\nNote that:\r\n* `tensorflow_core` only exists in 1.15, 2.0 and 2.1\r\n* `tensorflow.contrib` only exists in 1.15\r\n* tensorflow before 1.15 is no longer supported so it's recommended to not downgrade to it.\r\n\r\nIf after following all of the above there is still a problem, please open a new issue"]}, {"number": 36440, "title": "Dockerfile: Don't commit pip cache to Docker image", "body": "Using [dive](https://github.com/wagoodman/dive) on\r\ntensorflow/tensorflow:latest-gpu-py3 shows that /root/.cache/pip takes\r\nup more than 450MB of space. This commit forces pip to not use a cache\r\nwhen installing tensorflow in the Docker image(s).", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36440) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36440) for more info**.\n\n<!-- ok -->", "Thanks, good suggestion. Can you follow the directions [in the dockerfiles/ directory](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/README.md) to regenerate the dockerfiles, please?", "@angerson Whoops, missed that README. Thanks for pointing it out! I'll follow the instructions and report back.", "@angerson I ran `assembler.py --release dockerfiles --construct_dockerfiles`. Please see my updated commit."]}, {"number": 36439, "title": "Define TensorRT network with dynamic shapes", "body": "This PR builds enables TRT network creation using dynamic shapes. \r\n\r\nDynamic shapes can be only used in the explicit batch mode (experimental). This PR only implements TensorRT network definition using dynamic shapes. To actually build the engine and run inference, one needs to specify optimization profiles. That is implemented in the next PR. Therefore TRT Engine creation is expected to fail if explicit batch mode is selected and the input tensors have unknown dimensions. This is tested in [trt_engine_op_test](https://github.com/tfeher/tensorflow/blob/830cf9fabd3149dff625a66d9a80ab2b64f10176/tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op_test.cc#L205).\r\n\r\nThis PR is not supposed to change any existing behavior: inference in implicit batch mode, or in explicit mode with known (static) shapes should still work as before.\r\n\r\n**Note**\r\nWith the introduction of dynamic shapes, TF-TRT will have two different strategies to handle input with different shapes. (Confusingly the doc sometimes refers both as dynamic shapes). From the user point of view they are similar: the aim is to use TensorRT inference for inputs whose shape is unknown at the time when we call trt.convert().\r\n\r\n- The existing strategy is dynamic engine creation. It means that TensorRT engines can be created during runtime to handle different input shapes. \r\nhttps://github.com/tensorflow/tensorflow/blob/97b4c2c413d9c880e78e4d2616500cb30b773203/tensorflow/python/compiler/tensorrt/trt_convert.py#L1210-L1221\r\nhttps://github.com/tensorflow/tensorflow/blob/97b4c2c413d9c880e78e4d2616500cb30b773203/tensorflow/python/compiler/tensorrt/trt_convert.py#L136-L142\r\n Dynamic engine creation is the only supported ongine creation in TF v2. See the relevant doc: \r\nhttps://github.com/tensorflow/tensorflow/blob/97b4c2c413d9c880e78e4d2616500cb30b773203/tensorflow/python/compiler/tensorrt/trt_convert.py#L858-L859\r\n\r\n- This PR introduces dynamic input shapes (supported since TensorRT version 6), which is another way to deal with input data that has a shape not known at conversion time: we can define the TensorRT network with unknown input shapes. This way a single engine can handle input with different sizes. We still need to know the expected range of each input dimension (shape optimization profiles) at engine creation time. \r\n", "comments": []}, {"number": 36438, "title": "Build tensorflow error with keras api", "body": "### I build tensorflow follow [link](https://www.tensorflow.org/install/source)\r\n1. use r2.0\r\n1. bazel 0.25.0\r\n1. CUDA Version: 10.0 \r\n1. ubuntu 18.04\r\nHere my error\r\n```\r\nERROR: /root/tensorflow/tensorflow/python/keras/api/BUILD:13:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 83, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/keras/__init__.py\", line 32, in <module>\r\n    from tensorflow.python.keras import datasets\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/keras/datasets/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.keras.datasets import imdb\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/keras/datasets/imdb.py\", line 25, in <module>\r\n    from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/keras/preprocessing/__init__.py\", line 21, in <module>\r\n    import keras_preprocessing\r\nModuleNotFoundError: No module named 'keras_preprocessing'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 11062.314s, Critical Path: 746.52s\r\nINFO: 17931 processes: 17931 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "comments": ["Are you sure you ran `pip install -U --user keras_preprocessing --no-deps`?  That is included in the instructions you linked. ", "Yes. I sure\r\n```\r\nInstalling collected packages: keras-preprocessing\r\nSuccessfully installed keras-preprocessing-1.1.0\r\n\r\n```", "Can you run `pip show keras-preprocessing` and does the value in `Location`  match the value in `.tf_configure.bazelrc` for `build --action_env PYTHON_LIB_PATH` ?", "### here is it\r\n```\r\nWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\r\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\r\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\r\nName: Keras-Preprocessing\r\nVersion: 1.1.0\r\nSummary: Easy data preprocessing and data augmentation for deep learning models\r\nHome-page: https://github.com/keras-team/keras-preprocessing\r\nAuthor: Keras Team\r\nAuthor-email: None\r\nLicense: MIT\r\nLocation: /root/.local/lib/python3.6/site-packages\r\nRequires: numpy, six\r\nRequired-by: \r\n```", "And `cat .tf_configure.bazelrc` shows what?\r\n\r\nWhen you ran `./configure`\r\n\r\nFor `Please input the desired Python library path to use.` the answer should have been `/root/.local/lib/python3.6/site-packages` as that is the location where the python package are installed.", "Running into the same error\r\n Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed \r\n \r\nAditionally there is an import error, might be the same issue\r\nImportError: cannot import name 'gen_checkpoint_ops' from 'tensorflow.python.ops' (/private/var/tmp/_bazel_nikhilchitlurnavakiran/ed132e8d3b760bae902e7d17da8676dc/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/ops/__init__.py)\r\n\r\ngoing through the above discussion have listed the commands to verify\r\n\r\n**pip3 install -U --user keras_preprocessing --no-deps** \r\nWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\r\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\r\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\r\nRequirement already up-to-date: keras_preprocessing in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.1.0)\r\n\r\n**pip3 show keras-preprocessing**                        \r\nWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\r\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\r\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\r\nName: Keras-Preprocessing\r\nVersion: 1.1.0\r\nSummary: Easy data preprocessing and data augmentation for deep learning models\r\nHome-page: https://github.com/keras-team/keras-preprocessing\r\nAuthor: Keras Team\r\nAuthor-email: None\r\nLicense: MIT\r\nLocation: /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages\r\nRequires: six, numpy\r\nRequired-by: tensorflow\r\n\r\n**cat .tf_configure.bazelrc**\r\nbuild --action_env PYTHON_BIN_PATH=\"/Library/Frameworks/Python.framework/Versions/3.7/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages\"\r\nbuild --python_path=\"/Library/Frameworks/Python.framework/Versions/3.7/bin/python3\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --config=xla\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu,-nomac,-no_mac\r\ntest --build_tag_filters=-gpu,-nomac,-no_mac\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n", "@phamkhactu,\r\nAny updates regarding this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36438\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36438\">No</a>\n", "What is it with this bizarre coupling of keras to tensorflow (especially in TF 2.0)? TensorFlow should just be TensorFlow... 2.0 didn't make me use keras anymore than I did before. It just made me start using pytorch."]}, {"number": 36437, "title": "regarding ckpt.meta file", "body": "hi,i am doing my finetuning in tf 2.1.Since i am using bert model for finetuning,it produces checkpoints, I have .index,.data but i didn'T GET .meta file in checkpoint folder,since i am finetuning in tf 2.1.\r\nI need to run this mentioned down code in tensorflow verion 2,but this mentioned down code is presently in tf 1 version:(This code is basically for down sizing the model size by removing the trainable parameters)\r\n```import tensorflow as tf\r\nimport os\r\n#path that contains all 3 ckpt files of your fine-tuned model\r\npath = './bert_output'\r\n#path to output the new optimized model\r\noutput_path = os.path.join(path, 'optimized_model')\r\nsess = tf.Session()\r\nimported_meta = tf.train.import_meta_graph(os.path.join(path, 'model.ckpt-236962.meta')) #based on the steps of your fine-tuned model\r\nimported_meta.restore(sess, os.path.join(path, 'model.ckpt-236962')) #based on the steps of your fine-tuned model\r\nmy_vars = []\r\nfor var in tf.all_variables():\r\n    if 'adam_v' not in var.name and 'adam_m' not in var.name:\r\n        my_vars.append(var)\r\nsaver = tf.train.Saver(my_vars)\r\n\r\n\r\n", "comments": ["please reply", "@divyag11, Please take a look at [Tensorflow bert model](https://github.com/google-research/bert). Thanks", "ok", "@divyag11, Closing as it is resolved. Please feel free to open if still issue persists.Thanks ", "I've fine tuned a bunch of different networks at this point from the TF 2 Model zoo and none of them give me ckpt.meta files"]}, {"number": 36436, "title": "Keras.gradients() returns None in loss function", "body": "Good day and thank you for TensorFlow :-)\r\n\r\nI'm trying to implement a Wasserstein GAN with gradient penalty in TF2 (https://arxiv.org/pdf/1704.00028.pdf)\r\nWhen I compute the gradient of the discriminator output with respect to the interpolated input, I always get `None`.\r\nI have tried to compute the gradient in the layers of my model and in my loss function, in eager mode or not, using a `GradientTape` and I have checked that my discriminator has gradients activated.\r\nTo reproduce the issue, I have adapted the implementation [here]( https://github.com/kongyanye/cwgan-gp/blob/master/cwgan_gp.py) to work with TensorFlow as backend and I get exactly the same problem. (Link to adapted version below)\r\n\r\nWhat must I do to get this to work?\r\n\r\nThank you very much :-)\r\n\r\n**System information**\r\n```\r\n== check python ===================================================\r\npython version: 3.7.4\r\npython branch: v3.7.4\r\npython build version: ('v3.7.4:e09359112e', 'Jul  8 2019 14:54:52')\r\npython compiler version: Clang 6.0 (clang-600.0.57)\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Darwin\r\nos kernel version: Darwin Kernel Version 18.7.0: Sun Dec  1 18:59:03 PST 2019; root:xnu-4903.278.19~1/RELEASE_X86_64\r\nos release version: 18.7.0\r\nos platform: Darwin-18.7.0-x86_64-i386-64bit\r\nlinux distribution: ('', '', '')\r\nlinux os distribution: ('', '', '')\r\nmac version: ('10.14.6', ('', '', ''), 'x86_64')\r\nuname: uname_result(system='Darwin', node='MBP-van-Michel', release='18.7.0', version='Darwin Kernel Version 18.7.0: Sun Dec  1 18:59:03 PST 2019; root:xnu-4903.278.19~1/RELEASE_X86_64', machine='x86_64', processor='i386')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 10.0.1 (clang-1001.0.46.4)\r\nTarget: x86_64-apple-darwin18.7.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== check pips ===================================================\r\nnumpy                    1.18.0    \r\nprotobuf                 3.11.2    \r\ntensorflow               2.1.0     \r\ntensorflow-datasets      1.3.2     \r\ntensorflow-estimator     2.1.0     \r\ntensorflow-metadata      0.15.2    \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.1.0\r\ntf.version.GIT_VERSION = v2.1.0-rc2-17-ge5bf8de410\r\ntf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./env.sh: line 147: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.1.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 7, 4, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\npython src/cwan_gp.py\r\n2020-02-03 14:37:17.446341: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fef2a728470 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-02-03 14:37:17.446368: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTensor(\"loss/model_1_2_loss/gradients/model_1_2/flatten_3/Reshape_grad/Reshape:0\", shape=(32, 28, 28, 1), dtype=float32)\r\nNone\r\nTraceback (most recent call last):\r\n  File \"src/cwan_gp.py\", line 302, in <module>\r\n    wgan.train()\r\n  File \"src/cwan_gp.py\", line 232, in train\r\n    d_loss = self.critic_model.train_on_batch([imgs, labels, noise], [valid, fake, dummy])\r\n  File \"/Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1078, in train_on_batch\r\n    standalone=True)\r\n  File \"/Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 433, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"/Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n\r\n    /Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py:305 train_on_batch  *\r\n        outs, total_loss, output_losses, masks = (\r\n    /Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py:253 _process_single_batch\r\n        training=training))\r\n    /Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py:167 _model_loss\r\n        per_sample_losses = loss_fn.call(targets[i], outs[i])\r\n    /Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/losses.py:221 call\r\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\r\n    src/cwan_gp.py:123 gradient_penalty_loss\r\n        gradients_sqr = K.square(gradients)\r\n    /Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:2162 square\r\n        return math_ops.square(x)\r\n    /Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py:9965 square\r\n        \"Square\", x=x, name=name)\r\n    /Users/michel/workspace/tf-text2image/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py:486 _apply_op_helper\r\n        (input_name, err))\r\n\r\n    ValueError: Tried to convert 'x' to a tensor and failed. Error: None values not supported.\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe computed gradient is not None\r\n\r\n**Code to reproduce the issue**\r\nThis is a minimal example to reproduce the error\r\nhttps://github.com/MichelHalmes/tf-text2image/blob/72df135786c7273fb2e0bea27a30ab60024e92b2/src/cwan_gp.py\r\n\r\n**Other info / logs**\r\nThis person belivees that doing higher order gradients is only possible with theano: https://github.com/LuEE-C/WGAN-GP-with-keras-for-text/blob/master/README.md\r\n\r\nThis person seems to have had the same issue about a year ago : https://stackoverflow.com/questions/54076901/k-gradients-returning-none\r\n\r\n\r\n", "comments": ["Was able to reproduce the issue with tf 2.1. \r\nPlease see the gist [here](https://colab.research.google.com/gist/gadagashwini/0bc164937ebbb17b1b33f4ca7e23eff6/36436.ipynb). Thanks!", "Hi @jvishnuvardhan,\r\nIs there any update on this issue? :-)", "Just an update:\r\nI managed to get around my issue by converting the `interpolated_image` into a `tf.Variable()`.\r\nI compute the gradient outside the Model using a `GradientTape`\r\n\r\nIn the example above however, since the gradient is computed as part of the model, get the following issue:\r\n```\r\nValueError: Tensor-typed variable initializers must either be wrapped in an init_scope or callable (e.g., `tf.Variable(lambda : tf.truncated_normal([10, 40]))`) when building functions. Please file a feature request if this restriction inconveniences you.\r\n```\r\nI tried both suggested solutions without success.\r\n\r\nThis unblocks me, but I'm still surprised that this doesn't work.\r\n\r\nThanks for your help and for TF-2 \ud83d\udc4d ", "Hello @MichelHalmes,\r\nI'm having the exact same issue trying to implement improved wgan for 1D data. \r\n> Just an update:\r\n> I managed to get around my issue by converting the `interpolated_image` into a `tf.Variable()`.\r\n> I compute the gradient outside the Model using a `GradientTape`\r\n\r\nDid this worked for you? Did you come to other solution? \r\nThank you!\r\n\r\n", "@pabloi09 I'm having the exact same issue with the exact same task. \r\nDid you manage to solve it? Regards.", "Hi! I suggest you to create a custom model class the way it can be done via tensorflow 2. <a href=\"https://stackoverflow.com/questions/61058119/implementing-gradient-penalty-loss-with-tensorflow-2\">Here</a> I talked about the resources I followed up. I did not get good results for 1D data btw. \r\n\r\nBest regards!\r\n\r\n", "Hi @pabloi09, Hi @Matesanz,\r\n\r\nYes, I managed to get my WGAN-GP to work.\r\nIt is not the most efficient (ie a part of this has to run eagerly) but it works.\r\nHere is where I compute the gradient penalty:\r\nhttps://github.com/MichelHalmes/tf-text2image/blob/master/src/losses.py#L88\r\n\r\nIt gets used here:\r\nhttps://github.com/MichelHalmes/tf-text2image/blob/master/src/entrypoints/train_gan.py#L72\r\n\r\nHope this helps :-D\r\n\r\nPs: I still think this is an issue in Keras, since it should be possible to compute a gradient within a Keras layer, and which on top should be able to execute in graph-mode", "@MichelHalmes \r\nIf the issue is resolved, please feel free to move this issue to closed status.", "Why is this closed? Isn't this still an issue?", "> Hi! I suggest you to create a custom model class the way it can be done via tensorflow 2. <a href=\"https://stackoverflow.com/questions/61058119/implementing-gradient-penalty-loss-with-tensorflow-2\">Here</a> I talked about the resources I followed up. I did not get good results for 1D data btw. \n> \n> Best regards!\n> \n> \n\nI could resolve it following @pabloi09 answer. ", "@MichelHalmes,\r\nCan you please confirm if your issue is resolved by the tip mentioned in [pabloi09's comment](https://github.com/tensorflow/tensorflow/issues/36436#issuecomment-620726339)? Thanks! ", "> @MichelHalmes,\n> Can you please confirm if your issue is resolved by the tip mentioned in [pabloi09's comment](https://github.com/tensorflow/tensorflow/issues/36436#issuecomment-620726339)? Thanks! \n\nYes. It was.\nRegards. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36436\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36436\">No</a>\n"]}, {"number": 36435, "title": "Add TensorRT binding size dimension specification", "body": "This PR enables binding size specification (explained below). This is a building block towards enabling inference with dynamic shapes.\r\n\r\nCurrently dynamic shapes are not used, therefore this PR does not change any existing behavior.\r\n\r\nTensorRT bindings are an array of pointers to the input and output buffers for the network. In earlier versions of TensorRT, the size of the input and output tensors had to be specified during engine creation time. Since TensorRT 6, the inputs and outputs can have dynamic shapes (unknown during engine creation time). To perform inference with dynamic shapes, we have to specify the actual dimensions before we run the inference. This is implemented in this PR.\r\n\r\n", "comments": ["Updated the PR description."]}]