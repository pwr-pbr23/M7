[{"number": 22740, "title": "why does tf.norm return a complex tensor?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMacOS 10.14\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nN/A\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\nv1.11.0-rc2-4-gc19e29306c 1.11.0\r\n- **Python version**:\r\nPython 3.6.5 :: Anaconda custom (64-bit)\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\n`tf.norm(tf.Variable(0.0, dtype=tf.complex64)) `\r\n\r\n### Describe the problem\r\nThe norm is always positive, why does it return a tensor of type tf.complex64?\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Perhaps I was too succinct. I think this is more of a bug: a norm (mathematically speaking) always returns a number that is either zero or positive. A real number in any case, certainly never having an imaginary component.\r\nI would expect `tf.norm` to cast the returned tensor to a float. ", "I think it is a convenience issue: if your computation is already expressed in the complex dtype getting a norm you cannot pass to the same ops as your complex number is likely to lead to more bugs than not, and getting the real part of a complex tensor is easy if you do want to do it."]}, {"number": 22739, "title": "AttributeError: module 'tensorflow.python.keras.engine.base_layer' has no attribute 'Layer'", "body": "# I just update my keras so my tensorflow update from 1.7 update 1.9\r\nbut I found that tensorflow can't work,I just run `from tensorflow as tf`,the error show as title.My system just following with:\r\n- ubuntu16.0.4 64bit\r\n- gtx980m 8G\r\n- intel i7 6820hk\r\n- cuDNN7\r\n- CUDA 9.0\r\nI just tried that conda create a new environment and reinstall tensorflow with conda or pip,but the result is same.Use conda install from tsinghua's mirror,tensorflow's version is tensorflow1.10,pip install's tensorflow is tensorflow1.11,Had sameone happened the same question? \r\n```\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in <module>()\r\n     38 from tensorflow.python.keras.engine import training_generator\r\n     39 from tensorflow.python.keras.engine import training_utils\r\n---> 40 from tensorflow.python.keras.engine.network import Network\r\n     41 from tensorflow.python.keras.utils.generic_utils import slice_arrays\r\n     42 from tensorflow.python.ops import array_ops\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py in <module>()\r\n     64 \r\n     65 \r\n---> 66 class Network(base_layer.Layer):\r\n     67   \"\"\"A `Network` is a composition of layers.\r\n     68 \r\n\r\nAttributeError: module 'tensorflow.python.keras.engine.base_layer' has no attribute 'Layer'\r\n\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@bleedingfight Hi, thanks for the post. You have to use cuDNN 7 with CUDA 9 for any tensorflow-gpu version from 1.5 to 1.11. I understand from your post that you are using cuDNN5.1 with CUDA 9 which is not a preferred combination inorder to use tensorflow on Ubuntu. \r\nFor more information on the version compatibility, please find **Tested build configurations** table in [this link.](https://www.tensorflow.org/install/source#tested_build_configurations)\r\n", "@harshini-gadige Sorry,I don't understand your means.I post error info in the title,My GPU's computer capacity is 5.1,I found the libcudnn.so.7.0.5 in my cuda folder.Thanks for your help.I compiled it with tensorflow1.8 but 1.8 can't work now i don't unserstand why.Three days ago,my tensorflow(1.7) is good,but i install keras throught pip,tensorflow was update 1.9,but can't work.I uninstall it and reinstall it it can't work too.", "as our internals change, sometimes we saw things like this happen as pypi does not properly clean things up. Could you Try the following:\r\n```\r\npip uninstall tensorflow\r\nrm -rf ~/anaconda3/lib/python3.6/site-packages/tensorflow\r\npip install tensorflow\r\n```", "@gunan thanks for your help,it workd now.", "Awesome, thanks for the update.\r\nI will close this issue. But we are looking into finding a workaround for the bug in pip that causes this problem.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 22738, "title": "TensorFlow High level API with Low Level API", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@ArunkumarRamanan Hi, sorry to say that we didn't understand what the problem is from the above post. Appreciate if you explain the problem clearly. Also if you are looking for Tensorflow High level and low level API information, please find it below.\r\n[Tensorflow High level and low level api](https://www.tensorflow.org/guide/)\r\n\r\nYou may also refer below link to learn C++ low level API from the test scripts of C++ API.\r\n[Find here](https://www.tensorflow.org/api_docs/cc/)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22737, "title": "Batch Jacobian seems to have troubles to pass through `tf.nn.elu` and `tf.nn softplus`", "body": "### System information\r\n- Linux Ubuntu 16.04\r\n- tensorflow 1.11.0\r\n- python3.5.2\r\n\r\n### The problem\r\nBatch Jacobian seems to have troubles to pass through `tf.nn.elu` and `tf.nn softplus`\r\n\r\n### Source code\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.parallel_for.gradients import batch_jacobian\r\ntf.set_random_seed(1000)\r\nimport os\r\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\r\nimport argparse\r\n\r\n\r\nparser = argparse.ArgumentParser(description='test jacobian', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\n\r\nparser.add_argument('--activation', '-a', default=None)\r\n\r\nargs = parser.parse_args()\r\n\r\nprint(\"using tensorflow\"+tf.__version__)\r\nact1=None\r\nif args.activation:\r\n    act1 = getattr(tf.nn, args.activation)\r\n\r\nprint(\"activation = \"+str(act1))\r\n\r\nx = tf.placeholder(tf.float32, shape=[None, 784], name='input')\r\nmy_layer = tf.layers.dense(inputs=x,\r\n                           units=10,\r\n                           activation=act1\r\n                           )\r\n\r\njac = batch_jacobian(my_layer,x)\r\n```\r\n\r\n ### Error\r\n```\r\n$ python test-jacobian-simple.py -a softplus\r\nusing tensorflow1.11.0\r\nactivation = <function softplus at 0x7f345a279840>\r\nTraceback (most recent call last):\r\n  File \"test-jacobian-simple.py\", line 29, in <module>\r\n    jac = batch_jacobian(my_layer,x)\r\n  File \"/data1/env/tf1.11.0/lib/python3.5/site-packages/tensorflow/python/ops/parallel_for/gradients.py\", line 119, in batch_jacobian\r\n    pfor_output = control_flow_ops.pfor(loop_fn, output_row_size)\r\n  File \"/data1/env/tf1.11.0/lib/python3.5/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 129, in pfor\r\n    outputs.append(converter.convert(loop_fn_output))\r\n  File \"/data1/env/tf1.11.0/lib/python3.5/site-packages/tensorflow/python/ops/parallel_for/pfor.py\", line 1077, in convert\r\n    output = self._convert_helper(y)\r\n  File \"/data1/env/tf1.11.0/lib/python3.5/site-packages/tensorflow/python/ops/parallel_for/pfor.py\", line 1216, in _convert_helper\r\n    if flags.FLAGS.op_conversion_fallback_to_while_loop:\r\n  File \"/data1/env/tf1.11.0/lib/python3.5/site-packages/absl/flags/_flagvalues.py\", line 490, in __getattr__\r\n    raise _exceptions.UnparsedFlagAccessError(error_message)\r\nabsl.flags._exceptions.UnparsedFlagAccessError: Trying to access flag --op_conversion_fallback_to_while_loop before flags were parsed.\r\n```\r\n\r\nSame error with:\r\n`python test-jacobian-simple.py -a elu`\r\n\r\nIf I run with rel;u or leaky_relu instead it works fine:\r\n`python test-jacobian-simple.py -a relu`\r\n`python test-jacobian-simple.py -a leaky_relu`\r\n\r\nAny ideas on how to fix this or how to workaround?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "All relevant info are already specified, if you think something relevant is missing I'd be happy to provide the extra info.\r\nHave I written custom code: Yes and I put it up.\r\nOS Platform and Distribution: I already specified, Ubuntu 16.04\r\nTensorFlow installed from: `pip install --upgrade tensorflow-gpu`\r\nTensorFlow version:  1.11.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: 9.0 / 7.3\r\nGPU model and memory: geforce gtx 1080 ti\r\nExact command to reproduce: `python test-jacobian-simple.py -a elu`\r\nMobile device: Laptop or computer\r\n", "@ricvo The nonlinear gradients in Jacobian computation used in `elu ` is usually much slower and the performance is not as good as `relu `most of the time. If you want to use `elu`, try to convert the input shape in your example and reduce the number of units to a much smaller number.", "I would be happy if it would be just slower in the elu gradient...\r\nNB the problem is in the creation of the graph, not even in the computation!\r\nTry the code i sent, is a very small piece of code, and it does not even initialize a session.\r\nIt is a problem with flags in the parallel_for function in tensorflow\r\n**absl.flags._exceptions.UnparsedFlagAccessError: Trying to access flag\r\n--op_conversion_fallback_to_while_loop before flags were parsed.**\r\n\r\nwith `use_pfor=False`, there does not seem to be this issue (but of course this renders the whole computation way slower since is not using the parallel for but the tf.while I think..)\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/gradients.py", "@ricvo This issue is fixed in TensorFlow nightly build. You should be able to pass elu, softplus, relu and leaky_relu now.", "@wt-huang I just tried the nightly build and it seems to work, thank you very much!", "Are all your questions answered?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 22736, "title": "s", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you."]}, {"number": 22735, "title": "Fixed the paper reference for AttentionCellWrapper.", "body": "Fixed the paper reference for AttentionCellWrapper to a more appropriate one. Because the original reference is wrong, there are various options  (e.g. attn_length, attn_size, attn_vec_size, etc) are not clearly defined, which always brings some confusion. \r\nIn fact, it should be AttentionWrapper corresponding to https://arxiv.org/abs/1409.0473 and AttentionCellWrapper corresponding to https://arxiv.org/abs/1601.06733.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@ebrevdo Could you take a look at this?", "For reference, #4427 was the issue that initiated the incorrect change.\r\n\r\nIndeed, 1601.06733 *was* the original reference and the component *was*, in fact, inspired by that paper if I remember correctly and I clearly made a mistake when changing it. So yeah, let's accept this, thank you!"]}, {"number": 22734, "title": "MPI libraries maybe located under lib64 or lib32", "body": "Searching only under lib and not also lib64 or lib32 can break MPI detection on various systems. This patch also searches under lib64 and lib32.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Signed google CLA", "@gunan Could you take a look at this?", "added github user name to signed cla", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 22733, "title": "Fix  error dimensions pointer in add_vector_int32 operand_type and function call in add_squeeze_params ", "body": "\r\nSince .dimensions in operand_type is data pointer of dimensions, it should equal to pointer values instead of &num_values. Num_values  is the length of the vector. This bug will lead segmentation fault when NNAPI calIs squeeze operation.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "> I signed it!\r\n\r\n", "CLAs look good, thanks!\n\n<!-- ok -->", "@jdduke Could you take a look at this?", "It is the problem of my NNAPI implementation. "]}, {"number": 22732, "title": "CNN.Model pruning: no gain in speeding up of inference", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 7\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A\r\n- **TensorFlow installed from (source or binary)**:pip install\r\n- **TensorFlow version (use command below)**:1.10.1\r\n- **Python version**:3.6\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:use CPU\r\n- **GPU model and memory**:N/A\r\n- **Exact command to reproduce**:N/A\r\n\r\n\r\nI implemented iterative pruning of a convolutional neural network using the example tensorflow **/tensorflow/contrib/model_pruning/** in order to increase the speed of inference .\r\nThe result of the procedure is an increase in the number of zero weight values(increase sparcity), which are still stored in dense tensors. I do not see a gain in speed.\r\nHow can I eliminate null weights used in convolutional layers from calculations? Can I remove  conv  filter knowing its sparseness?\r\nPlease give some recomendation on these issues.\r\n", "comments": ["Its difficult to decipher why you aren't observing gain in speed. Some general guidelines to solve your issues:\r\nYou can try adding dropout layer to reduce sparsity.\r\nI wouldn't recommend removing conv layer since dropout should help in regularization.\r\nAlso I would like to recommend referring this [research paper](https://openreview.net/pdf?id=SkC_7v5gx)\r\nIt talks about how to design a network that balances its size, training time, and prediction accuracy.", "As noted here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/model_pruning#removing-pruning-ops-from-the-trained-graph-\r\nThe model_pruning library helps with the training of a sparse model. It is left upto the underlying hardware platform and inference engine to implement sparse ops to provide acceleration", "Closing this issue since Suyog's explanation throws light on the speed aspect related to CNN Model pruning. Feel free to reopen this issue if you get any additional information that we haven't considered.\r\n", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 22731, "title": "Layer Normalization : Choice of the correct axes where to apply mean and variance in case of LSTM Input", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: VERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n   No\r\n\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n\r\n\r\n- **CUDA/cuDNN version**: 9.1\r\n- **GPU model and memory**:  Tesla K80\r\n\r\n- **Exact command to reproduce**: no command to reproduce\r\n\r\n### Describe the problem\r\nI'm implementing layer_normalization in an LSTM cell. My input has the following shape                           \r\n**[batch,time,features]**.\r\nIn tensorflow, we have the [LayerNormBasicLsmtCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell).\r\nReading the code of this class, and in line [1417](https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L1417), the authors used the layer_norm function to apply the shift/scale of the inputs. \r\nGoing to the implementation of the layer_norm and in line [2311](https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/contrib/layers/python/layers/layers.py#L2311), tensorflow specified the axes where to compute the mean/variance. Knowing that `begin_param_axis=1` and the `inputs_rank=3` in my case.\r\nThe `norm_axes = list(range(1,3)) = [1,2]` so the mean/variance will be calculated on both axis time and features.\r\n**My question is** : \r\nwhy do we need to compute the mean and variance of both axis=time and axis=Features?\r\nIsn't okay to just do the nn.moments of the feature axis only? \r\nHow can we set up the axes correctly in case of LSTM cells?\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "Done", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 22730, "title": "tf.test.is_gpu_available() blocking indefinitely, using 100% CPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Barely. See below.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10, Version 10.0.17134.285.\r\n- **TensorFlow installed from (source or binary)**: Binary.\r\n- **TensorFlow version (use command below)**: b'v1.11.0-rc2-4-gc19e29306c' 1.11.0.\r\n- **Python version**: Python 3.6.6 (v3.6.6:4cf1f54eb7, Jun 27 2018, 03:37:03) [MSC v.1900 64 bit (AMD64)] on win32.\r\n- **CUDA/cuDNN version**: CUDA V9.0.176, cuDNN v7.3.1.\r\n- **GPU model and memory**: Nvidia GTX 970 4 GB.\r\n\r\n### Reproduction\r\n```\r\nimport tensorflow as tf\r\nprint(tf.test.is_gpu_available())\r\n```\r\n\r\n### Expected behaviour\r\nTest prints `True`.\r\n\r\n### Actual behaviour\r\nThe test script prints the following text and then hangs indefinitely, using 100% of one CPU core:\r\n```\r\n>>> tf.test.is_gpu_available()\r\n\r\n2018-10-03 23:08:26.805592: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n\r\n2018-10-03 23:08:27.171992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:\r\n\r\nname: GeForce GTX 970 major: 5 minor: 2 memoryClockRate(GHz): 1.253\r\n\r\npciBusID: 0000:01:00.0\r\n\r\ntotalMemory: 4.00GiB freeMemory: 3.31GiB\r\n\r\n2018-10-03 23:08:27.184134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n```", "comments": ["A user on Reddit pointed out that it might be a duplicate of https://github.com/tensorflow/tensorflow/issues/21836.", "@epicvrvs Hi, have you installed gpu version of tensorflow ? i.e pip install tensorflow-gpu. \r\nJust wanted to get confirmation on this first :)", "@harshini-gadige yes, I have installed the GPU version of TensorFlow. Not the CPU version.\r\n\r\n```\r\n>pip list | grep tensorflow\r\ntensorflow-gpu      1.11.0\r\n```", "I just realized that the same happens when just running `tf.Session()` so it's actually worse than I thought:\r\n```\r\n>>> session = tf.Session()\r\n2018-10-07 17:05:18.289180: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-10-07 17:05:18.766184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:\r\nname: GeForce GTX 970 major: 5 minor: 2 memoryClockRate(GHz): 1.253\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.31GiB\r\n2018-10-07 17:05:18.775396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n```\r\nEdit: I just checked with x64dbg. It seems to hang in an an infinite loop in nvptxjitcompiler.dll.\r\nEdit 2: It actually returned after 3 minutes. Wow, why is this so slow?\r\nEdit 3: Ah, I must have interrupted some global initialization routine that is simply really slow over and over again over the past days! It always works within 1-2 seconds now. Well, this is embarrassing."]}, {"number": 22729, "title": "TF 1.11 failed to build", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.5 LTS (Xenial Xerus)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: we are building TF 1.11, command below is not usable.\r\n\r\nHere is git commit info:\r\n```sh\r\n$ git log -1 \r\ncommit c19e29306ce1777456b2dbb3a14f511edf7883a8\r\nAuthor: Austin Anderson <angerson@google.com>\r\nDate:   Tue Sep 25 14:50:52 2018 -0700\r\n\r\n    Final version strings for 1.11.0 (#22513)\r\n```\r\nWe are compiling code from this commit without any custom modification.\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- **CUDA/cuDNN version**:CUDA 8.0 / cuDNN 7\r\n- **GPU model and memory**: (GeForce GTX 1080 Ti, 11178MiB) x 4\r\n- **Exact command to reproduce**: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures`\r\n\r\n### Describe the problem\r\nWe have successfully built TF 1.10 under the same machine and configuration, we can even build TF 1.10 as for now. However, it turns out that it's not working when building TF 1.11. Here are logs generated by bazel:\r\n\r\n### Source code / logs\r\n```\r\nINFO: From Compiling tensorflow/core/kernels/l2loss_op_gpu.cu.cc [for host]:\r\nexternal/com_google_absl/absl/strings/string_view.h(492): error: calling a __host__ function(\"__builtin_strlen\") from a __device__ function(\"absl::string_view::StrLenInternal\") i\r\ns not allowed\r\n\r\n1 error detected in the compilation of \"/tmp/tmpxft_00000f73_00000000-7_l2loss_op_gpu.cu.cpp1.ii\".\r\nERROR: /home/shyo/packages/tensorflow/tensorflow/core/kernels/BUILD:3602:1: output 'tensorflow/core/kernels/_objs/l2loss_op_gpu/l2loss_op_gpu.cu.pic.o' was not created\r\nERROR: /home/shyo/packages/tensorflow/tensorflow/core/kernels/BUILD:3602:1: not all outputs were created or valid\r\n\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 285.352s, Critical Path: 52.48s\r\nINFO: 2673 processes: 2673 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["I am experiencing the same issue with building TensorFlow 1.11.0 and 1.10.1 with CUDA 8.  This issue is specific to CUDA 8 due to a conflict with the builtin functions called from abseil-cpp.  Have not tried building 1.10.0.\r\n\r\nAs a work around, I was able to successfully build TensorFlow 1.10.1 by disabling the gcc builtin function preprocessor macro check via:\r\n`bazel --cxxopt=\"-U__has_builtin\"`\r\nHowever, this trick does not work for building 1.11.0.", "@shyoshyo Please refer to [this](https://www.tensorflow.org/install/source) for build configurations. You can also post tf_env.txt here after environment capture script.", "@wt-huang Thank you for your reply. :) \r\n\r\nI did follow the document from `https://www.tensorflow.org/install/source`. Specifically, I\r\n* got the source code via `git checkout r1.11; git pull origin r1.11`; \r\n* configured the build via `./configure`; and\r\n* built via `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures` but failed\r\n\r\n\r\nHere is the content of `tf_env.txt`:\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux ****(removed for privacy) 4.15.0-34-generic #37~16.04.1-Ubuntu SMP Tue Aug 28 10:44:06 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux ****(removed for privacy) 4.15.0-34-generic #37~16.04.1-Ubuntu SMP Tue Aug 28 10:44:06 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                              1.14.3   \r\nnumpydoc                           0.8.0    \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Oct  4 19:54:27 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:05:00.0  On |                  N/A |\r\n| 45%   66C    P8    21W / 250W |    709MiB / 11175MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |\r\n| 81%   88C    P2   122W / 250W |   5443MiB / 11178MiB |     95%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 108...  Off  | 00000000:09:00.0 Off |                  N/A |\r\n| 31%   51C    P8    19W / 250W |     12MiB / 11178MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 108...  Off  | 00000000:0A:00.0 Off |                  N/A |\r\n| 24%   43C    P8    19W / 250W |     12MiB / 11178MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1330      G   /usr/lib/xorg/Xorg                            67MiB |\r\n|    0     13190      G   /usr/lib/xorg/Xorg                            14MiB |\r\n|    0     25789      C   python                                       511MiB |\r\n|    1     25789      C   python                                      5431MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/lib/python3.5/dist-packages/torch/lib/libcudart-5d6d23a3.so.8.0.61\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n\r\n```\r\n\r\n", "> I am experiencing the same issue with building TensorFlow 1.11.0 and 1.10.1 with CUDA 8. This issue is specific to CUDA 8 due to a conflict with the builtin functions called from abseil-cpp. Have not tried building 1.10.0.\r\n> \r\n> As a work around, I was able to successfully build TensorFlow 1.10.1 by disabling the gcc builtin function preprocessor macro check via:\r\n> `bazel --cxxopt=\"-U__has_builtin\"`\r\n> However, this trick does not work for building 1.11.0.\r\n\r\n@kicksalva  Thank you :)\r\nThis trick does not work for me neither, but I was able to build TF 1.10.1 without disabling any macro.", "@kicksalva thanks for sharing the trick. \r\n@shyoshyo glad it worked for you. It is recommended to use CUDA 9.2 for TensorFlow 1.10 and 1.11.", "It doesn't seem that the issue has been resolved or a solution has been found for running TensorFlow 1.11 with CUDA 8. That still seems to be an open issue. I'm running into the same problem. As @kicksalva  said, the trick doesn't work for 1.11 and CUDA 8. "]}, {"number": 22728, "title": "while i was running the code          from keras.applications.vgg16 import VGG16 model = VGG16() i got the error Could not allocate ndarray. how to solve this?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22727, "title": "Error with model.fit when input is a dataset", "body": "EDIT: Probably solved, this is because no label information is provided. However, the error message is not very clear\r\n\r\n### System information\r\n- Tensorflow 1.10 and tf-nightly-gpu-1.12.0.dev20181004\r\n- Ubuntu 16.04\r\n\r\nmodel.fit() gives an error when the input data is a tensor that is concatenated from different datasets.\r\n\r\nWith this code example the error can be reproduced:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nif __name__ == '__main__':\r\n    \r\n    model = tf.keras.Sequential()\r\n    model.add(tf.keras.layers.Dense(32, input_shape=(48,48,3)))\r\n    model.add(tf.keras.layers.Dense(32))\r\n\r\n    model.summary()\r\n\r\n    # Compile with optimizer, loss and metrics\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\r\n    model.compile(loss=tf.keras.losses.mean_squared_error,\r\n                  optimizer=optimizer)\r\n\r\n    # make a dataset from a numpy array\r\n    x = np.random.sample((1000,48,48,3))\r\n    match = tf.data.Dataset.from_tensor_slices(x)\r\n\r\n    classes = []\r\n    for c in range(5):\r\n        # In a real application here a dataset.filter() is used\r\n        class_data = match\r\n        class_data = class_data.batch(4)\r\n        classes.append(class_data)\r\n    \r\n    data = list(map(lambda x: x.make_one_shot_iterator().get_next(), classes))\r\n    batch = tf.concat(axis=0, values=data)\r\n\r\n    model.fit(\r\n        batch,\r\n        steps_per_epoch=100,\r\n        epochs=100)\r\n```\r\nThe error is\r\n\r\n> myproject/nightly-env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 137, in fit_loop\r\n>     if issparse is not None and issparse(ins[i]) and not K.is_sparse(feed[i]):\r\n> IndexError: list index out of range\r\n\r\nThe code can be even simplified further:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nif __name__ == '__main__':\r\n    \r\n    model = tf.keras.Sequential()\r\n    model.add(tf.keras.layers.Dense(32, input_shape=(48,48,3)))\r\n    model.add(tf.keras.layers.Dense(32))\r\n\r\n    model.summary()\r\n\r\n    # Compile with optimizer, loss and metrics\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\r\n    model.compile(loss=tf.keras.losses.mean_squared_error,\r\n                  optimizer=optimizer)\r\n\r\n    # make a dataset from a numpy array\r\n    x = np.random.sample((1000,48,48,3))\r\n    match = tf.data.Dataset.from_tensor_slices(x)\r\n    match = match.batch(4)\r\n\r\n    model.fit(\r\n        match,\r\n        steps_per_epoch=100,\r\n        epochs=100)\r\n```\r\nThis gives the same error", "comments": ["Issue is related to https://github.com/tensorflow/tensorflow/issues/21894.", "@dmus yep it looks like this is bc you need to provide label data, but you're right the error message should be clearer", "This is fixed with latest tf-nightly version '1.15.0-dev20190808'. The error message has been updated. For instance;\r\n```python\r\nValueError: Arguments and signature arguments do not match. got: 17, expected: 18 `\r\n```"]}, {"number": 22726, "title": "Object detection app built using TFlite is crashing", "body": "Hi using tflite android app for object detection.\r\nFollowed the below link for building app:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android/app\r\n\r\nAPK is generated and installed on pixel 2XL devices.\r\nSpeech recognization works but object detection fails.\r\nAttaching the logs for reference\r\n[object_detection_logcat.txt](https://github.com/tensorflow/tensorflow/files/2446135/object_detection_logcat.txt)\r\n", "comments": ["@Umapraveen Hi, could you post a code snippet to reproduce this issue ? Appreciate your response.", "@Umapraveen Also request you to post the following information.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n", "Hello,\r\nIm using the code from following directory\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android/app\r\n\r\nCompilation and installation followed same as README.md in the above link.\r\nSystem information:\r\n\r\n    Have I written custom code : NO\r\n    OS Platform and Distribution : Ubuntu 16.04\r\n    Mobile device : PIXCEL 2XL\r\n    TensorFlow installed from (source or binary): Binary(pip install)\r\n    TensorFlow version (use command below): 1.9.0\r\n    Python version: 2.7.12\r\n    Bazel version (if compiling from source): Installed using apt-get\r\n\r\n    Exact command to reproduce: Build the app as described in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android/app/README.md\r\nadb install bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk\r\nLaunch app for object detection.\r\nApp crashes\r\n", "@achowdhery Hi, could you please take a look into this ?", "Nagging Assignee @achowdhery: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Working in latest git souce. so closing this issue"]}, {"number": 22725, "title": "AttributeError: module 'tensorflow.python.training.checkpointable' has no attribute 'CheckpointableBase'", "body": "This problem has been posted before but the response does not help - there is no tf.nightly in my environment.\r\n\r\nThis same code worked before.\r\n\r\nIn the base Anaconda environment it said that tf did not have the following attributes: keras and __version__.\r\n\r\nI created a new environment (COS801) in Anaconda.\r\n\r\nThis is the code:\r\n\r\nimport tensorflow as tf\r\n\r\nThis is the response:\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-1-3e38fd553753> in <module>()\r\n----> 1 import tensorflow as tf\r\n      2 for x in dir(tf): print(x)\r\n      3 mnist = tf.keras.datasets.mnist\r\n      4 \r\n      5 (x_train, y_train),(x_test, y_test) = mnist.load_data()\r\n\r\n~\\Anaconda3\\envs\\COS801\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 # pylint: disable=wildcard-import\r\n     26 from tensorflow.tools.api.generator.api import *  # pylint: disable=redefined-builtin\r\n\r\n~\\Anaconda3\\envs\\COS801\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     61 \r\n     62 # Framework\r\n---> 63 from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n     64 from tensorflow.python.framework.versions import *\r\n     65 from tensorflow.python.framework import errors\r\n\r\n~\\Anaconda3\\envs\\COS801\\lib\\site-packages\\tensorflow\\python\\framework\\framework_lib.py in <module>()\r\n    102 from tensorflow.python.framework.random_seed import set_random_seed\r\n    103 from tensorflow.python.framework.sparse_tensor import convert_to_tensor_or_sparse_tensor\r\n--> 104 from tensorflow.python.framework.importer import import_graph_def\r\n    105 \r\n    106 # Utilities for working with Tensors\r\n\r\n~\\Anaconda3\\envs\\COS801\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py in <module>()\r\n     30 from tensorflow.python.framework import dtypes\r\n     31 from tensorflow.python.framework import errors\r\n---> 32 from tensorflow.python.framework import function\r\n     33 from tensorflow.python.framework import op_def_registry\r\n     34 from tensorflow.python.framework import ops\r\n\r\n~\\Anaconda3\\envs\\COS801\\lib\\site-packages\\tensorflow\\python\\framework\\function.py in <module>()\r\n     34 from tensorflow.python.framework import ops\r\n     35 from tensorflow.python.ops import array_ops\r\n---> 36 from tensorflow.python.ops import resource_variable_ops\r\n     37 from tensorflow.python.ops import variable_scope as vs\r\n     38 from tensorflow.python.util import compat\r\n\r\n~\\Anaconda3\\envs\\COS801\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py in <module>()\r\n     33 from tensorflow.python.ops import gen_state_ops\r\n     34 from tensorflow.python.ops import math_ops\r\n---> 35 from tensorflow.python.ops import variables\r\n     36 # go/tf-wildcard-import\r\n     37 # pylint: disable=wildcard-import\r\n\r\n~\\Anaconda3\\envs\\COS801\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py in <module>()\r\n     38 \r\n     39 @tf_export(\"Variable\")\r\n---> 40 class Variable(checkpointable.CheckpointableBase):\r\n     41   \"\"\"See the @{$variables$Variables How To} for a high level overview.\r\n     42 \r\n\r\nAttributeError: module 'tensorflow.python.training.checkpointable' has no attribute 'CheckpointableBase'\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@FlipN2 Hi, please provide the above information as requested. Thank you !", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I have the same issue:\r\n\r\nIn [1]: import tensorflow as tf\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-1-41389fad42b5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\n/home/chava/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *  # pylint: disable=redefined-builtin\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\n/home/chava/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>()\r\n     61 \r\n     62 # Framework\r\n---> 63 from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n     64 from tensorflow.python.framework.versions import *\r\n     65 from tensorflow.python.framework import errors\r\n\r\n/home/chava/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py in <module>()\r\n    100 from tensorflow.python.framework.random_seed import set_random_seed\r\n    101 from tensorflow.python.framework.sparse_tensor import convert_to_tensor_or_sparse_tensor\r\n--> 102 from tensorflow.python.framework.importer import import_graph_def\r\n    103 \r\n    104 # Utilities for working with Tensors\r\n\r\n/home/chava/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/tensorflow/python/framework/importer.py in <module>()\r\n     30 from tensorflow.python.framework import dtypes\r\n     31 from tensorflow.python.framework import errors\r\n---> 32 from tensorflow.python.framework import function\r\n     33 from tensorflow.python.framework import op_def_registry\r\n     34 from tensorflow.python.framework import ops\r\n\r\n/home/chava/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/tensorflow/python/framework/function.py in <module>()\r\n     35 from tensorflow.python.framework import ops\r\n     36 from tensorflow.python.ops import array_ops\r\n---> 37 from tensorflow.python.ops import resource_variable_ops\r\n     38 from tensorflow.python.ops import variable_scope as vs\r\n     39 from tensorflow.python.util import compat\r\n\r\n/home/chava/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py in <module>()\r\n     33 from tensorflow.python.ops import gen_state_ops\r\n     34 from tensorflow.python.ops import math_ops\r\n---> 35 from tensorflow.python.ops import variables\r\n     36 # go/tf-wildcard-import\r\n     37 # pylint: disable=wildcard-import\r\n\r\n/home/chava/.pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in <module>()\r\n     38 \r\n     39 @tf_export(\"Variable\")\r\n---> 40 class Variable(checkpointable.CheckpointableBase):\r\n     41   \"\"\"See the @{$variables$Variables How To} for a high level overview.\r\n     42 \r\n\r\nAttributeError: module 'tensorflow.python.training.checkpointable' has no attribute 'CheckpointableBase'"]}, {"number": 22724, "title": "How should we use the script tf_upgrade?", "body": "I would like to use the [tf_upgrade](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/compatibility/tf_upgrade.py) script. However, of course, I cannot simply copy it to the directory which contains the file whose code I want to upgrade, because that script contains an import `from tensorflow.tools.compatibility import ast_edits` and, if I execute that script using `python tf_upgrade.py --infile main.py --outfile main-upgraded.py`, I get the error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tf_upgrade.py\", line 23, in <module>\r\n    from tensorflow.tools.compatibility import ast_edits\r\nImportError: No module named compatibility\r\n````\r\n\r\nIn fact, `compatibility` is not a module of `tools`. So, how exactly are we supposed to use the `tf_upgrade` script? The documentation or README me of https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/compatibility should be updated so that this info is present.\r\n\r\nIn a few posts on the web, it's written that we are simply supposed to copy that script, but this is not true (at least, not anymore).\r\n\r\nI'm using Python 2.7 and TensorFlow 1.11.0 on a Mac OS X 10.13.6 (High Sierra).", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@nbro A simple way to use tf_upgrade.py is to go to directory tree then replace \r\n`from tensorflow.tools.compatibility import ast_edits` with\r\n`import ast_edits`\r\n\r\nType this in the command line:\r\n`python tf_upgrade.py --infile renames_v2.py --outfile foo-upgraded.py`", "Closing this, feel free to reopen if any additional questions.", "@wt-huang So, do we need to download the whole directory? Or do we need to go to the `compatibility` folder under my TF installation directory? It's not clear. There should be a step-by-step clear and unambiguous explanation on how to use that script. ", "@nbro You can follow the steps I described above to use `tf_upgrade.py`, assuming you already have TensorFlow installed on your system. ", "@wt-huang Your steps above are ambigous, that's why I complained above. What do you mean by \"go to directory tree then replace\"? Do you mean in my TF installation? Or do I need to download that `compatibility` folder only and do what you suggested there?\r\n\r\nBtw, why would we need to modify any import? Why the guys that created that script did not make it in such a way that the script is just runnable?", "@nbro The `compatibility` folder should already be in your system if TensorFlow is installed properly. Follow the [installation guide](https://www.tensorflow.org/install/) if haven't done so. You should be able to issue the command I mentioned above without getting any error.  ", "@wt-huang Anyway, maybe you should have suggested a more mnemonic command instead of \r\n\r\n```python tf_upgrade.py --infile renames_v2.py --outfile foo-upgraded.py```\r\n\r\nYou could have suggested, e.g.\r\n\r\n```python tf_upgrade.py --infile path_to_your_file.py --outfile path_to_upgraded_file.py```\r\n\r\nAlso, I haven't tried yet your suggestion, but why would we need to change any import? Is this script supposed to be used in any other way than from the command line? If yes, then maybe two scripts could have been created to solve this problem of needing to change imports."]}, {"number": 22723, "title": "bug: tf.train.Saver.save() fails if 2 tf.contrib.cudnn_rnn.cudnnLSTM instances passed to saver are built with build() method", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.5 LTS\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy)**: N/A\r\n\r\n- **TensorFlow installed from (source or binary)**: binary\r\n\r\n- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1 1.10.1\r\n\r\n- **Python version**: 3.6.0\r\n\r\n- **Bazel version (if compiling from source)**: N/A\r\n\r\n- **CUDA/cuDNN version**:\r\nCUDA Version 9.0.176\r\ncudnn: 7.1.4\r\n\r\n- **GPU model and memory**:\r\nNVIDIA GeForce GTX 1080 Ti\r\n11GB\r\n\r\n- **Exact command to reproduce**:\r\n`python test.py`\r\n\r\n\r\n### Describe the problem\r\n**Description** \r\nIf 2 instances of `tensorflow.contrib.cudnn_rnn.CudnnLSTM` are built via `build()` method they can not be saved together in a checkpoint. \r\n\r\n**Bug borders**\r\nIf instances are built implicitly in `__call__()`, no exceptions are thrown. Next code executes OK.\r\n\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.cudnn_rnn import CudnnLSTM as CudnnLSTM\r\ninp = tf.zeros([10, 32, 100])\r\nlstm1 = CudnnLSTM(1, 128)\r\nlstm2 = CudnnLSTM(2, 256)\r\nlstm1(inp)\r\nlstm2(inp)\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    save_path = os.path.join('test_cudnn_lstm_save', '1')\r\n    if not os.path.exists(save_path):\r\n        os.makedirs(os.path.join(save_path))\r\n    saver.save(sess, save_path)\r\n```\r\n\r\n Also if only 1 instance is built using `build()` or if only 1 instance is passed to saver in `var_list` (see ex\u0441erpt below), save op is executed without exceptions.\r\n\r\n```python\r\nsaver = tf.train.Saver([lstm1.saveable])\r\n```\r\n\r\n**Relevance**\r\nI ran into this problem when tried to train my net on several GPUs. Shared CudnnLSTM params are built on CPU, while computations are made on GPUs.\r\n\r\nThank you in advance for any help!\r\n\r\n### Source code / logs\r\nCode to reproduce the bug\r\n\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.cudnn_rnn import CudnnLSTM as CudnnLSTM\r\ninp = tf.zeros([10, 32, 100])\r\nlstm1 = CudnnLSTM(1, 128)\r\nlstm2 = CudnnLSTM(2, 256)\r\nlstm1.build(inp.shape)\r\nlstm2.build(inp.shape)\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    save_path = os.path.join('test_cudnn_lstm_save', '1')\r\n    if not os.path.exists(save_path):\r\n        os.makedirs(os.path.join(save_path))\r\n    saver.save(sess, save_path)\r\n```\r\n\r\nHere is a traceback\r\n```\r\n2018-10-04 12:59:46.556727: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-10-04 12:59:46.635329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-10-04 12:59:46.635669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.01GiB\r\n2018-10-04 12:59:46.635682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-10-04 12:59:46.841898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-10-04 12:59:46.841930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-10-04 12:59:46.841937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-10-04 12:59:46.842099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9673 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-10-04 12:59:47.465382: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at save_restore_v2_ops.cc:134 : Invalid argument: Adding duplicate key: rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\r\nTraceback (most recent call last):\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\r\n    return fn(*args)\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1263, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Adding duplicate key: rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\r\n         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, cudnn_lstm/opaque_kernel/_1, transpose/_3, concat_5/_5, cudnn_lstm_1/opaque_kernel/_7, transpose_1/_9, transpose_2/_11, concat_11/_13, concat_17/_15)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 17, in <module>\r\n    saver.save(sess, save_path)\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1620, in save\r\n    {self.saver_def.filename_tensor_name: checkpoint_file})\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Adding duplicate key: rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\r\n         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, cudnn_lstm/opaque_kernel/_1, transpose/_3, concat_5/_5, cudnn_lstm_1/opaque_kernel/_7, transpose_1/_9, transpose_2/_11, concat_11/_13, concat_17/_15)]]\r\n\r\nCaused by op 'save/SaveV2', defined at:\r\n  File \"test.py\", line 11, in <module>\r\n    saver = tf.train.Saver()\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1281, in __init__\r\n    self.build()\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1293, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1330, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 775, in _build_internal\r\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 275, in _AddSaveOps\r\n    save = self.save_op(filename_tensor, saveables)\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 193, in save_op\r\n    tensors)\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1687, in save_v2\r\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\r\n    op_def=op_def)\r\n  File \"/home/anton/dpenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Adding duplicate key: rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\r\n         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, cudnn_lstm/opaque_kernel/_1, transpose/_3, concat_5/_5, cudnn_lstm_1/opaque_kernel/_7, transpose_1/_9, transpose_2/_11, concat_11/_13, concat_17/_15)]]\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nMobile device", "@tensorflowbutler Required fields are filled", "Thanks for reporting, this is a problem. TF should automatically create different namespace for the vars.\r\nAt present, as a workaround, you can create a separate variable scope\r\n```py\r\nwith tf.variable_scope('rnn1'):\r\n  lstm1 = CudnnLSTM(1, 128)\r\nwith tf.variable_scope('rnn2'):\r\n  lstm2 = CudnnLSTM(2, 256)\r\n```\r\n\r\nBtw we'll eventually support all cudnn layers under tf.keras.layers, but that requires some time for migrating and testing, just as a FYI.", "@protoget Thank you very much for advice!", "@PeganovAnton Hi, feel free to close this issue if resolved. Thanks !"]}, {"number": 22722, "title": "Will it cause performance loss, when the grads are IndexedSlices?", "body": "https://github.com/tensorflow/tensorflow/blob/6f5d7a97cd2c0741ddfa756853ce5321377b5d53/tensorflow/contrib/distribute/python/cross_tower_ops.py#L313", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 22721, "title": "Enable USE_CBLAS_GEMM for Conv ops in C++ API", "body": "### System information\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: using bazel\r\n- **TensorFlow version**: 1.10\r\n- **Bazel version**: 0.17.2\r\n- **Have I written custom code**: No\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: bazel run -c opt //tensorflow/example/example:example --define USE_CBLAS_GEMM=true\r\n- **Mobile device**: N/A\r\n\r\n### Describe the problem:\r\nTF is not built from source, I am just configuring a specific example. So I am doing inference on TF C++ convolution layers, using bazel. I added 'USE_CBLAS_GEMM' and 'USE_GEMM_FOR_CONV' into the config setting of the build file.\r\n\r\n```\r\nconfig_setting(\r\n    name = \"cblas\",\r\n    values = {\r\n        \"define\": \"USE_CBLAS_GEMM=1\",\r\n    },\r\n)\r\n```\r\nFor running, \r\n```\r\nbazel run -c opt //tensorflow/example/example:conv2d --define USE_CBLAS_GEMM=true\r\n```\r\n\r\nBut the flag is not enabled and I cannot find a way to make sure whether CBLAS is running or not.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@hagerrady13 It is stated in System Information that TensorFlow is installed from source, but in Description it is stated otherwise. Which one is correct? If installing TensorFlow with bazel, you can add `-DEIGEN_USE_BLAS ` to `bazel build` command.\r\n\r\nTo check whether CBLAS_GEMM is applied you can use:\r\n\r\n```\r\nnp.__config__.show()\r\n```\r\n", "@wt-huang TF is is installed with Bazel, I edited the description. I tried to add ``` -DEIGEN_USE_BLAS ``` to the build command as you suggested but it gives the following:\r\n``` ERROR: Invalid options syntax: -DEIGEN_USE_BLAS ```\r\n\r\nAny help?", "@hagerrady13 If you already have TensorFlow installed with Bazel, no need to use `-DEIGEN_USE_BLAS.` Just type `np.__config__.show()` in jupyter notebook, your will get an output containing `cblas` in the list of libraries.", "Closing, feel free to reopen if problem persists. "]}, {"number": 22720, "title": "User defined optimization list", "body": "Currently if user adds an optimizer to rewrite_config.optimizer list, default optimizers are disabled and user defined optimizers run. However if user modifies custom_optimizers list, optimizers in the list is appended to the default optimizers list. This PR makes the behavior for optimizers and custom_optimizers list identical in this respect.", "comments": ["@rmlarsen please reconsider.", "@rmlarsen but doesn't adding custom optimizer to the optimizer list doubly initializes the custom optimizer? See line https://github.com/tensorflow/tensorflow/blob/7e20bec6ff779d5d6142668aa2d897b98a2bd844/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L206 \r\nIf a custom optimizer is added to the optimizer list and then added to custom_optimizers for passing configuration, line 193 will initialize it for the optimizers list and line 199 will initialize it for the custom_optimizers list. I don't think that is the desired behavior. Re opening.", "As chatted in the meeting, I'm closing this one, feel free to reopen if there is a use case for it. :)"]}, {"number": 22719, "title": "Include .inc files for absl headers", "body": "Current setup.py collects only header files from absl package. However header files include .inc files which were not installed into include directory. This PR adds *.inc files into header list.", "comments": ["Thank you very much for the fix!\r\n@annarev @tatatodd @goldiegadde To fix custom ops we also need this cherrypicked into 1.12 branch.\r\n", "I noticed this is not available in 1.12.0rc0, will it be picked up in 1.12.0rc1?", "@annarev @tatatodd Did we cherrypick this into the release?", "@annarev @tatatodd @gunan, still not fixed in 1.12.0rc1 :-/", "Sorry, I missed this comment.  We'll get this into RC2.", "@tatatodd, thanks!"]}, {"number": 22718, "title": "tensorflow-gpu-1.10: failed call to cuInit: CUDA_ERROR_UNKNOWN", "body": "### System information\r\n\r\n- **OS Platform and Distribution** : Ubuntu 14.04\r\n\r\n- **TensorFlow version** : 1.10 gpu \r\n\r\n- **TensorFlow installed from (source or binary)**: binary\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no\r\n\r\n- **Python version**: 2.7\r\n\r\n- **CUDA/cuDNN version**: 9.0 / 7\r\n\r\n- **GPU model and memory**: Nvidia GeForce GTX TITAN X \r\n\r\n- **`nvidia-smi`**:\r\n\r\n+------------------------------------------------------+                       \r\n| NVIDIA-SMI 352.63     Driver Version: 384.130        |                       \r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX TIT...  Off  | 0000:02:00.0     Off |                  N/A |\r\n| 22%   61C    P0    75W / 250W |      0MiB / 12206MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX TIT...  Off  | 0000:03:00.0     Off |                  N/A |\r\n| 23%   63C    P0    78W / 250W |      0MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX TIT...  Off  | 0000:83:00.0     Off |                  N/A |\r\n| 22%   60C    P0    77W / 250W |      0MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX TIT...  Off  | 0000:84:00.0     Off |                  N/A |\r\n| 22%   61C    P0    72W / 250W |      0MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+-----------------\r\n\r\n- **`find /lib/modules/ | grep -i nvidia `**\r\n\r\n/lib/modules/4.2.0-27-generic/kernel/drivers/net/ethernet/nvidia\r\n/lib/modules/4.2.0-27-generic/kernel/drivers/net/ethernet/nvidia/forcedeth.ko\r\n/lib/modules/4.2.0-27-generic/kernel/drivers/video/fbdev/nvidia\r\n/lib/modules/4.2.0-27-generic/kernel/drivers/video/fbdev/nvidia/nvidiafb.ko\r\n/lib/modules/4.2.0-27-generic/updates/dkms/nvidia_384_uvm.ko\r\n/lib/modules/4.2.0-27-generic/updates/dkms/nvidia_384.ko\r\n/lib/modules/4.2.0-27-generic/updates/dkms/nvidia_384_modeset.ko\r\n/lib/modules/4.2.0-27-generic/updates/dkms/nvidia_384_drm.ko\r\n\r\n### Describe the problem\r\n\r\nI upgrade the nvidia driver througth command: **`sudo apt-get install nvidia-384`**.  Then i found there are serveal nvidia driver installed througth command: **`sudo dpkg --list | grep nvidia-*`**\uff0cso i uninstalled these driver except nvidia-384 use commadn: **`sudo apt-get remove xxx`**.  After that, the info as follows:\r\n\r\nii  nvidia-384                                            384.130-0ubuntu0.14.04.1                            amd64        NVIDIA binary driver - version 384.130\r\nii  nvidia-opencl-icd-384                                 384.130-0ubuntu0.14.04.1                            amd64        NVIDIA OpenCL ICD\r\nii  nvidia-prime                                          0.6.2.1                                             amd64        Tools to enable NVIDIA's Prime\r\nii  nvidia-settings                                       352.39-0ubuntu1                                     amd64        Tool for configuring the NVIDIA graphics driver\r\n\r\n**Error occured when i run tensorflow code as follows:**\r\n**`import tensorflow as tf`\r\n`tf.Session()`**\r\n\r\n`2018-10-03 23:13:51.656015: E tensorflow/stream_executor/cuda/cuda_driver.cc:397   ] failed call to cuInit: CUDA_ERROR_UNKNOWN\r\n2018-10-03 23:13:51.656131: I tensorflow/stream_executor/cuda/cuda_diagnostics.c   c:163] retrieving CUDA diagnostic information for host: root0-SCW4350-16\r\n2018-10-03 23:13:51.656166: I tensorflow/stream_executor/cuda/cuda_diagnostics.c   c:170] hostname: root0-SCW4350-16\r\n2018-10-03 23:13:51.656299: I tensorflow/stream_executor/cuda/cuda_diagnostics.c   c:194] libcuda reported version is: 384.130.0\r\n2018-10-03 23:13:51.656428: I tensorflow/stream_executor/cuda/cuda_diagnostics.c   c:198] kernel reported version is: 384.130.0\r\n2018-10-03 23:13:51.656465: I tensorflow/stream_executor/cuda/cuda_diagnostics.c   c:305] kernel version seems to match DSO: 384.130.0\r\n<tensorflow.python.client.session.Session object at 0x7f30a08ae5d0>`\r\n\r\ni have tried many solutions such as install `nvidia-modprobe` , `nvidia-cuda-mps-server` , `export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/usr/local/cuda-9.0/targets/x86_64-linux/lib/\"` all of these don't work. \r\n\r\nMaybe  It is noteworthy that **`nvidia-smi`** shows that `NVIDIA-SMI 352.63     Driver Version: 384.130` and **`sudo dpkg --list | grep nvidia-*`** shows that **`nvidia-settings   352.39-0ubuntu1 `**. It seems that some moulde of nvidia-352 are not uninstalled. And i tried to installed nvidia driver througth **`nvidia_xxxx.run`** file\uff0cbut the error remain while running tensorflow code.\r\n\r\nHopefully you can help me with this issue.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nTensorFlow installed from\nBazel version\nExact command to reproduce\nMobile device"]}, {"number": 22717, "title": "1.12-rc0 cherry-pick request: Pin wheel=0.31.1 in install_auditwheel.sh to work around issue", "body": "https://github.com/pypa/auditwheel/issues/102\r\n\r\nPiperOrigin-RevId: 215685104", "comments": ["Windows bazel failure is unrelated.", "Thanks Amit, I was gonna be trigger-happy about the windows failure too.  :)"]}, {"number": 22716, "title": "Static cast size_t to int in arguments 1,2 to forward_input_or_allocate_output()", "body": "Static cast size_t to int in arguments 1,2 to forward_input_or_allocate_output()\r\n\r\nThis fix resolves the following compiler error:\r\n\r\ntensorflow/core/kernels/mkl_relu_op.cc(1028): error C2398: Element '1':\r\nconversion from 'const std::size_t' to 'int' requires a narrowing conversion", "comments": []}, {"number": 22715, "title": "Trouble building from source Windows 10 Pro, MSVC, Cuda 10, Cudnn 7.3.1, Compute 7.5, Python 3.6 or 3.7", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 Pro October Release 1809\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nNo.\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.11 (current)\r\n- **Python version**:\r\n3.7, Anaconda 5.3\r\n- **Bazel version (if compiling from source)**:\r\n0.17.2\r\n- **GCC/Compiler version (if compiling from source)**:\r\nMSVC 14.0 (but I want to switch to 15.0)\r\n- **CUDA/cuDNN version**:\r\nCUDA 10.0, CuDNN 7.3, Compute 7.5\r\n- **GPU model and memory**:\r\nRTX 2080 8GB GDDR6\r\n- **Exact command to reproduce**:\r\n```\r\npython configure.py\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n### Describe the problem\r\n\r\nI can't find this information anywhere: How can you tell bazel to compile with Visual Studio 2017 (MSVC 15)?\r\n\r\nI tried setting the `BAZEL_VS` and `BAZEL_VC` environment variables to `C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\` and `C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC`, restarting the shell, and running `python configure.py`, and `bazel clean`, and `bazel build ...`, but it still uses MSVC 14 (2015) rather than MSVC 15 (2017).\r\n\r\nTrying to investigate if switching the compiler to VS2017 from VS2015 will solve my compilation issues.\r\n\r\n### Source code / logs\r\n\r\nI'm trying to investigate whether the following compile failures can be fixed by switching to VS 2017:\r\n\r\n```\r\n\r\n[12 / 21] [-----] BazelWorkspaceStatusAction stable-status.txt\r\nERROR: C:/users/joey/_bazel_joey/juz2ghmw/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/joey/_bazel_joey/juz2ghmw/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Joey/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Joey/Anaconda3/lib/site-packages\r\n    SET TEMP=C:\\Users\\Joey\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\Joey\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/protobuf_archive/python /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/python /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/python /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX /DHAVE_PTHREAD /wd4018 /wd4514 -DGOOGLE_PROTOBUF_HAS_ONEOF=1 -DPROTOBUF_PYTHON_ALLOW_OVERSIZE_PROTOS=1 /Fobazel-out/x64_windows-opt/bin/external/protobuf_archive/_objs/python/google/protobuf/pyext/_message.so/descriptor_containers.o /c external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(172): error C2440: '=': cannot convert from 'const char *' to 'char *'\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(172): note: Conversion loses qualifiers\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(189): error C2440: '=': cannot convert from 'const char *' to 'char *'\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(189): note: Conversion loses qualifiers\r\n[2,999 / 6,086] Compiling external/protobuf_archive/src/google/protobuf/generated_message_util.cc; 1s local ... (39 actions running)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: C:/users/joey/downloads/tensorflow/tensorflow/contrib/lite/python/BUILD:42:1 C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/joey/_bazel_joey/juz2ghmw/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Joey/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Joey/Anaconda3/lib/site-packages\r\n    SET TEMP=C:\\Users\\Joey\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\Joey\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/protobuf_archive/python /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/python /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/python /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX /DHAVE_PTHREAD /wd4018 /wd4514 -DGOOGLE_PROTOBUF_HAS_ONEOF=1 -DPROTOBUF_PYTHON_ALLOW_OVERSIZE_PROTOS=1 /Fobazel-out/x64_windows-opt/bin/external/protobuf_archive/_objs/python/google/protobuf/pyext/_message.so/descriptor_containers.o /c external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc\r\nINFO: Elapsed time: 6.080s, Critical Path: 1.93s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\nMy goal is to build with cuda 10 and cudnn 7.3 for windows.", "comments": ["I managed to change the compiler by setting `BAZEL_VC` and `BAZEL_VS` environment variables, resetting bazel via `bazel clean --expunge`, restarting computer and shell (not sure which/what/how many does the trick). Now I get the same \"Cannot convert  'const char *' to 'char *'\" errors in protobuf-whatever, so I guess it's not the compiler. Weird.\r\n\r\n```\r\nERROR: C:/users/joey/_bazel_joey/juz2ghmw/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/joey/_bazel_joey/juz2ghmw/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.15.26726\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.15.26726\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.15.26726\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.15.26726\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.15.26726\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17134.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\\\MSBuild\\15.0\\bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Joey/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Joey/Anaconda3/lib/site-packages\r\n    SET TEMP=C:\\Users\\Joey\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\Joey\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/protobuf_archive/python /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/python /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/python /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX /DHAVE_PTHREAD /wd4018 /wd4514 -DGOOGLE_PROTOBUF_HAS_ONEOF=1 -DPROTOBUF_PYTHON_ALLOW_OVERSIZE_PROTOS=1 /Fobazel-out/x64_windows-opt/bin/external/protobuf_archive/_objs/python/google/protobuf/pyext/_message.so/descriptor_containers.o /c external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(172): error C2440: '=': cannot convert from 'const char *' to 'char *'\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(172): note: Conversion loses qualifiers\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(189): error C2440: '=': cannot convert from 'const char *' to 'char *'\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc(189): note: Conversion loses qualifiers\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 14.784s, Critical Path: 8.98s\r\nINFO: 43 processes: 43 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```", "Now I switched from python 3.7 to 3.6, and am getting a [different error] (https://github.com/tensorflow/tensorflow/issues/19198) complaining about an overloaded `__hadd` function.\r\n\r\n```\r\nc:\\users\\joey\\_bazel_joey\\juz2ghmw\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/arch/CUDA/Half.h(212): error: more than one instance of overloaded function \"__hadd\" matches the argument list:\r\n            function \"__hadd(int, int)\"\r\n            function \"__hadd(__half, __half)\"\r\n            argument types are: (const Eigen::half, const Eigen::half)\r\n```", "@meteorcloudy ", "Looks like it's the same issue as https://github.com/tensorflow/tensorflow/issues/19198", "Medium has a [guest article](https://medium.com/@amsokol.com/update-1-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-and-c2e86fec9ef2) (probably written by one of the contributors here) that shows a patch procedure to fix the eigen stuff. Also said to `checkout` 1.11 before build. Trying it now... Ok it looks like it works... waiting for rest of compilation.\r\n\r\nIT WORKS!!!\r\nIn summary, if you are someone with a brand spankin' new RTX card and want compute 7.5, Cuda 10, and Cudnn 7.3, this is what you need to know:\r\n\r\n- Python 3.7 doesn't work because `protobuf` causes compiler errors\r\n- Python 3.6.6 works. Anaconda supports [making environments](https://conda.io/docs/user-guide/tasks/manage-python.html), you can make and choose 3.6 and leave your 3.7 one alone until the `protobuf` stuff is fixed. Note building with 3.6 means incompatible with 3.7. Hope this gets fixed.\r\n- Compute 7.5 (and anything above 5.3 I believe) requires patching `eigen` stuff, so follow [this guide](https://medium.com/@amsokol.com/update-1-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-and-c2e86fec9ef2) to patch it.\r\n- I think you need to `checkout` the v1.11 of tensorflow before anything also. Not sure, didn't test not doing it."]}, {"number": 22714, "title": "fix tf.batch_gather error when feeding int64 indices", "body": "fix #22638 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I found this problem has already been solved in [this commit](https://github.com/tensorflow/tensorflow/commit/6cc738da1748e819b9c8ee92dc2f1a7bdb291b50)."]}, {"number": 22713, "title": "Fixed false error triggering in ps distribution strategy for local run", "body": "# Problem description\r\n\r\nThe `ParameterServerStrategy` throws an error from `_verify_destinations_not_different_worker` when applied locally, where only one worker is available. The cause seems to be the line \r\n\r\n```python\r\nif d_spec.job == self._task_type and d_spec.task != self._task_id:\r\n```\r\n\r\nwhere the `d_spec` obtained from `\"'/device:CPU:0\"` gives a 0 to `d_spec.task`, while the `self._task_id` is still `None` as it is not set anywhere after its definition in `_initialize_local`.\r\n\r\n# Actions taken\r\n\r\nI'm not sure if `self._task_id` being `None` has any semantics in other contexts, so I have added a default value in the check to minimize the impact.\r\n\r\nPlease kindly review ", "comments": ["@yuefengz can you please review for correctness?", "Hello,\r\n\r\nI've the same problem and trying to apply the suggested change I don't see any improvements.\r\nIn my case I'm trying to run a PS on CPU with 2 workers on different GPUs so the PS logs following line:\r\n```\r\n ParameterServerStrategy with compute_devices = ['/replica:0/task:0/device:GPU:0', '/replica:0/task:0/device:GPU:1'], variable_device = '/device:CPU:0'\r\n```\r\nSo far it looks good.\r\n\r\nHowever when we do `d_spec = tf_device.DeviceSpec.from_string(d)` \r\nthen `d_spec.task == None` for `/device:CPU:0` and sometimes \r\nI see `d == '/replica:0/task:0/device:CPU:0'` that gives `d_spec.task == 0`.\r\n\r\nEither I don't get something important there or something is going wrong at higher level.", "@eryshev Hi, I guess this might be a better fix: https://github.com/tensorflow/tensorflow/commit/66dd1e21e7ab6e2aed8413880a7f2dd7f0a20e50. Could you try it out please?", "@yuefengz Hi, I've tested this change and it successfully launches the training. \r\nHowever I'm a bit of confused as I launched only one TF instance of `ps` type and it started working without any additional `worker` task.\r\n\r\nFor me it boils down to 2 questions:\r\n1. Do I still need to set `cluster_spec` via `TF_CONFIG` variable ? As for example\r\n```\r\nexport TF_CONFIG='{\r\n    \"cluster\": {\r\n        \"ps\": [\r\n            \"localhost: 2222\"\r\n        ],\r\n        \"worker\": [\r\n            \"localhost: 3333\",\r\n            \"localhost: 3334\"\r\n        ]\r\n    },\r\n    \"task\": {\r\n        \"type\":\"ps\",\r\n        \"index\":0\r\n    }\r\n}'\r\n```\r\n2. Do I need to launch multiple TF instances with different `TF_CONFIG`(2 workers and PS) when I want to train locally or with the strategy TF will do for me? It seems like TF launches the training but it uses only 1 GPU from two available(note that I set `num_gpus_per_worker` as 1 on creation of ParameterServerStrategy and task defined in TF_CONFIG has `ps` type)\r\nI'm sorry but I haven't seen any note about it in the documentation and I'm not familiar enough with codebase to figure it out by myself. \r\n\r\nThank for your work.", "@eryshev \r\n1. For distributed training, yes. 'TF_COFNIG' is required to set each job up.\r\n2. I am not sure I understand your question. But fir local training, you don't have to set 'TF_CONFIG'. For distributed training, 'TF_CONFIG' is required for all tasks including ps. But for ps, it is a just standard tensorflow server and the common practice is that it should be CPU-only machines.", "@yuefengz \r\nThe 66dd1e2 fixes the problem, thank you.\r\nConcerning my two questions:\r\n1. Ok I got how it works for distributed setup ([this section](https://www.tensorflow.org/deploy/distributed#putting_it_all_together_example_trainer_program) was quite helpful and I was missing it).\r\n2. However I'm not sure I understand what it does in local case:\r\n- Does it create multiple workers/sessions?\r\n- Is it between-graph replication?\r\n- Is it in-graph with towers over multiple GPUs?\r\n- Or operations are just split between GPUs and all variables are on CPU?\r\n\r\nSorry for lots of questions, if you think it's more relevant to ask them on SO, I will.\r\n", "@dave-msk could you please rebase to resolve the conflict?", "@rmlarsen @yuefengz  Rebased to resolve conflicts", "@dave-msk can you please re base to resolve conflict ?", "@rthadur Rebased and resolved conflicts. Please kindly review, thanks!", "> @rthadur Rebased and resolved conflicts. Please kindly review, thanks!\r\n\r\n@rmlarsen gentle ping", "Looking\r\n", "@yuefengz could you review this again? I'm not really familiar with the diststrat code.", "Can one of the admins verify this patch?", "closing this PR as contrib folder will be depricated in 2.0, thank you.\r\nCC @mihaimaruseac"]}, {"number": 22712, "title": "1.12-rc0 cherry-pick request: Increase error-epsilon for ProfilingTest::ProfilesAreCollected.", "body": "PiperOrigin-RevId: 215654327", "comments": []}, {"number": 22711, "title": "Fixed false error triggering in ps distribution strategy for local run", "body": "# Problem description\r\n\r\nThe `ParameterServerStrategy` throws an error from `_verify_destinations_not_different_worker` when applied locally, where only one worker is available. The cause seems to be the line \r\n\r\n```python\r\nif d_spec.job == self._task_type and d_spec.task != self._task_id:\r\n```\r\n\r\nwhere the `d_spec` obtained from `\"'/device:CPU:0\"` gives a 0 to `d_spec.task`, while the `self._task_id` is still `None` as it is not set anywhere after its definition in `_initialize_local`.\r\n\r\n# Actions taken\r\n\r\nI'm not sure if `self._task_id` being `None` has any semantics in other contexts, so I have added a default value in the check to minimize the impact.\r\n\r\nPlease kindly review ", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Closing to resubmit PR due to CLA issues", "What has happened to this PR? \r\nIt seems like I have the same issue https://github.com/tensorflow/tensorflow/issues/23376", "@eryshev I have resubmitted another PR here: https://github.com/tensorflow/tensorflow/pull/22713"]}]