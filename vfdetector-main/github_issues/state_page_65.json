[{"number": 29666, "title": "Build tensorflow c library for mips64.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n\r\n\r\n\r\n**Describe the problem**\r\nI want to build a tensorflow C library for MIPS architecture. \r\nSo in order to do that, I specified the following options before building:\r\n```\r\nexport CC=/usr/bin/mips64-linux-gnuabi64-gcc\r\nexport CXX=/usr/bin/mips64-linux-gnuabi64-g++\r\n```\r\n\r\nThose are cross-compilers for MIPS64.\r\nSo when I try to build bazel using:\r\n```\r\nbazel build //tensorflow/tools/lib_package:libtensorflow --spawn_strategy=standalone --verbose_failures\r\n```\r\nI get the following error:\r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/36c1970dc5e3eff66b396f9517eb3547/external/hwloc/BUILD.bazel:214:1: C++ compilation of rule '@hwloc//:hwloc' failed (Exit 1): mips64-linux-gnuabi64-gcc failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/36c1970dc5e3eff66b396f9517eb3547/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH='/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Users/kprosvir/Downloads/cmder/vendor/conemu-maximus5/ConEmu/wsl:/mnt/c/Users/kprosvir/Downloads/cmder/vendor/conemu-maximus5/ConEmu/Scripts:/mnt/c/Users/kprosvir/Downloads/cmder/vendor/conemu-maximus5:/mnt/c/Users/kprosvir/Downloads/cmder/vendor/conemu-maximus5/ConEmu:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files (x86)/Sennheiser/SoftphoneSDK:/mnt/c/ProgramData/Webex/Webex/Applications:/mnt/c/Users/kprosvir/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/kprosvir/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/kprosvir/AppData/Local/hyper/app-3.0.0/resources/bin:/mnt/c/Users/kprosvir/AppData/Local/Box/Box Edit:/snap/bin:/usr/local/go/bin:/root/go//bin' \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/mips64-linux-gnuabi64-gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/host/bin/external/hwloc/_objs/hwloc/topology-x86.pic.d '-frandom-seed=bazel-out/host/bin/external/hwloc/_objs/hwloc/topology-x86.pic.o' -fPIC -iquote external/hwloc -iquote bazel-out/host/bin/external/hwloc -isystem external/hwloc/hwloc -isystem bazel-out/host/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/host/bin/external/hwloc/include -g0 -I. -Ihwloc -Iinclude -Wno-vla '-DHWLOC_DUMPED_HWDATA_DIR=' '-DRUNSTATEDIR=' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/hwloc/hwloc/topology-x86.c -o bazel-out/host/bin/external/hwloc/_objs/hwloc/topology-x86.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nIn file included from external/hwloc/hwloc/topology-x86.c:23:0:\r\nexternal/hwloc/hwloc/topology-x86.c: In function 'cpuid_or_from_dump':\r\nexternal/hwloc/include/private/cpuid-x86.h:67:3: error: invalid 'asm': invalid use of '%k'\r\n   __asm__(\r\n   ^~~~~~~\r\nTarget //tensorflow/tools/lib_package:libtensorflow failed to build\r\n```\r\n\r\nSo the question is: is there any way to fix it and successfully build it for MIPS64?", "comments": ["quick fix is to skip building hwloc, check `tensorflow/core/platform/default/build_config.bzl`", "@freedomtan \r\nThanks! That helped a bit, not I'm getting:\r\n\r\n```\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/mips64-linux-gnuabi64-gcc-7 -o bazel-out/host/bin/external/nasm/nasm -Wl,-S '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,@bazel-out/host/bin/external/nasm/nasm-2.params)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n/usr/bin/ld.gold: error: /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/../../../../mips64-linux-gnuabi64/lib/../lib/crt1.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/../../../../mips64-linux-gnuabi64/lib/../lib/crti.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/crtbegin.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/assemble.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/directbl.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/directiv.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/error.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/eval.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/exprdump.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/exprlib.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/float.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/labels.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/listing.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/nasm.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/parser.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/pptok.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/pragma.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/preproc.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/preproc-nop.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/quote.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/rdstrnum.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/segalloc.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/stdscan.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/strfunc.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/tokhash.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/common.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/disasm.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/sync.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/macros.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/badenum.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/bsi.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/crc64.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/file.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/filename.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/hashtbl.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/ilog2.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/malloc.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/md5c.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/mmap.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/path.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/perfhash.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/raa.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/rbtree.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/readnum.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/realpath.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/saa.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/srcfile.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/string.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/strlist.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/ver.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/zerobuf.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/codeview.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/legacy.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/nulldbg.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/nullout.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/outaout.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/outas86.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/outbin.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/outcoff.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/outdbg.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/outelf.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/outform.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/outieee.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/outlib.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/outmacho.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/outobj.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/outrdf2.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/snprintf.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/strlcpy.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/strnlen.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/vsnprintf.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/disp8.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/iflag.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/insnsa.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/insnsb.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/insnsd.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/insnsn.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/regdis.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/regflags.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/regs.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: bazel-out/host/bin/external/nasm/_objs/nasm/regvals.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/crtend.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: error: /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/../../../../mips64-linux-gnuabi64/lib/../lib/crtn.o: not configured to support 64-bit big-endian object\r\n/usr/bin/ld.gold: warning: skipping incompatible /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/libstdc++.so while searching for stdc++\r\n/usr/bin/ld.gold: error: cannot find -lstdc++\r\n/usr/bin/ld.gold: warning: skipping incompatible /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/../../../../mips64-linux-gnuabi64/lib/../lib/libm.so while searching for m\r\n/usr/bin/ld.gold: warning: skipping incompatible /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/../../../../mips64-linux-gnuabi64/lib/libm.so while searching for m\r\n/usr/bin/ld.gold: error: input file does not match -EB/EL option\r\n/usr/bin/ld.gold: warning: skipping incompatible /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/libgcc_s.so.1 while searching for libgcc_s.so.1\r\n/usr/bin/ld.gold: warning: skipping incompatible /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/libgcc_s.so.1 while searching for libgcc_s.so.1\r\n/usr/bin/ld.gold: warning: skipping incompatible /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/../../../../mips64-linux-gnuabi64/lib/../lib/libgcc_s.so.1 while searching for libgcc_s.so.1\r\n/usr/bin/ld.gold: warning: skipping incompatible /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/../../../../mips64-linux-gnuabi64/lib/libgcc_s.so.1 while searching for libgcc_s.so.1\r\n/usr/bin/ld.gold: warning: skipping incompatible /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/../../../../mips64-linux-gnuabi64/lib/../lib/libc.so while searching for c\r\n/usr/bin/ld.gold: warning: skipping incompatible /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/../../../../mips64-linux-gnuabi64/lib/libc.so while searching for c\r\n/usr/bin/ld.gold: warning: skipping incompatible /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/libgcc_s.so.1 while searching for libgcc_s.so.1\r\n/usr/bin/ld.gold: warning: skipping incompatible /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/libgcc_s.so.1 while searching for libgcc_s.so.1\r\n/usr/bin/ld.gold: warning: skipping incompatible /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/../../../../mips64-linux-gnuabi64/lib/../lib/libgcc_s.so.1 while searching for libgcc_s.so.1\r\n/usr/bin/ld.gold: warning: skipping incompatible /usr/lib/gcc-cross/mips64-linux-gnuabi64/7/../../../../mips64-linux-gnuabi64/lib/libgcc_s.so.1 while searching for libgcc_s.so.1\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/lib_package:libtensorflow failed to build\r\nINFO: Elapsed time: 618.666s, Critical Path: 22.87s\r\nINFO: 1202 processes: 1202 local.\r\n```\r\nProbably something is wrong with linker?", "looks some big-endian/little-endian mismatch. check the endianess of your object files, system libraries, and linker flags. make sure that they are consistent.", "@gadagashwini  @freedomtan  Actually, I have a little understand on how to fix it.\r\n\r\nI specified a new linker (mips64-linux-gnuabi64-ld/ld.gold) and now i'm getting:\r\n\r\n```\r\nDuplicate file in archive: ./include/tensorflow/c/LICENSE, picking first occurrence\r\nERROR: /root/.cache/bazel/_bazel_root/36c1970dc5e3eff66b396f9517eb3547/external/jpeg/BUILD.bazel:223:1: Executing genrule @jpeg//:simd_x86_64_assemblage23 failed (Exit 126): bash failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/36c1970dc5e3eff66b396f9517eb3547/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH='/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Users/kprosvir/Downloads/cmder/vendor/conemu-maximus5/ConEmu/wsl:/mnt/c/Users/kprosvir/Downloads/cmder/vendor/conemu-maximus5/ConEmu/Scripts:/mnt/c/Users/kprosvir/Downloads/cmder/vendor/conemu-maximus5:/mnt/c/Users/kprosvir/Downloads/cmder/vendor/conemu-maximus5/ConEmu:/mnt/c/ProgramData/DockerDesktop/version-bin:/mnt/c/Program Files/Docker/Docker/resources/bin:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files (x86)/Sennheiser/SoftphoneSDK:/mnt/c/ProgramData/Webex/Webex/Applications:/mnt/c/Users/kprosvir/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/kprosvir/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/kprosvir/AppData/Local/hyper/app-3.0.0/resources/bin:/mnt/c/Users/kprosvir/AppData/Local/Box/Box Edit:/snap/bin:/usr/local/go/bin:/root/go//bin' \\\r\n\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; for out in bazel-out/host/bin/external/jpeg/simd/x86_64/jccolor-avx2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jccolor-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jcgray-avx2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jcgray-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jchuff-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jcphuff-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jcsample-avx2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jcsample-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jdcolor-avx2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jdcolor-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jdmerge-avx2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jdmerge-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jdsample-avx2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jdsample-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jfdctflt-sse.o bazel-out/host/bin/external/jpeg/simd/x86_64/jfdctfst-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jfdctint-avx2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jfdctint-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jidctflt-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jidctfst-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jidctint-avx2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jidctint-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jidctred-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jquantf-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jquanti-avx2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jquanti-sse2.o bazel-out/host/bin/external/jpeg/simd/x86_64/jsimdcpu.o; do\r\n  bazel-out/host/bin/external/nasm/nasm -f elf64    -DELF -DPIC -D__x86_64__    -I $(dirname bazel-out/host/bin/external/jpeg/jconfig.h)/    -I $(dirname bazel-out/host/bin/external/jpeg/jconfigint.h)/    -I $(dirname external/jpeg/simd/nasm/jsimdcfg.inc.h)/    -I $(dirname external/jpeg/simd/x86_64/jccolext-sse2.asm)/    -o $out    $(dirname external/jpeg/simd/x86_64/jccolext-sse2.asm)/$(basename ${out%.o}.asm)\r\ndone')\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n/bin/bash: line 1: bazel-out/host/bin/external/nasm/nasm: cannot execute binary file: Exec format error\r\nTarget //tensorflow/tools/lib_package:libtensorflow failed to build\r\nINFO: Elapsed time: 376.468s, Critical Path: 86.03s\r\nINFO: 1456 processes: 1456 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nSemes like I need to specify everything for host and target architecture, but I haven't found anything on this in official docs.", "@ymodak ", "This looks like a request for support on a new platform. We are not planning to support this platform first party, but we will accept any pull requests to add support for mips64"]}, {"number": 29640, "title": "[XLA]make xlaop optimization algorithm configurable", "body": "As far as my understanding to XLA,  a XlaCompile op which is responsible for compiling tfops into one executable op will be inserted into the graph.  but nowadays there is no horovod xlaop support in the current version, which means horovodallreduce is excluded from the _XlaRun op just like the following pic shows.\r\n<img width=\"1389\" alt=\"Screen Shot 5779-09-05 at 14 41 00\" src=\"https://user-images.githubusercontent.com/50541066/59266222-bbba7000-8c79-11e9-98c7-3759364cb5e4.png\">\r\n\r\nas i see it, in distributed environment,  the transmission of parameters over network  cost the most of time  during the calculation of allreduce op.  if we can parallelize the horovod ops and XLaRun, which means  transforming the timeline into the following,  a better perf can be obtained. \r\n\r\n![image](https://user-images.githubusercontent.com/50541066/59268095-911ee600-8c7e-11e9-9c6b-64b0635367f7.png)\r\n\r\nSo i'd like to know if there is a way to produce a separated _XlaRun, a horovod related part and a horovod non-related part,  in order to parallelize the HorovodAllreduce op and the _XlaRun. Besides, will it be possible to provide a synchronization interface  in XLA ? \r\n\r\nThanks.\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n1.14\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nall xlaop are compiled together \r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nAny existing code using Horovod ans xla at the same time\r\n**Any Other info.**\r\n", "comments": ["Does TF Classic run all-reduce in parallel with further computation?\r\n\r\nNote that the GPU will be doing significant work during the all-reduce, so overlapping is not necessarily as big a win as one might expect.\r\n\r\nIf this is a feature you want to implement, I'd guess the first step would be to build a prototype and show that it works.  But to set expectations, we're not going to be able to extensively help you debug it.\r\n\r\nSince this isn't a feature request we're going to work on, I'm going to unassign myself.", "![image](https://user-images.githubusercontent.com/50541066/59398710-bff2a480-8dc3-11e9-9d94-028be898d16d.png)\r\n @jlebar , as my test shows , the classic runtime can parallelize horovodallreduce op and those not depending on it. which is exactly what i want to implement in the XLA case. \r\nas you mentioned, most of work of allreduce are done in GPUs,  does this mean that the GPUs will be the next performance bottleneck  even if parallelizing the horovodallreduce and others?  ", "Regarding synchronization, you can implement it with \r\nhttps://www.tensorflow.org/api_docs/python/tf/no_op\r\nhttps://www.tensorflow.org/api_docs/python/tf/xla/experimental/jit_scope and\r\nhttps://www.tensorflow.org/api_docs/python/tf/control_dependencies\r\n\r\nYou would create a no_op node within a jit_scope(compile_ops=False) and then have control dependencies from it to the AllReduce and the unrelated clustered operations. This would result in a graph of the form:\r\n```\r\nCluster0 -> NoOp0 -> Cluster1\r\n                \\-> HorovodAllReduce\r\n```\r\n\r\nThis only works though if Cluster1 does not later depend on the AllReduce, so you might need the same logic later in the program, so Cluster1 doesn't depend on HorovodAllReduce.\r\n\r\nWe have avoided implementing something like this as an official api point in xla.py because jit is supposed to be a best effort tool where we are given full freedom to do the best we can. Alternatively we would expect people spending a large amount of time in fine tuning performance of their models to use xla.compile. That's not to discourage using a synchronization tool through manual dependencies, but it just runs the risk of harming the jit in some other way or preventing future improvements to the jit from being realized in your model, etc.\r\n\r\nAny ideas on how to make the clustering perform better automatically in your case would also be very appreciated. In general we want to have as large of clusters as possible and don't have models for how long operations will take, so it's difficult to predict a larger cluster or a smaller cluster with more parallelization will perform better.  Additionally, it's tricky in this case because Horovod is not in the core Tensorflow package, so we can't have the jit specially handle this operation in some way.", "@tpopp \r\nThanks for your suggestion, it sounds like an exciting idea to implement the parallelization.\r\n\r\nIn these days, I have made some low-level changes in xla: implement a XlaOpKernel of HorovodAllreduce and change the execution sequence of thunks from serialization to parallelization. it almost work nowadays.\r\n\r\nwhat do you think about this scheme? do you know the original ideas of making all thunks executing sequentially but not parallel?\r\n", "hi @LogX2\uff0c i   meet the same problem, can you show your optimized code\uff0cthank you very much ", "sorry for replying so late,  \r\ni have to say that i have given up this method, at least for now. \r\nIn order to parallelize computation and commutation, it is not enough to  just add an low-level communication XlaOp, which indeed  gives us a more beautiful timeline:  the HorovodOp is involved in XlaRun perfectly. But according my benchmark , in most scenarios, the performance turns out to be worse. In fact, we also need to adding another graph-level optimizer  which taking the XlaOp's communication time cost into consideration. Well, this is beyond my ability. ", "@LogX2 could you please explain more about how you make horovod ops involving into xlaRun. As horovod kernels run on a seperated horovod thread(after enqueued), how can it be presented by hlo and llvm.\r\nAnd if you solved this problem, it should have nothing to do with the graph optimization.", "@passerbyd    I transplanted the 3 horovod ops into tensorflow XLA GPU backend-- adding a new horovod thunk. As far as i know, the original horovodAllReduce is implemented as an AsyncOpKernel, while XlaOpKernel inherits OpKernel, so i removed the async mechanism of horovod ops and finally they were involved in _XlaRun, Just as i have explained before, i think just adding a network communication op such as a horovod op would prolong the time cost of _XlaRun significantly, and this may be solved in higher view such as graph optimizer: pick out the network communication XlaOpKernels in one cluster in earlier phase after estimating their execution time.", "thanks for the explanation. got your another issue #29240.", "@LogX2 would you mind sharing your code snippets? thanks.\r\n\r\n> @passerbyd I transplanted the 3 horovod ops into tensorflow XLA GPU backend-- adding a new horovod thunk. As far as i know, the original horovodAllReduce is implemented as an AsyncOpKernel, while XlaOpKernel inherits OpKernel, so i removed the async mechanism of horovod ops and finally they were involved in _XlaRun, Just as i have explained before, i think just adding a network communication op such as a horovod op would prolong the time cost of _XlaRun significantly, and this may be solved in higher view such as graph optimizer: pick out the network communication XlaOpKernels in one cluster in earlier phase after estimating their execution time.\r\n\r\n", "i just changed  jobs, when and how to contribute this feature is up to my\nformer employer\n\nYulu Jia <notifications@github.com> \u4e8e2020\u5e747\u67089\u65e5\u5468\u56db \u4e0b\u53484:56\u5199\u9053\uff1a\n\n> would you mind sharing your code snippets? thanks.\n>\n> @passerbyd <https://github.com/passerbyd> I transplanted the 3 horovod\n> ops into tensorflow XLA GPU backend-- adding a new horovod thunk. As far as\n> i know, the original horovodAllReduce is implemented as an AsyncOpKernel,\n> while XlaOpKernel inherits OpKernel, so i removed the async mechanism of\n> horovod ops and finally they were involved in _XlaRun, Just as i have\n> explained before, i think just adding a network communication op such as a\n> horovod op would prolong the time cost of _XlaRun significantly, and this\n> may be solved in higher view such as graph optimizer: pick out the network\n> communication XlaOpKernels in one cluster in earlier phase after estimating\n> their execution time.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/29640#issuecomment-656002437>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AMBTECWCTLGQ3W4SU5SAFS3R2WA2NANCNFSM4HW5OIGQ>\n> .\n>\n\n\n-- \nRegards\n\n*JiangXIAO*\n"]}, {"number": 29635, "title": "Allow growth seems to take more memory than needed", "body": "\r\n**System information**\r\n- Have I written custom code: yes using C++ API\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: I don't know\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.12\r\n- Python version: not used\r\n- Bazel version (if compiling from source): I don't know\r\n- GCC/Compiler version (if compiling from source): g++ 5.4.0\r\n- CUDA/cuDNN version: 10.1/libcudnn.so.7.5\r\n- GPU model and memory: GeForce GTX 1080 Ti 11175MiB\r\n\r\n**Describe the current behavior**\r\n\r\nIt seems setting `allow_growth` to true takes much more VRAM than needed. A simple classification inception_v3 network reports on nvidia-smi using 3705MiB whereas if I set a `per_process_gpu_memory_fraction` to for example 0.02, there is not performance impact when serving and it takes only 600MiB of VRAM. Going below 0.015 make it crash (as it obviously don't have enough VRAM to run).\r\n\r\n**Describe the expected behavior**\r\n\r\nSince I have many networks served on the same GPU, I would like them to take as low VRAM as possible. Each of them being in a separated process (easier to schedule). \r\nFinding the good `per_process_gpu_memory_fraction` for each neural network is a bit complicated since I have to grope.\r\n\r\n**Code to reproduce the issue**\r\n\r\nI think it is reproducable with any python code that load and serve a neural network. I unfortunately only have custom code for now. If you don't manage to reproduce, I will give a sample code.\r\n\r\n", "comments": ["@maingoh I am unable to reproduce isuue. Please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "It is directly inspired from https://github.com/tensorflow/models/blob/master/tutorials/image/imagenet/classify_image.py\r\nI just changed `run_inference_on_image()` to use a `tf.ConfigProto` and run on 1000k  images.\r\nUsing either `config.gpu_options.per_process_gpu_memory_fraction = 0.01` or `config.gpu_options.allow_growth = True` gives the same speed but `allow_growth` takes much more VRAM ~3700MiB. \r\nWhen setting the `gpu_memory_fraction` I get some warnings but there is actually no performance gains when using more memory (I think in my custom code I don't have them, I will check):\r\n```\r\n2019-06-13 11:53:31.484732: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 23.74MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-06-13 11:53:31.484788: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 533.62MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-06-13 11:53:31.486956: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 192.66MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-06-13 11:53:31.487582: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 126.6KiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-06-13 11:53:31.487605: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 23.74MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-06-13 11:53:31.487624: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-06-13 11:53:31.487638: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 200.2KiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-06-13 11:53:31.487652: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 9.34MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-06-13 11:53:31.487667: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 289.05MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-06-13 11:53:31.489299: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 13.84MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n```\r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport os.path\r\nimport re\r\nimport sys\r\nimport tarfile\r\nimport time\r\nimport numpy as np\r\nfrom six.moves import urllib\r\nimport tensorflow as tf\r\n\r\nFLAGS = None\r\n\r\n# pylint: disable=line-too-long\r\nDATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\r\n# pylint: enable=line-too-long\r\n\r\n\r\nclass NodeLookup(object):\r\n  \"\"\"Converts integer node ID's to human readable labels.\"\"\"\r\n\r\n  def __init__(self,\r\n               label_lookup_path=None,\r\n               uid_lookup_path=None):\r\n    if not label_lookup_path:\r\n      label_lookup_path = os.path.join(\r\n          FLAGS.model_dir, 'imagenet_2012_challenge_label_map_proto.pbtxt')\r\n    if not uid_lookup_path:\r\n      uid_lookup_path = os.path.join(\r\n          FLAGS.model_dir, 'imagenet_synset_to_human_label_map.txt')\r\n    self.node_lookup = self.load(label_lookup_path, uid_lookup_path)\r\n\r\n  def load(self, label_lookup_path, uid_lookup_path):\r\n    \"\"\"Loads a human readable English name for each softmax node.\r\n\r\n    Args:\r\n      label_lookup_path: string UID to integer node ID.\r\n      uid_lookup_path: string UID to human-readable string.\r\n\r\n    Returns:\r\n      dict from integer node ID to human-readable string.\r\n    \"\"\"\r\n    if not tf.gfile.Exists(uid_lookup_path):\r\n      tf.logging.fatal('File does not exist %s', uid_lookup_path)\r\n    if not tf.gfile.Exists(label_lookup_path):\r\n      tf.logging.fatal('File does not exist %s', label_lookup_path)\r\n\r\n    # Loads mapping from string UID to human-readable string\r\n    proto_as_ascii_lines = tf.gfile.GFile(uid_lookup_path).readlines()\r\n    uid_to_human = {}\r\n    p = re.compile(r'[n\\d]*[ \\S,]*')\r\n    for line in proto_as_ascii_lines:\r\n      parsed_items = p.findall(line)\r\n      uid = parsed_items[0]\r\n      human_string = parsed_items[2]\r\n      uid_to_human[uid] = human_string\r\n\r\n    # Loads mapping from string UID to integer node ID.\r\n    node_id_to_uid = {}\r\n    proto_as_ascii = tf.gfile.GFile(label_lookup_path).readlines()\r\n    for line in proto_as_ascii:\r\n      if line.startswith('  target_class:'):\r\n        target_class = int(line.split(': ')[1])\r\n      if line.startswith('  target_class_string:'):\r\n        target_class_string = line.split(': ')[1]\r\n        node_id_to_uid[target_class] = target_class_string[1:-2]\r\n\r\n    # Loads the final mapping of integer node ID to human-readable string\r\n    node_id_to_name = {}\r\n    for key, val in node_id_to_uid.items():\r\n      if val not in uid_to_human:\r\n        tf.logging.fatal('Failed to locate: %s', val)\r\n      name = uid_to_human[val]\r\n      node_id_to_name[key] = name\r\n\r\n    return node_id_to_name\r\n\r\n  def id_to_string(self, node_id):\r\n    if node_id not in self.node_lookup:\r\n      return ''\r\n    return self.node_lookup[node_id]\r\n\r\n\r\ndef create_graph():\r\n  \"\"\"Creates a graph from saved GraphDef file and returns a saver.\"\"\"\r\n  # Creates graph from saved graph_def.pb.\r\n  with tf.gfile.FastGFile(os.path.join(\r\n      FLAGS.model_dir, 'classify_image_graph_def.pb'), 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    _ = tf.import_graph_def(graph_def, name='')\r\n\r\n\r\ndef run_inference_on_image(image):\r\n  \"\"\"Runs inference on an image.\r\n\r\n  Args:\r\n    image: Image file name.\r\n\r\n  Returns:\r\n    Nothing\r\n  \"\"\"\r\n  if not tf.gfile.Exists(image):\r\n    tf.logging.fatal('File does not exist %s', image)\r\n  image_data = tf.gfile.FastGFile(image, 'rb').read()\r\n\r\n  # Creates graph from saved GraphDef.\r\n  create_graph()\r\n\r\n  config = tf.ConfigProto()\r\n  config.gpu_options.allow_growth = True\r\n  # config.gpu_options.per_process_gpu_memory_fraction = 0.01\r\n  with tf.Session(config=config) as sess:\r\n    # Some useful tensors:\r\n    # 'softmax:0': A tensor containing the normalized prediction across\r\n    #   1000 labels.\r\n    # 'pool_3:0': A tensor containing the next-to-last layer containing 2048\r\n    #   float description of the image.\r\n    # 'DecodeJpeg/contents:0': A tensor containing a string providing JPEG\r\n    #   encoding of the image.\r\n    # Runs the softmax tensor by feeding the image_data as input to the graph.\r\n    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\r\n    start_time = time.time()\r\n    for i in range(1000):\r\n        predictions = sess.run(softmax_tensor,\r\n                               {'DecodeJpeg/contents:0': image_data})\r\n    print(\"Elasped time:\", (time.time() - start_time))\r\n\r\ndef maybe_download_and_extract():\r\n  \"\"\"Download and extract model tar file.\"\"\"\r\n  dest_directory = FLAGS.model_dir\r\n  if not os.path.exists(dest_directory):\r\n    os.makedirs(dest_directory)\r\n  filename = DATA_URL.split('/')[-1]\r\n  filepath = os.path.join(dest_directory, filename)\r\n  if not os.path.exists(filepath):\r\n    def _progress(count, block_size, total_size):\r\n      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\r\n          filename, float(count * block_size) / float(total_size) * 100.0))\r\n      sys.stdout.flush()\r\n    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\r\n    print()\r\n    statinfo = os.stat(filepath)\r\n    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\r\n  tarfile.open(filepath, 'r:gz').extractall(dest_directory)\r\n\r\n\r\ndef main(_):\r\n  maybe_download_and_extract()\r\n  image = (FLAGS.image_file if FLAGS.image_file else\r\n           os.path.join(FLAGS.model_dir, 'cropped_panda.jpg'))\r\n  run_inference_on_image(image)\r\n\r\n\r\nif __name__ == '__main__':\r\n  parser = argparse.ArgumentParser()\r\n  # classify_image_graph_def.pb:\r\n  #   Binary representation of the GraphDef protocol buffer.\r\n  # imagenet_synset_to_human_label_map.txt:\r\n  #   Map from synset ID to a human readable string.\r\n  # imagenet_2012_challenge_label_map_proto.pbtxt:\r\n  #   Text representation of a protocol buffer mapping a label to synset ID.\r\n  parser.add_argument(\r\n      '--model_dir',\r\n      type=str,\r\n      default='/tmp/imagenet',\r\n      help=\"\"\"\\\r\n      Path to classify_image_graph_def.pb,\r\n      imagenet_synset_to_human_label_map.txt, and\r\n      imagenet_2012_challenge_label_map_proto.pbtxt.\\\r\n      \"\"\"\r\n  )\r\n  parser.add_argument(\r\n      '--image_file',\r\n      type=str,\r\n      default='',\r\n      help='Absolute path to image file.'\r\n  )\r\n  parser.add_argument(\r\n      '--num_top_predictions',\r\n      type=int,\r\n      default=5,\r\n      help='Display this many predictions.'\r\n  )\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```", "I could able to reproduce the issue with Tf 1.12.0 on colab. Thanks!", "I can also reproduce this with tf 1.13.1 but didn't manage to convert the code to tf 2.0 yet. ", "@maingoh Just to verify, did you get a chance to look at [Upgrade code to TensorFlow 2.0](https://www.tensorflow.org/beta/guide/upgrade) link.Thanks! ", "Yes I did, but couldn't find the import I needed, so I just looked at the tf sources, and managed to update it for tf 2.0:\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport os.path\r\nimport re\r\nimport sys\r\nimport tarfile\r\nimport time\r\nimport numpy as np\r\nfrom six.moves import urllib\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.platform import gfile\r\nfrom tensorflow.core.framework import graph_pb2\r\nfrom tensorflow.core.protobuf import config_pb2\r\nfrom tensorflow.python.client import session\r\n\r\n\r\nFLAGS = None\r\n\r\n# pylint: disable=line-too-long\r\nDATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\r\n# pylint: enable=line-too-long\r\n\r\n\r\nclass NodeLookup(object):\r\n  \"\"\"Converts integer node ID's to human readable labels.\"\"\"\r\n\r\n  def __init__(self,\r\n               label_lookup_path=None,\r\n               uid_lookup_path=None):\r\n    if not label_lookup_path:\r\n      label_lookup_path = os.path.join(\r\n          FLAGS.model_dir, 'imagenet_2012_challenge_label_map_proto.pbtxt')\r\n    if not uid_lookup_path:\r\n      uid_lookup_path = os.path.join(\r\n          FLAGS.model_dir, 'imagenet_synset_to_human_label_map.txt')\r\n    self.node_lookup = self.load(label_lookup_path, uid_lookup_path)\r\n\r\n  def load(self, label_lookup_path, uid_lookup_path):\r\n    \"\"\"Loads a human readable English name for each softmax node.\r\n\r\n    Args:\r\n      label_lookup_path: string UID to integer node ID.\r\n      uid_lookup_path: string UID to human-readable string.\r\n\r\n    Returns:\r\n      dict from integer node ID to human-readable string.\r\n    \"\"\"\r\n    if not gfile.Exists(uid_lookup_path):\r\n      tf.logging.fatal('File does not exist %s', uid_lookup_path)\r\n    if not gfile.Exists(label_lookup_path):\r\n      tf.logging.fatal('File does not exist %s', label_lookup_path)\r\n\r\n    # Loads mapping from string UID to human-readable string\r\n    proto_as_ascii_lines = gfile.GFile(uid_lookup_path).readlines()\r\n    uid_to_human = {}\r\n    p = re.compile(r'[n\\d]*[ \\S,]*')\r\n    for line in proto_as_ascii_lines:\r\n      parsed_items = p.findall(line)\r\n      uid = parsed_items[0]\r\n      human_string = parsed_items[2]\r\n      uid_to_human[uid] = human_string\r\n\r\n    # Loads mapping from string UID to integer node ID.\r\n    node_id_to_uid = {}\r\n    proto_as_ascii = gfile.GFile(label_lookup_path).readlines()\r\n    for line in proto_as_ascii:\r\n      if line.startswith('  target_class:'):\r\n        target_class = int(line.split(': ')[1])\r\n      if line.startswith('  target_class_string:'):\r\n        target_class_string = line.split(': ')[1]\r\n        node_id_to_uid[target_class] = target_class_string[1:-2]\r\n\r\n    # Loads the final mapping of integer node ID to human-readable string\r\n    node_id_to_name = {}\r\n    for key, val in node_id_to_uid.items():\r\n      if val not in uid_to_human:\r\n        tf.logging.fatal('Failed to locate: %s', val)\r\n      name = uid_to_human[val]\r\n      node_id_to_name[key] = name\r\n\r\n    return node_id_to_name\r\n\r\n  def id_to_string(self, node_id):\r\n    if node_id not in self.node_lookup:\r\n      return ''\r\n    return self.node_lookup[node_id]\r\n\r\n\r\ndef create_graph():\r\n  \"\"\"Creates a graph from saved GraphDef file and returns a saver.\"\"\"\r\n  # Creates graph from saved graph_def.pb.\r\n  with gfile.FastGFile(os.path.join(\r\n      FLAGS.model_dir, 'classify_image_graph_def.pb'), 'rb') as f:\r\n    graph_def = graph_pb2.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    _ = tf.import_graph_def(graph_def, name='')\r\n\r\n\r\ndef run_inference_on_image(image):\r\n  \"\"\"Runs inference on an image.\r\n\r\n  Args:\r\n    image: Image file name.\r\n\r\n  Returns:\r\n    Nothing\r\n  \"\"\"\r\n  if not gfile.Exists(image):\r\n    tf.logging.fatal('File does not exist %s', image)\r\n  image_data = gfile.FastGFile(image, 'rb').read()\r\n\r\n  # Creates graph from saved GraphDef.\r\n  create_graph()\r\n\r\n  config = config_pb2.ConfigProto()\r\n  config.gpu_options.allow_growth = True\r\n  # config.gpu_options.per_process_gpu_memory_fraction = 0.01\r\n  with session.Session(config=config) as sess:\r\n    # Some useful tensors:\r\n    # 'softmax:0': A tensor containing the normalized prediction across\r\n    #   1000 labels.\r\n    # 'pool_3:0': A tensor containing the next-to-last layer containing 2048\r\n    #   float description of the image.\r\n    # 'DecodeJpeg/contents:0': A tensor containing a string providing JPEG\r\n    #   encoding of the image.\r\n    # Runs the softmax tensor by feeding the image_data as input to the graph.\r\n    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\r\n    start_time = time.time()\r\n    for i in range(1000):\r\n        predictions = sess.run(softmax_tensor,\r\n                               {'DecodeJpeg/contents:0': image_data})\r\n    print(\"Elasped time:\", (time.time() - start_time))\r\n\r\ndef maybe_download_and_extract():\r\n  \"\"\"Download and extract model tar file.\"\"\"\r\n  dest_directory = FLAGS.model_dir\r\n  if not os.path.exists(dest_directory):\r\n    os.makedirs(dest_directory)\r\n  filename = DATA_URL.split('/')[-1]\r\n  filepath = os.path.join(dest_directory, filename)\r\n  if not os.path.exists(filepath):\r\n    def _progress(count, block_size, total_size):\r\n      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\r\n          filename, float(count * block_size) / float(total_size) * 100.0))\r\n      sys.stdout.flush()\r\n    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\r\n    print()\r\n    statinfo = os.stat(filepath)\r\n    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\r\n  tarfile.open(filepath, 'r:gz').extractall(dest_directory)\r\n\r\n\r\ndef main():\r\n  maybe_download_and_extract()\r\n  image = (FLAGS.image_file if FLAGS.image_file else\r\n           os.path.join(FLAGS.model_dir, 'cropped_panda.jpg'))\r\n  run_inference_on_image(image)\r\n\r\n\r\nif __name__ == '__main__':\r\n  parser = argparse.ArgumentParser()\r\n  parser.add_argument(\r\n      '--model_dir',\r\n      type=str,\r\n      default='/tmp/imagenet',\r\n      help=\"\"\"\\\r\n      Path to classify_image_graph_def.pb,\r\n      imagenet_synset_to_human_label_map.txt, and\r\n      imagenet_2012_challenge_label_map_proto.pbtxt.\\\r\n      \"\"\"\r\n  )\r\n  parser.add_argument(\r\n      '--image_file',\r\n      type=str,\r\n      default='',\r\n      help='Absolute path to image file.'\r\n  )\r\n  parser.add_argument(\r\n      '--num_top_predictions',\r\n      type=int,\r\n      default=5,\r\n      help='Display this many predictions.'\r\n  )\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  main()\r\n```\r\n\r\nThere is still the issue, even though allow growth made me win ~120MiB (nvidia-smi report 3585MiB) compared to tf 1.x. Speed is also the same.\r\n\r\nI also tried setting allow growth with the snippet code that the new documentation gives to allow growth but it gives the same results:\r\n```python\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    # Currently, memory growth needs to be the same across GPUs\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    # Memory growth must be set before GPUs have been initialized\r\n    print(e)\r\n```", "too problem to me\r\n", "The issue still persists with tf 2.4.1 and it probably won't be fixed by chance (can you update the labels ?)\r\n\r\nI think i am not the only one trying to minimize tf vram footprint https://github.com/tensorflow/tensorflow/issues/39605 and https://github.com/tensorflow/tensorflow/issues/43846.\r\n\r\nPlease, either give us a way to see the real vram needed so that we can set the appropriate memory fraction, or make sure allow growth only use the needed memory.\r\n\r\n\r\n ", "I was facing this issue while trying the run on a GPU with 16GB memory with Tensorflow 2.2. Based on the above comment, I understand the issue is persisting in TF2.4 as well.\r\n\r\nIn my case, I am using a ResNet50 model where the weights are trainable. By using the allow growth option, the process is allocated 54% of the memory which seemed unreasonably high. When I tried freezing the ResNet50 weights, I still see that the memory allocated is 54% which shouldn't the case since the layers are non-trainable. Alternately, I tried the fixed memory option where I pre-allocated only 3072 GB of memory per process and the model works fine. However, pre-allocating memory isn't a sustainable option for me since that would be also be contingent on batch sizes, etc. So, I would like to find out when this particular issue is being planned for a fix."]}, {"number": 29357, "title": "Please provide the list of supported hardware architectures (and/or build options) for the pre-built library", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/install/lang_c\r\n## Description of issue (what needs changing):\r\nThere is no mention on this page what options were used to build the library files.\r\nFor example the lib (\"Linux GPU support\") requires cuda 6.0 compute architecture (or above) which is not mentioned. While the pip package runs fine on 3.0 or above.\r\nOr which CPU instructions sets/extensions are supported and which are required (minimum).\r\nIt would be good if all of these were mentioned alongside the download link.\r\n\r\nI just run into this issue using a GPU with compute architecture 3.7, and TF says it ignores the device because it was not built to support that architecture. It also gives me an info message about the lib not built with SSE4.1 SSE4.2 AVX AVX2 FMA in mind.\r\n", "comments": []}, {"number": 29283, "title": "[TF 2.0 API Docs] tf.nn.convolution", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/convolution\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Usage example\r\n\r\nNeeds usage example\r\n", "comments": []}, {"number": 29217, "title": "Memory leak using C_API", "body": "Hello!\r\n\r\nI'm experiencing memory leak in the pre-built tensorflow library using the C_API.\r\nAre there any (preferably valgrind) docs/ignore files which describes whats are the possible false detections?\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, using the c_api\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 64 bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): libtensorflow-gpu-linux-x86_64-1.13.1\r\n- Python version: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A (5.4.0 20160609 which used to compile and link my executable to the tensorflow lib)\r\n- CUDA/cuDNN version: CUDA 10.0.130, cuDNN 7.5.0\r\n- GPU model and memory: Titan XP 12Gb (used only for CUDA), 1050Ti 4Gb (used only for the displays)\r\n\r\n**Describe the current behavior**\r\nMemory leak in TF_NewSession, TF_GraphImportGraphDef.\r\n\r\n**Describe the expected behavior**\r\nNo memory leak :)\r\n\r\n**Code to reproduce the issue**\r\n(sorry, my codebase is huge, this is only workflow how I use the c_api, error checking ommitted for simplicity)\r\n```\r\nTF_Graph* graph = TF_NewGraph();\r\nTF_Status* status = TF_NewStatus(); // are status objects re-usable?\r\nTF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();\r\nTF_ImportGraphDefOptionsSetDefaultDevice(opts, \"/gpu:0\");\r\nTF_Buffer* buffer = TF_NewBuffer();\r\n// code to load a .pb file into buffer\r\nTF_GraphImportGraphDef(graph, buffer, opts, status); // this is one line where valgrind says there is a memleak\r\nTF_DeleteImportGraphDefOptions(opts);\r\nTF_DeleteBuffer(buffer);\r\nbuffer = nullptr;\r\n// code to get input operations and create/fill input tensor with input data\r\nTF_SessionOptions* options = TF_NewSessionOptions();\r\nuint8_t config[7] = {0x32, 0x5, 0x20, 0x1, 0x2a, 0x01, 0x30}; // protobuf data for auto memory gpu_options.allow_growth=True and gpu_options.visible_device_list=\"0\" \r\nTF_SetConfig(options,(void*)config,7,status);\r\nTF_Session* session = TF_NewSession(graph, options, status);\r\nTF_DeleteSessionOptions(options);\r\noptions = nullptr;\r\n\r\n// code to create arrays for input and output tensors, etc\r\n// note that TF_SessionRun is called thousands of times, I cant associate any leak detected by valgrind to it (in other words, im not sure its leak free but I hope so :) )\r\nTF_SessionRun(session,\r\n              nullptr, // no options\r\n              inputs, input_tensors, static_cast<int>(ninputs), // Input tensors, input tensor values, number of inputs.\r\n              outputs, output_tensors, static_cast<int>(noutputs), // Output tensors, output tensor values, number of outputs.\r\n              nullptr, 0, // Target operations, number of targets.\r\n              nullptr, // Run metadata.\r\n              status // Output status.\r\n  );\r\n\r\n\r\n// code to call TF_DeleteTensor on the output_tensors (since TF_SessionRun passes the ownership to the caller code)\r\nTF_CloseSession(session, status);\r\nTF_DeleteSession(session, status); \r\nsession = nullptr;\r\nTF_DeleteGraph(graph);\r\ngraph = nullptr;\r\nTF_DeleteStatus(status);\r\nstatus = nullptr;\r\n\r\n```\r\n\r\n**Other info / logs**\r\nLog from valgrind (these are only the detections marked as \"definitely lost\"):\r\n```\r\n400 bytes in 1 blocks are definitely lost in loss record 144,731 of 148,938\r\n   at 0x4C2FB55: calloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\r\n   by 0x40138B4: allocate_dtv (dl-tls.c:322)\r\n   by 0x40138B4: _dl_allocate_tls (dl-tls.c:539)\r\n   by 0x515026E: allocate_stack (allocatestack.c:588)\r\n   by 0x515026E: pthread_create@@GLIBC_2.2.5 (pthread_create.c:539)\r\n   by 0x1A97ADC2: std::thread::_M_start_thread(std::shared_ptr<std::thread::_Impl_base>, void (*)()) (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21)\r\n   by 0x1A97AECC: std::thread::_M_start_thread(std::shared_ptr<std::thread::_Impl_base>) (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21)\r\n   by 0x24C3511F: tensorflow::(anonymous namespace)::PosixEnv::StartThread(tensorflow::ThreadOptions const&, std::string const&, std::function<void ()>) (in libtensorflow_framework.so)\r\n   by 0x24C0D8FC: tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int, bool, Eigen::Allocator*) (in libtensorflow_framework.so)\r\n   by 0x24C0DB6E: tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, std::string const&, int) (in libtensorflow_framework.so)\r\n   by 0x24BB479D: tensorflow::GraphRunner::GraphRunner(tensorflow::Env*) (in libtensorflow_framework.so)\r\n   by 0xCBC1C86: tensorflow::ShapeRefiner::ShapeRefiner(int, tensorflow::OpRegistryInterface const*) (in libtensorflow.so)\r\n   by 0xCBD68E8: tensorflow::ImportGraphDef(tensorflow::ImportGraphDefOptions const&, tensorflow::GraphDef const&, tensorflow::Graph*, tensorflow::ShapeRefiner*, tensorflow::ImportGraphDefResults*) (in libtensorflow.so)\r\n   by 0x6460FF0: GraphImportGraphDefLocked (in libtensorflow.so)\r\n\r\n400 bytes in 1 blocks are definitely lost in loss record 144,732 of 148,938\r\n   at 0x4C2FB55: calloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\r\n   by 0x40138B4: allocate_dtv (dl-tls.c:322)\r\n   by 0x40138B4: _dl_allocate_tls (dl-tls.c:539)\r\n   by 0x515026E: allocate_stack (allocatestack.c:588)\r\n   by 0x515026E: pthread_create@@GLIBC_2.2.5 (pthread_create.c:539)\r\n   by 0x309423EE: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.418.56)\r\n   by 0x309B4E15: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.418.56)\r\n   by 0x308E795C: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.418.56)\r\n   by 0x308E9BDE: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.418.56)\r\n   by 0x3081A8EB: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.418.56)\r\n   by 0x3096A85A: cuDevicePrimaryCtxRetain (in /usr/lib/x86_64-linux-gnu/libcuda.so.418.56)\r\n   by 0x2505CDDC: stream_executor::cuda::CUDADriver::CreateContext(int, stream_executor::DeviceOptions const&, stream_executor::cuda::CudaContext**) (in libtensorflow_framework.so)\r\n   by 0x25064CB3: stream_executor::cuda::CUDAExecutor::Init(int, stream_executor::DeviceOptions) (in libtensorflow_framework.so)\r\n   by 0x24FA1A86: stream_executor::StreamExecutor::Init(int, stream_executor::DeviceOptions) (in libtensorflow_framework.so)\r\n\r\n400 bytes in 1 blocks are definitely lost in loss record 144,733 of 148,938\r\n   at 0x4C2FB55: calloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\r\n   by 0x40138B4: allocate_dtv (dl-tls.c:322)\r\n   by 0x40138B4: _dl_allocate_tls (dl-tls.c:539)\r\n   by 0x515026E: allocate_stack (allocatestack.c:588)\r\n   by 0x515026E: pthread_create@@GLIBC_2.2.5 (pthread_create.c:539)\r\n   by 0x1A97ADC2: std::thread::_M_start_thread(std::shared_ptr<std::thread::_Impl_base>, void (*)()) (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21)\r\n   by 0x1A97AECC: std::thread::_M_start_thread(std::shared_ptr<std::thread::_Impl_base>) (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21)\r\n   by 0x24C3511F: tensorflow::(anonymous namespace)::PosixEnv::StartThread(tensorflow::ThreadOptions const&, std::string const&, std::function<void ()>) (in libtensorflow_framework.so)\r\n   by 0x24C0D8FC: tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int, bool, Eigen::Allocator*) (in libtensorflow_framework.so)\r\n   by 0x24C0DB6E: tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, std::string const&, int) (in libtensorflow_framework.so)\r\n   by 0x24BB479D: tensorflow::GraphRunner::GraphRunner(tensorflow::Env*) (in libtensorflow_framework.so)\r\n   by 0xCBC1C86: tensorflow::ShapeRefiner::ShapeRefiner(int, tensorflow::OpRegistryInterface const*) (in libtensorflow.so)\r\n   by 0xCBD5FA8: tensorflow::ConvertGraphDefToGraph(tensorflow::GraphConstructorOptions const&, tensorflow::GraphDef const&, tensorflow::Graph*) (in libtensorflow.so)\r\n   by 0xC804F89: tensorflow::GraphExecutionState::InitBaseGraph(tensorflow::BuildGraphOptions const&) (in libtensorflow.so)\r\n\r\n136 (80 direct, 56 indirect) bytes in 1 blocks are definitely lost in loss record 138,565 of 148,938\r\n   at 0x4C2E0EF: operator new(unsigned long) (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\r\n   by 0x24C3506A: tensorflow::(anonymous namespace)::PosixEnv::StartThread(tensorflow::ThreadOptions const&, std::string const&, std::function<void ()>) (in libtensorflow_framework.so)\r\n   by 0x24C0D8FC: tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int, bool, Eigen::Allocator*) (in libtensorflow_framework.so)\r\n   by 0x24C0DB9A: tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int) (in libtensorflow_framework.so)\r\n   by 0x24BBA1AA: tensorflow::LocalDevice::LocalDevice(tensorflow::SessionOptions const&, tensorflow::DeviceAttributes const&) (in libtensorflow_framework.so)\r\n   by 0x24BF3C4A: tensorflow::ThreadPoolDevice::ThreadPoolDevice(tensorflow::SessionOptions const&, std::string const&, tensorflow::gtl::IntType<tensorflow::Bytes_tag_, long long>, tensorflow::DeviceLocality const&, tensorflow::Allocator*) (in libtensorflow_framework.so)\r\n   by 0x24B54DB5: tensorflow::GPUCompatibleCPUDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (in libtensorflow_framework.so)\r\n   by 0x24B8934E: tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (in libtensorflow_framework.so)\r\n   by 0xC7E8B6C: tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (in libtensorflow.so)\r\n   by 0x24BE9EF8: tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (in libtensorflow_framework.so)\r\n   by 0x645C734: TF_NewSession (in libtensorflow.so)\r\n   \r\n LEAK SUMMARY:\r\n    definitely lost: 1,280 bytes in 4 blocks\r\n    indirectly lost: 56 bytes in 2 blocks\r\n      possibly lost: 682,056 bytes in 142 blocks\r\n    still reachable: 155,113,486 bytes in 238,127 blocks\r\n                       of which reachable via heuristic:\r\n                         stdstring          : 2,753,050 bytes in 69,263 blocks\r\n                         newarray           : 5,656 bytes in 12 blocks\r\n         suppressed: 925,188,500 bytes in 620,115 blocks\r\n\r\n```\r\nI'm using a valgrind ignore file which already ignores known false detections inside CUDA/cuDNN.\r\nAll of the definitely lost, indirectly lost and possibly lost comes from the tensorflow library.\r\n", "comments": ["As far as I can tell this means we're leaking threads. These leaks should be harmless (no need to shut down all background processing threads on process shutdown; also the number of threads is constant) but it'd still be nice not to have them.\r\n\r\n@mhong can you triage this to someone in the runtime team?", "Has there been any progress? It looks like this is still a thing as of r1.15 as well as on r2.2", "Sorry for the late reply.\r\n\r\nAs Alex said, this is because TensorFlow has a few static global threadpool. Here are a few examples:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/single_threaded_cpu_device.cc#L34\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/local_device.cc#L55\r\n\r\nBut these are harmless.", "> But these are harmless.\r\n\r\nI trust your opinion, though it's always a bit painful to have to deal with this amount of noise, even when it's harmless, and it can add troubles for other contributors. I know, we can always exclude bits from valgrind analysis, but I'm also lazy, somehow :-). I could confirm, reducing inter and intra op parallelism to 1 thread, that we only had 76kbits leaked. No idea if the reminder is legit leaks or false alarm, I'll investigate next week.", "> Sorry for the late reply.\r\n> \r\n> As Alex said, this is because TensorFlow has a few static global threadpool. Here are a few examples:\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/single_threaded_cpu_device.cc#L34\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/local_device.cc#L55\r\n> \r\n> But these are harmless.\r\n\r\n@qqfish Do you think it would be possible for TensorFlow to publish a valgrind suppression list that we can rely safely on? Looked through the repo and I could not find any.", "@KocsisV We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "@kumariko \r\nI'm not using tensorflow anymore, so I can't check if the issue is still present or not in 2.6. Due to compatibility issues back then I stayed with 1.X, as the early 2.X releases did not provide the C_API.\r\nQuickly checking the repository, there is still no docs listing falsely detected memory leaks (if there is any) for example in a form of a valgrind suppression file."]}, {"number": 29198, "title": "[2.0] SparseTensor shape becomes none after AutoGraph", "body": "The bug is found with `tf-nightly-gpu-2.0-preview (2.0.0.dev20190530)`.\r\n\r\nTo reproduce:\r\n\r\n<pre>\r\nimport tensorflow as tf\r\n\r\nx = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\r\n\r\ndef foo(q):\r\n  print(q.shape)\r\n\r\nfoo(x)                              # Output: (3,4)\r\ntf.function(foo)(x)                 # Output: (None, None)\r\n</pre>\r\n\r\nThe version `2.0.0alpha0` does not have this bug.", "comments": ["@llan-ml Able to reproduce the issue in tf-nightly-gpu-2.0-preview (2.0.0.dev20190530) and (2.0.0.dev20190603) . ", "@edloper I think this is related to the recent composite tensor destructuring in tf.function and not so much related to autograph.\r\n\r\nDoes that make sense to you? If so, what do you think we should do here?", "Prior to PR d2502f0, tf.function treated SparseTensor as opaque Python objects, and would retrace the function unless the *same* SparseTensor (the identical object) was passed in.  E.g.:\r\n\r\n```\r\n>>> a = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\r\n>>> b = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\r\n>>> c = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\r\n>>> @tf.function\r\n... def foo(q):\r\n...   print('tracing for', q.shape)\r\n\r\n# OLD BEHAVIOR (prior to PR d2502f0)\r\n>>> foo(a), foo(b), foo(c)  # retraces each time it's called.\r\ntracing for (3, 4)\r\ntracing for (3, 4)\r\ntracing for (3, 4)\r\n```\r\n\r\nPR d2502f0 updated `tf.function` to understand the structure of SparseTensor and related types (like RaggedTensor), and to not retrace the function when different SparseTensors are passed in.\r\n\r\n```\r\n# NEW BEHAVIOR (after PR d2502f0)\r\n>>> foo(a), foo(b), foo(c)  # note: only traced one time.\r\ntracing for (?, ?)\r\n```\r\n\r\nThis impacts `SparseTensor`, because the values of tensors (including `SparseTensor.dense_shape`) are not available inside the body of the traced function.  E.g.:\r\n\r\n```\r\n>>> t = tf.constant([1, 2, 3])\r\n>>> def foo(t):\r\n...   print tf.get_static_value(t)\r\n>>> foo(t)\r\n[1, 2, 3]\r\n>>> tf.function(foo)(t)\r\nNone\r\n```\r\n\r\nIn the case of SparseTensor, the shape is stored in a tensor (namely `SparseTensor.dense_shape`, so the value isn't available inside the function body.  (The SparseTensor.shape property effectively just returns `tf.get_static_value(self.dense_shape)`).\r\n\r\nIf you need the sparse tensor shape inside your function, and don't mind retracing for each different shape that you're called with, a workaround might be to do something like this:\r\n\r\n```\r\ndef foo(x):\r\n  return _foo(x, x.shape)\r\n\r\n@tf.function\r\ndef _foo(x, x_shape):\r\n  ...function body...\r\n```\r\n", "@edloper Thanks for you explanation. \r\n\r\nIn my case, I need to write a custom dense layer to support sparse inputs. The keras `Layer.__call__` will automatically defer the shape of inputs to build variables. However, the deferred shapes are always `None` for SparseTensors, and it will raise errors since the shape of variables being built cannot be determined.\r\n\r\nIf I remember correctly, keras just uses `inputs` to defer input shapes, and explicitly passing a shape array seems not to solve the problem. Possibly I need to pass an extra shape array to the `__init__` of my class.\r\n\r\nAnyway, I'm wondering if this behavior will still exist or you will make a change.", "@llan-ml \r\nplease let us know if the issue still persist", "@Saduf2019 Yes. If we use a sparse tensor as an argument of `tf.function`, each dimension of the shape of that sparse tensor inside the function is always None.", "I could reproduce the issue with TF 2.2-rc4.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/ab5b196622f4ef300470e641e974b340/untitled861.ipynb).Thanks!", "I could reproduce the issue with TF 2.2,2.3-rc1, nightly versions(`2.4.0-dev20200715`) .Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/27c8ebcf4760f0e07c6ad574346fb42f/untitled131.ipynb).Thanks!", "I could reproduce the issue with TF 2.5 .Please, find the gist [here](https://colab.research.google.com/gist/Saduf2019/e19bf295505b2f4c6af8c41fa345f995/untitled589.ipynb).Thanks!", "I could reproduce the issue with TF 2.6 .Please, find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/66c89f1252dabfbb95f65d0fe0f2c9f4/untitled589.ipynb#scrollTo=grgBDlrgCxqN).Thanks!", "@llan-ml This issue is not replicating in TF v2.7.0 on colab . Could you please have a look at the [gist](https://colab.research.google.com/gist/sushreebarsa/c44b219aafee43af566a6ccaf9b17061/gist29198.ipynb) and let us know if it helps? Thank you!", "@sushreebarsa The issue still persists. You should use `tf.function(foo)(x)` instead of `tf.function(foo(x))`."]}, {"number": 29039, "title": "Support the __cuda_array_interface__ protocol", "body": "- Are you willing to contribute it (Yes/No):\r\n\r\nNot me personally, but perhaps someone I work with\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nA number of GPU array computing libraries in Python (Numba, CuPy, PyTorch, RAPIDS) support the `__cuda_array_interface__`, protocol as described in the [Numba documentation](https://numba.pydata.org/numba-doc/dev/cuda/cuda_array_interface.html).  This protocol helps to migrate data between different GPU array computing systems without explicit coordination.  \r\n\r\n```python\r\nx = lib_1.create_array()\r\ny = lib_2.as_array(x)\r\n```\r\n\r\nor more concretely\r\n\r\n```python\r\nt = torch.Tensor(...)\r\nx = tensorflow.convert_to_tensor(t)\r\n```\r\n\r\nThis involves two changes to TensorFlow\r\n\r\n1.  We would add a property `__cuda_array_interface__` to tensor objects backed by a GPU that provided information about the GPU memory.\r\n2.  We would add checks in functions designed to convert external objects into TensorFlow tensors to check for this attribute and use it if present.\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nOnly in a backwards compatible way.  It will add a new property `__cuda_array_interface__` to tensor objects, an will also add a check for these objects in functions designed to convert other objects into TensorFlow tensors.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThis typically already exists on the CPU side with protocols like `__array__`, but on the GPU side things are still manual.  Protocols like this make it easier both to use these array computing libraries together, and also to make external systems that are generic and can interoperate with a number of different array computing libraries.\r\n\r\n**Any Other info.**\r\n\r\nRelevant issues in other libraries:\r\n\r\n-  https://github.com/pytorch/pytorch/issues/11914\r\n-  https://github.com/pytorch/pytorch/issues/15601\r\n-  https://github.com/rapidsai/cudf/issues/529\r\n-  https://github.com/cupy/cupy/pull/1144\r\n-  https://docs-cupy.chainer.org/en/stable/reference/interoperability.html\r\n\r\ncc @seibert ", "comments": ["@mhong I think this has nontrivial implications to the GPU runtime as it'll affect what streams we can enqueue a tensor in and when its memory is allowed to be valid. We can probably make this work if we make __cuda_array_interface__ sync the stream before returning a pointer.\r\n\r\nCan someone from the GPU runtime team comment on the feasibility of this?", "From a user point of view, I do not see any problem about doing a sync before returning the ptr.", "Adding @guptapriya and @poxvoculi, RE: GPU runtime. \ud83d\udc4d ", "cc @tatianashp ", "btw, we recently added support of `__cuda_array_interface__` to mpi4py, see this comment and links therein: https://github.com/chainer/chainer/issues/7476#issuecomment-511172683. I am not sure if tensorflow can work with MPI/mpi4py (looks like not?), but this could be of interest for many people working with `__cuda_array_interface__`.\r\n\r\ncc: @mrocklin ", "I'm very glad to hear it @leofang !  It's great to see so many community projects take on that protocol.\r\n\r\n>  I am not sure if tensorflow can work with MPI/mpi4py (looks like not?), but this could be of interest for many people working with __cuda_array_interface__.\r\n\r\nWell, presumably if TensorFlow chose to implement support for `__cuda_array_interface__` then people could more easily integrate mpi4py with TensorFlow in custom applications without TensorFlow having to provide explicit support.", "I agree with the assessment from @alextp, and would also like to get expert opinion from TF GPU engineers. @aaroey, what do you think? Thanks.", "@mhong right, I think stream and allocator management would be a problem. I'm not sure how pytorch allocate tensors, it may work if `t` is ready by the time when `convert_to_tensor(t)` is called. But in general I think this requires a non-trivial amount of changes to TF core. Marked as \"contributions welcome\".", "To be clear, there are people around who are ready to do this work if it's likely to go in.  \r\n\r\n> it may work if t is ready by the time when convert_to_tensor(t) is called. \r\n\r\nRight, so I think that people have suggested that an explicit synchronization step would be acceptable from a user's perspective.  I personally am not familiar with the Tensorflow API or internals, but I suspect that you all have such a step.\r\n\r\n>  I think this requires a non-trivial amount of changes to TF core\r\n\r\nThis is somewhat surprising.  I would expect that we might call some synchronization step to make sure that the memory is available, then get an address of that memory and construct the dictionary and be done.  Is there significantly more than this?  My apologies for raising issues in a project for which I do not have great familiarity.  Thank you for any slack you're able to give here.", "> > I think this requires a non-trivial amount of changes to TF core\r\n> \r\n> This is somewhat surprising. I would expect that we might call some synchronization step to make sure that the memory is available, then get an address of that memory and construct the dictionary and be done. Is there significantly more than this? My apologies for raising issues in a project for which I do not have great familiarity. Thank you for any slack you're able to give here.\r\n\r\nThis is indeed quite surprising, perhaps there's some confusion in the CUDA Array Interface? All it asks is really just a device pointer to some area in the global memory, which can be non-contiguous as long as the optional `strides` field is specified. The device on which the pointer is valid, if needed, can be retrieved by passing the device pointer to the CUDA API (`cudaPointerGetAttributes`). Can you elaborate why adding such support is challenging? I'm curious.\r\n\r\nDisclaimer: I am not at all familiar with TensorFlow either.", "Exposing a pointer is easy; making sure the information you want is\nactually in the memory pointed to by that pointer when you dereference it\nis less so, because TF's GPU runtime is asynchronous; you'd need to enqueue\nwork on TF's compute stream to deref that pointer or sync that stream, and\nwe don't have APIs to let you do either easily.\n\nOn Sat, Aug 31, 2019 at 12:00 AM Leo Fang <notifications@github.com> wrote:\n\n> I think this requires a non-trivial amount of changes to TF core\n>\n> This is somewhat surprising. I would expect that we might call some\n> synchronization step to make sure that the memory is available, then get an\n> address of that memory and construct the dictionary and be done. Is there\n> significantly more than this? My apologies for raising issues in a project\n> for which I do not have great familiarity. Thank you for any slack you're\n> able to give here.\n>\n> This is indeed quite surprising, perhaps there's some confusion in the\n> CUDA Array Interface? All it asks is really just a device pointer to some\n> area in the global memory, which can be non-contiguous as long as the\n> optional strides field is specified. The device on which the pointer is\n> valid, if needed, can be retrieved by passing the device pointer to the\n> CUDA API (cudaPointerGetAttributes). Can you elaborate why adding such\n> support is challenging? I'm curious.\n>\n> Disclaimer: I am not at all familiar with TensorFlow either.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/29039?email_source=notifications&email_token=AAABHROI2OHIUKY5EVSWUGTQHIJKRA5CNFSM4HPW33H2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5TG4MA#issuecomment-526806576>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRKEOFHCQSXNGZZJUB3QHIJKRANCNFSM4HPW33HQ>\n> .\n>\n\n\n-- \n - Alex\n", "IIUC the consensus is it would be ok for this to be a blocking request. Though someone please correct me if I am mistaken.", "Also xrefing a similar request for jax. ( https://github.com/google/jax/issues/1100 )", "@alextp Thanks for your clarification, I see the challenge now. \r\n\r\ncc @seibert @sklam: Do you think it's necessary to add the stream pointer on which the device pointer is last accessed to `__cuda_array_interface__`, should a synchronization be necessary for interoperability? ", "In [TF 1.15.0-rc0](https://github.com/tensorflow/tensorflow/releases/tag/v1.15.0-rc0) announce, it have this new feature:\r\n\r\n\u2022\tEagerTensor now supports numpy buffer interface for tensors.\r\n\r\n__cuda_array_interface__ is the equivalent GPU of that interface. Just to make sure people in this thread knew about that.", "> In [TF 1.15.0-rc0](https://github.com/tensorflow/tensorflow/releases/tag/v1.15.0-rc0) announce, it have this new feature:\r\n> \r\n> \u2022 EagerTensor now supports numpy buffer interface for tensors.\r\n> \r\n> **cuda_array_interface** is the equivalent GPU of that interface. Just to make sure people in this thread knew about that.\r\n\r\n[TensorToNdarray](https://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=TensorToNdarray&type=)\r\n[TF_TensorToPyArray](https://github.com/tensorflow/tensorflow/search?q=TF_TensorToPyArray&unscoped_q=TF_TensorToPyArray)\r\n[TFE_TensorHandleToNumpy](https://github.com/tensorflow/tensorflow/search?q=TFE_TensorHandleToNumpy&unscoped_q=TFE_TensorHandleToNumpy)\r\n\r\n[NdarrayToTensor](https://github.com/tensorflow/tensorflow/search?q=NdarrayToTensor&unscoped_q=NdarrayToTensor)\r\n[PyArrayToTF_Tensor](https://github.com/tensorflow/tensorflow/search?q=PyArrayToTF_Tensor&unscoped_q=PyArrayToTF_Tensor)\r\n[NumpyToTFE_TensorHandle](https://github.com/tensorflow/tensorflow/search?q=NumpyToTFE_TensorHandle&unscoped_q=NumpyToTFE_TensorHandle)\r\n\r\n", "Redirecting the discussion on supporting non-default streams in `__cuda_array_interface__` to numba/numba#4933.", "Once [TensorFlow DLPack support](https://github.com/tensorflow/community/pull/180) lands, what else remains to be done here?", "@sanjoy It should be fairly simple (for TF developers) to implement the proposed `__cuda_array_interface__` protocol. It is independent of the DLPack support and intended to be simpler than DLPack for both framework developers and general users. CuPy, for example, has the support of both: https://docs-cupy.chainer.org/en/stable/reference/interoperability.html; if you need, I can point you to the actual implementations. My impression is that DLPack needs a lengthier code than CUDA Array Interface does. \r\n\r\nOn the producer side, you only need to define a `__cuda_array_interface__` property (or attribute) for any TF tensors as a Python `dict` that contains the information specified by the protocol. On the receiver side, you need to accept any Python object in the TF tensor creation routines, examine whether it comes with a `__cuda_array_interface__` property, and create a tensor accordingly with zero copy. \r\n\r\nAs for the stream discussion:\r\n\r\n> Redirecting the discussion on supporting non-default streams in `__cuda_array_interface__` to [numba/numba#4933](https://github.com/numba/numba/issues/4933).\r\n\r\nAccording to what's discussed there, the consensus is that any CUDA stream issue (as @alextp brought up earlier) should be handled by the users. If the TF team considers it's better to be handled on the producer side, please feel free to do so. The worst scenario is that a stream or device synchronization is needed at the interoperating point, but as long as it's clearly documented, it's still way better than not having the support. (See how many +1 that the original issue has accumulated!) \r\n\r\nI still know nothing about TF, but I should be able to help review PRs related to `__cuda_array_interface__` (as part of my learning process) \ud83d\ude42", "Can you point me to the spec for `__cuda_array_interface__`?\r\n\r\n> My impression is that DLPack needs a lengthier code than CUDA Array Interface does.\r\n\r\nHow so?  In particular, IIUC DLPack tensors do not have the \"mask\" attribute (and so should be easier to support), although that seems to be optional in `__cuda_array_interface__` because Numba does not support it?\r\n\r\n> On the producer side, you only need to define a `__cuda_array_interface__` property (or attribute) for any TF tensors as a Python `dict` that contains the information specified by the protocol. On the receiver side, you need to accept any Python object in the TF tensor creation routines, examine whether it comes with a `__cuda_array_interface__` property, and create a tensor accordingly with zero copy.\r\n\r\nThis sounds very similar to DLPack actually.\r\n\r\nI'm wondering if we can should build support for `__cuda_array_interface__` together with DLPack.  Perhaps TF needs `TensorFromForeignMemory` and `TensorToForeignMemory` ops that produce and consume a `(memory pointer, shape, data type)` tuple.  Both DLPack and `__cuda_array_interface__` can be implemented on top of this functionality.\r\n\r\n> As for the stream discussion:\r\n> \r\n> > Redirecting the discussion on supporting non-default streams in `__cuda_array_interface__` to [numba/numba#4933](https://github.com/numba/numba/issues/4933).\r\n> \r\n> According to what's discussed there, the consensus is that any CUDA stream issue (as @alextp brought up earlier) should be handled by the users. If the TF team considers it's better to be handled on the producer side, \r\n\r\nThis isn't really up to the TF team to decide.  `__cuda_array_interface__` needs to have a specification independent of any particular implementation, and that should decide the contract around streams.  The contract can be \"there is no contract\", but it needs to be explicitly spelled out in the `__cuda_array_interface__` spec.\r\n\r\nI understand that this can sound pedantic, but IMO we need to be precise about these things if we want to have any hope of maintaining and debugging this functionality long term.\r\n\r\n>please feel free to do so. The worst scenario is that a stream or device synchronization is needed at the interoperating point, but as long as it's clearly documented, it's still way better than not having the support. (See how many +1 that the original issue has accumulated!)\r\n>\r\n> I still know nothing about TF, but I should be able to help review PRs related to `__cuda_array_interface__` (as part of my learning process)", "> Can you point me to the spec for __cuda_array_interface__?\r\n>\r\n\r\nAll needed information is provided in the original post above. The spec is here: http://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html\r\n\r\n> How so? In particular, IIUC DLPack tensors do not have the \"mask\"\r\n> attribute (and so should be easier to support), although that seems to be\r\n> optional in __cuda_array_interface__ because Numba does not support it?\r\n>\r\n\r\nI took a quick look at DLPack spec and I think they are quite similar as you said. A few minor differences I can think of are that the CUDA Array Interface has treatments for zero-size arrays and C-contiguity.\r\n\r\nNumba, CuPy, and mpi4py do not support mask. I don\u2019t know who uses it to be honest.\r\n\r\n>\r\n> This sounds very similar to DLPack actually.\r\n>\r\n> I'm wondering if we can should build support for __cuda_array_interface__\r\n> together with DLPack. Perhaps TF needs TensorFromForeignMemory and\r\n> TensorToForeignMemory ops that produce and consume a (memory pointer,\r\n> shape, data type) tuple. Both DLPack and __cuda_array_interface__ can be\r\n> implemented on top of this functionality.\r\n>\r\n\r\nI think this is a reasonable approach that other libraries more or less follow too.\r\n\r\n> This isn't really up to the TF team to decide. __cuda_array_interface__\r\n> needs to have a specification independent of any particular implementation,\r\n> and that should decide the contract around streams. The contract can be\r\n> \"there is no contract\", but it needs to be explicitly spelled out in the\r\n> __cuda_array_interface__ spec.\r\n>\r\n> I understand that this can sound pedantic, but IMO we need to be precise\r\n> about these things if we want to have any hope of maintaining and debugging\r\n> this functionality long term.\r\n>\r\n\r\nI think TF is by far the only library that has this issue. All other supporting libraries (documented in\r\nhttps://github.com/numba/numba/issues/4448) do not have any problem, neither does DLPack specify stream. As a library maintainer, I really don\u2019t want to see extra complexities added to the spec. As an open-source contributor, I refer and encourage you again to join the discussion in https://github.com/numba/numba/issues/4933.\r\n\r\nThanks!", "IIRC cuDF uses the mask. Though end users could fill in these values or extract the useful entries if needed.\n\nIn any event for consuming it\u2019s probably reasonable for you to raise an error if the mask is non-trivial. As far as producing would just leave the mask out.\n\ncc @kkraus14", "> All needed information is provided in the original post above. The spec is here: http://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html\r\n\r\nThanks!  I did see that link before but I didn't realize that that was the reference spec.\r\n\r\n> I took a quick look at DLPack spec and I think they are quite similar as you said. A few minor differences I can think of are that the CUDA Array Interface has treatments for zero-size arrays and C-contiguity. Numba, CuPy, and mpi4py do not support mask. I don\u2019t know who uses it to be honest.\r\n>> This sounds very similar to DLPack actually. I'm wondering if we can should build support for __cuda_array_interface__ together with DLPack. Perhaps TF needs TensorFromForeignMemory and TensorToForeignMemory ops that produce and consume a (memory pointer, shape, data type) tuple. Both DLPack and __cuda_array_interface__ can be implemented on top of this functionality.\r\n> I think this is a reasonable approach that other libraries more or less follow too.\r\n\r\nOk.  Do you think it makes sense to make the [DLPack RFC](https://github.com/tensorflow/community/pull/180) a joint RFC (if the DLPack authors are OK with this)?\r\n\r\n>> This isn't really up to the TF team to decide. __cuda_array_interface__ needs to have a specification independent of any particular implementation, and that should decide the contract around streams. The contract can be \"there is no contract\", but it needs to be explicitly spelled out in the __cuda_array_interface__ spec. I understand that this can sound pedantic, but IMO we need to be precise about these things if we want to have any hope of maintaining and debugging this functionality long term.\r\n\r\n> I think TF is by far the only library that has this issue. All other supporting libraries (documented in [numba/numba#4448](https://github.com/numba/numba/issues/4448)) do not have any problem, neither does DLPack specify stream. As a library maintainer, I really don\u2019t want to see extra complexities added to the spec.\r\n\r\nThat's completely fair.  We can of course start with something simple and add more features and optimizations only when there is a clear need.\r\n\r\nHowever, here is what I want to avoid: I want to avoid an informal agreement between `__cuda_array_interface__` and TensorFlow (and other frameworks) where TF is expected to \"do the Right Thing\" when it isn't clear what the right thing is.  It seems like for `__cuda_array_interface__` the Right Thing is that the memory behaves as if it were available on the default stream for the device.  Is that correct?  If yes, would you be open to adding this to the spec?\r\n\r\nThis also opens up a slightly more elegant (IMO) implementation strategy -- TF can probably add dependencies between the default stream and its own streams to implement the \"as if on default stream\" behavior.  I'd expect this to be faster than a host/device sync.\r\n\r\n> As an open-source contributor, I refer and encourage you again to join the discussion in [numba/numba#4933](https://github.com/numba/numba/issues/4933). Thanks!\r\n\r\nGood point, I'll add what I said above to that discussion.", "> Ok. Do you think it makes sense to make the [DLPack RFC](https://github.com/tensorflow/community/pull/180) a joint RFC (if the DLPack authors are OK with this)?\r\n\r\nI have no objection for any work plan as long as the support can be added \ud83d\ude42 But it\u2019d be nice to get them on board for this.\r\n\r\n> That's completely fair. We can of course start with something simple and add more features and optimizations only when there is a clear need.\r\n> \r\n> However, here is what I want to avoid: I want to avoid an informal agreement between `__cuda_array_interface__` and TensorFlow (and other frameworks) where TF is expected to \"do the Right Thing\" when it isn't clear what the right thing is. It seems like for `__cuda_array_interface__` the Right Thing is that the memory behaves as if it were available on the default stream for the device. Is that correct? If yes, would you be open to adding this to the spec?\r\n\r\nI incline to simply say the user is responsible to ensure the memory is ready at the interoperating point. However, if TF does not provide a public handle for users to sync the TF internal stream, then yes, I suppose this is the right thing to do. At least this would work with CuPy. (I don\u2019t know about the rest, but likely it\u2019d work too. We can discuss more in the Numba thread.) \r\n\r\n> This also opens up a slightly more elegant (IMO) implementation strategy -- TF can probably add dependencies between the default stream and its own streams to implement the \"as if on default stream\" behavior. I'd expect this to be faster than a host/device sync.\r\n\r\nYou meant something using `cudaStreamWaitEvent`? I guess it\u2019d be useful. Syncing between streams generally has lower overhead than between host/device.\r\n\r\n> > As an open-source contributor, I refer and encourage you again to join the discussion in [numba/numba#4933](https://github.com/numba/numba/issues/4933). Thanks!\r\n> \r\n> Good point, I'll add what I said above to that discussion.\r\n\r\nI saw that. Thanks @sanjoy! \r\n", "> You meant something using `cudaStreamWaitEvent`?\r\n\r\nYes.", "I noticed commit ( https://github.com/tensorflow/tensorflow/commit/210649dd56d7c4b75e3e8e2a851b61c80ae13dbb ) appears to implement some `__cuda_array_interface__` support. Am I reading that correctly? If so, what else still needs to be done here?", "This commit only implement the XLA part. It miss the TF part. The TF tensor themself need to support this interface.\n\nT\u00e9l\u00e9charger Outlook pour Android<https://aka.ms/ghei36>\n\n________________________________\nFrom: jakirkham <notifications@github.com>\nSent: Tuesday, March 31, 2020 8:12:33 PM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Frederic Bastien <fbastien@nvidia.com>; Manual <manual@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] Support the __cuda_array_interface__ protocol (#29039)\n\n\nI noticed commit ( 210649d<https://github.com/tensorflow/tensorflow/commit/210649dd56d7c4b75e3e8e2a851b61c80ae13dbb> ) appears to implement some __cuda_array_interface__ support. Am I reading that correctly? If so, what else still needs to be done here?\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/29039#issuecomment-606953247>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AABMF6ZUP4PCVLULG6J5F2TRKKBHDANCNFSM4HPW33HQ>.\n\n-----------------------------------------------------------------------------------\nThis email message is for the sole use of the intended recipient(s) and may contain\nconfidential information.  Any unauthorized review, use, disclosure or distribution\nis prohibited.  If you are not the intended recipient, please contact the sender by\nreply email and destroy all copies of the original message.\n-----------------------------------------------------------------------------------\n"]}, {"number": 28957, "title": "Eager execution in tf.data map_func", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):tf-2.0-alpha\r\n- Are you willing to contribute it (Yes/No):No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI really want the release version of TF2.0 could bring the eager mode in the [map_function](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) in its tf.data API. For now, TF2.0-alpha do not apply eager mode in the map_function in tf.data API. However, pepole usually use map_function in tf.data API to parse data and do some image augmentation. To parse data in TF is really convenient. But when it comes to image augmentation, the official TF API is far less sufficient(under non eager mode), for people could use some origin Python API or the API from many other third party libraries to do image augmentation which is badly in need of eager mode. So eager mode in tf.data map_function pipeline is really really necessary for us tensorflow users.\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["I think this would be awesome. I just had a lot of issues trying to do image augmentation with TF2 and tf.data. TF2 addons image also don't support graph mode for now. They are also working on a fix.", "Would [`tf.numpy_function`](https://www.tensorflow.org/api_docs/python/tf/numpy_function) help here? Pinging @jsimsa for visibility. \ud83d\ude42 ", "This is not planned to be supported in the near future. I will mark this feature request as \"contributions welcome\".\r\n\r\nFor arbitrary Python processing inside of tf.data functions you can either a) rely on Autograph to convert Python idioms to graph idioms and / or b) use `tf.py_function` to insert arbitrary Python processing inside of graph computation. Note that the presence of non-graph Python code inside of the input pipeline can negatively affect performance, prevent optimizations, and reduce interoperability with other TensorFlow APIs (such as the distribution strategy API)."]}, {"number": 28933, "title": "Optimize transpose ops", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n- Are you willing to contribute it (Yes/No): No (I lack the expertise to mess with the graph optimizer).\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTF is not very smart about transpositions. For example, the graph optimizer could do the following:\r\n1. Merge adjacent transpositions (each transpose op costs time and creates a new tensor).\r\n2. Pull transpositions through reshapes where possible (transposing with fewer dimensions should be faster, and is currently often *much* see #32).\r\n3. Merge transpose ops with adjacent matmul ops where possible (turn a matrix transpose into a transpose argument for the matmul op).\r\n\r\nCurrently, none of these appear to happen.\r\nI can provide some test cases that illustrate these improvements if desired.\r\n\r\n**Will this change the current api? How?**\r\nNo API changes.\r\n\r\n**Who will benefit with this feature?**\r\nPeople who build graphs with lots of transpose operations.\r\n", "comments": []}, {"number": 28789, "title": "ExtractImagePatches Op Request", "body": "**System information**\r\n- Ubuntu 14.04\r\n- Tensorflow Installed from Binary:\r\n- TensorFlow version 1.13:\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, MAXIMUM, MAX_POOL_2D, MUL, PAD. Here is a list of operators for which you will need custom implementations: ExtractImagePatches.\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\nThis is the onedrive link for the file\r\n[Google Drive Link](https://drive.google.com/open?id=1pu_IAj2b9GoU4HCDd___MLg_20ikB8DZ)\r\n\r\n**Any other info / logs**\r\n\r\nI was trying to convert Tiny YOLO model to the tflite format when i encountered this error.\r\n", "comments": ["@anidh Please provide the correct link. Thanks!", "Hi @gadagashwini Sorry for the broken link, I've updated the issue with new google drive link."]}, {"number": 28786, "title": "catch keyboard interrupt in tf.function", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0-alpha\r\n- Are you willing to contribute it (Yes/No): Maybe depends on workload\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.** keyboard interrupt in tf.function. Currently I cannot keyboard interrupt or break the loop in tf.function made training function.\r\n\r\n**Will this change the current api? How?** Depends on how tf 2.0 works. Might need to change the way c++ and python interface so that we can kill the threads.\r\n\r\n**Who will benefit with this feature?** Developers. We don't need to pkill a training process, restarts it, load checkpoint and do inference. ", "comments": ["I don't have the bandwidth to work on this now but would welcome a PR.", "If this is still open, I'd like to take a stab at it.", "@mrry does this overlap with the cancellationmanager thing you started looking at?", "There is probably some overlap, although I was only planning to develop support for programmatic cancellation, and not intercepting Python\u2019s `KeyboardInterrupt`.\r\n\r\n@vedhasp It would be great if you took a stab at adding this, and I\u2019d be happy to help you get it in! Before sending a complete Pull Request, could you sketch your plans on this issue thread, and we can make sure that it\u2019s compatible with the existing cancellation support in the TensorFlow runtime? Thanks!"]}, {"number": 28673, "title": "Unsupported Operation (MEAN) while trying to apply GpuDelegate to tflite ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- I am writing a custom Android app, built in Android Studio using the method mentioned [here](https://www.tensorflow.org/lite/performance/gpu#android_with_android_studio)\r\n- Host system: Linux Ubuntu 16.04\r\n- Target Device: Android s8+\r\n- tflite installed from (source or binary): nightly AAR (org.tensorflow:tensorflow-lite:0.0.0-nightly)\r\n- tflite-gpu installed from (source or binary): nightly AAR (org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly)\r\n- GPU model and memory: Adreno 540\r\n\r\n**Describe the current behavior**\r\nWhen I attempt to load my .tflite model on the Android (after applying the GpuDelegate as described [here](https://www.tensorflow.org/lite/performance/gpu#android)), it fails saying that 'MEAN' is an unsupported operation (see traceback below)\r\n\r\n**Describe the expected behavior**\r\nThe net loads and runs fine if I do not attempt to apply the GPU delegate before the attempting to load it. The only difference in the code is the following lines:\r\n```\r\nc.delegate = new GpuDelegate();\r\nc.tfliteOptions.addDelegate(c.delegate);\r\n```\r\n\r\n**Code to reproduce the issue**\r\nUnfortunately I cannot provide my .tflite file, but it obviously contains a 'MEAN' operation. Is there a script anywhere in the tflite repo that prints out all the operations contained in a .tflite file? I looked briefly but was unable to find one.\r\n\r\n**Other info / logs**\r\n```\r\n05-13 12:09:56.101 3348-3348/org.qus.viewqualTFLite E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: org.qus.viewqualTFLite, PID: 3348\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{org.qus.viewqualTFLite/org.qus.viewqualTFLite.ViewQualActivity}: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:\r\n    MEAN: Operation is not supported.\r\n    First 43 operations will run on the GPU, and the remaining 7 on the CPU.TfLiteGpuDelegate Prepare: Tensor ref has unsupported number of dimensions: 5Node number 50 (TfLiteGpuDelegate) failed to prepare.\r\n    \r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2955)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3030)\r\n        at android.app.ActivityThread.-wrap11(Unknown Source:0)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1696)\r\n        at android.os.Handler.dispatchMessage(Handler.java:105)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6938)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:327)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1374)\r\n     Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:\r\n    MEAN: Operation is not supported.\r\n    First 43 operations will run on the GPU, and the remaining 7 on the CPU.TfLiteGpuDelegate Prepare: Tensor ref has unsupported number of dimensions: 5Node number 50 (TfLiteGpuDelegate) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:83)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:60)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)\r\n        at org.qus.viewqualTFLite.TensorFlowQUSRunner.create(TensorFlowQUSRunner.java:154)\r\n        at org.qus.viewqualTFLite.ViewQualActivity.onCreate(ViewQualActivity.java:200)\r\n        at android.app.Activity.performCreate(Activity.java:7183)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1220)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2908)\r\n            ... 9 more\r\n```", "comments": ["You can produce a listing of all of the ops and tensors in your .tflite file as an HTML file using the visualize tool:\r\n\r\nbazel run //tensorflow/lite/tools:visualize model.tflite visualized_model.html\r\n\r\nThat should help you see where the MEAN op is.", "@Mr-Grieves \r\n\r\nIf MEAN is used as part of batch norm, it needs to be removed (at inference, batch size will be most probably 1).  If MEAN is used for something else, you probably can't remove it, and we have internal requests to accommodate those \"accumulators\", but it may take some time for us to write it in shader code.  I will update the thread if we release it."]}, {"number": 28521, "title": "LSTM vs Conv2D, tf-nightly-gpu (CUDNN_STATUS_INTERNAL_ERROR)", "body": "Hello,\r\n\r\ninfo:\r\ntf-nightly-gpu (1.14.1.dev20190508)\r\ncuda 10.0\r\ncuDNN v7.4.1\r\nRTX 2080\r\nubuntu 16.04\r\n\r\nI'm trying to execute some code. (ML)\r\n\r\ni'm useing LSTM and i get no error !\r\n\r\ni'm useing Conv2D and i get this error here:\r\n`2019-05-08 14:45:09.120400: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-05-08 14:45:09.724859: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-05-08 14:45:10.964612: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-05-08 14:45:10.967118: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 55, in <module>\r\n    model.fit(x, y, epochs=1, batch_size=n_batch)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\", line 802, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 357, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\", line 3180, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1456, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d/Conv2D}}]]\r\n\t [[loss/mul/_93]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d/Conv2D}}]]\r\n0 successful operations.\r\n0 derived errors ignored.`\r\n\r\ni think there are some problem with Conv2D on GPU.\r\n\r\nCan some one help me ?", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n\r\n", "i got error in this code here: (Conv2D)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pickle\r\nimport cv2\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\r\nfrom tensorflow import keras\r\n\r\nIMG_SIZE = 50\r\n\r\ndef prepare(file):\r\n    img_array = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\r\n    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\r\n\r\n    predictdata = tf.reshape(new_array, (1, 50, 50))\r\n    predictdata = np.expand_dims(predictdata, -1)\r\n    return predictdata\r\n\r\n\r\npickle_ind = open(\"x.pickle\", \"rb\")\r\nx = pickle.load(pickle_ind)\r\nx = np.array(x, dtype=float)\r\nx = np.expand_dims(x, -1)\r\n# x = x/255.0\r\n\r\npickle_ind = open(\"y.pickle\", \"rb\")\r\ny = pickle.load(pickle_ind)\r\n\r\nn_batch = len(x)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(50, 50, 1)))\r\nmodel.add(MaxPooling2D((2, 2)))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D((2, 2)))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(1, activation='softmax'))\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x, y, epochs=1, batch_size=n_batch)\r\nprediction = model.predict([prepare('demo1.jpg')], batch_size=n_batch, steps=1, verbose=1)\r\n\r\nprint(prediction)\r\n```", "Hello,\r\n\r\ni added this here\r\n\r\n```\r\nimport tensorflow as tf\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\r\nconfig = tf.ConfigProto(gpu_options=gpu_options)\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config)\r\n```\r\ngot new error ! (OOM when allocating tensor with shape[24946,32,48,48] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc)", "Please have a look on [this](https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-462242641) and let us know if that helps. Thanks!", "it still does not help.", "Can you try by using GPU memory growth only (without setting per_process_gpu_memory_fraction)?\r\n```python\r\nimport tensorflow as tf\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n```", "Hello,\r\n\r\nRTX 2080\r\nubuntu 16.04\r\ncuda 10.0\r\ncuDNN v7.4.1\r\n\r\nIt work's with LSTM but then i trying to compile tensorflow with GPU if failed with conv2d\r\n\r\nHere is my code:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pickle\r\nimport cv2\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\r\nfrom tensorflow import keras\r\n\r\n# config = tf.ConfigProto()\r\n# config.gpu_options.allow_growth = True\r\n# sess = tf.Session(config=config)\r\n\r\n# config = tf.ConfigProto()\r\n# config.gpu_options.allow_growth = True\r\n# sess = tf.Session(config=config)\r\n# keras.backend.set_session(sess)\r\n\r\n# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\r\n# config = tf.ConfigProto(gpu_options=gpu_options)\r\n# config.gpu_options.allow_growth = True\r\n# session = tf.Session(config=config)\r\n\r\nIMG_SIZE = 50\r\n\r\ndef prepare(file):\r\n    img_array = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\r\n    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\r\n\r\n    predictdata = tf.reshape(new_array, (1, 50, 50))\r\n    predictdata = np.expand_dims(predictdata, -1)\r\n    return predictdata\r\n\r\n\r\npickle_ind = open(\"x.pickle\", \"rb\")\r\nx = pickle.load(pickle_ind)\r\nx = np.array(x, dtype=float)\r\nx = np.expand_dims(x, -1)\r\n# x = x/255.0\r\n\r\npickle_ind = open(\"y.pickle\", \"rb\")\r\ny = pickle.load(pickle_ind)\r\n\r\nn_batch = len(x)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(50, 50, 1)))\r\nmodel.add(MaxPooling2D((2, 2)))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D((2, 2)))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(1, activation='softmax'))\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x, y, epochs=1, batch_size=n_batch)\r\nprediction = model.predict([prepare('demo1.jpg')], batch_size=n_batch, steps=1, verbose=1)\r\n\r\nprint(prediction)\r\n```\r\n\r\nif i use case1:\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n```\r\noutput:\r\n ```\r\nFile \"test.py\", line 60, in <module>\r\n    model.fit(x, y, epochs=1, batch_size=n_batch)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\", line 802, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 357, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\", line 3234, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1456, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d/Conv2D}}]]\r\n\t [[loss/mul/_93]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d/Conv2D}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n\r\nif i use case2:\r\n```\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\r\nconfig = tf.ConfigProto(gpu_options=gpu_options)\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config)\r\n```\r\n\r\noutput:\r\n```\r\n2019-05-17 08:27:18.373408: W tensorflow/core/framework/op_kernel.cc:1491] OP_REQUIRES failed at conv_ops.cc:484 : Resource exhausted: OOM when allocating tensor with shape[24946,32,48,48] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 60, in <module>\r\n    model.fit(x, y, epochs=1, batch_size=n_batch)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\", line 802, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 357, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\", line 3234, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1456, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\r\n  (0) Resource exhausted: OOM when allocating tensor with shape[24946,32,48,48] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node conv2d/Conv2D}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[loss/mul/_93]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n  (1) Resource exhausted: OOM when allocating tensor with shape[24946,32,48,48] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node conv2d/Conv2D}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n0 successful operations.\r\n0 derived errors ignored\r\n```\r\n\r\nif i use nothing, case3\r\n```\r\n2019-05-17 08:44:54.821562: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-05-17 08:44:55.518255: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-05-17 08:44:56.761691: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-05-17 08:44:56.765670: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 60, in <module>\r\n    model.fit(x, y, epochs=1, batch_size=n_batch)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\", line 802, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 357, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\", line 3234, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1456, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d/Conv2D}}]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d/Conv2D}}]]\r\n\t [[loss/mul/_93]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```", "Hi,\r\n\r\nCan you please provide a self contained reproducer?  I cannot run https://github.com/tensorflow/tensorflow/issues/28521#issuecomment-491200216 because I don't have the \"x.pickle\" and \"y.pickle\" files."]}, {"number": 28498, "title": "cyclic graph is not supported?", "body": "Please correct me if I am wrong.\r\nI am building cyclic graph. However cyclic graph is not supported in tensorflow. \r\n\r\n**System information**\r\n- TensorFlow version: 2.0.0-alpha0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Who will benefit with this feature?**\r\ntf is more expressive with cyclic graph. tf.while can be build with cyclic graph with conditional node.  Recursion, such as https://stackoverflow.com/questions/43129225/python-recursive-tensorflow-fibonacci-number-calculation, can be written easily.\r\n", "comments": ["Can you may be more specific about the cyclic graph? What would you expect to run?", "For example, [Sentiment Analisys with Recursive Neural Network](https://chainer-colab-notebook.readthedocs.io/en/latest/notebook/official_example/sentiment.html)\r\n\r\nRecursive Neural Network can be implemented neatly with cyclic graph. Maybe Graph Neural Network too. These models are very hard to implement in current version TensorFlow. \r\n\r\nIs TensorFlow assuming that the Computation Graph is acyclic, especially in the XLA module?\r\n\r\n", "@framsc We are checking to see if you still need help on this issue. Could you please check with latest stable version of TF which is `TF2.7` ? Thanks", "> For example, [Sentiment Analisys with Recursive Neural Network](https://chainer-colab-notebook.readthedocs.io/en/latest/notebook/official_example/sentiment.html)\r\n> \r\n> Recursive Neural Network can be implemented neatly with cyclic graph. Maybe Graph Neural Network too. These models are very hard to implement in current version TensorFlow.\r\n> \r\n> Is TensorFlow assuming that the Computation Graph is acyclic, especially in the XLA module?\r\n\r\nYes, simply speaking, they are DAGs (as also pointed [out here](https://stackoverflow.com/questions/37571518/tensorflow-graphs-are-tensorflow-graphs-dag-what-happens-in-assignadd-operati)). There is still the possibility to have while loops and similar, that's probably the best bet.", "It is possible to build cyclic TF graphs, but only using low-level (and undocumented) APIs, not via the Python API which is limited to tracing (hence cannot build cyclic graphs).\r\n\r\nThe nodes in a cycle cannot all be from the same frame (to understand what frames are, have a look at http://download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf). In other words, at the moment there are only a few ops which can be used to close a cycle. Though one could technically add more (e.g. to support recursion).\r\n\r\nAs long as the condition above is met, it is technically possible to build cyclic graphs, though that's not well supported at the moment. If you're in Python, one potential avenue is https://www.tensorflow.org/api_docs/python/tf/graph_util/import_graph_def, though we're working on replacing that API with something better supported in TF2."]}, {"number": 28438, "title": "TensorFlow Lite conversion of frozen graph error", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (or github SHA if from source): 2f9cc84ba3f5d59753d843f167adee2e2534c143\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\nWhile converting [faster_rcnn_nas](http://download.tensorflow.org/models/object_detection/faster_rcnn_nas_coco_2018_01_28.tar.gz) from tensoflow trained model to tensorflow lite found some error.\r\n```\r\nUnsupported Control Flow Ops:\r\n1. Enter\r\n2. Exit\r\n3. Merge\r\n4. Switch\r\n5. LoopCond\r\n\r\nUnsupported Lite Ops:\r\n1. NonMaxSuppressionV2\r\n2. TensorArrayGatherV3\r\n3. TensorArrayReadV3\r\n4. TensorArrayScatterV3\r\n5. TensorArraySizeV3\r\n6. TensorArrayV3\r\n7. TensorArrayWriteV3\r\n\r\nUnsupported Quantize Ops:\r\n1. Div\r\n2. Exp\r\n3. Cast\r\n4. Fill\r\n5. Range\r\n6. Size\r\n7. Tile\r\n8. ZerosLike\r\n9. Unpack \r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n[faster_rcnn_nas_lite_error.txt](https://github.com/tensorflow/tensorflow/files/3148187/faster_rcnn_nas_lite_error.txt)\r\n", "comments": ["@jdduke  This seems to be a problem faced by alot of people in the past, is there any plans of implementing the data flow ops in the tflite ? Also would like to know your take on implementing it along with flow_in's which is tricky thing to do.\r\nIf you are ok would like to raise on RFC for this and take it forward for implementation.\r\n\r\nRegards\r\nAmit", "@miaout17 can you de-dupe against our tracking control flow issue? We'll be improving the documentation around control flow-related conversion in the near future (an area we're actively working to improve over the next several quarters).", "Would it be helpful at all for us to add an error message in TOCO where it points directly to a FAQ or Github issue (for control flow / LSTMs) when there is an Unsupported Control Flow Op?", "Let's use #28485 for tracking TensorFlow Lite control flow support. A RFC already existed. \r\n@alanchiao sounds a good idea. ", "Unpack PR is already raised & approved: #27881", "Range op quantization support found under #27103", "Quantization support for Exp is raised as PR now.\r\n\r\nRegards\r\nAmit", "The PR for Quantization support for Exp was closed. Any update on when/if this will be include in future?\r\n\r\nThanks,\r\nAlex"]}, {"number": 28421, "title": "Why we cannot define two networks in the same graph when they are trained independently", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.4.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWhy we cannot define two networks in the same graph when they are trained independently.\r\n\r\nIt can be done only using two different grpahs.\r\n\r\n**Will this change the current api? How?**\r\nNo, just allow to define two networks in the same graph that have to be trained independently.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nDevelopers and production code. Because it's time and memory consuming to create two graphs.\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 28385, "title": "[C API] cond op", "body": "**System information**\r\n- TensorFlow version (you are using): master\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n**Describe the feature and the current behavior/state.** \r\nFeature: a cond operator for the C API, similar to [tf.cond](https://www.tensorflow.org/api_docs/python/tf/cond).\r\n\r\nCurrent state: The C API currently includes the control flow primitives needed to implement cond, but it does not include the cond op itself. \r\n\r\n**Will this change the current api? How?**\r\nThe C API would include the cond op\r\n\r\n**Who will benefit with this feature?**\r\nAll language bindings built on top of the C API could use its cond op, rather than having to individually implement their own from control flow primitives. \r\n", "comments": ["Two questions:\r\n\r\n-  I'd eventually like to use the cond op through the Java bindings. Since while loops are implemented at the C API level (rather than in the higher level language bindings using primitives), I assume we'd want to implement cond at the C API level too. Is this accurate? \r\n\r\n- given that control flow in TF is moving towards a functional representation, is it valuable to contribute the non-functional cond op to the C API? To me, it seems simplest to start by implementing the non-functional cond, and later move towards functional cond. Do you agree?\r\n\r\nFor reference, C API support for functional cond is discussed as future work in the cond_v2 design doc [here](https://github.com/tensorflow/community/blob/master/rfcs/20180507-cond-v2.md): \"C API support. Ideally other language bindings support conditional execution as well. The C API already includes the primitives for other bindings to implement something similar to tf.cond that produces an If op, but the C API TF_AddGradients method would need to support If ops in order for other bindings to (easily) allow autodiff of conditionals.\" \r\n", "cc: @saxenasaurabh ", "Thanks @melissagrueter, comments inlined. \r\n\r\n> I'd eventually like to use the cond op through the Java bindings. Since while loops are implemented at the C API level (rather than in the higher level language bindings using primitives), I assume we'd want to implement cond at the C API level too. Is this accurate?\r\n\r\nYes, implementing cond in the C API would make sense.\r\n\r\n> given that control flow in TF is moving towards a functional representation, is it valuable to contribute the non-functional cond op to the C API? To me, it seems simplest to start by implementing the non-functional cond, and later move towards functional cond. Do you agree?\r\n\r\nIf `TF_AddGradients` supports taking gradients of low level control flow ops `Switch`, `Merge` etc. (I will need to look into this but maybe @skye already knows), then yeah we could go this route just to avoid dealing with functional op gradients for now. If not, we can look into the functional cond. I think the forward pass for functional cond would be much easier to implement than the non-functional one, and less prone to bugs, which tempts to try this in the first place but I am fine with either since it will still be strictly better than having no cond in the C API :)", "I'm pretty sure TF_AddGradients does not handle Switch and Merge. It's not as bad as while_loop, but the legacy cond gradient implementation is pretty complicated (e.g. see CondContext). It would be much better long-term to implement functional cond instead, although might take some short-term extra work because the C API only has very low-level support for functions. But I think it shouldn't be too crazy to do something similar to the TF_NewWhile API, and turn the branch TF_Graphs into functions instead of adding switch/merge nodes."]}, {"number": 28276, "title": "Graph transform tool  Node Quantization gives error ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Via Docker tensorflow/serving:1.13.0-gpu \r\n- TensorFlow version (use command below): 1.13\r\n- CUDA/cuDNN version: From Docker  tensorflow/serving:1.13.0-gpu\r\n- GPU model and memory: NVIDIA V100 32 GB\r\n\r\n\r\n**Describe the current behaviour**\r\nAfter Quantizing Weights and Nodes using the Graph transform tool on the SSD model from TF model zoo I get the following error\r\n\r\n**Code snippet**\r\n\r\n```\r\ntransforms = ['add_default_attributes', \\\r\n  'strip_unused_nodes', \\\r\n  'remove_nodes(op=Identity, op=CheckNumerics)',\\\r\n  'fold_constants(ignore_errors=true)',\r\n  'fold_batch_norms',\r\n  'fold_old_batch_norms',\r\n  'quantize_weights',\r\n  'quantize_nodes',\r\n  'strip_unused_nodes',\r\n  'sort_by_execution_order']\r\n\r\noptimize_graph('/coding/ssd_inception_v2_coco_2018_01_28', 'frozen_inference_graph.pb' ,\r\n               transforms, output_node_names,outname='optimized_model_weight_quant.pb')\r\n```\r\n**Error Snippet**\r\n\r\n`details = \"input_max_range must be larger than input_min_range.`\r\n\r\n**Describe the expected behaviour**\r\nShould work\r\n\r\n**Code to reproduce the issue**\r\n\r\nAll details are logged here \r\nhttps://medium.com/@alexcpn/optimizing-any-tensorflow-model-using-tensorflow-transform-tools-and-using-tensorrt-1cc190cafe1f\r\n+ colab here\r\nhttps://colab.research.google.com/drive/1wQpWoc40kf__WSjfTqDaReMx6fFjUn48\r\nClient \r\n\r\n**Other info / logs**\r\nDocker container used for the optimization is : tensorflow/tensorflow:1.13.0rc1-gpu-jupyter\r\nDocker used for serving : tensorflow/serving:1.13.0-gpu \r\nModel used  *ssd_resnet_50_fpn_coco* form TF model zoo -https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\r\n\r\n```\r\n(Got an error, <_Rendezvous of RPC that terminated with:\r\n\tstatus = StatusCode.INVALID_ARGUMENT\r\n\tdetails = \"input_max_range must be larger than input_min_range.\r\n\t [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_87/Area/mul_eightbit/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_87/Area/sub_1/quantize}}]]\r\n\t [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/zeros_like_83}}]]\"\r\n\tdebug_error_string = \"{\"created\":\"@1555723203.356344655\",\"description\":\"Error received from peer\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1036,\"grpc_message\":\"input_max_range must be larger than input_min_range.\\n\\t [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_87/Area/mul_eightbit/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_87/Area/sub_1/quantize}}]]\\n\\t [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/zeros_like_83}}]]\",\"grpc_status\":3}\"\r\n>)\r\n\r\n```\r\n\r\nAlso I tried before running a model that was converted from Keras to tensorflow - Retinanet.(CNN) Again QunatizeNodes did not work with the following error. Colab for that here \r\nhttps://colab.research.google.com/drive/1u79vDN4MZuq6gYIOkPmWsbghjunbDq6m\r\n\r\n```\r\n_Rendezvous: <_Rendezvous of RPC that terminated with:\r\n\tstatus = StatusCode.UNIMPLEMENTED\r\n\tdetails = \"Broadcast between [1,9,4] and [221,1,4] is not supported yet.\r\n\t [[{{node anchors_3/add_2/eightbit}}]]\r\n\t [[{{node filtered_detections/map/while/non_max_suppression_17/NonMaxSuppressionV3}}]]\"\r\n\tdebug_error_string = \"{\"created\":\"@1552348256.140400062\",\"description\":\"Error received from peer\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1017,\"grpc_message\":\"Broadcast between [1,9,4] and [221,1,4] is not supported yet.\\n\\t [[{{node anchors_3/add_2/eightbit}}]]\\n\\t [[{{node filtered_detections/map/while/non_max_suppression_17/NonMaxSuppressionV3}}]]\",\"grpc_status\":12}\"\r\n```", "comments": ["Any update regarding this. I can help in someway conducting some tests", "Any update on this? "]}, {"number": 28264, "title": "CPU support for dilation rates larger than 1", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrent behavior for a model we are training is that CPU training yields errors:\r\n`tensorflow/core/common_runtime/executor.cc:624] Executor failed to create kernel. Invalid argument: Current libxsmm and customized CPU implementations do not yet support dilation rates larger than 1.\r\n         [[{{node Train_1/Optimizer/TrainOperation/gradients/Conv2D_72_grad/Conv2DBackpropFilter}}]]\r\n`\r\n\r\nGPU training is successful. We would like to have parity between CPU and GPU train as not all developers have a local GPU host. This appears to be a documented issue but I could not find any mention of when it may be fixed or what may be blocking this issue. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_grad_filter_ops.cc#L223\r\n\r\nThis is a feature request to complete the TODO mentioned in the above link. \r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\nAnyone that is trying to train on CPU for dilated convolutions\r\n**Any Other info.**\r\n", "comments": ["Hi, are there any updates on this?\r\nIn my use case, I don't necessarily want to train the model fully, but just test things out, which usually means I am using CPU.", "I am running into the same issue with both `Conv1D` and `Conv1DTranspose`. Thank you very much.\r\n\r\nEdit: I should add that I tried working around this using `Conv2DTranspose` with an appropriate stride, but I ran into the same error. Thanks again for taking a look at this.", "@penpornk I'm told that you may work on CPU implementation? Can you take a look?"]}, {"number": 28195, "title": "[C API] while loop: unable to access operations defined outside of the loop from within the loop", "body": "*System information*\r\n\r\n* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.4\r\n* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n* TensorFlow installed from (source or binary): source\r\n* TensorFlow version (use command below): master\r\n* Python version: Python 3.7.3\r\n* Bazel version (if compiling from source): 0.24.1\r\n* GCC/Compiler version (if compiling from source): \r\n* CUDA/cuDNN version: N/A\r\n* GPU model and memory: N/A\r\n\r\n*Describe the current behavior*\r\n\r\nI am unable to access operations defined outside of the while loop from within the loop. \r\n\r\nThe C API while loop creates separate conditional and body graphs, so an error is thrown when we try to use operations defined in the outer graph within the body graph. See earlier discussion with @skye [here](https://github.com/tensorflow/tensorflow/issues/26371#issuecomment-482358762).\r\n\r\n*Describe the expected behavior*\r\n\r\nI would expect the behavior of the C API while loop to match the Python API while loop, where accessing operations defined outside the while loop works. \r\n\r\nI've included a minimal working example in Python that demonstrates the expected behavior below. In this example, we are able to use \u201cincrement\u201d in the loop body even though it's defined outside the loop.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nincrement = tf.constant(1, name='one')\r\n\r\ndef loop_cond(loop_var):\r\n    return tf.math.less(loop_var, 10)\r\n\r\ndef loop_body(loop_var):\r\n    return loop_var + increment\r\n\r\nloop_input = tf.Variable(0, name='loop_input')\r\n\r\nloop_output = tf.while_loop(loop_cond, loop_body, [loop_input])\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    print(sess.run(loop_input)) // should be 0\r\n    print(sess.run(loop_output)) // should be 10 \r\n```\r\n\r\n\r\n*Code to reproduce the issue*\r\n\r\nI replicated the example above as a unit test in while_loop_test.cc. Once again, we try to use \u201cincrement\u201d within the loop body, but here we get an error since it's not part of the body graph. The error message is \u201cNode 'add': Unknown input node 'scalar'\u201d. \r\n\r\n```\r\nTEST_F(CApiWhileLoopTest, AccessOuterOp) {\r\n  Init(1);\r\n  // increment = 1\r\n  // while (i < 10) {\r\n  //   i = i + increment\r\n  // }\r\n\r\n  // Create increment *in the outer graph*\r\n  TF_Operation* increment = ScalarConst(1, graph_, s_);\r\n\r\n  // Create cond graph: i < 10 \r\n  TF_Operation* ten = ScalarConst(10, params_->cond_graph, s_);\r\n  TF_Operation* less_than = LessThan(params_->cond_inputs[0], {ten, 0}, params_->cond_graph, s_);\r\n  DCHECK_EQ(TF_OK, TF_GetCode(s_)) << TF_Message(s_);\r\n  params_->cond_output = {less_than, 0};\r\n\r\n  // Create body graph: i = i + increment\r\n  TF_Operation* add = Add(params_->body_inputs[0], {increment, 0}, params_->body_graph, s_);\r\n  ASSERT_EQ(TF_OK, TF_GetCode(s_)) << TF_Message(s_);\r\n  params_->body_outputs[0] = {add, 0};\r\n\r\n  ExpectOK();\r\n}\r\n```\r\nThis test can be copy-pasted into while_loop_test.cc and run with the following command:\r\n bazel run //tensorflow/c:while_loop_test. \r\n\r\n\r\n*Other info / logs*\r\nThe above examples are intended to be as minimal as possible, so they're not practical. However, accessing outside operations would be important when updating external variables, for example when training within a loop (we would need to update external weights and biases).\r\n\r\nWe discovered this issue while using TF Java, after exposing the C API while loop to Java in [this commit](https://github.com/tensorflow/tensorflow/commit/6799f6e990f856452c467f31448c211fce94bbc1).\r\n\r\n\r\n", "comments": ["cc @saxenasaurabh ", "In my earlier discussion with Skye about this issue (linked above), we discussed capturing as a solution to this problem:\r\n\r\n\u201cBased on some offline discussion, I think we're leaning towards adding the ability to \"capture\" Tensors in a TF_Graph. This is very similar to allowing inputs from other Graphs. This would be a step towards representing functions as graphs in the C API, which is probably a prerequisite for while_v2, so it'd be nice to move towards that final goal.\u201d - Skye\r\n\r\n@saxenasaurabh, is that still what the TF team has in mind? If so, would that necessitate implementing a version of Python's FuncGraphs for the C API, or is there an alternative approach you'd suggest? Of course, I'd like to consider the simplest solutions first. \r\n\r\nThanks!", "Hi Melissa, sorry for the delay. I should have looked into this earlier, but I just noticed that the \"Unknown input node\" message comes from importing a GraphDef: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/graph/graph_constructor.cc#L561\r\n\r\nI'm guessing this means that the body graph can somehow already reference nodes from the outer graph (maybe because we usually check this in Python?), but then it complains when we try to import the body graph into the outer graph:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L2430\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L2282\r\n\r\nA quick fix might be to just add _all_ the nodes in the outer graph to the input_map of that import, not just the loop inputs (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L2266). Melissa, do you wanna give this a try and see if it works, or if we get more errors? There might be performance issues down the line creating such a large input_map, but we can start with this just to see if this is the only problem. Please let me know if you need more help or context.", "Hi @skye,\r\n\r\nThank you for your super helpful (as always) insights! \r\n\r\nJust a couple things I wanted to clarify:\r\n\r\nI thought that the input_map maps nodes in the src graph that we'd like to replace to their replacements in the dst graph. In the existing code, src_inputs are input_map's keys and dst_inputs are the corresponding values, because we want to replace the loop input nodes created in the cond and body graph with the loop input nodes created in the outer graph. See comment [here](https://github.com/tensorflow/tensorflow/blob/e9bd33a41d181593748d12af950f35381e9e530e/tensorflow/c/c_api.cc#L2244): \u201cAny node in src_graph with input src_inputs[i] will have that input replaced with dst_inputs[i].\u201d  Since in this case we don't actually want to replace any nodes, were you suggesting adding every node in the outer graph to input_map as both a key and its own value? (In order to avoid triggering [this condition](https://github.com/tensorflow/tensorflow/blob/9d558ddba177859ebd7e4018a405f68dcc5e1836/tensorflow/core/graph/graph_constructor.cc#L555) for any input from the outer graph.) Let me know if I've misunderstood any of the above.\r\n\r\nAlso, for additional context if at all useful/interesting, this is actually sort of similar to the hack I tried earlier when trying to enable similar functionality using control dependencies  \u2014 except I added the control dependencies I wanted to opts.control_dependencies [here](https://github.com/tensorflow/tensorflow/blob/e9bd33a41d181593748d12af950f35381e9e530e/tensorflow/c/c_api.cc#L2272) rather than adding potential inputs to opts.input_map a couple lines above. That's what I was referring to in [this comment](https://github.com/tensorflow/tensorflow/issues/26371#issuecomment-477820714) in our previous thread (\"I found a relatively simple way to add control inputs from the outer graph to the loop body (by only changing while loop code, not allowing control inputs between separate graphs in general)\").\r\n\r\nThanks again for all your help Skye, I always appreciate it! \r\n", "Ah you're right, my suggestion didn't make much sense. What I'm trying to achieve is that the [ImportGraphDef call](https://github.com/tensorflow/tensorflow/blob/e9bd33a41d181593748d12af950f35381e9e530e/tensorflow/c/c_api.cc#L2282) allows `gdef` to reference nodes from the outer graph, instead of raising the \"Unknown input node\" error. I _think_ you can do this by populating `opts.input_map` with every outer node's name+index as both the key and the value. The keys of input_map are supposed to appear in gdef, not in the outer graph, but I don't think we actually check this (it would happen [here](https://github.com/tensorflow/tensorflow/blob/9d558ddba177859ebd7e4018a405f68dcc5e1836/tensorflow/core/graph/graph_constructor.cc#L435), although I could be missing something).\r\n\r\nDoes this make sense? It's kind of a hack, but I just wanna see if this works and then we can figure out how to make it less hacky, or if there are more fundamental problems we need to solve.", "Got it, thanks for clarifying! I will give that a try and report back. ", "Hi @skye! \r\n\r\nI gave it a try and my AccessOuterOp unit test (see above) is now passing, but a few of the other tests are now failing:\r\n\r\n[ FAILED ] CApiWhileLoopTest.NestedLoop\r\n[ FAILED ] CApiWhileLoopTest.InvalidCondOutputNode\r\n[ FAILED ] CApiWhileLoopTest.InvalidBodyOutputNode\r\n[ FAILED ] CApiWhileLoopTest.WrongGraph\r\n\r\nIt seems to me that the InvalidCondOutputNode, InvalidBodyOutputNode, and WrongGraph tests would all be expected to fail now, since they're all looking for an error if you set the cond or body outputs to a node in the outer graph (and this hack is intended to prevent such an error). \r\n\r\nI'm not sure yet why this causes the NestedLoop test to fail. Here's the error NestedLoop produces:\r\n{{node test_loop/body/inner_loop/Merge}} has inputs from different frames. The input {{node test_loop/body/inner_loop/NextIteration}} is in frame 'test_loop'. The input {{node test_loop/body/inner_loop/Enter}} is in frame 'inner_loop'. \r\n\r\n", "Oof, I bet NestedLoop is failing because there's a problem with my input_map hack: if both the outer and inner graph have a node with the same name, there's no way to distinguish if a given input refers to the outer or inner tensor. I'm guessing NestedLoop is running into this because the inner and outer loops will end up using the same node names. This wasn't a problem before because keys in the input_map always referred to the inner graph being imported.\r\n\r\nThis is unfortunate, because it means we'll probably need something more heavy-duty, like how Python FuncGraphs \"capture\" external tensors explicitly instead of just referring to them. We can maybe work around it for now by creating a unique name scope that we use when creating the while loop graphs... the C API doesn't have name scopes though :( This is even hackier, but can you create a name scope in Java that you use when creating the while loop graphs? Otherwise maybe we should think about how to properly refer to or \"capture\" external tensors in the C API.", "What if we just add a list of outer graph nodes to allow as a parameter to copygraph? There should be no ambiguity if they always refer to nodes in the outer graph, and we could presumably allow these as inputs to the subgraph the same way we allow the nodes that are values in the input_map. \r\n", "That might work for the particular use cases you have in mind, but there's still the fundamental problem that if you have an input \"foo:0\", an internal node \"foo\", and an external node \"foo\" (in the parameter whitelist), there's no way to determine which node \"foo:0\" is referring to. Having the whitelist makes you less likely to find yourself in this situation, but it's still possible.", "Hmm, let me back up a bit -- could you clarify why you believe this a naming issue? I don't quite see it yet.\r\n\r\nThe nodes that cause NestedLoop to fail seem to already be named uniquely depending on which graph they're in \u2014 their names all include inner_loop:\r\n\r\n{{node test_loop/body/**inner_loop**/Merge}} has inputs from different frames. The input {{node test_loop/body/**inner_loop**/NextIteration}} is in frame 'test_loop'. The input {{node test_loop/body/**inner_loop**/Enter}} is in frame 'inner_loop'.\r\n\r\nThey also all appear to be in the same graph (in the inner_loop). If the problem was mixing up nodes in the inner and outer graph due to their names being the same, wouldn't we expect Merge and its inputs not to all be in the same graph?", "Update I am getting NestedLoop to pass now, just need to dig a bit more to make sure the fix can work in general... :) ", "Hi @skye ,\r\n\r\nAs you could have predicted, my latest quick fix didn't work out \u2014 I understand the naming issue now. :) \r\n\r\nIt seems to me that the best way forward is to go for the long-term fix and move toward functional control flow. As we've discussed [in another thread](https://github.com/tensorflow/tensorflow/issues/28385), I'm also interested in getting a cond op working. To that end, I'd like to avoid putting too much time into short-term while-loop-specific hacks, since a non-functional cond would presumably have the same issue as the current non-functional while. From what I'm seeing here and our discussions so far, my understanding is that the best way to make progress on both cond and while is to get something like FuncGraphs working in the C API. Do you agree?\r\n\r\nIf so, do you have an idea of what the steps are to getting functional control flow working in the C API?  We've talked about FuncGraphs as a good first step, but I'm not yet sure how big a chunk of the work toward achieving functional control flow FuncGraphs are, or if FuncGraphs depend on other things the C API doesn't support that I'd need to add first. \r\n\r\nIf FuncGraphs are the best first step, I've been thinking it might make sense to proceed like this:\r\n\r\n- Implement FuncGraphs for the C API\r\n- Use FuncGraphs as the cond and body subgraphs of the existing non-functional while loop, just to get while up and running quickly and test the FuncGraphs (Is this possible? This is similar to what I think you were suggesting as a first step for cond in the [other thred](https://github.com/tensorflow/tensorflow/issues/28385))\r\n- Implement functional cond (From looking at the functional control flow RFCs, it looks like functional cond is significantly easier to tackle than functional while, so it looks like a better place to start) \r\n- Implement functional while \r\n\r\nLet me know what you think. Any recommendations you have about how to go about tackling this would of course be incredibly helpful! \r\n", "One more question \u2014 I see that for the Python's functional control flow, there's a lowering pass to the non-functional cond and while. Is it critical to be able to lower functional control flow ops into their non-functional equivalents? In other words, will we still end up needing to fix the non-functional control flow even if we do implement functional control flow? ", "Hi Melissa, once again sorry for leaving you hanging.\r\n\r\nTo answer your latest question, the lowering happens in the TF runtime, so you'll get it \"for free\" once functional cond + while are implemented. To actually answer your question, it's technically not necessary, but the non-lowered path isn't really tested so in practice you do wanna lower.\r\n\r\nI'm wary to send you down the FuncGraph + functional control flow path because I'm worried it's a lot of work, and also will require some attention from TF API owners, who are perpetually very busy. But, it does seem like hacking this together isn't gonna work, so if you understand there's a risk this will stall, let's do it.\r\n\r\nOne thing to note up front -- it will be a lot more work get this all working with gradients. Are you gonna need to compute gradients through control flow?\r\n\r\nYour overall plan sounds good. The key here will implementing FuncGraphs in the most minimal way possible to get functional control flow working (note that https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/func_graph.py is almost 1000 lines of Python... we don't want that). I think this means tracking inputs/outputs, and captures (which are represented as an input + an external Tensor that we always pass to that input). I think we should initially develop this as an experimental API in case we have to change it.\r\n\r\nBefore we dig into this too much, are you sure you want to continue? :) (and will you need gradients?) I'll see if I can get some internal feedback too.", "Hi Skye,\r\n\r\nMy goal is to get while and cond ops working, with gradients, in whatever is the most efficient (and also stable) way. \r\n\r\nWould the following more reasonable than my earlier plan to fully implement functional control flow? \r\n\r\n* Implement some minimal FuncGraph for the C API\r\n* Use FuncGraphs as the cond/body subgraphs of the existing non-functional while loop\r\n* Write a non-functional cond that just uses FuncGraphs as the branches\r\n\r\nA couple of questions to help me understand whether the above would actually be less work than my previous plan: \r\n\r\nRE: gradients,\r\n\r\n* If we use FuncGraphs as the cond/body subgraphs of the existing non-functional while, will the existing while loop gradient implementation just work? If not, what would it take to get the gradients working (and would it be the same amount of work as it would be to get gradients working for a functional while loop)?\r\n    * In the case that the existing while loop gradient implementation just works, we still don't have the corresponding CondContext implementation for cond. Would it be simpler to implement CondContext in that case, or would it make more sense to just get the gradients working for the functional cond? \r\n\r\nAlso, is there existing C API support for functions that will be relevant/helpful when getting FuncGraphs working? I've looked briefly at [c_api_function.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api_function.cc) \u2014 will any of that support for functions be useful?\r\n\r\nThanks again for all the help, Skye! :) \r\n", "Hi @skye! Just following up :) ", "cc @martinwicke - this issue has gotten stuck and is blocking SIG JVM. ", "@saxenasaurabh could you take a look? I think sending people down the functional while path is now probably the right thing to do. ", "Sorry this got pushed to the back-burner since we have been focusing on fixing bugs for 2.0. \r\nI am planning to find some time over the next few weeks to design how FuncGraphs in the C API might look like. I think this will be important to pursue soon since we are starting to see issues in other areas e.g. C++ shape inference, colocations etc. because of missing nested graph information. Maybe I can send out a RFC so that the SIG JVM community can join in.", "Thanks @saxenasaurabh for following up! I'd love to hear what you come up with in terms of design -- an RFC sounds like a great idea! "]}, {"number": 28049, "title": "Added an argument to control the padding for flatten_atrous", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28049) for more info**.\n\n<!-- need_sender_cla -->", "I signed the CLA", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28049) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28049) for more info**.\n\n<!-- ok -->", "Can one of the admins verify this patch?", "@andrewharp  Can you please take a look on this PR ?", "I don't really think I know enough to review this change. Maybe someone from TFLite/mobile?", "@petewarden, Can you please review this PR ? Thanks!", "@petewarden, Can you please review this PR ? Thanks!", "@rmlarsen Can you please review this PR ? Thanks!", "@rmlarsen Can you please review this PR ? Thanks!", "@rmlarsen Can you please review this PR ? Thanks!", "@rmlarsen Can you please review this PR ? Thanks!", "@rmlarsen Can you please review this PR ? Thanks!"]}, {"number": 27899, "title": "Can't easily write custom C++ ops which update resource variables", "body": "**System information**\r\n- TensorFlow version (you are using): HEAD\r\n- Are you willing to contribute it (Yes/No): No (moving headers is best done inside Google)\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWe're trying to update optimizers like the ones at https://github.com/openai/blocksparse/blob/master/blocksparse/optimize.py to support parameters stored as resource variables. Looking at the TensorFlow source code for the built-in optimizer ops, it seems necessary to have relatively complicated locking and copy-on-write logic to handle the different states a resource variable could be in. TensorFlow uses helper functions in [`core/kernels/training_op_helpers.h`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_op_helpers.h) to do this, calling `MaybeLockVariableInputMutexesInOrder` and `GetInputTensorFromVariable`.\r\n\r\nUnfortunately, `training_op_helpers.h` is not in the public TensorFlow headers, so we can't just `#include` it. Worse, it depends on internal functions that `libtensorflow_framework.so` doesn't even export (notably, `tensorflow::functor::DenseUpdate` for the copy-on-write), so copying the headers also doesn't suffice.\r\n\r\nCan we move this file and the relevant dependencies into `framework` or a similarly public location?\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes.  Those helpers would become public.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nPeople writing new C++ optimizers.\r\n\r\n**Any Other info.**\r\n\r\nRelated StackOverflow question (cc @daniel-ziegler): https://stackoverflow.com/questions/55661246/how-to-write-custom-c-ops-which-update-resource-variables", "comments": ["@alextp, do we have any better solution than exposing this api surface to solve Geoffrey's needs?", "I don't have a plan to expose this in a better way, but I think this should be doable as a PR (no weird internal dependencies I can think of) so I'll mark as contributions welcome now.", "It looks as if this issue can be solved now by linking with `_pywrap_tensorflow_internal.so` in addition to `libtensorflow_framework.so`. `DenseUpdate` etc. are included there. While I didn't find this documented, the library does come with the package on PyPI."]}, {"number": 27846, "title": "quantize_graph incompatible shapes", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 9 cuDNN 7.1\r\n- GPU model and memory:GTX 1080ti\r\n\r\n\r\n**Source code:**\r\n\r\n```\r\n if FLAGS.base_architecture not in ['resnet_v2_50', 'resnet_v2_101']:\r\n\u00a0 \u00a0\u00a0 \u00a0\u00a0\u00a0raise ValueError(\"'base_architrecture' must be either 'resnet_v2_50' or 'resnet_v2_101'.\")\r\n\r\n\u00a0 \u00a0 if FLAGS.base_architecture == 'resnet_v2_50':\r\n\u00a0 \u00a0\u00a0 \u00a0\u00a0\u00a0base_model = resnet_v2.resnet_v2_50\r\n\u00a0 \u00a0 else:\r\n\u00a0 \u00a0\u00a0 \u00a0\u00a0\u00a0base_model = resnet_v2.resnet_v2_101\r\n\r\n\u00a0 \u00a0 with tf.contrib.slim.arg_scope(resnet_v2.resnet_arg_scope(batch_norm_decay=_BATCH_NORM_DECAY)):\r\n\u00a0 \u00a0\u00a0 \u00a0\u00a0\u00a0logits, end_points = base_model(inputs,\r\n\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0 num_classes=None,\r\n\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0 is_training=is_train,\r\n\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0 global_pool=False,\r\n\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0 output_stride=FLAGS.output_stride) # output_stride=16\r\n\u00a0 \u00a0\u00a0\u00a0inputs_size = tf.shape(inputs)[1:3]\r\n\u00a0 \u00a0 net = end_points[ FLAGS.base_architecture + '/block4']\u00a0\u00a0# [None, 16, 16, 2048]\r\n```\r\n\r\n\r\n**problem description:**\r\nI was trying to use tf.contrib.quantize.experimental_create_training_graph to conduct fake quantize training, then convert to tflite model. Firstly, i load the resnet_v2_101 pre-trained model (output_stride is set to 16), forward-pass is ok, then when i use  tf.contrib.quantize.experimental_create_training_graph to create fake quantize nodes, the error happened:\r\n\r\n**log:**\r\n  File \"D:/tensorflow_project/pocketflow-deeplabv3/nets/deeplabv3_at_pascal2012_run.py\", line 72, in <module>\r\n    tf.app.run()\r\n  File \"D:\\anaconda\\install_path\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"D:/tensorflow_project/pocketflow-deeplabv3/nets/deeplabv3_at_pascal2012_run.py\", line 53, in main\r\n    learner = create_learner(sm_writer, model_helper)\r\n  File \"D:\\tensorflow_project\\pocketflow-deeplabv3\\learners\\learner_utils.py\", line 60, in create_learner\r\n    learner = UniformQuantTFLearner(sm_writer, model_helper)\r\n  File \"D:\\tensorflow_project\\pocketflow-deeplabv3\\learners\\uniform_quantization_tf\\learner.py\", line 98, in __init__\r\n    self.__build_train()\r\n  File \"D:\\tensorflow_project\\pocketflow-deeplabv3\\learners\\uniform_quantization_tf\\learner.py\", line 199, in __build_train\r\n    scope=self.model_scope_quan)   # rewrite the graph\r\n  File \"D:\\anaconda\\install_path\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\quantize\\python\\quantize_graph.py\", line 197, in experimental_create_training_graph\r\n    scope=scope)\r\n  File \"D:\\anaconda\\install_path\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\quantize\\python\\quantize_graph.py\", line 70, in _create_graph\r\n    is_training=is_training)\r\n  File \"D:\\anaconda\\install_path\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\quantize\\python\\fold_batch_norms.py\", line 53, in FoldBatchNorms\r\n    graph, is_training, freeze_batch_norm_delay=freeze_batch_norm_delay)\r\n  File \"D:\\anaconda\\install_path\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\quantize\\python\\fold_batch_norms.py\", line 136, in _FoldFusedBatchNorms\r\n    match.output_tensor)\r\n  File \"D:\\anaconda\\install_path\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\quantize\\python\\common.py\", line 157, in RerouteTensor\r\n    c._update_input(i, t0)  # pylint: disable=protected-access\r\n  File \"D:\\anaconda\\install_path\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2061, in _update_input\r\n    self._tf_input(index))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot update edge, incompatible shapes: [?,8,8,512] and [?,16,16,512].\r\n\r\n\r\n", "comments": ["I am getting the same error on my Tensorflow 1.12 and AWS G3 linux instance. Trying to do quantization aware training on segmentation model. ", "This looks like SpaceToBatch and BatchToSpace in case of Atrous convolution isn't handled properly. \r\nThis is what I am getting: _\"tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot update edge, incompatible shapes: [128,9,9,128] and [8,33,33,128].\"_\r\n", "Did you solve this error? Any updates?", "any update?\r\n", "Same here, can not quantize any resnet variant, however nas_pnasnet and mobilenet can be quantized.\r\nBy the way this is the documentation followed: https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/quantize.md\r\n"]}, {"number": 27831, "title": "Add built-in helper functions for _bytes_feature, _float_feature and _int64_feature from \"Using TFRecords and tf.Example\" page", "body": "The TF documentation page \"Using TFRecords and tf.Example\" https://www.tensorflow.org/tutorials/load_data/tf_records lists these helper functions:\r\n\r\n```\r\n# The following functions can be used to convert a value to a type compatible\r\n# with tf.Example.\r\n\r\ndef _bytes_feature(value):\r\n  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\r\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n\r\ndef _float_feature(value):\r\n  \"\"\"Returns a float_list from a float / double.\"\"\"\r\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\n\r\ndef _int64_feature(value):\r\n  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\r\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n```\r\n\r\nSearching github code shows these have been cut and pasted 3453 times into other projects:\r\nhttps://github.com/search?q=_bytes_feature+_float_feature&type=Code\r\nand presumably many more times besides.\r\n\r\nCould TF include helper functions for these and other common tf.train.Features/Examples helpers?\r\n\r\n**System information**\r\n- TensorFlow version: 1.13.1\r\n- Are you willing to contribute it: Yes (at some point)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nExamples and Features are recommended as the canonical way to store TF datasets.  However understanding the protobufs is non-trivial: they are multiple layers deep and have a verbose API.\r\n\r\n**Will this change the current api? How?**\r\nThis will make the API simpler and more pythonic for building usual Features and Examples.\r\n\r\n**Who will benefit with this feature?**\r\nAll users building datasets.\r\n\r\n**Any Other info.**\r\n", "comments": ["agreed, this is boilerplate, tf could easily have this. tfrecords are cool but could be simplified dramatically and be better for it"]}, {"number": 27748, "title": "Object detection API config file/protobuf errors", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Google Deep Learning VM version m15, based on Debian GNU/Linux 9.8**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **I used one of the binaries pre-built for this particular type of VM, tensorflow_gpu-1.12.0-cp27-cp27mu-linux_x86_64.whl**\r\n- TensorFlow version: **1.12**\r\n- Python version: **2.7**\r\n- Installed using virtualenv? pip? conda?: **Used pre-installed binary, however I am using a virtualenv environment for the project**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: **1 x NVIDIA Tesla K80, 4 vCPUs, 15 GB memory, hosted on Compute Engine**\r\n\r\n**Describe the problem**\r\nIssue previously posted on Stack Overflow, as I kept running into exception `TypeError: For training mode, the  train_config must be a train_pb2.TrainConfig.` when attempting to run training using an adapted version of the faster_rcnn_resnet50_coco.config configuration. However, there are no obvious issues with config file itself.\r\n[https://stackoverflow.com/questions/55238397/tensorflow-object-detection-train-config-file-error/55287833?noredirect=1#comment97930542_55287833](url)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nThe value of isinstance(train_config, train_pb2.TrainConfig) equated to false, which doesn't make sense as the result of printing ( type(config) ) resulted in <class 'object_detection.protos.train_pb2.TrainConfig'>, which I believe is the correct file type.\r\n\r\nI manually rebuilt the protobuf compilers as per the documentation here: [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md#manual-protobuf-compiler-installation-and-usage](url) however I kept running into the same problem. As we thought there may still be an issue with the protoc library, I manually removed and reinstalled protobuf version 3.0, before attempting training again. This resulted in the following traceback:\r\n\r\n`Traceback (most recent call last):\r\n  File \"trainer/task.py\", line 24, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 59, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/node_def_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/attr_value_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/tensor_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__ha\r\nndle__pb2\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/resource_handle_pb2.py\", line 22, in <modu\r\nle>\r\n    serialized_pb=_b('\\n/tensorflow/core/framework/resource_handle.proto\\x12\\ntensorflow\\\"r\\n\\x13ResourceHandleProt\r\no\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tcontainer\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x03 \\x01(\\t\\\r\nx12\\x11\\n\\thash_code\\x18\\x04 \\x01(\\x04\\x12\\x17\\n\\x0fmaybe_type_name\\x18\\x05 \\x01(\\tBn\\n\\x18org.tensorflow.framework\r\nB\\x0eResourceHandleP\\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\xf8\\x01\\x01\\x62\\x06proto3')\r\nTypeError: __new__() got an unexpected keyword argument 'serialized_options'\r\n`\r\nThis problem still persists despite changing the protobuf library to version 3.4.\r\n\r\n**Any other info / logs**\r\n\r\nI also had to use the command `pip install scikit-image --force-reinstall` in order to get the correct version of NumPy for this project, as I believe the latest release of NumPy caused some incompatibility issues with python 2.7 and was resulting in the error: AttributeError: 'numpy.ufunc' object has no attribute '__module__'\r\n", "comments": ["Hi Bobbi, sorry for the late response. Let's switch back to the protobuf lib that worked for you (3.6?). \r\n\r\nCould you please share your error message again? and which commit did you sync to?"]}, {"number": 27726, "title": "Support large embeddings with `MirroredStrategy` and `MultiWorkerMirroredStrategy`", "body": "**System information**\r\n- TensorFlow version (you are using): TF 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, `MirroredStrategy` and `MultiWorkerMirroredStrategy` assume a model replica including its embeddings could fit in one GPU or one machine. Allgather is used to aggregated gradients with respect to embeddings.\r\n\r\nWe would like to understand whether it is necessary and important to support\r\n1. placing embeddings on host with `MirroredStrategy` and why using `ParameterServerStrategy` is not ideal\r\n2. replicating embeddings on host with `MultiWorkerMirroredStrategy` and why in this case using `ParameterServerStrategy` is not ideal\r\n2. sharding embeddings in `MultiWorkerMirroredStrategy` and why `ParameterServerStrategy` is not ideal.\r\n\r\n**Will this change the current api? How?**\r\nYes.\r\n\r\n**Who will benefit with this feature?**\r\nUsers who use large embeddings in their models.\r\n\r\n**Any Other info.**\r\nN/A\r\n", "comments": ["I was starting out with MirroredStrategy with embeddings and realized it wasn't as easy as with other model building. Anyone have any tutorial or got something working? Seems this is an open topic almost 2 years old", "What's the progress of this issue?"]}, {"number": 27725, "title": "Support synchronous training with parameter servers using Distribution Strategies", "body": "**System information**\r\n- TensorFlow version (you are using): TF 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nRight now we have `MirroredStrategy` and `MultiWorkerMirroredStrategy` for synchronous training where variables and ops are all replicated and all-reduce is used for gradient aggregation. We also have `ParameterServerStrategy` where gradient updates from workers are purely asynchronous since `SyncReplicasOptimizer` is buggy and has been deprecated.\r\n \r\nWe would like to first collect use cases where synchronous training with `MirroredStrategy` and `MultiWorkerMirroredStrategy` is not ideal and synchronous training with parameter servers is necessary.\r\n\r\nIf this feature is necessary and important enough, we will then use this issue to track the progress of the development of this feature.\r\n\r\nWe have a separate feature request to support large embeddings with `MirroredStrategy` and `MultiWorkerMirroredStrategy`: https://github.com/tensorflow/tensorflow/issues/27726\r\n\r\n**Will this change the current api? How?**\r\nYes.\r\n\r\n**Who will benefit with this feature?** \r\nThose who use distributed training.\r\n\r\n**Any Other info.**\r\nN/A\r\n", "comments": ["What's the progress of this issue?", "What's the progress of this issue?", "This feature is promising. What's the progress of this issue? @yuefengz ", "We will send out a RFC for PSStrategy soon.", "There is any plan? Thanks @yuefengz ", "@yuefengz any developments on this ?", "hi , Is there some progress with this issue?"]}, {"number": 27724, "title": "Support model parallelism in tf.distribute.Strategy", "body": "Current tf.distribute.Strategy only supports data parallelism. We would like to support the following form of model parallelism: user specifies a number of \"logical devices\" per replica upfront, and then explicitly places ops on a specific logical device. \r\n\r\n\r\n", "comments": ["We can benefit from this in https://github.com/tensorflow/adanet: we have heterogeneous subgraphs within a tf.Graph that we would like to distribute across logical devices or worker tasks. \r\n\r\nTo handle this use-case, we a [custom round-robin strategy for the distributed cluster use-case](https://adanet.readthedocs.io/en/v0.6.1/distributed.html) (loosely-related to tf.distribute.Strategy). Would this make sense as a `tf.distribute.Strategy`?"]}, {"number": 27703, "title": "When using long time inference TFLite GPU not working in mobile (demo app)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device= Pixel 2, Samsung Galaxy 6, 7, 8\r\n- TensorFlow Lite version : experimental-0.0.1\r\n- GPU : Adreno 540 \r\n\r\n**Describe the current behavior**\r\nUsing more than about 10 minutes, keep running Demo App, but not working inference.\r\nNormal inference time: about 50ms (pixel 2)\r\nAbnormal inference time: about 3ms (pixel 2)\r\nThe same problem occurs in other phones.(Galaxy series ( >= 6), Xiaomi redmi note series)\r\n\r\n**Code to reproduce the issue**\r\n[Just demo code](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo)\r\n", "comments": ["We had a situation that `glDispatchProgram` would just fail after 400k calls (or something like that) on a particular phone.  Hm, I'm actually surprised that it happens on so many phones as you have described."]}]