[{"number": 13808, "title": "add swish activation function to python ops", "body": "Swish activation function `swish(x) = x * sigmoid(x)` has\r\njust been published by the Google Brain team. This is a\r\nsimple implementation of the same in TF's python interface.\r\n\r\nRef: https://export.arxiv.org/pdf/1710.05941", "comments": ["Can one of the admins verify this patch?", "As the paper says, we already have a version to check in.  It's only slightly more complicated because you want to avoid keeping two activation buffers worth of memory for backprop by recomputing it in the backward pass.  I'll make sure @PrajitR gets his change in soon ;)", "Oh, I missed that part. Thanks for pointing this out :)"]}, {"number": 13807, "title": "tensorflow-android library 1.4.0-rc0 seems to contain tensorflow 1.3.0-rc2", "body": "For my work I require the new String Tensor feature introduced for the Java API in tensorflow 1.4. I tested my network with my Java code on my Desktop which works fine (using org.tensorflow:tensorflow:1.4.0-rc0). \r\n\r\nHowever, when I use the same code in an Android project (using org.tensorflow:tensorflow-android:1.4.0-rc0), I get the following error leading me to the guess that the wrong version was packaged:\r\n\r\n```\r\n10-18 16:33:33.987 21288-22070/test.android.fisheye W/System.err: java.util.concurrent.ExecutionException: java.lang.UnsupportedOperationException: non-scalar DataType.STRING tensors are not supported yet (version 1.3.0-rc2). Please file a feature request at https://github.com/tensorflow/tensorflow/issues/new\r\n10-18 16:33:33.988 21288-22070/test.android.fisheye W/System.err:     at java.util.concurrent.FutureTask.report(FutureTask.java:94)\r\n10-18 16:33:33.988 21288-22070/test.android.fisheye W/System.err:     at java.util.concurrent.FutureTask.get(FutureTask.java:164)\r\n10-18 16:33:33.988 21288-22070/test.android.fisheye W/System.err:     at test.neuronalnetwork.service.TaskWorkerLoop$Loop.run(TaskWorkerLoop.java:71)\r\n10-18 16:33:33.988 21288-22070/test.android.fisheye W/System.err:     at java.lang.Thread.run(Thread.java:762)\r\n```\r\n\r\nI also downloaded the org.tensorflow:tensorflow-android:1.4.0-rc0 from maven central and had a look at the class files. They look like the class files from the 1.3.0 version. \r\n\r\nFurthermore, when looking at the Tensor and Tensorflow classes, you can see that the version displayed in the error message above, is actually read from the native library. \r\n\r\nDo you agree, or did I miss something? If you do agree, can you please upload a real 1.4.0-rc0 to maven central?", "comments": ["@av8ramit did we forget to change a string?", "@case540 did we upload the Java maven packages for rc0?", "Yes, the Maven packages were uploaded for rc0 so that is not the issue at least.", "As I tried to point out, I think you compiled an older version and uploaded that. Because it does not seem that only the string is wrong. Instead, the whole feature \"string tensors\" is missing in that version (but should be in there). ", "I just saw you released rc1 already. With rc1 my issue is fixed. Thanks guys and keep up the good work!"]}, {"number": 13806, "title": "What will happen if one of the worker becomes dead in distribute tensorflow?", "body": "Hi, I deployed a **distribute tensorflow** cluster to train a Deep Neural Network system, but in the training process, one of the worker broke down for some unknown reason(maybe the bad network), while the other workers were still training data, their training progresses didn't stop and continued going on. And after I restarted the broken worker, the ps node could send data to the broken worker, the broken worker also could train data with other workers, but the amazing thing happened: **the good workers' training progresses were set to zero, and they began to retrain the data with the broken worker node. So my previous training progresses were all gone...** That's to say, the tarining process restarted. is there anybody know how to solve this problem?\r\nMy tensorflow version: 0.10.0\r\n\r\nThanks in advance!", "comments": ["I'd pose this question on stack overflow to make sure you are following best practices for open source distributed tensor flow w.r.t. to saving checkpoints.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13805, "title": "Tensorflow serving API does not support python3.5 ", "body": "Tensorflow serving API only supports python2.7.   It is not work for python3 .5", "comments": ["/CC @sukritiramesh\r\n\r\nDid you try with python3.5? What happens there?", "@drpngx Yes, I try to install it in pip with python3.5 and fails. The command, **pip install tensorflow_serving_api** , does not work, but python2.7 can run this command and find this package.    There is no tensorflow serving api package for python3.5. ", "How does it fail?", "Apparently TensorFlow Serving only supports Python 2.\r\nhttps://github.com/tensorflow/serving/issues/581", "@drpngx The error information \r\n  **Could not find a version that satisfies the requirement tensorflow-serving-api (from versions: )\r\nNo matching distribution found for tensorflow-serving-api**\r\n", "@Carmezim  Thank you for your answer.", "Forced to use anaconda3 while working on a cluster with no sudo permission, i figured out that `pip install tensorflow-serving-api` only works for python 2.7.12 or higher python 2.7.9 fails.\r\n@jieliuu you might check your version + good luck\r\n", "Closing as this is resolved\r\n\r\n\r\n"]}, {"number": 13804, "title": "Figure out CUDA and cuDNN versions", "body": "So I'm setting up an image for a group to be used in the cloud. \r\nGiven that there certain cloud related things which we don't have premissions to we want to be able to verify that Tensorflow is using indeed what we intended. However, there does not seem to be any comprehensive way of understanding at the moment what exactly is it using. The output after creating a session is:\r\n```\r\n2017-10-18 10:49:51.880192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-10-18 10:49:51.880752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:1e.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\n2017-10-18 10:49:51.880818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 \r\n2017-10-18 10:49:51.880849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y \r\n2017-10-18 10:49:51.880883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)\r\n```\r\nDoes this means the cuda fails since I don't see any \"succesffully loaded libcuda...\" or anthing like that. Does it even load cuDNN? Is there any way to directly check it (once is enough don't need to be printed every time. \r\nI did not find any documentations and there was not too much here in the issues except people not beeing able to do it. ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13803, "title": "Adding a variable of learning rate for each layer", "body": "I am wondering that is it easy to add a variable in tf.get_variable function, which allows us to initialize the learning rate for each layer easily. Since I found out that it's not easy to do in tensorflow but easy to do in other toolboxes.", "comments": ["I think the easiest way is to use the optimizer API to get the gradients. It will be returned as a nested map where you have the variable names on one side, and the gradients on the other. They come pre-multiplied by the global learning rate, and you can re-multiply.\r\n\r\nDid you try on stackoverflow? Maybe there's an even simpler way.", "Thanks for your comments. I found some solutions on stackoverflow, here is the link:\r\nhttps://stackoverflow.com/questions/34945554/how-to-set-layer-wise-learning-rate-in-tensorflow\r\nI didn't try, but I guess it works in that way. However it does not work like what I would expect.\r\n ", "Hi, @xialeiliu , so this is a feature request, right?", "Yes, you are right.\n\nOn 19 October 2017 at 14:41, Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\nwrote:\n\n> Hi, @xialeiliu <https://github.com/xialeiliu> , so this is a feature\n> request, right?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13803#issuecomment-337894364>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APamPuIU3jf0KaqRF_ZhGrXH8RAe1k1gks5st0N0gaJpZM4P9ejm>\n> .\n>\n", "What is the feature request? What is wrong with the second answer on stack overflow?", "I found second answer is convenient to pass the variables needed to optimizer. Sorry for the late reply.  "]}, {"number": 13802, "title": "deadlock in fork() because of OpenBlas", "body": "TensorFlow: 1.3.0 (v1.3.0-rc2-20-g0787eee)\r\n\r\nI'm not exactly sure whether this is a TF specific problem, or OpenBlas specific, or at what place this should be fixed.\r\n\r\nAt some part in my code, I want to start a subprocess, via `Popen`, and it uses `fork()` internally. Before that, I already initialized the TF session and thus have initialized the thread pools.\r\n\r\nThe `fork()` will cause a deadlock because OpenBlas has used `pthread_atfork()` to register `blas_thread_shutdown_()` to do some cleanup, which will wait for a lock, which probably was acquired by some of the other threads at that time. Stacktrace:\r\n\r\n```\r\n#0  0x00002b289b1702ad in __lll_lock_wait () from /u/zeyer/tools/glibc217/libpthread.so.0\r\n#1  0x00002b289b16dabd in pthread_cond_signal@@GLIBC_2.3.2 () from /u/zeyer/tools/glibc217/libpthread.so.0\r\n#2  0x00002b294d9fb68e in blas_thread_shutdown_ () from /usr/lib/libopenblas.so.0\r\n#3  0x00002b289b44b965 in fork () from /u/zeyer/tools/glibc217/libc.so.6\r\n#4  0x00002b28ae1f1c47 in subprocess_fork_exec (self=<optimized out>, args=<optimized out>)\r\n    at /tmp/python3-20170710-4344-zt9hmb/Python-3.6.1/Modules/_posixsubprocess.c:672\r\n#5  0x00002b289acf79c9 in _PyCFunction_FastCallDict (func_obj=0x2b28adfa34c8, args=0x1653030, nargs=<optimized out>, \r\n    kwargs=kwargs@entry=0x0) at Objects/methodobject.c:234\r\n#6  0x00002b289acf7c97 in _PyCFunction_FastCallKeywords (func=func@entry=0x2b28adfa34c8, stack=stack@entry=0x1653030, \r\n    nargs=<optimized out>, kwnames=kwnames@entry=0x0) at Objects/methodobject.c:295\r\n#7  0x00002b289ad8b171 in call_function (pp_stack=pp_stack@entry=0x7ffe97057f30, oparg=oparg@entry=17, \r\n    kwnames=kwnames@entry=0x0) at Python/ceval.c:4798\r\n#8  0x00002b289ad8eeb7 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284\r\n...\r\n```\r\n\r\n[This OpenBLAS issue](https://github.com/xianyi/OpenBLAS/issues/240) and [this Sage issue](https://trac.sagemath.org/ticket/22021) might be related. Basically the Sage solution is to disable the multi-threading support of OpenBLAS by setting `OMP_NUM_THREADS=1` but I actually want to use the multi-threading support if possible.\r\n", "comments": ["I wonder if error will happen if you use MKL instead of OpenBLAS (ps, conda install mkl will install 2018 version which upgrades your numpy and causes numpy version mismatch, need mkl version 2017)", "Probably not, as the atfork handler registered by OpenBlas triggers this.\r\nThis could also be fixed on the CPython side, by avoiding `fork()` in their `subprocess_fork_exec` implementation, and using e.g. `vfork()`, `posix_spawn()` or `syscall(SYS_clone, SIGCHLD, 0)` instead ([via](https://stackoverflow.com/questions/46810597/forkexec-without-atfork-handlers)). I reported that for CPython [here](https://bugs.python.org/issue31814).", "To fork a TF session doesn't seem very safe ([reference](https://github.com/tensorflow/tensorflow/issues/5448#issuecomment-258934405)) but I'm not familiar with the details. If you're using GPU, then a fork sounds more dangerous.", "you may have better luck doing fork before \"import tensorflow\", I've seen weird segfaults if you do it after", "Actually, in my case, I just want to start a subprocess (via `subprocess.Popen`, or any other way), so I just need that fork+exec works, or maybe that I can use an alternative, like vfork+exec or posix_spawn, which probably does not have this issue, as it would not execute the atfork handlers.\r\n\r\nAs I read and thought more about this now, I don't think that there is anything on the TF side which can/should be done about this. A safe subprocess_fork_exec can be done on the CPython side, and the issue itself maybe should be fixed on the OpenBlas side. So maybe we can close this issue here.\r\n\r\nNote that it's not trivial in all cases to avoid any subprocess creation at all. Even the `audioread` package e.g. can trigger this, via `ctypes.util.find_library`, which will use some external tool. Consider this stacktrace (where it hangs):\r\n```\r\n  File \"/u/zeyer/.linuxbrew/Cellar/python3/3.6.1/lib/python3.6/subprocess.py\", line 1260 in _execute_child\r\n  File \"/u/zeyer/.linuxbrew/Cellar/python3/3.6.1/lib/python3.6/subprocess.py\", line 707 in __init__\r\n  File \"/u/zeyer/.linuxbrew/Cellar/python3/3.6.1/lib/python3.6/ctypes/util.py\", line 271 in _findSoname_ldconfig\r\n  File \"/u/zeyer/.linuxbrew/Cellar/python3/3.6.1/lib/python3.6/ctypes/util.py\", line 302 in find_library\r\n  File \"/u/zeyer/.local/lib/python3.6/site-packages/audioread/__init__.py\", line 59 in _ca_available\r\n  File \"/u/zeyer/.local/lib/python3.6/site-packages/audioread/__init__.py\", line 85 in audio_open\r\n  File \"/u/zeyer/.local/lib/python3.6/site-packages/librosa/core/audio.py\", line 107 in load\r\n```\r\n", "In case anyone else stumbles here, what I do now, and remember that I just need fork+exec to work, not fork without exec. I have this simple C code and compile it as a lib `patch_atfork.so`:\r\n\r\n```\r\n#include <stdio.h>\r\n#include <stdlib.h>\r\n\r\nint pthread_atfork(void (*prepare)(void), void (*parent)(void), void (*child)(void)) {\r\n  printf(\"Ignoring pthread_atfork call!\\\\n\");\r\n  fflush(stdout);\r\n  return 0;\r\n}\r\n```\r\n\r\nThen I load it via `LD_PRELOAD=patch_atfork.so` before running my script. That will probably make the forked-environment useless for some cases but exec will work now.\r\n", "Actually, overriding `pthread_atfork` via `LD_PRELOAD` does not always work. See [here](https://stackoverflow.com/questions/46845496/ld-preload-and-linkage) for details.\r\n\r\nYet another solution is to override `fork` via `LD_PRELOAD` with this:\r\n```\r\npid_t fork(void) {\r\n  return syscall(SYS_clone, SIGCHLD, 0);\r\n}\r\n```\r\n\r\nThis seems to work now (see my recent referenced commit).", "/CC @gunan @rmlarsen ", "I'm closing this issue, since it seems fixing this is outside the scope of TF. Please reopen if I'm mistaken, and feel free to continue the discussion. @albertz thanks for sharing your solution.", "Just in case anyone encounters this bug again in the future, the underlying cause was most likely a bug in OpenBlas (https://github.com/xianyi/OpenBLAS/issues/2270), which is fixed at OpenBlas head.\r\n\r\nI also changed TF to attempt to work around the issue when TF itself forks a subprocess by using `posix_spawnp()` in https://github.com/tensorflow/tensorflow/commit/051542c14d85ce8de8bae5822fa746d335ce3df6#diff-d4603d952d6033265683a212d7def70514fddad4b0ffb023e42eb34cb24b08b8\r\n\r\n\r\n", "@hawkinsp thank you for the suggestion. I possibly found another tensorflow+fork related issue (#51832). Since in my sample the `fork()` call is not performed by TF itself your `posix_spawnp()` workaround probably doesn't apply. Also I tried to forcely upgrade numpy to version 1.21.2, which uses OpenBlas 3.17, but that also doesn't work. It seems still related to linear algebra operations though, because disabling threading for [individual](https://www.tensorflow.org/api_docs/python/tf/config/threading/set_intra_op_parallelism_threads?hl=it) operations workaround the issue in some scenarios.\r\n\r\n**EDIT**:  after more debugging, my linked issue appears to be not OpenBlas related at all. It looks like tensorflow and/or Eigen are not `fork()` safe, at least in some scenarios where it is desirable to be so. Also keras python module, which I use in my use case, may (unnecessarily?) load threading support in tensorflow during model load.", "> At some part in my code, I want to start a subprocess, via Popen, and it uses fork() internally.\r\n\r\nThat's part of the problem.  Popen type functions have no reason not to use `posix_spawn()` or `vfork()` where those are available.  Typically `posix_spawn()` uses `vfork()` internally; `vfork()` does not call `atfork` handlers, so `posix_spawn()` implementations that use `vfork()` do not call atfork handlers either.\r\n\r\n> The `fork()` will cause a deadlock because OpenBlas has used `pthread_atfork()` to register `blas_thread_shutdown_()` to do some cleanup, which will wait for a lock, which probably was acquired by some of the other threads at that time.\r\n\r\nIf there's a deadlock in this, then OpenBlas is at fault for that deadlock, and an issue should be filed with it.\r\n\r\nPerhaps it's not a deadlock but a scalability issue, causing some threads to be starved of entering `blas_thread_shutdown_()`?  If so `blas_thread_shutdown_()` could stand to be restructured.  A typical thing to do in these cases is to track the `PID` that initialized the library and then force a cleanup later (not in an `atfork` handler) when the library is invoked on the child side.  The benefit of delaying cleanup on the child side is that if the child is going to exec-or-exit without executing non-`async-signal-safe` code then there is no need to waste any CPU cycles cleaning up, and as a side benefit there would be no chance of deadlock or starvation either in those cases."]}, {"number": 13801, "title": "Feature Request: Scripts for build tensorflow with FORTIFY", "body": "FORTIFY is an important security feature that can help us discover hidden bugs. \r\nThis is success story from Android:\r\nhttps://android-developers.googleblog.com/2017/04/fortify-in-android.html\r\n\r\nPlease do the same thing for tensorflow, provide a checked build(with FORTIFY turned on) publicly. \r\n\r\n", "comments": ["You mean build with `-DFORTIFY=1`? I think we do that by default.\r\n\r\n/CC @gunan ", "You are right.\r\n\\# hardening-check tools/proto_text/gen_proto_text_functions.runfiles/org_tensorflow/tensorflow/tools/proto_text/gen_proto_text_functions\r\ntools/proto_text/gen_proto_text_functions.runfiles/org_tensorflow/tensorflow/tools/proto_text/gen_proto_text_functions:\r\n Position Independent Executable: no, normal executable!\r\n Stack protected: yes\r\n**Fortify Source functions: yes (some protected functions found)**\r\n Read-only relocations: yes\r\n Immediate binding: yes\r\n\r\nSorry for bothering you. ", "Thanks for checking!"]}, {"number": 13800, "title": "Fix MPI and Verbs compilation when not using GPUs", "body": "Code compilation would fail when building without CUDA support, but with MPI and/or Verbs support. This PR adds the required dependencies directly into the MPI/Verbs BUILD files and fixes those compilation errors.", "comments": ["Can one of the admins verify this patch?", "Do you know what the compilation error says?  Just want to verify that this is the right dependency to add.", "This is the error:\r\n```\r\nERROR: /home/jbedorf/codes/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1): gcc failed: error executing command \r\n(....)\r\nIn file included from ./tensorflow/core/platform/stream_executor.h:26:0,\r\n                 from ./tensorflow/core/common_runtime/gpu/gpu_util.h:23,\r\n                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:30:\r\n./tensorflow/stream_executor/dso_loader.h:32:30: fatal error: cuda/cuda_config.h: No such file or directory\r\n #include \"cuda/cuda_config.h\"\r\n                              ^\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n", "Thanks!  So it seems as though since mpi_rendezvous_mgr depends on gpu_util directly, it should depend on the library that is exported by gpu_util.\r\n\r\ngpu_util looks to be exported by both core:gpu_runtime and core:gpu_runtime_impl, where the latter has a dependency on :stream_executor already.\r\n\r\nPerhaps the bug that needs to be fixed is that core:gpu_runtime should depend on stream_executor, since one of its headers includes stream_executor.  Can you give that a shot? ", "cc @allenlavoie since it looks like this split might have been done by him at some point and may have more context.", "Adding a stream_executor dependency to gpu_runtime should be fine. stream_executor is a header-only rule and has a separate implementation rule (all these rules are split for the changes described in https://github.com/tensorflow/tensorflow/commit/5c7f9e31). May have just been an omission when I did the split.", "@vrv Adding the dependency to core:gpu_runtime also resulted in a successful built. The PR has been updated. ", "@tensorflow-jenkins test this please"]}, {"number": 13799, "title": "Feature Request: need to support dynamically RDMA gid setting in tensorflow/tensorflow/contrib/verbs/rdma.cc", "body": "In tensorflow/tensorflow/contrib/verbs/rdma.cc when calling  ibv_query_gid() the gid_index field is hard-coded as 0, which could not work well in real world.\r\nTo fix this, it is better to add a user-specified option. \r\n", "comments": ["@yanivbl6 pls help involve persons here for more discussions, thanks.", "Seems to be fixed already in: https://github.com/tensorflow/tensorflow/pull/13564\r\n"]}, {"number": 13798, "title": "Update boringssl to a0fb951d2a26a8ee746b52f3ba81ab011a0af778", "body": "This fix update boringssl to `a0fb951d2a26a8ee746b52f3ba81ab011a0af778`, which contains bug fix related to #13733.\r\n\r\nSee https://github.com/tensorflow/tensorflow/pull/13734#issuecomment-337440239 for update details.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@davidben let us know if this is okay to merge", "(I don't actually work on Tensorflow.)", "Yes, but I'm not sure if what your suggestion about removing the patch is what was done; I thought the idea was to remove the patch entirely, not to just undo the changes to the existing patch.", "@vrv In the PR the `third_party/boringssl/add_boringssl_s390x.patch` file has been deleted.\r\n\r\nIn `tensorflow/workspace.bzl` file, the boringssl section has been changed from `patched_http_archive` -> `native.http_archive` so patching is gone.\r\n\r\nAlso, in case `s390x` support is desired in TensorFlow in the future, from the comment link provided (https://github.com/tensorflow/tensorflow/pull/11170#issuecomment-329252942) it is possible to disable the GCP support to handle the big-endian issue. I don't have any access to `s390x` so I might not be able to help in that case though.\r\n\r\n", "Ok great, thanks.  I didn't have the context.  This PR looks good, though I guess the patch file should just be deleted entirely."]}, {"number": 13797, "title": "Branch 172556044", "body": "", "comments": ["@tensorflow-jenkins test this please", "One small test failure (known), merging."]}, {"number": 13795, "title": "Branch 172543319", "body": "", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 13794, "title": "error", "body": "\r\n\r\n---------\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13793, "title": "Tensorflow not detecting GPU even when CUDA is installed", "body": "Hi there,\r\nI am running the following on Ubuntu 16.04 LTS and I have GPU 1070 and tensorflow version 1.0.1 (installed and checked through conda -list\r\n\r\nI am trying to run a simple piece of code in tf\r\nwith tf.device('/gpu:0'):\r\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n    c = tf.matmul(a, b)\r\n\r\nwith tf.Session() as sess:\r\n    print (sess.run(c))\r\n\r\nAnd I am getting the following error:\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'MatMul_1': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\r\n\r\nRegarding the CUDA installed, I have the following information obtained by running nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2016 NVIDIA Corporation\r\nBuilt on Tue_Jan_10_13:22:03_CST_2017\r\nCuda compilation tools, release 8.0, V8.0.61\r\n\r\nTheano is able to detect and use GPU\r\nUsing gpu device 0: GeForce GTX 1070 (CNMeM is disabled, cuDNN 5110)\r\n\r\n\r\nHow to get TF to use the GPU. Or what CUDA version should I install to make it work with GPU.\r\nI understand this is not the platform to ask questions related to CUDA but if someone can point me in the right direction then it will be helpful.\r\n\r\nThank You.", "comments": ["Please paste the logs, and please, please, fill out as much as you can from the template.", "@chans1239 \r\nWhat does nvidia-smi return? Did you install tensorflow-gpu? Do you have cuDNN installed, and is it the CUDA 8.0 version?", "nvidia-smi returned the following:\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.82                 Driver Version: 375.82                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    Off  | 0000:01:00.0      On |                  N/A |\r\n| N/A   38C    P8     8W /  N/A |    282MiB /  8111MiB |      5%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1034    G   /usr/lib/xorg/Xorg                             171MiB |\r\n|    0      1764    G   compiz                                          40MiB |\r\n|    0      2679    G   ...el-token=77106A4CB88D22FD5BF26E235E3576CE    68MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\nIt turns out it had tensorflow - CPU version only. Installing tensorflow-gpu and correcting few errors solved the problem. This issue can be closed.\r\n\r\nThank you for the help.\r\n\r\n", "@chans1239 Can you please tell what all errors you solved and which version of tensorflow you installed?", "I installed the tensorflow-gpu version and the solved the issue!"]}, {"number": 13792, "title": "Fix minor typos in TF Boosted Trees", "body": "Fix minor typos in TF Boosted Trees", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13791, "title": "Add `int64` axis support for `tf.cumsum` and `tf.cumprod`", "body": "This fix adds `int64` axis support for `tf.cumsum` and `tf.cumprod`.\r\n\r\nThough `int64` is the registered data type for `axis` (`Tidx`) in math_ops.cpp, the corresponding kernel is not available.\r\n\r\nThe issue could be described as:\r\n```\r\n>>> import tensorflow as tf\r\n>>> v = tf.cumsum([1, 2, 3], tf.constant(0, tf.int64))\r\n>>> tf.Session().run(v)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Cumsum' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; T in [DT_COMPLEX128]; Tidx in [DT_INT32]\r\n  device='CPU'; T in [DT_COMPLEX64]; Tidx in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; Tidx in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; Tidx in [DT_INT32]\r\n...\r\n```\r\n\r\nThis fix adds the missing kernels and adds additional test cases for them.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Looks nice!  @tensorflow-jenkins test this please", "The only failure is on Windows CMAKE which seems to be a Jenkins network issue:\r\n```\r\n22:45:53  > git fetch --tags --progress https://github.com/tensorflow/tensorflow.git +refs/pull/*:refs/remotes/origin/pr/*\r\n22:45:57 ERROR: Error fetching remote repo 'origin'\r\n22:45:57 hudson.plugins.git.GitException: Failed to fetch from https://github.com/tensorflow/tensorflow.git\r\n22:45:57 \tat hudson.plugins.git.GitSCM.fetchFrom(GitSCM.java:817)\r\n22:45:57 \tat hudson.plugins.git.GitSCM.retrieveChanges(GitSCM.java:1084)\r\n```\r\n\r\nI think a rerun may resolve it."]}, {"number": 13790, "title": "compile failure with --config=mkl", "body": "Building from git 27767d8e9c1325979cf32ff5b81c10df9006fd57 with `TF_NEED_MKL=1` and `TF_DOWNLOAD_MKL=1`. It seems d835d677ade78a41e0e097f67c87b6ab8588a90a introduced a compile failure:\r\n\r\n```\r\nERROR: /build/tensorflow-git/src/tensorflow-mkl/tensorflow/core/kernels/BUILD:819:1: C++ compilation of rule '//tensorflow/core/kernels:transpose_op' failed (Exit 1).\r\ntensorflow/core/kernels/mkl_transpose_op.cc:53:10: error: template-id 'MKLTranspose2D<float>' for 'tensorflow::Status tensorflow::MKLTranspose2D(char, const tensorflow::Tensor&, tensorflow::Tensor*)' does not match any template declaration\r\n   Status MKLTranspose2D<T>(const char trans, const Tensor& in, Tensor* out) { \\\r\n          ^~~~~~~~~~~~~~~~~\r\ntensorflow/core/kernels/mkl_transpose_op.cc:53:10: note: in definition of macro 'INSTANTIATE'\r\n   Status MKLTranspose2D<T>(const char trans, const Tensor& in, Tensor* out) { \\\r\n          ^~~~~~~~~~~~~~\r\ntensorflow/core/kernels/mkl_transpose_op.cc:44:6: note: candidate is: template<class T> void tensorflow::{anonymous}::MKLTranspose2D(char, const tensorflow::Tensor&, tensorflow::Tensor*)\r\n void MKLTranspose2D(const char trans, const Tensor& in, Tensor* out) {}\r\n      ^~~~~~~~~~~~~~\r\ntensorflow/core/kernels/mkl_transpose_op.cc:122:1: error: expected '}' at end of input\r\n }  // namespace tensorflow\r\n ^\r\n```\r\n\r\nThere are multiple problems in there. This patch gets it closer to compiling (correcting return type of `MKLTranspose2D` base template, adding missing line-continuation escape):\r\n\r\n```diff\r\ndiff --git a/tensorflow/core/kernels/mkl_transpose_op.cc b/tensorflow/core/kernels/mkl_transpose_op.cc\r\nindex 89a1d5e8a..93da2a6ea 100644\r\n--- a/tensorflow/core/kernels/mkl_transpose_op.cc\r\n+++ b/tensorflow/core/kernels/mkl_transpose_op.cc\r\n@@ -41,7 +41,7 @@ namespace tensorflow {\r\n \r\n namespace {\r\n template <typename T>\r\n-void MKLTranspose2D(const char trans, const Tensor& in, Tensor* out) {}\r\n+Status MKLTranspose2D(const char trans, const Tensor& in, Tensor* out);\r\n \r\n // Documentation here: https://software.intel.com/en-us/node/520863\r\n // Parameters: (ordering:row-major, operation:transpose, num_rows, num_cols,\r\n@@ -54,7 +54,7 @@ void MKLTranspose2D(const char trans, const Tensor& in, Tensor* out) {}\r\n     mkl_##PREFIX##omatcopy('R', trans, in.dim_size(0), in.dim_size(1), 1,     \\\r\n                            in.flat<T>().data(), in.dim_size(1),               \\\r\n                            out->flat<T>().data(), in.dim_size(0));            \\\r\n-    return Status::OK();\r\n+    return Status::OK();                                                      \\\r\n   }\r\n \r\n   INSTANTIATE(float, s)\r\n@@ -66,7 +66,7 @@ void MKLTranspose2D(const char trans, const Tensor& in, Tensor* out) {}\r\n   static const char kMKLTranspose = 'T';\r\n   static const char kMKLConjugateTranspose = 'C';\r\n \r\n-  }  // namespace tensorflow\r\n+  }  // anonymous namespace\r\n \r\n   Status MklTransposeCpuOp::DoTranspose(OpKernelContext* ctx, const Tensor& in,\r\n                                         gtl::ArraySlice<int32> perm,\r\n```\r\n\r\nBut I'm not sure how to handle this issue (conversion of the `alpha` argument into float/double/MKL_Complex8/MKL_Complex16):\r\n\r\n```\r\nERROR: /home/steven/Development/misc-packages/tensorflow-git/src/tensorflow-mkl/tensorflow/core/kernels/BUILD:819:1: C++ compilation of rule '//tensorflow/core/kernels:transpose_op' failed (Exit 1).\r\ntensorflow/core/kernels/mkl_transpose_op.cc: In function 'tensorflow::Status tensorflow::{anonymous}::MKLTranspose2D(char, const tensorflow::Tensor&, tensorflow::Tensor*) [with T = std::complex<float>]':\r\ntensorflow/core/kernels/mkl_transpose_op.cc:57:69: error: could not convert '1' from 'int' to 'MKL_Complex8 {aka _MKL_Complex8}'\r\n                                out->flat<T>().data(), in.dim_size(0));            \\\r\n                                                                     ^\r\ntensorflow/core/kernels/mkl_transpose_op.cc:63:5: note: in expansion of macro 'INSTANTIATE'\r\n     INSTANTIATE(complex64, c)\r\n     ^\r\ntensorflow/core/kernels/mkl_transpose_op.cc: In function 'tensorflow::Status tensorflow::{anonymous}::MKLTranspose2D(char, const tensorflow::Tensor&, tensorflow::Tensor*) [with T = std::complex<double>]':\r\ntensorflow/core/kernels/mkl_transpose_op.cc:57:69: error: could not convert '1' from 'int' to 'MKL_Complex16 {aka _MKL_Complex16}'\r\n                                out->flat<T>().data(), in.dim_size(0));            \\\r\n                                                                     ^\r\ntensorflow/core/kernels/mkl_transpose_op.cc:64:5: note: in expansion of macro 'INSTANTIATE'\r\n     INSTANTIATE(complex128, z)\r\n     ^\r\ntensorflow/core/kernels/mkl_transpose_op.cc: At global scope:\r\ntensorflow/core/kernels/mkl_transpose_op.cc:95:10: error: 'template<bool conjugate> class tensorflow::MklConjugateTransposeCpuOp' used without template parameters\r\n   Status MklConjugateTransposeCpuOp::DoTranspose(OpKernelContext* ctx,\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```", "comments": ["I can confirm that I'm also experiencing this issue on git 75da6f6", "@rmlarsen Can you comment on this one? Thanks...", "This appears now fixed by f1054553eafc74df8be9425c3344e71af98962ad"]}, {"number": 13789, "title": "Feature Request: C++ gradient for LRN (Local Response Normalization)", "body": "Implement the gradient for LRN in c++ so that it is available for TF_AddGradients\r\n\r\nThis is the python code that I think would need to be ported:\r\nhttps://github.com/tensorflow/tensorflow/blob/48be6a56d5c49d019ca049f8c48b2df597594343/tensorflow/python/ops/nn_grad.py#L516\r\n\r\nI'm using this issue to call dibs on this gradient port per @bpiel's suggestion\r\nI was also advised to mention @suharshs ", "comments": ["Thanks @gdeer81! feel free to also link to other discussions for context if they are online.", "I think I am looking in the wrong place for the implementation for this because the code I mentioned above is just collecting some attributes and then calling out to `gen_nn_ops._lrn_grad` : https://github.com/tensorflow/tensorflow/blob/48be6a56d5c49d019ca049f8c48b2df597594343/tensorflow/python/ops/nn_grad.py#L522\r\n\r\nthis is allegedly imported from gen_nn_ops: `from tensorflow.python.ops import gen_nn_ops`\r\n\r\nBut I can't seem to find this anywhere.  \r\n\r\nCan someone point me to the right place?", "@gdeer81 This is an example where the grad logic is already implemented as an op. In this case, the op is called `lrn_grad`.", "To add to @bpiel 's comment, see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/gradients/nn_grad.cc#L65\r\n\r\nfor an example of another op (Relu) which just calls its corresponding ReluGrad op as its gradients function. You can likely do something similar for LRN.", "Awesome, thanks for the quick feedback. So I'm just going to name this LRNHelper and try to follow the conventions in both of those examples", "@suharshs since this is my first one, would it be too much to ask to get someone to write a test for me?  I'd feel more confident in my implementation if I could make a test someone else wrote pass ", "@gdeer81 I'll do it."]}, {"number": 13788, "title": "Feature request: Prevent predict_scores() in tf.contrib.learn.estimators from reloading graph variables from checkpoint file everytime", "body": "Hi there,\r\n\r\nI am using tf.contrib.learn.DNNLinearCombinedRegressor with wide and deep feature columns in Tesnorflow 1.3. I use an input_fn to parse pandas dataframes to this regressor for training. I have both real and categorical features, and so my deep feature columns are made of sparse columns with embedding as well as real valued columns. After training, I want to use the trained model for prediction in another application. I can call estimator.predict_scores() function to do this, but it seems very slow. Mainly because, it seems to be reloading the graph variables from last checkpoint file created during training everytime it is called. Can we prevent this for faster predictions, so that it reloads the variables only the first time it is called ? My application is simple, and I would like to not use Tensorflow serving.\r\n\r\nThanks.", "comments": ["Do you mean that you invoke the `predict_scores()` for multiple times? Yes, `predict_scores` always constructs graph every time. Hence, you can (1) collect all data and feed them all at once, or (2) use tf-serving.", "@facaiy  Thanks for your reply. Yes, I call predict_scores() multiple times. I have an application where-in new samples (feature set) are generated one at a time, and I need to show their scores. So, I send each sample as it is generated to predict_scores() to get the predicted score, requiring multiple calls to predict_scores().  Tensorflow serving seems like an overkill for my simple local desktop application. So, I was trying to avoid it. I wish there was a simpler way than that.\r\n", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "This is a known issue. We've decided to keep Estimator free of state -- please use contrib/predictor to get that behavior.\r\n\r\nFor most cases, you can pass a generator as input. As long as the generator yields new input, the model is kept in memory."]}, {"number": 13787, "title": "no module named models.rnn.translate", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["There is no content in the description. Please resubmit."]}, {"number": 13786, "title": "no module", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["please fill out template and include reproducible test case."]}, {"number": 13785, "title": "TypeError in freeze_graph tool while trying to freeze a graph in Tensorflow 1.3", "body": "I train a tf.contrib.learn estimator (specifically, DNNLinearCombineRegressor) in Python and saved the model\u2019s parameters and graph by specifying model_dir when defining the estimator. My estimator uses embedding and real valued feature columns, and a custom input_fn to parse the data. After training is done, I try to freeze the graph using the CLI as mentioned in this [post](https://stackoverflow.com/questions/46223252/error-while-freezing-the-model-freeze-graph), and get this error --` TypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(\"dnn/hiddenlayer_0/biases:0\", shape=(10,), dtype=float32)` in saver.py in the function _ValidateAndSliceInputs.  Is this because I am using an input_fn with pandas dataframe, or is it because of the tf.feature_columns that I am using? \r\n\r\n\r\nI want to freeze the graph so that predict_scores() doesnt reload the graph variables from checkpoint file everytime for prediction. I am using Tensorflow 1.3 with Anaconda Python 3.5 on Windows 10. \r\n\r\nThanks in advance. \r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I am not sure, if the freeze_graph works with input_fn, and estimators. So, it could be a bug. I have posted on StackOverflow before posting here. I also found other people facing similar problems on StackOverflow without any solution. Alternatively, the feature request is to prevent predict_scores() for estimators to not reload graph weights from checkpoint file everytime (or after first call) to make it faster. This will allow faster test time models without going into Tensorflow serving\r\n\r\nThanks.", "Try following command to freeze the graph. We are able to create .pb file for text_classification.py (uses estimator and input function) example in tensorflow using the same command:\r\n\r\npython  path_to/tensorflow/python/tools/freeze_graph.py --input_graph ./log/graph.pbtxt --input_checkpoint ./log/model.ckpt-0 --output_graph ./log/frozen_model.pb --output_node_names=ENet/logits_to_softmax\r\n\r\n", "I'm having exact same problem as @rutadesai \"TypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(\"dnn/hiddenlayer_0/bias:0\", shape=(2,), dtype=float32)\". @rutadesai Have you resolved your problem? Thanks!", "@drpngx Thanks for your suggestion. However, on [StackOverflow ](https://stackoverflow.com/search?q=names_to_saveables+must+be+a+dict+) there are a lot of open issues without answers. \r\n\r\nI also have an issue, which @rutadesai and @xia-sun have. Would be nice to have any suggestions how can it be solved.", "@rutadesai did you solve the issue? Was it because of pandas input function? or because of feature columns?\r\n\r\nI'm using `input_fn = tf.estimator.inputs.pandas_input_fn(x=x_train, y=y_train, batch_size=100, num_epochs=None, shuffle=True)` as an input function for `LinearClassifier.train()`.\r\n\r\nAll my feature columns have type `tf.feature_column.categorical_column_with_vocabulary_list`", "+1", "Getting same issue. Stackoverflow does not have answer to it as of today. ", "@dkhmelenko, @xia-sun  sorry for late reply! I couldn't solve this issue :(", "Same issue using `tf.estimator.DNNRegressor` , any update on this?", "In the same boat using estimators...", "the same issue", "same", "Do we still have the same problem in the latest version?", "@drpngx yes, the issue still exists in Tensorflow 1.8.0.\r\n\r\nI created a separate ticket for it with the detailed description (https://github.com/tensorflow/tensorflow/issues/18523), but it was not processed so far.", "Thank you!\n\nOn Thu, Jun 7, 2018, 12:14 PM Dmytro Khmelenko <notifications@github.com>\nwrote:\n\n> @drpngx <https://github.com/drpngx> yes, the issue still exists in\n> Tensorflow 1.8.0.\n>\n> I created a separate ticket for it with the detailed description (#18523\n> <https://github.com/tensorflow/tensorflow/issues/18523>), but it was not\n> processed so far.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13785#issuecomment-395531627>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbTGjcFL8DI1a0QAqQ2bGea3Sfaitks5t6XuDgaJpZM4P8Pkp>\n> .\n>\n", "@drpngx do you know if there is a plan to fix this issue?", "In general, we want to fix all issues, once we find out what they are. Of course, a fix is always welcome if you get to it before we do.", "same issue using v1.9", "@OzzieFZI please have further comments on #18523."]}, {"number": 13784, "title": "Docker image file generated by parameterized_docker_build.sh fails (3)", "body": "**Issue**\r\n\r\nFixed previous docker install issue [#13379](https://github.com/tensorflow/tensorflow/issues/13379)\r\n\r\nExecuting parameterized_docker_build.sh generates new docker file but image fails to turn into an docker image, throws error message.\r\n\r\n**System information**\r\n\r\n- I have used a stock example script provided in TensorFlow\r\n- Windows 10 professional\r\n- TensorFlow install as docker image tensorflow/tensorflow:1.3.0-devel-py3\r\n- TensorFlow version 1.3\r\n- Python version 3\r\n- Bazel version 0.5.0\r\n- CUDA/cuDNN version not relevant (CPU install)\r\n- GPU model and memory not relevant (CPU install)\r\n- Docker CE 17.09.0-ce\r\n\r\n`$which docker`\r\n> /usr/bin/docker\r\n\r\n**Command triggering issue**\r\n\r\n`$/tensorflow/tensorflow/tools/docker/parameterized_docker_build.sh`\r\n\r\n**Error messages**\r\n\r\n> Required build parameters:\r\n>   TF_DOCKER_BUILD_TYPE=cpu\r\n>   TF_DOCKER_BUILD_IS_DEVEL=no\r\n>   TF_DOCKER_BUILD_DEVEL_BRANCH=\r\n> \r\n> Optional build parameters:\r\n>   TF_DOCKER_BUILD_CENTRAL_PIP=http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl\r\n>   TF_DOCKER_BUILD_IMAGE_NAME=\r\n>   TF_DOCKER_BUILD_VERSION=\r\n>   TF_DOCKER_BUILD_PORT=\r\n>   TF_DOCKER_BUILD_PUSH_CMD=\r\n> \r\n> FINAL_IMAGE_NAME: tensorflow/tensorflow\r\n> FINAL_TAG: latest\r\n> Original Dockerfile: /tensorflow/tensorflow/tools/docker/Dockerfile\r\n> \r\n> \r\n> Docker build will occur in temporary directory: /tmp/tmp.nBIOwyaShf\r\n> Downloading pip wheel from: http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl\r\n> \r\n> Modified Dockerfile at: /tmp/tmp.nBIOwyaShf/Dockerfile\r\n> \r\n> Building docker image with image name and tag: /tensorflow:latest\r\n> invalid argument \"/tensorflow:latest\" for t: invalid reference format\r\n> See 'docker build --help'.\r\n> FAIL: docker build of /tensorflow:latest with Dockerfile /tmp/tmp.nBIOwyaShf/Dockerfile failed\r\n\r\n**Inspecting the resulting docker file for further introspection**\r\n\r\n`$cat /tmp/tmp.nBIOwyaShf/Dockerfile`\r\n\r\n> FROM ubuntu:16.04\r\n> \r\n> MAINTAINER Craig Citro <craigcitro@google.com>\r\n> \r\n> RUN apt-get update && apt-get install -y --no-install-recommends \\\r\n>         build-essential \\\r\n>         curl \\\r\n>         libfreetype6-dev \\\r\n>         libpng12-dev \\\r\n>         libzmq3-dev \\\r\n>         pkg-config \\\r\n>         python \\\r\n>         python-dev \\\r\n>         rsync \\\r\n>         software-properties-common \\\r\n>         unzip \\\r\n>         && \\\r\n>     apt-get clean && \\\r\n>     rm -rf /var/lib/apt/lists/*\r\n> \r\n> RUN curl -O https://bootstrap.pypa.io/get-pip.py && \\\r\n>     python get-pip.py && \\\r\n>     rm get-pip.py\r\n> \r\n> RUN pip --no-cache-dir install \\\r\n>         Pillow \\\r\n>         h5py \\\r\n>         ipykernel \\\r\n>         jupyter \\\r\n>         matplotlib \\\r\n>         numpy \\\r\n>         pandas \\\r\n>         scipy \\\r\n>         sklearn \\\r\n>         && \\\r\n>     python -m ipykernel.kernelspec\r\n> \r\n> RUN pip --no-cache-dir install http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl\r\n> \r\n> \r\n> COPY jupyter_notebook_config.py /root/.jupyter/\r\n> \r\n> COPY notebooks /notebooks\r\n> \r\n> COPY run_jupyter.sh /\r\n> \r\n> EXPOSE 6006\r\n> EXPOSE 8888\r\n> \r\n> WORKDIR \"/notebooks\"\r\n> \r\n> CMD [\"/run_jupyter.sh\", \"--allow-root\"]\r\n\r\n**Additional introspection with docker image file**\r\n\r\nCopied and pasted the parameterized_docker_build.sh output docker file content and tried to execute on docker hub which fails.\r\n\r\nDocker hub log reports yields the following additional error information:\r\n\r\n> Step 6/9 : RUN pip --no-cache-dir install http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl\r\n> \r\n>  ---> Running in fd0d49a714e4\r\n> \r\n> \u001b[91mtensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl is not a supported wheel on this platform.\r\n> \u001b[0m\r\n> Removing intermediate container fd0d49a714e4\r\n> \r\n> The command '/bin/sh -c pip --no-cache-dir install http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl' returned a non-zero code: 1", "comments": ["@caisq Can you take a look?", "**Complementary test for docker image file introspection through docker hub**\r\n\r\nVisited [Tensorflow's Jenkins automation server](http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/ ), found out that there was another version named tensorflow-1.head-cp35-cp35m-linux_x86_64.whl instead of tensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl\r\n\r\nEdited the docker file to substitute one by the other which triggered the exact same error when trying to build docker image via docker hub.\r\n\r\nDocker hub log issued following error message:\r\n\r\n> Step 6/9 : RUN pip --no-cache-dir install http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-linux_x86_64.whl\r\n> \r\n>  ---> Running in ec97557752ad\r\n> \r\n> \u001b[91mtensorflow-1.head-cp35-cp35m-linux_x86_64.whl is not a supported wheel on this platform.\r\n> \u001b[0m\r\n> Removing intermediate container ec97557752ad\r\n> \r\n> The command '/bin/sh -c pip --no-cache-dir install http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-linux_x86_64.whl' returned a non-zero code: 1", "**Complementary test for docker image file introspection through docker hub**\r\n\r\nRemade a docker image up to the moment a pip install for tensorflow is required.\r\nThen noticed I missed pip3....\r\n\r\nRe-reading the above image file generated by parameterized_docker_build.sh, seems the script installs Python2 then install a Python3 version of Tensorflow which is obviously wrong.\r\n\r\nProblem seems to be with TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON3 which doesn't trigger properly the selection of a Python3 version of Tensorflow via a pip3 install.\r\n\r\nRe-executing complete sequence to confirm the above, please bear with me.", "Sorry if I've misunderstood your issue, but think this will solve your problems. I'm using Ubuntu 14.04 and the tensorflow/tensorflow docker image. The NV_GPU option allows GPU access of my two 980 cards to the docker container. I think the overall problem is caused by an unset ENV $USER variable.\r\n\r\n1. Launch the container using directory mapping to avoid docker-in-docker ulimit and process pid/socket issues:\r\n\r\nNV_GPU=0,1 nvidia-docker run --rm -it --ulimit nproc=1024:2048 -v /var/lib/docker:/var/lib/docker/ -v /var/run/docker.sock:/var/run/docker.sock -v /var/run/docker.pid:/var/run/docker-ssd.pid -v /var/run/docker.pid:/var/run/docker.pid --name jupyter_tensorflow -p 8888:8888 gcr.io/tensorflow/tensorflow:latest-devel-gpu /bin/bash\r\n\r\n2. Once at the bash prompt, install the following:\r\n\r\napt-get update &&\r\napt-get install -y apt-transport-https ca-certificates curl software-properties-common &&\r\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - &&\r\nadd-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" &&\r\napt-get install -y iputils-ping &&\r\napt-get install -y nano &&\r\napt-get update &&\r\napt-get install -y docker-ce &&\r\nwhich docker\r\n\r\n3. Set ENV variables:\r\n\r\nrm -rf /tmp/tmp*\r\ncd /tensorflow/tensorflow/tools/docker\r\n\r\nexport USER=tensorflow\r\n\r\nexport TF_DOCKER_BUILD_IS_DEVEL=NO\r\nexport TF_DOCKER_BUILD_TYPE=CPU\r\nexport TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON2\r\nexport NIGHTLY_VERSION=\"1.head\"\r\nexport TF_DOCKER_BUILD_IMAGE_NAME=tensorflow\r\nexport TF_DOCKER_BUILD_VERSION=latest\r\n\r\nexport TF_DOCKER_BUILD_CENTRAL_PIP=$(echo ${TF_DOCKER_BUILD_PYTHON_VERSION} | sed s^PYTHON2^http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=${TF_DOCKER_BUILD_PYTHON_VERSION},label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-${NIGHTLY_VERSION}-cp27-cp27mu-manylinux1_x86_64.whl^ | sed s^PYTHON3^http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-${NIGHTLY_VERSION}-cp35-cp35m-manylinux1_x86_64.whl^)\r\n\r\n4. Execute the parameterized_docker_build script to build a local image with the  above settings:\r\n\r\nchmod +x /tensorflow/tensorflow/tools/docker/parameterized_docker_build.sh\r\n/tensorflow/tensorflow/tools/docker/parameterized_docker_build.sh\r\n\r\nHope this helps.\r\n\r\n  \r\n ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Could not replicate issue with clean install, closing."]}, {"number": 13783, "title": "How can I do multples input Api c?", "body": "Hi guys, \r\n\r\nI am trying to make build my project in c++. The model use CTC and I don't have idea with re-writte code for main.cc. So,  It has input four values: `Tensor(\"the_input:0\", shape=(?, 128, 64, 1), dtype=float32) Tensor(\"the_labels:0\", shape=(?, 16), dtype=float32) Tensor(\"input_length:0\", shape=(?, 1), dtype=int64) Tensor(\"label_length:0\", shape=(?, 1), dtype=int64)`\r\n\r\nI am use [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc](url) as model base for new code, however, the input is erro. \r\n\r\n\r\nCan someone help me? \r\n\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- ** Linux Ubuntu 16.04**:\r\n- **TensorFlow installed by git alfer compiling**:\r\n- **TensorFlow 1.3.0**:\r\n- **Python version 3.5**: \r\n- **Bazel version 0.6.1**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU Titan X pascal 12GB**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nI a m triyng create a image ocr mobilie as tensorflow CTC.  \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13782, "title": "QuantConv2D", "body": "Hi, I've followed the documentation and TF Github code and couldn't find the relation between QuantizedConv2D to GEMMlowp. Does QuantizedConv2D use, implicitly, in a way the QuantizedMatMul(quantized_matmul_op.cc), under the hood, where the latter calls GemmlowpMultiply explicitly? Otherwise how the QuantizedConv2D using the great benefit of google GEMM HW platform specific implementation?\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13781, "title": "Fix #13731 by adding HistogramdFixedWidth in hidden_ops.txt and create the python wrapper", "body": "This fix fixes the build breaks in #13731. The issue was caused by the fact that API compatibility requires a default value for `nbins=100` while the test utility code in contrib need to pass a tensor (not through attribute).\r\n\r\nThis fix adds `HistogramdFixedWidth` in hidden_ops.txt so that `histogram_fixed_width` is hidden in `gen_math_ops.py`. Then `histogram_fixed_width` calls the hidden `gen_math_ops._histogram_fixed_width`.\r\n\r\nIn this way, both api compatibility and test utility code in contrib requires no changes.\r\n\r\nSee (comment) https://github.com/tensorflow/tensorflow/pull/13731#issuecomment-337186002 for reference.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "/cc @rmlarsen @gunan @vrv @hpucha , either this PR or the revert PR (#13776) could be taken to address the build breaks (and close the other PR). Sorry for the inconvenience that may cause.", "Let's try if this fixes the problem.\r\nJenkins, test this please."]}, {"number": 13780, "title": "I don't understand why I get these errors when i used dict () instead of { }. and how could I solve it?", "body": "This is the code. is very similar to tensorflow tutorial but uses estimator instead of classifier .                                  \r\n# 6.  Definition del model\r\ndef model_fn1(features, labels, mode=None, params=None, config=None):\r\n    # 6.1. Connect the first hidden layer to the # (features[\"x\"]) with relu activation\r\n    hidden = tf.layers.dense(features[\"x\"], 10,\r\n                             activation=tf.nn.relu)\r\n\r\n    # 6.2. Connect the second hidden layer to the first hidden with elu activation\r\n    hidden1 = tf.layers.dense(hidden, 10, activation=tf.nn.relu)\r\n\r\n    # 6.3. Connect output to the the second hidden layer without activation\r\n    out_y = tf.layers.dense(hidden1, 22)\r\n    out_y = tf.reshape(out_y, [-1, 22])\r\n\r\n    # 6.4. Provide an estimator spec for `ModeKeys.PREDICT`.\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode,\r\n            predictions={'': out_y})\r\n            #predictions = dict(out_y))\r\n\r\n    # 6.5. Calculate loss using mean squared error and another approach\r\n    loss = tf.losses.mean_squared_error(labels, out_y)\r\n\r\n    # 6.6. Training sub-graph\r\n    optimizer = tf.train.GradientDescentOptimizer(\r\n        learning_rate=0.01)\r\n    train_op = optimizer.minimize(\r\n        loss=loss, global_step=tf.train.get_global_step())\r\n\r\n    # 6.7 Calculate root mean squared error as additional eval metric\r\n    eval_metric_ops = {\r\n        \"rmse\": tf.metrics.root_mean_squared_error(\r\n            tf.cast(labels, tf.float64), out_y)\r\n    }\r\n\r\n    # 6.8. Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=mode,\r\n        loss=loss,\r\n        train_op=train_op,\r\n        eval_metric_ops=eval_metric_ops)\r\n\r\n\r\n# 7. Creation of an estimator\r\nestimator1 = tf.estimator.Estimator(model_fn=model_fn1, params=None,\r\n                                    model_dir='/home/jennydariska/targetDirectory/project_1/project1/test/')\r\n\r\n# 8. Running of our model\r\nwith tf.Session() as session:\r\n\r\n    input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": x_train}, y=y_train,\r\n                                                  shuffle=True, num_epochs=None)\r\n\r\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": x_train}, y=y_train,\r\n                                                        num_epochs=None, shuffle=False)\r\n\r\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": x_eval}, y=y_eval,\r\n                                                       num_epochs=1, shuffle=False)\r\n    # 8.1 Training of the estimator\r\n    estimator1.train(input_fn=input_fn, steps=5000)\r\n\r\n    # 8.2 Evaluation of how well our model did.\r\n    train_metrics = estimator1.evaluate(input_fn=train_input_fn, steps=500)\r\n    eval_metrics = estimator1.evaluate(input_fn=eval_input_fn)\r\n    print(\"train metrics: %r\" % train_metrics)\r\n    print(\"eval metrics: %r\" % eval_metrics)\r\n    print(\"Loss: %s\" % eval_metrics[\"loss\"])\r\n\r\n    # 8.3 Prediction for the news samples\r\n    predict_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\": new_samples}, num_epochs=1,\r\n        shuffle=False)\r\n\r\n    predictions = estimator1.predict(input_fn=predict_input_fn)\r\n    print(predictions)\r\n\r\n    for i in enumerate(predictions):\r\n        print(\"Prediction %s: \" % i )\r\n`                                                                                                                                                                             when i use {}, i get this error: \r\n<ERROR:tensorflow:==================================\r\nObject was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):\r\n<tf.Tensor 'report_uninitialized_variables_1/boolean_mask/Gather:0' shape=(?,) dtype=string>\r\nIf you want to mark it as used call its \"mark_used()\" method.\r\n>  \r\n when i use dict(), i get this \r\n<TypeError: 'Tensor' object is not iterable.>.I can not figure out where the problem lies\r\n\r\n> ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13779, "title": "MonitoredSession.close() doesn't stop a thread when using SyncReplicasOptimizer hook", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 3.5.2\r\n\r\n### Describe the problem\r\nIf using SyncReplicasOptimizer, the MonitoredSession is not able to stop a thread when calling session.close(). It will wait the whole \"stop_grace_period_secs\" and then close, reporting that a thread could not be stopped:  `INFO:tensorflow:Coordinator stopped with threads still running: Thread-5` (the thread number may vary).\r\n\r\nHere you can find the piece of code (reduced as much as possible) to reproduce it:\r\n\r\n### Source code\r\n```\r\nimport tensorflow as tf\r\nimport logging\r\nlogging.getLogger().setLevel(logging.INFO) #To see the message \"Coordinator stopped with threads still running\"\r\n\r\nopt = tf.train.RMSPropOptimizer(1.0)\r\nopt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=1, total_num_replicas=1)\r\nsync_hook = opt.make_session_run_hook(is_chief=True, num_tokens=0)\r\n\r\nglobal_step = tf.train.create_global_step() #global_step is necessary for SyncReplicasOptimizer\r\nvar = tf.Variable(0.0) #dummy variable and gradient\r\ngrad = tf.constant(1.0)\r\n\r\nopt.apply_gradients([(grad, var)], global_step = global_step)\r\n\r\nprint(\"CREATING SESSION\", flush=True)\r\nsess = tf.train.MonitoredSession(hooks = [sync_hook],\r\n                                 stop_grace_period_secs=10) #increasing this number will just make you wait more ;)\r\n\r\nprint(\"CLOSING... here's the problem!\", flush=True)\r\nsess.close()\r\n\r\nprint(\"DONE\", flush=True)\r\n```\r\n\r\n### Output\r\nIt hangs at sess.close() for \"stop_grace_period_secs\" (showing the message \"CLOSING...\") \r\n```\r\nINFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=1; total_num_replicas=1\r\nCREATING SESSION\r\nCLOSING... here's the problem!\r\nINFO:tensorflow:Coordinator stopped with threads still running: Thread-1\r\nDONE\r\n```\r\n\r\nSometimes, I additionally get the following traceback (when calling it from ipython I always get it):\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 254, in _run\r\n    coord.request_stop(e)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 211, in request_stop\r\n    six.reraise(*sys.exc_info())\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 238, in _run\r\n    enqueue_callable()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1235, in _single_operation_run\r\n    target_list_as_strings, status, None)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.CancelledError: TakeGrad operation was cancelled\r\n         [[Node: sync_replicas/AccumulatorTakeGradient = AccumulatorTakeGradient[_class=[\"loc:@sync_replicas/conditional_accumulator\"], dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](sync_replicas/conditional_accumulator, sync_replicas/AccumulatorTakeGradient/num_required)]]\r\n```\r\nAfter several runs of this same script I also got a segmentation fault, but only once.", "comments": ["The `SEGV` should not happen. Can you repro this?\r\n\r\nIt's a calling pattern that I'm not used to. @ispirmustafa could you take a look?", "Adding @isaprykin ", "This is most likely because you chose to say\r\n`sync_hook = opt.make_session_run_hook(is_chief=True, num_tokens=0)`\r\n\r\ninstead of\r\n`sync_hook = opt.make_session_run_hook(is_chief=True)`\r\n\r\nwith [the default value of num_tokens = -1](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/training/sync_replicas_optimizer.py#L432).\r\n\r\nIs the exception still raised for you when you revert num_tokens?  Would it work for you to keep the default as-is?", "Hi @isaprykin, thanks for your help.\r\n\r\nLeaving the default value of num_tokens = -1 still raises the exception, you can try it running the code provided (can you reproduce the error?).\r\n\r\nThis code is a simplification, of course, I actually use several processes which I want to synchronize with an effective barrier, and as far as I understood this means that num_tokens needs to be 0.\r\n\r\nI am not able to reproduce the SEGV, @drpngx. It just happened once.", "Hey sorry you still have an issue.  I made my response sound slightly confident exactly because the exception stopped for me.  And I did try running your code.  It didn't happen under `python`, but it did happen under `ipython`.  After I removed the `num_tokens=-1` bit, `ipython` stopped throwing the exception.\r\n\r\nIf you are confident, I'll try again later and try to make sure we have exactly same versions of everything (I did test on TF 1.3).", "Yes, the problem persists. Note that in a normal python shell it might not show the exception (because the process is terminating) but it will show `INFO:tensorflow:Coordinator stopped with threads still running: Thread-1`  (since the root logger level has been set to INFO).\r\n\r\nAdding, for instance, `time.sleep(5)` after closing the session will always show the exception using a normal python shell. \r\n\r\nIn my code, I made sure to finalize everything before closing the session, so all this is not really a problem for me right now, but I thought it was worth reporting it.", "The info message appearing is okay.\r\n\r\nWhat do you in your code to finalize and prevent the exception? ", "The last thing my script does is closing the session, so I actually don't prevent it.\r\n\r\n> The info message appearing is okay.\r\n\r\nIs it? Note that the problem is that sess.close() doesn't return because the thread could not be stopped (it does return thanks to the timeout).", "@mjunyentb   I'm also testing training with SyncReplicasOptimizer on MonitoredSession\r\nbut it hangs and never starts. ", "i also have this problem,using monitoredsession,when close will happen thread still running,hope the answer", "According to [this record](https://github.com/tensorflow/tensorflow/commit/6287fa848fb86994a503353ceb3bd663c3def4ac#diff-078d8aa8dc9179be5e99435311ade88f) that adds [ignore_live_threads=True](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/monitored_session.py#L1076), the stuck threads get killed later elsewhere.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing since @isaprykin provided a solution."]}, {"number": 13778, "title": "program could not run in docker but could run on local machine", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\nCentOS7 in local machine\r\nUbuntu 16.04.2 LTS in docker image\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\nv1.3.0-rc0-33-g6f0d70e 1.3.0-rc1\r\n- **Python version**: \r\n3.5.3\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n8.0 6.0\r\n- **GPU model and memory**:\r\nM40 24G\r\n\r\n\r\n\r\n### Describe the problem\r\nI could run my program on local machine, but can not run on docker(ubuntu image).\r\n\r\nthe stacktrace of the program in nvidia-docker:\r\n\r\n```\r\n2017-10-17 09:02:50.757982: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758004: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758020: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758052: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758073: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758089: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758134: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758154: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758178: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758209: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758229: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758246: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758289: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758323: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758337: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758464: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758500: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758532: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758562: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758593: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758624: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758851: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758876: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758891: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758922: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758942: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758957: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.758986: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.759006: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.759021: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.759065: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.759085: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.759100: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.759129: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.759149: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.759170: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.759203: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.759223: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n2017-10-17 09:02:50.759283: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\nTraceback (most recent call last):\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1306, in _run_fn\r\n    status, run_metadata)\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n\t [[Node: grads_0/gradients/AddN_2519/_461 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_289171_grads_0/gradients/AddN_2519\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/cli.py\", line 113, in <module>\r\n    tf.app.run()\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/cli.py\", line 110, in main\r\n    m(config)\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/main.py\", line 25, in main\r\n    _train(config)\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/main.py\", line 102, in _train\r\n    loss, summary, train_op = trainer.step(sess, batches, get_summary=get_summary)\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/trainer.py\", line 71, in step\r\n    loss, train_op = sess.run([self.loss, self.train_op], feed_dict=feed_dict)\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n\t [[Node: grads_0/gradients/AddN_2519/_461 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_289171_grads_0/gradients/AddN_2519\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op 'grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs', defined at:\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/cli.py\", line 113, in <module>\r\n    tf.app.run()\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/cli.py\", line 110, in main\r\n    m(config)\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/main.py\", line 25, in main\r\n    _train(config)\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/main.py\", line 87, in _train\r\n    trainer = MultiGPUTrainer(config, models)\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/trainer.py\", line 52, in __init__\r\n    grads = self.opt.compute_gradients(loss, var_list=self.var_list)\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 386, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 542, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 348, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 542, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py\", line 689, in _AddGrad\r\n    rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 395, in _broadcast_gradient_args\r\n    name=name)\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'model_0/main/logits2/exp_mask', defined at:\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/cli.py\", line 113, in <module>\r\n    tf.app.run()\r\n[elided 2 identical lines from previous traceback]\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/main.py\", line 25, in main\r\n    _train(config)\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/main.py\", line 84, in _train\r\n    models = get_multi_gpu_models_new(config)\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/model_fusion.py\", line 23, in get_multi_gpu_models_new\r\n    model = Model(config, scope, rep=gpu_idx == 0)\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/model_fusion.py\", line 75, in __init__\r\n    self._build_forward()\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/basic/model_fusion.py\", line 223, in _build_forward\r\n    is_train=self.is_train, func=config.answer_func, scope='logits2')\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/my/tensorflow/nn.py\", line 107, in get_logits\r\n    is_train=is_train)\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/my/tensorflow/nn.py\", line 83, in linear_logits\r\n    logits = exp_mask(logits, mask)\r\n  File \"/nfsdata/guotong_data/bi-att-flow-dev/my/tensorflow/general.py\", line 138, in exp_mask\r\n    return tf.add(val, (1 - tf.cast(mask, 'float')) * VERY_NEGATIVE_NUMBER, name=name)\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 82, in add\r\n    result = _op_def_lib.apply_op(\"Add\", x=x, y=y, name=name)\r\n  File \"/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n\r\nInvalidArgumentError (see above for traceback): Incompatible shapes: [32,1,400] vs. [32,1,213]\r\n\t [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]\r\n\t [[Node: grads_0/gradients/AddN_2519/_461 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_289171_grads_0/gradients/AddN_2519\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n\r\nProcess finished with exit code 1\r\n```\r\nthe 213 of `[32,1,213]` in stacktrace may change to 160 or other number as I change the GPU type (M40-11G , P100-15G in nvidia-docker)\r\n\r\nIn local machine it could run without the stacktrace above.\r\n", "comments": ["The main reason may be the lines below , which is added by me.\r\n```\r\nq_j_400 = []\r\nfor j in range(400):\r\n    q_j_400.append(tf.squeeze(tf.matmul(tf.transpose(u,[0,2,1]),tf.expand_dims(tf.nn.softmax(u_logits[j]),-1)),[2])) # each element [32,200,30],[32,30,1]\r\n```\r\nNote that the stacktrace doesn't point to the lines above.", "Sorry, It is my fault.\r\nSomething wrong with my code.\r\nI haven't copy the whole folder from local machine to docker image.\r\nI only copy one edited python file.\r\nAlthough I don't know the exact reason.\r\nBut it is not the problem with the local machine and docker.\r\nIt does **could run** in docker if I copy the whole folder from local machine to docker image.\r\n\r\n"]}]