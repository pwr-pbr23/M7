[{"number": 36713, "title": "Remove unnecessary \"T\" before \"tf.Tensor\" in all_reduce.py", "body": "I think these are not necessary unless I am missing something obvious here.", "comments": []}, {"number": 36712, "title": "KeyError of identity node, when using Skip Connection and RandomDrop together", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- **Python version**: 3.7.6 Anaconda\r\n- **CUDA/cuDNN version**: 10.2 / 7.6.5\r\n- **GPU model and memory**: RTX2070 mobile, 8GB\r\n\r\n### Minimal example:\r\n```\r\nimport tensorflow as tf\r\n\r\nclass OpOrSkip(tf.keras.layers.Layer):\r\n    def __init__(self, op):\r\n        super().__init__()\r\n        self.op = op\r\n        \r\n    def call(self, x):\r\n        rnd = tf.random.uniform(())\r\n        if rnd < 0.5:\r\n            return self.op(x)\r\n        else:\r\n            return x\r\n\r\ndef skip_conv(s):\r\n    x = tf.keras.layers.Conv2D(3, 3, padding='same')(s)\r\n    return x + s #tf.identity(s, name='s')\r\n\r\ndef func_as_model(func, shape):\r\n    inp = tf.keras.Input(shape)\r\n    out = func(inp)\r\n    return tf.keras.Model(inputs=inp, outputs=out)\r\n\r\ninputs = tf.keras.Input((32, 32, 3))\r\nSkipConv = func_as_model(skip_conv, [32, 32, 3])\r\n\r\nx = OpOrSkip(SkipConv)(inputs)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=x)\r\n```\r\n\r\n### Describe the problem\r\nI am building a resnet network, so a network with skip connections, but also with RandomDrop, that is dropping the whole residual with some probability. However, I can't do this in tensorflow 2, wheras it was possible in tensorflow 1. The error occurs only when both skip connection and RandomDrop are preset at once.\r\n\r\nMy observation is tensorflow confuses shortcut in `skip_conv` with another node. When I try to name this shortcut using `tf.identity`, I get yet another error. However I expect it to work without explicitly naming the identity node.\r\n\r\nThe alternative for `s` is commented in the code (`tf.identity(s, name='s')`). Error in the first version occurs when only defining the model. When named the identity node, the error occurs only when using model in a graph (in `@tf.function` decorated function).\r\n\r\n### Source code / logs\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-9-c655ea21defb> in <module>()\r\n     27 SkipConv = func_as_model(skip_conv, [32, 32, 3])\r\n     28 \r\n---> 29 x = OpOrSkip(SkipConv)(inputs)\r\n     30 model = tf.keras.Model(inputs=inputs, outputs=x)\r\n\r\nC:\\Users\\gaha\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    771                     not base_layer_utils.is_in_eager_or_tf_function()):\r\n    772                   with auto_control_deps.AutomaticControlDependencies() as acd:\r\n--> 773                     outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    774                     # Wrap Tensors in `outputs` in `tf.identity` to avoid\r\n    775                     # circular dependencies.\r\n\r\nC:\\Users\\gaha\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py in wrapper(*args, **kwargs)\r\n    235       except Exception as e:  # pylint:disable=broad-except\r\n    236         if hasattr(e, 'ag_error_metadata'):\r\n--> 237           raise e.ag_error_metadata.to_exception(e)\r\n    238         else:\r\n    239           raise\r\n\r\nKeyError: in converted code:\r\n\r\n    <ipython-input-1-56c161fa68ed>:12 call  *\r\n        if rnd < 0.5:\r\n    C:\\Users\\gaha\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\autograph\\operators\\control_flow.py:918 if_stmt\r\n        basic_symbol_names, composite_symbol_names)\r\n    C:\\Users\\gaha\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\autograph\\operators\\control_flow.py:956 tf_if_stmt\r\n        error_checking_orelse)\r\n    C:\\Users\\gaha\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py:507 new_func\r\n        return func(*args, **kwargs)\r\n    C:\\Users\\gaha\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\control_flow_ops.py:1174 cond\r\n        return cond_v2.cond_v2(pred, true_fn, false_fn, name)\r\n    C:\\Users\\gaha\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\cond_v2.py:83 cond_v2\r\n        op_return_value=pred)\r\n    C:\\Users\\gaha\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py:1019 func_graph_from_py_func\r\n        func_graph.variables = variables\r\n    C:\\Users\\gaha\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\auto_control_deps.py:324 __exit__\r\n        for inp in op.inputs:\r\n    C:\\Users\\gaha\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:2163 inputs\r\n        c_api.GetOperationInputs(self._c_op)))\r\n    C:\\Users\\gaha\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:3696 _get_tensor_by_tf_output\r\n        op = self._get_operation_by_tf_operation(tf_output.oper)\r\n    C:\\Users\\gaha\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:3660 _get_operation_by_tf_operation\r\n        return self._get_operation_by_name_unsafe(op_name)\r\n    C:\\Users\\gaha\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:3656 _get_operation_by_name_unsafe\r\n        return self._nodes_by_name[name]\r\n\r\n    KeyError: 'input_17'\r\n```\r\n", "comments": ["I am still not able to create such network in functional manner. Creating a class is the only option for now.", "@gahaalt Sorry for the late response. Is this still an issue for you? \r\nI couldn't reproduce the issue with recent `tf-nightly`. Looks like this was resolved in `tf-nightly`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/2cea2d31e7486a24a8a1a66496c89378/36712.ipynb)\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Yes, it looks like it works now in tf 2.5. Thank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36712\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36712\">No</a>\n"]}, {"number": 36711, "title": "Error TFLite converting tensorflow graph generated with config file with experimental_new_converter = True", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (or github SHA if from source): TF Nightly 2.2.0-dev20200212\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([inception_func])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.target_spec.supported_types = [tf.int8, tf.uint8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-02-12 14:44:53.272916: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-02-12 14:44:53.293807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: TITAN Xp computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 30 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 510.07GiB/s\r\n2020-02-12 14:44:53.294407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 1 with properties: \r\npciBusID: 0000:02:00.0 name: TITAN Xp computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 30 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 510.07GiB/s\r\n2020-02-12 14:44:53.294542: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-02-12 14:44:53.295848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-12 14:44:53.297069: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-02-12 14:44:53.297272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-02-12 14:44:53.298655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-02-12 14:44:53.299389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-02-12 14:44:53.299452: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n2020-02-12 14:44:53.299461: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1595] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-02-12 14:44:53.299683: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-02-12 14:44:53.327596: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3597955000 Hz\r\n2020-02-12 14:44:53.328433: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55838544dd10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-02-12 14:44:53.328472: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-02-12 14:44:53.331414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-12 14:44:53.331444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      \r\n<generator object image_files_gen at 0x7febddc4ed50>\r\n2020-02-12 14:44:54.788049: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\r\n2020-02-12 14:44:54.788083: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1363] Profiler found 2 GPUs\r\n2020-02-12 14:44:54.788196: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcupti.so.10.1'; dlerror: libcupti.so.10.1: cannot open shared object file: No such file or directory\r\n2020-02-12 14:44:54.788207: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1408] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2020-02-12 14:44:54.788215: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1447] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI could not be loaded or symbol could not be found.\r\n[<tf.Tensor: shape=(1, 100, 4), dtype=float32, numpy=\r\narray([[[0.4523084 , 0.19130976, 0.48489496, 0.25743166],\r\n        [0.44898474, 0.19012845, 0.48815066, 0.2607174 ],\r\n        [0.6068166 , 0.04942099, 0.63641137, 0.11622739],\r\n        [0.45264566, 0.1908576 , 0.48534685, 0.2560804 ],\r\n        [0.45322064, 0.19343509, 0.48580036, 0.25778043],\r\n        [0.50848585, 0.576692  , 0.53330845, 0.6251781 ],\r\n        [0.45322064, 0.19343509, 0.48580036, 0.25778043],\r\n        [0.4494503 , 0.17628026, 0.48778558, 0.2566436 ],\r\n        [0.527445  , 0.33889085, 0.55464226, 0.39898926],\r\n        [0.45296955, 0.1903181 , 0.48529047, 0.25696498],\r\n        [0.5204598 , 0.32256034, 0.5480597 , 0.39266023],\r\n        [0.63632804, 0.6682412 , 0.6719902 , 0.7374546 ],\r\n        [0.45650837, 0.19338551, 0.4927548 , 0.277421  ],\r\n        [0.44446883, 0.1737675 , 0.4827005 , 0.25779355],\r\n        [0.72652483, 0.2473509 , 0.7562058 , 0.30082873],\r\n        [0.9542563 , 0.843302  , 1.        , 0.96409976],\r\n        [0.11040416, 0.00447438, 0.13210008, 0.03558907],\r\n        [0.45264566, 0.1908576 , 0.48534685, 0.2560804 ],\r\n        [0.4523084 , 0.19130976, 0.48489496, 0.25743166],\r\n        [0.44075978, 0.14530185, 0.48041832, 0.25093415],\r\n        [0.15409914, 0.94093376, 0.1896761 , 1.        ],\r\n        [0.45264566, 0.1908576 , 0.48534685, 0.2560804 ],\r\n        [0.6068166 , 0.04942099, 0.63641137, 0.11622739],\r\n        [0.50790685, 0.7605634 , 0.53323144, 0.82372105],\r\n        [0.10795376, 0.0060998 , 0.13245544, 0.04638027],\r\n        [0.4466028 , 0.18642351, 0.4799665 , 0.24573368],\r\n        [0.52835655, 0.28773227, 0.5562326 , 0.34618238],\r\n        [0.45650837, 0.19338551, 0.4927548 , 0.277421  ],\r\n        [0.3502379 , 0.44612858, 0.38212806, 0.50805885],\r\n        [0.64212465, 0.39890158, 0.6739552 , 0.49036568],\r\n        [0.45171025, 0.19052231, 0.4847984 , 0.25762528],\r\n        [0.12097565, 0.00399387, 0.1428402 , 0.03510833],\r\n        [0.4466028 , 0.18642351, 0.4799665 , 0.24573368],\r\n        [0.17729592, 0.19604556, 0.20469955, 0.26705974],\r\n        [0.45841685, 0.14860368, 0.5037309 , 0.26684842],\r\n        [0.11995812, 0.00256568, 0.14446093, 0.05109425],\r\n        [0.5166499 , 0.2938283 , 0.5506055 , 0.39516765],\r\n        [0.5196249 , 0.32896924, 0.54816073, 0.38678855],\r\n        [0.31064248, 0.7969677 , 0.35238957, 0.8906676 ],\r\n        [0.51701736, 0.30636752, 0.55060136, 0.40481085],\r\n        [0.22814384, 0.32451725, 0.259156  , 0.41317737],\r\n        [0.50587946, 0.30761832, 0.53356034, 0.37745112],\r\n        [0.45861158, 0.2037143 , 0.4902828 , 0.2635113 ],\r\n        [0.4471084 , 0.184802  , 0.47876382, 0.25051934],\r\n        [0.43582347, 0.19232129, 0.4744096 , 0.26392525],\r\n        [0.45296955, 0.1903181 , 0.48529047, 0.25696498],\r\n        [0.01486549, 0.        , 0.07320484, 0.08066794],\r\n        [0.        , 0.09366378, 0.21820027, 0.9756663 ],\r\n        [0.54991406, 0.5637065 , 0.691194  , 0.96959376],\r\n        [0.59876406, 0.04728752, 0.6286751 , 0.11585537],\r\n        [0.53492194, 0.08620362, 0.5909355 , 0.24983023],\r\n        [0.70816135, 0.00839768, 0.74530756, 0.06555284],\r\n        [0.686458  , 0.944192  , 0.75062335, 1.        ],\r\n        [0.4471084 , 0.184802  , 0.47876382, 0.25051934],\r\n        [0.13161138, 0.45191422, 0.15738365, 0.5095992 ],\r\n        [0.4885881 , 0.80888736, 0.54836   , 0.978459  ],\r\n        [0.13416171, 0.00377405, 0.1561068 , 0.03560173],\r\n        [0.45158228, 0.19188425, 0.48981753, 0.2612535 ],\r\n        [0.75087017, 0.00396507, 0.78542954, 0.06046526],\r\n        [0.637136  , 0.66719157, 0.66992843, 0.7352523 ],\r\n        [0.5243317 , 0.29501545, 0.5598284 , 0.39330977],\r\n        [0.44330403, 0.1994324 , 0.48607805, 0.27255926],\r\n        [0.4471084 , 0.184802  , 0.47876382, 0.25051934],\r\n        [0.96380365, 0.84537166, 1.        , 0.9437652 ],\r\n        [0.44857678, 0.18810245, 0.48808834, 0.25654176],\r\n        [0.5276355 , 0.2980371 , 0.55767304, 0.364582  ],\r\n        [0.51808655, 0.3211314 , 0.5516435 , 0.41390526],\r\n        [0.44262165, 0.2258066 , 0.49634498, 0.2604202 ],\r\n        [0.51671165, 0.23575601, 0.5696979 , 0.38195243],\r\n        [0.5159677 , 0.2698248 , 0.56696165, 0.4135533 ],\r\n        [0.16784409, 0.18931931, 0.19625813, 0.26827484],\r\n        [0.44924247, 0.11680316, 0.48714614, 0.24786523],\r\n        [0.6068166 , 0.04942099, 0.63641137, 0.11622739],\r\n        [0.5007901 , 0.58252937, 0.527007  , 0.63281745],\r\n        [0.4662144 , 0.19344129, 0.5073866 , 0.26448467],\r\n        [0.19347599, 0.5412288 , 0.21763065, 0.5982214 ],\r\n        [0.44075978, 0.14530185, 0.48041832, 0.25093415],\r\n        [0.44787258, 0.2035003 , 0.48078   , 0.26272708],\r\n        [0.36003056, 0.43164626, 0.39625302, 0.5073702 ],\r\n        [0.85064745, 0.2225066 , 0.8861109 , 0.31016666],\r\n        [0.5810017 , 0.11662646, 0.6081669 , 0.17228721],\r\n        [0.45315146, 0.17665324, 0.48381698, 0.2442483 ],\r\n        [0.63825023, 0.        , 1.        , 1.        ],\r\n        [0.27971706, 0.21467996, 0.33935127, 0.41237378],\r\n        [0.20111686, 0.9416385 , 0.23956841, 1.        ],\r\n        [0.44525808, 0.17430884, 0.47413707, 0.23905429],\r\n        [0.44446883, 0.1737675 , 0.4827005 , 0.25779355],\r\n        [0.44451416, 0.1914759 , 0.4826401 , 0.27257037],\r\n        [0.18308899, 0.00234124, 0.20454633, 0.03471272],\r\n        [0.56354165, 0.11042208, 0.5989641 , 0.18208715],\r\n        [0.8662053 , 0.26750824, 1.        , 0.6562464 ],\r\n        [0.45214534, 0.21824962, 0.48541468, 0.2723656 ],\r\n        [0.5272091 , 0.18768561, 0.5903445 , 0.34028006],\r\n        [0.1258346 , 0.9409971 , 0.16100453, 1.        ],\r\n        [0.44988436, 0.19079384, 0.48194766, 0.25922737],\r\n        [0.5381035 , 0.7981993 , 0.5771181 , 0.885656  ],\r\n        [0.51808256, 0.29529324, 0.54818624, 0.36529002],\r\n        [0.44561428, 0.12746832, 0.48472154, 0.25478205],\r\n        [0.55583   , 0.39829937, 0.6881298 , 0.8807714 ],\r\n        [0.12827086, 0.        , 0.15974712, 0.02394507]]], dtype=float32)>, <tf.Tensor: shape=(1, 100), dtype=float32, numpy=\r\narray([[0.92555547, 0.04032728, 0.04001778, 0.03919104, 0.0389992 ,\r\n        0.03491211, 0.03117302, 0.02736771, 0.02710325, 0.02648216,\r\n        0.02614886, 0.02420676, 0.02387115, 0.02345794, 0.02287492,\r\n        0.02275831, 0.02204138, 0.02131733, 0.02058503, 0.02054   ,\r\n        0.0202291 , 0.0200913 , 0.01930887, 0.01929054, 0.0186362 ,\r\n        0.01838633, 0.01820371, 0.01814541, 0.0180732 , 0.01789805,\r\n        0.01755115, 0.01670685, 0.01663533, 0.01650512, 0.01644138,\r\n        0.01569021, 0.01568368, 0.01562142, 0.0155603 , 0.01543599,\r\n        0.01460987, 0.01423776, 0.0142113 , 0.01356006, 0.01343337,\r\n        0.01308289, 0.01273718, 0.01269153, 0.01266637, 0.01263455,\r\n        0.01250893, 0.01249737, 0.01232877, 0.01232168, 0.01225841,\r\n        0.01223144, 0.01213449, 0.01195824, 0.01176769, 0.01173973,\r\n        0.01167473, 0.01161668, 0.01140118, 0.0113658 , 0.01134393,\r\n        0.01127735, 0.01115933, 0.01114836, 0.01104337, 0.01092488,\r\n        0.0109069 , 0.01084158, 0.01068258, 0.01066345, 0.0105837 ,\r\n        0.01057971, 0.01048639, 0.01046959, 0.01027715, 0.01023829,\r\n        0.01021823, 0.01019359, 0.01015976, 0.01008341, 0.01005188,\r\n        0.00993064, 0.00975928, 0.00972691, 0.00967205, 0.00965327,\r\n        0.00961038, 0.00959793, 0.00955415, 0.00954556, 0.00948521,\r\n        0.0093832 , 0.00936151, 0.00935119, 0.00931564, 0.00920397]],\r\n      dtype=float32)>, <tf.Tensor: shape=(1, 100), dtype=float32, numpy=\r\narray([[15., 13.,  4.,  3., 12., 15., 10., 16.,  8.,  9., 10., 13., 15.,\r\n        15., 15.,  8., 14.,  1.,  2., 16., 12., 14., 10., 10.,  4., 12.,\r\n        13., 13.,  5.,  8.,  8., 14., 13.,  4., 15.,  4., 16.,  8.,  5.,\r\n        10.,  3., 10., 12.,  2., 12.,  4., 11., 16.,  3.,  4., 10., 13.,\r\n         5., 10.,  4., 10., 14., 11., 12.,  8., 16., 15.,  1.,  7.,  6.,\r\n        13., 16., 14., 10., 10.,  4., 16., 16., 15., 15.,  4.,  1., 12.,\r\n        12., 10.,  4., 10., 16.,  4., 12., 10.,  3., 16., 14.,  8.,  1.,\r\n        15.,  3.,  4.,  7.,  8., 10., 10.,  3., 14.]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([100.], dtype=float32)>]\r\n2020-02-12 14:44:56.654298: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1430] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2020-02-12 14:44:56.654980: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:203]  GpuTracer has collected 0 callback api events and 0 activity events.\r\nCONVERTING\r\n\r\n\r\n\r\n\r\n2020-02-12 14:44:56.953373: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\r\n2020-02-12 14:44:56.953474: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-02-12 14:44:57.201293: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55839683e470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-02-12 14:44:57.201338: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1\r\n2020-02-12 14:44:57.201351: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): TITAN Xp, Compute Capability 6.1\r\n2020-02-12 14:44:57.205900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: TITAN Xp computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 30 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 510.07GiB/s\r\n2020-02-12 14:44:57.207642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 1 with properties: \r\npciBusID: 0000:02:00.0 name: TITAN Xp computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 30 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 510.07GiB/s\r\n2020-02-12 14:44:57.207835: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-02-12 14:44:57.207873: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-12 14:44:57.207900: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-02-12 14:44:57.207925: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-02-12 14:44:57.207949: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-02-12 14:44:57.207973: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-02-12 14:44:57.208090: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n2020-02-12 14:44:57.208110: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1595] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-02-12 14:44:57.208187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-12 14:44:57.208206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 1 \r\n2020-02-12 14:44:57.208221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N Y \r\n2020-02-12 14:44:57.208234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 1:   Y N \r\n2020-02-12 14:44:57.433135: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-02-12 14:44:57.433170: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 2451 nodes (0), 2754 edges (0), time = 28.091ms.\r\n2020-02-12 14:44:57.433174: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 2451 nodes (0), 2754 edges (0), time = 30.465ms.\r\n2020-02-12 14:44:57.433178: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_1_false_5569\r\n2020-02-12 14:44:57.433182: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 20 nodes (0), 22 edges (0), time = 0.312ms.\r\n2020-02-12 14:44:57.433185: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 20 nodes (0), 22 edges (0), time = 0.299ms.\r\n2020-02-12 14:44:57.433189: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_2_true_5627\r\n2020-02-12 14:44:57.433192: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-02-12 14:44:57.433195: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-02-12 14:44:57.433198: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_3_true_5686\r\n2020-02-12 14:44:57.433204: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-02-12 14:44:57.433207: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-02-12 14:44:57.433210: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_true_5533\r\n2020-02-12 14:44:57.433214: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-02-12 14:44:57.433217: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-02-12 14:44:57.433224: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_false_5652\r\n2020-02-12 14:44:57.433228: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-02-12 14:44:57.433231: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-02-12 14:44:57.433236: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_2_false_5628\r\n2020-02-12 14:44:57.433242: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 20 nodes (0), 22 edges (0), time = 0.278ms.\r\n2020-02-12 14:44:57.433246: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 20 nodes (0), 22 edges (0), time = 0.295ms.\r\n2020-02-12 14:44:57.433251: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_true_5592\r\n2020-02-12 14:44:57.433256: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-02-12 14:44:57.433260: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-02-12 14:44:57.433264: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_1_true_5568\r\n2020-02-12 14:44:57.433269: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-02-12 14:44:57.433273: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-02-12 14:44:57.433277: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: Postprocessor_BatchMultiClassNonMaxSuppression_map_while_cond_4116\r\n2020-02-12 14:44:57.433282: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-02-12 14:44:57.433286: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-02-12 14:44:57.433290: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: Preprocessor_map_while_body_20\r\n2020-02-12 14:44:57.433295: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-02-12 14:44:57.433299: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-02-12 14:44:57.433303: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_false_5534\r\n2020-02-12 14:44:57.433308: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-02-12 14:44:57.433312: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-02-12 14:44:57.433316: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_true_5651\r\n2020-02-12 14:44:57.433321: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-02-12 14:44:57.433325: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-02-12 14:44:57.433329: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_3_false_5687\r\n2020-02-12 14:44:57.433334: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 20 nodes (0), 22 edges (0), time = 0.284ms.\r\n2020-02-12 14:44:57.433339: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 20 nodes (0), 22 edges (0), time = 0.301ms.\r\n2020-02-12 14:44:57.433343: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_false_5711\r\n2020-02-12 14:44:57.433347: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n2020-02-12 14:44:57.433352: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-02-12 14:44:57.433356: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_true_5710\r\n2020-02-12 14:44:57.433361: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-02-12 14:44:57.433365: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-02-12 14:44:57.433369: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: Preprocessor_map_while_cond_19\r\n2020-02-12 14:44:57.433374: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n2020-02-12 14:44:57.433378: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-02-12 14:44:57.433382: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_false_5510\r\n2020-02-12 14:44:57.433387: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 20 nodes (0), 22 edges (0), time = 0.793ms.\r\n2020-02-12 14:44:57.433391: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 20 nodes (0), 22 edges (0), time = 0.877ms.\r\n2020-02-12 14:44:57.433395: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: Postprocessor_BatchMultiClassNonMaxSuppression_map_while_body_4117\r\n2020-02-12 14:44:57.433400: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 1290 nodes (0), 1804 edges (0), time = 8.567ms.\r\n2020-02-12 14:44:57.433405: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 1290 nodes (0), 1804 edges (0), time = 10.381ms.\r\n2020-02-12 14:44:57.433409: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_true_5509\r\n2020-02-12 14:44:57.433414: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-02-12 14:44:57.433418: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-02-12 14:44:57.433423: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_false_5593\r\n2020-02-12 14:44:57.433427: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-02-12 14:44:57.433432: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-02-12 14:44:59.331976: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\r\n2020-02-12 14:44:59.332072: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-02-12 14:44:59.333207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: TITAN Xp computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 30 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 510.07GiB/s\r\n2020-02-12 14:44:59.333751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 1 with properties: \r\npciBusID: 0000:02:00.0 name: TITAN Xp computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 30 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 510.07GiB/s\r\n2020-02-12 14:44:59.333842: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-02-12 14:44:59.333858: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-12 14:44:59.333868: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-02-12 14:44:59.333878: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-02-12 14:44:59.333887: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-02-12 14:44:59.333896: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-02-12 14:44:59.333939: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n2020-02-12 14:44:59.333946: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1595] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-02-12 14:44:59.333988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-12 14:44:59.333995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 1 \r\n2020-02-12 14:44:59.334002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N Y \r\n2020-02-12 14:44:59.334006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 1:   Y N \r\n2020-02-12 14:44:59.918734: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-02-12 14:44:59.918767: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 937 nodes (-1514), 1045 edges (-1709), time = 270.114ms.\r\n2020-02-12 14:44:59.918772: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 937 nodes (0), 1045 edges (0), time = 79.506ms.\r\n2020-02-12 14:44:59.918775: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_true_5710_frozen\r\n2020-02-12 14:44:59.918779: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 10 nodes (0), 8 edges (0), time = 0.278ms.\r\n2020-02-12 14:44:59.918782: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 10 nodes (0), 8 edges (0), time = 0.171ms.\r\n2020-02-12 14:44:59.918785: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_false_5711_frozen\r\n2020-02-12 14:44:59.918789: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 8 nodes (0), 5 edges (0), time = 0.17ms.\r\n2020-02-12 14:44:59.918793: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 8 nodes (0), 5 edges (0), time = 0.119ms.\r\n2020-02-12 14:44:59.918796: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_1_true_5568_frozen\r\n2020-02-12 14:44:59.918801: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 5 nodes (-3), 4 edges (-3), time = 0.285ms.\r\n2020-02-12 14:44:59.918805: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 5 nodes (0), 4 edges (0), time = 0.093ms.\r\n2020-02-12 14:44:59.918808: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: Postprocessor_BatchMultiClassNonMaxSuppression_map_while_cond_4116_frozen\r\n2020-02-12 14:44:59.918812: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 18 nodes (0), 7 edges (0), time = 0.253ms.\r\n2020-02-12 14:44:59.918819: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 18 nodes (0), 7 edges (0), time = 0.199ms.\r\n2020-02-12 14:44:59.918824: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_true_5592_frozen\r\n2020-02-12 14:44:59.918829: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 10 nodes (0), 8 edges (0), time = 0.248ms.\r\n2020-02-12 14:44:59.918833: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 10 nodes (0), 8 edges (0), time = 0.172ms.\r\n2020-02-12 14:44:59.918838: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_true_5533_frozen\r\n2020-02-12 14:44:59.918846: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 10 nodes (0), 8 edges (0), time = 0.22ms.\r\n2020-02-12 14:44:59.918851: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 10 nodes (0), 8 edges (0), time = 0.198ms.\r\n2020-02-12 14:44:59.918855: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_false_5510_frozen\r\n2020-02-12 14:44:59.918860: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 18 nodes (-2), 20 edges (-2), time = 0.72ms.\r\n2020-02-12 14:44:59.918865: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 18 nodes (0), 20 edges (0), time = 0.366ms.\r\n2020-02-12 14:44:59.918869: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: Preprocessor_map_while_cond_19_frozen\r\n2020-02-12 14:44:59.918873: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 11 nodes (0), 7 edges (0), time = 0.222ms.\r\n2020-02-12 14:44:59.918878: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 11 nodes (0), 7 edges (0), time = 0.141ms.\r\n2020-02-12 14:44:59.918882: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_2_false_5628_frozen\r\n2020-02-12 14:44:59.918886: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 18 nodes (-2), 20 edges (-2), time = 0.716ms.\r\n2020-02-12 14:44:59.918891: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 18 nodes (0), 20 edges (0), time = 0.369ms.\r\n2020-02-12 14:44:59.918895: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_true_5509_frozen\r\n2020-02-12 14:44:59.918899: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 5 nodes (-3), 4 edges (-3), time = 0.278ms.\r\n2020-02-12 14:44:59.918904: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 5 nodes (0), 4 edges (0), time = 0.096ms.\r\n2020-02-12 14:44:59.918908: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_false_5593_frozen\r\n2020-02-12 14:44:59.918913: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 8 nodes (0), 5 edges (0), time = 0.2ms.\r\n2020-02-12 14:44:59.918917: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 8 nodes (0), 5 edges (0), time = 0.125ms.\r\n2020-02-12 14:44:59.918921: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: Postprocessor_BatchMultiClassNonMaxSuppression_map_while_body_4117_frozen\r\n2020-02-12 14:44:59.918926: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 1256 nodes (-34), 1804 edges (0), time = 35.923ms.\r\n2020-02-12 14:44:59.918930: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 1256 nodes (0), 1804 edges (0), time = 22.273ms.\r\n2020-02-12 14:44:59.918935: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_false_5652_frozen\r\n2020-02-12 14:44:59.918939: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 8 nodes (0), 5 edges (0), time = 0.212ms.\r\n2020-02-12 14:44:59.918943: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 8 nodes (0), 5 edges (0), time = 0.129ms.\r\n2020-02-12 14:44:59.918948: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: Preprocessor_map_while_body_20_frozen\r\n2020-02-12 14:44:59.918952: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 28 nodes (-9), 25 edges (-9), time = 1.367ms.\r\n2020-02-12 14:44:59.918956: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 28 nodes (0), 25 edges (0), time = 0.432ms.\r\n2020-02-12 14:44:59.918961: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_3_true_5686_frozen\r\n2020-02-12 14:44:59.918965: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 5 nodes (-3), 4 edges (-3), time = 0.29ms.\r\n2020-02-12 14:44:59.918969: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 5 nodes (0), 4 edges (0), time = 0.099ms.\r\n2020-02-12 14:44:59.918974: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_3_false_5687_frozen\r\n2020-02-12 14:44:59.918978: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 18 nodes (-2), 20 edges (-2), time = 0.801ms.\r\n2020-02-12 14:44:59.918982: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 18 nodes (0), 20 edges (0), time = 0.382ms.\r\n2020-02-12 14:44:59.918986: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_1_false_5569_frozen\r\n2020-02-12 14:44:59.918989: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 18 nodes (-2), 20 edges (-2), time = 0.745ms.\r\n2020-02-12 14:44:59.918994: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 18 nodes (0), 20 edges (0), time = 0.384ms.\r\n2020-02-12 14:44:59.918998: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_true_5651_frozen\r\n2020-02-12 14:44:59.919002: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 10 nodes (0), 8 edges (0), time = 0.228ms.\r\n2020-02-12 14:44:59.919006: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 10 nodes (0), 8 edges (0), time = 0.203ms.\r\n2020-02-12 14:44:59.919011: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: cond_false_5534_frozen\r\n2020-02-12 14:44:59.919015: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 8 nodes (0), 5 edges (0), time = 0.174ms.\r\n2020-02-12 14:44:59.919019: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 8 nodes (0), 5 edges (0), time = 0.126ms.\r\n2020-02-12 14:44:59.919024: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: PadOrClipBoxList_cond_2_true_5627_frozen\r\n2020-02-12 14:44:59.919028: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 5 nodes (-3), 4 edges (-3), time = 0.298ms.\r\n2020-02-12 14:44:59.919032: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 5 nodes (0), 4 edges (0), time = 0.101ms.\r\nTraceback (most recent call last):\r\n  File \"convert_to_tflite2.py\", line 105, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/eric/.anaconda3/envs/tf_nightly_gpu/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 513, in convert\r\n    **converter_kwargs)\r\n  File \"/home/eric/.anaconda3/envs/tf_nightly_gpu/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 496, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/eric/.anaconda3/envs/tf_nightly_gpu/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 227, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-02-12 14:45:01.509402: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:108] Ignored output_format.\r\n2020-02-12 14:45:01.509437: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:111] Ignored drop_control_dependency.\r\n2020-02-12 14:45:01.712175: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-02-12 14:45:01.739469: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3597955000 Hz\r\n2020-02-12 14:45:01.740229: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55dc06f27020 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-02-12 14:45:01.740266: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-02-12 14:45:01.744300: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-02-12 14:45:01.935539: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55dc09be74d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-02-12 14:45:01.935576: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1\r\n2020-02-12 14:45:01.935585: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): TITAN Xp, Compute Capability 6.1\r\n2020-02-12 14:45:01.937373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: TITAN Xp computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 30 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 510.07GiB/s\r\n2020-02-12 14:45:01.938710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 1 with properties: \r\npciBusID: 0000:02:00.0 name: TITAN Xp computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 30 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 510.07GiB/s\r\n2020-02-12 14:45:01.938995: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-02-12 14:45:01.941769: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-12 14:45:01.944257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-02-12 14:45:01.944765: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-02-12 14:45:01.947695: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-02-12 14:45:01.949183: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-02-12 14:45:01.949331: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n2020-02-12 14:45:01.949349: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1595] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-02-12 14:45:01.949431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-12 14:45:01.949448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 1 \r\n2020-02-12 14:45:01.949462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N Y \r\n2020-02-12 14:45:01.949473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 1:   Y N \r\nloc(callsite(\"Preprocessor/map/TensorArrayV2_1\"(\"/home/eric/.anaconda3/envs/tf_nightly_gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/lift_to_graph.py\":339:0) at callsite(\"/home/eric/.anaconda3/envs/tf_nightly_gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/wrap_function.py\":321:0 at callsite(\"convert_to_tflite2.py\":50:0 at \"convert_to_tflite2.py\":72:0)))): error: operand type 'tensor<i32>' is not compatible with preceding operands; expected rank: 1\r\nTraceback (most recent call last):\r\n  File \"/home/eric/.anaconda3/envs/tf_nightly_gpu/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/eric/.anaconda3/envs/tf_nightly_gpu/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/eric/.anaconda3/envs/tf_nightly_gpu/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/eric/.anaconda3/envs/tf_nightly_gpu/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/eric/.anaconda3/envs/tf_nightly_gpu/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/eric/.anaconda3/envs/tf_nightly_gpu/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: /home/eric/.anaconda3/envs/tf_nightly_gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/lift_to_graph.py:339:11: error: operand type 'tensor<i32>' is not compatible with preceding operands; expected rank: 1\r\n          op=op, graph=graph, op_map=op_map, base_graph=base_graph)\r\n          ^\r\n/home/eric/.anaconda3/envs/tf_nightly_gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/wrap_function.py:321:9: note: called from\r\n        base_graph=self._func_graph)\r\n        ^\r\nconvert_to_tflite2.py:50:9: note: called from\r\n        tf.nest.map_structure(import_graph.as_graph_element, outputs))\r\n        ^\r\nconvert_to_tflite2.py:72:1: note: called from\r\ninception_func = wrap_frozen_graph(detection_graph.as_graph_def(add_shapes=True), inputs=input_arrays, outputs=output_arrays)\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nCan't attach, but here's most of the .config file that was used to generate the graph (frozen graph).\r\nmodel {\r\n  ssd {\r\n    inplace_batchnorm_update: true\r\n    freeze_batchnorm: false\r\n    num_classes: 16   \r\n    box_coder {\r\n      faster_rcnn_box_coder {\r\n        y_scale: 10.0\r\n        x_scale: 10.0\r\n        height_scale: 5.0\r\n        width_scale: 5.0\r\n      }\r\n    }\r\n    matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.5\r\n        unmatched_threshold: 0.5\r\n        ignore_thresholds: false\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n        use_matmul_gather: true\r\n      }\r\n    }\r\n    similarity_calculator {\r\n      iou_similarity {\r\n      }\r\n    }\r\n    encode_background_as_zeros: true\r\n    anchor_generator {\r\n      multiscale_anchor_generator {\r\n        min_level: 3\r\n        max_level: 7\r\n        anchor_scale: 4.0\r\n        aspect_ratios: [1.0, 2.0, 0.5]\r\n        scales_per_octave: 2\r\n      }\r\n    }\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 640\r\n        width: 640\r\n      }\r\n    }\r\n    box_predictor {\r\n      weight_shared_convolutional_box_predictor {\r\n        depth: 256\r\n        class_prediction_bias_init: -4.6\r\n        conv_hyperparams {\r\n          activation: RELU_6,\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.00004\r\n            }\r\n          }\r\n          initializer {\r\n            random_normal_initializer {\r\n              stddev: 0.01\r\n              mean: 0.0\r\n            }\r\n          }\r\n          batch_norm {\r\n            scale: true,\r\n            decay: 0.997,\r\n            epsilon: 0.001,\r\n          }\r\n        }\r\n        num_layers_before_predictor: 4\r\n        kernel_size: 3\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'ssd_mobilenet_v1_fpn'\r\n      min_depth: 16\r\n      depth_multiplier: 1.0\r\n      conv_hyperparams {\r\n        activation: RELU_6,\r\n        regularizer {\r\n          l2_regularizer {\r\n            weight: 0.00004\r\n          }\r\n        }\r\n        initializer {\r\n          random_normal_initializer {\r\n            stddev: 0.01\r\n            mean: 0.0\r\n          }\r\n        }\r\n        batch_norm {\r\n          scale: true,\r\n          decay: 0.997,\r\n          epsilon: 0.001,\r\n        }\r\n      }\r\n      override_base_feature_extractor_hyperparams: true\r\n    }\r\n    loss {\r\n      classification_loss {\r\n        weighted_sigmoid_focal {\r\n          alpha: 0.25\r\n          gamma: 2.0\r\n        }\r\n      }\r\n      localization_loss {\r\n        weighted_smooth_l1 {\r\n        }\r\n      }\r\n      classification_weight: 1.0\r\n      localization_weight: 1.0\r\n    }\r\n    normalize_loss_by_num_matches: true\r\n    normalize_loc_loss_by_codesize: true\r\n    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 1e-8\r\n        iou_threshold: 0.6\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n**Failure details**\r\nModel crashes on conversion\r\n\r\n\r\n**Any other info / logs**\r\n\r\nWe use the generator scripts in the tensorflow object_detection library to actually make and train our neural net, so it's not easy to see what code would be breaking this. ", "comments": ["@erichaydel Is this still an issue for you? Can you please test with `tf-nightly` and let us know whether it is persisting?\r\n\r\nIf this is still an issue, please share a standalone code to reproduce the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36711\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36711\">No</a>\n"]}, {"number": 36710, "title": "Infinite recursion in TFModuleWrapper.__getattr__", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Docker container gcr.io/deeplearning-platform-release/tf-cpu.1-15\r\n- **TensorFlow version (use command below)**: v1.15\r\n- **Python version**: Python 3.5.6 :: Anaconda, Inc.\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen running a complex Apache Beam job using Tensorflow to create TFRecords, the jobs fails with \r\n....\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow_core/python/util/module_wrapper.py\", line 192, in __getattr__\r\n    attr = getattr(self._tfmw_wrapped_module, name)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow_core/python/util/module_wrapper.py\", line 192, in __getattr__\r\n    attr = getattr(self._tfmw_wrapped_module, name)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow_core/python/util/module_wrapper.py\", line 192, in __getattr__\r\n    attr = getattr(self._tfmw_wrapped_module, name)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow_core/python/util/module_wrapper.py\", line 160, in __getattribute__\r\n    attr_map = object.__getattribute__(self, '_tfmw_attr_map')\r\nRecursionError: maximum recursion depth exceeded while calling a Python object\r\n\r\nSorry, I have been unable to isolate a test case. However, TFModuleWrapper.__getattr__ does look vulberable to recursion errors. See https://nedbatchelder.com/blog/201010/surprising_getattr_recursion.html for a discussion of a similar problem.\r\n\r\nI am not using kivy, so this is probably different than: https://github.com/tensorflow/tensorflow/issues/27312 \r\n", "comments": ["@chrishmorris, Can you provide the sample code to reproduce the reported issue. Thanks!", "Appears to be related to https://github.com/uqfoundation/dill/issues/300. I had a DoFn with a custom init method and a super call, and used save_main_session. So my beam job would have failed, for reasons unconnected with Tensorflow. However, the behaviour of TFModuleWrapper.getattr generated an especially obscure message for the error. ", "Yes this looks like it is being related to dill. Thanks for reporting.", "Thanks for reporting the issue @chrishmorris. Do you see the same issue with newer version of TF?", "Closing this. @chrishmorris feel free to re-open if you are still having issue with this."]}, {"number": 36709, "title": "Update 80-performance-issue.md", "body": "", "comments": ["@jvishnuvardhan Could you please resolve the conflicts? Thanks!", "Closing this as required changed were carried out using another cl/294974844 "]}, {"number": 36708, "title": "First python project for class--- could someone please help me understand?- exif data using python ", "body": "\r\nHere are the instructions and at the bottom is exp-11's script. I have no idea what to do. VERY new to programming.. thank you ahead of time!\r\n\r\nUsing the Exp-11.py script provide as a baseline your assignment is as follows:\r\n\r\n1) Allow the user to enter a path\r\n\r\n2) Using that path, process all the .jpg files contained in that folder  (note you will need to create a directory with jpg images)\r\n\r\n3) Extract, EXIF data from each of the images and create a pretty table output.  Note, you will go beyond the basics and extract whatever camera or photo data exists for each photo.\r\n\r\n4) Plot the geolocation of each image on a map. \r\n\r\n(Note, there are several ways to do this)  However, the easiest method would be to use MapMaker App, at https://mapmakerapp.com/\r\nyou can either manually enter the lat/long values your code generates or you can place your results in a CSV file and upload the data to the map.   \r\nNOTE, this is a manual step process\r\n\r\n5) Submit both your script and a screenshot of the results.\r\n\r\n\r\n\r\n\r\n\r\n'''\r\nEXIF Data Acquistion\r\nJanuary 2019\r\nVersion 1.1\r\n'''\r\nfrom __future__ import print_function\r\n\r\n'''\r\nCopyright (c) 2019 Chet Hosmer, Python Forensics\r\n\r\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software\r\nand associated documentation files (the \"Software\"), to deal in the Software without restriction, \r\nincluding without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, \r\nand/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, \r\nsubject to the following conditions:\r\n\r\nThe above copyright notice and this permission notice shall be included in all copies or substantial \r\nportions of the Software.\r\n\r\n'''\r\n# Usage Example:\r\n# python Exp-11.py \r\n#\r\n# Requirement: Python 2.x or 3.x\r\n#\r\n# Requirement: 3rd Party Library that is utilized is: PILLOW\r\n#                   pip install PILLOW  from the command line\r\n\r\n\r\n''' LIBRARY IMPORT SECTION '''\r\n\r\nimport os                       # Python Standard Library : Operating System Methods\r\nimport sys                      # Python Standard Library : System Methods\r\nfrom datetime import datetime   # Python Standard Libary datetime method from Standard Library\r\n\r\n# import the Python Image Library \r\n# along with TAGS and GPS related TAGS\r\n# Note you must install the PILLOW Module\r\n# pip install PILLOW\r\n\r\nfrom PIL import Image\r\nfrom PIL.ExifTags import TAGS, GPSTAGS\r\n\r\n\r\n# import the prettytable library\r\nfrom prettytable import PrettyTable\r\n\r\ndef ExtractGPSDictionary(fileName):\r\n    ''' Function to Extract GPS Dictionary '''\r\n    try:\r\n        pilImage = Image.open(fileName)\r\n        exifData = pilImage._getexif()\r\n\r\n    except Exception:\r\n        # If exception occurs from PIL processing\r\n        # Report the \r\n        return None, None\r\n\r\n    # Interate through the exifData\r\n    # Searching for GPS Tags\r\n\r\n    imageTimeStamp = \"NA\"\r\n    cameraModel = \"NA\"\r\n    cameraMake = \"NA\"\r\n    gpsData = False\r\n\r\n    gpsDictionary = {}\r\n\r\n    if exifData:\r\n\r\n        for tag, theValue in exifData.items():\r\n\r\n            # obtain the tag\r\n            tagValue = TAGS.get(tag, tag)\r\n\r\n            # Collect basic image data if available\r\n\r\n            if tagValue == 'DateTimeOriginal':\r\n                imageTimeStamp = exifData.get(tag).strip()\r\n\r\n            if tagValue == \"Make\":\r\n                cameraMake = exifData.get(tag).strip()\r\n\r\n            if tagValue == 'Model':\r\n                cameraModel = exifData.get(tag).strip()\r\n\r\n            # check the tag for GPS\r\n            if tagValue == \"GPSInfo\":\r\n\r\n                gpsData = True;\r\n\r\n                # Found it !\r\n                # Now create a Dictionary to hold the GPS Data\r\n\r\n                # Loop through the GPS Information\r\n                for curTag in theValue:\r\n                    gpsTag = GPSTAGS.get(curTag, curTag)\r\n                    gpsDictionary[gpsTag] = theValue[curTag]\r\n\r\n        basicExifData = [imageTimeStamp, cameraMake, cameraModel]    \r\n\r\n        return gpsDictionary, basicExifData\r\n\r\n    else:\r\n        return None, None\r\n\r\n# End ExtractGPSDictionary ============================\r\n\r\n\r\ndef ExtractLatLon(gps):\r\n    ''' Function to Extract Lattitude and Longitude Values '''\r\n\r\n    # to perform the calcuation we need at least\r\n    # lat, lon, latRef and lonRef\r\n    \r\n    try:\r\n        latitude     = gps[\"GPSLatitude\"]\r\n        latitudeRef  = gps[\"GPSLatitudeRef\"]\r\n        longitude    = gps[\"GPSLongitude\"]\r\n        longitudeRef = gps[\"GPSLongitudeRef\"]\r\n\r\n        lat = ConvertToDegrees(latitude)\r\n        lon = ConvertToDegrees(longitude)\r\n\r\n        # Check Latitude Reference\r\n        # If South of the Equator then lat value is negative\r\n\r\n        if latitudeRef == \"S\":\r\n            lat = 0 - lat\r\n\r\n        # Check Longitude Reference\r\n        # If West of the Prime Meridian in \r\n        # Greenwich then the Longitude value is negative\r\n\r\n        if longitudeRef == \"W\":\r\n            lon = 0- lon\r\n\r\n        gpsCoor = {\"Lat\": lat, \"LatRef\":latitudeRef, \"Lon\": lon, \"LonRef\": longitudeRef}\r\n\r\n        return gpsCoor\r\n\r\n    except:\r\n        return None\r\n\r\n# End Extract Lat Lon ==============================================\r\n\r\n\r\ndef ConvertToDegrees(gpsCoordinate):\r\n    ''' Function to CONVERT GPS COORIDINATES TO DEGRESS '''\r\n    d0 = gpsCoordinate[0][0]\r\n    d1 = gpsCoordinate[0][1]\r\n    try:\r\n        degrees = float(d0) / float(d1)\r\n    except:\r\n        degrees = 0.0\r\n\r\n    m0 = gpsCoordinate[1][0]\r\n    m1 = gpsCoordinate[1][1]\r\n    try:\r\n        minutes = float(m0) / float(m1)\r\n    except:\r\n        minutes=0.0\r\n\r\n    s0 = gpsCoordinate[2][0]\r\n    s1 = gpsCoordinate[2][1]\r\n    try:\r\n        seconds = float(s0) / float(s1)\r\n    except:\r\n        seconds = 0.0\r\n\r\n    floatCoordinate = float (degrees + (minutes / 60.0) + (seconds / 3600.0))\r\n\r\n    return floatCoordinate\r\n\r\n''' MAIN PROGRAM ENTRY SECTION '''\r\n\r\nif __name__ == \"__main__\":\r\n    '''\r\n    pyExif Main Entry Point\r\n    '''\r\n    print(\"\\nExtract EXIF Data from JPEG Files\")\r\n\r\n    print(\"Script Started\", str(datetime.now()))\r\n    print()\r\n\r\n    ''' PROCESS EACH JPEG FILE SECTION '''\r\n\r\n    latLonList = []\r\n    targetFile = \"test.jpg\"                 # file must be located in the same folder\r\n    if os.path.isfile(targetFile):\r\n        gpsDictionary, exifList = ExtractGPSDictionary(targetFile)\r\n            \r\n        if exifList:\r\n            TS = exifList[0]\r\n            MAKE = exifList[1]\r\n            MODEL = exifList[2]\r\n        else:\r\n            TS = 'NA'\r\n            MAKE = 'NA'\r\n            MODEL = 'NA'\r\n\r\n        print(\"Photo Details\")\r\n        print(\"-------------\")\r\n        print(\"TimeStamp:    \", TS)\r\n        print(\"Camera Make:  \", MAKE)\r\n        print(\"Camera Model: \", MODEL)\r\n        \r\n        if (gpsDictionary != None):\r\n\r\n            # Obtain the Lat Lon values from the gpsDictionary\r\n            # Converted to degrees\r\n            # The return value is a dictionary key value pairs\r\n\r\n            dCoor = ExtractLatLon(gpsDictionary)\r\n\r\n            print(\"\\nGeo-Location Data\")\r\n            print(\"-----------------\")\r\n\r\n            if dCoor:\r\n                lat = dCoor.get(\"Lat\")\r\n                latRef = dCoor.get(\"LatRef\")\r\n                lon = dCoor.get(\"Lon\")\r\n                lonRef = dCoor.get(\"LonRef\")\r\n                \r\n                if ( lat and lon and latRef and lonRef):\r\n                    print(\"Lattitude: \", '{:4.4f}'.format(lat))\r\n                    print(\"Longitude: \", '{:4.4f}'.format(lon))\r\n                else:\r\n                    print(\"WARNING No GPS EXIF Data\")\r\n            else:\r\n                print(\"WARNING No GPS EXIF Data\")                    \r\n        else:\r\n            print(\"WARNING\", \" not a valid file\", targetFile)\r\n\r\n    # Create Result Table Display using PrettyTable\r\n    ''' GENERATE RESULTS TABLE SECTION'''\r\n\r\n    ''' Result Table Heading'''\r\n    resultTable = PrettyTable(['File-Name', 'Lat','Lon', 'TimeStamp', 'Make', 'Model'])\r\n    ''' Your work starts here '''\r\n    \r\n    print()", "comments": ["@rcizzle94 This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 36707, "title": "TFLite can't quantize op transpose_conv", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.15.2, 2.1.0\r\n\r\nQuantization doesn't support op: TRANSPOSE_CONV\r\n\r\n```python \r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef gen_calibration_dataset():\r\n    for _ in range(10):\r\n        yield [np.random.rand(1,2,2,2).astype(np.float32)]\r\n\r\ndef get_keras_model_conv():\r\n    input_0 = tf.keras.layers.Input(shape=[2, 2, 2])\r\n    transpose_conv_1 = tf.keras.layers.Conv2DTranspose(filters=2, kernel_size=[2, 2],\r\n                                                       use_bias=False, padding=\"same\")(input_0)\r\n    model = tf.keras.models.Model(inputs=[input_0], outputs=[transpose_conv_1])\r\n    model.summary()\r\n    return model\r\n\r\ndef gen_model():\r\n    keras_model = get_keras_model_conv()\r\n    keras_model.save('test_model.h5')\r\n    converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file('test_model.h5')\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.int8\r\n    converter.inference_output_type = tf.int8\r\n    converter.representative_dataset = gen_calibration_dataset\r\n    tflite_quant_model = converter.convert()\r\n    open('test.tflite', 'wb').write(tflite_quant_model)\r\n\r\nprint(\"TF version: {}\".format(tf.__version__))\r\ngen_model()\r\n```\r\n\r\nerror mesg:\r\n> Traceback (most recent call last):\r\n>   File \"test_RICH.py\", line 30, in <module>\r\n>     gen_model()\r\n>   File \"test_RICH.py\", line 26, in gen_model\r\n>     tflite_quant_model = converter.convert()\r\n>   File \"/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 469, in convert\r\n>     self.experimental_new_quantizer)\r\n>   File \"/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 243, in _calibrate_quantize_model\r\n>     inference_output_type, allow_float, enable_mlir_quantizer)\r\n>   File \"/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 81, in calibrate_and_quantize\r\n>     enable_mlir_quantizer)\r\n>   File \"/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 115, in QuantizeModel\r\n>     return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, *args)\r\n> RuntimeError: Quantization not yet supported for op: TRANSPOSE_CONV", "comments": ["This is working for me in nightly -- please try upgrading to tf-nightly==2.2.0.dev20200212 and running your test again.", "> This is working for me in nightly -- please try upgrading to tf-nightly==2.2.0.dev20200212 and running your test again.\r\n\r\nthanks for have a quick look. yes, works on tf-nightly, issue closed.", "When i tried the above code  on v2 lite converter with 'from_keras_model' for a keras model with UINT8 data type, it adds a quantize layer at beginning and dequantize layer at the end(netron viewer). I even tried to do the same with a saved model version of the same keras model; but results are same. Also tried with nightly versions dev20200212 and dev20200216. Anybody else facing this issue and how can it be resolved?\r\n\r\nHere is the code:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(reshape_model)\r\n#converter.experimental_new_converter=True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\n\r\ntflite_quant_model = converter.convert()\r\nopen(\"/content/quant_model.tflite\", \"wb\").write(tflite_quant_model)\r\n```\r\n\r\n\r\nHowever, if i change types from **tf.uint8 to tf.int8**  in v1 lite converter it seems to work, even though in official doc it's still uint8 !!!\r\nShould we then convert our float values to **-128 to 127** range for inference and how to accomplish conversion with TF2.0 ?\r\nRef: https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations\r\n", "i have the same error, as \"Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 201, name: input_1 \", and the input type still float32 for mobilenetv2\r\n[{'name': 'input_1', 'index': 201, 'shape': array([  1, 128, 128,   3], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n", "@mlinxiang  At which phase of conversion are you getting this error? Are your model inputs still float? If you view your model using a tool like netron, i suppose you will see both input and outputs as float. It may have added a quantize layer at beginning and a dequantize at the end  even though quantization was successful? ", "just as you say, input output are float32, and follow with *quant is int8 which as expect need to be uint8.  i change input image to float the model run success, and result is right, but this tflite model can't run on my mtk platform which has an apu core which need all uint8 tflite model\r\n![image](https://user-images.githubusercontent.com/6283983/75040457-a6c6f980-54f5-11ea-9732-0d5b43f4ed0f.png)\r\n![image](https://user-images.githubusercontent.com/6283983/75040791-45535a80-54f6-11ea-8a62-3d4676b88a0f.png)\r\n", "@mlinxiang I faced simialar problem with my model.  My input  images were actually normalized between -2.0 to +2.0 range (instead of 0.0 to 255.0)  during training. When i tried to convert it to full integer format with quantization, it resulted in a model (with tf2.0 nightly using v1 converter as shown in the code) similar to yours with quantize and dequantize layers at the ends. Accelerators may not support (i believe) these layers explicitly. I used **tf.int8 instead of tf.uint8** during **post-training quantization**(with representative dataset) according to the aforementioned code. Finally it got rid of these layers and new inputs were of type **int8**. I successfully converted to **coral edge tpu**  format and was able to run it without any issues so far. After adding appropriate normalization in code  it seems to give **correct results**.", "when i change code to follow everything is ok\r\n**converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_model_h5)**\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n    converter.representative_dataset = rep_data_gen\r\n\r\n    tflite_model = converter.convert()\r\n    open(tflite_model_path, \"wb\").write(tflite_model)\r\n![image](https://user-images.githubusercontent.com/6283983/75043009-52724880-54fa-11ea-903c-37ef74c53dd0.png)\r\n", "@mlinxiang  But that's the same code @psunn has given in the issue right? Didn't you try that in the first place?Still is it running in the device and your issue resolved completely?", "his type is as follow \r\nconverter.inference_input_type = **tf.int8**\r\nconverter.inference_output_type = **tf.int8**\r\n", "Interesting ...my case was just reverse. As per the official documentation it was **tf.uint8**; but it resulted in extra dequantize and quantize layers in my case and removed them when i changed it to **tf.int8** i.e there were not explicit quantize or dequantize layers in model when i use tf.int8. May be it depends on value ranges of activation's of the original model !!!\r\n\r\nBy the way is it working as expected ? Does the device run the **dequantize**(is it present now?) layer without error?", "compared the quant model and float model on my 3.3k test images, my model acc (keypoint heatmap predict distance) increase from float:3.1648   to  quant: 5.3375 pixel distance, most result are similar, but some blur or bad quality image result are really bad, float model is more balance. \r\nmaybe quant-aware training is a better choice as my previous exps, but keras not support right now\r\n", "Yes, post-training quantization seems to degrade the output quality, especially in scenarios where high pixel accuracies or precise boundaries are desired. Even int tf 1.x, quantization aware training support only limited convolutional architectures. But on the flip side it gives 40x speed-up, reduced power, memory etc. !!!"]}, {"number": 36706, "title": "Why do we need to convert Keras model to tf-estimator in TF-2.0", "body": "I want to upgrade my custom estimator model from tf1 to tf2, and I follow this link to upgrade\r\n\r\nThey recommend the use tf.keras.estimator.model_to_estimator\r\n\r\n**TensorFlow-2.0 official doc**\r\n> We recommend that you define your model using Keras, then use the tf.keras.estimator.model_to_estimator utility to turn your model into an estimator. The code below shows how to use this utility when creating and training an estimator.\r\n\r\nIn TF-1.x, in order to scale model to training on multiple workers, we convert Keras to tf-estimator.\r\n\r\nBut in TF-2.x, Keras already support distributed training well, so my question is\r\n\r\n1. Why do we need to convert Keras to estimator in TF-2.0, dose any benefits?\r\n2. Can we use low-level TF API (like tf.function()) to build custom estimator model ?\r\n\r\nI can't find any example to build estimator model in TF-2.0 and without Keras.", "comments": ["> 1. Why do we need to convert Keras to estimator in TF-2.0, dose any benefits?\r\n\r\nIf you use infrastructure or other tooling that relies on Estimators, you can still build a Keras model and use model_to_estimator to convert the Keras model to an Estimator for use with downstream systems.\r\n\r\n> 2. Can we use low-level TF API (like tf.function()) to build custom estimator model ?\r\nAs you can see [here](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#eager_compatibility), Calling methods of Estimator will work while eager execution is enabled. However, the model_fn and input_fn is not executed eagerly, Estimator will switch to graph mode before calling all user-provided functions (incl. hooks), so their code has to be compatible with graph mode execution.\r\nSo using tf.function() here doesn't make sense.\r\n\r\nPlease correct me if you think I  have misinterpreted your question. Thanks!\r\n\r\n", "@gowthamkpr Thanks for the reply. \r\nInstead of use Keras, I want to build a custom estimator model that build by low-level TF API.", "You can follow the following[ tutorial](https://medium.com/tensorflow/how-to-write-a-custom-estimator-model-for-the-cloud-tpu-7d8bd9068c26) to write your custom estimator model.\r\nIf you have more questions please post them in stackoverflow as there is a bigger community to respond. Thanks!"]}, {"number": 36705, "title": "TFLite: allocate_tensors() fails for 2 dense layers with quantization", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.15.0\r\n\r\nI'm trying to convert a model built with keras to tflite in order to run it on an Edge TPU. If I use more than one dense layer, allocate_tensors() throws an error. It only happens for full integer quantization with a representative dataset. Here is a minimal example which reproduces this error:\r\n\r\n```\r\nimport keras\r\nimport tensorflow as tf\r\nimport tflite_runtime.interpreter as tflite\r\nimport numpy as np\r\n\r\n\r\nn = 16\r\n\r\ndense_1 = keras.layers.Dense(n)\r\ndense_2 = keras.layers.Dense(n)\r\n\r\nx = keras.layers.Input(shape=(n,))\r\n\r\ny = dense_1(x)\r\ny = dense_2(y)\r\n\r\nmodel = keras.models.Model(inputs=[x], outputs=[y])\r\n\r\nmodel.compile(optimizer='rmsprop', loss='mean_squared_error')\r\nmodel.save('example.h5')\r\n\r\ninput_shapes = {}\r\ninput_shapes['input_1'] = (1, n)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file('example.h5', input_shapes=input_shapes)\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n\r\ndef rep_data_gen():\r\n\tfor _ in range(100):\r\n\t\tyield [np.random.rand(1, n).astype(np.float32)]\r\n\r\nconverter.representative_dataset = rep_data_gen\r\n\r\ntflite_model = converter.convert()\r\nopen(\"example.tflite\", \"wb\").write(tflite_model)\r\n\r\ninterpreter = tflite.Interpreter(\"example.tflite\",\r\n\t\t\t\t\t\t\t\t experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\r\n\r\ninterpreter.allocate_tensors()\r\n```\r\n\r\nThe error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"example.py\", line 43, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"/home/julian/anaconda3/envs/keras/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 244, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/home/julian/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 106, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/kernel_util.cc:106 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.Node number 2 (FULLY_CONNECTED) failed to prepare.\r\n```\r\n\r\nNote that the error doesn't occur when I drop ```y = dense_2(y)```. I feel like I am missing something simple, but what is it? This is the simplest two-layer model I can imagine, so it should really work.", "comments": ["@daverim allocation works without quantization. Any idea?", "I'm having the exact same issue, someone knows what is the problem?", "@julian-urban \r\nUnrelated, but make sure you [compile the model](https://coral.ai/docs/edgetpu/compiler/) after converting it to a tflite model, otherwise model will still be executed on CPU. Also don't forget this:\r\n```\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n```\r\n\r\nAlso, your input shape is dynamic, which isn't supported with the edgetpu compiler:\r\n```\r\nx = keras.layers.Input(shape=(n,))\r\n```\r\nCan you try fixing that?\r\n\r\nFYI, [here](https://gist.github.com/Namburger/4cc57ab3cbe4bf5abbc1d3e84536b75c) is a dummy example for keras model", "@Namburger \r\nThanks for your answer. I set the input and output type to tf.uint8 and fixed the input shape to (1, n). I tried to compile the model, but unfortunately it throws the same error:\r\n\r\n```\r\n$ edgetpu_compiler example.tflite \r\nEdge TPU Compiler version 2.0.291256449\r\nERROR: :129 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.\r\nERROR: Node number 2 (FULLY_CONNECTED) failed to prepare.\r\n\r\n\r\nModel compiled successfully in 1 ms.\r\n\r\nInput model: example.tflite\r\nInput size: 2.02KiB\r\nOutput model: example_edgetpu.tflite\r\nOutput size: 1.85KiB\r\nOn-chip memory available for caching model parameters: 0.00B\r\nOn-chip memory used for caching model parameters: 0.00B\r\nOff-chip memory used for streaming uncached model parameters: 0.00B\r\nNumber of Edge TPU subgraphs: 0\r\nTotal number of operations: 0\r\nOperation log: example_edgetpu.log\r\nSee the operation log file for individual operation details.\r\n$\r\n```\r\n\r\nIt says 'Model compiled successfully', but if I try to load it and allocate tensors, the same error message appears. Any ideas? Thanks for bringing the dummy example to my attention, I will check it out and see if it helps my issue.", "@julian-urban \r\nHumn, so I'm still seeing this:\r\n```\r\nERROR: :129 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.\r\nERROR: Node number 2 (FULLY_CONNECTED) failed to prepare.\r\n```\r\nPossibly due to undefined weight, can you show me your current code for producing model by any chance? \r\n", "@Namburger \r\n\r\nThe dummy example with Conv2D worked (also with more than one layer). I adapted the code to have two Dense layers:\r\n\r\n```\r\nimport tensorflow as tf\r\nif not str(tf.__version__).startswith('1.15'):\r\n    print('please use tensorflow 1.15')\r\n    exit()\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Conv2D, Dense\r\n\r\ntf.enable_eager_execution()\r\n\r\nn = 64\r\n\r\n# image_shape = (n,n,3)\r\nimage_shape = (n,)\r\n\r\n# Creating a dummy keras model here\r\nx = Input(shape=image_shape)\r\n\r\n# y = Conv2D(3, (3, 3), padding='same')(x)\r\n# y = Conv2D(3, (3, 3), padding='same')(y)\r\n\r\ny = Dense(n)(x)\r\ny = Dense(n)(y)\r\n\r\nmodel = Model(inputs=x, outputs=y)\r\nmodel.summary()\r\nmodel.save('keras_model.h5', include_optimizer=False)\r\n\r\ndef representative_dataset_gen():\r\n    for i in range(100):\r\n        # creating fake images\r\n        image = tf.random.normal([1] + list(image_shape))\r\n        yield [image]\r\n\r\n# actual conversion\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file('keras_model.h5')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = tf.lite.RepresentativeDataset(representative_dataset_gen)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # For EdgeTPU, no float ops allowed\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\n# save model\r\ntflite_model = converter.convert()\r\nopen('dummy.tflite', 'wb').write(tflite_model)\r\n```\r\n\r\nThe same error appears when I try to compile it. Note that I still have (n,) as the input shape, I tried fixed ones as well but to no avail. I don't think the problem is related to this. Do I need to initialize the weights manually? If I remember correctly, I tried that and it didn't help. Maybe it's a keras issue? Is there a dummy example that just uses tensorflow?", "@julian-urban Right now this looks like a tflite convertion issue to me as @terryheo mentioned that it [works without quantization](https://github.com/tensorflow/tensorflow/issues/36705#issuecomment-585591604). Maybe @daverim will have more insight?\r\n\r\nI do have a dummy example for converting a .pb file, it downloads a mobile net model and converts it to tflite. Although not sure if it'd help you in this case: \r\nhttps://gist.github.com/Namburger/f44a938886ad4a0325ca2f30263fcee0\r\n\r\nOn another note, just wanted to make you aware that `Dense` is not a [supported operation](https://coral.ai/docs/edgetpu/models-intro/#supported-operations) by the compiler at this time. So once this is fixed, chances are the model still won't pass the compiler (plus the dynamic input size). \r\n\r\nI still think it's worth investigating why you are not able to allocate_tensors() after conversion though.", "@Namburger \r\n\r\n> On another note, just wanted to make you aware that Dense is not a supported operation by the compiler at this time. So once this is fixed, chances are the model still won't pass the compiler (plus the dynamic input size).\r\n\r\nThat explains a lot, however, I do see 'FullyConnected' on the list. How do I implement the operation in keras or tensorflow? Should I just use matrix-vector multiplication syntax?", "I'm not a hundred percent sure on this, but maybe [this](https://stats.stackexchange.com/a/383797) will help? ", "Running into this issue as well. I tried converting the graph to a .pb file (freezing the graph) as @Namburger  suggested and then using TFLiteConverter.from_frozen_graph() but that also failed with the same error. \r\n\r\nLooking at the MobileNet architecture[1] it seems like there is only one FC layer so perhaps that is why this bug was not seen earlier. It seems surprising that no model with 2 FC layers has been compiled for the Edge TPU yet. Does anyone know one so I can try to replicate it?\r\n\r\nI tried creating a fully connected layer using tf.matmul, tf.contrib.layers.fully_connected, and tf.compat.v1.layers.Dense. They all failed with the same error.\r\n\r\n[1] Table 1 of https://arxiv.org/pdf/1704.04861.pdf", "Workaround: Make the layers have different sizes. @julian-urban \r\n\r\nStrangely, if the layer sizes are different the issue does not manifest. Got the idea from https://github.com/tensorflow/tensorflow/issues/30173 which I believe is a duplicate of this. I wrote some code to demonstrate my claim:\r\n\r\n```\r\n# How to Reproduce issue:\r\n# 1) Install TensorFlow 1.15 ('1.15.0') and edgetpu_compiler\r\n# 2) python simple_example.py (this file)\r\n# 3) edgetpu_compiler issue_36705_dummy.tflite\r\n#    - Will fail with ERROR: :129 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.\r\n# 4) Set 'will_work = True' below:\r\n# 5) Repeat steps 2 & 3 and it will compile successfully.\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pdb \r\n\r\n\r\ndef main():\r\n  tflite_filepath = \"issue_36705_dummy.tflite\"\r\n  will_work = False  # Switch to trigger bug/fix\r\n  \r\n  # Hyperparameters\r\n  tflite_batch_size = 1 \r\n  input_size = 64\r\n\r\n  num_layers = 2 \r\n  default_layer_size = 1024\r\n  if will_work:\r\n    # Make the layers different sizes to avoid the issue.\r\n    layer_sizes = [default_layer_size - i for i in range(num_layers)]\r\n  else:\r\n    layer_sizes = [default_layer_size] * num_layers\r\n  \r\n  # Build Graph\r\n  input_shape = (tflite_batch_size, input_size)\r\n  x = tf.compat.v1.placeholder(tf.float32, shape=input_shape)\r\n\r\n  output = x \r\n  for i in range(num_layers):\r\n    # Can use tf.matmul or tf.keras.layers.Dense here too:\r\n    output = tf.contrib.layers.fully_connected(output, layer_sizes[i])\r\n\r\n  # Build TFLite\r\n  with tf.compat.v1.Session() as sess:\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n\r\n    converter = tf.compat.v1.lite.TFLiteConverter.from_session(sess, \r\n        [x], [output])\r\n\r\n    def representative_dataset_gen():\r\n      for i in range(100):\r\n        yield [np.ones(input_shape).astype(np.float32)]\r\n\r\n    # UINT8 + Edge TPU Constraints.\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.representative_dataset = representative_dataset_gen\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n\r\n    # Build and write binary.\r\n    tflite_binary = converter.convert()\r\n    open(tflite_filepath, \"wb\").write(tflite_binary)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  main()\r\n```", "@shariq-audiofocus Thanks for the workaround, I'll try it out. I guess you can train your network as desired and then embed the learned weights into layers of varying size. It's a shame though that such a simple model requires this amount of hacking to get it to run. I'm not a particular fan of the documentation either, the available information is really sparse and scattered. I suppose the Edge TPU was rushed to production and now we have to deal with the resulting shortcomings, I'm just speculating though... don't get me wrong, it's still a great platform/project. However, encountering issues like this one when you just want to try some super elementary, straightforward stuff simply doesn't leave a great impression, is all I'm saying.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36705\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36705\">No</a>\n"]}, {"number": 36704, "title": "Why K.mean is used in tf.keras.losses.binary_crossentropy ?", "body": "In [line 993](https://github.com/tensorflow/tensorflow/blob/a641fa1c5c10f0a278c7671fd6f7df550a74935d/tensorflow/python/keras/losses.py#L993) of the code of `tf.keras.losses.binary_crossentropy`, `K.mean` is called on axis `-1` of `K.binary_crossentropy(y_true, y_pred, from_logits=from_logits)`.\r\n\r\nI wonder why there is this `K.mean` call and why `tf.keras.losses.binary_crossentropy` doesn't simply return `K.binary_crossentropy(y_true, y_pred, from_logits=from_logits)`.\r\n\r\nOn the contrary, `tf.keras.losses.categorical_crossentropy` and `tf.keras.losses.sparse_categorical_crossentropy` just return the call to their `tf.keras.backend` equivalent.\r\n\r\nI think it may be inconsistent and misleading, especially because `tf.keras.losses.categorical_crossentropy` and `tf.keras.backend.categorical_crossentropy` behave similarly, but not `tf.keras.losses.binary_crossentropy` and `tf.keras.backend.binary_crossentropy`, so higher level objects like `tf.keras.metrics.CategoricalCrossentropy` may not work as one would expect.", "comments": ["Thank you @durandg12 . Historically, the loss functions have been expecting that inputs are at least 2D and computing mean on the last axis in order to support sample weighting correctly. \r\n\r\n```\r\ny_true: Ground truth values. shape = [batch_size, d0, .. dN].\r\ny_pred: The predicted values. shape = [batch_size, d0, .. dN].\r\nsample_weight: Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size], then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight. (Note on dN-1: all metric functions reduce by 1 dimension, usually the last axis (-1)).\r\n```\r\n\r\nIf your input labels are [batch_size, d0] the result from the functions will be [batch_size] ie. one loss value per sample. This applies to binary, categorical and sparse categorical crossentropy functions.\r\n\r\n`tf.keras.losses.binary_crossentropy([[0], [1]], [[0.3], [0.8]])`\r\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.3566748 , 0.22314338], dtype=float32)>\r\n\r\n`tf.keras.losses.categorical_crossentropy([[0, 1, 0], [0, 0, 1]], [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])`\r\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.05129331, 2.3025851 ], dtype=float32)>\r\n\r\n`tf.keras.losses.sparse_categorical_crossentropy([1, 2], [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])`\r\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.05129344, 2.3025851 ], dtype=float32)>\r\n\r\nIn all the three cases above, the # samples = 2.\r\n\r\nHope this clears the confusion between the cross entropy functions. \r\n\r\n\r\n\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36704\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36704\">No</a>\n"]}, {"number": 36703, "title": "Unable to remove model from memory", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, it can be found in the following repository https://github.com/acvander/kaggle_real_or_not\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 18.04\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\nv2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- **Python version**:\r\n3.7.5\r\n- **CUDA/cuDNN version**:\r\n10.2\r\n- **GPU model and memory**:\r\n1060 6GB\r\n- **Exact command to reproduce**:\r\nThese are for data preprocessing\r\n\r\n```python main.py --mode=preprocess```\r\n```python main.py --mode=preprocess_bert```\r\n\r\nThis is the training command\r\n\r\n```python main.py --mode=train_bert --model_dir=./tmp/k_bert --model_name=bert --epochs=2 --subset=0.25```\r\n\r\n### Describe the problem\r\nI believe this is a similar problem to #36587. Upon starting to train the second fold, the first model is not properly removed from memory and results in an OOM error.\r\nI run the ```del``` command to delete the model and bert_layers as well as ```tf.keras.backend.clear_session()```\r\n### Source code / logs\r\n```\r\n2020-02-12 12:22:44.782689: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2020-02-12 12:22:44.784338: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvrtc.so.10.2: cannot open shared object file: No such file or directory\r\n2020-02-12 12:22:44.784352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\nI0212 12:22:46.138824 139915467714688 train_bert.py:72] loading data\r\nI0212 12:22:46.162351 139915467714688 train_bert.py:80] loading model\r\nI0212 12:22:46.163624 139915467714688 train_bert.py:95] training fold #1\r\nI0212 12:22:46.164219 139915467714688 resolver.py:79] Using /tmp/tfhub_modules to cache modules.\r\nTrain on 635 samples, validate on 1268 samples\r\nEpoch 1/2\r\n632/635 [============================>.] - ETA: 0s - loss: 0.5996 - f1_score: 0.6930   \r\nEpoch 00001: val_f1_score improved from -inf to 0.75631, saving model to ./tmp/k_bert/bert_0.h5\r\n635/635 [==============================] - 89s 141ms/sample - loss: 0.6001 - f1_score: 0.6929 - val_loss: 0.5407 - val_f1_score: 0.7563\r\nEpoch 2/2\r\n632/635 [============================>.] - ETA: 0s - loss: 0.4509 - f1_score: 0.8212 \r\nEpoch 00002: val_f1_score improved from 0.75631 to 0.80126, saving model to ./tmp/k_bert/bert_0.h5\r\n635/635 [==============================] - 51s 80ms/sample - loss: 0.4508 - f1_score: 0.8205 - val_loss: 0.4792 - val_f1_score: 0.8013\r\nI0212 12:25:23.587857 139915467714688 train_bert.py:95] training fold #2\r\nTrain on 634 samples, validate on 1269 samples\r\nEpoch 1/2\r\n  8/634 [..............................] - ETA: 29:17WARNING:tensorflow:Can save best model only with val_f1_score available, skipping.\r\nW0212 12:26:02.588592 139915467714688 callbacks.py:1018] Can save best model only with val_f1_score available, skipping.\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 119, in <module>\r\n    app.run(main)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"main.py\", line 99, in main\r\n    subset=FLAGS.subset)\r\n  File \"/home/adam/machine_learning/real_or_not/training/train_bert.py\", line 125, in train_bert\r\n    callbacks=callbacks)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 85, in distributed_function\r\n    per_replica_function, args=args)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 763, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1819, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2164, in _call_for_each_replica\r\n    return fn(*args, **kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 433, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 312, in train_on_batch\r\n    output_loss_metrics=output_loss_metrics))\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 273, in _process_single_batch\r\n    model.optimizer.apply_gradients(zip(grads, trainable_weights))\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 434, in apply_gradients\r\n    self._create_slots(var_list)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/adam.py\", line 149, in _create_slots\r\n    self.add_slot(var, 'm')\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 599, in add_slot\r\n    initial_value=initial_value)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 260, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 254, in _variable_v2_call\r\n    shape=shape)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2080, in creator\r\n    return next_creator(*args, **kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2080, in creator\r\n    return next_creator(*args, **kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2080, in creator\r\n    return next_creator(*args, **kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 485, in variable_capturing_scope\r\n    lifted_initializer_graph=lifted_initializer_graph, **kwds)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 178, in __init__\r\n    initial_value() if init_from_fn else initial_value,\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops_v2.py\", line 98, in __call__\r\n    return array_ops.zeros(shape, dtype)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 2446, in zeros\r\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 233, in fill\r\n    result = gen_array_ops.fill(dims, value, name=name)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 3244, in fill\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/adam/.pyenv/versions/real-or-not/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 6606, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[4096,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill] name: Adam/bert_model/encoder/layer_4/output/kernel/m/Initializer/zeros/\r\n```\r\n\r\n", "comments": ["@acvander, Can you provide the code snippet to reproduce the issue. Thanks!", "```\r\ndef train_bert(model_dir: str = './tmp/bert_default',\r\n               model_name: str = 'bert_default',\r\n               shuffle: bool = True,\r\n               fig_name: str = 'bert',\r\n               epochs: int = 3,\r\n               subset: float = 1.0):\r\n    # load data\r\n    logging.info('loading data')\r\n    with shelve.open('./tmp/bert_data/bert_shelf') as shelf:\r\n        train_input = shelf['train_input']\r\n        train_output = shelf['train_output']\r\n        test_input = shelf['test_input']\r\n        max_token_len = shelf['max_token_len']\r\n        bert_url = shelf['bert_url']\r\n\r\n    logging.info('loading model')\r\n    bert = hub.load(bert_url)\r\n    bert_layer = hub.KerasLayer(bert_url, trainable=True)\r\n    model = build_bert_model(bert_layer, max_len=max_token_len)()\r\n    initial_model_path = os.path.join(model_dir, 'initial_model.h5')\r\n    model.save(initial_model_path)\r\n\r\n    os.makedirs(model_dir, exist_ok=True)\r\n\r\n    splitter = StratifiedKFold(n_splits=3, shuffle=shuffle, random_state=2020)\r\n\r\n    # Subset data for debugging\r\n    train_output = train_output[:int(subset * len(train_output))]\r\n\r\n    x = range(len(train_output))\r\n    k_folds = list(splitter.split(\r\n        x, train_output[:, 0]))  # only need to use one value for grouping\r\n\r\n    histories = []\r\n    for i, (valid_idxs, train_idxs) in enumerate(k_folds):\r\n        logging.info('training fold #{}'.format(i + 1))\r\n\r\n        model = load_model(initial_model_path,\r\n                           custom_objects={'KerasLayer': hub.KerasLayer},\r\n                           compile=False)\r\n\r\n        model_path = os.path.join(model_dir, '{}_{}.h5'.format(model_name, i))\r\n\r\n        # compile modelf\r\n        compile_model(model, learn_rate=2e-6, model_path=model_path)\r\n\r\n        # separate data\r\n        fold_train_input = {\r\n            key: val[train_idxs]\r\n            for (key, val) in train_input.items()\r\n        }\r\n        fold_train_output = train_output[train_idxs]\r\n        fold_valid_input = {\r\n            key: val[valid_idxs]\r\n            for (key, val) in train_input.items()\r\n        }\r\n        fold_valid_output = train_output[valid_idxs]\r\n\r\n        callbacks = create_callbacks(model_path=model_path)\r\n\r\n        history = model.fit(fold_train_input,\r\n                            fold_train_output,\r\n                            validation_data=(fold_valid_input,\r\n                                             fold_valid_output),\r\n                            epochs=epochs,\r\n                            batch_size=8,\r\n                            callbacks=callbacks)\r\n        plot_training_data(\r\n            history.history,\r\n            '{}_{}.png'.format(os.path.join(model_dir, fig_name), i))\r\n        histories.append(history)\r\n\r\n    plot_k_fold_data(\r\n        histories, '{}_combined.png'.format(os.path.join(model_dir, fig_name)))\r\n\r\n    return\r\n```", "@acvander, Thanks for the code but looks like code is incomplete to replicate the reported issue. Please provide more information to debug. Thanks", "@acvander, Any update on reproducible code snippet. Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36702, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "Hi,\r\n\r\nI keep hitting an import error shown below\r\n\r\nsetup is Windows 10 64 bit - Processor\tIntel(R) Core(TM) i7-7820HQ - GPU Nividia Quadro M1200, \r\n\r\nRunning Spyder version 4.0 (tried 4.01 also) with python 3.7.6 (also tried 3.8) via virtual env  with tensorflow 2.  Despite a weeks worth of various combinations I am having no luck. Any help much appreciated \r\n\r\nHome setup works fine but I need it working on my work PC. I have a local install as I don't have admin rights.\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\user.user\\Python 3.7.6 64bit\\3.7.6\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\user.user\\Python 3.7.6 64bit\\3.7.6\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\user.user\\Python 3.7.6 64bit\\3.7.6\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\user.user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\user.user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\nJust to sample over 100 similar issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\n\r\nPlease make sure you do a search in the future."]}, {"number": 36701, "title": "Fix: fixed LSTMBlockCell cuda kernel", "body": "Fixed bug in cuda kernel of LSTMBlockCell, implemented test which checks LSTMBlockCell gradients equality on cpu and gpu.", "comments": ["Thanks for the fix!"]}, {"number": 36700, "title": "tf2 isn't enabled in tensorflow_core.python.keras.layers.__init__", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.1\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: Not used\r\n- GPU model and memory: Not used\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhenever you import a layer using the path:\r\n `from tensorflow.python.keras.layers`\r\n\r\nIt will import the layer using the tensorflow 1.X behavior.\r\nWhich isn't the case when we use tensorflow.keras.layers.\r\n\r\nThe issue is that every networks from tf.keras.applications (resnet, densenet...) use those import which can lead to some severe bugs (e.g: BatchNormalization).\r\n\r\n**Describe the expected behavior**\r\nImporting from `from tensorflow.python.keras.layers` and `from tensorflow.keras.layers` should have exactly the same behavior (2.X).\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nfrom tensorflow.python.keras.layers import BatchNormalization as buggy_BN\r\nfrom tensorflow.keras.layers import BatchNormalization as good_BN\r\n\r\nprint(good_BN()._USE_V2_BEHAVIOR) # TRUE\r\nprint(buggy_BN()._USE_V2_BEHAVIOR) # FALSE\r\n```\r\n\r\n**Other info / logs**\r\n\r\nI think that the issue could be fix by changing this [tensorflow_core.python.tf2](https://github.com/tensorflow/tensorflow/blob/9bd55fcb645500a2c859cb3390f32b3a7c48327f/tensorflow/python/tf2.py#L43).\r\n\r\nfrom\r\n\r\n```python\r\ndef enabled():\r\n  \"\"\"Returns True iff TensorFlow 2.0 behavior should be enabled.\"\"\"\r\n  if _force_enable is None:\r\n    return os.getenv(\"TF2_BEHAVIOR\", \"0\") != \"0\"\r\n  else:\r\n    return _force_enable\r\n```\r\nto\r\n\r\n```python\r\ndef enabled():\r\n  \"\"\"Returns True iff TensorFlow 2.0 behavior should be enabled.\"\"\"\r\n  if _force_enable is None:\r\n    return os.getenv(\"TF2_BEHAVIOR\", \"1\") != \"0\"\r\n  else:\r\n    return _force_enable\r\n```\r\nit will make TF2_BEHAVIOR enabled by default", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/7097b0aa2d706811bc19d086206fd55a/36700.ipynb). Thanks!", "@EmGarr Importing using `tensorflow_core.` is not a suggested way for importing any module as mentioned [here](https://github.com/tensorflow/tensorflow/issues/32957#issuecomment-543819065) and [another resource](https://github.com/tensorflow/tensorflow/issues/33075#issuecomment-539070546). Thanks!", "@jvishnuvardhan I do not use this layer but however all the base model of tensorflow keras do use those imports. It means that whenever you use a Resnet50 (or others) you'll use the batch normalization from tf 1.X which has a totally different behavior for trainable is False. ", "> @jvishnuvardhan I do not use this layer but however all the base model of tensorflow keras do use those imports. It means that whenever you use a Resnet50 (or others) you'll use the batch normalization from tf 1.X which has a totally different behavior for trainable is False.\r\n\r\nI encountered this problem just yesterday when attempting to use transfer learning with pretrained models from tensorflow.keras.application-package. The validation loss and accuracy aren\u2019t progressing as one would expect (except with VGG16 and others which don\u2019t use BN). Still after loading the model weights I manually set this flag to be true for each BN layers and after that everything seemed to work as one would expect.", "@EmGarr we have encountered this issue and was able to solve this as mentioned [here](https://github.com/tensorflow/tensorflow/issues/36366#issuecomment-601985968)\r\n\r\nHowever this has to be fixed by tf team for good as all pretrtained keras models are referencing v1 layers and giving bad results", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36700\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36700\">No</a>\n", "this issue was fixed in this [commit](https://github.com/tensorflow/tensorflow/commit/410852dbd24899e22f0020f9fdc9757f527dda55) \r\n\r\nand the fix has been cherrypicked into r2.2 branch.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36700\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36700\">No</a>\n"]}, {"number": 36699, "title": "Fix typos in the base_layer input casting warning", "body": "", "comments": ["Thanks for the typo fixes! I didn't realize how often I used the wrong form of \"its\"."]}, {"number": 36698, "title": "[ROCm] Fix for a bug in the ROCm implementation for matrix_triangular_solve op", "body": "/cc @whchung @cheshire @chsigg ", "comments": []}, {"number": 36697, "title": "Training suddenly freezes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): unknown 2.0.0 (see below)\r\n\r\ntensorboard=2.0.0=pyhb38c66f_1\r\ntensorflow=2.0.0=gpu_py37h57d29ca_0\r\ntensorflow-base=2.0.0=gpu_py37h390e234_0\r\ntensorflow-estimator=2.0.0=pyh2649769_0\r\ntensorflow-gpu=2.0.0=h0d30ee6_0\r\ntensorflow-probability=0.8.0=py_0\r\n\r\n\r\n- Python version: Python 3.7.6 (Anaconda)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cudatoolkit=10.0.130=0 / cudnn=7.6.4=cuda10.0_0\r\n- GPU model and memory: Nvidia GeForce RTX 2070 8GB\r\n\r\n\r\n**Describe the current behavior**\r\nTraining a CNN using `tf.keras.Model.fit()` and tensorflow's data pipeline with tfrecord files randomly seems to freeze/stop/hang. Whenever this happens, the console process stays open but the CPU/GPU will return to 0% utilization. Using verbose=1 as well as tensorboard also shows that no progress is being made anymore.\r\n\r\nWaiting for a while does not help. However, if I restart the training without rebooting my PC, the hang is much more likely to occur again.\r\n\r\nLooking into Process explorer, I can see that there is only one really active thread:\r\n![grafik](https://user-images.githubusercontent.com/6770131/74343696-c0f13100-4dab-11ea-9858-e7bb70908107.png)\r\n\r\nA Callstack for the thread is attached below. The upper few frames (nvcuda.dll!cuProfilerStop) change when refreshing but the rest of the frame stays constant.\r\n\r\n\r\n**Describe the expected behavior**\r\nThe training should continue normally.\r\n\r\n\r\n**Code to reproduce the issue**\r\nSince I was not able to reproduce the problem reliably, I have no idea which part of my code might actually be important, The data pipeline uses `dataset.map()` with `num_parallel_calls = tf.data.experimental.AUTOTUNE` for multiprocessing and contains two `tf.np_function()`s (after which I have to use tf.ensure_shape to recover the correct shape for the data).\r\n\r\n**Other info / logs**\r\n\r\n```\r\nnvcuda.dll!cuProfilerStop+0x226617\r\nnvcuda.dll!cuProfilerStop+0x17243c\r\nnvcuda.dll+0x4bbbe\r\nnvcuda.dll+0x10c29d\r\nnvcuda.dll!cuProfilerStop+0x5cc42\r\nnvcuda.dll!cuProfilerStop+0x5da20\r\nnvcuda.dll+0x10bbf0\r\nnvcuda.dll+0x10be3c\r\nnvcuda.dll+0xeac0\r\nnvcuda.dll!cuCtxSynchronize+0x1c2\r\n_pywrap_tensorflow_internal.pyd!std::unique_ptr<tensorflow::Status::State,std::default_delete<tensorflow::Status::State> >::~unique_ptr<tensorflow::Status::State,std::default_delete<tensorflow::Status::State> >+0x10b4f\r\n_pywrap_tensorflow_internal.pyd!tensorflow::Env::NowSeconds+0xd16\r\n_pywrap_tensorflow_internal.pyd!tensorflow::AllocationRecord::Clear+0x550c\r\n_pywrap_tensorflow_internal.pyd!google::protobuf::RepeatedPtrField<tensorflow::InterconnectLink>::Add+0x8f0c\r\n_pywrap_tensorflow_internal.pyd!std::vector<tensorflow::DtypeAndPartialTensorShape,std::allocator<tensorflow::DtypeAndPartialTensorShape> >::operator=+0x623\r\n_pywrap_tensorflow_internal.pyd!TFE_TensorHandleResolve+0x227\r\n_pywrap_tensorflow_internal.pyd!tensorflow::DataTypeSet::Contains+0x2350\r\n_pywrap_tensorflow_internal.pyd!std::vector<tensorflow::monitoring::Point::Label,std::allocator<tensorflow::monitoring::Point::Label> >::reserve+0x85a\r\npython37.dll!PyMethodDef_RawFastCallKeywords+0x387\r\npython37.dll!PyMethodDef_RawFastCallKeywords+0xa5c\r\npython37.dll!PyEval_EvalFrameDefault+0x403\r\npython37.dll!PyFunction_FastCallDict+0xdd\r\npython37.dll!PyObject_FastCall_Prepend+0x6c\r\npython37.dll!PySet_Contains+0x50d\r\npython37.dll!PyErr_NoMemory+0x24eaf\r\npython37.dll!PyEval_SliceIndex+0x42\r\npython37.dll!PySlice_Unpack+0x9d\r\n_multiarray_umath.cp37-win_amd64.pyd+0xc117e\r\n_multiarray_umath.cp37-win_amd64.pyd+0xc0093\r\npython37.dll!PyEval_EvalFrameDefault+0x7e4\r\npython37.dll!PyEval_EvalCodeWithName+0x1a6\r\npython37.dll!PyFunction_FastCallDict+0x1ba\r\npython37.dll!PySlice_New+0x23d\r\npython37.dll!PyEval_EvalFrameDefault+0x1174\r\npython37.dll!PyEval_EvalCodeWithName+0x1a6\r\npython37.dll!PyFunction_FastCallDict+0x1ba\r\npython37.dll!PyObject_Call_Prepend+0x6c\r\npython37.dll!PyType_GetDocFromInternalDoc+0x22d\r\npython37.dll!PyObject_Call+0x75\r\n_pywrap_tensorflow_internal.pyd!std::default_delete<tensorflow::kernel_factory::OpKernelRegistrar::PtrOpKernelFactory>::operator()+0x97d\r\n_pywrap_tensorflow_internal.pyd!std::default_delete<tensorflow::kernel_factory::OpKernelRegistrar::PtrOpKernelFactory>::operator()+0x420\r\n_pywrap_tensorflow_internal.pyd!tensorflow::NodeDef::mutable_experimental_debug_info+0xef56\r\n_pywrap_tensorflow_internal.pyd!tensorflow::NodeDef::mutable_experimental_debug_info+0x11f78\r\n_pywrap_tensorflow_internal.pyd!tensorflow::data::DatasetBaseIterator::RecordElement+0x6f\r\n_pywrap_tensorflow_internal.pyd!Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop+0x3f6\r\n_pywrap_tensorflow_internal.pyd!Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop+0x701\r\n_pywrap_tensorflow_internal.pyd!tensorflow::WindowsFileSystem::TranslateName+0x255\r\n_pywrap_tensorflow_internal.pyd!tensorflow::SavedAsset::GetCachedSize+0x2c19\r\nucrtbase.dll!beginthreadex+0x142\r\nKERNEL32.DLL!BaseThreadInitThunk+0x14\r\nntdll.dll!RtlUserThreadStart+0x21\r\n```\r\n", "comments": ["@muellesi, Could you provide the standalone code to replicate the issue. ", "@muellesi, Is this still an issue?", "Unfortunately yes, but it only happens very sporadically and I was not yet able to reproduce the problem reliably. Therefore I also don't have any piece of standalone code that reliably reproduces the issue. However, I had the problem on three different PCs now. If I find a repro, I will report back.", "@muellesi, Any update on code snippet?", "same issue!", "I met this bug again!\r\nThe training is suddenly stop, **no errors, no exceptions, and no logs**.\r\n<img width=\"1541\" alt=\"\u622a\u5c4f2020-03-04\u4e0b\u53487 21 49\" src=\"https://user-images.githubusercontent.com/34032031/75874800-73696080-5e4d-11ea-9327-9f0a3c88724c.png\">\r\nThe training step is stop at `783`.\r\n\r\nI use the `GPU7` inside a docker container. Here is the outputs of `nvidia-smi`:\r\n<img width=\"753\" alt=\"\u622a\u5c4f2020-03-04\u4e0b\u53487 23 28\" src=\"https://user-images.githubusercontent.com/34032031/75874918-af042a80-5e4d-11ea-9769-7e0e1690f27d.png\">\r\n\r\nThe process is just exist, no any informations. \r\n", "@luozhouyang : Do you also use the tensorflow dataset pipline? And if so, do you also use np_function? Maybe we can find  a common factor between our cases and draw a conclusion on what might cause the problem.", "Yes, I construct a `tf.data.TFRecordDataset` from a list of `.tfrecord` files. \r\nI found that, if I set `dataset = dataset.shuffle(buffer_size=2000000)` then the training is more likely stop. But if I set the `buffer_size=1500000`, the training goes well. The usage of memory in both settings is about `16G/32G`, so it's not a OOM exception.", "@muellesi, Any update on reproducible code snippet? Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I'm having a similar issue. I am training a model with a fairly complex tf.data.datset pipeline with one tf.np_function. I use a map function that takes a batch and modifies it according to the other elements in the batch. The bigger the batch size, the easier it is to send the process to sleep at some random time. But even running the same script twice, it might have different outcomes. I'm using tf2.2 in a linux machine with 4 processors and 32GB of ram. When the process halts the memory usage is not too high ~20% ", "I have the same issue with TF 2.3.2 on Windows 10 with tf.data.Dataset, tf.keras.applications CNN models and model.fit() .\r\nThe training freeze/hang forever at a random batch inside an epoch. I need to restart my training over and over gain starting with latest checkpoint saved.  Sometime N epochs complete and it hangs at a random batch in the N+1 epochs. This is really annoying !\r\nFor info I use tf.py_function inside a tf.data.Dataset.map() call.", "I found this which might be useful https://github.com/tensorflow/tensorflow/issues/36072#issuecomment-664672783\r\nUPDATE : I solved this issue by only **updating my nvidia graphic card driver from version 431.70 to version 451.48**.\r\n[This new driver version is compatible with CUDA 10 and 11](https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility)."]}, {"number": 36696, "title": "UpSampling2D doesn't support bfloat16 in TF 2.1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5\r\n- GPU model and memory: NVIDIA Titan V, 12 GB\r\n\r\n**Describe the current behavior**\r\nTypeError: Value passed to parameter 'images' has DataType bfloat16 not in list of allowed values: int8, uint8, int16, uint16, int32, int64, float16, float32, float64\r\n\r\n**Describe the expected behavior**\r\nbfloat16 should work with this layer\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.keras.mixed_precision.experimental.set_policy('mixed_bfloat16')\r\n\r\noptimizer = tf.optimizers.SGD(learning_rate=0.1, momentum=0.9)\r\n\r\ninput=tf.keras.layers.Input(shape=(256, 256, 3))\r\n\r\nx=tf.keras.layers.Conv2D(32,(3,3))(input)\r\nx=tf.keras.layers.UpSampling2D()(x)\r\n\r\nx=tf.keras.layers.Conv2D(32,(3,3))(x)\r\nout=tf.keras.layers.Activation('sigmoid', dtype='float32')(x)\r\n\r\nmy_model = tf.keras.models.Model(inputs=input, outputs=out)\r\n\r\noptimizer = tf.keras.optimizers.RMSprop()\r\nmy_model.compile(loss='binary_crossentropy', optimizer=optimizer)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n> line 10, in\r\n> x=tf.keras.layers.UpSampling2D()(x)\r\n> File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 773, in call\r\n> outputs = call_fn(cast_inputs, *args, **kwargs)\r\n> File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\convolutional.py\", line 2004, in call\r\n> interpolation=self.interpolation)\r\n> File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 2782, in resize_images\r\n> x, new_shape, method=image_ops.ResizeMethod.NEAREST_NEIGHBOR)\r\n> File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\image_ops_impl.py\", line 1357, in resize_images_v2\r\n> skip_resize_if_same=False)\r\n> File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\image_ops_impl.py\", line 1133, in _resize_images_common\r\n> images = resizer_fn(images, size)\r\n> File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\image_ops_impl.py\", line 1337, in resize_fn\r\n> images_t, new_size, half_pixel_centers=True)\r\n> File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_image_ops.py\", line 3419, in resize_nearest_neighbor\r\n> name=name)\r\n> File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 576, in _apply_op_helper\r\n> param_name=input_name)\r\n> File \"C:\\Users\\mdlambe1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 61, in _SatisfiesTypeConstraint\r\n> \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\n> TypeError: Value passed to parameter 'images' has DataType bfloat16 not in list of allowed values: int8, uint8, int16, uint16, int32, int64, float16, float32, float64\r\n> ", "comments": ["@LambertMark I can reproduce the issue. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/2802ef0d49e7ebf66c5418375f8df6c2/tf36696.ipynb) is the colab for our reference. \r\n\r\n`tf.keras.layers.UpSampling2D` uses `tf.keras.backend.resize_images` that may not support `bfloat16`.  Let's find it. Thanks!", "This is fixed. You can try it with today's nightly build. Let me know if you still encounter any issues."]}, {"number": 36695, "title": "Function String Keras Attention Layer contains error/typo", "body": "It is a documentation issue, but embedded in the code. This seems most appropriate issue template.\r\n\r\n## URL(s) with the issue:\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/layers/dense_attention.py#L187-L313\r\n\r\n## Description of issue (what needs changing):\r\nThere is an error in the provided example (L236 - 276):\r\n\r\nHere is a code example for using `Attention` in a CNN+Attention network:\r\n  ```python\r\n  value_input = tf.keras.Input(shape=(None,), dtype='int32')\r\n  value_embeddings = token_embedding(query_input)\r\n  ```\r\n\r\nThis last one should be ```value_embeddings = token_embedding(value_input)```\r\n", "comments": ["This issue looks like it is resolved", "@joshz123 Thanks. Yes, it was updated in the [`master`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/dense_attention.py). \r\n\r\nAs it was resolved, I am closing this issue. Thanks!"]}, {"number": 36694, "title": "Different gradient results for same custom loss function in Tensorflow 2.0", "body": "Tensorflow 2.0 & Python 3.7\r\n\r\n```\r\ndef custom_loss2(output2, label2):\r\n    loss_value = K.mean(binary_crossentropy(label2, output2)) \r\n    return loss_value\r\ndef EWC_loss(new_weights, old_weights, fisher_matrix, rate):\r\n    sum_w = 0\r\n    for v in range(len(fisher_matrix)):\r\n        sum_w += tf.reduce_sum(tf.multiply(fisher_matrix[v], tf.square(new_weights[v] - old_weights[v]))) \r\n    return sum_w*rate\r\n```\r\n\r\nTwo different trains, the same results were expected but two very different results came out. \r\nFirst one, just let rate = 0.5\r\n\r\n```\r\ndel optimizer\r\ndel tape\r\noptimizer = tf.keras.optimizers.SGD()\r\newc_model = tf.keras.models.clone_model(model)\r\nold_weights = model.trainable_variables.copy()\r\nfor epoch in range(num_epochs):\r\n    with tf.GradientTape() as tape:\r\n        out = ewc_model(features_t)\r\n        new_weights = ewc_model.trainable_variables.copy()\r\n        ewc_loss = EWC_loss(new_weights, old_weights, fisher_matrix, 0.5)\r\n        loss = ewc_loss + custom_loss2(out, labels_2_t)\r\n        grad = tape.gradient(loss, ewc_model.trainable_variables)\r\n        optimizer.apply_gradients(grads_and_vars=zip(grad, ewc_model.trainable_variables))\r\n    if (epoch+1)%100 == 0:\r\n        print(\"epch: {}, loss: {}\".format(epoch, loss.numpy()))\r\n        print(ewc_loss.numpy(), loss.numpy())\r\n```\r\n\r\nIn case II, just put the factor out and let rate = 1.0 but put 0.5 to ewc_loss:\r\n\r\n```\r\ndel optimizer\r\ndel tape\r\noptimizer = tf.keras.optimizers.SGD()\r\newc_model = tf.keras.models.clone_model(model)\r\nold_weights = model.trainable_variables.copy()\r\nfor epoch in range(num_epochs):\r\n    with tf.GradientTape() as tape:\r\n        out = ewc_model(features_t)\r\n        new_weights = ewc_model.trainable_variables.copy()\r\n        ewc_loss = 0.5*EWC_loss(new_weights, old_weights, fisher_matrix, 1.0)\r\n        loss = ewc_loss + custom_loss2(out, labels_2_t)\r\n        grad = tape.gradient(loss, ewc_model.trainable_variables)\r\n        optimizer.apply_gradients(grads_and_vars=zip(grad, ewc_model.trainable_variables))\r\n    if (epoch+1)%100 == 0:\r\n        print(\"epch: {}, loss: {}\".format(epoch, loss.numpy()))\r\n        print(ewc_loss.numpy(), loss.numpy())\r\n```\r\n\r\nTests were with random samples, but the same for those test codes described above. \r\nWe evaluated the model using two test sets, loss = custom_loss2(prd, labels_2). \r\nThe first case returned:\r\n\r\n```\r\ntf.Tensor(14.144677747478463, shape=(), dtype=float64)\r\ntf.Tensor(14.254150624518838, shape=(), dtype=float64)\r\n```\r\n\r\nbut the case II returned:\r\n\r\n```\r\ntf.Tensor(0.15645654679566814, shape=(), dtype=float64)\r\ntf.Tensor(0.263113701303186, shape=(), dtype=float64)\r\n```\r\n\r\nCan anyone identify this issue? If the gradient was wrong, then the risk of applying tf.GradientTape() is very high using custom loss function.", "comments": ["@aGiant, I tried replicating the issue, but getting different error.\r\nPlease take a look at [gist](https://colab.sandbox.google.com/gist/gadagashwini/bc099589a92797d7f66dedad888e0433/untitled392.ipynb) and provide more information to reproduce issue. Thanks!", "@gadagashwini \r\nFor the first run, just delete optimizer and tape like\r\n```\r\n# del optimizer\r\n# del tape\r\n```\r\n\r\nModel was defined as:\r\n```\r\n# Input layer, one hidden layer\r\ninput_layer = Input(batch_shape=(None, 20))\r\ndense_1 = Dense(1028)(input_layer)\r\noutput_2 = Dense(1, activation=\"sigmoid\")(dense_1)\r\nmodel = Model(inputs=input_layer, outputs= output_2)\r\nprint(model.summary())\r\n```\r\nData was generated by \r\n```\r\nn_sample = 1000\r\nfix = np.array([range(n_sample),]*20).transpose()\r\nfeatures = np.cos(fix + np.random.rand(n_sample,20))\r\nlabels_2 = np.cos(np.array([range(n_sample),]*1).transpose() + np.random.rand(n_sample,1))\r\nlabels_2 = np.array([labels_2>=0]).astype(float)\r\n```", "> @aGiant, I tried replicating the issue, but getting different error.\r\n> Please take a look at [gist](https://colab.sandbox.google.com/gist/gadagashwini/bc099589a92797d7f66dedad888e0433/untitled392.ipynb) and provide more information to reproduce issue. Thanks!\r\n\r\nnow the error was re-produced: https://colab.research.google.com/gist/aGiant/7893ade6d21be98bebbeb91f69fc0b84/untitled392.ipynb", "I have tried on colab with TF version 2.2.0-rc2 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/e790680dd021f39d1daa0a8826c2a1ea/untitled765.ipynb). Thanks!", "@aGiant This small mismatch in the results is due to cloning model. When you use clone a model, it instantiate a model and it creates new weights. [check here](https://www.tensorflow.org/api_docs/python/tf/keras/models/clone_model). Print weights for each of those two cases, you can see different weights.\r\n\r\n> Model cloning is similar to calling a model on new inputs, except that it creates new layers (and thus new weights) instead of sharing the weights of the existing layers.\r\n\r\nIf you want exactly same results but wants to clone model, then you can use `set_weights` as follows.\r\n\r\n`ewc_model.set_weights(model.get_weights()) # add this line to use exactly same weights `\r\n\r\nI did exactly as mentioned above and got exactly same results from both the cases. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/25e15063ea6700582e37a51945a27f11/untitled765.ipynb).\r\n\r\nPlease close the issue if this was resolved for you. Thanks!\r\n", "> @aGiant This small mismatch in the results is due to cloning model. When you use clone a model, it instantiate a model and it creates new weights. [check here](https://www.tensorflow.org/api_docs/python/tf/keras/models/clone_model). Print weights for each of those two cases, you can see different weights.\r\n> \r\n> > Model cloning is similar to calling a model on new inputs, except that it creates new layers (and thus new weights) instead of sharing the weights of the existing layers.\r\n> \r\n> If you want exactly same results but wants to clone model, then you can use `set_weights` as follows.\r\n> \r\n> `ewc_model.set_weights(model.get_weights()) # add this line to use exactly same weights `\r\n> \r\n> I did exactly as mentioned above and got exactly same results from both the cases. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/25e15063ea6700582e37a51945a27f11/untitled765.ipynb).\r\n> \r\n> Please close the issue if this was resolved for you. Thanks!\r\n\r\nMany thanks for your reply!\r\nYes, I saw the solution there and explained the results. \r\n\r\nBut the task is not to use the same weights but to get the almost the same convergent from any initial start points. In the end, the difference (abs error) between the two approaches should small enough, i.e. <= 1e-6. But the re-production from https://colab.research.google.com/gist/ravikyram/e790680dd021f39d1daa0a8826c2a1ea/untitled765.ipynb, error = 0.11463253099176529 >>1e-6. \r\nMaybe, it's because of the SGD. ", "@aGiant In my experience I have never seen<<1e-6 difference when complex custom_loss functions are used. There could be many reasons why it is `error = 0.11463253099176529 >>1e-6.`.  It could be due to the custom_loss or random_weights generated at the cloning inference, or SGD, or random_samples, or data size, or numerical error etc. Thanks!\r\nHope this helps you. Thanks! ", "@jvishnuvardhan\r\nHere is another test using SGD with error <= 1e-6 with many different initial start points and they all got the same error delta within the same epochs = 22.\r\n![image](https://user-images.githubusercontent.com/25084782/78538987-24b73980-77f2-11ea-9144-9f56e16c8650.png)\r\n\r\nWith tensorflow 2.0, with SGD, same data, same setting till error<=1e-6, it took 151 \u00b1 n, with different initial start points, I got different epoch...\r\n![image](https://user-images.githubusercontent.com/25084782/78539005-2b45b100-77f2-11ea-829c-2c555112a07f.png)\r\n\r\n\r\n", "@aGiant Looks like this was resolved in recent `tf-nightly`. The losses from two approaches are same. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/7e5689d9d23203acd54f266b8e88a35d/untitled765.ipynb) and let us know whether it is resolved for you or not. \r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "Closing this issue as this was resolved in `tf-nightly`. Please let us know if you face the error again, we can reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36694\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36694\">No</a>\n"]}, {"number": 36693, "title": "Spelling of placeholder incorrect in line 53... :P", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://github.com/tensorflow/tensorflow/blob/r0.8/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\nthe spelling of placeholder is incorrect in line 53\r\n\r\n### Clear description\r\nthe spelling in placehoolder and should be placeholder\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nMaybe.. If you accept the request \r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@Spider8801 Could you please fill the template, share the issue code and log of error faced for us to help.", "Please assign this to me\r\n", "Okay\n\nOn Mon, Feb 17, 2020, 11:08 Ayushman Kumar <notifications@github.com> wrote:\n\n> Please assign this to me\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36693?email_source=notifications&email_token=AHVMHCNZM2N2FCO7KR5ZPK3RDIZWPA5CNFSM4KTYFNDKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEL5JJGA#issuecomment-586847384>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AHVMHCJU24QO3ETA47L5ZGTRDIZWPANCNFSM4KTYFNDA>\n> .\n>\n", "Moving this to closed status as its been addressed in #36811"]}, {"number": 36692, "title": "TFLiteConverter: segmentation fault when dilation is not (1,1) in CONV_2D", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): tf-nightly\r\n- TensorFlow version (use command below): v1.12.1-24656-g0ec37eb 2.2.0-dev20200212\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI have segmentaion fault when I run the script.\r\n\r\n**Describe the expected behavior**\r\nNo crashes.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy\r\nimport tensorflow\r\nimport tensorflow.keras\r\nimport tensorflow.lite\r\n\r\ndef representative_dataset_gen():\r\n    yield [numpy.random.uniform(low=-1, high=1, size=(1,28,28,16)).astype(numpy.float32)]\r\n\r\n\r\nmodel=tensorflow.keras.Sequential()\r\nmodel.add(\r\n    tensorflow.keras.layers.Conv2D(\r\n        filters=16, kernel_size=7, dilation_rate=(2,2), input_shape=(28,28,16),\r\n        use_bias=True, bias_initializer='ones'\r\n    )\r\n)\r\n\r\nconverter = tensorflow.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tensorflow.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tensorflow.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.experimental_new_converter = False\r\n\r\ntflite_model = converter.convert()\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nIf I set experimental_new_converter = True, it works\r\nIf I set use_bias = False, it works\r\nIf I set dilation_rate=(1,1), it works\r\n\r\nLogs:\r\n\r\n2020-02-12 11:18:37.697439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N \r\n2020-02-12 11:18:37.701589: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-02-12 11:18:37.701631: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 12 nodes (-7), 13 edges (-2), time = 0.839ms.\r\n2020-02-12 11:18:37.701648: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 12 nodes (0), 13 edges (0), time = 0.249ms.\r\nWARNING:absl:Please consider switching to use new converter by setting experimental_new_converter to true. Old converter (TOCO) is deprecated and flow will be switched on by default to use new converter soon.\r\nTraceback (most recent call last):\r\n  File \"/home/elezhe01/work/scripts/test_crash.py\", line 24, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/elezhe01/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 513, in convert\r\n    **converter_kwargs)\r\n  File \"/home/elezhe01/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py\", line 496, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/elezhe01/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py\", line 227, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-02-12 11:18:39.730113: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 6 operators, 13 arrays (0 quantized)\r\n2020-02-12 11:18:39.730252: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 6 operators, 13 arrays (0 quantized)\r\nFatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x00007f8f78473740 (most recent call first):\r\n  File \"/home/elezhe01/.local/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56 in execute\r\n  File \"/home/elezhe01/.local/lib/python3.6/site-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/home/elezhe01/.local/lib/python3.6/site-packages/absl/app.py\", line 299 in run\r\n  File \"/home/elezhe01/.local/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40 in run\r\n  File \"/home/elezhe01/.local/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93 in main\r\n  File \"/home/elezhe01/.local/bin/toco_from_protos\", line 8 in <module>\r\nSegmentation fault (core dumped)\r\n", "comments": ["@amahendrakar The version of TF is actually 2.2\r\nI put output of the command v1.12.1-24656-g0ec37eb 2.2.0-dev20200212 - TF Git version is the first number and the second is the version of TF", "@wwwind,\r\nSorry for the incorrect label, fixed it.\r\nI was able to reproduce the issue on [TF-nightly](https://colab.sandbox.google.com/gist/amahendrakar/0c0c73823cc734da006e2af211424186/36692_nightly.ipynb) v2.2.0-dev20200212. However, the code runs on [TF 2.1](https://colab.sandbox.google.com/gist/amahendrakar/913ccc0dcca84a1d51189b3c76f34d7f/36692.ipynb) without any issues. Please find the attached Gist. Thanks!", "@amahendrakar Thanks for the investigation! \r\nWe are using tf-night, so I would like to confirm: it will be fixed in the tf-night, right ?", "This change https://github.com/tensorflow/tensorflow/pull/36856 fixed the crash", "@wwwind,\r\nIs this still an issue? Please feel free to close the issue if it is resolved. Thanks!", "No, the fix has been merged."]}, {"number": 36691, "title": "_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN10tensorflow8OpKernelE", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 16.04.6`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `N/A`\r\n- TensorFlow installed from (source or binary): `source`\r\n- TensorFlow version: `2.1.0 (master)`\r\n- Python version: `Python 3.5.2`\r\n- Installed using virtualenv? pip? conda?: `apt`\r\n- Bazel version (if compiling from source): `1.2.1`\r\n- GCC/Compiler version (if compiling from source): `gcc 5.4.0`\r\n- CUDA/cuDNN version: `CUDA 10.2 / cuDNN 7.6.5.32-1+cuda10.2`\r\n- GPU model and memory: `NVIDIA Tesla V100`\r\n\r\n\r\n\r\n**Describe the problem**\r\nObserved after 00befcdeb87f1fc490d247d127ee438f63fe3666 commit.\r\nTensorFlow building fails with the error:\r\n```\r\n05:24:13  ./tensorflow/c/eager/tape.h: In instantiation of 'tensorflow::Status tensorflow::eager::{anonymous}::InitialGradients(const tensorflow::eager::VSpace<Gradient, BackwardFunction, TapeTensor>&, tensorflow::gtl::ArraySlice<long long int>, const std::unordered_map<long long int, TapeTensor>&, tensorflow::gtl::ArraySlice<Gradient*>, const TensorTape&, tensorflow::eager::OpTape<BackwardFunction, TapeTensor>&, std::unordered_map<long long int, std::vector<LhsScalar*> >*) [with Gradient = _object; BackwardFunction = std::function<_object*(_object*, const std::vector<long long int>&)>; TapeTensor = PyTapeTensor; tensorflow::gtl::ArraySlice<long long int> = absl::Span<const long long int>; tensorflow::gtl::ArraySlice<Gradient*> = absl::Span<_object* const>; tensorflow::eager::TensorTape = std::unordered_map<long long int, long long int>; tensorflow::eager::OpTape<BackwardFunction, TapeTensor> = std::unordered_map<long long int, tensorflow::eager::OpTapeEntry<std::function<_object*(_object*, const std::vector<long long int>&)>, PyTapeTensor>, std::hash<long long int>, std::equal_to<long long int>, std::allocator<std::pair<const long long int, tensorflow::eager::OpTapeEntry<std::function<_object*(_object*, const std::vector<long long int>&)>, PyTapeTensor> > > >]':\r\n05:24:13  ./tensorflow/c/eager/tape.h:663:30:   required from 'tensorflow::Status tensorflow::eager::GradientTape<Gradient, BackwardFunction, TapeTensor>::ComputeGradient(const tensorflow::eager::VSpace<Gradient, BackwardFunction, TapeTensor>&, tensorflow::gtl::ArraySlice<long long int>, tensorflow::gtl::ArraySlice<long long int>, const std::unordered_map<long long int, TapeTensor>&, tensorflow::gtl::ArraySlice<Gradient*>, std::vector<LhsScalar*>*) [with Gradient = _object; BackwardFunction = std::function<_object*(_object*, const std::vector<long long int>&)>; TapeTensor = PyTapeTensor; tensorflow::gtl::ArraySlice<long long int> = absl::Span<const long long int>; tensorflow::gtl::ArraySlice<Gradient*> = absl::Span<_object* const>]'\r\n05:24:13  tensorflow/python/eager/pywrap_tfe_src.cc:2566:27:   required from here\r\n05:24:13  ./tensorflow/c/eager/tape.h:576:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n05:24:13     for (int i = 0; i < target_tensor_ids.size(); ++i) {\r\n05:24:13                       ^\r\n05:24:13  ./tensorflow/c/eager/tape.h:588:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n05:24:13           for (int j = 0; j < op_it->second.output_tensor_info.size(); ++j) {\r\n05:24:13                             ^\r\n05:24:52  ERROR: /scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/tensorflow/python/keras/api/BUILD:130:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)\r\n05:24:52  Traceback (most recent call last):\r\n05:24:52    File \"/home/swx-jenkins/.cache/bazel/_bazel_swx-jenkins/2c3650f385da66a5652cb752baf8a83c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n05:24:52      from tensorflow.python.pywrap_tensorflow_internal import *\r\n05:24:52    File \"/home/swx-jenkins/.cache/bazel/_bazel_swx-jenkins/2c3650f385da66a5652cb752baf8a83c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n05:24:52      _pywrap_tensorflow_internal = swig_import_helper()\r\n05:24:52    File \"/home/swx-jenkins/.cache/bazel/_bazel_swx-jenkins/2c3650f385da66a5652cb752baf8a83c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n05:24:52      _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n05:24:52    File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n05:24:52      return load_dynamic(name, filename, file)\r\n05:24:52    File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n05:24:52      return _load(spec)\r\n05:24:52  ImportError: /home/swx-jenkins/.cache/bazel/_bazel_swx-jenkins/2c3650f385da66a5652cb752baf8a83c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN10tensorflow8OpKernelE\r\n05:24:52  \r\n05:24:52  During handling of the above exception, another exception occurred:\r\n05:24:52  \r\n05:24:52  Traceback (most recent call last):\r\n05:24:52    File \"/home/swx-jenkins/.cache/bazel/_bazel_swx-jenkins/2c3650f385da66a5652cb752baf8a83c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n05:24:52      from tensorflow.python.tools.api.generator import doc_srcs\r\n05:24:52    File \"/home/swx-jenkins/.cache/bazel/_bazel_swx-jenkins/2c3650f385da66a5652cb752baf8a83c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 50, in <module>\r\n05:24:52      from tensorflow.python import pywrap_tensorflow\r\n05:24:52    File \"/home/swx-jenkins/.cache/bazel/_bazel_swx-jenkins/2c3650f385da66a5652cb752baf8a83c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 69, in <module>\r\n05:24:52      raise ImportError(msg)\r\n05:24:52  ImportError: Traceback (most recent call last):\r\n05:24:52    File \"/home/swx-jenkins/.cache/bazel/_bazel_swx-jenkins/2c3650f385da66a5652cb752baf8a83c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n05:24:52      from tensorflow.python.pywrap_tensorflow_internal import *\r\n05:24:52    File \"/home/swx-jenkins/.cache/bazel/_bazel_swx-jenkins/2c3650f385da66a5652cb752baf8a83c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n05:24:52      _pywrap_tensorflow_internal = swig_import_helper()\r\n05:24:52    File \"/home/swx-jenkins/.cache/bazel/_bazel_swx-jenkins/2c3650f385da66a5652cb752baf8a83c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n05:24:52      _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n05:24:52    File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n05:24:52      return load_dynamic(name, filename, file)\r\n05:24:52    File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n05:24:52      return _load(spec)\r\n05:24:52  ImportError: /home/swx-jenkins/.cache/bazel/_bazel_swx-jenkins/2c3650f385da66a5652cb752baf8a83c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN10tensorflow8OpKernelE\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n.tf_configure.bazelrc:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python3.5/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env TF_CUDA_VERSION=\"10.2\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_NCCL_VERSION=\"2.6.0\"\r\nbuild --action_env TF_CUDA_PATHS=\"/hpc/local/oss/cuda10.2/cuda-toolkit,/usr,/usr/local/cuda\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/hpc/local/oss/cuda10.2/cuda-toolkit\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr\"\r\nbuild --action_env NCCL_INSTALL_PATH=\"<cut>/nccl/stable\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"7.0\"\r\nbuild --action_env LD_LIBRARY_PATH=\"<cut>/nccl/stable/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/nccl_rdma_sharp_plugin/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/ucx/lib/ucx:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/ucx/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/sharp/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/hcoll/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/ompi/lib:/hpc/local/oss/cuda10.2/cuda-toolkit/lib64:/hpc/local/oss/cuda10.2/cuda-toolkit/lib64/stubs:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc-5\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_env=LD_LIBRARY_PATH\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\nCC @av8ramit", "comments": ["Can you check if https://github.com/tensorflow/tensorflow/commit/c4c8b17313e3a1dbdfb6ca0c5702d247a26604c5 fixes it for you? I've been able to reproduce and fix that issue on an 18.04 docker container with gcc5.", "Thanks, I'll try it but seems there's another building regression (need to double check it):\r\n```\r\n23:10:11  DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\n23:10:11  DEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at /home/swx-jenkins/.cache/bazel/_bazel_swx-jenkins/2c3650f385da66a5652cb752baf8a83c/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n23:10:11   - /home/swx-jenkins/.cache/bazel/_bazel_swx-jenkins/2c3650f385da66a5652cb752baf8a83c/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n23:10:11   - /scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/WORKSPACE:37:1\r\n23:10:12  Loading: 0 packages loaded\r\n23:10:12      currently loading: tensorflow/tools/pip_package\r\n23:10:19  INFO: Call stack for the definition of repository 'local_config_cuda' which is a cuda_configure (rule definition at /scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/third_party/gpus/cuda_configure.bzl:1183:18):\r\n23:10:19   - /scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/tensorflow/workspace.bzl:87:5\r\n23:10:19   - /scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/WORKSPACE:19:1\r\n23:10:19  Loading: 0 packages loaded\r\n23:10:19      currently loading: tensorflow/tools/pip_package\r\n23:10:19  ERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n23:10:19     Traceback (most recent call last):\r\n23:10:19  \tFile \"/scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1181\r\n23:10:19  \t\t_create_local_cuda_repository(<1 more arguments>)\r\n23:10:19  \tFile \"/scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/third_party/gpus/cuda_configure.bzl\", line 930, in _create_local_cuda_repository\r\n23:10:19  \t\tcuda_lib_outs.append(<1 more arguments>)\r\n23:10:19  \tFile \"/scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/third_party/gpus/cuda_configure.bzl\", line 930, in cuda_lib_outs.append\r\n23:10:19  \t\t_basename(repository_ctx, <1 more arguments>)\r\n23:10:19  \tFile \"/scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/third_party/gpus/cuda_configure.bzl\", line 930, in _basename\r\n23:10:19  \t\tpath.basename\r\n23:10:19  object of type 'string' has no field 'basename'\r\n23:10:19  ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n23:10:19  \tFile \"/scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1181\r\n23:10:19  \t\t_create_local_cuda_repository(<1 more arguments>)\r\n23:10:19  \tFile \"/scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/third_party/gpus/cuda_configure.bzl\", line 930, in _create_local_cuda_repository\r\n23:10:19  \t\tcuda_lib_outs.append(<1 more arguments>)\r\n23:10:19  \tFile \"/scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/third_party/gpus/cuda_configure.bzl\", line 930, in cuda_lib_outs.append\r\n23:10:19  \t\t_basename(repository_ctx, <1 more arguments>)\r\n23:10:19  \tFile \"/scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/third_party/gpus/cuda_configure.bzl\", line 930, in _basename\r\n23:10:19  \t\tpath.basename\r\n23:10:19  object of type 'string' has no field 'basename'\r\n23:10:19  WARNING: Target pattern parsing failed.\r\n23:10:19  ERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n23:10:19  \tFile \"/scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1181\r\n23:10:19  \t\t_create_local_cuda_repository(<1 more arguments>)\r\n23:10:19  \tFile \"/scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/third_party/gpus/cuda_configure.bzl\", line 930, in _create_local_cuda_repository\r\n23:10:19  \t\tcuda_lib_outs.append(<1 more arguments>)\r\n23:10:19  \tFile \"/scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/third_party/gpus/cuda_configure.bzl\", line 930, in cuda_lib_outs.append\r\n23:10:19  \t\t_basename(repository_ctx, <1 more arguments>)\r\n23:10:19  \tFile \"/scrap/jenkins/workspace/_ML_DevOps_team/ml-tensorflow-ci-pipeline/tensorflow/third_party/gpus/cuda_configure.bzl\", line 930, in _basename\r\n23:10:19  \t\tpath.basename\r\n23:10:19  object of type 'string' has no field 'basename'\r\n```", "Sorry for the inconvenience @artemry-mlnx I'll try and see what I can do about that right away.", "That should be fixed now as well: https://github.com/tensorflow/tensorflow/commit/a15fe8f6066932e53c171519a011282cdb6ea451", "@av8ramit \r\nThanks a lot!\r\nTensorFlow is built successfully now, but horovod installation with `pip install horovod` fails now (we use it for [TF CNN benchmark](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks) test). I'm investigating the issue - may the above commits cause such error?\r\n```\r\n06:26:48    2020-02-15 03:26:38.528006: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\r\n06:26:48    INFO: Unable to build TensorFlow plugin, will skip it.\r\n06:26:48    \r\n06:26:48    Traceback (most recent call last):\r\n06:26:48      File \"/tmp/pip-install-e8noyyjq/horovod/setup.py\", line 254, in get_tf_flags\r\n06:26:48        return tf.sysconfig.get_compile_flags(), tf.sysconfig.get_link_flags()\r\n06:26:48      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/sysconfig.py\", line 65, in get_compile_flags\r\n06:26:48        flags.append('-I%s' % get_include())\r\n06:26:48      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/sysconfig.py\", line 43, in get_include\r\n06:26:48        return _os_path.join(_os_path.dirname(tf.__file__), 'include')\r\n06:26:48    AttributeError: module 'tensorflow_core' has no attribute '__file__'\r\n06:26:48    \r\n06:26:48    During handling of the above exception, another exception occurred:\r\n06:26:48    \r\n06:26:48    Traceback (most recent call last):\r\n06:26:48      File \"/tmp/pip-install-e8noyyjq/horovod/setup.py\", line 1408, in build_extensions\r\n06:26:48        build_tf_extension(self, options)\r\n06:26:48      File \"/tmp/pip-install-e8noyyjq/horovod/setup.py\", line 869, in build_tf_extension\r\n06:26:48        build_ext, options['COMPILE_FLAGS'])\r\n06:26:48      File \"/tmp/pip-install-e8noyyjq/horovod/setup.py\", line 257, in get_tf_flags\r\n06:26:48        tf_include_dirs = get_tf_include_dirs()\r\n06:26:48      File \"/tmp/pip-install-e8noyyjq/horovod/setup.py\", line 182, in get_tf_include_dirs\r\n06:26:48        tf_inc = tf.sysconfig.get_include()\r\n06:26:48      File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/sysconfig.py\", line 43, in get_include\r\n06:26:48        return _os_path.join(_os_path.dirname(tf.__file__), 'include')\r\n06:26:48    AttributeError: module 'tensorflow_core' has no attribute '__file__'\r\n```", "That error seems unrelated. I'd recommend filing a new issue or seeing if another exists. I'll also keep a lookout. Closing this for now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36691\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36691\">No</a>\n", "Thanks, it looks like `pip install horovod` related error has been fixed."]}, {"number": 36690, "title": "Support cross compilation of TFLite pip package", "body": "This pull request add the support of cross compiling variables.\r\nIt allow to cross compile TFLite library and python module for platforms other than aarch64, Rpi and iOS.\r\nIt has been successfully tested with Yocto Openembedded environment to build TensorFlow Lite for the new STM32MP1 Microprocessor target from STMicroelectronics.\r\n\r\nIt also remove some bashisms in pip build script that prevent to build TFLite in server machine with a dash shell. \r\n\r\nBR\r\nVincent", "comments": ["Hi,\r\n\r\nThe build issue has been encounter in different build server on which the host machine has the dash shell set by default.\r\nIn that case, the execution of the lite/tools/pip_package/build_pip_package.sh script is failing with the following error:\r\nbuild_pip_package.sh: 22:  Bad substitution\r\n\r\nIn this patch, I  am just focusing on the build_pip_package.sh because it is the one that is used from the Yocto environment build system to build the python package.\r\n\r\nVincent", "The first line of build_pip_package.sh is \"#!/usr/bin/env bash\".\r\nCan you install bash on the build server?\r\n\r\n> Hi,\r\n> \r\n> The build issue has been encounter in different build server on which the host machine has the dash shell set by default.\r\n> In that case, the execution of the lite/tools/pip_package/build_pip_package.sh script is failing with the following error:\r\n> build_pip_package.sh: 22: Bad substitution\r\n> \r\n> In this patch, I am just focusing on the build_pip_package.sh because it is the one that is used from the Yocto environment build system to build the python package.\r\n> \r\n> Vincent\r\n\r\n", "I understand your point.\r\nI will in that case abandoned the first patch and update the pull request.\r\nVincent", "@vinceab Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned a new patch has been uploaded. that should fix Ubuntu Sanity errors. "]}, {"number": 36689, "title": "libtensorfow-lite.a linking issue", "body": "This pull request fix issues encounter while linking the generated libtensorflow-lite.a into a C/C++ application.\r\nMultiple symbols where not defined such as:\r\n- type_erased.cc:(.text+0x36c): undefined reference to 'absl::Mutex::Lock()\r\n- rfft2d.cc:(.text+0x594): undefined reference to 'rdft2d'\r\n\r\nabsl/hash and absl/flags that where referencing absl/synchronization have been removed from the build and fft2d/fftsg2d.c has been added to the build.\r\n\r\nThis has been successfully tested against the STM32MP1 OpenSTLinux distribution.\r\n\r\nBR\r\nVincent", "comments": ["Patches updated.\r\ntensorflow/lite/tools/make/downloads/absl/absl/hash/internal/print_hash_of.cc has been removed from CORE_CC_EXCLUDE_SRCS.", "@terryheo  Can it be backported to r1.15 ?", "@apivovarov could you share which script is failing on r1.15?", "If you app uses whole `libtensorflow-lite.a` then the linking will fail with the following error\r\n```\r\n$ g++ -I /opt/tensorflow-1.15 \\\r\nmyapp.cc \\\r\n-Wl,--whole-archive \\\r\n/opt/tensorflow-1.15/tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a \\\r\n-Wl,--no-whole-archive \\\r\n-lpthread\r\n/opt/tensorflow-1.15/tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a(rfft2d.o): In function `tflite::ops::custom::rfft2d::Rfft2dImpl(int, int, double**, int*, double*)':\r\nrfft2d.cc:(.text+0x730): undefined reference to `rdft2d'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\nWe need whole archive because app fails at runtime if we do no use the whole archive.\r\nI guess it is nothing wrong with using all objects from `libtensorflow-lite.a` archive in `myapp`\r\n", "confirmed. I'll work no it.", "`libtensorflow-lite.a` which is built from r2.1 contains `.o` files with `main()` function.\r\n```\r\nprint_hash_of.o\r\ntune_tool.o\r\n```\r\nAs a result linking user's program with tflite archive results in `multiple definition of 'main'` (we use `-Wl,--whole-archive` linker option for `libtensorflow-lite.a`)\r\n\r\n```\r\ntensorflow/lite/tools/make/downloads/absl/absl/hash/internal/print_hash_of.cc\r\ntensorflow/lite/experimental/ruy/tune_tool.cc\r\n```\r\nThe issue was fixed in r2.2 (should it be backported to r2.1?)\r\n\r\nfix for `tune_tool.cc` and `print_hash_of.cc`\r\nhttps://github.com/tensorflow/tensorflow/commit/b66f1593e8c4331890dff94ada5e13fbb2ac893b\r\n\r\nanother fix for `print_hash_of.cc`\r\nhttps://github.com/tensorflow/tensorflow/commit/ccbbc5846c8fadd8dfda27d1b4b47eeb496b0a78", "I've created PR https://github.com/tensorflow/tensorflow/pull/39773 for r2.1 branch\r\n"]}, {"number": 36687, "title": "tflite object detection predictions are always same irrespective of input", "body": "Hi have a tensorflow tflite model that i am trying to deploy on an android device, the issue is that the outputs of the model are always same. Irrespective of input.\r\n\r\nCan anyone help ?\r\n\r\n\r\nHere is the link to that model: https://esriis-my.sharepoint.com/:u:/g/personal/san10428_esri_com/EbO-1fGDczRKnW9E2-hDhbQB17aufX_se-VdMBZrhaid3A?e=UKRiyO\r\n\r\nHere is the link to the android project: https://esriis-my.sharepoint.com/:u:/g/personal/san10428_esri_com/ES17ARjqhcdEnvgNlpq3O5MBaDOtHN1H7kk3Q75Y9M5VTA?e=IzemCE\r\n\r\n\r\ntensorflow version: 2.0.0\r\n", "comments": ["In my gradle build settings i updated the version of tensorflow binary to 2.0.0 and it is working fine now, strange things. If something is wrong it should raise and error instead of predicting random values."]}, {"number": 36686, "title": "Dataset padded_batch fails with InvalidArgumentError", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n```\r\nrt = tf.ragged.constant([[1,2,3], [2], [1,2,3,4], [1,1]])\r\nds = tf.data.Dataset.from_tensor_slices(rt)\r\nds.padded_batch(2, padded_shapes=[None])\r\n```\r\nFails with the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/vas/anaconda3/envs/text-ai/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 1481, in padded_batch\r\n    drop_remainder)\r\n  File \"/home/vas/anaconda3/envs/text-ai/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 3858, in __init__\r\n    output_shapes=structure.get_flat_tensor_shapes(self._structure))\r\n  File \"/home/vas/anaconda3/envs/text-ai/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\", line 4091, in padded_batch_dataset_v2\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/vas/anaconda3/envs/text-ai/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 6606, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Mismatched type between padding value 0 and input dataset's component 0: int32 vs. variant [Op:PaddedBatchDatasetV2]\r\n```\r\nHowever:\r\n```\r\nds.map(lambda x: x).padded_batch(2, padded_shapes=[None])\r\n```\r\nworks as expected", "comments": ["I could replicate the issue with Tf 2.1 and Latest tf-nightly.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/72fc06af220d44e2f7a1bd9208528b7a/untitled391.ipynb). Thanks!", "@edloper could you please take a look? As far as I can tell, this is related to boxing ragged tensors in a variant so that they can be batched/unbatched. Thanks.", "I'll look into what the problem is.  But in the meantime, note that if you don't actually want/need padding added, you can use [tf.data.experimental.dense_to_ragged_batch](https://www.tensorflow.org/api_docs/python/tf/data/experimental/dense_to_ragged_batch) to batch items into ragged tensors:\r\n\r\n```\r\nrt = tf.ragged.constant([[1,2,3], [2], [1,2,3,4], [1,1]])\r\nds = tf.data.Dataset.from_tensor_slices(rt)\r\nds2 = ds.apply(tf.data.experimental.dense_to_ragged_batch(2))\r\nfor element in ds2:\r\n  print(element)\r\n# <tf.RaggedTensor [[1, 2, 3], [2]]>\r\n# <tf.RaggedTensor [[1, 2, 3, 4], [1, 1]]>\r\n```\r\n\r\n", "Ok, here's a bit more information:\r\n\r\n* A DataSet's elements can be encoded as Tensors or RaggedTensors.  If they're encoded as RaggedTensors, then they have a ragged_rank indicating how many ragged dimensions they have.  You can check how elements are encoded by printing `my_dataset.element_spec`.\r\n\r\n* When you use `ds = tf.data.Dataset.from_tensor_slices(rt)`, you construct a DataSet whose elements are encoded RaggedTensors with `ragged_rank = rt.ragged_rank-1`.  In your example, `rt` has one ragged dimension, so the elements in the dataset `ds` are RaggedTensors with `ragged_rank=0`.\r\n\r\n* If a DataSet's elements are RaggedTensors, then you can batch them, even if they don't have the same size.  The result will be a RaggedTensor with one more ragged dimension.  So you can e.g. do:\r\n\r\n  ```\r\n  >>> ds = tf.data.Dataset.from_tensor_slices(rt)\r\n  >>> # (elements of ds are stored as RaggedTensors w/ ragged_rank=0)\r\n  >>> ds2 = ds.batch(2) # elements are RaggedTensors \r\n  >>> # (elements of ds2 are stored as RaggedTensors w/ ragged_rank=1, and 2 rows)\r\n  ```\r\n\r\n  I.e., you don't need to use `tf.data.experimental.dense_to_ragged_batch` in this context -- you can just use `ds.batch()`.\r\n\r\n* When you use `ds2 = ds.map(lambda: lambda)`, it converts RaggedTensors w/ `ragged_rank=0` (i.e., with no ragged dimensions) into plain Tensors:\r\n\r\n  ```\r\n  >>> ds = tf.data.Dataset.from_tensor_slices(rt)\r\n  >>> print(ds.element_spec)\r\n  RaggedTensorSpec(TensorShape([Dimension(None)]), tf.int32, 0, tf.int64)\r\n\r\n  >>> ds2 = ds.map(lambda x: x)\r\n  >>> print(ds2.element_spec)\r\n  TensorSpec(shape=(?,), dtype=tf.int32, name=None)\r\n  ```\r\n\r\n* `tf.data.experimental.dense_to_ragged_batch` is mostly useful if you have a dataset whose elements are encoded as Tensors, and you want to batch them into RaggedTensors.  If your elements are already encoded as RaggedTensors, you can just use batch().\r\n \r\n* `padded_batch` does not currently support RaggedTensors (or SparseTensors).  It requires that the dataset's elements be Tensors.\r\n\r\n* The reason that `ds.map(lambda x: x).padded_batch(2, padded_shapes=[None])` works is that the `map` converts the RaggedTensor elements into Tensor elements, and padded_batch is happy since its inputs have Tensor elements.\r\n\r\nFor now, we should probably add a check to `PaddedBatchDataset` that checks if any inputs are ragged, and if so, raises an exception.  (It looks like there's already an exception raised if any inputs are sparse).  In the future, we might want to update PaddedBatchDataset to support ragged tensors -- not sure from a brief look at the code how hard that would be.\r\n\r\nBut work-arounds are possible:\r\n\r\n1. If your original dataset's elements are encoded using RaggedTensors w/ ragged_rank=0, then you can use `ds.map(lambda x: x).padded_batch(...)`.\r\n\r\n2. If your original dataset's elements are encoded using RaggedTensors w/ any ragged_rank, then you can use: `ds.batch(...).map(lambda x: x.to_tensor())`", "I just want to point out that Keras expects a `tf.data.Dataset.zip((x, y))` object where the inputs match the named tensors in the Dataset x and the outputs match the named tensors in y.\r\nSo you have to do something like\r\n\r\n```python\r\ndef jesus_harold_christ(x, y):\r\n    return (x, y)\r\n\r\nds = tf.data.Dataset.zip((x, y))\r\nds = ds.shuffle()\r\nds = ds.map(jesus_harold_christ).padded_batch(...)\r\n```", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/bce5af439667785862d19ef3535dcade/35650.ipynb). Thanks!", "Since padded batch with ragged tensors is unsupported, you can convert the ragged tensor to uniform tensor and then perform padded batch like below.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nrt = tf.ragged.constant([[1,2,3], [2], [1,2,3,4], [1,1]])\r\nrt = rt.to_tensor(default_value=0)\r\nds = tf.data.Dataset.from_tensor_slices(rt)\r\nA = ds.padded_batch(2, padded_shapes=[None])\r\nfor element in A.as_numpy_iterator():\r\n  print(element)\r\n```\r\nOutput:\r\n\r\n```\r\n[[1 2 3 0]\r\n [2 0 0 0]]\r\n[[1 2 3 4]\r\n [1 1 0 0]]\r\n```\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36686\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36686\">No</a>\n"]}, {"number": 36685, "title": "Doc example update", "body": "Wrong variable called in example, query_input -> value_input", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36685) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36685) for more info**.\n\n<!-- ok -->", "Please open against master.", "@grofte closing this PR. As mentioned by @mihaimaruseac, @qlzh727, please open the PR against the master branch. Thanks!"]}, {"number": 36684, "title": "Failed to run visualize.py (tflite model visualizer)", "body": "python visualize.py\r\nTraceback (most recent call last):\r\n File \"visualize.py\", line 30, in <module>\r\n from tensorflow.lite.python import schema_py_generated as schema_fb\r\nImportError: cannot import name schema_py_generated\r\n\r\nThanks for your help!\r\n", "comments": ["@lizhen2017 \r\nPlease, provide the exact sequence of commands / code that you executed before running into the problem. Thanks!", "Please use https://www.tensorflow.org/lite/guide/faq#how_do_i_inspect_a_tflite_file", "> Please use https://www.tensorflow.org/lite/guide/faq#how_do_i_inspect_a_tflite_file\r\nAs mentioned I tried:\r\nbazel run //tensorflow/lite/tools:visualize model.tflite visualized_model.html\r\nbut it gave me an error: Linking of rule '@flatbuffers//:flatc' failed (Exit 1). PFA: screenshot\r\n![tf_lite_visualization_issue](https://user-images.githubusercontent.com/28675926/75039059-da962500-54dd-11ea-80e4-694eb21d76ba.png)\r\n", "same problem, any process is there?\r\nI checked the source codes of tensorflow but I cannot find schema_py_generated file under tensorfow.lite.python folder", "Did you follow https://www.tensorflow.org/install/source before using the visualizer?", "I get the error message:\r\nAttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'\r\nI have installed from source using docker.\r\nPlease help @terryheo  ", "@sujaybabruwad plz share commands you used.", "I'm also getting the same error \r\n`AttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'`\r\nwhen attempting to run visualize.py on a .tflite model.", "This is the same issue with https://github.com/tensorflow/tensorflow/issues/41846 which is still under investigating."]}, {"number": 36683, "title": "DLL load failed: The specified module could not be found.", "body": "Warning:\r\nThis Python interpreter is in a conda environment, but the environment has\r\nnot been activated.  Libraries may fail to load.  To activate this environment\r\nplease see https://conda.io/activation\r\n\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["@balaganesh1 \r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the latest [microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\nMake sure you update environment path for cuda(please refer this [https://www.tensorflow.org/install/gpu#windows_setup](https://www.tensorflow.org/install/gpu#windows_setup) ). Make sure if there is a library that is in a different location/not installed on your system that cannot be loaded.Also, please follow the instructions from [Tensorflow website.](https://www.tensorflow.org/install/gpu#windows_setup)\r\nPlease, check Your CPU/Python is on 32 bits?\r\n\r\nPlease, refer #[36167](https://github.com/tensorflow/tensorflow/issues/36167) and see if it helps you. Thanks!\r\nPLease let us know the tensorflow version you are using.", "You didn't fill the issue template at all.\r\n\r\nMost likely you are on windows, in which case just searching for similar issues would have pointed you to other duplicated issues.\r\n\r\nClosing as duplicate.\r\n\r\n#36167 (comment)\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36683\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36683\">No</a>\n"]}]