[{"number": 41168, "title": "Add universal Tensor/EagerTensor to ndarray conversion function", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tf-nightly==2.3.0dev20200622\r\n- Are you willing to contribute it (Yes/No): Not skilled enough or knowledgeable enough of TF architecture\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently there are many different ways to convert individual types of Tensor-like objects into corresponding ndarrays; however, each of these has a specific use condition.\r\n<EagerTensor>.numpy() : only works with eager execution\r\n<Tensor> : must be evaluated inside a tf.function or a Graph. Might not work in all environments either ( I couldn't get this to work)\r\nThere is a function that converts ProtoTensors, but it doesn't work on Tensors or EagerTensors.\r\n\r\nThe goal is to have a function that can convert at least Tensors and EagerTensors to ndarrays regardless of whether Eager Execution is enabled or not.\r\n\r\nit should be callable in a manner like `X = tf.make_ndarray(model(X))`\r\n\r\n**Will this change the current api? How?** To the best of my knowledge, it will not change the surface level API.\r\n\r\n**Who will benefit with this feature?**\r\nThis will particularly benefit anyone who has to run multiple, separately-trained networks in the same python session where at least one of them is a port from TF 1.X that requires eager execution to be disabled. Because Eager Execution is a global setting (to the best of my knowledge), this causes the output to be as Tensors when they may normally be expected to be EagerTensors.\r\n`X=model(X)` returns 'X' as a EagerTensor with EagerExecution but returns a Tensor with EagerExecution disabled. This could easily cause downstream data processing problems (i.e. numpy math operations no longer work because of non-compatibility).\r\n\r\nThe implementation of this would remove a significant barrier to making generalized code/networks. It also prevents unnecessary imports of tensorflow or tensorflow functions.\r\n\r\nSupposedly, Tensors have to be evaluated in a graph because of something to do with unknown states (or something that sounds similar) and EagerTensors don't have that limitation. However, TF clearly has the tf.math module and knows what the numbers are and knows how to do math with them. I don't see why the tf.math module can't simply have a function that can return a ndarray.\r\n\r\n**Any Other info.**\r\n", "comments": ["Graph mode only deals with symbolic tensors, so ndarrays don't fit there. This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "This is absolutely a feature request. I never claimed it was a bug either. Please do not put words in my mouth. To my knowledge I filled out all of the issue template for a feature request aside from \"Additional Info\"\r\n\r\nI have already used stack exchange and the frank reality is that most question are left unanswered. Most (if not all) of the results that are pulled up by searching are strictly related tensorflow 1.X. There are functions that transform one kind of tensor to other tensors but nothing that transforms a tensor to an EagerTensor or directly into a ndarray. I am well aware of the the whole session.run() method, but when I've tried getting a session (I don't remember which function that is) it will return the global session (or create a new one if one doesn't exist). The output of the model that forced eager execution (it is not my model but a different package built from TF 1 that uses compatibility functions of TF 2) can no longer be fed into a tf.keras built autoencoder (running non-eagerly because of the graph constructed ANN turns off eager execution) because the tensor didn't originate from the same graph. Why tensors are not portable between models is beyond me. Both models work fine if they are alone.\r\n\r\nI am aware that there are functions that allow the enabling and disabling of eager execution, but those functions also come with documentation that claims that they can only be run before any models or sessions are constructed. I can't just turn it on and off."]}, {"number": 41167, "title": "Sign compare warning fixes batch 11", "body": "[-Wsign-compare] warnings with associated ids: \r\n[\r\n242, 246, 247, 252, 253, 254, 256, 257, \r\n260, 263, 264, 265, 269, 275, 280, 288, \r\n295, 308, 309, 324, 328, 330, 334, 341, \r\n344, 345, 346, 351, 354, 356, 357, 358, \r\n360, 361, 364, 366, 367, 368, 369, 370, \r\n371, 373, 375, 379, 397, 436, 455, 461, \r\n518,\r\n]", "comments": ["This also has build failures\r\n\r\n```\r\ntensorflow/compiler/xla/service/hlo_creation_utils.cc:408:55: error: 'it' was not declared in this scope\r\n   for (int64 i = 0, iter_limit = operands.size(); i < it; ++i) {\r\n                                                       ^~\r\ntensorflow/compiler/xla/service/hlo_creation_utils.cc:408:55: note: suggested alternative: 'i'\r\n   for (int64 i = 0, iter_limit = operands.size(); i < it; ++i) {\r\n                                                       ^~\r\n                                                       i\r\n```", "@mihaimaruseac ", "diagnosing", "Closing as it has been handled by new PRs."]}, {"number": 41166, "title": "Tensorflow importing issue", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none\r\n- TensorFlow installed from (source or binary): tensorflow.org\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\nI can not import tensorflow, it shows an import issue that i tried to fix, but it is still not working. Any help is appreciated. Thanks a lot\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@neon-neon \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from T[ensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41166\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41166\">No</a>\n"]}, {"number": 41165, "title": "test mr", "body": "", "comments": ["Can you rebase this on top of master to fix the conflicts? Can you provide a little more context around the issues this PR is fixing? Is it possible to split this PR into a few smaller cherry-picks to make reviewing easier (and parallelizable)?"]}, {"number": 41164, "title": "TypeError: __init__() got an unexpected keyword argument 'lambda'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nN/A (google colab)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nn/a\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n1.15.2\r\n- Python version:\r\n3.6.9\r\n- Bazel version (if compiling from source):\r\nn/a\r\n\r\n**Describe the current behavior**\r\nI cannot load a saved model containing a subclass of keras.regularizer.Regularizer.\r\nError = TypeError: __init__() got an unexpected keyword argument 'lambda'\r\n\r\n**Describe the expected behavior**\r\nI expect it to load my model since the model works well and is saved correctly.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nclass My_Regularizer(Regularizer):\r\n    def __init__(self, lambd, matrix_x, matrix_y, matrix_z):\r\n        self.lambd = lambd\r\n        self.matrix_x = matrix_x\r\n        self.matrix_y = matrix_y\r\n        self.matrix_z = matrix_z\r\n\r\n    def __call__(self, x):\r\n        return tf.linalg.tensor_diag(tf.diag_part(self.lambd * K.dot(K.transpose(K.square(x)), K.variable(self.matrix_x, dtype='float32') + K.variable(self.matrix_y, dtype='float32') + K.variable(self.matrix_z, dtype='float32'))))\r\n\r\n    def get_config(self):\r\n        return {'lambd': float(self.lambd),\r\n                'matrix_x': self.matrix_x,\r\n                'matrix_y': self.matrix_y,\r\n                'matrix_z': self.matrix_z,\r\n               }\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-8-d7b61e297765> in <module>()\r\n----> 1 test = load_model('/content/drive/My Drive/Memoire/Wolfsky models/final_regularized_model.h5')\r\n\r\n13 frames\r\n/usr/local/lib/python3.6/dist-packages/keras/regularizers.py in from_config(cls, config)\r\n     24     @classmethod\r\n     25     def from_config(cls, config):\r\n---> 26         return cls(**config)\r\n     27 \r\n     28 \r\n\r\nTypeError: __init__() got an unexpected keyword argument 'lambda'\r\n```\r\n", "comments": ["@Delarti,\r\nCould you please provide the complete code to reproduce the issue reported here. Alternatively, you can share the Colab file you are running.\r\n\r\nAlso, is there any particular reason you are using TF v1.15? Please upgrade to TF v2.2 and check if you are facing the same issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41164\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41164\">No</a>\n"]}, {"number": 41163, "title": "Update version numbers for TensorFlow 2.3.0-rc1", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 3 -> 3\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.3.0-rc0\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.3.0rc0\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 41162, "title": "Two different styles of Tensorflow implementation for the same network architecture lead to two different results and behaviors?", "body": "\r\n- OS Platform: Linux Centos 7.6\r\n- Distribution: Intel Xeon Gold 6152 (22x3.70 GHz); \r\n- GPU Model: NVIDIA Tesla V100 32 GB;\r\n- Number of nodes/CPU/Cores/GPU: 26/52/1144/104;\r\n- TensorFlow installed from (source or binary): official webpage\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.8\r\n\r\n**Description of issue:**\r\n\r\nWhile I was implementing my proposed method, using the second style of implementation (see below), I realized that the performance of the algorithm is indeed strange. To be more precise, the accuracy decreases and loss value increases while the number of epochs increases. \r\n\r\nSo I narrow down the problem and finally, I decided to modify some codes from TensorFlow official page to check what is happening. As it is explained in TF v2 official webpage there are two styles of implementation which I have adopted as follows. \r\n\r\n- I have modified the code provided in \"getting started of TF v2\" the link below:\r\n   \r\n  [TensorFlow 2 quickstart for beginners](https://www.tensorflow.org/tutorials/quickstart/beginner) \r\n  \r\n as follows:\r\n\r\n  import tensorflow as tf\r\n  from sklearn.preprocessing import OneHotEncoder\r\n  from sklearn.datasets import make_classification\r\n  from sklearn.model_selection import train_test_split\r\n\r\n  learning_rate = 1e-4\r\n  batch_size = 100\r\n  n_classes = 2\r\n  n_units = 80\r\n\r\n\r\n    # Generate synthetic data / load data sets\r\n    x_in, y_in = make_classification(n_samples=1000, n_features=10, n_informative=4, n_redundant=2,\r\n                                     n_repeated=2, n_classes=2, n_clusters_per_class=2, weights=[0.5, 0.5],\r\n                                     flip_y=0.01, class_sep=1.0, hypercube=True,\r\n                                     shift=0.0, scale=1.0, shuffle=True, random_state=42)\r\n\r\n    x_in = x_in.astype('float32')\r\n    y_in = y_in.astype('float32').reshape(-1, 1)\r\n\r\n    one_hot_encoder = OneHotEncoder(sparse=False)\r\n    y_in = one_hot_encoder.fit_transform(y_in)\r\n    y_in = y_in.astype('float32')\r\n\r\n    x_train, x_test, y_train, y_test = train_test_split(x_in, y_in, test_size=0.4, random_state=42, shuffle=True)\r\n    x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42, shuffle=True)\r\n    print(\"shapes:\", x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_val.shape, y_val.shape)\r\n\r\n    V = x_train.shape[1]\r\n\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Dense(n_units, activation='relu', input_shape=(V,)),\r\n        tf.keras.layers.Dropout(0.2),\r\n        tf.keras.layers.Dense(n_classes)\r\n    ])\r\n\r\n    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n\r\n    model.compile(optimizer='adam',\r\n                  loss=loss_fn,\r\n                  metrics=['accuracy'])\r\n\r\n    model.fit(x_train, y_train, epochs=5)\r\n\r\n    model.evaluate(x_test, y_test, verbose=2)\r\n\r\n\r\nthe output is as it is expected, as one can see below:\r\n\r\n600/600 [==============================] - 0s 419us/sample - loss: 0.7114 - accuracy: 0.5350\r\nEpoch 2/5\r\n600/600 [==============================] - 0s 42us/sample - loss: 0.6149 - accuracy: 0.6050\r\nEpoch 3/5\r\n600/600 [==============================] - 0s 39us/sample - loss: 0.5450 - accuracy: 0.6925\r\nEpoch 4/5\r\n600/600 [==============================] - 0s 46us/sample - loss: 0.4895 - accuracy: 0.7425\r\nEpoch 5/5\r\n600/600 [==============================] - 0s 40us/sample - loss: 0.4579 - accuracy: 0.7825\r\n\r\ntest: 200/200 - 0s - loss: 0.4110 - accuracy: 0.8350\r\n\r\nTo be more precise, the training accuracy increases and the loss value decrease as the number epochs increases (which is expected and it is normal).\r\n\r\nHOWEVER, the following chunk of code which is adapted from the link below:\r\n\r\n  [TensorFlow 2 quickstart for experts](https://www.tensorflow.org/tutorials/quickstart/advanced)\r\n\r\nas follows:\r\n\r\nimport tensorflow as tf\r\nfrom sklearn.preprocessing import OneHotEncoder\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nlearning_rate = 1e-4\r\nbatch_size = 100\r\nn_classes = 2\r\nn_units = 80\r\n\r\n    # Generate synthetic data / load data sets\r\n    x_in, y_in = make_classification(n_samples=1000, n_features=10, n_informative=4, n_redundant=2,\r\n                                     n_repeated=2, n_classes=2, n_clusters_per_class=2, weights=[0.5, 0.5],\r\n                                     flip_y=0.01, class_sep=1.0, hypercube=True,\r\n                                     shift=0.0, scale=1.0, shuffle=True, random_state=42)\r\n\r\n    x_in = x_in.astype('float32')\r\n    y_in = y_in.astype('float32').reshape(-1, 1)\r\n\r\n    one_hot_encoder = OneHotEncoder(sparse=False)\r\n    y_in = one_hot_encoder.fit_transform(y_in)\r\n    y_in = y_in.astype('float32')\r\n\r\n    x_train, x_test, y_train, y_test = train_test_split(x_in, y_in, test_size=0.4, random_state=42, shuffle=True)\r\n    x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42, shuffle=True)\r\n\r\n    print(\"shapes:\", x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_val.shape, y_val.shape)\r\n\r\n    training_dataset = tf.data.Dataset.from_tensor_slices(\r\n        (x_train, y_train)).batch(batch_size)\r\n\r\n    valid_dataset = tf.data.Dataset.from_tensor_slices(\r\n        (x_val, y_val)).batch(batch_size)\r\n\r\n    testing_dataset = tf.data.Dataset.from_tensor_slices(\r\n        (x_test, y_test)).batch(batch_size)\r\n\r\n    V = x_train.shape[1]\r\n\r\n\r\n    class MyModel(tf.keras.models.Model):\r\n        def __init__(self):\r\n            super(MyModel, self).__init__()\r\n            self.d1 = tf.keras.layers.Dense(n_units, activation='relu', input_shape=(V,))\r\n            self.d2 = tf.keras.layers.Dropout(0.2)\r\n            self.d3 = tf.keras.layers.Dense(n_classes,)\r\n\r\n        def call(self, x):\r\n            x = self.d1(x)\r\n            x = self.d2(x)\r\n            return self.d3(x)\r\n\r\n\r\n    # Create an instance of the model\r\n    model = MyModel()\r\n\r\n    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n\r\n    optimizer = tf.keras.optimizers.Adam()\r\n\r\n    train_loss = tf.keras.metrics.Mean(name='train_loss')\r\n    train_accuracy = tf.keras.metrics.BinaryCrossentropy(name='train_accuracy')\r\n\r\n    test_loss = tf.keras.metrics.Mean(name='test_loss')\r\n    test_accuracy = tf.keras.metrics.BinaryCrossentropy(name='test_accuracy')\r\n\r\n\r\n    @tf.function\r\n    def train_step(images, labels):\r\n        with tf.GradientTape() as tape:\r\n            # training=True is only needed if there are layers with different\r\n            # behavior during training versus inference (e.g. Dropout).\r\n            predictions = model(images,)  # training=True\r\n            loss = loss_object(labels, predictions)\r\n        gradients = tape.gradient(loss, model.trainable_variables)\r\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\n        train_loss(loss)\r\n        train_accuracy(labels, predictions)\r\n\r\n\r\n    @tf.function\r\n    def test_step(images, labels):\r\n        # training=False is only needed if there are layers with different\r\n        # behavior during training versus inference (e.g. Dropout).\r\n        predictions = model(images,)  # training=False\r\n        t_loss = loss_object(labels, predictions)\r\n\r\n        test_loss(t_loss)\r\n        test_accuracy(labels, predictions)\r\n\r\n\r\n    EPOCHS = 5\r\n\r\n    for epoch in range(EPOCHS):\r\n        # Reset the metrics at the start of the next epoch\r\n        train_loss.reset_states()\r\n        train_accuracy.reset_states()\r\n        test_loss.reset_states()\r\n        test_accuracy.reset_states()\r\n\r\n        for images, labels in training_dataset:\r\n            train_step(images, labels)\r\n\r\n        for test_images, test_labels in testing_dataset:\r\n            test_step(test_images, test_labels)\r\n\r\n        template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\r\n        print(template.format(epoch + 1,\r\n                              train_loss.result(),\r\n                              train_accuracy.result(),\r\n                              test_loss.result(),\r\n                              test_accuracy.result()))\r\n\r\n\r\nBehaves indeed strange. Here is the output of this piece of code:\r\n\r\nEpoch 1, Loss: 0.7299721837043762, Accuracy: 3.8341376781463623, Test Loss: 0.7290592193603516, Test Accuracy: 3.6925911903381348\r\nEpoch 2, Loss: 0.6725851893424988, Accuracy: 3.1141700744628906, Test Loss: 0.6695905923843384, Test Accuracy: 3.2315549850463867\r\nEpoch 3, Loss: 0.6256862878799438, Accuracy: 2.75959849357605, Test Loss: 0.6216427087783813, Test Accuracy: 2.920461416244507\r\nEpoch 4, Loss: 0.5873140096664429, Accuracy: 2.4249706268310547, Test Loss: 0.5828182101249695, Test Accuracy: 2.575272560119629\r\nEpoch 5, Loss: 0.555053174495697, Accuracy: 2.2128372192382812, Test Loss: 0.5501811504364014, Test Accuracy: 2.264410972595215\r\n\r\nAs one can see, not only the values of accuracy are strange but also instead of increasing, once the number of epochs increase, they decrease? Please note the strange behaviour of loss value.\r\n\r\n**May you please explain what is happening here?**\r\n\r\n", "comments": ["Note that `tf.keras.metrics.BinaryCrossentropy` does not compute accuracy -- it computes just crossentropy (which would be 0 for 100% accurate output). Try using `tf.keras.metrics.BinaryAccuracy` for `{train,test}_accuracy`. Also, you should pass the `training=True/False` parameter you have commented out.", "@Sorooshi \r\nPlease update as per above comment.", "I made the following changes:\r\n\r\nloss_object = tf.keras.metrics.BinaryCrossentropy(from_logits=True)\r\n\r\n@tf.function\r\ndef train_step(images, labels):\r\n      with tf.GradientTape() as tape:\r\n      # training=True is only needed if there are layers with different\r\n      # behavior during training versus inference (e.g. Dropout).\r\n      predictions = model(images, training=True)\r\n      loss = loss_object(labels, predictions)\r\n gradients = tape.gradient(loss, model.trainable_variables)\r\n optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\nHere is \r\n TypeError: call() got an unexpected keyword argument 'training'\r\n\r\nAnd if I pass:\r\npredictions = model(images,) i.e comment training=True I face the following error:\r\n\r\nValueError: No gradients provided for any variable: ['my_model/dense/kernel:0', 'my_model/dense/bias:0', 'my_model/dense_1/kernel:0', 'my_model/dense_1/bias:0'].", "@Sorooshi \r\nPlease refer to these links for the error \"ValueError: No gradients provided for any variable:\"\r\nand let us know. [link](https://github.com/tensorflow/tensorflow/issues/1511#issuecomment-555848660) [link1](https://github.com/tensorflow/tensorflow/issues/27949) [link1](https://stackoverflow.com/questions/41689451/valueerror-no-gradients-provided-for-any-variable) [link2](https://datascience.stackexchange.com/questions/54498/no-gradients-provided-for-any-variable) ", "@sorooshi Also, your `MyModel.call` needs to accept the `training` argument and pass it to `d1`, `d2` and `d3` calls, as a named argument, i.e., `self.d1(x, training=training)`.", "@foxik why in the original code (link below) the author did not write the code as you pointed out?\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/41162\r\n\r\n\r\nMoreover, would you please be more specific? (I don't understand what should I do exactly) ", "@sorooshi There are several problems with your approach, but as far as I can tell, there is no bug in Tensorflow.\r\n\r\nGenerally, these types of questions (you implemented something and it does not what you expect) are better asked at places like Stack Overflow; these issues are meant to report bugs in Tensorflow itself.\r\n\r\nBut to answer your question -- [here is your original code imported in Colab](https://colab.research.google.com/drive/1lIQ5IxMffiPangWNeLiMQ0Ik6TT6wfMp?usp=sharing) and [here is a modified working version of your code](https://colab.research.google.com/drive/1Ud8GRDnYZoWelSolwXKhtxTj6PdJ5VGF?usp=sharing). The notable problems addressed in the modified version are:\r\n- use accuracy for the metrics, instead of the crossentropy you use\r\n- when you have 2 outputs and one-hot encoded lables, the loss should be `CategoricalCrossentropy` and the metrics `CategoricalAccuracy`.\r\n  - the `BinaryCrossentropy`, which you used, is appropriate when you have 1 output neuron and labels 0/1.\r\n\r\n    Even so, if you use `BinaryCrossentropy(from_logits=True)`, the appropriate metric is `tf.metrics.BinaryAccuracy(threshold=0.0)`, because when the `sigmoid` activation function is not used in the model, the threshold for distinguishing the two classes becomes 0.0 instead of the default 0.5 (given that `sigmoid(0.0) = 0.5`).\r\n\r\n  - `SparseCategorical{Crossentropy,Accuracy}` would be suitable if you used numbers 0/1 as labels (i.e., the onehot encoding for `y` would not be used)\r\n\r\n- the version with `model.fit` actually shuffles the data, so you should add `.shuffle` call to your training dataset\r\n\r\n- the `model.fit` version implicitly uses `batch_size=32`, while the manual version used `100`, which made the training slower and the results comparatively worse in the 5 epochs (but they would be comparable after ~100 epochs or so)\r\n\r\n- because you use a `Dropout` layer, it is important to correctly pass the `training` argument (which was not needed in the original Quickstart for experts without a Dropout layer)\r\n\r\nWith all these, the results are very similar.\r\n\r\n@Saduf2019 I believe this can be closed.", "@Sorooshi\r\nPlease confirm if we may move this to closed status.", "Dear @foxik \r\n\r\nThank you very much for your help and reply.\r\n\r\nIn your first reply, I understood my blunder about mistakenly using BinaryCrossEntropy instead of BinaryAccuracy (during the evaluation)\r\n\r\nBut still, my question about passing training argument exist.\r\nConcretely, I even copied and pasted your code into Pycharm and I tried to run it locally (not in Colab nor Jupyter Notebook) and I receive the following error:\r\n\r\nTypeError: call() got an unexpected keyword argument 'training'\r\n\r\n\r\n@Saduf2019 I think yes we could move to closed status (it was my blunder and I apologize for it)\r\n\r\nHowever, if I do not understand why the aforementioned issue about the training argument occurs I will open another issue:) (unless someone could reply to me on this issue)\r\n\r\n\r\nPS:\r\n\r\nTo @foxik and other people you may read this later:\r\n\r\n- As about shuffling the data, once you split them or even during the generation process, they will be shuffled by default.\r\n\r\n- As batch_size you are correct not only larger batch size increases the execution time but apparently (at this in this data set) the larger the batch size the lower the accuracy. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "I have found the problem about the passing training argument thanks to this [link](https://www.tensorflow.org/api_docs/python/tf/keras/Model):\r\n\r\nhere is how the call function should be:\r\n\r\n```\r\ndef call(self, x, training=False):\r\n    x = self.d1(x)\r\n    if training:\r\n        x = self.d2(x, training=training)\r\n    return self.d3(x)\r\n```\r\n"]}, {"number": 41160, "title": "Mnist forward pass", "body": "@saxenasaurabh \r\n\r\nFirst attempt for mnist forward pass with abstract tensors; branching off of previous tests in `abstract_tensor_matmul_test` . Relevant file for this PR is mnist_forward_pass.cc ", "comments": []}, {"number": 41159, "title": "InternalError: Tensorflow type 21 not convertible to numpy dtype.", "body": "`kl = tf.reduce_mean(compute_kld( p_logit , p_logit_r ))\r\ngrad_kl = tf.gradients( kl ,r_tensor)[0]\r\nr_vadv = tf.stop_gradient(grad_kl)\r\nr_vadv = make_unit_norm( r_vadv )/3.0\r\np_logit_no_gradient = tf.stop_gradient(p_logit)\r\n\r\nvadv_tensor = tf.add(clean_emb, grad_kl)\r\nprint('Gradient: ',type(grad_kl), grad_kl.shape)\r\nprint('Adversarial embedding: ',type(vadv_tensor), vadv_tensor.shape)`\r\n\r\nGradient:  <class 'tensorflow.python.framework.ops.Tensor'> (None, 500, 50)\r\nAdversarial embedding:  <class 'tensorflow.python.framework.ops.Tensor'> (None, 500, 50)\r\n\r\n`hidden = LSTM(units=128)(vadv_tensor)\r\noutput = Dense(units=32, activation='relu')(vadv_tensor)\r\nq_logit = Dense(units=1, activation='relu')(output)\r\n\r\nprint('Q_Logit: ',type(q_logit), q_logit.shape)`\r\n\r\n# Error\r\n\r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  run_metadata_ptr)\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-8-9bcdd73aa054> in <module>()\r\n----> 1 hidden = LSTM(units=128)(vadv_tensor)\r\n      2 output = Dense(units=32, activation='relu')(vadv_tensor)\r\n      3 q_logit = Dense(units=1, activation='relu')(output)\r\n      4 \r\n      5 print('Q_Logit: ',type(q_logit), q_logit.shape)\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)\r\n   1470         ret = tf_session.TF_SessionRunCallable(self._session._session,\r\n   1471                                                self._handle, args,\r\n-> 1472                                                run_metadata_ptr)\r\n   1473         if run_metadata:\r\n   1474           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nInternalError: Tensorflow type 21 not convertible to numpy dtype.\r\n\r\n---------------------------------------------------------------------------\r\n\r\nI am using Tensorflow 1.x. Any suggestions on how I can solve this error. \r\nThanks for your consideration.\r\n\r\nThe error can be replicated from this collab: https://colab.research.google.com/drive/1RlZTQaqM8Mb2MnyviUO0K88QXztFlY8Z?usp=sharing\r\n\r\nThe dataset used for the above code:\r\n[npy.zip](https://github.com/tensorflow/tensorflow/files/4886083/npy.zip)\r\n\r\n", "comments": ["@lokesharma-dev,\r\nPlease take a look [this comment](https://github.com/tensorflow/tensorflow/issues/34683#issuecomment-572791147) from a similar issues and let us know if it helps. Thanks!"]}, {"number": 41158, "title": "Add ram file block cache", "body": "@mihaimaruseac \r\nThis PR adds `ram_file_block_cache` ( implementation of the interface `file_block_cache`). Below are the changes I made compared to the current approach.\r\n- `Status` to `TF_Status`\r\n- `env::Thread` to `TF_Thread` ( and thread-related function )\r\n- `mutex` to `absl::Mutex`\r\n- `mutex_lock` to `absl::MutexLock`\r\n- `notification` to `absl::Notification`\r\n- `conditional_variable` to `absl::CondVar`\r\n- `thread_annotations.h` to `\"absl/base/thread_annotations.h\"` (`TF_GUARDED_BY` to `ABSL_GUARDED_BY`, etc)\r\n\r\nThis PR is huge but I do not know how to break it down properly. Do you have any idea ? Thank you\r\n", "comments": ["Let's go as it is since the changes are well documented above and it is easy to review from the point of view of added functionality. You can always split a large PR such as this by making one where only the subclass is added but with no methods and then several new ones where methods get implemented.", "One more change. \r\n\r\nI copy `tensorflow/core/lib/gtl/cleanup.h` to `tensorflow/c/experimental/filesystem/gcs/plugins/cleanup.h` since it is a header-only library and does not depends on any library of `tensorflow/core` but only 1 macro from `tensorflow/core/platform/macros.h` ( I forgot to delete this line but it is ok since I have already removed that macro `TF_MUST_USE_RESULT`)\r\n\r\nI will send you a PR to add test and remove the line `include \"tensorflow/core/platform/macros.h\"`"]}, {"number": 41157, "title": "TimeDistributed does not infer output batch size when timesteps=None", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version (use command below): v2.2.0-0-g2b96f3662b 2.2.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI am trying to create a convolutional LSTM model which makes use of `ConvLSTM2D` as well as other layers such as `Conv2D` and `MaxPool2D`, wrapped in `TimeDistributed` layers. I'm using the Functional API.\r\n\r\nThis model should be stateful. Therefore, I pass the batch size to the `Input` layer, but I would like to keep the `timesteps` dimension flexible, so I set that to `None`.\r\n\r\nThe problem arises when using `TimeDistributed` layers, because their output batch size is always set to `None`, even if their input batch size is fixed (see following summary).\r\n\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_18 (InputLayer)        [(4, None, 64, 64, 1)]    0         \r\n_________________________________________________________________\r\ntime_distributed_27 (TimeDis (None, None, 64, 64, 1)   257       # no LSTMs after this point because batch_size=None\r\n=================================================================\r\n```\r\n\r\nAs a result, stateful LSTM layers cannot be used after `TimeDistributed`, as they will fail with \"ValueError: If a RNN is stateful, it needs to know its batch size\".\r\n\r\n**Describe the expected behavior**\r\n\r\nI certainly could be missing something, but I don't see any reason why the TimeDistributed layer shouldn't be able to infer its output batch size correctly (is it not always the same as the input batch size?). Even if that's not true, shouldn't it be possible to have a mechanism to help the layer figure it out?\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nLink to a simple example in Colab:\r\nhttps://colab.research.google.com/drive/11efUrQpkahgk-jgb0-4Flwii7Fd1y5Ej?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@jmontalt \r\nPlease refer to below issues with similar error and let us know if it helps.\r\n\r\n#36363 #31028 [link](https://github.com/keras-team/keras/issues/7770) [link1](https://github.com/keras-team/keras/issues/7770#issuecomment-326762841)  [link2](https://github.com/tensorflow/tensorflow/issues/31028#issuecomment-520290743)", "Hi, thank you very much for your reply. It looks like the problem is the same as that described in #36363 for the `Reshape` layer, and indeed the same solution (i.e. setting the static shape using `set_shape` at the end of the layer's `call` method) works for me. Should I raise a PR with the equivalent fix for `TimeDistributed`?", "@jmontalt\r\nPlease go ahead with the PR creation, proceeding to move this issue to closed status as its resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41157\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41157\">No</a>\n", "@jmontalt Did you edit the PR ? (I was not able to find it and I still have the issue with tf-nightly 2.4.0.dev20200906).\r\nIf not, can you publish here the solution please ?", "This slipped my mind, but I have created the PR now. In the meantime, you can fix the issue using `set_shape` on the output of your TimeDistributed layer.\r\n```\r\nlayer = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(16, (3, 3)))\r\ninputs = tf.keras.Input(batch_shape=(1, None, 32, 32, 1))\r\noutputs = layer(inputs)\r\noutputs.set_shape(layer.compute_output_shape(inputs.shape))\r\n```", "Thanks a lot, I applied your PR locally and it is working well !\r\n(BTW a review is required in order to merge, one test has failed) \r\n"]}, {"number": 41156, "title": "Persistent GradientTape not working with LSTM", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes. Adapted code from [this tutorial](https://keras.io/guides/customizing_what_happens_in_fit/)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Build 1903\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7.7\r\n\r\n**Describe the current behavior**\r\n\r\nTraining a model with `tf.keras.layers.LSTM`  and  custom `train_step` that uses a `tf.GradientTape(persistent=True)` is \r\nstuck at the first batch. The python process slowly uses all available RAM.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model should train the same way as when using `persistent=False`.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/16tBJzlHqPY5oL5im20noFSN8luVzPnjt?usp=sharing\r\n", "comments": ["I have tried in colab with TF versions 2.2,nightly versions(`2.4.0-dev20200707`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/6f20f5647385a1841529f98b9de85066/untitled93.ipynb).Thanks!", "Seems to happen with GRU as well but not SimpleRNN. Possibly related to issue [#39697](https://github.com/tensorflow/tensorflow/issues/39697) ?", "@GPla, I think this is the same issue as  #39697 and tf core team is trying to address it. Please track the progress in  #39697 and I am closing this issue for now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41156\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41156\">No</a>\n"]}, {"number": 41155, "title": "Sign compare warning fixes batch 10", "body": "in resolution of [-Wsign-compare] warnings with ids:\r\n[\r\n232, 233, 238, 239, 240, 243, 255, 266,\r\n268, 274, 283, 285, 286, 287, 299, 300, \r\n301, 303, 305, 306, 307, 311, 312, 313,\r\n314, 315, 317, 318, 320, 321, 322, 323,\r\n331, 337, 339, 340, 347, 348, 359, 363,\r\n374, 377, 380, 381, 435, 440, 442, 445,\r\n448, 449, 452, 453, 454, 456, 457, 458,\r\n460, 463, 464, 465, 466, 467, 471, 473\r\n]\r\n\r\n@mihaimaruseac ", "comments": ["```\r\ntensorflow/compiler/xla/literal.cc:1050:19: error: 'in64' does not name a type; did you mean 'C64'?\r\n             const in64 accum_indices_size = accum_indices->size();\r\n                   ^~~~\r\n                   C64\r\ntensorflow/compiler/xla/literal.cc:1052:17: error: 'accum_indices_size' was not declared in this scope\r\n                 accum_indices_size < rank) {\r\n                 ^~~~~~~~~~~~~~~~~~\r\ntensorflow/compiler/xla/literal.cc:1052:17: note: suggested alternative: 'accum_indices'\r\n                 accum_indices_size < rank) {\r\n                 ^~~~~~~~~~~~~~~~~~\r\n                 accum_indices\r\n```\r\n\r\nProbably missing an include", "@mihaimaruseac ", "diagnosing", "Closing as it has been handled by new PRs."]}, {"number": 41154, "title": "g-1", "body": "just trying a thing", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41154) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 41153, "title": "tensorflow/tensorflow:2.1.1-gpu has python3 instead of python2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  N/A\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version (use command below): v2.1.0-33-g3ffdb91 2.1.1\r\n- Python version: See details\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\nDocker image tensorflow/tensorflow:2.1.1-gpu contains python3.6.9\r\n\r\n**Describe the expected behavior**\r\nDocker image tensorflow/tensorflow:2.1.1-gpu contains python2.7.17 as is expected for docker images <= 2.1 https://hub.docker.com/r/tensorflow/tensorflow/\r\n\r\n**Standalone code to reproduce the issue**\r\n$docker run tensorflow/tensorflow:2.1.1-gpu python --version\r\nPython 3.6.9            \r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nNote:  tensorflow/tensorflow:2.1.0-gpu contains python2 \r\n$docker run tensorflow/tensorflow:2.1.0-gpu python --version\r\nPython 2.7.17      \r\n", "comments": ["This is working as intended\r\n\r\n> Versioned images <= 1.15.0 (1.x) and <= 2.1.0 (2.x) have Python 3 (3.5 for Ubuntu 16-based images; 3.6 for Ubuntu 18-based images) in images tagged \"-py3\" and Python 2.7 in images without \"py\" in the tag. All newer images are Python 3 only. Tags containing -py3 are deprecated.\r\n\r\nVersion 2.1.1 is not smaller than 2.1.0.\r\n\r\nAny patch release after January 2020 will be only Python3, regardless if the original final release included python 2 support.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41153\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41153\">No</a>\n"]}, {"number": 41151, "title": "uint8 model runtime input(s) num is 2.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2-1\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import load_model\r\n\r\nkeras_file = \"./keras-0127127.h5\"\r\n\r\nmodel = load_model(keras_file)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_type = tf.uint8    #tf.lite.constants.QUANTIZED_UINT8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflite_uint8_model = converter.convert()\r\nopen(\"uint8.tflite\", \"wb\").write(tflite_uint8_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\nI use netron software to visualiz the \"uint8.tflite\" model, it shows that:\r\n![image](https://user-images.githubusercontent.com/20535427/86765182-68cd0b00-c07b-11ea-83c2-026fbc5b5c50.png)\r\nWe can see some  DEPTHWISE_CONV_2D operations have 2 inputs.\r\n\r\n**Also, please include a link to the keras model I used above**\r\n\r\n```\r\nhttps://github.com/chenpengf0223/semantic_segmentation_distillation/blob/master/keras-0127127.h5\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing model that c++ tflite gpu delegate library can not process.\r\n\r\n\r\nWhen I use c++ tflite gpu delegate library to run the model, I get such log:\r\n\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Following operations are not supported by GPU delegate:\r\nDEPTHWISE_CONV_2D: Expected 1 runtime input tensor(s), but node has 2 runtime input(s).\r\nDEQUANTIZE: Expected 1 runtime input tensor(s), but node has 0 runtime input(s).\r\n20 operations will run on the GPU, and the remaining 41 operations will run on the CPU.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.", "comments": ["@srjoglekar246, could you take a look at this issue? It's related to quant model support in gpu delegate.", "@chenpengf0223 From the output, it looks like the model is partially running on GPU.\r\n\r\nFor some of the nodes, looks like the quantization isn't happening correctly. Could you try the following params for conversion:\r\n\r\n```\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8  # or tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n```\r\n\r\n(note: no `converter.inference_type`)\r\n\r\nIf that also leads to a similar output, there might be some issues with the quantization tooling - DEQUANTIZE nodes with no input tensors should never happen, so there might be some errors. Feel free to attach the tflite model as well, if things don't work after the above changes.", "@srjoglekar246 Thanks for your reply, I try your params as following:\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import load_model\r\nimport numpy as np\r\nimport random\r\n\r\ndef representative_dataset_gen():\r\n  for _ in range(100):\r\n      # Get sample input data as a numpy array in a method of your choosing.\r\n      data = np.random.random((1,512,512,3))\r\n      #print(data.dtype)\r\n      data=data.astype('float32')\r\n      #print(data)\r\n      yield [data]\r\n\r\nkeras_file = \"./keras-0127127.h5\"\r\nmodel = load_model(keras_file)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_uint8_model = converter.convert()\r\nopen(\"int8.tflite\", \"wb\").write(tflite_uint8_model)\r\n\r\nI get a correct tflite  model, and I attach the model :\r\nhttps://github.com/chenpengf0223/semantic_segmentation_distillation/blob/master/int8.tflite\r\n\r\nThen I use tflite gpu delegate to run the model, the function \"interpreter_->Invoke()\" costs about 52ms, (I calcaute the time from the 11th frame in a repeated style).\r\n\r\nIf I run the float model for the same network, the function \"interpreter_->Invoke()\" costs about 40ms.\r\nI do not know if it is a normal phenomenon.\r\n\r\nThanks!", "Do you mean the float model on GPU is faster than int8 model on GPU? Or that float model on CPU is faster than int8 on GPU?\r\n\r\nIf float model on GPU is faster than int8 model on GPU, could you share the float TFLite model also to debug?\r\n\r\nThere should not be such a big difference between float and int8.", "@srjoglekar246  I run the float model and the int8 model on GPU using tflite gpu delegate, I attach the float32 model here:\r\nhttps://github.com/chenpengf0223/semantic_segmentation_distillation/blob/master/float32.tflite \r\n\r\nThe float32 model conversion is as following:\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import load_model\r\nimport numpy as np\r\n\r\nkeras_file = \"./keras-0127127.h5\"\r\nmodel = load_model(keras_file)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_float_model = converter.convert()\r\nopen(\"float32.tflite\", \"wb\").write(tflite_float_model)\r\nprint('tflite model conversion is done.')", "Looks like it is working as expected. \r\n\r\nFor quantized models, we need to perform some extra operations to 'simulate' quantized inference on the GPU - this is done to ensure accuracy (compared to floating point).\r\nThis leads to increased latency, depending on characteristics such as number of input/output tensors, number of ops, etc.\r\n\r\nReducing the number of output tensors (6 in this case) will probably help.\r\n\r\nHowever, it all boils down to what you want to trade-off: \r\nFor very small model size but higher latency, choose int8. For 4x larger model but great speed, use float32. For something in between (slight accuracy tradeoff, 2x larger model) use [float16](https://www.tensorflow.org/lite/performance/post_training_float16_quant)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41151\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41151\">No</a>\n"]}, {"number": 41150, "title": "The order of weights is changed for array of layers inside sub classed model", "body": "The order of  moving variance and moving mean weights(BatchNormalization layer) changes when array of layers is created inside sub classed model\r\n\r\n````python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import *\r\nprint(tf.__version__)\r\n\r\nclass A(tf.keras.layers.Layer):\r\n    def  __init__(self,  name):\r\n      super(A, self).__init__(name=name)\r\n      self.conv = Conv2D(32, 3)\r\n      self.bn = BatchNormalization()  \r\n    def call(self,  x):\r\n      x = self.conv(x)\r\n      x = self.bn(x)\r\n      return x\r\n\r\nclass B(tf.keras.Model):\r\n    def __init__(self):\r\n        super(B, self).__init__()\r\n        self.blocks = []\r\n        for i in range(3):\r\n          self.blocks.append(A(f'a{i}'))    \r\n    \r\n    def call(self, x):\r\n        for block in self.blocks:\r\n          x = block(x)\r\n        return x\r\n\r\nb = B()\r\nb.build((1, 128, 128, 3))\r\nprint([weight.name for weight in b.weights])\r\n\r\n#The output is as below\r\n'a0/conv2d/kernel:0',\r\n 'a0/conv2d/bias:0',\r\n 'a0/batch_normalization/gamma:0',\r\n 'a0/batch_normalization/beta:0',\r\n 'a1/conv2d_1/kernel:0',\r\n 'a1/conv2d_1/bias:0',\r\n 'a1/batch_normalization_1/gamma:0',\r\n 'a1/batch_normalization_1/beta:0',\r\n 'a2/conv2d_2/kernel:0',\r\n 'a2/conv2d_2/bias:0',\r\n 'a2/batch_normalization_2/gamma:0',\r\n 'a2/batch_normalization_2/beta:0',\r\n 'a0/batch_normalization/moving_mean:0',\r\n 'a0/batch_normalization/moving_variance:0',\r\n 'a1/batch_normalization_1/moving_mean:0',\r\n 'a1/batch_normalization_1/moving_variance:0',\r\n 'a2/batch_normalization_2/moving_mean:0',\r\n 'a2/batch_normalization_2/moving_variance:0'\r\n\r\n\r\n\r\n\r\n", "comments": []}, {"number": 41149, "title": "bug in create an op of tensorflow 1.15 ", "body": "- tensorflow\r\n  - tensorflow core\r\n    - create an op\r\n       - GPU kernels\r\nThere is an example of GPU kernel implementation.\r\nThere are three files:\r\nkernel_example.h  kernel_example.cc  kernel_example.cu.cc\r\nWhen I compile the kernel_example.cu.cc with the commond:\r\n```\r\nnvcc -std=c++11 -c -o kernel_example.cu.o kernel_example.cu.cc \\\r\n  ${TF_CFLAGS[@]} -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC\r\n```\r\nthere are some errors:\r\n```\r\nkernel_example.h: error: a class or namespace qualified name is required\r\nkernel_example.h: warning: nonstandard qualified name in global scope declaration\r\nkernel_example.h: error: class template \"ExampleFunctor \" has already been defined\r\n```\r\nHow to deal with that?", "comments": ["@szliu90 \r\nIs there any particular reason for using older version of tf when their are newer versions available.", "> @Saduf2019 \r\n> Is there any particular reason for using older version of tf when their are newer versions available.\r\n\r\nMany people use the GPUs together. The CUDA version can't be changed to fit my own demand.", "Hi @szliu90 ,\r\n\r\nThis looks like a general CUDA/C++ programming question.  Have you tried asking on StackOverflow?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41149\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41149\">No</a>\n"]}, {"number": 41148, "title": "Tensorflow GPU needs to wait a few minutes each time before computing", "body": "**Tensorflow GPU needs to wait a few minutes each time before computing,GPU runs slower than CPU because of waiting**\r\n\r\n**System information**\r\n- OS Platform: ubuntu18.04\r\n- TensorFlow:  2.2\r\n- Python version:3.7\r\n- CUDA/cuDNN version: cuda10.2/cudnn7.6.5\r\n- GPU model and memory: GTX 950m, memory is 2048MB\r\n\r\nThe code  is official's helloworld demo .\r\n[https://www.tensorflow.org/tutorials/quickstart/beginner](url)\r\n\r\nHere is the information of output.\r\n `2020-07-07 13:59:25.106703: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 950M, Compute Capability 5.0``\r\n\r\n\r\nI've tried the official apt installation, but it's still the same. Does anyone have the same problem? How did you solve it.\r\n", "comments": ["@huoxingwen \r\n\r\nRequest you to share colab link or simple standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "> @huoxingwen\r\n> \r\n> Request you to share colab link or simple standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n\r\nThe code is official's helloworld demo .\r\n[https://www.tensorflow.org/tutorials/quickstart/beginner](url)", "@huoxingwen \r\n\r\nI have tried in colab with TF version 2.2 and i am seeing the same time for both [cpu](https://colab.research.google.com/gist/ravikyram/773c240585600c5242703be55ef9b50d/untitled99.ipynb) and [gpu](https://colab.research.google.com/gist/ravikyram/a57a6e33bc02650a521a6a5188740d98/untitled99.ipynb).I noticed it takes few more minutes in installing Tensorflow with GPU.Apart from that almost performance is same.Thanks!", "> @huoxingwen\r\n> \r\n> I have tried in colab with TF version 2.2 and i am seeing the same time for both [cpu](https://colab.research.google.com/gist/ravikyram/773c240585600c5242703be55ef9b50d/untitled99.ipynb) and [gpu](https://colab.research.google.com/gist/ravikyram/a57a6e33bc02650a521a6a5188740d98/untitled99.ipynb).I noticed it takes few more minutes in installing Tensorflow with GPU.Apart from that almost performance is same.Thanks!\r\n\r\n\r\nThank  for your replying, I  tested it in window10 and google colab, there is no problem and run code instantly. However it still have to wait  a few mins to start computing in ubuntu16 & ubuntu18 & ubuntu20. it shows that the memory of graphics card is high, but the gpu performance is low.\r\n", "@huoxingwen I see that you are using cuda 10.2 along with TF 2.2 version.\r\nThe TF 2.2 pre built packages support cuda 10.1 \r\nSo you may want to switch to cuda 10.1 , set appropriate cuda env paths and try again.\r\nRight now what's happening is that your computation is falling back on cpu due incorrect [gpu setup](https://www.tensorflow.org/install/gpu#software_requirements).\r\n\r\n", "> @huoxingwen I see that you are using cuda 10.2 along with TF 2.2 version.\r\n> The TF 2.2 pre built packages support cuda 10.1\r\n> So you may want to switch to cuda 10.1 , set appropriate cuda env paths and try again.\r\n> Right now what's happening is that your computation is falling back on cpu due incorrect [gpu setup](https://www.tensorflow.org/install/gpu#software_requirements).\r\n\r\nThank you for your idea. I used to think this way and I've changed the version of cuda to 10.1, but it's the same. And my system information is after the change.\r\n\r\nOS Platform: ubuntu18.04\r\nTensorFlow: 2.2\r\nPython version:3.7\r\nCUDA/cuDNN version: cuda10.1/cudnn7.6.5\r\nGPU model and memory: GTX 950m, memory is 2048MB\r\n\r\n", "As a sanity check  can you please test if gpu is detected by your machine\r\n```python\r\ntf.config.list_physical_devices('GPU')\r\n```\r\n(https://www.tensorflow.org/api_docs/python/tf/config/list_physical_devices)", "> As a sanity check can you please test if gpu is detected by your machine\r\n> \r\n> ```python\r\n> tf.config.list_physical_devices('GPU')\r\n> ```\r\n> \r\n> (https://www.tensorflow.org/api_docs/python/tf/config/list_physical_devices)\r\n\r\n> I had tried this code \r\n> `tf.config.list_physical_devices('GPU')`\r\n> and got the result\r\n\r\n> > 2020-07-14 06:18:04.804962: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n> 2020-07-14 06:18:04.809154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2020-07-14 06:18:04.809499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\n> pciBusID: 0000:01:00.0 name: GeForce GTX 950M computeCapability: 5.0\r\n> coreClock: 1.124GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 26.82GiB/s\r\n> 2020-07-14 06:18:04.809804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n> 2020-07-14 06:18:04.811523: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2020-07-14 06:18:04.813105: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n> 2020-07-14 06:18:04.813470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n> 2020-07-14 06:18:04.816187: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n> 2020-07-14 06:18:04.817683: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n> 2020-07-14 06:18:04.821731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 2020-07-14 06:18:04.821897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2020-07-14 06:18:04.822220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2020-07-14 06:18:04.822409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n> [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n\r\n", "GTX 950m has compute capability 5.0 and TF nightly does not ship with SASS with this compute capability.  So it is JIT compiling PTX and that is likely the pause you see.\r\n\r\nAre you able to build TF from source?  The best solution for is probably to build from source and include sm_50 in the build.", "@huoxingwen,\r\nCan you please confirm if your issue is resolved? If not, can you please respond to the above comment? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41147, "title": "Creating an instance of ResNet50() uses high GPU Memory", "body": "Creating a model using the below method results in 14GB of GPU usage\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.applications import MobileNet, MobileNetV2, resnet\r\nbase_model = resnet.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\r\n\r\n![image](https://user-images.githubusercontent.com/52433565/86726230-f5b49c00-c05c-11ea-8676-f7cf1b8a26fb.png)\r\n\r\nAny reason for such high GPU usage ?", "comments": ["@kartik1395,\r\nPlease take a look at [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) to limit GPU memory growth and let us know if it helps. Thanks!", "@amahendrakar i followed the guide. Seems its not capturing all the memory at one go. Thanks!"]}, {"number": 41146, "title": "UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.", "body": "https://www.tensorflow.org/tutorials/generative/style_transfer\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nno. downloaded the notebook and ran verbatim. \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 20.04\r\n\r\n- TensorFlow installed from (source or binary):\r\npip\r\n- TensorFlow version (use command below):\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version:\r\n3.8.3\r\n- CUDA/cuDNN version:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 206...  Off  | 00000000:0A:00.0  On |                  N/A |\r\n| 29%   48C    P2    73W / 175W |   7950MiB /  7979MiB |     41%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n- GPU model and memory:\r\nNvidia RTX 2060 Super 8GB\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://www.tensorflow.org/tutorials/generative/style_transfer jupyter notebook from this article, unmodified\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-21-5d399fb9441b> in <module>\r\n      1 import tensorflow_hub as hub\r\n      2 hub_module = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/1')\r\n----> 3 stylized_image = hub_module(tf.constant(content_image), tf.constant(style_image))[0]\r\n      4 tensor_to_image(stylized_image)\r\n\r\n~/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py in _call_attribute(instance, *args, **kwargs)\r\n    484 \r\n    485 def _call_attribute(instance, *args, **kwargs):\r\n--> 486   return instance.__call__(*args, **kwargs)\r\n    487 \r\n    488 \r\n\r\n~/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   1603       TypeError: For invalid positional/keyword argument combinations.\r\n   1604     \"\"\"\r\n-> 1605     return self._call_impl(args, kwargs)\r\n   1606 \r\n   1607   def _call_impl(self, args, kwargs, cancellation_manager=None):\r\n\r\n~/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_impl(self, args, kwargs, cancellation_manager)\r\n   1643       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\r\n   1644           list(kwargs.keys()), list(self._arg_keywords)))\r\n-> 1645     return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n   1646 \r\n   1647   def _filtered_call(self, args, kwargs):\r\n\r\n~/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n     98       captured_inputs = list(\r\n     99           map(get_cross_replica_handle, captured_inputs))\r\n--> 100     return super(_WrapperFunction, self)._call_flat(args, captured_inputs,\r\n    101                                                     cancellation_manager)\r\n    102 \r\n\r\n~/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1743         and executing_eagerly):\r\n   1744       # No tape is watching; skip to running the function.\r\n-> 1745       return self._build_call_outputs(self._inference_function.call(\r\n   1746           ctx, args, cancellation_manager=cancellation_manager))\r\n   1747     forward_backward = self._select_forward_and_backward_functions(\r\n\r\n~/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    591       with _InterpolateFunctionError(self):\r\n    592         if cancellation_manager is None:\r\n--> 593           outputs = execute.execute(\r\n    594               str(self.signature.name),\r\n    595               num_outputs=self._num_outputs,\r\n\r\n~/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     57   try:\r\n     58     ctx.ensure_initialized()\r\n---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n\r\nUnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node InceptionV3/Conv2d_1a_3x3/Conv2D (defined at /home/quinn/code/github.com/quinn/ml/venv/lib/python3.8/site-packages/tensorflow_hub/module_v2.py:102) ]] [Op:__inference_pruned_18205]\r\n\r\nFunction call stack:\r\npruned\r\n```\r\n", "comments": ["i closed some stuff, got memory down to `1542MiB /  7979MiB`. still having same problem. ", "i added this mysterious incantation to the start of the script and it seems to work now:\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n```\r\nshould i preface everything i do in TF with this ? ", "You can just set the env variable TF_FORCE_GPU_ALLOW_GROWTH to `true` (and no need to change anything in your code) ... (this issue is related to RTX cards)", "@jnd77 thanks! is there documentation somewhere explaining why this affects RTX cards specifically?  ", "I never saw the documentation.\r\nJust an issue I came across first time we bought an RTX card ...", "@jnd77 \r\nPlease refer to issues with similar error, [link](https://github.com/tensorflow/tensorflow/issues/25160#issuecomment-647104234) [link1](https://github.com/tensorflow/tensorflow/issues/36025#issuecomment-630375877) #37725  #34355 #24828", "@Saduf2019 thanks a lot for pointing to other issues. Actually am not the one who raised the issue. I was just pointing out I never saw in the official documentation the importance of this env variable for RTX cards.\r\n\r\nMaybe it should be added somewhere, or alternatively the default value should become True, since more and more people will work with RTX cards.\r\n\r\nFeel free to check with @quinn if the issue can be closed.", "@quinn \r\nPlease update as per above comment.", "`TF_FORCE_GPU_ALLOW_GROWTH` does seem to fix it. thank you @jnd77 ! @Saduf2019 Yes, I read many SO/github issues before posting this. I did eventually fix it with the code snippet I found. The env var solution is much succinct, but it seems like there should be either be a fix in the code for this, or official documentation added to capture searches and address the various combinations of hardware and library versions that seem to cause this error", "@quinn \r\nPlease confirm if we may move this issue to closed status as its fixed.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41146\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41146\">No</a>\n", "This is an issue of mismatch between loaded runtine CuDNN version and version with which the Tensorflow source was compiled.\r\n\r\nI had to downgrade my conda CuDNN version from 7.6.5 to 7.6.4.\r\n\r\nSteps I followed\r\n\r\n`conda uninstall cudnn==7.6.5` \r\n\r\nNow to find the required version just run the Tensorflow code again and check the error.\r\n`Loaded runtime CuDNN library: 7.1.4 but the source was compiled with: 7.6.4.  CuDNN library major and minor version needs to match `\r\n\r\nInstall the required version\r\n`conda install cudnn==7.64`\r\n\r\n\r\n\r\n"]}, {"number": 41145, "title": "Sign compare warning fixes batch 9", "body": "in resolution of [-Wsign-compare] warnings with ids:\r\n[\r\n231, 237, 289, 291, 292, 304, 384, \r\n387, 389, 390, 391, 392, 393, 394, \r\n385, 395, 398, 399, 400, 407, 478, \r\n519\r\n]\r\n\r\n@mihaimaruseac", "comments": ["```\r\ntensorflow/compiler/mlir/tensorflow/transforms/promote_resources_to_args.cc:345:36: error: 'eturn_operands' was not declared in this scope\r\n   const int return_operands_size = eturn_operands.size();\r\n                                    ^~~~~~~~~~~~~~\r\ntensorflow/compiler/mlir/tensorflow/transforms/promote_resources_to_args.cc:345:36: note: suggested alternative: 'return_operands'\r\n   const int return_operands_size = eturn_operands.size();\r\n                                    ^~~~~~~~~~~~~~\r\n                                    return_operands\r\n```", "> ```\r\n> tensorflow/compiler/mlir/tensorflow/transforms/promote_resources_to_args.cc:345:36: error: 'eturn_operands' was not declared in this scope\r\n>    const int return_operands_size = eturn_operands.size();\r\n>                                     ^~~~~~~~~~~~~~\r\n> tensorflow/compiler/mlir/tensorflow/transforms/promote_resources_to_args.cc:345:36: note: suggested alternative: 'return_operands'\r\n>    const int return_operands_size = eturn_operands.size();\r\n>                                     ^~~~~~~~~~~~~~\r\n>                                     return_operands\r\n> ```\r\n\r\nThis is a file it seems I did not change.", "Right, it was a transient error fixed in the meanwhile"]}, {"number": 41144, "title": "Simplify code for keras_cpu_benchmark_test", "body": "Since we build the `benchmark_util` to get the results of each training process and I'd like to change the `keras_cpu_benchmark_test.py` to the same format with `keras_examples_benchmark_test.py`.", "comments": []}, {"number": 41143, "title": "keras preprocessing layer random generator fix", "body": "1. tracking state variable in stateful random generators;\r\n2. apply the random generator fix on other Ranodm___ layers to RandomCrop;\r\n3. add test coverage for training phase (previously only tested inference phase) for `image_preprocessing_distribution_test`", "comments": ["@yixingfu  Can you please resolve conflicts? Thanks!", "@yixingfu  Can you please resolve conflicts? Thanks!", "Merging a conflict dismissed the approval. Can you approve it again?  Thanks! @tanzhenyu ", "@yixingfu  Can you please address Ubuntu Sanity errors? Thanks!", "With the modified the test,  `test_distribution_test_distribution_MirroredCPUAndGPU_mode_graph` and `test_distribution_test_distribution_CentralStorageCPUAndGPU_mode_graph` are failing.  One of them has `NotImplementedError`. These were skipped when I tried locally, and are only exposed with the CI build. What should we do? @tanzhenyu ", "> With the modified the test, `test_distribution_test_distribution_MirroredCPUAndGPU_mode_graph` and `test_distribution_test_distribution_CentralStorageCPUAndGPU_mode_graph` are failing. One of them has `NotImplementedError`. These were skipped when I tried locally, and are only exposed with the CI build. What should we do? @tanzhenyu\r\n\r\nCan you set tf.config.set_soft_device_placement(True) for these and re-submit?", "> > With the modified the test, `test_distribution_test_distribution_MirroredCPUAndGPU_mode_graph` and `test_distribution_test_distribution_CentralStorageCPUAndGPU_mode_graph` are failing. One of them has `NotImplementedError`. These were skipped when I tried locally, and are only exposed with the CI build. What should we do? @tanzhenyu\r\n> \r\n> Can you set tf.config.set_soft_device_placement(True) for these and re-submit?\r\n\r\nThanks for the suggestion. I have pushed the change. It's not convenient for me to run any test locally right now, can you do a kokoro:force-run or approve to see if it works? ", "Oh sorry the importing was erroneous. Corrected. ", "Looks like something is failing?", "> Looks like something is failing?\r\n\r\nI didn't quite understand the failure log. To me it looks like the failures are not related to changes made here?", "@yixingfu Can you please resolve conflicts? Thanks!", "Any updates here?", "@yixingfu  Any update on this PR? Please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Hey @tanzhenyu, should I look into this?", "@yixingfu Can you please resolve conflicts? Thanks!", "It has been 29 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 41142, "title": "[CherryPick r2.3] Fix the profiler's step event naming", "body": "", "comments": []}, {"number": 41140, "title": "r2.3 cherry-pick request: Shift padded NMS compat window forward to fix TFLite conversion", "body": "Since the new implementation does not convert to TFLite, we are extending the compat window so that conversion works in TF2.3.\r\n\r\nPiperOrigin-RevId: 319856402\r\nChange-Id: I60eeb4e411609511046cc4676c2f3ad630b3769d", "comments": ["Letting @azaks3 decide if the cherry-pick can go in.\r\n\r\nNote that there is a Ubuntu CPU failure on the presubmits.", "Added Smit's changes for presubmit fix."]}, {"number": 41139, "title": "ERROR: No matching distribution found for tensorflow", "body": "I'm trying to install Tensorflow with python 3.8. After running the command `python3.8 -m pip install tensorflow` I got this error:\r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n```\r\n\r\n`pip debug --verbose` returns:`\r\n\r\n```\r\npip version: pip 20.1.1 from /home/****/ENV/lib/python3.8/site-packages/pip (python 3.8)\r\nsys.version: 3.8.0 (default, Nov 12 2019, 19:43:25)\r\n[GCC 5.4.0]\r\nsys.executable: /home/****/ENV/bin/python\r\nsys.getdefaultencoding: utf-8\r\nsys.getfilesystemencoding: utf-8\r\nlocale.getpreferredencoding: UTF-8\r\nsys.platform: linux\r\nsys.implementation:\r\n  name: cpython\r\n'cert' config value: :env:, install, wheel\r\nREQUESTS_CA_BUNDLE: None\r\nCURL_CA_BUNDLE: /etc/pki/tls/certs/ca-bundle.crt\r\npip._vendor.certifi.where(): /home/****/ENV/lib/python3.8/site-packages/pip/_vendor/certifi/cacert.pem\r\npip._vendor.DEBUNDLED: False\r\nvendored library versions:\r\n  appdirs==1.4.3\r\n  CacheControl==0.12.6\r\n  colorama==0.4.3\r\n  contextlib2==0.6.0.post1 (Unable to locate actual module version, using vendor.txt specified version)\r\n  distlib==0.3.0\r\n  distro==1.5.0 (Unable to locate actual module version, using vendor.txt specified version)\r\n  html5lib==1.0.1\r\n  ipaddress==1.0.23\r\n  msgpack==1.0.0 (Unable to locate actual module version, using vendor.txt specified version)\r\n  packaging==20.3\r\n  pep517==0.8.2\r\n  progress==1.5\r\n  pyparsing==2.4.7\r\n  requests==2.23.0\r\n  certifi==2020.04.05.1\r\n  chardet==3.0.4\r\n  idna==2.9\r\n  urllib3==1.25.8\r\n  resolvelib==0.3.0\r\n  retrying==1.3.3 (Unable to locate actual module version, using vendor.txt specified version)\r\n  setuptools==44.0.0 (Unable to locate actual module version, using vendor.txt specified version)\r\n  six==1.14.0\r\n  toml==0.10.0\r\n  webencodings==0.5.1 (Unable to locate actual module version, using vendor.txt specified version)\r\nCompatible tags: 30\r\n  cp38-cp38-linux_x86_64\r\n  cp38-abi3-linux_x86_64\r\n  cp38-none-linux_x86_64\r\n  cp37-abi3-linux_x86_64\r\n  cp36-abi3-linux_x86_64\r\n  cp35-abi3-linux_x86_64\r\n  cp34-abi3-linux_x86_64\r\n  cp33-abi3-linux_x86_64\r\n  cp32-abi3-linux_x86_64\r\n  py38-none-linux_x86_64\r\n  py3-none-linux_x86_64\r\n  py37-none-linux_x86_64\r\n  py36-none-linux_x86_64\r\n  py35-none-linux_x86_64\r\n  py34-none-linux_x86_64\r\n  py33-none-linux_x86_64\r\n  py32-none-linux_x86_64\r\n  py31-none-linux_x86_64\r\n  py30-none-linux_x86_64\r\n  cp38-none-any\r\n  py38-none-any\r\n  py3-none-any\r\n  py37-none-any\r\n  py36-none-any\r\n  py35-none-any\r\n  py34-none-any\r\n  py33-none-any\r\n  py32-none-any\r\n  py31-none-any\r\n  py30-none-any\r\n```\r\n\r\n**System information**\r\n- OS Platform and Distribution: CentOS Linux release 7.7.1908 (Core)\r\n- Python version: 3.8.0 / 64 bits\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- GCC/Compiler version: 5.4.0\r\n- pip version: 20.1.1\r\n\r\n\r\n\r\n\r\n**EDIT:**\r\nThe command bellow worked for me:\r\n`pip3 install --user tensorflow-gpu`", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41139\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41139\">No</a>\n"]}, {"number": 41138, "title": "[CherryPick r2.3] Fix the profiler's step event naming", "body": "", "comments": []}, {"number": 41137, "title": "Sign compare warning fixes batch 8", "body": "in resolution of [-Wsign-compare] warnings with ids: \r\n[\r\n220, 302, 382, 401, 415, 416, 417, 418,\r\n419, 422, 423, 424, 425, 426, 427, 431, \r\n469, 480, 481, 482, 484, 485, 488, 487, \r\n489, 500, 501, 502, 503,\r\n]\r\n\r\n@mihaimaruseac ", "comments": ["@mihaimaruseac , typo correction to fix failing build. ", "Can you fix build errors please?", "This still fails", "diagnosing.", "Closing as it has been handled by new PRs."]}, {"number": 41136, "title": "Add custom scalar loss to 'categorical_cross_entropy'", "body": "I wish to add a custom loss to regular cross-entropy used when we build/compile a tf model using model.compile()\r\nHowever, I get an error as provided in the below snapshot. \r\n\r\nCode:\r\ndef createEmbedding(features):\r\n  seq = Input(shape=(300,))\r\n  emb = Embedding(input_dim=len(vocab)+1,\r\n                  output_dim = 50,\r\n                  weights = [embedding_matrix],\r\n                  trainable=False)(seq)\r\n  emb_model = Model(seq, emb)\r\n  clean_emb = emb_model(features)\r\n  return clean_emb\r\n\r\ndef createModel(embedding_features):\r\n  emb_tensor = Input(shape=(300,50,))\r\n  hidden = LSTM(units=128)(emb_tensor)\r\n  output = Dense(units=32, activation='relu')(hidden)\r\n  model = Model(inputs=emb_tensor, outputs=output)\r\n  logits = model(embedding_features)\r\n  return emb_tensor, output, logits\r\n\r\ndef compute_kld(p_logit, q_logit):\r\n  p = tf.nn.softmax(p_logit)\r\n  q = tf.nn.softmax(q_logit)\r\n  kl_score = tf.reduce_sum( p * (tf.math.log(p+1e-16) - tf.math.log(q+1e-16)), axis = 1)\r\n  return kl_score\r\n\r\ndef calculateGradient(clean_features, noised_features):\r\n  with tf.GradientTape(watch_accessed_variables=False) as tape:\r\n    tape.watch(noised_features)\r\n    _, _, p_logit = createModel(clean_features)\r\n    _, _, p_logit_r = createModel(noised_features)\r\n    kl_score = compute_kld(p_logit, p_logit_r)\r\n    print(kl_score)\r\n  grads = tape.gradient(kl_score, noised_features)\r\n  return grads\r\n\r\nfeatures, labels = next(iter(train_dataset))\r\n\r\nclean_features = createEmbedding(features)\r\nnoised_features = tf.add(clean_features, 0.01)\r\n\r\nprint('Clean Embedding: ', type(clean_features), clean_features.shape) # <class 'tensorflow.python.framework.ops.EagerTensor'> (1024, 300, 50)\r\nprint('Noised Embedding: ', type(noised_features), noised_features.shape) # <class 'tensorflow.python.framework.ops.EagerTensor'> (1024, 300, 50)\r\n\r\nclean_ip_tensor, clean_op_tensor, p_logit = createModel(clean_features)\r\nnoise_ip_tensor, noise_op_tensor, p_logit_r = createModel(noised_features)\r\nprint('P_Logit: ',type(p_logit), p_logit.shape) # <class 'tensorflow.python.framework.ops.EagerTensor'> (1024, 32)\r\nprint('P_Logit_R: ',type(p_logit_r), p_logit_r.shape) # <class 'tensorflow.python.framework.ops.EagerTensor'> (1024, 32)\r\n\r\ngrads = calculateGradient(clean_features, noised_features)\r\nnorm_ball = tf.math.l2_normalize(grads, axis=None, epsilon=1e-12, name=None)\r\ntype(norm_ball), norm_ball.shape\r\nrvadv = (grads/norm_ball) * -1\r\nvadv_features = tf.add(clean_features, rvadv)\r\n\r\nvat_ip_tensor, vat_op_tensor, q_logit = createModel(vadv_features)\r\nvat_loss = compute_kld(p_logit, q_logit) # <tf.Tensor: shape=(1024,), dtype=float32; these will be scalar value\r\n# I wish to add this vat_loss to regular categorical cross entropy loss\r\n\r\np = Dense(units=1, activation='softmax')(clean_op_tensor)\r\nmodel = Model(inputs=clean_ip_tensor, outputs=p)\r\nmodel.add_loss(vat_loss) # Error\r\nmodel.summary()\r\n\r\n# Future implementation\r\nmodel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\nmodel.fit(clean_features, labels)\r\n\r\n![Screenshot 2020-07-06 at 22 16 46](https://user-images.githubusercontent.com/63437596/86637762-682e6900-bfd6-11ea-8e46-cfd329eed5e6.png)\r\n\r\nAny suggestions on how can I work around this scenario. \r\nThanks for your time and consideration\r\n", "comments": ["@lokesharma-dev \r\nPlease provide with indented code such that we can replicate the issue faced, we ran the code shared, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e0fc9ec4cce70933d69d50db9300d081/untitled262.ipynb) for the same.\r\nor if possible share a colab gist with the error faced.", "@Saduf2019 . Please find the colab link: \r\nhttps://colab.research.google.com/drive/1kCh04Wb0w3O_0ikuYY_Q8OwTlwwM2Sh-?usp=sharing\r\n\r\nThe data used for the above colab: \r\n[npy.zip](https://github.com/tensorflow/tensorflow/files/4886027/npy.zip)\r\n\r\nWhat I attempt of doing is from this gist [https://gist.github.com/divamgupta/c778c17459c1f162e789560d5e0b2f0b](url). However, the example is explained in TensorFlow 1.x. Any suggestion on how to implement the same in version 2.x.\r\n\r\nThanks"]}]