[{"number": 18861, "title": "Running two Models in two GPUs for prediction in c++", "body": "### System information\r\n- **Have I written custom code  (as opposed to using a stock example script provided in TensorFlow): N/A\r\n- **OS Platform and Distribution (e.g., Debian 9.1):\r\n- **TensorFlow installed from (source bazel):\r\n- **TensorFlow version (use command below) 1.7:\r\n- **Python version 3.6: \r\n- **Bazel version (if compiling from source) 0.11:\r\n- **GCC/Compiler version (if compiling from source GCC)6.3:\r\n- **CUDA/cuDNN version 9.1. cudnn 7.1:\r\n- **GPU model and memory 2 GPU GTX 1080 TI, 11GB:\r\n- **Exact command to reproduce: compiling by c++  \r\n\r\n\r\n### Describe the problem\r\n   i would like to use two gpus at the same time for make a preduction of two models by using this of code: session_options.config.mutable_gpu_options()->set_visible_device_list(\"0\"); unfortunatlly i got this error (Invalid allocator type: 0):\r\n2018-04-25 14:35:17.895671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-04-25 14:35:17.895717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-04-25 14:35:17.895722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 \r\n2018-04-25 14:35:17.895727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N \r\n2018-04-25 14:35:17.895961: E tensorflow/core/common_runtime/gpu/process_state.cc:125] Invalid allocator type: 0\r\n2018-04-25 14:35:17.895977: E tensorflow/core/common_runtime/direct_session.cc:167] Internal: Failed to get memory allocator for TF GPU 0 with 10913103872 bytes of memory.\r\n\r\n### Source code / logs\r\n    session_options.config.mutable_gpu_options()->set_visible_device_list(\"0\");   --->error is here\r\n    session_options.config.mutable_gpu_options()->set_allow_growth(true);\r\n    session.reset(tensorflow::NewSession(tensorflow::SessionOptions(session_options)));\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Running multiple models in parallel with different GPUs in Tensorflow for Prediction:\r\n\r\n### Describe the problem\r\nI would like to run two models for prediction in two different GPUs to speed up the running time (by creating two instance of sessions in two GPUs).\r\nthe first model is computing to the first GPU and the second is computing in the second GPU by executing each session for a specific device, something like that in python \"with tf.device('/gpu:0') \"\r\n\r\n### Source code / logs\r\n\r\n    int GPUID = std::stoi(params->getGpuDeviceStr());\r\n    setenv(\"CUDA_VISIBLE_DEVICES\", \"\", GPUID);\r\n\r\n    std::cout << \"Initial  visible_device_list : \"<<session_options.config.gpu_options().visible_device_list() << std::endl;\r\n    session_options.config.mutable_gpu_options()->set_allow_growth(true);\r\n    session_options.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(\r\n            params->getGpuMemoryRatio());\r\n\r\n### output:\r\n2018-04-30 10:18:56.625199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:09:00.0\r\ntotalMemory: 10,91GiB freeMemory: 10,75GiB\r\n2018-04-30 10:18:56.750435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:42:00.0\r\ntotalMemory: 10,91GiB freeMemory: 10,42GiB\r\n2018-04-30 10:18:56.751296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1223] Device peer to peer matrix\r\n2018-04-30 10:18:56.751324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1229] DMA: 0 1 \r\n2018-04-30 10:18:56.751332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 0:   Y Y \r\n2018-04-30 10:18:56.751337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 1:   Y Y \r\n2018-04-30 10:18:56.751345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1308] Adding visible gpu devices: 0, 1\r\n2018-04-30 10:18:57.110046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10055 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1)\r\n2018-04-30 10:18:57.110819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10050 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1)\r\nRunning tensorflow in Version 1.5.0\r\n\r\n### Discussion:\r\ni cannot see any improvement in time-consuming by running two model simultaneously by creating two threads to run two models in two GPUs. the time consuming by running two model in two GPUs is approximately the same as running by using only one GPU. \r\n\r\n", "Unfortunately, though `visible_deivces_list` is included in `ConfigProto`, it is actually a per-process setting. In fact, this is true of almost all options inside the [`GPUOptions`](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/core/protobuf/config.proto#L123) protocol buffer. \r\n\r\nApologies for that.\r\n\r\nIn the mean time, a possible work around is to have the models themselves explicitly use different GPUs. You can do so by explicitly constructing the models as so (e.g.., using `with tf.device('/gpu:1')` when creating the graph in a Python program), or editing the graph before creating the session (a similar workaround [in Java was suggested on StackOverflow](https://stackoverflow.com/a/47915987/6708503)).\r\n\r\nHope that helps in the short term.\r\n\r\nAssigning to @yifeif - who is looking into at least making this subtlety more explicit and then perhaps even overcoming this limitation.\r\n\r\nAlso CCing @akshayka who was looking into more dynamic assignment of devices to models at runtime.", "thanks a lot, @asimshankar for your answer. I will try to do something similar to Java. ", "For the record, this mechanism works just fine in Tensorflow 1.3 and Tensorflow 1.5. It broke in the transition to 1.6. We are remaining on 1.3 until this is resolved.\r\n", "@rundembear : To confirm, are you saying that the mechanism suggested in https://github.com/tensorflow/tensorflow/issues/18861#issuecomment-385610497 works fine in 1.3 and 1.5, but not 1.6?\r\n\r\nIf so, what fails in 1.6? Could you add some detail here? Ideally a [minimal, complete, verifiable](https://stackoverflow.com/help/mcve) snippet if possible.", "@asimshankar  I entered a separate ticket for this, sorry, should have had this link in my comment. This is via python, but it is, I believe, the same mechanism. I am trying to do this with multiple copies of the same model, but still essentially the same issue. This includes a very small code snippet which runs fine on 1.3 and 1.5. and crashes from 1.6 onwards.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/19083\r\n\r\n```\r\nimport tensorflow as tf\r\nG =tf.Graph()\r\nsess1 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='0')))\r\nsess2 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='1')))\r\n```", "@asimshankar Just a bump on this, it is still tagged 'awaiting response' although I responded five days ago. Thanks!", "@rundembear : Sorry, I missed your update somehow.\r\nAnyway, I see that it crashes with an unhelpful message to the user, which should be fixed.\r\n\r\nHowever, I don't think it quite worked in 1.3 either - in that it wouldn't crash, but it would basically end up ignoring the `visible_device_list` the second time around. I'll dig in further (don't have a multi-GPU machine handy at the moment), but could you verify that in 1.3 it wasn't actually using two GPUs?\r\n\r\nAlso CCing @yifeif who is working to make these error messages clearer.", "@asimshankar and @yifeif  I have just run a very simple test using our production code. I logged into a machine with 8 GPUs and ran a command which allows me to run my graph, one thread per GPU (but all in the same process). If I use 'visible_device_list' to map the correct GPU for each thread to be 0 at that thread, I see load distributed across all 8 GPUs (using nvidia-smi) and I can execute a typical request in 2 seconds. If I comment out the 'visible_device_list' (and this is the only change) I see load only on GPU 0 and the same request takes 8.2 seconds. Which sounds about right as our code currently only hits about 50% GPU utilization at each GPU (trust me, I'm working on that). \r\nSo I feel fairly strongly that this is working as expected in 1.3.", "Thanks @rundembear \r\n\r\n@aaroey : Is it possible that the changes to support virtual devices broke this?\r\nI see that [in the r1.5 branch](https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/core/common_runtime/gpu/gpu_device.cc#L756, we're creating `Device*` objects based on the `visible_devices_list` value.\r\n\r\nBut by the [1.6 branch](https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/common_runtime/gpu/gpu_device.cc#L930) we've started check-failing when [there is a mapping change between `TfGpuId` and `CudaGpuId`](https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/common_runtime/gpu/gpu_id_utils.cc#L67)\r\n\r\nSo I suspect this was indeed broken by https://github.com/tensorflow/tensorflow/commit/14cb8e14a8fb1e78e2ce623e4198972762e6e253\r\n\r\nSorry for the trouble @rundembear - I was mistaken about which fields were process-global in 1.3.", "@asimshankar No trouble at all. I appreciate the need to be sure before spending too much time on such things. Thanks!\r\n ", "Hi @rundembear,\r\n\r\nThanks for pointing out this. After some discussion, we think that the way you configure TF gpu device is not truly supported by TF, even in v1.3. If I understand correctly, your program has multiple threads, each creates a device with same name (\"/gpu:0\") but pointing to different physical gpu hardware. This will cause problem when any code/lib wants to access the gpu hardware using the gpu id in the device name, as all access will be directed to the first gpu, not the gpu that the device pointing to. There are actually several code paths like this in TF, for example, grappler will extract the id from the device name and use it to call methods like `cudaGetDeviceProperties()`, so regardless which actual device your graph is using, grappler will always use the information from the first physical gpu to do the optimization. There are other use cases, including NCCL, TensorRT integration, and maybe others that I don't know.\r\n\r\nAbove all, the way how you configure TF gpu may work under certain circumstances (like what you observed), but is a way that TF doesn't intend to support. In general, TF doesn't fully support different config proto within same process, and different sessions should use same gpu config. As a result, I would suggest, if possible, to list all available gpu ids in `visible_device_list`, and use one for each graph. Or we can discuss more if you have any questions.\r\n\r\nThanks.", "@aaroey Thank you very much for your explanation. I don't feel like our language is quite lining up so I'm not sure whether your concern is warranted.  I am not doing anything to 'rename' the GPUs. As far as I know /gpu:3 is still /gpu:3 but I have created a tensorflow session in which reference to /gpu:0 will be redirected to /gpu:3. So if this mechanism were working as described the problem you raise would be a non-issue. If the graph has been optimized so that all of the operations are efficiently scheduled to run on /gpu:0, but within a session this is remapped then I should still be running the same graph on all GPUs. \r\n\r\nI have a feeling that what you are saying is that this mechanism should never have existed and there isn't actually a use case for which it makes sense, but perhaps I am missing something?\r\n\r\nOr are you saying that there are operations that occur outside of any sessions that see all of the GPUs, and thus might do the wrong thing? But is this really a problem if the entire graph is assigned to GPU 0?\r\n\r\nLet me clarify what our use case is to explain why this not being supported is rather unfortunate. We are carrying out Bayesian Inference and distributing our sampling across multiple GPUs. The thing is, we are running exactly the same graph on every GPU. If we do things as you suggest then I need to actually create N copies of the graph with each graph being run on a different GPU. Alternatively I can rearchitect my code into a master/servant model using multiprocessing and have each process manipulate CUDA_VISIBLE_DEVICES so that it can only see one GPU. While you may have decided that this is not something that you want to support this is exactly what the documentation says this mechanism is for. ", "Hi @rundembear,\r\n\r\nSorry for the confusion, let me clarify:\r\n1. it's totally fine to use different configs in different process, there is no restriction on that. For example, you may set CUDA_VISIBLE_DEVICES=1 in one process and 2 in another, you may also set visible_gpu_device or any other ConfigProto options differently.\r\n1. in the same process, if possible, we should use same [GPUOptions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L16) for all sessions, as most of the options inside GPUOptions are process-wide (AFAIK, per_process_gpu_memory_fraction, allocator_type, allow_growth, visible_device_list, experimental are all process-wide options). If we use different GPUOptions for different sessions, unexpected behavior may/would occur, depending on which options you set differently.\r\n\r\nIn your case, specifically for the code:\r\n```\r\nG =tf.Graph()\r\nsess1 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='0')))\r\nsess2 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='1')))\r\n```\r\n\r\n`sess1` and `sess2` are in same process and using same graph but different GPUOptions options (visible_device_list is different).\r\n\r\n- For `sess1`, a `BaseGPUDevice` will be created with name '/gpu:0' and pointing to **physical gpu 0**\r\n- For `sess2`, a `BaseGPUDevice` will be created with name '/gpu:0' but pointing to **physical gpu 1**.\r\n\r\nNote that both device have same name '/gpu:0', so any code that use only the device name (not the BaseGPUDevice object itself) to access the physical gpu will be directed to physical gpu 0, this is what I called 'unexpected behavior'. For example, assume that `G` has a node placed in '/gpu:0', grappler will use the information from physical gpu 0 to optimize your graph in `sess2` which is not what we want.\r\n\r\nIn order to apply the same graph to different gpu, we can use `with tf.device()` with different device name when building/importing the graph.\r\n\r\nThanks.", "@aaroey Thank you so much for the clarification. In theory, as our current system is set up in a SIMD-like way, what we are doing quite possibly is fine, but I understand that that is by accident rather than design and that there may well be side-effects which at some point will bite us. It also sounds as if we would be best-advised to rework our code to behave in a supported way. \r\n\r\nYou have definitely given me enough to go on in terms of things to try and changes that will definitely work.\r\n\r\nThanks again for your time and patience!", "Thanks @rundembear for understanding. :)", "I'm closing this issue, feel free to reopen if there are any other questions.", "@aaroey  One more question. I just tried reading multiple copies of my graph using this:\r\n\r\n        with open(graphfile, 'rb') as f:\r\n            graph_text = f.read()\r\n\r\n        for g in range(len(graph_object.devices)):\r\n            graph_object.graphs.append(tf.Graph())\r\n            with tf.device(graph_object.devices[g]):\r\n                graph_def = tf.GraphDef()\r\n                graph_def.ParseFromString(graph_text)\r\n                with graph_object.graphs[g].as_default():\r\n                    tf.import_graph_def(graph_def, name='')\r\n\r\n\r\nI ran this on 3 GPUs, so the devices list was:  ['/gpu:0', '/gpu:1', '/gpu:2']\r\n\r\nDoing this places all of the load onto GPU 0. \r\n\r\nSo when you said this: \r\n\r\n\"In order to apply the same graph to different gpu, we can use with tf.device() with different device name when building/importing the graph.\"\r\n\r\neither I am misunderstanding what you meant, or this mechanism does not work as you had hoped. \r\n\r\nIf I am missing something, please correct what I am doing wrong; Otherwise it looks like the only way to accomplish what I want to do is to use multi-processing. That certainly is an option, just wanted to try this idea first. \r\n\r\nBTW, I also tried using the 'with tf.device()' when I created the session and when I ran the session. Still sends everything to GPU 0. Sigh.", "Hi @rundembear,\r\n\r\nI think we need to put `with graph_object.graphs[g].as_default():` before `with tf.device(graph_object.devices[g]):`, as `tf.device()` use [`get_default_graph().device()`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L4991).\r\n\r\nI tested the following code and the printed graphdef has the correct device assigned (I haven't run the graph though):\r\n```\r\nfrom google.protobuf import text_format\r\nimport tensorflow as tf\r\n\r\nhello = tf.constant('Hello, TensorFlow!')\r\ntf.train.write_graph(tf.get_default_graph(), '/tmp', 'g1')\r\n\r\nwith open('/tmp/g1', 'rb') as f:\r\n  graph_text = f.read()\r\nprint(graph_text)\r\nprint('-' * 100)\r\n\r\ngraphs = []\r\nfor g in range(2):\r\n  graphs.append(tf.Graph())\r\n  with graphs[g].as_default():\r\n    graph_def = tf.GraphDef()\r\n    text_format.Merge(graph_text, graph_def)\r\n    with tf.device('/gpu:' + str(g)):\r\n      tf.import_graph_def(graph_def, name='')\r\n    print(tf.get_default_graph().as_graph_def())\r\n    print('-' * 100)\r\n```\r\n\r\nThanks.", "@aaroey You rock!!! That does the trick! Thank you so much for all of your help.\r\n\r\n", "@rundembear You're very welcome, and I'm glad that we can help. Let me know if you have any other questions. :)", "@kerolos \r\n\r\nIn c++ this can be done with graph modification before creation the session, something like this:\r\n\r\n```\r\ntensorflow::Session * load_graph(const std::string &path, const std::string &device = std::string())\r\n{\r\n    tensorflow::GraphDef graph_def;\r\n    tensorflow::ReadBinaryProto(tensorflow::Env::Default(), path, &graph_def);\r\n\r\n    if(!device.empty())\r\n    {\r\n        for(int i = 0; i < graph_def.node_size(); ++i)\r\n        {\r\n            graph_def.mutable_node(i)->set_device(device);\r\n        }\r\n    }\r\n\r\n    tensorflow::Session *session;\r\n    tensorflow::NewSession(tensorflow::SessionOptions(), &session);\r\n    session->Create(graph_def);\r\n    return session;\r\n}\r\n\r\n...\r\n\r\ntensorflow::Session *session_gpu_0 = load_graph(graph_file_path, \"/gpu:0\");\r\ntensorflow::Session *session_gpu_1 = load_graph(graph_file_path, \"/gpu:1\");\r\n\r\n```\r\n\r\n", "I met the issuse at tf1.6 as well.", "@nikkymen \r\n\r\nI'm using tf::LoadSavedModel to load the model. How can I specify the gpu divice with this c++ API?", "@x10000year \r\nIt seems to me that this is impossible.\r\n\r\nYou can try to modify the implementation of tf::LoadSavedModel from \r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/loader.cc](url)\r\n\r\nWith the modification of `GraphDef` from my post, before the session is created, you should get what you intended.", "@nikkymen \r\nThanks for your solution, but is this only suit for Linux? I tried on windows and an error occurs:\r\n\"Invalid argument: No attr with name 'gpu:0 for input 'input'\"", "@Rongnian \r\nI tested it on Linux, but I see no reason that it does not work under Windows. Probably, under Windows other identifiers for devices are necessary.", "@Rongnian \r\nI don't know if this is still an issue for you but I experienced the same error today. I found a solution that worked for me: only set the device for nodes where the device string is non-empty.\r\n\r\n```\r\nfor(int i = 0; i < graph_def.node_size(); ++i)\r\n{\r\n      if (graph_def.mutable_node(i)->device().empty() == false)\r\n            graph_def.mutable_node(i)->set_device(device);\r\n}\r\n```\r\n\r\nAnd the device string that I set it to is /device:GPU:x (you can simply check the format that is used by printing the current value).", "> Hi @rundembear,\r\n> \r\n> I think we need to put `with graph_object.graphs[g].as_default():` before `with tf.device(graph_object.devices[g]):`, as `tf.device()` use [`get_default_graph().device()`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L4991).\r\n> \r\n> I tested the following code and the printed graphdef has the correct device assigned (I haven't run the graph though):\r\n> \r\n> ```\r\n> from google.protobuf import text_format\r\n> import tensorflow as tf\r\n> \r\n> hello = tf.constant('Hello, TensorFlow!')\r\n> tf.train.write_graph(tf.get_default_graph(), '/tmp', 'g1')\r\n> \r\n> with open('/tmp/g1', 'rb') as f:\r\n>   graph_text = f.read()\r\n> print(graph_text)\r\n> print('-' * 100)\r\n> \r\n> graphs = []\r\n> for g in range(2):\r\n>   graphs.append(tf.Graph())\r\n>   with graphs[g].as_default():\r\n>     graph_def = tf.GraphDef()\r\n>     text_format.Merge(graph_text, graph_def)\r\n>     with tf.device('/gpu:' + str(g)):\r\n>       tf.import_graph_def(graph_def, name='')\r\n>     print(tf.get_default_graph().as_graph_def())\r\n>     print('-' * 100)\r\n> ```\r\n> Thanks.\r\n\r\nI wonder how to integrate this with `tf.contrib.predictor.from_saved_model`: for the same reason I cannot set different devices in different `tf.predictor.from_saved_model(<model_folder>, config=tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='<devices>')))`\r\n\r\n**Update**\r\nI solved it by specifying the `graph` argument in `tf.contrib.predictor.from_saved_model`:\r\n\r\n```\r\npredict_fn = []\r\nfor gpu in range(n_gpus):\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        with tf.device('/gpu:%d' % gpu):\r\n            tf.import_graph_def(graph_def)  # loaded earlier\r\n            predict_fn.append(predictor.from_saved_model(model_folder, graph=graph, config=tf.ConfigProto(allow_soft_placement=True)))\r\n```", "@nikkymen Have you tried the c++ fix in a recent tensorflow build? Setting devices using your code doesn't seem to work for me because of libprotobuf string issues. Any suggestions? ", "hi all,  I find a solution for using multiple session with multiple gpu cards, follow are some main steps, \r\n1. if you build libtensorflow_cc.so with monolithic configure, please add protobuf in tf_version_script.lds\r\n\r\n2. if you want to bind a gpu card with multiple session, you can use virtual_devices to split gpu memory\r\n```\r\n // session options are process level configure\r\n  tensorflow::SessionOptions options;\r\n  tensorflow::ConfigProto &config = options.config;\r\n  config.mutable_gpu_options()->set_visible_device_list(\"0,1\");\r\n  config.mutable_gpu_options()->set_allow_growth(false);\r\n  // config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(0.8);\r\n  auto vd1 = config.mutable_gpu_options()->mutable_experimental()->add_virtual_devices();\r\n  vd1->add_memory_limit_mb(3000.0);\r\n  vd1->add_memory_limit_mb(6000.0);\r\n  auto vd2 = config.mutable_gpu_options()->mutable_experimental()->add_virtual_devices();\r\n  vd2->add_memory_limit_mb(2000.0);\r\n  vd2->add_memory_limit_mb(5000.0);\r\n```\r\n3. create a session that bind a graph with compute on the given virtual device,\r\n```\r\n// modify the graph def\r\nfor(int i = 0; i < graph_def.node_size(); ++i) {\r\n   graph_def.mutable_node(i)->set_device(device);\r\n}\r\n```", "@jlzhann That sounds really interesting. I will give it a try!  \r\nThanks for your help", "@nikkymen  @jlzhann  @sorny92 \r\nIn C++ you can set device for a graph when loading it with  [`TF_ImportGraphDefOptionsSetDefaultDevice`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h#L722) (or its C++ equivalent, I'm not on the TF C++ stack, just casually P/Invoking the API in C#).", "@nikkymen @jlzhann  @sorny92 @Franziska-Mueller\r\nhave you run two models in two GPUs successfully in C++? I have tried with many method as is mentioned above, but failed.\r\nit seems that the model graph has some operations that have no GPU kernels, so we can't set the graph device explicitly.\r\nand my tensorflow version is 1.12\r\n\r\nthe error information is below:\r\n`Invalid argument: Cannot assign a device for operation model/inference/assert_greater_equal/Assert/Assert: Could not satisfy explicit device specification '/device:GPU:1' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'\r\n\r\n         [[{{node model/inference/assert_greater_equal/Assert/Assert}} = Assert[T=[DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/device:GPU:1\"](model/inference/assert_greater_equal/All, model/inference/assert_greater_equal/Assert/Assert/data_0, model/inference/assert_greater_equal/Assert/Assert/data_1, phones, model/inference/assert_greater_equal/Assert/Assert/data_3, model/inference/Const)]]\r\n`", "@bit-ghost be sure to [allow soft placement](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L424) in ConfigProto", "@hillin \r\nthanks for your help! It succeeded. and sorry for disturbing you because i'm not familiar with tensorflow.\r\nAdditionally, we can use `tensorflow::graph::SetDefaultDevice(\"/device:GPU:1\", pgraph_def);` to bind specific GPU device with a graph ", "In python it can be done as follows:\r\n\r\n```\r\ndef get_frozen_graph(graph_file):\r\n    \"\"\"Read Frozen Graph file from disk.\"\"\"\r\n    with tf.gfile.GFile(graph_file, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n    return graph_def\r\n\r\ntrt_graph1 = get_frozen_graph('/home/ved/ved_1/frozen_inference_graph.pb')\r\n\r\nwith tf.device('/gpu:1'):\r\n    [tf_input_l1, tf_scores_l1, tf_boxes_l1, tf_classes_l1, tf_num_detections_l1, tf_masks_l1] = tf.import_graph_def(trt_graph1, \r\n                    return_elements=['image_tensor:0', 'detection_scores:0', \r\n                    'detection_boxes:0', 'detection_classes:0','num_detections:0', 'detection_masks:0'])\r\n    \r\ntf_sess1 = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\r\n\r\ntrt_graph2 = get_frozen_graph('/home/ved/ved_2/frozen_inference_graph.pb')\r\n\r\nwith tf.device('/gpu:0'):\r\n    [tf_input_l2, tf_scores_l2, tf_boxes_l2, tf_classes_l2, tf_num_detections_l2] = tf.import_graph_def(trt_graph2, \r\n                    return_elements=['image_tensor:0', 'detection_scores:0', \r\n                    'detection_boxes:0', 'detection_classes:0','num_detections:0'])\r\n    \r\ntf_sess2 = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\r\n```\r\n", "> @nikkymen\r\n> \r\n> I'm using tf::LoadSavedModel to load the model. How can I specify the gpu divice with this c++ API?\r\n\r\n  tensorflow::RunOptions run_options;\r\n  //run_options.set_timeout_in_ms(100);\r\n  std::string tf_model_path = path_prefix + \"/\" + name_prefix;\r\n  tensorflow::Status status = tensorflow::LoadSavedModel(options, run_options, tf_model_path, tags, _bundle_wrapper->_bundle.get());\r\n  if (!status.ok()) {\r\n    printf(\"Loading tf model failed. dir[ %s ], status[ %s ]\\n\", tf_model_path.c_str(), status.ToString().c_str());\r\n    return -1;\r\n  }\r\n  printf(\"Loading tf model success. dir[ %s ], status[ %s ]\\n\", tf_model_path.c_str(), status.ToString().c_str());\r\n\r\n  tensorflow::MetaGraphDef graph_def = _bundle_wrapper->_bundle->meta_graph_def;\r\n  std::unique_ptr<tensorflow::Session>& session = _bundle_wrapper->_bundle->session;\r\n\r\n  tensorflow::graph::SetDefaultDevice(\"/gpu:0\", &graph_def.graph_def());\r\n  session->Create(graph_def.graph_def());\r\n", "> > @nikkymen\r\n> > I'm using tf::LoadSavedModel to load the model. How can I specify the gpu divice with this c++ API?\r\n> \r\n> tensorflow::RunOptions run_options;\r\n> //run_options.set_timeout_in_ms(100);\r\n> std::string tf_model_path = path_prefix + \"/\" + name_prefix;\r\n> tensorflow::Status status = tensorflow::LoadSavedModel(options, run_options, tf_model_path, tags, _bundle_wrapper->_bundle.get());\r\n> if (!status.ok()) {\r\n> printf(\"Loading tf model failed. dir[ %s ], status[ %s ]\\n\", tf_model_path.c_str(), status.ToString().c_str());\r\n> return -1;\r\n> }\r\n> printf(\"Loading tf model success. dir[ %s ], status[ %s ]\\n\", tf_model_path.c_str(), status.ToString().c_str());\r\n> \r\n> tensorflow::MetaGraphDef graph_def = _bundle_wrapper->_bundle->meta_graph_def;\r\n> std::unique_ptrtensorflow::Session& session = _bundle_wrapper->_bundle->session;\r\n> \r\n> tensorflow::graph::SetDefaultDevice(\"/gpu:0\", &graph_def.graph_def());\r\n> session->Create(graph_def.graph_def());\r\n\r\nWhen trying to use this method I have the following error \r\n\r\n```\r\n2021-09-16 09:52:20.248610: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: Invalid argument: Converting GraphDef to Graph has failed with an error: 'Cannot add function '__inference__traced_restore_73624' because a different function with the same name already exists.' The binary trying to import the GraphDef was built when GraphDef version was 808. The GraphDef was produced by a binary built when GraphDef version was 838. The difference between these versions is larger than TensorFlow's forward compatibility guarantee, and might be the root cause for failing to import the GraphDef.\r\n2021-09-16 09:52:20.282896: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] layout failed: Not found: No attr named '/gpu:2' in NodeDef:\r\n         [[{{node Conv1/kernel}}]]\r\n2021-09-16 09:52:20.319407: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: Invalid argument: Converting GraphDef to Graph has failed with an error: 'Cannot add function '__inference__traced_restore_73624' because a different function with the same name already exists.' The binary trying to import the GraphDef was built when GraphDef version was 808. The GraphDef was produced by a binary built when GraphDef version was 838. The difference between these versions is larger than TensorFlow's forward compatibility guarantee, and might be the root cause for failing to import the GraphDef.\r\nSegmentation fault (core dumped)\r\n```\r\nAre there any updates with the LoadSavedModel c++ API to use multiple GPU???"]}, {"number": 18860, "title": "Change the swap threshold by adding an elastic percentage", "body": "Originally, heuristic memory swapping feature won't improve much on the batch size, that's because IdentifySwappingCandidates are fully trusting the statically analysis.\r\nPer my experiment, this did not improve the batch size at all. \r\n\r\nWhile If we make the swapping threshold to be lower,for example, when analyzed memory usage is 0.8 * GPU mem size, then swapping feature is triggered. \r\n\r\nAccording to my experiments, for my K80 (12G), ResNet50, batch size can be improved from 128 to 150. ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "I signed it!", "I signed it!", "seems, i was using the wrong email to submit, discard this, and will create a new pull request. "]}, {"number": 18859, "title": "trying to retrain my CNN model in digits recognition", "body": "", "comments": ["`Caused by op 'convolutional/Placeholder', defined at:\r\n  File \"run.py\", line 12, in <module>\r\n    from app import app\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/Users/mac/Desktop/PythonApi2/app/__init__.py\", line 6, in <module>\r\n    from app import views\r\n  File \"<frozen importlib._bootstrap>\", line 1017, in _handle_fromlist\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/Users/mac/Desktop/PythonApi2/app/views.py\", line 13, in <module>\r\n    keep_prob = tf.placeholder(tf.float32)\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1777, in placeholder\r\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4521, in placeholder\r\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\r\n    op_def=op_def)\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'convolutional/Placeholder' with dtype float\r\n\t [[Node: convolutional/Placeholder = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]`", " train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\n sess.run(train_step , feed_dict={x: image, y_: label})", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "im  using MAC OS X high Sierra \r\ntensorflow 1.7.0\r\n\r\nhere is my code \r\n\r\nthanks a lot ! \r\n<img width=\"881\" alt=\"capture d ecran 2018-04-26 a 12 00 05 pm\" src=\"https://user-images.githubusercontent.com/32748111/39302097-691031ec-4949-11e8-81c8-b797ddd6750f.png\">\r\n", "i didn't find how to solve this problem ! ", "Nagging Assignee @bignamehyp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "There is a bug in your user code.  So this question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation. Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 18858, "title": "How to show the loss curve of training set and validation set at the same time using the customed estimator?", "body": "Hi, recently I used [custom_estimator.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/get_started/regression/custom_regression.py) to build regression model.  In order to clear out the changes of loss value in the training set and validation set. I need to know that how to show the loss curve of training and validation set at the same time. I tried to use train_and_evaluate api of estimator and i got the following picture.\r\n![image](https://user-images.githubusercontent.com/19348442/39224762-76df9b92-487b-11e8-888d-b64825e99c82.png)\r\nAs it show, the result of evaluation is a point, but i want a line like the loss curve of training set. Just like the picture as shown below.\r\n![image](https://user-images.githubusercontent.com/19348442/39280146-2d129f22-4930-11e8-81e7-a2379b924eb0.png)\r\nHere is my system information:\r\n\r\n- Have i written custom code: N/A\r\n\r\n- OS: Tested on windows 10 1709.\r\n\r\n- Tensorflow installed from Anaconda 5.1.0 with python 3.6.4\r\n\r\n- Tensorflow version-tested on tensorflow-gpu 1.7.0\r\n\r\n- CUDA/cuDNN version: 9.0 for TF 1.7\r\n\r\n- GPU mode: Nvidia Quadro  K2100M\uff0c 2G of memory\r\n\r\n- Bazel version: N/A\r\n\r\n- Exact command to reproduce: N/A\r\nHere is the customed estimator:\r\n\r\n```\r\ndef my_dnn_regression_fn(features, labels, mode, params):\r\n    top = tf.feature_column.input_layer(features, params['feature_columns'])\r\n\r\n    for units in params.get('hidden_units', [20]):\r\n        top = tf.layers.dense(inputs=top, units=units, activation=tf.nn.relu)\r\n\r\n    output_layer = tf.layers.dense(inputs=top, units=1)\r\n\r\n    output_layer = tf.cast(output_layer, tf.float64)\r\n   \r\n    predictions = tf.squeeze(output_layer, 1)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        # In 'PREDICT' mode we only need to return predictions.\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode, predictions={\"predictions\": predictions})\r\n\r\n    # calculate the loss using mean squared error\r\n    average_loss = tf.losses.mean_squared_error(labels, predictions)\r\n\r\n    # Pre-made estimators use the total_loss instead of the average,\r\n    # so report total_loss for compatibility.\r\n    batch_size = tf.shape(labels)[0]\r\n    total_loss = tf.to_float(batch_size) * average_loss\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = params.get(\"optimizer\", tf.train.AdamOptimizer)\r\n        optimizer = optimizer(params.get(\"learning_rate\", None))\r\n        train_op = optimizer.minimize(\r\n            loss=average_loss, global_step=tf.train.get_global_step())\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode, loss=total_loss, train_op=train_op)\r\n\r\n    # In the evaluation mode we will calculate evaluation metrics.\r\n    assert mode == tf.estimator.ModeKeys.EVAL\r\n\r\n    # Calculate root mean squared error\r\n    rmse = tf.metrics.root_mean_squared_error(labels, predictions)\r\n\r\n    # Add the rmse to collection of evaluation metrics.\r\n    eval_metrics = {\"rmse\": rmse}\r\n\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=mode,\r\n        # Report sum of error for compatibility with pre-made estimators.\r\n        loss=total_loss,\r\n        eval_metric_ops=eval_metrics)\r\n````\r\n\r\nAnd here I used train_and_evaluate api like this:\r\n```\r\n    model = tf.estimator.Estimator(\r\n        model_fn=my_dnn_regression_fn,\r\n        model_dir=\r\n        \"./models/temp\",\r\n        params={\r\n            'feature_columns': feature_columns,\r\n            'learning_rate': 0.1,\r\n            'optimizer': tf.train.AdamOptimizer,\r\n            'hidden_units': [20, 20, 20, 20]\r\n        })\r\n    train_spec = tf.estimator.TrainSpec(input_fn=input_train,max_steps=10000)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=input_dev,steps=10000,throttle_secs=60,start_delay_secs=0)\r\n    tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\r\n```\r\nDid I set the parameter properly? Or, is there other solution for this?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I suspect this is a possible answer: \r\nhttps://stackoverflow.com/questions/40146428/show-training-and-validation-accuracy-in-tensorflow-using-same-graph\r\nBut it would be great if we could do this without having to custom-write code.\r\nI think it's particularly informative to see how the _validation set_ is performing compared to the _training set_.\r\nCould this be a simple toggle/view/filter (unsure what to call it) in _TensorBoard_?\r\n\r\nps - I think `stat:awaiting response` label can be ignored -- that information is irrelevant for this issue.", "Is it just a documentation lack label?", "It is really helpful to see _loss_ & _accuracy_ right next to each other. I think it would be a great feature to have as a default setting. And it _really_ is important to see the loss for _training_ and _validation_ together -- to see if they begin to diverge.\r\n\r\nA rough proposal (not styled for TensorBoard, but still):\r\n\r\n![image](https://user-images.githubusercontent.com/17264277/43359820-7acb6e02-9277-11e8-9fac-2b57a8bc4529.png)\r\n", "This seems like a feature request; @dsmilkov is this your territory?", "I didn't work on the charts in TensorBoard, but @jart would be able to help/delegate here.", "Seems like a good feature to have. Unsure if the above @tensorflowbutler message means the issue is going to get auto-closed or it means the issue will now get more attention. Either way -- saying 'seems like a good feature to have' \ud83d\ude09 ", "@jart, gentle ping: could you please advise or delegate?", "I am also looking for this feature, it would be great to have it. ", "+1 to have this feature out-of-the-box", "Evaluation runs on checkpoints. May be the reason you see only one evaluation step is there is only one checkpoint. Could you please play with `tf.estimator.RunConfig(save_checkpoints_steps=SOME_SMALL_VALUE_TO_VERIFY)`", "I think that issue is deeper inside Estimator architecture.\r\nIn my case i see all required validation metrics, but non of them for training phase.\r\n\r\nThis is because EstimatorSpec returned in training stage did not contain eval_metric_ops (look at any estimator _Head). \r\nEstimator's internal methods that use EstimatorSpec in train phase (as i think) don't use at eval_metric_ops too.\r\n\r\nIf we look at [custom estimator guide](https://www.tensorflow.org/guide/custom_estimators#evaluate) accuracy will be shown in TensorBoard only if we use custom model_fn and log it ourselves with tf.summary.scalar('accuracy', accuracy[1])", "Hi @zjy8006 I'm closing this issue since I think checkpointing is the main reason you couldn't see more evaluation points.", "Hi @ispirmustafa , in my experience setting the checkpoints  with `tf.estimator.RunConfig(save_checkpoints_steps=SOME_SMALL_VALUE_TO_VERIFY)` does not work either. I tried this with 1, 10, 1000 and 10000 (which was the total number of my steps), all leading to somewhat same results. Although this makes the number of checkpoints vary, the number of points in eval plot are still 2 at max. (the below image shows my tensorboard plot after setting steps to 1) \r\n![accuracy](https://user-images.githubusercontent.com/20380468/56091835-30b33880-5ec9-11e9-9b8b-c27d9090e658.png)\r\n", "So what is the final solution to this? Has TensorFlow added this feature or fixed this \"bug\"?", "No, there is no easy solution.\r\nEstimatorSpec for training does not include metrics.\r\n\r\nFirst way you can go - estimate and write metrics manually from custom model_fn.\r\n\r\nSecond way i made for myself - estimator wrapper.\r\nHere it is https://github.com/shkarupa-alex/tfmiss/blob/master/tfmiss/estimator/extenders.py (since my package requires to be built with bazel you may copy this particular file)\r\nBased on https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/add_metrics", "I think the correct way is to use hooks or listeners. But this is non-trivial. ", "Can someone please reopen this issue since it was never really resolved?"]}, {"number": 18857, "title": "Error when using TF-Lite visualizer to create the HTML file from a TF-Lite model", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.9\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: 16GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n1. I am trying to use TF-Lite visualization tool to create a graph image on the toco quantized model. However, it dose not work. \r\n2. I am also tried to use visualize.py to create the html file, but it can not create the corresponding JSON file of the model. It seems that the binary file of \"flatc\" cannot be found.\r\n\r\n### Source code / logs\r\n1. using toco command line tools to converting and quantizing TF model to TF-Lite model\r\n2. using bazel to build and run TF-Lite visualizer\r\n> bazel run tensorflow/contrib/lite/tools:visualize -- quant_mobilenet_v1_infer.tflite quant_mobilenet_v1_infer.html\r\n\r\nHowever, I got error info:\r\n> RuntimeError: Invalid filename 'quant_mobilenet_v1_infer.tflite'\r\n> ERROR: Non-zero return code '1' from command: Process exited with status 1\r\n\r\nwhich is caused by CreateHtmlFile function in visualize.py\r\n\r\n", "comments": ["Try: \r\nbazel run tensorflow/contrib/lite/tools:visualize -- `realpath ./quant_mobilenet_v1_infer.tflite` ", "The same problem:\r\n> RuntimeError: Invalid filename 'realpath'\r\nERROR: Non-zero return code '1' from command: Process exited with status 1\r\n", "Oh.. I see. You need back-quotes around the last portion of the command:\r\n\r\nbazel run tensorflow/contrib/lite/tools:visualize -- \\`realpath ./quant_mobilenet_v1_infer.tflite`", "@andrehentz Thanks a lot.  The previous error is solved.  However, another error occurs:\r\n> IOError: [Errno 2] No such file or directory: '/tmp/quant_mobilenet_v1_infer.json'\r\nERROR: Non-zero return code '1' from command: Process exited with status 1\r\n\r\nIt seems that \"flatc\" is not correctly generated?\r\n", "That's possible, but it looks like the tool is looking for it in the wrong place. We will fix that.", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "same problem , is there any update?", "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n**OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS high Sierra\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): 1.7\r\nPython version: 2.7\r\nBazel version (if compiling from source): 0.12.0\r\nGCC/Compiler version (if compiling from source): 4.9\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce:**\r\n\r\n\r\nDescribe the problem\r\nI am trying to use TF-Lite visualization tool to create a graph image on the toco convert model. However, it only build but not run and produce html file.\r\n\r\nAfter download mobileNet with *tflite. \r\n1)\r\ncd all the way to visualize.py \r\n`\"bazel run visualize.py -- ~/mobilenet/mobilenet_v1_1.0_224.tflite ./foo.html \"\r\n`\r\nbazel only builds, but no foo.html produced. \r\n\r\n\r\n2) if \r\n\r\nbazel run tensorflow/contrib/lite/tools:visualize -- ~/mobilenet/mobilenet_v1_1.0_224.tflite ./foo.html\r\n\r\nbazel still builds, but smilar errors like the above reported are seen\r\n\r\nsh: third_party/flatbuffers/flatc: No such file or directory\r\nTraceback (most recent call last):\r\n\r\nIOError: [Errno 2] No such file or directory: '/tmp/mobilenet_v1_1.0_224.json'\r\n", "The realpath trick mentioned above has no help. ", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): 1.8\r\nPython version: 2.7\r\nBazel version (if compiling from source): 0.11.0\r\nGCC/Compiler version (if compiling from source): Apple LLVM version 9.1.0 (clang-902.0.39.2)\r\nCUDA/cuDNN version: \r\nGPU model and memory: \r\nExact command to reproduce:\r\n\r\nI'm having the same problem. Attempting to use the command\r\n~~~~\r\nbazel run tensorflow/contrib/lite/tools:visualize -- `realpath ./graph.tflite` ./tmp/viz.html\r\n~~~~\r\n\r\nThe full error is:\r\n~~~~\r\nsh: third_party/flatbuffers/flatc: No such file or directory\r\nthird_party/flatbuffers/flatc -t --strict-json --defaults-json -o /tmp third_party/tensorflow/contrib/lite/schema/schema.fbs -- /Users/andrewginns/tensorflow-r1.8/tensorflow/contrib/lite/graph.tflite\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_andrewginns/8cde35fd4c3cb2b9d28d654996fb7651/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/contrib/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/contrib/lite/tools/visualize.py\", line 391, in <module>\r\n    main(sys.argv)\r\n  File \"/private/var/tmp/_bazel_andrewginns/8cde35fd4c3cb2b9d28d654996fb7651/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/contrib/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/contrib/lite/tools/visualize.py\", line 387, in main\r\n    CreateHtmlFile(tflite_input, html_output)\r\n  File \"/private/var/tmp/_bazel_andrewginns/8cde35fd4c3cb2b9d28d654996fb7651/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/contrib/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/contrib/lite/tools/visualize.py\", line 307, in CreateHtmlFile\r\n    data = json.load(open(real_output))\r\nIOError: [Errno 2] No such file or directory: '/tmp/graph.json'\r\nERROR: Non-zero return code '1' from command: Process exited with status 1\r\n~~~~", "It seems to be a general bug. Many people see it ", "I have been able to replicate this issue using tensorflow-1.9.0 and Linux. I think the issue is with paths to the schema and also the missing flatc file in thirdparty/flatbuffers/flatc \r\n\r\nThe current workaround that works for me is to install flatbuffers cloning Google/flatbuffers repo using cmake to build. After which, I updated the paths to _BINARY and _SCHEMA at the top of tensorflow/contrib/lite/tools/visualize.py \r\n\r\nExample: \r\n\r\n_BINARY = path/to/flatc/file/in/cloned/flatbuffers/folder \r\n\r\n_SCHEMA = \r\nabsolute/path/to/schema.fbs \r\n\r\nSpecify the output file to a path of your choice for example: \r\nBazel run tensorflow/contrib/lite/tools:visualize -- model.tflite ~/Desktop/model_viz.html \r\n\r\nThis has allowed me to visualize my tflite models. Hope that helps!", "~~@Sri-vatsa could you provide instructions for how you fixed the missing flatc file? I'm a little lost as to what repo you cloned and how you updated the paths.~~\r\n\r\n@Sri-vatsa @HwMohanLiu @samsun639 @tingxingdong \r\n\r\nFigured it out. Requires cmake\r\n1. Clone https://github.com/google/flatbuffers\r\n2. Extract and navigate to the flatbuffers-master folder\r\n3. Run the appropriate cmake command\r\n```\r\ncmake -G \"Unix Makefiles\"\r\ncmake -G \"Visual Studio 10\"\r\ncmake -G \"Xcode\"\r\n```\r\n4. Make\r\n```\r\nmake\r\n```\r\n5. Test that it was successful\r\n```\r\n./flattests\r\n```\r\n6. Navigate to your tensorflow-master folder\r\n7. Edit tensorflow/contrib/lite/tools/visualize.py\r\n8. Change the _BINARY = path/to/flatc/file/in/cloned/flatbuffers/folder\r\n9. Change the _SCHEMA =absolute/path/to/schema.fbs\r\n10. Your TFLite visualise command should now work!\r\n\r\nFor reference my paths looks like:\r\n_SCHEMA = \"home/user/Downloads/tensorflow-master/tensorflow/contrib/lite/schema/schema.fbs\"\r\n_BINARY = \"home/user/Downloads/flatbuffers-master/flatc\"", "@andrewginns Thanks for writing the step by step instructions! I was typing it on my phone so I tried to keep it short.", "@HwMohanLiu Is this still an open issue?", "No, I followed the solution of **andrewginns** and it works now."]}, {"number": 18856, "title": "No module named tensorflow.python.platform", "body": "environment: ubuntu 1604, python 2.7\r\n\r\n```\r\nsudo pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/   https://mirrors.tuna.tsinghua.edu.cn/tensorflow/linux/cpu/tensorflow-1.7.0-cp27-none-linux_x86_64.whl\r\n```\r\nthere is error, but why???\r\n```\r\nubuntu@vm:~/code/tensorflow$ python tensorflow/python/tools/freeze_graph.py --input_graph=/tmp/mobilenet_v1_1.0_224.pb --input_binary=true --output_graph=/tmp/frozen_mobilenet_v1_224.pb --output_node_names=MobilenetV1/Predictions/Reshape_1\r\nTraceback (most recent call last):\r\n  File \"tensorflow/python/tools/freeze_graph.py\", line 45, in <module>\r\n    from tensorflow.core.framework import graph_pb2\r\n  File \"/home/ubuntu/code/tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/ubuntu/code/tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/ubuntu/code/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\nImportError: No module named platform\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "thanks, I found the solution, that I need to run command at the $(source_code_root)/tensorflow/tensorflow folder", "and my command need to modified as:\r\npython -m tensorflow.python.tools.freeze_graph \\\r\n--input_graph=/tmp/mobilenet_v1_1.0_224.pb \\\r\n--input_checkpoint=/tmp/checkpoints/mobilenet_v1_1.0_224.ckpt \\\r\n--input_binary=true \\\r\n--output_graph=/tmp/mobilenet_v1_1.0_224_frozen.pb \\\r\n--output_node_names=MobilenetV1/Predictions/Reshape_1", "Glad you figured it out!"]}, {"number": 18855, "title": "[tflite] Extract NEON check function for aarch64", "body": "For aarch64, we do not have to include `cpu-features.h`.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@wateret  wondering if you still need this PR , if yes can you please resolve conflicts"]}, {"number": 18854, "title": "Fix variance initialisers", "body": "Pull request for resolving issue #18706", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Does it work now?", "CLAs look good, thanks!\n\n<!-- ok -->", "@fchollet Should I undo the changes then? I'm sorry for causing confusion", "Yes, basically:\r\n\r\n- Please only make unit test changes that are related to the changes in your PR.\r\n- Don't follow existing unit tests in this file too closely, because they are not a good example in this case.\r\n\r\nJust don't use variable scopes and you'll be fine. Also feel free to omit unit tests that are orthogonal to your changes, such as `testDuplicatedInitializer` and `testZeroSize`.\r\n\r\nJust `VarianceScalingInitializationTest` with `testNormal` and `testUniform` will do just fine.", "@fchollet Nothing has changed. I just rebased my branch onto the current master branch.", "Sorry for the delay in getting this tested. Could you please rebase to fix the merge conflicts?", "The rebase has been done. Just let me know when you need anything from my part.", "@drpngx  This is a breaking change. It should be mentioned in release notes.", "Makes sense. CC @av8ramit "]}, {"number": 18853, "title": "The child thread can not use the main thread's graph? ", "body": "I load model in the main thread, and passing it to child thread\uff0c but the child thread run error.\r\nlogs\uff1a\r\nhe name 'input:0' refers to a Tensor which does not exist. The operation, 'input', does not exist in the graph.\"\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thanks for your reply.\r\nubuntu16.04\r\ntensorflow 1.7\r\nno GPU\r\nbazel 0.9\r\n\r\nI use facenet model to do face recogonition, when i load model in main thread and do inference is ok\uff0c\r\nbut when i load model in mian thread and  do inference in child thread\uff0cthen is failed.\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there. \r\n\r\nIf you think we misinterpreted a bug report please post again and include the code that causes the error.\r\n", "Thank you all the same."]}, {"number": 18852, "title": "[Feature Request + WIP] Azure blob storage filesystem plugin", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.7.0-3-g024aecf414 1.7.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.12.0-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.1.0 (clang-902.0.39.1)\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nOpening this issue to mark work I've been doing on implementing a file system for azure blob storage to complement gcs and s3. Opening so that if others are interested or want to test can do so. Also haven't created a WIP PR as still heavily testing and cleaning up. \r\n\r\nImitates the GCS implementation closely and uses curl and boringssl libraries that are already included in part of the build.\r\n\r\n### Source code / logs\r\nCurrent work that will lead to a PR: https://github.com/damienpontifex/tensorflow/tree/az-storage", "comments": ["Closing this following after [guidance](https://github.com/tensorflow/tensorflow/pull/21649#issuecomment-448351480) to move this to [tensorflow_io](https://github.com/tensorflow/io)"]}, {"number": 18851, "title": "BUG: fix unmatched dtype between y_train and y_test", "body": "y_train_np is uint8, while y_test_np is int64:\r\n\r\n```python\r\nfrom tensorflow.python.keras.datasets import cifar10\r\n(x_train_np, y_train_np), (x_test_np, y_test_np) = cifar10.load_data()\r\nprint(y_train_np.dtype, y_test_np.dtype)\r\n# uint8 int64\r\n```", "comments": ["Nagging Assignee @ekelsen: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@fchollet negligible change, could you take a look? Thanks.", "Nagging Assignee @ekelsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18850, "title": "Mac OS X Python 3.5 binary runtime warning for Tensorflow 1.5.0+", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.4\r\n- **TensorFlow installed from (source or binary)**: binary (or whatever pip is)\r\n- **TensorFlow version (use command below)**: 1.5.0+\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: 8GB CPU\r\n- **Exact command to reproduce**: `python -c \"import tensorflow\"`\r\n\r\n### Describe the problem\r\n\r\nI seem to have the opposite problem of [https://github.com/tensorflow/tensorflow/issues/14182](https://github.com/tensorflow/tensorflow/issues/14182)\r\n\r\nI'm using `Python 3.5`, but when I try to use a Tensorflow version 1.5 or above with the command\r\n\r\n    python -c \"import tensorflow\"\r\n\r\nI get a `RuntimeWarning` saying:\r\n\r\n    /Users/<user>/.pyenv/versions/3.5.3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: \r\n    compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match \r\n    runtime version 3.5\r\n      return f(*args, **kwds)\r\n\r\nWhen I install using `pip install tensorflow==1.5.0`, it installs from `https://files.pythonhosted.org/packages/b1/57/1d27695a4572d70b8cd365dd358b45a41ece811d675910e175254779885e/tensorflow-1.5.0-cp35-cp35m-macosx_10_11_x86_64.whl`\r\n\r\nInstalling with `pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.5.0-py3-none-any.whl` doesn't make the warning go away either.\r\n\r\n\r\nI get this warning for any Tensorflow version 1.5.0 and above. Incidentally, 1.5.0 is where the runtime binary for Python 3.6 was fixed, so I'm wondering if this is related. I have no issues when installing Tensorflow 1.5.0+ on Ubuntu 16.04 with `pip install tensorflow` with Python 3.5.2 so it seems to be a Mac OS X problem.\r\n\r\n### Source code / logs\r\n\r\nI get the warning whenever I run `import tensorflow`", "comments": ["Nagging Assignee @skye: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@gunan can you comment or reassign? Thanks!", "Do you see just a warning, or do you also face problems when running things?\r\nWe might be just renaming the package we built for python 36 on macos, because we cannot easily create macos environments using docker.", "I didn't face any issues, but I also wasn't running any complicated stuff because it was my introudction to tensorflow and I was just playing around with the basics", "I ran into the same issue. \r\n\r\nI am using python version 3.5.5 and after \"import tensorflow\" I do get the following warning:\r\n\r\n`/anaconda2/envs/tensorflow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\r\n  return f(*args, **kwds)`\r\n\r\nPlease clarification what to do and how to solve? \r\n\r\nNote:\r\nIf I change my python version from 3.5.5 to 3.6.5 the warning message disappears, so is that an issue with python 3.5?", "Nagging Assignee @gunan: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Do you see any errors in practice?\r\nAny unexpected results, or any unexpected failures?\r\nLooks like this is just a warning message.\r\nWe are still working on the new system to build more packages, but it may take some time to land.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "this should be resolved now.", "i still have the issue with python 3.6.5\r\n<img width=\"835\" alt=\"screen shot 2018-09-14 at 10 34 49 am\" src=\"https://user-images.githubusercontent.com/1424974/45565856-dc988680-b809-11e8-9037-a85ab3ef8430.png\">\r\n", "```\r\n=> python -c \"import tensorflow\"\r\n/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n```\r\n\r\nI get the same warning on MacOS 10.13.6. Tried to install Tensorflow with latest Python v.3.7.0, yet that didn't work... \r\n\r\nUpdate: Reverting Tensorflow to an earlier build and using https://github.com/lakshayg/tensorflow-build resolved the warning.", "Tensorflow doesnt exist for python 3.7 yet because of a reserved keyword issue. [20517](https://github.com/tensorflow/tensorflow/issues/20517)"]}, {"number": 18849, "title": "CUPTI events are missing from tf.train.Server", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: r1.7, r1.6, r1.5\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: \r\n- **CUDA/cuDNN version**: 9.1/7.1.3, 9.0/7.0.5\r\n- **GPU model and memory**: K80/V100/GTX TITAN\r\n- **Exact command to reproduce**: See the description\r\n\r\n### Describe the problem\r\nI am collecting runtime tracing using `tf.RunOptions.FULL_TRACE`. When a direct session is used (`master=None`), CUPTI events (`/stream:*` devices) are included in `tf.RunMetadata`. However, a remote (grpc) session does not have any. Is there a particular reason behind it?\r\n\r\nFor example the following script has the CUPTI:\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim.nets as nets\r\n\r\nmodel, _ = nets.resnet_v2.resnet_v2_50(tf.random_uniform([16,299,299,3]))\r\n\r\nwith tf.train.MonitoredTrainingSession() as sess:\r\n     run_metadata = tf.RunMetadata()\r\n     sess.run(model.op, run_metadata=run_metadata, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE))\r\n\r\nprint([d.device for d in run_metadata.step_stats.dev_stats])\r\nassert any([\"stream:all\" in d.device for d in run_metadata.step_stats.dev_stats])\r\n```\r\nThe example output on my device is:\r\n```\r\n['/device:GPU:0/stream:7', '/device:GPU:0/memcpy', '/device:GPU:0/stream:25', '/device:GPU:0/stream:24', '/device:GPU:0/stream:13', '/job:localhost/replica:0/task:0/device:GPU:0', '/device:GPU:0/stream:18', '/device:GPU:0/stream:all', '/device:GPU:0/stream:23', '/device:GPU:0/stream:31', '/device:GPU:0/stream:20', '/device:GPU:0/stream:22', '/device:GPU:0/stream:19', '/device:GPU:0/stream:21']\r\n```\r\n\r\nHowever, the following example does not have any CUPTI events:\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim.nets as nets\r\n\r\nserver = tf.train.Server(tf.train.ClusterSpec({\"worker\":[\"localhost:2222\"]}), \"worker\", 0)\r\nmodel, _ = nets.resnet_v2.resnet_v2_50(tf.random_uniform([16,299,299,3]))\r\n\r\nwith tf.train.MonitoredTrainingSession(master=\"grpc://localhost:2222\") as sess:\r\n     run_metadata = tf.RunMetadata()\r\n     sess.run(model.op, run_metadata=run_metadata, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE))\r\n\r\nprint([d.device for d in run_metadata.step_stats.dev_stats])\r\nassert any([\"stream:all\" in d.device for d in run_metadata.step_stats.dev_stats])\r\n```\r\n\r\nThe following is the output on my device:\r\n```\r\n['/job:worker/replica:0/task:0/device:GPU:0']\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAssertionError\r\n```\r\n\r\nI have tried this with multiple version of TensorFlow (r1.7, r1.6, r1.5) and both `grpc_tensorflow_server` and `tf.train.Server`.\r\n", "comments": ["@mrry @prb12 Could you look at this?", "Hi @xldrx !1.x versions are not supported anymore. You can follow [migration](https://www.tensorflow.org/guide/migrate) documentation to use features from the 1.x version in the 2.8 version. Please create a new issue if it is still replicating in the 2.8 version. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 18848, "title": "Add int8 support to div operations", "body": "I ran into issues when performing division operations using tf.int8 datatypes, getting errors like:\r\n\r\nNo OpKernel was registered to support Op 'FloorDiv' with these attrs.\r\nNo OpKernel was registered to support Op 'TruncateDiv' with these attrs.\r\n\r\nThis PR registers int8 as a datatype for these division ops.", "comments": ["```\r\ntensorflow/core/kernels/cwise_op_div.cc:31:60: error: macro \"REGISTER9\" passed 14 arguments, but takes just 13\r\n           uint16, int8, int16, int64, complex64, complex128);\r\n                                                            ^\r\ntensorflow/core/kernels/cwise_op_div.cc:30:1: error: 'REGISTER9' does not name a type\r\n REGISTER9(BinaryOp, GPU, \"Div\", functor::div, float, Eigen::half, double, uint8,\r\n```", "@pvaneck can you please resolve conflicts ?", "Conflicts have been resolved.", "> Conflicts have been resolved.\r\n\r\nThank you , @drpngx can you please review ", "Sorry for the slow response, but I'm not familiar enough with this area to review it. @drpngx, are you able to take over the review?", "Can one of the admins verify this patch?", "@pvaneck can you please resolve conflicts and address Josh's review comments.", "closing this PR as contrib folder will be depricated in 2.0, thank you.\r\nCC @mihaimaruseac", "@rthadur note this PR affects core, it just has a single file touching contrib. It could simply be reverted. \r\n\r\nHowever, the PR is stalled. @pvaneck are you still interested in adding int8 support or should we close this? So sorry it's taken this long :(.", "Removing the contrib file while solving conflicts or reopening a new PR are both good alternatives.", "I guess we can close until we hear from @pvaneck. Feel free to re-open!\r\n\r\nThanks again for your work, and sorry it's taken so incredibly long."]}, {"number": 18847, "title": "_session->Run() crushes at Eigen::internal::handmade_aligned_free", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, a slightly modified version of label_image\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: built from source using tensorflow/contrib/makefile\r\n- **TensorFlow version (use command below)**: r1.7\r\n- **Python version**:  N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**:  5.4.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nI modified contrib/makefile/Makefile and tf_op_files.txt to use machine generated ops (built from Bazel) for my customized libtensorflow_core.a. I have two graphs in my code, one was created using C++ API for input image pre-processing, and the other one was loaded from pb model file. My C++ project ran fine for the first graph and crashed at _session->Run() for the second graph with segmentation fault.  \r\n\r\nWhen linking my project against libtensorflow_cc.so and libtensorflow_framework.so built from Bazel, my code runs fine with no problem. \r\n\r\n\r\n### Source code / logs\r\n\r\nThe code stopped at the first call of conv2d() at line 422 of conv_ops.cc\r\n\r\n`    launcher_(context, use_cudnn_, cudnn_use_autotune_, input, filter,\r\n              dilation_rows, dilation_cols, stride_rows, stride_cols, padding_,\r\n              output, data_format_);`\r\n\r\nwith the following backtrace of the entire stack. This looks like a double free problem? or Should I compile the Makefile with some flag settings for Eigen? \r\n\r\n> 1  __GI___libc_free                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  malloc.c                      2951 0x7fffef463512 \r\n2  Eigen::internal::handmade_aligned_free                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Memory.h                      98   0x7ffff69fa6f3 \r\n3  Eigen::internal::aligned_free                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Memory.h                      179  0x7ffff69fa6f3 \r\n4  Eigen::TensorEvaluator<Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, std::array<long, 1ul>, std::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 24, 8, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, std::array<long, 1ul>, std::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 24, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, std::array<long, 1ul>, std::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, std::array<long, 1ul>, std::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0>>::~Context TensorContractionThreadPool.h 402  0x7ffff69fa6f3 \r\n5  Eigen::TensorEvaluator<Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::evalProduct<true, true, false, 0>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        TensorContractionThreadPool.h 310  0x7ffff69fa6f3 \r\n6  Eigen::TensorContractionEvaluatorBase<Eigen::TensorEvaluator<Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>>::evalTo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            TensorContraction.h           418  0x7ffff69fbd55 \r\n7  Eigen::TensorContractionEvaluatorBase<Eigen::TensorEvaluator<Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>>::evalSubExprsIfNeeded                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              TensorContraction.h           402  0x7ffff69fbd55 \r\n8  Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 4> const, Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const> const, Eigen::ThreadPoolDevice>::evalSubExprsIfNeeded                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       TensorMorphing.h              129  0x7ffff69fbd55 \r\n9  Eigen::TensorEvaluator<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<long, 4> const, Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const> const> const, Eigen::ThreadPoolDevice>::evalSubExprsIfNeeded                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              TensorAssign.h                129  0x7ffff69fbd55 \r\n10 Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<long, 4> const, Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const> const> const, Eigen::ThreadPoolDevice, true>::run                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                TensorExecutor.h              149  0x7ffff69fbd55 \r\n11 Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorReshapingOp<Eigen::DSizes<long, 4> const, Eigen::TensorContractionOp<std::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const>>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               TensorDevice.h                35   0x7ffff69fbd55 \r\n12 tensorflow::functor::SpatialConvolutionFunc<Eigen::ThreadPoolDevice, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              conv_2d.h                     60   0x7ffff69fbd55 \r\n13 tensorflow::functor::SpatialConvolution<Eigen::ThreadPoolDevice, float>::operator()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               conv_2d.h                     72   0x7ffff69c2641 \r\n14 tensorflow::(anonymous namespace)::LaunchGeneric<Eigen::ThreadPoolDevice, float>::operator()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      conv_ops.cc                   104  0x7ffff69c2641 \r\n15 tensorflow::LaunchConv2DOp<Eigen::ThreadPoolDevice, float>::operator()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            conv_ops.cc                   126  0x7ffff6a2028d \r\n16 tensorflow::Conv2DOp<Eigen::ThreadPoolDevice, float>::Compute                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     conv_ops.cc                   422  0x7ffff6a2028d \r\n17 tensorflow::ThreadPoolDevice::Compute                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             threadpool_device.cc          60   0x7ffff4f70744 \r\n18 tensorflow::(anonymous namespace)::ExecutorState::Process                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         executor.cc                   1658 0x7ffff4f1e212 \r\n19 std::_Mem_fn_base<void (tensorflow::(anonymous namespace)::ExecutorState:: *)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), true>::operator()<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode&, long long int&, void>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    functional                    600  0x7ffff4f0bd05 \r\n20 std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState:: *)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState *, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>::__call<void, 0ul, 1ul, 2ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  functional                    1074 0x7ffff4f0bd05 \r\n21 std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState:: *)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState *, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>::operator()<, void>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           functional                    1133 0x7ffff4f0bd05 \r\n22 std::_Function_handler<void(), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState:: *)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState *, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>>::_M_invoke(const std::_Any_data &)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            functional                    1871 0x7ffff4f0bd05 \r\n23 std::function<void ()>::operator()() const                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        functional                    2267 0x7ffff515db81 \r\n24 tensorflow::thread::EigenEnvironment::ExecuteTask                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 threadpool.cc                 83   0x7ffff515db81 \r\n25 Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NonBlockingThreadPool.h       232  0x7ffff515db81 \r\n26 std::function<void ()>::operator()() const                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        functional                    2267 0x7ffff515b92f \r\n27 tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}::operator()() const                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      threadpool.cc                 56   0x7ffff515b92f \r\n28 std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       functional                    1871 0x7ffff515b92f \r\n29 ??                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   0x7fffefd80c80 \r\n30 start_thread                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      pthread_create.c              333  0x7ffff00516ba \r\n31 clone                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             clone.S                       109  0x7fffef4e641d \r\n", "comments": ["tried on both r1.6 and r1.8 branches. crashed at the same location", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 75 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It seems you may not be initializing eigen properly? I don't know what modifications you made, and you haven't included test cases, so it is difficult to reproduce or offer suggestions. Please provide more details.", "issue solved some time ago and sorry forgot to update it here. Basically the problem is that I used different -march options for compiling tensorflow and my own libraries. Tensorflow by defaults uses -march=native, while I uses -march=corei7 to disable avx for my library.  "]}, {"number": 18846, "title": "Enable int8 support for FloorDiv", "body": "int8 is enabled for FloorDiv in math_ops.cc though the kernel was not registered.\r\n\r\nThis fix register the int8 kernel for FloorDiv, and enables the test case for it.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18845, "title": "[Cherrypick] Fix critical metrics computation bug with Model in Eager mode.", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to determine that you authored the commits in this PR.  Maybe you used a different email address in the git commits than was used to sign the CLA?  If someone else authored these commits, then please add them to this pull request and have them confirm that they're okay with them being contributed to Google.  If there are co-authors, make sure they're formatted properly.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- unknown_author -->", "It seems I committed with my default GitHub email and not my Google email, hence the CLA warning. That's fine, please by-pass it. Thanks!", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to determine that you authored the commits in this PR.  Maybe you used a different email address in the git commits than was used to sign the CLA?  If someone else authored these commits, then please add them to this pull request and have them confirm that they're okay with them being contributed to Google.  If there are co-authors, make sure they're formatted properly.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- unknown_author -->", "Thanks Yifei!\n\nOn Wed, Apr 25, 2018, 10:56 Yifei Feng <notifications@github.com> wrote:\n\n> Merged #18845 <https://github.com/tensorflow/tensorflow/pull/18845>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/18845#event-1594715513>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AArWb_-oTLb_9O9bcg23IVzFRu1kTZdKks5tsLjFgaJpZM4TimBL>\n> .\n>\n"]}, {"number": 18844, "title": "Update tensorboard dep to 1.8.x", "body": "TensorBoard 1.8.0 has been released to PyPI: https://pypi.org/project/tensorboard/1.8.0/", "comments": []}, {"number": 18843, "title": "Fixing the mock import error for devel docker.", "body": "", "comments": []}, {"number": 18842, "title": "Fix a bug where string::substr is used with wrong position.", "body": "@samikama ", "comments": ["@aaroey There was already a PR for this \r\n", "@samikama which one?", "#18197\r\n"]}, {"number": 18841, "title": "tensorflow/compiler/xla/statusor.h not included in installation, but required for compiling custom TensorFlow operators", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 27\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: v1.8.0-rc1-909-g968addadfd 1.8.0-rc1\r\n- **Python version**: 3.6m\r\n- **Bazel version (if compiling from source)**: 0.12.0\r\n- **GCC/Compiler version (if compiling from source)**: 6.4\r\n- **CUDA/cuDNN version**: 9.1/7.1\r\n- **GPU model and memory**: GTX 1080 8GB\r\n- **Exact command to reproduce**:\r\n```\r\ninclude Makefile.config\r\nTF_INC = `$(PYTHON) -W ignore -c 'import tensorflow as tf; print(tf.sysconfig.get_include())'`\r\nTF_CFLAGS = `$(PYTHON) -W ignore -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))'`\r\nTF_LFLAGS = `$(PYTHON) -W ignore -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))'`\r\nCUDA_INC = $(CUDA_HOME)/../\r\nGPUCC     = nvcc -ccbin=$(CXX)\r\nCFLAGS    = -std=c++11 $(TF_CFLAGS) -I. -I$(CUDA_INC) -I$(TF_INC)\r\nGPUCFLAGS = -c\r\nLFLAGS    = $(ODFLAGS) -pthread -fopenmp -shared -fPIC $(TF_LFLAGS)\r\nGPULFLAGS = -x cu -Xcompiler \"$(CFLAGS) $(LFLAGS)\" --expt-relaxed-constexpr\r\nGPUDEF    = -DGOOGLE_CUDA=1\r\nCGPUFLAGS = -lcuda\r\nSRC       = cart_hex_interpol.cc\r\nGPUSRC    = cart_hex_interpol.cu.cc\r\nSRC_O\t  = cart_hex_interpol.o\r\nGPUSRC_O  = cart_hex_interpol.cu.o\r\nLIB       = cart_hex_interpol.so\r\nall: gpu\r\ndefault: gpu\r\ncpu:\r\n\t$(CXX) $(CFLAGS) $(SRC) $(LFLAGS) -o $(LIB)\r\ngpu:\r\n\t$(GPUCC) $(GPUDEF) $(CFLAGS) $(GPUCFLAGS) $(GPUSRC) $(GPULFLAGS) -o $(GPUSRC_O)\r\n\t$(CXX) $(GPUDEF) $(CFLAGS) $(SRC) $(GPUSRC_O) $(LFLAGS) $(CGPUFLAGS)  -o $(LIB)\r\nclean:\r\n\trm -f $(SRC_O) $(GPUSRC_O) $(LIB)\r\n```\r\n\r\n### Describe the problem\r\nWhen compiling a custom TensorFlow operator using a Makefile and including CUDA/GPU code, the compilation fails due to missing `fatal error: tensorflow/compiler/xla/statusor.h: No such file or directory`. The files are missing in `/usr/local/lib/python3.6/site-packages/tensorflow/` because the headers do not get installed during installation of the wheel package for TensorFlow.\r\nA temporary fix is to copy the headers from source:\r\n```\r\nsudo mkdir /usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/compiler/\r\nsudo mkdir /usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/compiler/xla\r\nsudo cp tensorflow/compiler/xla/*.h /usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/compiler/xla/\r\n```\r\nThis issue was not present in TensorFlow 1.5, but must have been introduced since then. Compiling with or without XLA makes no difference.\r\n\r\n### Source code / logs\r\n```\r\nmake gpu -j2\r\nnvcc -ccbin=/usr/local/gcc-6.4/bin/g++-6.4 -DGOOGLE_CUDA=1 -std=c++11 `python3 -W ignore -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))'` -I. -I/usr/local/cuda/../ -I`python3 -W ignore -c 'import tensorflow as tf; print(tf.sysconfig.get_include())'` -c cart_hex_interpol.cu.cc -x cu -Xcompiler \"-std=c++11 `python3 -W ignore -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))'` -I. -I/usr/local/cuda/../ -I`python3 -W ignore -c 'import tensorflow as tf; print(tf.sysconfig.get_include())'` -O3 -pthread -fopenmp -shared -fPIC `python3 -W ignore -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))'`\" --expt-relaxed-constexpr -o cart_hex_interpol.cu.o\r\nIn file included from /usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/dso_loader.h:26:0,\r\n                 from /usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/platform/stream_executor.h:26,\r\n                 from /usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/util/cuda_launch_config.h:27,\r\n                 from /usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/util/cuda_kernel_helper.h:22,\r\n                 from cart_hex_interpol.cu.cc:5:\r\n/usr/local/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:21:46: fatal error: tensorflow/compiler/xla/statusor.h: No such file or directory\r\n #include \"tensorflow/compiler/xla/statusor.h\"\r\n\r\n```\r\n", "comments": ["Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "same problem! tf 1.8", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@reedwm, @martinwicke, looks like this issue is breaking custom operators on TensorFlow-GPU.  TensorFlow 1.8.0 did not have this issue.  Can XLA compiler headers be bundled into the wheel file?", "@case540 \r\n\r\nDid we remove some headers form the pip? Can we add them back in RC1?", "@martinwicke I think pre-TensorFlow 1.8, these XLA headers were not referenced by code outside of XLA, but now they are (even when not compiling with XLA). I think I saw some comments in the headers speaking about including the header to reduce code duplication. That must have introduced this bug.", "@case540, @martinwicke, any luck with fixing this issue?  Probably the easiest fix is just to include xla headers in the wheel?", "We are working on moving the tensorflow/compiler/xla/statusor.h header in core TF. This way, we won't have to include XLA headers in the whl and it should fix the build issue you are seeing.\r\n\r\nThis should make it into the 1.9 official release.", "Thanks, @case540!\r\n\r\nNot sure if it's related, but Horovod build fails with latest `tf-nightly` package due to missing Eigen headers:\r\n\r\n```\r\n  cc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++\r\n  In file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/numeric_types.h:20:0,\r\n                   from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:23,\r\n                   from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:23,\r\n                   from horovod/tensorflow/mpi_ops.cc:23:\r\n  /usr/local/lib/python2.7/dist-packages/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:10: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory\r\n   #include \"unsupported/Eigen/CXX11/Tensor\"\r\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  compilation terminated.\r\n```\r\n\r\nToday:\r\n```\r\n./tf_nightly-1.10.0.dev20180621.data/purelib/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/Tensor\r\n./tf_nightly-1.10.0.dev20180621.data/purelib/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/TensorSymmetry\r\n./tf_nightly-1.10.0.dev20180621.data/purelib/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/Tensor\r\n```\r\n\r\nYesterday:\r\n```\r\n./tf_nightly-1.10.0.dev20180620.data/purelib/tensorflow/include/unsupported/Eigen/CXX11/TensorSymmetry\r\n./tf_nightly-1.10.0.dev20180620.data/purelib/tensorflow/include/unsupported/Eigen/CXX11/Tensor\r\n./tf_nightly-1.10.0.dev20180620.data/purelib/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/Tensor\r\n./tf_nightly-1.10.0.dev20180620.data/purelib/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/TensorSymmetry\r\n./tf_nightly-1.10.0.dev20180620.data/purelib/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/Tensor\r\n./tf_nightly-1.10.0.dev20180620.data/purelib/external/eigen_archive/unsupported/Eigen/CXX11/TensorSymmetry\r\n./tf_nightly-1.10.0.dev20180620.data/purelib/external/eigen_archive/unsupported/Eigen/CXX11/Tensor\r\n```", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I believe this has been fixed.  Thanks, guys!", "Thanks. The Horovod failure is fixed? We did move a bunch of includes around, and we added StatusOr to non-XLA includes, so it should be fixed. It would be nice to be sure.", "@martinwicke, yes - I no longer see this issue with 1.9.0-rc2.  Thanks!", "thanks, if 1.9 release i'll upgrade to it @martinwicke "]}, {"number": 18840, "title": "Fix typos in index.md", "body": "Fixed some typos including small grammar fixes and double spaces.", "comments": ["The fail is in regard to //tensorflow/tools/ci_build/builds:gen_makefile_out which is unrelated to this documentation fix."]}, {"number": 18838, "title": "Add DeviceSet to Cluster", "body": "Add DeviceSet to Cluster so we can access memory allocators when running grappler optimizations.\r\nThis is required by the TensorRT integration with TF.", "comments": ["@samikama "]}, {"number": 18837, "title": "Update README.md", "body": "", "comments": []}, {"number": 18836, "title": "Update README.md", "body": "", "comments": []}, {"number": 18835, "title": "Issue with building the Hexagon HVX DSP toolchain with TF", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8r\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.4\r\n- **CUDA/cuDNN version**: 8\r\n- **GPU model and memory**: GTX 1060\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\n@satok16 @tensorflower-gardener , Could you please clarify the different version-combination of tools to use in an HVX-TF toolchain. It can save many people's effort to find and match these together. You have already mentioned in [HVX_TF](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx), that for `Qualcomm SDK 3.0`, the compatible nnlib version is [Aug-2017-commit](https://source.codeaurora.org/quic/hexagon_nn/nnlib/commit/?id=721b2d58f0f4e2d5b182f41e6b7c4db5356bf0fb). However, some features are missing there and we need to use the latest version with `SDK 3.3.3`, however, these libraries (`nnlib`, `libcontroller`, `TF-(sub)makefiles`) don't match, thus it has become a messy process to figure out which is related to what version. \r\n\r\n**1)** A sample error compiling libcontroller after compiling nnlib with latest SDK (3.3.3):\r\n\r\n```\r\nsrc_impl/hexagon_controller.c: In function 'hexagon_controller_AppendNode':\r\nsrc_impl/hexagon_controller.c:484:70: error: 'hexagon_nn_output' has no member named 'max_size'\r\n     pos += snprintf(&output_param_buf[pos], 500, \"(%d), \", outputs[i].max_size);\r\n                                                                      ^\r\nmake[1]: *** [android_Release/hexagon_controller.o] Error 1\r\nmake[1]: Leaving directory `~/Qualcomm/Hexagon_SDK/3.3.3/examples/common/hexagon_controller'\r\nmake: *** [tree] Error 2\r\n\r\n```\r\n**2)** If I use your suggested commit and SDK 3.0, this is the output I am having:\r\n\r\n```\r\nnative : hexagon_graph_execution_test.cc:129 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\r\nnative : hexagon_graph_execution_test.cc:135 header size = 54\r\nnative : hexagon_graph_execution_test.cc:137 image size = 40\r\nnative : hexagon_graph_execution_test.cc:139 width = 299\r\nnative : hexagon_graph_execution_test.cc:141 height = -299\r\nnative : hexagon_graph_execution_test.cc:286 Ioading image finished.\r\nnative : hexagon_graph_execution_test.cc:185 Loading image finished.\r\nnative : hexagon_graph_execution_test.cc:189 Copy data to tensor.\r\nnative : hexagon_graph_execution_test.cc:307 Run graph\r\nInit hexagon with max attributes (Controller version = 101)\r\nnative : hexagon_control_wrapper.cc:104 Add input: Mul, 0\r\nnative : hexagon_control_wrapper.cc:127 Allocate inout buffer\r\nnative : hexagon_control_wrapper.cc:304 Setup graph completed\r\nPrepare failed! returned 0xffffffff\r\n\r\nNN Id = -755923072\r\nExecute graph!\r\nExecution failed!\r\nexecute got err: -1\r\n\r\nNN Id = -755923072\r\nExecution failed\r\nNN Id = -755923072\r\nFailed to read data.\r\nnative : hexagon_graph_execution_test.cc:313 Output byte size = 4032\r\nnative : hexagon_graph_execution_test.cc:314 Output shape = [1,1008]\r\nnative : graph_transfer_utils.cc:47 === Dump ranking ===\r\nnative : graph_transfer_utils.cc:50 0: 1000, dumbbell, 0\r\nnative : graph_transfer_utils.cc:50 1: 999, carbonara, 0\r\nnative : graph_transfer_utils.cc:50 2: 998, stole, 0\r\nnative : graph_transfer_utils.cc:50 3: 997, rubber eraser, 0\r\nnative : graph_transfer_utils.cc:50 4: 996, coffee mug, 0\r\nnative : graph_transfer_utils.cc:50 5: 995, flagpole, 0\r\nnative : graph_transfer_utils.cc:50 6: 994, parallel bars, 0\r\nnative : graph_transfer_utils.cc:50 7: 993, cheeseburger, 0\r\nnative : graph_transfer_utils.cc:50 8: 992, bubble, 0\r\nnative : graph_transfer_utils.cc:50 9: 991, beaker, 0\r\nFinalize hexagon\r\n[       OK ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime (5848 ms)\r\n[----------] 1 test from GraphTransferer (5848 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test case ran. (5849 ms total)\r\n[  PASSED  ] 1 test.\r\n\r\n  YOU HAVE 5 DISABLED TESTS\r\n```\r\n\r\n**3)**  Also in the [toturial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx), you did not mention anything about downloading/using an inception-v3 frozen-quantized model. If one follows the building each library from the source, where is the part related to use the [`tensorflow_inception_v3_stripped_optimized_quantized.pb`] ? Since, when I test with my custom quantized_frozen inception-v3 model, (renaming it to be the same as the original 2016 file (`tensorflow_inception_v3_stripped_optimized_quantized.pb`), I have the error output as:\r\n\r\n```\r\n...\r\nnative : hexagon_graph_execution_test.cc:533 Ioading image finished.\r\nt1(loading image time)=0.026770\r\nnative : hexagon_graph_execution_test.cc:546 Build fused graph\r\nnative : remote_fused_graph_execute_utils.cc:259 Error during inference: Not found: FeedInputs: unable to find feed output Mul\r\nnative : graph_transfer_utils.cc:110 Check failed: status.ok()\r\nAborted\r\n\r\n```\r\nThanks.\r\n\r\n\r\n\r\n", "comments": ["Tagging @satok16 for this one as well.", "Nagging Assignee @karmel: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@satok16 , any comment on this?", "@amirjamez\r\nWhich kind of hardware are you using?  Did you follow the troubleshoot in the tutorial?", "@satok16 it is an open Q 820. Yes, I did. apparently, there is a mismatch between the lib-controller versions, nnlib, and the TF-toolchain. Libcontroller used in your toolchain (the one that downloaded the prebuilt library) is v90 and it belongs to a very old version of TF (maybe v1.1 or v1.2), thus using new TFs to build the library will result in failure. However, nnlib needs to be newer to support more ops and these initiates the conflicts. ", "\"Prepare failed! returned 0xffffffff\" means that it couldn't create NN in HVX.  So, I assume there may be some incompatibility issues on your hardware.  You may want to build nnlib by yourself for your hardware and that may work.  This feature is pretty much frozen mode, so you may want to wait for tflite for better hardware supports.", "@satok16 thanks. I have built nnlib with `SDK 3.3.3`, but the test app provided in the test folder does not recognize the image when I pass. [Full info here.](https://stackoverflow.com/questions/50266601/running-the-nnlib-test-app-on-the-hexagon-hvx-dsp). Have you overcome this issue? ", "No, we don't use SDK 3.3 here as this is frozen mode.", "@satok16, but what if someone wanted to use his modified `inception_v3` (newer than the one used in the toolchain as this was for December 2016) as input, it is needed to have the latest nnlib to support the ops and thus, one needs a newer version of SDK and lib controller to build those. So the whole chain will crumble again. ", "I see, if you want to get newer SDK version's support, I'd recommend you to wait for tflite's supports on nnlib as efforts are going there.", "OK. @satok16, could you at least help me out to pass the image mismatch issue that I am getting by executing the nnlib test app (graph_app). This happens both with `SDK 3.0` and `SDK 3.3.3`. Looking at the code, no matter what parameters I pass, I am stuck with this issue:\r\n\r\n`image size 25763 does not match element size 1, depth 3, width 299, height 299`\r\n\r\nif I manually pass this segment (commenting out the check), I will have either small data, or, data overflow and lose adb connection everytime I execute it:\r\n\r\n```\r\nimage size 25763 does not match element size 1, depth 3, width 92, height 92, elements 25763, area 8587\r\n filesize=25763 elementsize=1 height=92 width=92 depth=3\r\nRun!\r\nexecute got err: -1\r\nhexagon/ops/src/op_output.c:59:output 0 too small\r\nRank,Softmax,index,string\r\n0,0.000000,1023,OVERFLOW!\r\n1,0.000000,1,kit fox\r\n2,0.000000,2,English setter\r\n3,0.000000,3,Siberian husky\r\n4,0.000000,4,Australian terrier\r\n```\r\n\r\nWhat is the correct `weight`, `width`, `depth` and `elements` parameters to be used with `keyboard_299x299.jpg`. Thanks.", "Did you check the sample image which is used in build_and_run_inception_hexagon.sh?", "@satok16, yes I did. Same issue [here](https://stackoverflow.com/questions/43995584/shell-gets-stuck-when-standalone-graph-app-in-hexagon-nnlib-is-ran) and [here](https://stackoverflow.com/questions/50266601/running-the-nnlib-test-app-on-the-hexagon-hvx-dsp). it is `img_299x299.bmp` and has 269156 bytes. Given the conditions in nnlib i need to have:\r\n\r\n```\r\ntest/graph_app.c\r\n...\r\n326         elements = filesize / elementsize;\r\n327         area = elements / depth;\r\n328         if (height * width == 0) {\r\n329                 height = width = sqrt(area);\r\n330                 assumed->height = height;\r\n331                 assumed->width = width;\r\n332         }\r\n333         if  ((filesize % elementsize != 0)){\r\n334                  ||(elements % depth != 0)\r\n335                  (height * width != area) {\r\n336                 printf(\"image size %d does not match \"\r\n...\r\n\r\n```\r\nthus, assuming a square box (width == height) and depth = 3 I need to have an image of size: 299*299*3 = 268203 which currently it is not (it is 269156)", "\"bitmap\" has a header and you can't feed the bitmap binary to the inception v3 model as is.  You should load the image to a float array as hexagon_graph_execution_test.cc is doing.", "@satok16 thanks. I see how you did it at the [hexagon_test.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/hexagon/hexagon_graph_execution_test.cc). So how about `.jpg` and `.dat`?  There is a keyboard_299x299.jpg in the test folder, doesn't it have to work out-of-the-box?\r\n\r\nhmm, is that the reason there is a python script at `scripts/imagedump.py`. Does it get a .bmp and outputs just the .dat for that matter? Actually, I just used the script to convert the `jpg` to `.dat` and it seems that I passed the image match issue. Now, I am stuck at the error: `execute got err: -1`here:\r\n\r\n```\r\nUsing </data/keyboard_299x299.dat>\r\nfilesize=268203 elementsize=1 height=299 width=299 depth=3\r\nRun!\r\nexecute got err: -1\r\nhexagon/ops/src/op_output.c:59:output 0 too small\r\noutput size=4096\r\nRank,Softmax,index,string\r\n0,0.000000,1023,OVERFLOW!\r\n1,0.000000,1,kit fox\r\n2,0.000000,2,English setter\r\n3,0.000000,3,Siberian husky\r\n4,0.000000,4,Australian terrier\r\nrun failed: -1\r\n```", "You can just convert the jpg to bitmap or directly to a float array in your program and feed it to the model.", "Thanks, @satok16. Could you provide insights on why the bellow error occurs while executing the graph standalone app on HVX? \r\n\r\n```\r\nUsing </data/keyboard_299x299.dat>\r\nfilesize=268203 elementsize=1 height=299 width=299 depth=3\r\nRun!\r\nexecute got err: -1\r\nhexagon/ops/src/op_output.c:59:output 0 too small\r\noutput size=4096\r\nRank,Softmax,index,string\r\n0,0.000000,1023,OVERFLOW!\r\n1,0.000000,1,kit fox\r\n2,0.000000,2,English setter\r\n3,0.000000,3,Siberian husky\r\n4,0.000000,4,Australian terrier\r\nrun failed: -1\r\n```\r\nThis interestingly works with `SDK 3.0` and `nnlib august 2017`, but on both versions we have execution error -1. ", "Nagging Assignee @karmel: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @karmel: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@satok16 Could you let me know how to print all the append nodes in [Hexagon_control_wrapper.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/hexagon/hexagon_control_wrapper.cc) given inception-v3? I guess this way I may be able to debug the issue. Thanks", "Nagging Assignee @karmel: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @karmel: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@satok16 -- any update here?", "@satok16 I managed to build the libraries and the hexagon_excution_graph adding `-E` to the [build toturial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx#build-tensorflow-linking-hexagon-library). It takes around 10 minutes to compile (after we already built lib_controller and lib_hexagon_skel.so) and I am searching to reduce its compilation time. \r\n\r\nAlso, my other question was [unanswered here](https://github.com/tensorflow/tensorflow/issues/20206). I appreciate if you let me know how did you freeze and optimize the `inception-v3` model that doesn't have `tf.Squeeze()` inside. As you know, this method is not yet implemented on HVX. When I freeze and optimize for inference, these layers are still there, I guess some part of the graph should be stripped/removed but using the graph_transformation I was not able to reproduce the model. Thanks.", "Nagging Assignee @karmel: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Mobile efforts have been on TF Lite (https://www.tensorflow.org/mobile/tflite/). If you have any other concerns, please create another issue for easier tracking. Thanks!"]}, {"number": 18834, "title": "feature request :plz add GPU support for float16 in tf.nn.lrn", "body": "Have I written custom code yes\r\nOS Platform and Distribution ubuntu16.04\r\nTensorFlow installed from source\r\nTensorFlow version:1.7\r\nBazel version 0.11.1\r\nCUDA/cuDNN version 9\r\nGPU model and memory TITAN Xp\r\nExact command to reproduce:N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 31 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 18833, "title": "Fix typo in CMakeLists.txt", "body": "", "comments": []}, {"number": 18832, "title": "Provide tf-nightly in PyPI for up-to-date python versions", "body": "Hi, is there any reason that there are no up-to-date `tf-nightly` - Packages on PyPI?\r\nhttps://pypi.org/simple/tf-nightly/\r\n\r\nThe latest `tf-nightly` version for Python 3.6 is `1.8.0.dev20180331`. \r\nUsing Python 3.5 I get `1.9.0.dev20180423`.\r\nThis is annoying for users on Ubuntu 18.04, as there is only python 3.6\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9.0 nightly\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\n", "comments": ["Nagging Assignee @case540: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @case540: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @case540: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @case540: It has been 61 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @case540: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @case540: It has been 91 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @case540: It has been 106 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18831, "title": "Adding a ValidationHook", "body": "This can replace `ValidationMonitor` (which is now deprecated). It doesn't have the fancy stuff `ValidationMonitor` has like early-stopping and whatnot but it's a start.", "comments": ["Could you add some tests for this new ValidationHook. Thanks!", "My apologies for the late reply. I have been busy with work lately. Unfortunately, I don't know much about creating tests for Hooks. Can someone else take it?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}]