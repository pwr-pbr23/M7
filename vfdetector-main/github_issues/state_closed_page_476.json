[{"number": 39522, "title": "tf.math.l2_normalize support for complex dtypes", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n[tf.math.l2_normalize](https://www.tensorflow.org/api_docs/python/tf/math/l2_normalize) currently does not appear to support complex datatypes:\r\n```\r\ntf.math.l2_normalize(1j)\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-22-148da328f68d> in <module>\r\n----> 1 tf.math.l2_normalize(1j)\r\n\r\n~/anaconda3/envs/*/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py in l2_normalize_v2(x, axis, epsilon, name)\r\n    641     x = ops.convert_to_tensor(x, name=\"x\")\r\n    642     square_sum = math_ops.reduce_sum(math_ops.square(x), axis, keepdims=True)\r\n--> 643     x_inv_norm = math_ops.rsqrt(math_ops.maximum(square_sum, epsilon))\r\n    644     return math_ops.multiply(x, x_inv_norm, name=name)\r\n    645 \r\n\r\n~/anaconda3/envs/*/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py in maximum(x, y, name)\r\n   5770         raise\r\n   5771     except _core._NotOkStatusException as e:\r\n-> 5772       _ops.raise_from_not_ok_status(e, name)\r\n   5773   # Add nodes to the TensorFlow graph.\r\n   5774   try:\r\n\r\n~/anaconda3/envs/*/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6604   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6605   # pylint: disable=protected-access\r\n-> 6606   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6607   # pylint: enable=protected-access\r\n   6608 \r\n\r\n~/anaconda3/envs/*/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: Could not find valid device for node.\r\nNode:{{node Maximum}}\r\nAll kernels registered for op Maximum :\r\n  device='XLA_GPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]\r\n  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n [Op:Maximum]\r\n```\r\n\r\nThis is surprising / unexpected given that the basic computational blocks, like squaring, summing, square root, and dividing, all support complex datatypes. The issue appears to be that `math_ops.maximum` requires a real tensor as `epsilon` is assumed to be a real-valued lower-bound on `square_sum`. I'm not sure of the most natural solution at the moment.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone trying to do something like normalize a complex tensor. As a use case, consider trying to use a `CosineSimilarity` loss on a network with a complex output.\r\n\r\n**Any Other info.**\r\n\r\nN/A", "comments": ["Added a PR #39529 to add complex64/complex128 support for `tf.math.l2_normalize`"]}, {"number": 39521, "title": "Undefined symbol in debug mode", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.2.0-rc3\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: pip \r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): gcc 7.5.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nTensorflow was built inside the 'tensorflow/tensorflow:devel-py3' docker image with digest 0285d09f16ff\r\n\r\n**Describe the problem**\r\n\r\nAfter building tensorflow w/ debug info, tensorflow fails when imported with the following error: \r\n\r\n``` python\r\nImportError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: \r\nundefined symbol: _ZN10tensorflow4data12experimental14SnapshotReader34kSnappyReaderOutputBufferSizeBytesE \r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. I cherry picked https://github.com/tensorflow/tensorflow/pull/39239/commits/cd8b64f7fbdc2bc8700b71dadb4c51744a752095 to fix https://github.com/tensorflow/tensorflow/issues/37498\r\n2. bazel build --config=dbg --strip=never //tensorflow/tools/pip_package:build_pip_package\r\n3. ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n4. cd /tmp/tensorflow_pkg\r\n4. pip3 install tensorflow-2.2.0rc3-cp36-cp36m-linux_x86_64.whl\r\n5. python3 -c 'import tensorflow'\r\n\r\nIf there is anything else worth providing (readelf output?) please let me know", "comments": ["@conjam \r\nCan you please refer to [this comment](https://github.com/tensorflow/tensorflow/issues/12326#issuecomment-323346397) with similat error message but different version, and let us know if it helps.", "> @conjam\r\n> Can you please refer to [this comment](https://github.com/tensorflow/tensorflow/issues/12326#issuecomment-323346397) with similat error message but different version, and let us know if it helps.\r\n\r\nI tried what was recommended in this comment, but to no avail -- explicitly, I tried reinstalling protobuf version 3.3.0 but 3.8.0 is the min required for tf v2.2.0, so all I did was was re-install protobuf version 3.11.0.\r\n\r\nFWIW, I checked out v2.2.0 and v2.1.0; v2.2.0 has the exact same issue as mentioned above, but tf v2.1.0 does not have this issue. \r\n\r\nIs there a docker image that is preferred for building tf v2.2.0 with py3 support (GPU support not required)? ", "Changed title since you mention issue happens at head of r2.2 branch", "@conjam\r\nIs this still an issue", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Yes, still reproducible. Please let me know If more info from me is needed.\r\nIssue looks similar: debug build, undefined symbol on 2.2 and working on 2.1", "V2.4.0 (Master, ToT) Same issue:\r\n/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow4data12experimental19SnapshotDatasetV2Op18kFileFormatVersionE\r\n", "You are seeing different undefined symbols though. Can you do a `bazel clean --expunge` and compile again? Also, if you're building from master, can you also add the output of `git log -n5 --pretty=oneline`?", "> Can you do a `bazel clean --expunge` and compile again?\r\n\r\nPerformed, _pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow4data12experimental19SnapshotDatasetV2Op18kFileFormatVersionE\r\n\r\n> Also, if you're building from master, can you also add the output of `git log -n5 --pretty=oneline`\r\n\r\nba5dddcf708e1762b924e4cd7ef07c54b34708f5 (HEAD -> master, origin/master, origin/HEAD) compat: Update forward compatibility horizon to 2020-07-06\r\n077bc8282c3c0fffe710516ea72b8faa8b5f9ab7 Update GraphDef version to 454.\r\n23873d4d6518a6f169600a076001dc2d7a661dd0 Fix a crash on BenchmarkTfLiteModel with delegate\r\n108aa90714c8369a2392fba43fa62c2d0cec523a Remove the constraint the QConst couldn't be shared\r\n1da8b52504ce5acb531a23a5db0240e72a8d80de Integrate LLVM at https://github.com/llvm/llvm-project/commit/edba2864a7a8\r\n\r\n", "What is the command you are using to build?", "> \r\n> \r\n> What is the command you are using to build?\r\n\r\nJAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64 bazel --host_jvm_args '-Djavax.net.ssl.trustStore=/etc/ssl/certs/java/cacerts' build --host_javabase '@local_jdk//:jdk' --config=dbg tensorflow/tools/pip_package:build_pip_package\r\n\r\nAlso I have an edit in `third_party/aws/aws-checksums.bazel` to make workaround for debug build:\r\n`defines = [                                                                                                                                                                               \r\n        \"DEBUG_BUILD\"                                                                                                                                                                         \r\n    ]\r\n`", "Can you run the following please?\r\n\r\n```\r\nbazel clean --expunge  # probably also adding the JDK args in front\r\nJAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64 bazel --host_jvm_args '-Djavax.net.ssl.trustStore=/etc/ssl/certs/java/cacerts' build --host_javabase '@local_jdk//:jdk' --config=dbg tensorflow/tools/pip_package:build_pip_package &> log.txt\r\n```\r\n\r\nand then *attach* `log.txt`?", "> \r\n> \r\n> Can you run the following please?\r\n> \r\n> ```\r\n> bazel clean --expunge  # probably also adding the JDK args in front\r\n> JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64 bazel --host_jvm_args '-Djavax.net.ssl.trustStore=/etc/ssl/certs/java/cacerts' build --host_javabase '@local_jdk//:jdk' --config=dbg tensorflow/tools/pip_package:build_pip_package &> log.txt\r\n> ```\r\n> \r\n> and then _attach_ `log.txt`?\r\n\r\nRebuilt, reinstalled:\r\n`_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow4data12experimental19SnapshotDatasetV2Op18kFileFormatVersionE`\r\n\r\nLog zipped, original file 80mbytes\r\n[log.zip](https://github.com/tensorflow/tensorflow/files/4892257/log.zip)\r\n\r\nBtw does wheel assembly log looks fine?:\r\n```\r\nwarning: no files found matching 'README'\r\nwarning: no files found matching '*.pyd' under directory '*'\r\nwarning: no files found matching '*.pyi' under directory '*'\r\nwarning: no files found matching '*.pd' under directory '*'\r\nwarning: no files found matching '*.dylib' under directory '*'\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\nwarning: no files found matching '*.csv' under directory '*'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*.proto' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/third_party'\r\n```\r\n", "Oh, you manage to build and link and it only fails at import time. Apologies, missed this part in the initial comment.\r\n\r\nCan you also attach the output of `python -vvv -c \"import tensorflow\" &> log.txt`? I didn't see anything significant in the build log but maybe with the new log things should be easier.\r\n\r\nThe warnings are ok, since the code that creates the pip looks for files that can only exist on Windows (for example). We should clean that up but it's of a very low priority (though we accept PRs)", "> Can you also attach the output of `python -vvv -c \"import tensorflow\" &> log.txt`? I didn't see anything significant in the build log but maybe with the new log things should be easier.\r\n\r\nHere please \r\n[log2.txt](https://github.com/tensorflow/tensorflow/files/4894734/log2.txt)\r\n\r\n@mihaimaruseac I am trying to find reason behind segfault crush of tflite model under python interpreter. Will in general debug build of TF give me extra info/data in crush location?", "We almost never built with debug information, instead we use printf-debugging and sanitizers.\r\n\r\nI'll look at both logs later today / by Monday.", "I am suffering from the same issue. Can you please enable the debug build? it helps a lot to be able to debug when trying to learn the internal implementation.", "+1 - I'm running into this issue as well. It seems wild that the debug build is broken and unsupported.", "FWIW, if you're willing to be a few releases behind I found that the v2.1.0 release builds w/ debug symbols", "I finally succeeded compile the debug mode of tf2.3.0 by using \"bazel build -c opt --copt=-g\"", "> I finally succeeded compile the debug mode of tf2.3.0 by using \"bazel build -c opt --copt=-g\"\r\n\r\nCould you share your complete \"bazel build\" command?", "> > I finally succeeded compile the debug mode of tf2.3.0 by using \"bazel build -c opt --copt=-g\"\r\n> \r\n> Could you share your complete \"bazel build\" command?\r\n\r\nbazel build -c opt --copt=-g //tensorflow/tools/pip_package:build_pip_package --verbose_failures", "I'm running into this issue as well in version 2.4.1. It seems wild that the debug build is broken and unsupported.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/anaconda3/envs/python3.8tf2.4/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: dlopen(/usr/local/anaconda3/envs/python3.8tf2.4/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: __ZN10tensorflow4data12experimental19SnapshotDatasetV2Op13kReaderPrefixE\r\n  Referenced from: /usr/local/anaconda3/envs/python3.8tf2.4/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Expected in: flat namespace\r\n in /usr/local/anaconda3/envs/python3.8tf2.4/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n```\r\n\r\n```\r\n85c8b2a817f95a3e979ecd1ed95bff1dc1335cff (HEAD, tag: v2.4.1, origin/r2.4) Merge pull request #46560 from tensorflow-jenkins/version-numbers-2.4.1-3958\r\nb44a7e3822caae51837b140ea571cf71050f8186 Update version numbers to 2.4.1\r\nfd24839e4ce403490af01b682616678fdb95103a Merge pull request #46557 from tensorflow-jenkins/relnotes-2.4.1-3926\r\n4072b0d80854faa28334d41b2f126e6d5bd8daf2 Update RELEASE.md\r\n20be98a7b31ff79dc83e02fa42761431ae7c2f5d Insert release notes place-fill\r\n```\r\n\r\n## build command\r\n\r\n```\r\nbazel\r\nbuild \\\r\n--config=dbg \\\r\n--copt=\"-Og\" \\\r\n--config=noaws \\\r\n--config=nogcp \\\r\n--config=nohdfs \\\r\n--config=nonccl \\\r\n--repository_cache ../tensorflow-mac-cache \\\r\n--verbose_failures \\\r\n--explain debug_build.log \\\r\n--verbose_explanations \\\r\n//tensorflow/tools/pip_package:build_pip_package\r\n```", "I also get a usable debug build on TF 2.3.2 with  @0gur1's technique of using `-c opt --copt=-g`. I had experienced the same symptom reported in the description.", "On 2.5.0, seeing the same issue but with a different symbol: `_ZN10tensorflow4data17FinalizeDatasetOp15kHasCapturedRefE`\r\n\r\n`-c opt --copt=-g` no longer helps. Linker will fail with \"relocation overflows\" error.", "same here. tried all sort of debug build options with 2.6.0\r\ngetting /tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow4data17FinalizeDatasetOp15kHasCapturedRefE\r\n\r\nis debug build broken? is there a new method to build debug version of TF?", "I get the same problem on r2.4.1\r\nI solved it by using --copt -g --strip=never rather than -c dbg", "> same here. tried all sort of debug build options with 2.6.0\r\n> getting /tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow4data17FinalizeDatasetOp15kHasCapturedRefE\r\n> \r\n> is debug build broken? is there a new method to build debug version of TF?\r\n\r\ntry --copt -g --strip=never\uff0cnot -c dbg\uff0cit make sense on 2.4.1~", "the `nightly` branch is ok with `--copt=\"-O0\" -c dbg`\r\nsucceed on earlier commit\r\n`3fd3ae1fbb10961dd1aa6805280674c781fd4609`", "@conjam,\r\n\r\nCan you try building the latest stable version of tensorflow i.e `2.6.0` and lets us know if the issue still persists in newer version? You can follow this [guide](https://www.tensorflow.org/install/source#tested_build_configurations) to build from source. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39521\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39521\">No</a>\n", "Didn't see the --copt ...", "I will try again\r\n\r\n"]}, {"number": 39520, "title": "tf.signal.rfft documentation refers to Tcomplex as an argument ", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/signal/rfft\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe table of `Args` in the documentation includes `Tcomplex`:\r\n\r\n```\r\nTcomplex | An optional\u00a0tf.DType\u00a0from:\u00a0tf.complex64,\u00a0tf.complex128. Defaults to\u00a0tf.complex64.\r\n```\r\n\r\nBut the function does not accept this argument. Calling `tf.signal.rfft(..., Tcomplex=...)` results in the error:\r\n\r\n```\r\nTypeError: _rfft() got an unexpected keyword argument 'Tcomplex'\r\n```\r\n\r\nThis makes sense given the signature in the documentation:\r\n\r\n```\r\ntf.signal.rfft(\r\n    input_tensor, fft_length=None, name=None\r\n)\r\n```\r\n\r\nand https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/ops/signal/fft_ops.py#L114-L140\r\n\r\n### Submit a pull request?\r\n\r\nNo. I could not find where this table was generated in the code.", "comments": ["@rryan \r\n\r\nIt looks like this is caused by the these two wrappers:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/signal/fft_ops.py#L114-L140\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/signal/fft_ops.py#L145-L172\r\n\r\nThey hide the Tcomplex/Treal argument to the wrapped function, but docstring is copied over.\r\n\r\n```\r\nhelp(tf.signal.rfft)\r\n```\r\n```\r\n...\r\n\r\nArgs:\r\n  ...\r\n  Tcomplex: An optional `tf.DType` from: `tf.complex64, tf.complex128`. Defaults to `tf.complex64`.\r\n  name: A name for the operation (optional).\r\n```", "Hey @MarkDaoust , can I contribute to the issue??", "This is fixed."]}, {"number": 39519, "title": "[INTEL MKL] Threadpool changes for pooling ops.", "body": "", "comments": []}, {"number": 39518, "title": "[INTEL MKL] threadpool support for mkl_conv_bwd ops.", "body": "", "comments": []}, {"number": 39517, "title": "[INTEL MKL] threadpool support for relu, eltwise and softmax.", "body": "", "comments": []}, {"number": 39516, "title": "Resubmit \"Keras grouped convolutions\"", "body": "This PR resubmits the changes from #36773 which were rolled back in dd2ea875d92eeb83e81b1cb92e29e61d488e98b2.\r\n\r\nI couldn't reproduce the failure mentioned in https://github.com/tensorflow/tensorflow/pull/36773#issuecomment-627079900 locally with or without XLA, so I am resubmitting the changes to see if CI is happy now.\r\n@tanzhenyu could you take a look?", "comments": ["@tanzhenyu Any chance you could approve this so CI can run? Since this is a `git revert` of the commit that rolled back the merge of #36773, there should be no need for an indepth review again.", "I rebased the PR and resolved the merge conflicts. @gbaned is there anything that still blocks this?", "I'm not sure why it didn't show up in external CI. But here's what we got:\r\n\r\n```\r\n  File \"/tensorflow/python/keras/layers/convolutional_test.py\", line 296, in test_conv3d\r\n    self._run_test(kwargs, expected_output_shape)\r\n  File \"/tensorflow/python/keras/layers/convolutional_test.py\", line 265, in _run_test\r\n    expected_output_shape=expected_output_shape)\r\n  File \"/tensorflow/python/framework/test_util.py\", line 1717, in decorated\r\n    result = f(self, *args, **kwargs)\r\n  File \"/tensorflow/python/keras/testing_utils.py\", line 227, in layer_test\r\n    model.train_on_batch(input_data, actual_output)\r\n  File \"/tensorflow/python/keras/engine/training.py\", line 1476, in train_on_batch\r\n    logs = train_function(iterator)\r\n  File \"/tensorflow/python/eager/def_function.py\", line 766, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/tensorflow/python/eager/def_function.py\", line 826, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/tensorflow/python/eager/function.py\", line 2812, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/tensorflow/python/eager/function.py\", line 1838, in _filtered_call\r\n    cancellation_manager=cancellation_manager)\r\n  File \"/tensorflow/python/eager/function.py\", line 1915, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/tensorflow/python/eager/function.py\", line 549, in call\r\n    ctx=ctx)\r\n  File \"/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.UnimplementedError:  Hit a case for convolution that is not implemented on GPU.\r\n\t [[{{node cluster_131_1/xla_compile}}]] [Op:__inference_train_function_7135]\r\n```\r\n\r\nEither this doesn't pass the GPU test, or doesn't pass XLA test", "@tanzhenyu Thanks for taking another look, very much appreciated.\r\n\r\nAfter some digging I was able to run the tests with XLA on a cloud VM and could reproduce the failure locally. It looks like currently `train_on_batch` fails for 3D grouped convolutions when XLA is enabled.\r\nSince the underlying failure is not related to this PR and would also appear when using grouped convolutions with `tf.nn.conv3d` on `master`, I disbled the relevant part of the test in 215161ff7a7cd136feac27686561bcedbdaef493. Please let me know if that works for now.", "Thanks for approving, looks like CI is also happy this time \ud83d\udc9a ", "> Thanks for approving, looks like CI is also happy this time \ud83d\udc9a\r\n\r\nGreat, thanks for fixing!"]}, {"number": 39515, "title": "Boolean support for tf.arg{min,max}", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `macOS Catalina 10.15.2 (19C57)`\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `2.2.0`\r\n- Python version: `3.7.5`\r\n\r\n**Describe the feature and the current behavior/state.**\r\nFeature: `tf.argmax` and `tf.argmin` do not fail when input is of dtype `tf.bool`.\r\nCurrent_behavior: `tf.argmax` and `tf.argmin` fail when input is of dtype `tf.bool`.\r\n\r\n**Will this change the current api? How?**\r\nWould add new supported input type to `tf.argmax`.\r\n\r\n**Who will benefit with this feature?**\r\nUsers who need to find the first for non-zero indices from their boolean tensor along specific axis.\r\n\r\n**Sketch of a test case**\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.debugging.assert_equal(tf.argmax((False, True)), tf.cast(1, tf.int64))\r\ntf.debugging.assert_equal(tf.argmin((False, True)), tf.cast(0, tf.int64))\r\n```\r\n\r\n**Logs for current behavior**\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.debugging.assert_equal(tf.argmax((False, True)), tf.cast(1, tf.int64))\r\n...\r\ntensorflow.python.framework.errors_impl.NotFoundError: Could not find valid device for node.\r\nNode:{{node ArgMax}}\r\nAll kernels registered for op ArgMax :\r\n  device='XLA_CPU'; output_type in [DT_INT32, DT_INT64]; Tidx in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='XLA_CPU_JIT'; output_type in [DT_INT32, DT_INT64]; Tidx in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='CPU'; T in [DT_INT64]; output_type in [DT_INT64]\r\n  device='CPU'; T in [DT_INT64]; output_type in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; output_type in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]; output_type in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT16]; output_type in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT16]; output_type in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]; output_type in [DT_INT64]\r\n  device='CPU'; T in [DT_INT16]; output_type in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT8]; output_type in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT8]; output_type in [DT_INT32]\r\n  device='CPU'; T in [DT_INT8]; output_type in [DT_INT64]\r\n  device='CPU'; T in [DT_INT8]; output_type in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; output_type in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; output_type in [DT_INT32]\r\n  device='CPU'; T in [DT_BFLOAT16]; output_type in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; output_type in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; output_type in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; output_type in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; output_type in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; output_type in [DT_INT32]\r\n [Op:ArgMax]\r\n```", "comments": ["Looking at the `argmax_op_test.py`, it actually looks like this case is tested already: https://github.com/tensorflow/tensorflow/blob/cda7fe6da69f6ef1b25b66bf5b22c97c57e974ae/tensorflow/python/kernel_tests/argmax_op_test.py#L129-L132\r\n\r\nShould this be a bug instead of feature request?", "@hartikainen The bool support was only added in b0cd75dc99237a7438c70a2f6f7489e64b5472e5 . It is not in 2.2.0 but is in tf-nightly build.", "Oh, I didn't realize it's already been implemented! Apologies for the noise."]}, {"number": 39514, "title": "Pip install tensorflow, got an error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nnone\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version:\r\ntensorflow 2.2.0\r\n- Python version:\r\n3.6.10\r\n- Installed using virtualenv? pip? conda?:\r\nconda\r\n- Bazel version (if compiling from source):\r\nnone\r\n- GCC/Compiler version (if compiling from source):\r\nnone\r\n- CUDA/cuDNN version:\r\n19.273\r\n- GPU model and memory:\r\n1.7tac\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nLaunched CMD.exe and ran command \"conda install tensorflow\". Returned ERROR, no matching distribution found!\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39514\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39514\">No</a>\n", "@EmersonLiepe \r\n\r\nAre you using 64-bit version of Python or 32 bit version?.Please, refer [link1](https://stackoverflow.com/questions/38896424/tensorflow-not-found-using-pip).Also, let us know which version of Tensorflow you are using?.Please, follow instruction in [Tensorfow website](https://www.tensorflow.org/install/pip).Thanks!\r\nAlso refer to [link](https://github.com/tensorflow/tensorflow/issues/9680#issuecomment-407278974), [link2](url).\r\nIn case this does not help resolve your problem please share error log.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "It's either a Python on 32 bits or a Python installed from MS Marketplace. Please install Python from the official page at python.org, using a 64 bits version", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39514\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39514\">No</a>\n"]}, {"number": 39513, "title": "issue plz reply", "body": "When i run this code \r\npython flow --model cfg/yolo.cfg --loadbin/yolo.weights --demo videofile.mp4 --gpu 1.0 --saveVideo\r\nthis error occurs \r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Noddy\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\\r\npython\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Noddy\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\\r\npython\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Noddy\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\\r\npython\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\Noddy\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line\r\n243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Noddy\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line\r\n343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"flow\", line 4, in <module>\r\n    from darkflow.cli import cliHandler\r\n  File \"C:\\Users\\Noddy\\Downloads\\darkflow-master\\darkflow\\cli.py\", line 3, in <m\r\nodule>\r\n    from .net.build import TFNet\r\n  File \"C:\\Users\\Noddy\\Downloads\\darkflow-master\\darkflow\\net\\build.py\", line 1,\r\n in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Noddy\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\\r\n__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Noddy\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\\r\npython\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Noddy\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\\r\npython\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Noddy\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\\r\npython\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Noddy\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\\r\npython\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Noddy\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\\r\npython\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\Noddy\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line\r\n243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Noddy\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line\r\n343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\nplz see this error and reply as soon as possible\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@shreyas1212 \r\n\r\nRequest you to fill [issue template ](https://github.com/tensorflow/tensorflow/issues/new/choose)\r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website.](https://www.tensorflow.org/install/source_windows)\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "I didnt get how to solve the error\n\n\nOn Wed, 13 May 2020 at 21:32, ravikyram <notifications@github.com> wrote:\n\n> @shreyas1212 <https://github.com/shreyas1212>\n>\n> Request you to fill issue template\n> <https://github.com/tensorflow/tensorflow/issues/new/choose>\n>\n> What is make/model of your cpu?\n> I suspect your cpu model does not support AVX instructions sets.See hardware\n> requirements\n> <https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements>\n> Make sure to download the latest microsoft visual c++ redistributable\n> from here\n> <https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads>\n> .\n> .Also, please follow the instructions from to install from Tensorflow\n> website. <https://www.tensorflow.org/install/source_windows>\n>\n> Please, check Your CPU/Python is on 32 bits?Please, refer #36167\n> <https://github.com/tensorflow/tensorflow/issues/36167> and see if it\n> helps you.Please, refer similar issue #36167\n> <https://github.com/tensorflow/tensorflow/issues/36167> #36151\n> <https://github.com/tensorflow/tensorflow/issues/36151> #36138\n> <https://github.com/tensorflow/tensorflow/issues/36138> #36054\n> <https://github.com/tensorflow/tensorflow/issues/36054> #36045\n> <https://github.com/tensorflow/tensorflow/issues/36045> #36020\n> <https://github.com/tensorflow/tensorflow/issues/36020> #36003\n> <https://github.com/tensorflow/tensorflow/issues/36003> #35988\n> <https://github.com/tensorflow/tensorflow/issues/35988> #35903\n> <https://github.com/tensorflow/tensorflow/issues/35903> #35880\n> <https://github.com/tensorflow/tensorflow/issues/35880> #35865\n> <https://github.com/tensorflow/tensorflow/issues/35865> #35805\n> <https://github.com/tensorflow/tensorflow/issues/35805> #35789\n> <https://github.com/tensorflow/tensorflow/issues/35789> #35773\n> <https://github.com/tensorflow/tensorflow/issues/35773> #35772\n> <https://github.com/tensorflow/tensorflow/issues/35772> #35767\n> <https://github.com/tensorflow/tensorflow/issues/35767> #35766\n> <https://github.com/tensorflow/tensorflow/issues/35766> #35749\n> <https://github.com/tensorflow/tensorflow/issues/35749> #35721\n> <https://github.com/tensorflow/tensorflow/issues/35721> #35618\n> <https://github.com/tensorflow/tensorflow/issues/35618> #35204\n> <https://github.com/tensorflow/tensorflow/issues/35204>\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/39513#issuecomment-628087214>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APSF4ZNJT2I3XPGXRSSNI7TRRK77XANCNFSM4M73PK6Q>\n> .\n>\n", "i didnt get how to solve this issue\n\n\nOn Wed, 6 May 2020 at 19:29, aurdino with shreyas <\nshreyasbhuyan321@gmail.com> wrote:\n\n> I didnt get how to solve the error\n>\n>\n> On Wed, 13 May 2020 at 21:32, ravikyram <notifications@github.com> wrote:\n>\n>> @shreyas1212 <https://github.com/shreyas1212>\n>>\n>> Request you to fill issue template\n>> <https://github.com/tensorflow/tensorflow/issues/new/choose>\n>>\n>> What is make/model of your cpu?\n>> I suspect your cpu model does not support AVX instructions sets.See hardware\n>> requirements\n>> <https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements>\n>> Make sure to download the latest microsoft visual c++ redistributable\n>> from here\n>> <https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads>\n>> .\n>> .Also, please follow the instructions from to install from Tensorflow\n>> website. <https://www.tensorflow.org/install/source_windows>\n>>\n>> Please, check Your CPU/Python is on 32 bits?Please, refer #36167\n>> <https://github.com/tensorflow/tensorflow/issues/36167> and see if it\n>> helps you.Please, refer similar issue #36167\n>> <https://github.com/tensorflow/tensorflow/issues/36167> #36151\n>> <https://github.com/tensorflow/tensorflow/issues/36151> #36138\n>> <https://github.com/tensorflow/tensorflow/issues/36138> #36054\n>> <https://github.com/tensorflow/tensorflow/issues/36054> #36045\n>> <https://github.com/tensorflow/tensorflow/issues/36045> #36020\n>> <https://github.com/tensorflow/tensorflow/issues/36020> #36003\n>> <https://github.com/tensorflow/tensorflow/issues/36003> #35988\n>> <https://github.com/tensorflow/tensorflow/issues/35988> #35903\n>> <https://github.com/tensorflow/tensorflow/issues/35903> #35880\n>> <https://github.com/tensorflow/tensorflow/issues/35880> #35865\n>> <https://github.com/tensorflow/tensorflow/issues/35865> #35805\n>> <https://github.com/tensorflow/tensorflow/issues/35805> #35789\n>> <https://github.com/tensorflow/tensorflow/issues/35789> #35773\n>> <https://github.com/tensorflow/tensorflow/issues/35773> #35772\n>> <https://github.com/tensorflow/tensorflow/issues/35772> #35767\n>> <https://github.com/tensorflow/tensorflow/issues/35767> #35766\n>> <https://github.com/tensorflow/tensorflow/issues/35766> #35749\n>> <https://github.com/tensorflow/tensorflow/issues/35749> #35721\n>> <https://github.com/tensorflow/tensorflow/issues/35721> #35618\n>> <https://github.com/tensorflow/tensorflow/issues/35618> #35204\n>> <https://github.com/tensorflow/tensorflow/issues/35204>\n>> Thanks!\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/39513#issuecomment-628087214>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/APSF4ZNJT2I3XPGXRSSNI7TRRK77XANCNFSM4M73PK6Q>\n>> .\n>>\n>\n", "@shreyas1212 \r\n\r\nIt may occur because you may not have [Visual C++ Redistributable for Windows](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).Please, install Installing C++ redistributables and see if it resolve your issue.Thanks!", "We should close as duplicate since user wants a copy-paste-able string of commands instead of reading the hundreds of duplicated issues or the comments above which have the solution", "Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\nJust to sample over 100 similar issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\n\r\nPlease make sure you do a search in the future.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39513\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39513\">No</a>\n", "Thank you so much that worked but now i have another error if you can solve\nthis error it will be a huge help\n\nC:\\Users\\xxx\\ObjectDetection>ObjectDetection.py\n2020-05-06 19:21:40.487279: W\ntensorflow/stream_executor/platform/default/dso_lo\nader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror:\ncudart64\n_101.dll not found\n2020-05-06 19:21:40.487279: I\ntensorflow/stream_executor/cuda/cudart_stub.cc:29]\n Ignore above cudart dlerror if you do not have a GPU set up on your\nmachine.\nTraceback (most recent call last):\n  File \"C:\\Users\\xxx\\ObjectDetection\\ObjectDetection.py\", line 9, in\n<module>\n    import mss\nModuleNotFoundError: No module named 'mss'\n\nOn Fri, 15 May 2020 at 06:05, tensorflow-butler[bot] <\nnotifications@github.com> wrote:\n\n> Are you satisfied with the resolution of your issue?\n> Yes\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39513>\n> No\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39513>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/39513#issuecomment-628957711>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APSF4ZJYOHDYNKA2XJBNQLDRRSE5DANCNFSM4M73PK6Q>\n> .\n>\n", "yes it worked but now i have another error im stuck at my projects plz\nreply as soon as possible the code which i have written is simple\nObjectDetection.py but this error is occuring\n2020-05-06 19:44:53.869975: W\ntensorflow/stream_executor/platform/default/dso_lo\nader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror:\ncudart64\n_101.dll not found\n2020-05-06 19:44:53.869975: I\ntensorflow/stream_executor/cuda/cudart_stub.cc:29]\n Ignore above cudart dlerror if you do not have a GPU set up on your\nmachine.\nTraceback (most recent call last):\n  File \"C:\\Users\\Noddy\\ObjectDetection\\ObjectDetection.py\", line 26, in\n<module>\n\n    opener.retrieve(\"http://download.tensorflow.org/models/object_detection/\"\n+\nMODEL_FILE, MODEL_FILE)\n  File\n\"C:\\Users\\Noddy\\AppData\\Local\\Programs\\Python\\Python36\\lib\\urllib\\request\n.py\", line 1791, in retrieve\n    fp = self.open(url, data)\n  File\n\"C:\\Users\\Noddy\\AppData\\Local\\Programs\\Python\\Python36\\lib\\urllib\\request\n.py\", line 1757, in open\n    return getattr(self, name)(url)\n  File\n\"C:\\Users\\Noddy\\AppData\\Local\\Programs\\Python\\Python36\\lib\\urllib\\request\n.py\", line 1936, in open_http\n    return self._open_generic_http(http.client.HTTPConnection, url, data)\n  File\n\"C:\\Users\\Noddy\\AppData\\Local\\Programs\\Python\\Python36\\lib\\urllib\\request\n.py\", line 1932, in _open_generic_http\n    response.status, response.reason, response.msg, data)\n  File\n\"C:\\Users\\Noddy\\AppData\\Local\\Programs\\Python\\Python36\\lib\\urllib\\request\n.py\", line 1952, in http_error\n    return self.http_error_default(url, fp, errcode, errmsg, headers)\n  File\n\"C:\\Users\\Noddy\\AppData\\Local\\Programs\\Python\\Python36\\lib\\urllib\\request\n.py\", line 1957, in http_error_default\n    raise HTTPError(url, errcode, errmsg, headers, None)\nurllib.error.HTTPError: HTTP Error 403: Forbidden\n\nPLZ REPLY AS FAST AS YOU CAN\n\nOn Fri, 15 May 2020 at 06:05, tensorflow-butler[bot] <\nnotifications@github.com> wrote:\n\n> Are you satisfied with the resolution of your issue?\n> Yes\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39513>\n> No\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39513>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/39513#issuecomment-628957711>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APSF4ZJYOHDYNKA2XJBNQLDRRSE5DANCNFSM4M73PK6Q>\n> .\n>\n"]}, {"number": 39512, "title": "Is tf.distribute.Strategy API use multi-processing\uff1f", "body": "As python has GIL when use multi threading, so  tf.distribute.Strategy is using multi-processing or multi-threading?\r\n\r\n", "comments": ["Good question. Since you mention multi threading, I would assume you are probably talking about MirroredStrategy which can run on multiple GPUs? If that's the case, the answer is two folded:\r\n1. If you are using purely eager execution, currently the implementation is not optimized and effectively there is no parallelism. I believe you will get a warning about the performance. This mode is only recommended for debugging / developing purpose.\r\n2. More often you should use Keras compile/fit ([example](https://www.tensorflow.org/tutorials/distribute/keras)) or custom training loops with tf.function ([example](https://www.tensorflow.org/tutorials/distribute/custom_training)), in this mode your python code is just constructing the computation graph and the real heavy lifting is done in TensorFlow runtime (C++), which uses C++ multi-threading so python GIL doesn't come into play.", "\r\n\r\n\r\n\r\n\r\n\r\nHi @ckkuang, I tried with `python pdb` to find out where multiple-threading is used to execute the wrapped python functions undef `tf.distribute.MirroredStrategy`, but got lost in `tensors = pywrap_tfe.TFE_Py_Execute(...)`. And I don't see any python multi-threading is used.\r\n\r\n```python\r\n@tf.function\r\ndef _train_step(inputs):\r\n    return tf.identity(inputs)\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint(strategy.run(_train_step, args=(tf.constant(1.0))))\r\n```\r\n\r\nCould you please point out how TF launched the python functions for multiple GPU devices with multiple CPU threads?", "It doesn't use python multi-threading. The wrapped function is compiled into a TensorFlow graph, which will be eventually executed by TF graph [executor](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.h#L51). The graph executor uses two bounded thread pools: one for running multiple ops concurrently (called intra-op threadpool), and the other for parallelizing individual ops (called inter-op threadpool).\r\n\r\nIn the example you posted, the graph contains N (N equals to #GPUs) identity nodes and each node is assigned to one GPU. At runtime the N nodes run concurrently using threads in the intra-op threadpool. By default the number of threads in that pool equals to the your CPU cores.\r\n\r\nFor GPU devices there are three [modes](https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/core/common_runtime/gpu/gpu_device.cc#L485-L496):\r\n  * global: GPU uses threads shared with CPU in the intra-op thread-pool. This is currently the default.\r\n  * gpu_private: GPU uses threads dedicated to this device. This usually saves setting up thread context for enqueuing GPU kernels, and can sometimes improve performance (see [link](https://www.tensorflow.org/guide/gpu_performance_analysis)).\r\n  * gpu_shared: All GPUs share a dedicated thread pool.\r\n\r\nNote that CPU thread launching GPU computations is not blocking - it just enqueues the GPU kernel to a compute stream."]}, {"number": 39511, "title": "Removes duplicate space from retracing warning message", "body": "", "comments": []}, {"number": 39510, "title": "Which older versions(<2) are compatible with CUDA 10.1?", "body": "I am facing compatibility problem with CUDA 10.1 and TF 1.3.\r\nI would like to know,\r\nWhich TF versions (<2) are supported by CUDA 10.1? ", "comments": ["<img width=\"899\" alt=\"Screen Shot 2020-05-13 at 1 35 47 PM\" src=\"https://user-images.githubusercontent.com/42785357/81862551-bc5d2580-951e-11ea-9820-87d6e97cec67.png\">\r\n\r\nWe don't have TF 1.X binaries with Cuda 10.1 support.\r\nSee https://www.tensorflow.org/install/source#gpu"]}, {"number": 39509, "title": "Invalid results when running TFLite + ruy computation within a NodeJS v11+ addon on ARMv7", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, `libdeepspeech.so`: https://github.com/mozilla/DeepSpeech/pull/2952\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Buster, Armbian Buster\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): r2.2, master\r\n- TensorFlow version (use command below): r2.2, master\r\n- Python version: N/A\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): GCC 6.5.0 (RPi toolchain integrated in TensorFlow), GCC 7.2.1 (Linaro toolchain custom-added to TensorFlow\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nModel computation differs when running the library inside a nodejs process (v11.0.0+), on ARMv7 hardware\r\n\r\n**Describe the expected behavior**\r\nModel computation should be the same\r\n\r\n**Standalone code to reproduce the issue**\r\nReproduction environment is complicated for now (need to build libdeepspeech, the nodejs addon, install and run and compare to non nodejs), working on a much smaller one as of now.\r\n\r\nHow much simple would this needs to be? Our setup is a bit complicated.\r\n\r\nOur model uses floats as input, so we need `EvalHybrid` to use the threaded-enabled fast-path enabled by `-DTFLITE_WITH_RUY_GEMV`.\r\n\r\nBuilding for Android:\r\n```\r\nPYTHON_BIN_PATH=/usr/bin/python PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages TF_ENABLE_XLA=0 TF_NEED_OPENCL_SYCL=0 TF_NEED_CUDA=0 TF_NEED_ROCM=0 TF_NEED_MPI=0 TF_DOWNLOAD_CLANG=0 CC_OPT_FLAGS=\"-march=native -Wno-sign-compare\" TF_SET_ANDROID_WORKSPACE=1 ANDROID_NDK_HOME=$HOME/Documents/codaz/Mozilla/DeepSpeech/Android/android-ndk-r18b/ ANDROID_NDK_API_LEVEL=21 ANDROID_SDK_HOME=$HOME/Documents/codaz/Mozilla/DeepSpeech/Android/SDK/ ANDROID_API_LEVEL=27 ANDROID_BUILD_TOOLS_VERSION=28.0.3 ./configure && bazel clean && bazel build -s --verbose_failures --workspace_status_command=\"bash native_client/bazel_workspace_status_cmd.sh\" --config=monolithic --config=android --config=android_arm --define=runtime=tflite --action_env ANDROID_NDK_API_LEVEL=21 --cxxopt=-std=c++11 --copt=-D_GLIBCXX_USE_C99 //native_client:libdeepspeech.so\r\n```\r\n\r\nRunning on Android (Nokia 1.3, QM215 Cortex-A53 SoC):\r\n```\r\nDRX:/data/local/tmp $ LD_LIBRARY_PATH=$(pwd)/ ./deepspeech --model model_ldc93s1_16-2000.tflite --audio LDC93S1_pcms16le_1_16000.wav                                                                                                                                                                                                                                                                                    \r\nTensorFlow: v2.2.0-rc3-31-ga6cee0345c\r\nDeepSpeech: v0.7.0-30-gbb716efe\r\nINFO: Initialized TensorFlow Lite runtime.\r\naudio_format=1\r\nnum_channels=1\r\nsample_rate=16000 (desired=16000)\r\nbits_per_sample=16\r\nres.buffer_size=93594\r\nshe had your dark suit in greasy wash water all year\r\n```\r\n\r\nBuilding for RPi3:\r\n```\r\nPYTHON_BIN_PATH=/usr/bin/python PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages TF_ENABLE_XLA=0 TF_NEED_OPENCL_SYCL=0 TF_NEED_CUDA=0 TF_NEED_ROCM=0 TF_NEED_MPI=0 TF_DOWNLOAD_CLANG=0 CC_OPT_FLAGS=\"-march=native -Wno-sign-compare\" TF_SET_ANDROID_WORKSPACE=0 ./configure && bazel clean && bazel build -s --verbose_failures --workspace_status_command=\"bash native_client/bazel_workspace_status_cmd.sh\" --config=monolithic --crosstool_top=@local_config_arm_compiler//:toolchain --cpu=armeabi --define=raspberry_pi_with_neon=true --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --copt=-march=armv7-a --copt=-mfloat-abi=hard --copt=-mfpu=neon-fp-armv8 --copt=-DRASPBERRY_PI --copt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-mno-unaligned-access --define=tensorflow_mkldnn_contraction_kernel=0 --define=runtime=tflite --copt=-funsafe-math-optimizations --copt=-ftree-vectorize --copt=-pipe --copt=-DTFLITE_WITH_RUY_GEMV --define=tflite_with_ruy=true -c opt --copt=-pthread --linkopt=-lpthread //native_client:libdeepspeech.so\r\n```\r\n\r\nRunning (C++ binary) on RPi3:\r\n```\r\n$ ./deepspeech --model model_ldc93s1_16-2000.tflite --audio LDC93S1_pcms16le_1_16000.wav\r\nTensorFlow: v2.2.0-rc3-31-ga6cee0345c\r\nDeepSpeech: v0.7.0-30-gbb716efe\r\nshe had your dark suit in greasy wash water all year\r\n```\r\n\r\nRunning (NodeJS binding) on RPi3:\r\n```\r\n$ ./node ~/node_modules/.bin/deepspeech --model model_ldc93s1_16-2000.tflite --audio LDC93S1_pcms16le_1_16000.wav\r\nLoading model from file model_ldc93s1_16-2000.tflite\r\nTensorFlow: v2.2.0-rc3-31-ga6cee0345c\r\nDeepSpeech: v0.7.0-30-gbb716efe\r\nstatic napi_value__* DeepSpeechNAPI::CreateModel(napi_env, napi_callback_info) ModelSate: 0x3287d98\r\nstatic napi_value__* DeepSpeechNAPI::CreateModel(napi_env, napi_callback_info) ModelSate(int64_t): 52985240\r\nLoaded model in 0.004686s.\r\nRunning inference.\r\nstatic napi_value__* DeepSpeechNAPI::SpeechToText(napi_env, napi_callback_info) ModelSate(int64_t): 52985240\r\nstatic napi_value__* DeepSpeechNAPI::SpeechToText(napi_env, napi_callback_info) ModelSate: 0x3287d98\r\nshe h yyour drk suit in greasy wash waer all year\r\nInference took 2.038s for 2.925s audio file.\r\n```\r\n\r\n**Other info / logs**\r\nI have tested many hypothesis:\r\n - changing toolchain to gcc 6.5.0 bundled by tensorflow (we use a different one by default)\r\n - re-writing the nodejs swig-generated wrapper with n-api, in a very basic form\r\n - repro on master (commit 5be613ef4f3ec2608deed653ab4815bbbcfbe7f8)\r\n - repro on master with newer ruy (commit 808ff748e0c7dc746a413fe45fa022d63e6253e8)\r\n - bisected tensorflow: first repro is when tflite + ruy get the ability to run threads (commit be369f57e9e46d03ccd62f1031f9dc484c1016de)\r\n - bisected nodejs, issue first arises in https://github.com/nodejs/node/pull/21983/commits (obviously, hard to actionate)\r\n - repro with different model size (if input size is not a multiple of 4, **works**, we do not use threads somehow because of https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc#L1210)\r\n - same code, same nodejs version runs fine on ARM64 (Armbian on S905X), also excluded the SoC itself and the distro (repro under Armbian on S905X when running multilib armv7, repro on RPi3 and RPi4)\r\n - unable to reproduce and to get indication of any weird thing happening when running under valgrind on other platforms (valgrind on armv7/raspbian seems broken, valgrind on armv7/armbian dies because of unsupported instruction produced by `vfmaq_f32` in eigen)\r\n - **disabling** `kNeon` path in ruy but keeping threads, the computation works\r\n - **disabling** threads with `kNeon` enabled works\r\n - obviously verified that the input of the model is correct (dumped mfcc vectors, input states and output logits, and verified they were different only under nodejs runtime)\r\n   - input here: https://github.com/lissyx/DeepSpeech/blob/bb716efe1ead50fc822d4f5faf0f2fa757adb2d5/native_client/tflitemodelstate.cc#L293-L299\r\n   - output here: https://github.com/lissyx/DeepSpeech/blob/bb716efe1ead50fc822d4f5faf0f2fa757adb2d5/native_client/tflitemodelstate.cc#L308-L316\r\n   - verified dumping the vector values (and verified as well the copy function)\r\n   - we run several pass for the audio file, per small timesteps of 320ms, the very first output is already broken\r\n - no problem with the python bindings, java (android), even running concurrent threads (c++)\r\n - obviously tried debug build with no optimization at all\r\n - model trained on r1.15 and used on r2.2 (we produced a r2.2-trained one and there was the same issue)\r\n\r\n**Current questions I am unable to reply**\r\n - is running under NodeJS exposing a bug that we have everywhere but that does not manifest?\r\n - `v8` used by NodeJS is using both threads **and** NEON instructions, when ruy's ARM code is also using threads and NEON in hand-written ASM?", "comments": ["This should include the audio file and model (smallest I could get) reproducing [data_issue_39509.zip](https://github.com/tensorflow/tensorflow/files/4622361/data_issue_39509.zip)", "This should allow running on Android [android_build.zip](https://github.com/tensorflow/tensorflow/files/4622455/android_build.zip)\r\n", "This should allow `npm install` on rpi3/raspbian buster (nodejs v11.0.0 will use the napi wrapper) [deepspeech-0.7.0.tar.gz](https://github.com/tensorflow/tensorflow/files/4622471/deepspeech-0.7.0.tar.gz)\r\n", "This should allow running on Linux/ARMv7 (Raspbian Buster), pure C++ [linux_build.zip](https://github.com/tensorflow/tensorflow/files/4622483/linux_build.zip)\r\n", "Removing myself from the reviewer list since I am not familiar with nodejs.", "> Removing myself from the reviewer list since I am not familiar with nodejs.\r\n\r\nEven if unfamiliar with nodejs (I am not as well), could someone share ideas on how to properly and consistently verify what happens during computation? There's several layers of C++ templating finishing in `asm` code, and I still fail to verify where in the chain does the discrepency happens.", "Update: I might have correct values by adding one `RUY_MAKE_ZERO(q7);`\r\n\r\nThis change removed it, and there's no explanation: https://github.com/tensorflow/tensorflow/commit/2359c4e45e1226e4a9d4072c2b7753b5ae731f44", "PR #39951 is a tentative fix on the r2.2 branch. I'm unsure of what would be the recommended way to fix it further:\r\n - should it be only fixed on master\r\n - on master, should it be a patch on top of ruy, or should the fix be directly in ruy repo", "PR against upstream ruy: https://github.com/google/ruy/pull/69", "And I could verify the patch on current tensorflow master to also fix the issue:\r\n```\r\npi@gateway:~/ds $ LD_LIBRARY_PATH=$(pwd)/ ./deepspeech --model model_ldc93s1_16-2000.tflite --audio LDC93S1_pcms16le_1_16000.wav ; LD_LIBRARY_PATH=$(pwd)/ ./node ~/node_modules/.bin/deepspeech --model model_ldc93s1_16-2000.tflite --audio LDC93S1_pcms16le_1_16000.wav \r\nTensorFlow: 1.15.0-rc1-21531-gea8e87c8e9\r\nDeepSpeech: v0.7.0-93-g2d5a963b\r\nshe had your dark suit in greasy wash water all year\r\nLoading model from file model_ldc93s1_16-2000.tflite\r\nTensorFlow: 1.15.0-rc1-21531-gea8e87c8e9\r\nDeepSpeech: v0.7.0-93-g2d5a963b\r\nstatic napi_value__* DeepSpeechNAPI::CreateModel(napi_env, napi_callback_info) ModelSate: 0x1b8be80\r\nstatic napi_value__* DeepSpeechNAPI::CreateModel(napi_env, napi_callback_info) ModelSate(int64_t): 28884608\r\nLoaded model in 0.003122s.\r\nRunning inference.\r\nstatic napi_value__* DeepSpeechNAPI::SpeechToText(napi_env, napi_callback_info) ModelSate(int64_t): 28884608\r\nstatic napi_value__* DeepSpeechNAPI::SpeechToText(napi_env, napi_callback_info) ModelSate: 0x1b8be80\r\nshe h yyour drk suit in greasy wash waer all year\r\nInference took 1.993s for 2.925s audio file.\r\npi@gateway:~/ds $ LD_LIBRARY_PATH=$(pwd)/ ./deepspeech --model model_ldc93s1_16-2000.tflite --audio LDC93S1_pcms16le_1_16000.wav ; LD_LIBRARY_PATH=$(pwd)/ ./node ~/node_modules/.bin/deepspeech --model model_ldc93s1_16-2000.tflite --audio LDC93S1_pcms16le_1_16000.wav \r\nTensorFlow: 1.15.0-rc1-21531-gea8e87c8e9\r\nDeepSpeech: v0.7.0-93-g2d5a963b\r\nshe had your dark suit in greasy wash water all year\r\nLoading model from file model_ldc93s1_16-2000.tflite\r\nTensorFlow: 1.15.0-rc1-21531-gea8e87c8e9\r\nDeepSpeech: v0.7.0-93-g2d5a963b\r\nstatic napi_value__* DeepSpeechNAPI::CreateModel(napi_env, napi_callback_info) ModelSate: 0x1593d80\r\nstatic napi_value__* DeepSpeechNAPI::CreateModel(napi_env, napi_callback_info) ModelSate(int64_t): 22625664\r\nLoaded model in 0.003206s.\r\nRunning inference.\r\nstatic napi_value__* DeepSpeechNAPI::SpeechToText(napi_env, napi_callback_info) ModelSate(int64_t): 22625664\r\nstatic napi_value__* DeepSpeechNAPI::SpeechToText(napi_env, napi_callback_info) ModelSate: 0x1593d80\r\nshe had your dark suit in greasy wash water all year\r\nInference took 2.006s for 2.925s audio file.\r\n```", "Thanks for the ruy PR. Merging it at the moment. We will also need to update TensorFlow's references to the ruy repo to point to this new commit. I'll take care of this in a few hours.", "Perfect, I was just asking about that on tensorflow side. ", "@bjacob Gentle ping: I see some commit to `third_party/ruy` that moves to 1b313682ef8b8fc8ed08719c610d1c3503b016bf, should I assume this is not yet uptodate with the fix?", "[1b313682](https://github.com/google/ruy/commit/1b313682ef8b8fc8ed08719c610d1c3503b016bf) is a 27-days-old commit.\r\n\r\nI am preparing the update, it will update to [1a8b7eab](https://github.com/google/ruy/commit/1a8b7eabd5039cd1423b3e22e6d7241d261576dc).\r\n\r\nI hope for it to be merged today.", "It's merged now:\r\nhttps://github.com/tensorflow/tensorflow/commit/6faecb105eecdf444ea7b4875fbb226e21556066", "Awesome, thanks a lot!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39509\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39509\">No</a>\n", "Unfortunately, the update got rolled back again. Maybe tomorrow..."]}, {"number": 39508, "title": "with_values for SparseTensor", "body": "This provides a `with_values` function for `SparseTensor` in analogy to the `with_values` function of `RaggedTensor`. See #39507", "comments": ["@jaingaurav Can you please take a look on this PR ? Thanks!", "@edloper @penpornk: Could you please take a look here?", "@edloper Can you please review this PR ? Thanks!", "@ngc92 Can you please address Ubuntu Sanity errors? Thanks!", "@ngc92 I think the issue is that the entries in `tensorflow.sparse.-sparse-tensor.pbtxt` need to be sorted alphabetically -- i.e., you need to move the entry for `with_values` to be after the entry for `get_shape`.  (I believe that the API golden files are compared with a simple diff.)", "@ngc92 For the API golden diff failure in `Ubuntu CPU`, you can run this command to fix it automatically:\r\n```\r\nbazel run tensorflow/tools/api/tests:api_compatibility_test -- --update_goldens True\r\n```\r\n\r\nFor the `Ubuntu Sanity` failure, the errors are:\r\n```\r\nFAIL: Found 7 non-allowlisted pylint errors:\r\ntensorflow/python/framework/sparse_tensor_test.py:102: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/framework/sparse_tensor_test.py:104: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/framework/sparse_tensor_test.py:105: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/framework/sparse_tensor_test.py:106: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/framework/sparse_tensor_test.py:107: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/framework/sparse_tensor_test.py:110: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/framework/sparse_tensor_test.py:111: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 6\r\n```\r\nPlease fix the indentation accordingly.", "@ngc92 Can you please check @edloper's , @penpornk's comments and keep us posted. Thanks!", "@ngc92  Any update on this PR? Please. Thanks!", "@gbaned Sorry for the delay, I was on vacation and had only my laptop. The golden and indentation problems should be fixed now."]}, {"number": 39507, "title": "with_values and map_flat_values for SparseTensor", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`tf.RaggedTensor` offers two utility functions, `with_values` and `map_flat_values`,\r\nwhich allow to modify the content of the `RaggedTensor` while keeping the structure.\r\n\r\nThis feature request is for adding the same to `tf.SparseTensor`.\r\n\r\n**Will this change the current api? How?**\r\nAdd a new function `with_values` to `tf.SparseTensor` which updates the values\r\nbut leaves the indices unchanged.\r\nAdd a new function `tf.sparse.map_flat_values` (or maybe just `map_values`) that applies\r\nan operation to the values Tensor of a `SparseTensor`, and updates it.\r\n\r\n**Who will benefit with this feature?**\r\nThis would make the interfaces of sparse and ragged tensors more similar, and provides to very basic utility functions. \r\n\r\n**Any Other info.**\r\nI do have a PR ready for the `with_values` function. \r\nAm working on `map_flat_values`. In the case of multiple `SparseTensor`s, should this function check that they have identical indices or not? `ragged.map_flat_values` checks row_splits, but I guess in a typical application the number of elements in row splits will be significantly lower than the number of values, whereas in a sparse tensor there are more indices (if the tensor is at least 2 dimensional) than values, making for a larger relative performance impact.", "comments": ["Closing this issue since the associated PR has been merged. \r\nFeel free to reopen if required. Thank you!"]}, {"number": 39506, "title": "CUDA installation for Ubuntu 18.4", "body": "When I want to install CUDA for Ubuntu 18.4 I follow this guide: https://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_101\r\n\r\nWhen I want to install `cuda-10-1` it sais: `E: Unable to locate package cuda-10-1`.\r\n\r\nDid NVIDIA change anything on its side? Does the documentation need an update? Some weeks ago this installation was all ok...", "comments": ["@PhilipMay \r\nCan you please refer to this [link](https://askubuntu.com/questions/1153604/problems-installing-cuda-10-0), [link2](https://stackoverflow.com/questions/58921156/unable-to-install-cuda-on-ubuntu-16-04) and let us know if it helps.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39505, "title": "Support ragged output in TextVectorization layer", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently TextVectorization supports ragged input, but always outputs dense tensor padded with 0.\r\nIt would be cool if in INT mode it will output ragged tensor, so caller may call downstream layers that supports ragged tensors and convert to dense when he needed.\r\n\r\n**Will this change the current api? How?**\r\nNo api changes\r\n\r\n**Who will benefit with this feature?**\r\nUsers of ragged tensors\r\n", "comments": ["@shkarupa-alex \r\n\r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "I think this one is being refered to : https://www.tensorflow.org/guide/ragged_tensor", "@shkarupa-alex You can  pass a RaggedTensor into StringLookup, which will return a RaggedTensor. The rest of the TextVectorization preprocessing can be done via standard TF ops.\r\n\r\nClosing  this issue since String Lookup  layer is now available in TF 2.3.  Thanks!"]}, {"number": 39504, "title": "TextVectorization layer error with 3D input data", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): Colab 2.x\r\n- TensorFlow version (use command below): v2.2.0-0-g2b96f3662b 2.2.0\r\n- Python version: Colab default\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n**Describe the current behavior**\r\nTextVectorization layer limits input shape to 2D tensors.\r\n\r\n**Describe the expected behavior**\r\nTextVectorization layer should work well with tensors of any shape, e.g.:\r\n1D - skipgram word2vec input ([batch of words])\r\n2D - default [batch; words] input for many nlp tasks (currently works)\r\n3D - fasttext skipgram input ([batch; words; character ngrams]\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/122NTBK6Gr2V2cHSVyXG5xqANh_N-oeNo?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-9-fdd620ffcaa5> in <module>()\r\n      1 # Fails with 1D data\r\n----> 2 vectorization(tf.ragged.constant(data1))\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in set_shape(self, shape)\r\n   1105       raise ValueError(\r\n   1106           \"Tensor's shape %s is not compatible with supplied shape %s\" %\r\n-> 1107           (self.shape, shape))\r\n   1108 \r\n   1109   # Methods not supported / implemented for Eager Tensors.\r\n\r\nValueError: Tensor's shape (11,) is not compatible with supplied shape (None, None)\r\n```\r\n", "comments": ["Both 1D and 3D data fail with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/f882868bd255a2bd98c3c159749ed6a3/39504.ipynb).\r\n\r\nWith [TF-nightly](https://colab.research.google.com/gist/amahendrakar/6397d8451d5ecfea2f4b870f8fbf2b64/39504-tf-nightly.ipynb#scrollTo=tVSbBFvoD9Bp) 1D data works fine, facing issues with 3D data. Please find the attached gist. Thanks!", "I would like to work on this issue", "Are you trying to do anything other than a straight index lookup with the TextVectorization layer?", "> Are you trying to do anything other than a straight index lookup with the TextVectorization layer?\r\n\r\nNo, i don't. Just INT lookup for any-shaped input required for me.", "Hi @shkarupa-alex, this should be fixed in the nightly. Also, you might be interested in the StringLookup layer (also exported in nightly) which will do string->integer mapping directly without normalization or other overhead and should work on input of any dimensionality. Try it out and let me know what you think!", "Could you please check in nightly version and let us know. Please feel free reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39504\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39504\">No</a>\n"]}, {"number": 39503, "title": "google colab GPU is too slow (x1 time CPU speed) when using tensorflow 1.15.0", "body": "hello every one;\r\nI tried to run the code below in google colab with both of version of tensorflow 2.0 and 1.15.0\r\nwith the 2.0 version i had no problems (mostly GPU's speed was over 29 time faster than CPU's speed) but when using the 1.15.0 version the GPU was as fast as CPU no difference at all. any solutions or suggestions. i need 1.15.0 version because of the object detection api. thank you very much for your help\r\n                                                                  Code\r\n`%tensorflow_version 1.x\r\nimport tensorflow.compat.v2 as tf\r\n#import tensorflow as tf\r\nprint(tf.__version__)\r\n#tf.enable_v2_behavior\r\ndevice_name = tf.test.gpu_device_name()\r\nif device_name != '/device:GPU:0':\r\n  raise SystemError('GPU device not found')\r\nprint('Found GPU at: {}'.format(device_name))\r\nimport timeit\r\n\r\ndevice_name = tf.test.gpu_device_name()\r\nif device_name != '/device:GPU:0':\r\n  print(\r\n      '\\n\\nThis error most likely means that this notebook is not '\r\n      'configured to use a GPU.  Change this in Notebook Settings via the '\r\n      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\r\n  raise SystemError('GPU device not found')\r\n\r\ndef cpu():\r\n  with tf.device('/cpu:0'):\r\n    random_image_cpu = tf.random.normal((100, 100, 100, 3))\r\n    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\r\n    return tf.math.reduce_sum(net_cpu)\r\n\r\ndef gpu():\r\n  with tf.device('/device:GPU:0'):\r\n    random_image_gpu = tf.random.normal((100, 100, 100, 3))\r\n    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\r\n    return tf.math.reduce_sum(net_gpu)\r\n  \r\n\r\ncpu()\r\ngpu()\r\n\r\n\r\nprint('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\r\n      '(batch x height x width x channel). Sum of ten runs.')\r\nprint('CPU (s):')\r\ncpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\r\nprint(cpu_time)\r\nprint('GPU (s):')\r\ngpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\r\nprint(gpu_time)\r\nprint('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))`\r\n                                                             results\r\n`  TensorFlow 1.x selected.\r\nversion tensorflow:1.15.0\r\nFound GPU at: /device:GPU:0\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nTime (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\r\nCPU (s):\r\n0.17279088399936882\r\nGPU (s):\r\n0.18540516800021578\r\nGPU speedup over CPU: 0x`\r\n\r\n`tf.version= 2.2.0\r\nFound GPU at: /device:GPU:0\r\nTime (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\r\nCPU (s):\r\n2.4908546100000137\r\nGPU (s):\r\n0.10798676599998203\r\nGPU speedup over CPU: 23x\r\n2.2.0`", "comments": ["@mamidpou \r\nCode shared does not seem complete and indented, please share code such that we can replicate the issue faced, or if possible share a colab gist for us to analyse the issue faced.", "@Saduf2019 \r\nthis is the link to the google colab notebook thanks for your response:\r\nhttps://colab.research.google.com/drive/1EaWha1zW42e8pJ5m2KXKFPe9bzPHhhLf?usp=sharing", "I am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/b99ec30a4e3065e7e326f57ce1208d2d/replicate.ipynb)", "My tranining was also very slow because I was doing  `!pip install tensorflow==1.15.0`\r\n `!pip install tensorflow-gpu==1.15.0` solved the issue (~30 times faster)", "@dpinol \r\nNo difference even when i use !pip install tensorflow-gpu==1.15.0 problem not solved for me. have you done any configuration before installing tensorflow???  \r\nthe output:\r\ntensorflow version 1.15.0\r\nFound GPU at: /device:GPU:0\r\nTime (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\r\nCPU (s):\r\n0.18695778899973448\r\nGPU (s):\r\n0.16901270900007148\r\nGPU speedup over CPU: 1x\r\n", "I tried with TF 2.2 version and got 32X speed up.\r\nPlease upgrade to latest TF version as we are continuously monitoring it and you can expect bug/performance improvements in newer TF versions.\r\nUnfortunately we are not accepting changes to TF 1.X unless they are security fix related issues.\r\nThanks!", "Thank you for your r\u00e9ponse but i need to use object detection api that is\nnot yet provided into tf version 2\n\nLe mer. 20 mai 2020 \u00e0 00:09, Yasir Modak <notifications@github.com> a\n\u00e9crit :\n\n> I tried with TF 2.2 version and got 32X speed up.\n> Please upgrade to latest TF version as we are continuously monitoring it\n> and you can expect bug/performance improvements in newer TF versions.\n> Unfortunately we are not accepting changes to TF 1.X unless they are\n> security fix related issues.\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/39503#issuecomment-631134364>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APSDQY66PCP7MIMZ5LWAVPLRSMGTPANCNFSM4M7VWXDQ>\n> .\n>\n", "> My tranining was also very slow because I was doing `!pip install tensorflow==1.15.0`\r\n> `!pip install tensorflow-gpu==1.15.0` solved the issue (~30 times faster)\r\n\r\nI thought I was having the same issue, and for some mysterious reason doing the pip install temporarily resolved it, but in reality the bottleneck was in loading the training data from the mounted Google Drive directory. So it is worth benchmarking which step of the process is slow (also check Runtime/Manage Sessions and check that none of the other sessions are hijacking the GPU RAM).", "Object detection api for TF2 is a work in progress. \r\nRefer this GitHub [thread](https://github.com/tensorflow/models/issues/6423) to know more.\r\nClosing this issue for now. Thanks!"]}, {"number": 39502, "title": "DLL load failed pywrap_tensorflow_internal", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Installed using pip\r\n- TensorFlow version: version 2.2\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: CUDA - 10.1 and cuDNN - 7.6.5\r\n- GPU model and memory: Nvidia Geforce 1050 Ti\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm trying to install TensorFlow GPU. All the requirements are downloaded according to [this](https://www.tensorflow.org/install/gpu). I have downloaded and used TensorFlow GPU before but everytime when i download i get same type error. Previously downgrading TensorFlow solved the issue however this time it didn't work. I get error ImportError: DLL load failed: The specified module could not be found. and cryptic error saying pywrap installation failed.  \r\nThe issue is similar to [this](https://github.com/tensorflow/tensorflow/issues/22512).  \r\nI followed the instructions in the issue to solve it.  \r\nThe msvcp140.dll file is present in my system32 folder.\r\n![Capture](https://user-images.githubusercontent.com/38111546/81801381-f71c8900-9531-11ea-9813-c41a3894a537.PNG)\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI get the problem when  I import Tensorflow.\r\n`import tensorflow`\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nFull traceback:\r\n```\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\anike\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\anike\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\anike\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\anike\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\anike\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\anike\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\anike\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\anike\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\anike\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\anike\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\anike\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\anike\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\anike\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\n```\r\n\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@aniketbote \r\n\r\nIt may occur because you may not have Visual C++ Redistributable for Windows.\r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "\r\nInstalling C++ redistributables solved the issue.\r\nThank you for the help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39502\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39502\">No</a>\n"]}, {"number": 39501, "title": "Errors encountered in model training", "body": "\r\nI encountered an errors when I was training mask_rcnn model with my own dataset .  So I ask for you help . I will appreciate it if you can help me out.\r\n\r\nCaused by op 'GatherV2_4', defined at:\r\n  File \"model_main.py\", line 110, in <module>\r\n    tf.app.run()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\first_keras\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"model_main.py\", line 106, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\first_keras\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 471, in train_and_evaluate\r\n    return executor.run()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\first_keras\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 610, in run\r\n    return self.run_local()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\first_keras\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 711, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\first_keras\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\first_keras\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1207, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\first_keras\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1237, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\first_keras\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"C:\\Users\\\u745e\\Documents\\GitHub\\models\\research\\object_detection\\model_lib.py\", line 308, in model_fn\r\n    features[fields.InputDataFields.true_image_shape])\r\n  File \"C:\\Users\\\u745e\\Documents\\GitHub\\models\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 846, in predict\r\n    true_image_shapes))\r\n  File \"C:\\Users\\\u745e\\Documents\\GitHub\\models\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 1016, in _predict_second_stage\r\n    image_shape, true_image_shapes)\r\n  File \"C:\\Users\\\u745e\\Documents\\GitHub\\models\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 746, in _proposal_postprocess\r\n    anchors, image_shape_2d, true_image_shapes)\r\n  File \"C:\\Users\\\u745e\\Documents\\GitHub\\models\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 1700, in _postprocess_rpn\r\n    groundtruth_weights_list)\r\n  File \"C:\\Users\\\u745e\\Documents\\GitHub\\models\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 1786, in _sample_box_classifier_batch\r\n    single_image_groundtruth_weights)\r\n  File \"C:\\Users\\\u745e\\Documents\\GitHub\\models\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 1914, in _sample_box_classifier_minibatch_single_image\r\n    groundtruth_weights=groundtruth_weights)\r\n  File \"C:\\Users\\\u745e\\Documents\\GitHub\\models\\research\\object_detection\\core\\target_assigner.py\", line 196, in assign\r\n    reg_weights = self._create_regression_weights(match, groundtruth_weights)\r\n  File \"C:\\Users\\\u745e\\Documents\\GitHub\\models\\research\\object_detection\\core\\target_assigner.py\", line 334, in _create_regression_weights\r\n    groundtruth_weights, ignored_value=0., unmatched_value=0.)\r\n  File \"C:\\Users\\\u745e\\Documents\\GitHub\\models\\research\\object_detection\\core\\matcher.py\", line 214, in gather_based_on_match\r\n    gathered_tensor = self._gather_op(input_tensor, gather_indices)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\first_keras\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2675, in gather\r\n    return gen_array_ops.gather_v2(params, indices, axis, name=name)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\first_keras\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3331, in gather_v2\r\n    \"GatherV2\", params=params, indices=indices, axis=axis, name=name)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\first_keras\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\first_keras\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\first_keras\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\first_keras\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): indices[0] = 0 is not in [0, 0)\r\n         [[node GatherV2_4 (defined at C:\\Users\\\u745e\\Documents\\GitHub\\models\\research\\object_detection\\core\\matcher.py:214)  = GatherV2[Taxis=DT_INT32, Tindices=DT_INT64, Tparams=DT_FLOAT, _device=\"/device:CPU:0\"](cond/Merge, Reshape_8, GatherV2_3/axis)]]\r\n         [[node IteratorGetNext (defined at model_main.py:106)  = IteratorGetNext[output_shapes=[[1], [1,?,?,3], [1,2], [1,3], [1,100], [1,100,4], [1,100,4], [1,100,4], [1,100], [1,100,?,?], [1,100], [1,100], [1]], output_types=[DT_INT32, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_FLOAT, DT_BOOL, DT_FLOAT, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](IteratorV2)]]", "comments": ["@A12-RUI,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code, the dataset and the TensorFlow version you are using. Thanks!", "I used  coco type dataste ,and tensorflow version is 1.12.0.\r\ncode:\r\n```\r\n# coding: utf-8\r\n\r\n# # Object Detection Demo\r\n# Welcome to the object detection inference walkthrough!  This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image. Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start.\r\n\r\n\r\nfrom distutils.version import StrictVersion\r\nimport numpy as np\r\nimport os\r\nimport six.moves.urllib as urllib\r\nimport sys\r\nimport tarfile\r\nimport tensorflow as tf\r\nimport zipfile\r\nfrom IPython.display import display\r\nfrom collections import defaultdict\r\nfrom io import StringIO\r\nfrom matplotlib import pyplot as plt\r\nfrom PIL import Image\r\n\r\n# This is needed since the notebook is stored in the object_detection folder.\r\n\r\nsys.path.append(r'C:\\Users\\\u745e\\Documents\\GitHub\\models\\research')\r\nfrom object_detection.utils import ops as utils_ops\r\n\r\n# if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):\r\n#   raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')\r\n\r\n\r\n\r\n\r\n# This is needed to display the images.\r\n# get_ipython().magic(u'matplotlib inline')\r\n\r\n\r\n# ## Object detection imports\r\n# Here are the imports from the object detection module.\r\n\r\n\r\n\r\nfrom utils import label_map_util\r\n\r\nfrom utils import visualization_utils as vis_util\r\n\r\n\r\n# # Model preparation\r\n\r\n# ## Variables\r\n#\r\n# Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_FROZEN_GRAPH` to point to a new .pb file.\r\n#\r\n# By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies.\r\n\r\n# What model to download.\r\nMODEL_NAME = 'C:/Users/\u745e/Documents/GitHub/models/research/object_detection/test_defects_detection'   ###############\r\n# MODEL_FILE = MODEL_NAME + '.tar.gz'\r\n# DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\r\n\r\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\r\nPATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\r\n\r\n# List of the strings that is used to add correct label for each box.\r\nPATH_TO_LABELS = os.path.join('data', 'test_label_map.pbtxt')  ################\r\n\r\nNUM_CLASSES = 4  ######################\r\n\r\n\r\n# ## Download Model\r\n\r\n\r\n\r\n# opener = urllib.request.URLopener()\r\n# opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\r\n'''\r\ntar_file = tarfile.open(MODEL_FILE)\r\nfor file in tar_file.getmembers():\r\n  file_name = os.path.basename(file.name)\r\n  if 'frozen_inference_graph.pb' in file_name:\r\n    tar_file.extract(file, os.getcwd())\r\n'''\r\n\r\n# ## Load a (frozen) Tensorflow model into memory.\r\n\r\n\r\n\r\ndetection_graph = tf.Graph()\r\nwith detection_graph.as_default():\r\n  od_graph_def = tf.GraphDef()\r\n  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\r\n    serialized_graph = fid.read()\r\n    od_graph_def.ParseFromString(serialized_graph)\r\n    tf.import_graph_def(od_graph_def, name='')\r\n\r\n\r\n# ## Loading label map\r\n# Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine\r\n\r\n\r\n\r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\n\r\n\r\n\r\ndef load_image_into_numpy_array(image):\r\n  (im_width, im_height) = image.size\r\n  return np.array(image.getdata()).reshape(\r\n      (im_height, im_width, 3)).astype(np.uint8)\r\n\r\n\r\n# # Detection\r\n\r\n\r\n\r\n# For the sake of simplicity we will use only 2 images:\r\n# image1.jpg\r\n# image2.jpg\r\n# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\r\nPATH_TO_TEST_IMAGES_DIR = 'C:/Users/\u745e/Documents/GitHub/models/research/object_detection/defects_test_images'    ####################\r\n#TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, '{}.jpg'.format(i)) for i in range(1, 11) ]\r\n##########################\r\nos.chdir(PATH_TO_TEST_IMAGES_DIR)\r\nTEST_IMAGE_PATHS = os.listdir(PATH_TO_TEST_IMAGES_DIR)\r\n# Size, in inches, of the output images.\r\nIMAGE_SIZE = (12, 8)\r\n\r\n\r\ndef run_inference_for_single_image(image, graph):\r\n  with graph.as_default():\r\n    with tf.Session() as sess:\r\n      # Get handles to input and output tensors\r\n      ops = tf.get_default_graph().get_operations()\r\n      all_tensor_names = {output.name for op in ops for output in op.outputs}\r\n      tensor_dict = {}\r\n      for key in [\r\n          'num_detections', 'detection_boxes', 'detection_scores',\r\n          'detection_classes', 'detection_masks'\r\n      ]:\r\n        tensor_name = key + ':0'\r\n        if tensor_name in all_tensor_names:\r\n          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\r\n              tensor_name)\r\n      if 'detection_masks' in tensor_dict:\r\n        # The following processing is only for single image\r\n        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\r\n        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\r\n        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\r\n        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\r\n        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\r\n        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\r\n        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\r\n            detection_masks, detection_boxes, image.shape[0], image.shape[1])\r\n        detection_masks_reframed = tf.cast(\r\n            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\r\n        # Follow the convention by adding back the batch dimension\r\n        tensor_dict['detection_masks'] = tf.expand_dims(\r\n            detection_masks_reframed, 0)\r\n      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\r\n\r\n      # Run inference\r\n      output_dict = sess.run(tensor_dict,\r\n                             feed_dict={image_tensor: np.expand_dims(image, 0)})\r\n\r\n      # all outputs are float32 numpy arrays, so convert types as appropriate\r\n      output_dict['num_detections'] = int(output_dict['num_detections'][0])\r\n      output_dict['detection_classes'] = output_dict[\r\n          'detection_classes'][0].astype(np.uint8)\r\n      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\r\n      output_dict['detection_scores'] = output_dict['detection_scores'][0]\r\n      if 'detection_masks' in output_dict:\r\n        output_dict['detection_masks'] = output_dict['detection_masks'][0]\r\n  return output_dict\r\n\r\nnum=20\r\nfor image_path in TEST_IMAGE_PATHS:\r\n  image = Image.open(image_path)\r\n  # the array based representation of the image will be used later in order to prepare the\r\n  # result image with boxes and labels on it.\r\n  image_np = load_image_into_numpy_array(image)\r\n  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\r\n  image_np_expanded = np.expand_dims(image_np, axis=0)\r\n  # Actual detection.\r\n  output_dict = run_inference_for_single_image(image_np, detection_graph)\r\n  # Visualization of the results of a detection.\r\n  vis_util.visualize_boxes_and_labels_on_image_array(\r\n      image_np,\r\n      output_dict['detection_boxes'],\r\n      output_dict['detection_classes'],\r\n      output_dict['detection_scores'],\r\n      category_index,\r\n      instance_masks=output_dict.get('detection_masks'),\r\n      use_normalized_coordinates=True,\r\n      line_thickness=8)\r\n  plt.imshow(image_np)\r\n  display(Image.fromarray(image_np))\r\n  aaa=Image.fromarray(image_np)\r\n  aaa.save('test00%d.jpg'%num)\r\n  num=num+1\r\n```", "@A12-RUI,\r\nIs there any specific reason you are using TF v1.12? \r\nCould you please upgrade TensorFlow to v1.15 or v2.2 and let us know if you are facing the same issue. Thanks!", "I have solve the problem \uff0cthanks\uff01"]}, {"number": 39500, "title": "Fix typo preventing compilation", "body": "When trying to build the `person_detection` example for ESP32, I got an error message:\r\n\r\n    $ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_person_detection_esp_project\r\n    $ cd tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection/esp-idf\r\n    $ git clone https://github.com/espressif/esp32-camera.git components/esp32-camera\r\n    $ idf.py build\r\n    [\u2026]\r\n    ../main/esp/app_camera_esp.h  :46:27: error: 'FRAMESIZE_96x96' undeclared (first use in this function); did you mean 'FRAMESIZE_96X96'?\r\n     #define CAMERA_FRAME_SIZE FRAMESIZE_96x96\r\n                               ^~~~~~~~~~~~~~~\r\n    [\u2026]\r\n\r\nThis commit fixes the issue.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39500) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39500) for more info**.\n\n<!-- ok -->", "Hi fredrec@, can you take a look at this PR?"]}, {"number": 39499, "title": "Error after running the program", "body": "I ran this program to check if  Tensorflow is installed or not in jupyter notebook. And Got the following error.\r\n\r\nimport tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))\r\n\r\n\r\n\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-8-25b92e4d5dec>\", line 2, in <module>\r\n    hello = tf.constant('Hello, TensorFlow!')\r\nAttributeError: module 'tensorflow' has no attribute 'constant'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'AttributeError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 2453, in <module>\r\n    from tensorflow.python.util import deprecation\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 25, in <module>\r\n    from tensorflow.python.platform import tf_logging as logging\r\nImportError: cannot import name 'tf_logging' from 'tensorflow.python.platform' (C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow\\python\\platform\\__init__.py)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\inspect.py\", line 1502, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\inspect.py\", line 1460, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n    from . _api.v2 import audio\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-8-25b92e4d5dec>\", line 2, in <module>\r\n    hello = tf.constant('Hello, TensorFlow!')\r\nAttributeError: module 'tensorflow' has no attribute 'constant'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'AttributeError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 2453, in <module>\r\n    from tensorflow.python.util import deprecation\r\n  File \"C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 25, in <module>\r\n    from tensorflow.python.platform import tf_logging as logging\r\nImportError: cannot import name 'tf_logging' from 'tensorflow.python.platform' (C:\\Users\\Vishal\\Anaconda3\\New\\lib\\site-packages\\tensorflow\\python\\platform\\__init__.py)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "comments": ["@Monsieurvishal \r\n\r\nPlease refer to [link](https://stackoverflow.com/questions/51585095/module-tensorflow-has-no-attribute-constant), [link1](https://github.com/tensorflow/tensorflow/issues/3369#issuecomment-240217811), [link2](https://www.edureka.co/community/67232/attributeerror-module-tensorflow-has-attribute-constant), [link3](https://python-forum.io/Thread-Attribute-Error-while-using-tensor-flow)\r\n#36167 (comment) [link4](https://www.itread01.com/content/1543048209.html) and let us know if this helps resolve yoir issue.\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\n\r\n\r\nAlso:\r\nWorkaround:\r\n1.)Uninstall tensorflow from pip and pip3\r\nsudo pip uninstall tensorflow\r\nsudo pip3 uninstall tensorflow\r\n\r\n2.)Uninstall python & python3 \r\nsudo apt-get remove python-dev python3-dev python-pip python3-pip\r\n\r\n3.)Install only a single version of  python(I used python 3)\r\nsudo apt-get install python3-dev python3-pip\r\n\r\n4.)Install tensorflow to python3\r\nsudo pip3 install --upgrade pip\r\n\r\nfor non GPU tensorflow, run this command \r\n\r\nsudo pip3 install --upgrade tensorflow\r\n\r\nfor GPU tensorflow, run below command\r\nsudo pip3 install --upgrade tensorflow-gpu\r\n", "what is the version of tensor flow you are trying to work with", "Today I downloaded latest version of TF , and I downgraded it to version 2  to check if problem persists", "Its very much confusing to install TF in my machine. Does Google collab has all feature so that i can learn  TF in Google collab?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39499\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39499\">No</a>\n"]}, {"number": 39497, "title": "tf.keras.layers.RNN object within tf.keras.Model not automatically built after calling when `go_backward` not set to `True`", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Docker (tensorflow/tensorflow:2.2.0-gpu)\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: as shipped with official docker image (tensorflow/tensorflow:2.2.0-gpu)\r\n- GPU model and memory: Nvidia GTX 1050, 4GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.keras.layers.RNN` objects defined within tf.keras.Model are not automatically built after calling the model with input tensors **if argument `go_backward` is not set to `True`**. \r\n**Describe the expected behavior**\r\nthe `built` flag should be true for tf.keras.layers.RNN objects within `tf.keras.Model` object should be flaged as built after calling the model object with tensor.\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n# from crf import CRF\r\n\r\nclass CrfModel(tf.keras.Model):\r\n    def __init__(self, *args, **kwargs):\r\n        super(CrfModel, self).__init__(*args, **kwargs)\r\n\r\n\r\nclass BiRNNCrf(CrfModel):\r\n    def __init__(self, vocab_size, embedding_dim, cell_creator, num_tags, *args, **kwargs):\r\n        super().__init__(self, *args, **kwargs)\r\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\r\n        self.forward_cell = cell_creator()\r\n        self.forward_rnn_layer = tf.keras.layers.RNN(self.forward_cell, return_sequences=True, return_state=False)\r\n        self.backward_cell = cell_creator()\r\n        self.backward_rnn_layer = tf.keras.layers.RNN(self.backward_cell, return_sequences=True, return_state=False, go_backwards=True)\r\n        self.bi_rnn_layer = tf.keras.layers.Bidirectional(self.forward_rnn_layer, backward_layer=self.backward_rnn_layer)\r\n        self.fc = tf.keras.layers.Dense(num_tags)\r\n        # self.crf = CRF(num_tags)\r\n\r\n    def lookup(self, inputs):\r\n        out = self.embedding(inputs)\r\n        return out\r\n    \r\n    def birnn(self, inputs, sequence_mask):\r\n        out = self.bi_rnn_layer(inputs, mask=sequence_mask)\r\n        return out\r\n\r\n    # @tf.function(input_signature=[tf.TensorSpec([None, None, None], dtype=tf.float32, name=\"input_ids\"), tf.TensorSpec([None], dtype=tf.int32, name=\"sequence_length\")])\r\n    def call(self, inputs, sequence_length):\r\n        out = self.lookup(inputs)\r\n        mask = tf.sequence_mask(sequence_length, maxlen=tf.shape(inputs)[1])\r\n        out = self.birnn(out, mask)\r\n        out = self.fc(out)\r\n        # out = self.crf(out, sequence_length)\r\n        return out\r\n\r\ndef cell_creator():\r\n    return tf.keras.layers.LSTMCell(300)\r\n\r\n\"\"\"\r\ndef build(self, input_shape, *args, **kwargs):\r\n    super(self, tf.keras.Model).build(input_shape, *args, **kwargs)\r\n    self.crf.build(input_shape)\r\n\"\"\"\r\n\r\nif __name__ == \"__main__\":\r\n    nn = BiRNNCrf(1000, 300, cell_creator, 3)\r\n    assert isinstance(nn, tf.keras.Model)\r\n    # nn.build(input_shape=[tf.TensorShape([None,None,3]), tf.TensorShape([None])]) # Not allowed by tensorflow\r\n    nn(inputs=tf.constant([[1, 0, 0], [1, 1, 1]]), sequence_length=tf.constant([1,3]))\r\n    print(nn.forward_rnn_layer.built) # False\r\n    print(nn.backward_rnn_layer.built) # True\r\n\r\n    nn.summary() # Fails\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nLog from Tensorflow indicating that the `forward_rnn_layer` is not automatically built\r\n```\r\nValueError: You tried to call `count_params` on lstm_cell, but the layer isn't built. You can build it manually via: `lstm_cell.build(batch_input_shape)`.\r\n```", "comments": ["Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/2cc1f01bc42df21f3ef2cb3e76529354/39497.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/941caaa1c2abd3f6f4e356a396cda012/39497-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Thanks for reporting the issue.\r\n\r\nI think the root cause is that we clone for the forward layer and keep it in the bidirectional wrapper as a standalone instance. This is why the forward_layer.built is not propagate to the self. forward_rnn_layer. Note that model.summary() might print out duplicated weights information since weights will be double reported from self. bi_rnn_layer and forward/backward layers and cells.\r\n\r\nThe fix for this issue to not assigning the forward/backward layer/cell as the attribute for the model. \r\n\r\n```\r\n  def __init__(self, vocab_size, embedding_dim, cell_creator, num_tags, *args, **kwargs):\r\n    super().__init__(self, *args, **kwargs)\r\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\r\n    forward_rnn_layer = tf.keras.layers.RNN(cell_creator(), return_sequences=True, return_state=False)\r\n    backward_rnn_layer = tf.keras.layers.RNN(cell_creator(), return_sequences=True, return_state=False,\r\n                                                  go_backwards=True)\r\n    self.bi_rnn_layer = tf.keras.layers.Bidirectional(forward_rnn_layer, backward_layer=backward_rnn_layer)\r\n    self.fc = tf.keras.layers.Dense(num_tags)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39497\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39497\">No</a>\n", "> Thanks for reporting the issue.\r\n> \r\n> I think the root cause is that we clone for the forward layer and keep it in the bidirectional wrapper as a standalone instance. This is why the forward_layer.built is not propagate to the self. forward_rnn_layer. Note that model.summary() might print out duplicated weights information since weights will be double reported from self. bi_rnn_layer and forward/backward layers and cells.\r\n> \r\n> The fix for this issue to not assigning the forward/backward layer/cell as the attribute for the model.\r\n> \r\n> ```\r\n>   def __init__(self, vocab_size, embedding_dim, cell_creator, num_tags, *args, **kwargs):\r\n>     super().__init__(self, *args, **kwargs)\r\n>     self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\r\n>     forward_rnn_layer = tf.keras.layers.RNN(cell_creator(), return_sequences=True, return_state=False)\r\n>     backward_rnn_layer = tf.keras.layers.RNN(cell_creator(), return_sequences=True, return_state=False,\r\n>                                                   go_backwards=True)\r\n>     self.bi_rnn_layer = tf.keras.layers.Bidirectional(forward_rnn_layer, backward_layer=backward_rnn_layer)\r\n>     self.fc = tf.keras.layers.Dense(num_tags)\r\n> ```\r\n\r\nIt works with this solution. The \"side effect\" here is that unlike Tensorflow 2.1 where the original implementation works, number of weights in forward/backward rnn layers that `summary()` prints out will be aggregated and represented as a single \"Bidirectional\" layer in 2.2. Didn't know if this is the desired behavior here. "]}, {"number": 39496, "title": "Bug caused by creating custom layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS but it should work with Ubuntu as well.\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.6\r\n- GPU model and memory: CPU\r\n\r\nI found a bug related to creating custom layer in TF. Here is a short code to replicate::\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\nclass CustomModel(tf.keras.layers.Layer):\r\n    #this class is for source sequence\r\n    def __init__(self,):\r\n        super(CustomModel, self).__init__()\r\n\r\n    def build(self, input_shape):\r\n        self.kernel = self.add_weight(shape=(32, 512, 512),\r\n                                      initializer=tf.keras.initializers.glorot_uniform(seed=1),\r\n                                      trainable=True) #dumb\r\n    def call(self, inputs):\r\n        return inputs[0]\r\n\r\ndef main():\r\n    def create_model(source_vocab, target_vocab, relationship_vocab):\r\n        source = tf.keras.layers.Input(dtype='int32', shape=(64,), name='source')\r\n        target = tf.keras.layers.Input(dtype='int32', shape=(64,), name='target')\r\n        relationship = tf.keras.layers.Input(dtype='int32', shape=(1,), name='relationship')\r\n        embedding_source = tf.keras.layers.Embedding(512, 512, input_length=64)(source)\r\n        embedding_target = tf.keras.layers.Embedding(512, 512, input_length=64)(target)\r\n        final_layer = CustomModel()([relationship, embedding_source, embedding_target])\r\n        model = tf.keras.models.Model(inputs=[source, target, relationship], outputs=final_layer)\r\n        return model\r\n    model = create_model(1000, 1000, 500)\r\n    print(model.summary())\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\nI believe this is a bug so I posted it here so that you can find and fix it.\r\nMeanwhile one workaround I found is to change the order of parameters. Below you can find the solution:\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\nclass CustomModel(tf.keras.layers.Layer):\r\n    #this class is for source sequence\r\n    def __init__(self,):\r\n        super(CustomModel, self).__init__()\r\n\r\n    def build(self, input_shape):\r\n        self.kernel = self.add_weight(shape=(32, 512, 512),\r\n                                      initializer=tf.keras.initializers.glorot_uniform(seed=1),\r\n                                      trainable=True) #dumb\r\n    def call(self, inputs):\r\n        return inputs[0]\r\n\r\ndef main():\r\n    def create_model(source_vocab, target_vocab, relationship_vocab):\r\n        source = tf.keras.layers.Input(dtype='int32', shape=(64,), name='source')\r\n        target = tf.keras.layers.Input(dtype='int32', shape=(64,), name='target')\r\n        relationship = tf.keras.layers.Input(dtype='int32', shape=(1,), name='relationship')\r\n        embedding_source = tf.keras.layers.Embedding(512, 512, input_length=64)(source)\r\n        embedding_target = tf.keras.layers.Embedding(512, 512, input_length=64)(target)\r\n        final_layer = CustomModel()([embedding_source, embedding_target, relationship])\r\n        model = tf.keras.models.Model(inputs=[source, target, relationship], outputs=final_layer)\r\n        return model\r\n    model = create_model(1000, 1000, 500)\r\n    print(model.summary())\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\n\r\n\r\n\r\n\r\n", "comments": ["@hoangcuong2011 \r\nI ran the code shared by you, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/7a7550923f0b89778291f17d2ac356b1/untitled181.ipynb).\r\nCould you tell us what i the error as we can see an output with out any error as per the gist.\r\nI ran both the codes shared and see the same output.", "Is there any particular reason for using an older version of tf, could you please upgrade to a later version.", "@Saduf2019: hello - what you run was with TF 2.1. It seems works on this version indeed.\r\nNonetheless it does not work for TF 1.15. (see the [gist](https://colab.research.google.com/drive/1h5eG4tqmewTFdVA_WZrkwMOPrQ3oJIdx?usp=sharing)).\r\nI understand this bug is only with TF 1.15 so it is with a low priority from your side to fix (perhaps you will leave it as it is). I just wanted to raise your attention on this. I myself still use this version simply because some code I wrote could not run with TF 2.1. Nonetheless I know that it is just a matter of time and I need to move to TF. 2x one day, though.\r\n", "@hoangcuong2011 \r\nWe are not going to have any bug fixes on 1.15, please upgrade to 2.x.", "Fixed in 2.x, masking this as closed", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39496\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39496\">No</a>\n"]}, {"number": 39495, "title": "Bug when using TensorFlow Lite Object Detection Android Demo for a custom model", "body": "Hi, I'm using the [TensorFlow Lite Object Detection Android Demo](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) for my object detection model on Pixel 4.\r\nThe demo is all right and operating normally on Pixel 4, but when I used other custom model such as [ssd_mobilenet_v2_mnasfpn_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync_2020_05_06.tar.gz) and [ssd_mobilenet_v3_large_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_large_coco_2020_01_14.tar.gz), the app will flash and quit, sometimes report error:\r\n![Screenshot_20200513-162420](https://user-images.githubusercontent.com/47172283/81790612-351cab80-9538-11ea-91d1-e1b904f5faa5.png)\r\n\r\nCan anyone help about this error?\r\n", "comments": ["@Ringhu, thanks for reporting this. We will take a look. @lintian06 \r\nMeanwhile,  maybe you can also check more detailed crash logs, to see which part has issues. Welcome PR for fixing :-)", "As this is a custom model from [object detection](https://github.com/tensorflow/models/tree/master/research/object_detection) research models , inputs and outputs might be slightly different from case to case. They need to be aligned with those [inputs and outputs](https://github.com/tensorflow/examples/blob/4d3a5d2b3a8fa9eeba9789544b10aaa1db3b71ac/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java#L181-L191) in the Android App.\r\n\r\nI would suggest check inputs and outputs, especially focusing on the size and data type. In the [code sample](https://www.tensorflow.org/lite/guide/inference), you can check the input and output by using `input_details` and `output_details` in our python API.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi @lintian06 , thank you for ur response first. I checked the input and ouput details:\r\n```\r\n[{'name': 'normalized_input_image_tensor', 'index': 260, 'shape': array([  1, 300, 300,   3], dtype=int32), 'shape_signature': array([  1, 300, 300,   3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0078125, 128), 'quantization_parameters': {'scales': array([0.0078125], dtype=float32), 'zero_points': array([128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n>>> output_details = interpreter.get_output_details()\r\n>>> print(output_details)\r\n[{'name': 'TFLite_Detection_PostProcess', 'index': 252, 'shape': array([  1, 100,   4], dtype=int32), 'shape_signature': array([  1, 100,   4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:1', 'index': 253, 'shape': array([  1, 100], dtype=int32), 'shape_signature': array([  1, 100], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:2', 'index': 254, 'shape': array([  1, 100], dtype=int32), 'shape_signature': array([  1, 100], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'TFLite_Detection_PostProcess:3', 'index': 255, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n```\r\nI modified the `TFLiteObjectDetectionAPIModel.java` file, but still got the crash problem.\r\n\r\nBesides, the demo file supports 80 classes in COCO, since the latest version of COCO contains 91 classes, I suggest update the demo repo if it's possible.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39495\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39495\">No</a>\n", "Do we have solution to this issue? I am facing this issue as well.", "I get the exception \"Detector could not be initialized\" when I try to use a .tflite file that I created from MobileNet SSD V2 from the Tensorflow Object Detection API (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)"]}, {"number": 39494, "title": "Distributed training and efficient data input pipeline", "body": "Hi, I'm using tensorflow 2.0 in 4 gpus machine, my tensorflow code using keras api, like follows:\r\n```\r\nmodel = CNN_model()\r\nmodel.compile(optimizer='adam', loss='mse')\r\nmodel.fit()\r\n```\r\nand my data pipeline is user-defined generator, like follows:\r\n```\r\ndef train_data():\r\n    while True:\r\n        for i in shuffle(train_id):\r\n            df = pd.read_pickle(df_train_val['file_name'].iloc[i],\r\n                                compression='gzip')\r\n            feature = df.to_numpy().reshape(df.shape[0], -1)\r\n            yield feature.reshape(-1, 56),feature.reshape(-1, 56)\r\ngn = tf.data.Dataset.from_generator(train_data, output_types=(tf.float32, tf.float32),\r\n                                          output_shapes=((None, 56),(None, 56))\r\n                                         ).prefetch(tf.data.experimental.AUTOTUNE)\r\n```\r\ntrain_id is a file_index\uff0cI have more than 200K files in local machine, so in this case one batch is a file, my problem is the gpu util is very low, about 15%\uff0cI tried tf.distribute.MirroredStrategy, just by \r\n```\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    model = CNN_model()\r\n    model.compile(optimizer='adam', loss='mse')\r\nmodel.fit()\r\n```\r\nbut, it didn't work, and slower than one gpu, I see the gpu util is lowed.\r\nso, I have 3 questions: how can I make the input from_generator pipeline more efficient and let the gpu util increase or speed up the training process? why prefetch and tf.distribute.MirroredStrategy not work? is there somewhere for me to study these?", "comments": ["Did you try scaling up your batch sizes appropriately when using distributed training? Not increasing your batch sizes when running multiple GPUs can be a major bottleneck in maximizing GPU usage. \r\n\r\nI am also not sure what kind of data you are trying to train here, but it may be a good idea to perform  some preprocessing via ```pd.read_pickle``` and the relevant functions before trying to create a ```Dataset``` of any kind. After all, the fastest operations come from using purely TensorFlow functions when trying to preprocess and churn out the inputs to the model.", "@iobtl  for generator Dataset, the batch is 1, I use 4 gpus, so batch is 4, data is very common float value of 56 features, and each batch is from a pickle file because all data can't be in memory at one time", "@Anhaoxu is your data in the form of images? or other forms? \r\nHave you considered serializing your data in multiple TFRecord files instead? Maybe try to store more than one training instance per file. Following from there you can interleave ```tf.data.TFRecordDataset``` to create a ```Dataset``` object and perform batching from there. ", "@Anhaoxu,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39494\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39494\">No</a>\n"]}, {"number": 39493, "title": "I try to install tensoflow version 1.5 and it does not happen!", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n**- TensorFlow version: 1.5**\r\n- Python version:\r\n**- Installed using virtualenv? pip? conda?: pip**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**I try to install TensorFlow version 1.5. I need it for my Mask R-CNN model. I work in Kaggle notebook. Until two days ago this problem did not exist.**\r\n\r\n**This is what I do. It is the first thing I do before continuing with my project:\r\n```\r\n!pip install tensorflow==1.5 #need that because of Mask-RCNN version\r\n!pip install keras==2.1.5\r\n\r\nimport tensorflow\r\nprint(tensorflow.__version__)\r\nimport keras\r\nprint(keras.__version__)\r\n```\r\n**\r\n\r\n\r\n**This is the error it shows:\r\n_**WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f082d25d978>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/tensorflow/\r\nWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f082d25d198>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/tensorflow/\r\nWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f082d25d3c8>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/tensorflow/\r\nWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f082d25d5c0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/tensorflow/\r\nWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f082d28c390>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/tensorflow/\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.5 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==1.5\r\nWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fa3061aae80>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/keras/\r\nWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fa3061dd5c0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/keras/\r\nWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fa3061dd630>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/keras/\r\nWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fa3061dd550>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/keras/\r\nWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fa3061dd400>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/keras/\r\nERROR: Could not find a version that satisfies the requirement keras==2.1.5 (from versions: none)\r\nERROR: No matching distribution found for keras==2.1.5**_\r\n**\r\n\r\n", "comments": ["@YanaSSS \r\nIs there any particular reason to use old version of tensorflow, could you please use later version and let us know if the issue resolves.\r\nplease confirm if you used [link](https://www.tensorflow.org/install/pip) to install.\r\nPlease refer to this [link](https://github.com/tensorflow/tensorflow/issues/26182#issuecomment-467986463), [link1](https://www.reddit.com/r/tensorflow/comments/7sut99/how_to_install_tensorflow_150_using_official_pip/)", "Yes. The reason is that I use [Mask RCNN model](https://github.com/matterport/Mask_RCNN) for my project and it does not work with the latest TensorFlow version. Until two days ago I was able to install TensorFlow 1.5 and it worked just fine. I was able to train my model. \r\nSince yesterday though I cannot install it and I cannot run my model and work on my project. I have no idea why this happened since I have not changed my code at all.  ", "I solved the issue. It was in the Kaggle settings, not in the installation.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39493\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39493\">No</a>\n"]}, {"number": 39492, "title": "How to invoke Flex delegate for FlexIdentityN node on Raspberry Pi", "body": "**System information**\r\n- Google Colab and Raspberry Pi 4\r\n\r\n\r\nFlexIdentityN Tensorflow Lite Custom Ops Error\r\n\r\nHello,\r\n\r\nFor our project we tried to build an Image Classification Model using a CNN to detect 4 categories of waste. The link to the full model code is shared below.\r\n\r\nhttps://colab.research.google.com/drive/1aAaQ7kq2BDYRlspmM-N2Uuh6CZkQGRNb?usp=sharing\r\n\r\nWe used Tensorflow 1.15 instead of the latest Tensorflow 2.2 because I couldn\u2019t get the validation split command in model.fit to work on Tensorflow 2 (Any explanation for this problem will be appreciated too). Looking at newer tutorials now, I believe we have to split the data into training and testing data, instead of using validation split, correct? \r\n\r\nThe model needs to be deployed on a Raspberry Pi 4 module. To do that whilst converting the model file to a .tflite file to run on the Raspberry Pi, we encountered this problem.\r\n\r\nErrors\r\n\r\n(1.\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.\r\nTraceback (most recent call last):\r\nFile \"/tensorflow-1.15.2/python3.6/bin/toco_from_protos\", line 8, in\r\nsys.exit(main())\r\nFile \"/tensorflow-1.15.2/python3.6/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\r\napp.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\nFile \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/platform/app.py\", line 40, in run\r\n_run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\nFile \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n_run_main(main, args)\r\nFile \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\nsys.exit(main(argv))\r\nFile \"/tensorflow-1.15.2/python3.6/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\r\nenable_mlir_converter)\r\n\r\n2.\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.)\r\n\r\n![FlexIdentityN seen in Netron](https://user-images.githubusercontent.com/62724378/81785708-3b049380-951c-11ea-9570-c31dece7aa6d.png)\r\n\r\n\r\nAs of now, I have no explanation as to how the FlexIdentityN Layer is formed and why. Any explanation on that will be appreciated as well.\r\n\r\nThese Errors can be solved(?) Using the following code.\r\n```\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                         tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\n\r\n\r\nHowever the same IdentityN Error pops up on the Raspberry Pi 4 when running the code also shown in the Google Colab file. The Error on the Raspberry Pi is shown below.\r\n\r\n![FlexIdentityN Error](https://user-images.githubusercontent.com/62724378/81785756-4fe12700-951c-11ea-8f55-9c2194daf41c.png)\r\n\r\n\r\nThe link to the full Raspberry Pi Code is below.\r\n\r\nhttps://colab.research.google.com/drive/1ptbKJ5CmQDjnTChzEIJ-5uyjYaiNYWqg?usp=sharing\r\n\r\nI\u2019ve followed this tutorial on YouTube by Edje Electronics to install Tensorflow on the Raspberry Pi.\r\n\r\nhttps://www.youtube.com/watch?v=aimSGOAUI8Y\r\n\r\nThe Tutorial automatically installs the latest version in a virtual environment. (Maybe the difference in tensorflow version could be a problem)\r\n\r\nThank You.\r\n", "comments": ["Hi Keegan007,\r\nSince TFLite does not have IdentityN op, it is formed as FlexIdentityN to use the Tensorflow implementation.\r\nHow are you running your model on Raspberry?\r\nThere are some guide about this here: https://www.tensorflow.org/lite/guide/ops_select", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39492\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39492\">No</a>\n"]}]