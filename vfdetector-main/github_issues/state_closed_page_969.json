[{"number": 24353, "title": "TFTRT: Make segmenter deterministic", "body": "Due to using pointers as key for std::set, TFTRT's segmenter was not deterministic. This led to problems (specifically with InceptionV4) in INT8 mode because the graph is created multiple times and the mismatch led to accuracy issues.", "comments": []}, {"number": 24352, "title": "[Intel MKL]: Fixing Build Regression + clang-format changes", "body": "", "comments": ["`MacOS Contrib`, `MacOS Python2 and CC`, `Windows Bazel`, and `Windows Bazel GPU` are known failures. As these four are the only failing/pending tests, I'm going to tag this ready-to-pull."]}, {"number": 24351, "title": "RAM-efficient shuffling in dataset", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12.0\r\n- Are you willing to contribute it (Yes/No): Maybe, if I get some direction\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently to shuffle datasets, `shuffle()` needs to read `shuffle_buffer` worth of records into memory. If the records are ~large and the dataset contains ordered/consequitve/correlated records then the user may need to pre-shuffle and/or split the data and/or commmit a good chunk of ram for shuffling the records sufficiently to avoid overfitting to features present in the particular part of the dataset.\r\n\r\nIt would be possible to trade off reading the dataset just once in order to shuffle more efficiently in terms of memory usage. For example, the ram-efficient dataset shuffler could read the dataset multiple times starting from offsets `0 .. N` and skip over `N` records on each pass. \r\n\r\n**Will this change the current api? How?**\r\n\r\nIt would add a new method to `tf.data.Dataset`, or maybe an option/experiment for `shuffle()`.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyne training on sequential (I guess correleated) datasets where individual records are on large enough to become a limiting factor for large `shuffle_buffer` sizes.", "comments": ["IIUC, your proposal relies on the existence of an efficient implementation of skip / seek for the dataset to be shuffled. Unfortunately, not all dataset types meet this requirement. For instance, `TFRecordDataset` -- commonly used for storing data at rest -- does not support efficient skip/ seek and to seek to an offset, you need to sequentially read all the data up to that offset.\r\n\r\nThe good news is that you could achieve RAM-efficient shuffling using existing tf.data API:\r\n\r\n```\r\n# `dataset` stores the dataset to shuffle\r\ndataset = dataset.window(N).flat_map(lambda x: x.shuffle(M))\r\n```\r\n\r\nThis will partition the dataset into windows of size `N` and shuffle each of them using buffer of size `M`.\r\n"]}, {"number": 24350, "title": "[XLA:GPU] Add sm_75 to LLVM supported procs list", "body": "This is required to prevent fallback to the default sm version when running on GPUs with the Turing architecture.\r\n\r\nAttn @jlebar \r\n\r\nI only noticed this because I got the below error. It also suggests that sm_30 is not actually supported by XLA and should be dropped (especially as the default). TF itself also only supports >= sm_35. I can add that to this PR if you think that's a good idea.\r\n\r\n```\r\n2018-12-13 15:04:11.447530: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/nvptx_backend_lib.cc:135] Unknown compute capability (7, 5) .Defaulting to telling LLVM that we're compiling for sm_30\r\n2018-12-13 15:04:11.854629: I tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:535] ptxas /tmp/tempfile-astrobox-7d7fa700-29182-57cef552f0d6f, line 1059; error   : Instruction 'atom.{min,max}.{s64,u64}' requires .target sm_32 or higher\r\n2018-12-13 15:04:11.854697: I tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:535] ptxas fatal   : Ptx assembly aborted due to errors \r\n```", "comments": ["@d0k is OOO for the next week, so I can take this.", "> I only noticed this because I got the below error. It also suggests that sm_30 is not actually supported by XLA and should be dropped (especially as the default). \r\n\r\nAgree, sounds like we should bump that to sm_35.  Why don't we do it in a separate patch in case we have to revert one of them for some reason?"]}, {"number": 24349, "title": "Add the eager coverage for some experimental dataset kernels", "body": "This PR add the eager coverage for `MakeTFRecordDatasetTest` and `MapAndBatchTest`.", "comments": ["@shivaniag could you please take a look ... I believe this PR might be duplicate of some of your recent changes", "@shivaniag (cc'ed @jsimsa ) I just saw your comments [here](https://github.com/tensorflow/tensorflow/pull/24207#issuecomment-447129506). Feel free to close it if it is duplicated with your merging PR. \r\n\r\nThere is another related PR here: https://github.com/tensorflow/tensorflow/pull/24330.", "@feihugis thank you for the contribution ... to avoid duplicating efforts in the future, could you please keep me in the loop on what tf.data TODOs are you working on? if you are interested, we might also be able to suggest some things for you to work on", "Thank you very much, @jsimsa! It will be perfect if you could suggest some things for me to work on. In the future, I will also create an issue to check if there is any duplicated work before I start to work on a PR. ", "Here is a suggestion for a larger project. Let me know if you are interested:\r\n\r\nThe goal would be to develop C++ infrastructure for testing tf.data kernel implementations in C++. Currently, the C++ tf.data kernels are only tested through Python bindings which are not fine-grained enough and some public C++ APIs are not always tested.\r\n\r\nThe flow of testing the C++ API would be:\r\n- create an instance of the dataset op\r\n- invoke the `Compute` method which takes input arguments and produces `DatasetBase` object wrapped in a variant\r\n- test the public API of the `DatasetBase` object, including the `MakeIterator` method which produces `IteratorBase` object\r\n- test the public API of the `IteratorBase` object\r\n", "@jsimsa Yeah, I am very interested in this project! Do you have any suggestion about how to start the work? May I submit a PR once I finish an initial version? Then we can discuss more details based on the PR. Or, you already have something that I can continue to work on?  ", "There is currently no code for this. I suggest you create a fork of the tensorflow repo and start building a prototype for which can create a PRs targeting your fork. That way we can iterate on your code through code reviews more often.\r\n\r\nOnce the prototype is working, you would create a PR to the tensorflow repo.", "@jsimsa Sounds good! Will do it and keep you updated. ", "Nagging Reviewer @jsimsa, @shivaniag: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Stale PR, closing this."]}, {"number": 24348, "title": "[Go]: Fix broken generated of wrappers.go", "body": "I believe this PR does what #24287 set out to do.\r\n\r\n@Xib1uvXi\r\n", "comments": []}, {"number": 24347, "title": ".numpy() was missing in code example: tf.add(1, 2).numpy()", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I have signed the CLA", "Accidentally closed the pull request; reopening it.", "CLAs look good, thanks!\n\n<!-- ok -->", "@stevelang789  Could you please sign the CLA in order to help this PR to get merged. If you are facing any difficulties, please close this PR and open a new one. ", "Closing this PR and opening a new one as per @hgadig's suggestion.", "> Closing this PR and opening a new one as per @hgadig's suggestion.\r\n\r\nThank you. Please make sure you sign CLA"]}, {"number": 24346, "title": "`import tensorflow` fails after `import pandas`; RUNPATH seemingly ignored", "body": "### System information\r\n| Category | Info |\r\n| --- | --- |\r\n| Have I written custom code? | no |\r\n| OS/distro | Linux, RHEL 7.4 |\r\n| TF installed from | source |\r\n| TF version | 1.9.0 |\r\n| Python version | 3.6.5 |\r\n| Bazel version | 0.14.1 |\r\n| GCC version | 7.3.0 |\r\n| CUDA/cuDNN version | 9.2.88 and 7.1.4 |\r\n| GPU model/memory | 16 GB |\r\n| Command to reporoduce | `import pandas; import tensorflow` |\r\n\r\n### Problem description\r\n\r\nThe successful import of pandas and TensorFlow depends on the order in which they are loaded:\r\n\r\n```python\r\n# Works\r\nimport tensorflow\r\nimport pandas\r\n```\r\n\r\n```python\r\n# Fails\r\nimport pandas\r\nimport tensorflow\r\n```\r\n\r\nThe full error message is below, but here's an excerpt:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n...\r\n  File \"/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n```\r\n\r\nWhat's confusing for me: `_pywrap_tensorflow_internal.so`'s RUNPATH contains `/apps/gcc/7.3.0/lib64`, which has the version of `libstdc++.so.6` that should be (and normally is) used, and LD_LIBRARY_PATH is not set--so I'm confused as to why `/lib64/libstdc++.so.6` even enters the equation. It also seems wrong that the order of import dictates which `libstdc++.so.6` is used.\r\n\r\nJust to be sure, I temporarily changed  `_pywrap_tensorflow_internal.so`'s RUNPATH to an RPATH in case pandas was (weirdly) setting LD_LIBRARY_PATH, but it made no difference--the same error persists.\r\n\r\nReproducing might be annoying/not worth the time, but in case anyone wants to try I built Python and TensorFlow from source using GCC 7.3.0 (which was itself built from source) and installed pandas with pip. I would be just as happy with a suggestion on what to try next/where to look around, though.\r\n\r\n### Source code / logs\r\n\r\nFull error message:\r\n\r\n```\r\n>>> import pandas\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /apps/python-3.6.5-tensorflow/1.9.0/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "comments": ["This should not happen with the latest tensorflow version. We encourage users to use latest versions of tensorflow(1.11 or 1.12). Could you please try with the TF version 1.12 and see if you are running into this issue. Also please upgrade pandas with _pip install --upgrade pandas_.\r\n\r\nAlso make sure you are building TF with bazel 0.15 for latest version and 0.11 for TF v 1.9", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Sorry for the late response. I went ahead and updated pandas and installed TF 1.12 with bazel 0.15, but ran into a similar (probably related?) issue:\r\n\r\n```python\r\n>>> import pandas \r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/apps/python-3.6.5-tensorflow/1.12.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/apps/python-3.6.5-tensorflow/1.12.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/apps/python-3.6.5-tensorflow/1.12.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.22' not found (required by /apps/python-3.6.5-tensorflow/1.12.0/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/apps/python-3.6.5-tensorflow/1.12.0/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/apps/python-3.6.5-tensorflow/1.12.0/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/apps/python-3.6.5-tensorflow/1.12.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/apps/python-3.6.5-tensorflow/1.12.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/apps/python-3.6.5-tensorflow/1.12.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/apps/python-3.6.5-tensorflow/1.12.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.22' not found (required by /apps/python-3.6.5-tensorflow/1.12.0/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n```\r\n\r\nAs before, `ldd` shows that the libraries can find the right `libstdc++.so.6` rather than just using the one in `/lib64`:\r\n\r\n```console\r\n$ ldd $PFX/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so | grep libstdc++\r\n\tlibstdc++.so.6 => /apps/gcc/7.3.0/lib64/libstdc++.so.6 (0x00007fe506c34000)\r\n```", "Thanks for the report.\r\nAdding @allenlavoie @jhseu ", "Also adding @yifeif @hlopko \r\nMarcel, is it possible that when building from sources, bazel ends up using a different libstdc++ then pandas is configured to use?", "Could you try running something like:\r\n\r\n`strace python3.6 -c \"import tensorflow\" 2>&1 | grep libstdc`\r\n\r\nand \r\n\r\n`strace python3.6 -c \"import pandas; import tensorflow\" 2>&1 | grep libstdc`\r\n\r\nIt would be nice to know what ends up loading the lib64 one.", "@allenlavoie hopefully this is helpful:\r\n\r\n```console\r\n$ strace python3.6 -c \"import tensorflow\" 2>&1 | grep -B15 libstdc\r\nopen(\"/apps/gcc/7.3.0/lib64/x86_64/librt.so.1\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)\r\nstat(\"/apps/gcc/7.3.0/lib64/x86_64\", 0x7ffdf11ea050) = -1 ENOENT (No such file or directory)\r\nopen(\"/apps/gcc/7.3.0/lib64/librt.so.1\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)\r\nstat(\"/apps/gcc/7.3.0/lib64\", {st_mode=S_IFDIR|S_ISGID|0775, st_size=93, ...}) = 0\r\nopen(\"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 4\r\nfstat(4, {st_mode=S_IFREG|0644, st_size=91275, ...}) = 0\r\nmmap(NULL, 91275, PROT_READ, MAP_PRIVATE, 4, 0) = 0x7f36fd172000\r\nclose(4)                                = 0\r\nopen(\"/lib64/librt.so.1\", O_RDONLY|O_CLOEXEC) = 4\r\nread(4, \"\\177ELF\\2\\1\\1\\3\\0\\0\\0\\0\\0\\0\\0\\0\\3\\0>\\0\\1\\0\\0\\0P\\\"\\0\\0\\0\\0\\0\\0\"..., 832) = 832\r\nfstat(4, {st_mode=S_IFREG|0755, st_size=44448, ...}) = 0\r\nmmap(NULL, 2128952, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 4, 0) = 0x7f36e16c0000\r\nmprotect(0x7f36e16c7000, 2093056, PROT_NONE) = 0\r\nmmap(0x7f36e18c6000, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 4, 0x6000) = 0x7f36e18c6000\r\nclose(4)                                = 0\r\nopen(\"/zapps7/python-3.6.5-tensorflow/1.12.0/lib/python3.6/site-packages/tensorflow/python/../../_solib_k8/_U@mkl_Ulinux_S_S_Cmkl_Ulibs_Ulinux___Uexternal_Smkl_Ulinux_Slib/libstdc++.so.6\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)\r\nopen(\"/zapps7/python-3.6.5-tensorflow/1.12.0/lib/python3.6/site-packages/tensorflow/python/libstdc++.so.6\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)\r\nopen(\"/zapps7/python-3.6.5-tensorflow/1.12.0/lib/python3.6/site-packages/tensorflow/python/../libstdc++.so.6\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)\r\nopen(\"/apps/gcc/7.3.0/lib64/libstdc++.so.6\", O_RDONLY|O_CLOEXEC) = 4\r\n$\r\n$ # use `grep -m1` since other two results are just the error messages printing\r\n$ strace python3.6 -c \"import pandas; import tensorflow\" 2>&1 | grep -m1 -B15 libstdc # drop second and third results\r\nclose(3)                                = 0\r\nmmap(NULL, 262144, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f64ded4e000\r\nstat(\"/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/site-packages/matplotlib\", {st_mode=S_IFDIR|S_ISGID|0775, st_size=88, ...}) = 0\r\nstat(\"/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/site-packages/matplotlib/_path.cpython-36m-x86_64-linux-gnu.so\", {st_mode=S_IFREG|0775, st_size=190624, ...}) = 0\r\nopen(\"/apps/python/3.6.5/gcc-7.3.0/lib/python3.6/site-packages/matplotlib/_path.cpython-36m-x86_64-linux-gnu.so\", O_RDONLY|O_CLOEXEC) = 3\r\nread(3, \"\\177ELF\\2\\1\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\3\\0>\\0\\1\\0\\0\\0\\300\\204\\0\\0\\0\\0\\0\\0\"..., 832) = 832\r\nfstat(3, {st_mode=S_IFREG|0775, st_size=190624, ...}) = 0\r\nmmap(NULL, 2288808, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f64deb1f000\r\nmprotect(0x7f64deb4d000, 2093056, PROT_NONE) = 0\r\nmmap(0x7f64ded4c000, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x2d000) = 0x7f64ded4c000\r\nclose(3)                                = 0\r\nopen(\"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3\r\nfstat(3, {st_mode=S_IFREG|0644, st_size=91275, ...}) = 0\r\nmmap(NULL, 91275, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f64f26a2000\r\nclose(3)                                = 0\r\nopen(\"/lib64/libstdc++.so.6\", O_RDONLY|O_CLOEXEC) = 3\r\n```\r\n\r\nIt feels similar to [this issue](https://stackoverflow.com/questions/17142479/libraries-which-exist-in-a-binarys-elf-runpath-are-not-being-used) on Stack Overflow, but `ldd` shows the right libstdc++ being found.", "Oh interesting, yeah the `-B15` is helpful there. So it looks like matplotlib is going straight for the /lib64/ version, then the linker uses the cached /lib64/ load for TensorFlow.\r\n\r\nI'm not sure what TF can do in this case. Presumably it works if you `LD_PRELOAD=/apps/gcc/7.3.0/lib64/libstdc++.so.6 python3.6 -c \"import pandas; import tensorflow\"` ? That should be more or less equivalent to what happens when the imports are in the other order. Since the newer libstdc++ gets used and it's backwards compatible, both libraries are happy. Conversely, LD_PRELOAD with the /lib64/ version presumably fails because that version isn't forward compatible with the version TF was compiled against.\r\n\r\nMaybe the thing to investigate then is why matplotlib is going for the /lib64/ version first. Maybe there's a way to configure its search paths so it goes for the newer libstdc++?", "Yeah, setting LD_PRELOAD/prepending to LD_LIBRARY_PATH works. I'll just do that for now, and report back here if I find a less [dangerous](http://xahlee.info/UnixResource_dir/_/ldpath.html) solution. I guess this doesn't really have to do with TF so much as the overall setup--sorry for the noise!"]}, {"number": 24345, "title": "[XLA] Wrap LCB::GetOrCreateLocalClient with StatusOr<T>", "body": "\r\nPrevents crashes when GPUs present, but below minimum capability level\r\n\r\nFixes [google/jax issue #39](https://github.com/google/jax/issues/39)", "comments": ["@ajweiss Can you please run clang-format on your code.", "Formatted, squashed and force-pushed!", "@froystig  Could you also approve this PR if it looks good. Approving would help to proceed with the next steps for merging. Thank you !", "Strange, it seems to center the asterisk for pointers even when in Google mode.  I also see it elsewhere in the codebase:\r\n\r\n[scatter_expander.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/scatter_expander.cc#L59)\r\n\r\nI'm not sure what the story is, maybe the macro is confusing clang-format somehow?  The official style guide says they should be left or right justified.\r\n\r\nI can manually fix it up, but I could also see that causing problems with automated tools that use clang-format down the line and it would be inconsistent with the other places in the codebase.  It does however, appear to format correctly when the `const` keyword is used when defining the pointer in the macro.\r\n\r\nI'm inclined to think it's best to leave as-is generated by clang-format in Google mode, and then resolve the clang-format issue and the formatting across the codebase, but I'm happy to fix it up by hand as well.  Let me know which you would prefer. ", "Understood and agreed. Thanks for clarifying!", "I see the CI checks are failing.  Let me know if I should rebase this on HEAD or if that's expected.  (I ask as it will probably trigger another review)", "> I see the CI checks are failing. Let me know if I should rebase this on HEAD or if that's expected. (I ask as it will probably trigger another review)\r\n\r\nI'm taking care of this PR and helping to get it merged. I'll keep you posted if anything is required from your end. Thanks !"]}, {"number": 24344, "title": "Pretrained tensorflow resnet-101 on MPII human pose data", "body": "Hello,\r\n\r\ndo you know if a pretrained resnet-101 tensorflow model exists on MPII human pose dataset? I could not find any so far..\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 24343, "title": "Broadcasting batch_gather", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nBatch gather works like this\r\n\r\n```python3\r\nresult[i, .., k] = input[i, ..., k-1, indexes[i, ..., k]]\r\n```\r\nNow, lets say that `indexes.shape == [s1,s2,s3, k]` and `input.shape == [s1, 1, s3]`.  In order to be able to use batch_gather we must do\r\n\r\n```python3\r\ninput_ = tf.tile(input, (1, s2, 1))\r\nresult = tf.batch_gather(input_, indexes)\r\n``` \r\nwhich is wasteful, as we needlessly copy the tensor, for tiling.\r\n\r\nIn case this feature already exists and I missed it, please direct and forgive me.\r\n\r\n**Will this change the current api? How?** No\r\n\r\n**Who will benefit with this feature?** Most serious users, who work with masks and such.\r\n\r\n**Any Other info.**\r\n", "comments": ["I believe what you described can be achieved by `tf.gather_nd`", "@ppwwyyxx You are correct. I have now implemented it using gather_nd. I am closing this issue, as the feature is already present, with little work on gather_nd.", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24343)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24343)\r\n"]}, {"number": 24342, "title": " Predict fuel efficiency: regression example should not normalize one-hot values", "body": "**Describe the current behavior**\r\nIn the Predict fuel efficiency: regression lesson (TensorFlow->Learn->Tutorials->Learn and use ML->Regression), the normalize method is called to normalize the entire dataset, including the one-hot columns.\r\n\r\n**Describe the expected behavior**\r\nThe one-hot columns in the dataset should not be normalized.\r\n\r\n**Code to reproduce the issue**\r\ndataset['USA'] = (origin == 1)*1.0\r\ndataset['Europe'] = (origin == 2)*1.0\r\ndataset['Japan'] = (origin == 3)*1.0\r\ntrain_dataset = dataset.sample(frac=0.8,random_state=0)\r\ntrain_stats = train_dataset.describe()\r\ntrain_stats.pop(\"MPG\")\r\ntrain_stats = train_stats.transpose()\r\n\r\ndef norm(x):\r\n  return (x - train_stats['mean']) / train_stats['std']\r\nnormed_train_data = norm(train_dataset)\r\n\r\n", "comments": ["@richeyd  -  I think that would not make any difference. But let me confirm and keep you posted.\r\n@lamberta  Could you please check if the normalization to be removed for one hot columns or does it really impact the behaviour ?", "Thanks for reporting. Can you send a PR here (https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/regression.ipynb) to fix it.", "@richeyd,\r\nIn the latest [Predict Fuel Effiency : Regression Example](https://www.tensorflow.org/tutorials/keras/regression), the **`One-Hot Values`** are no more normalized. Can you please confirm if we can close this issue? Thanks!", ">  the One-Hot Values are no more normalized. \r\n\r\nIt still does. This is the line of interest, where it adapts the normalization layer to all the features:\r\n\r\n```\r\nnormalizer.adapt(np.array(train_features))\r\n```\r\n\r\n> should not normalize one-hot values\r\n\r\nI would say \"they don't need to be normalized.\". But It doesn't do any harm, and it keeps this tutorial **simple**.\r\n\r\nI think an apropriate fix here would be to add a `Note:` paragraph that to section, and include links to these:\r\n\r\n1. https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\r\n2. https://www.tensorflow.org/guide/keras/preprocessing_layers", "@MarkDaoust,\r\nSure, I can add that. ", "Closing the issue as the PR is merged."]}, {"number": 24341, "title": "[WIP][tensor_forest] decouple split candidate initialization and stats update", "body": "addresses https://github.com/tensorflow/tensorflow/issues/24040", "comments": ["no need for this.. "]}, {"number": 24340, "title": "Can't open tensorflow.org", "body": "I can't open https://www.tensorflow.org/ but I can open https://tensorflow.google.cn, why is that? If tensorflow has changed its URL why didn't update it? When I google tensorflow it also leads to https://www.tensorflow.org, but it cannot be accessed.\r\nDoes anyone else having this problem?", "comments": ["Most likely a routing issue between your exchange and tensorflow.org - contact your ISP if it hasn't been fixed in a day or two. (For the record, this isn't really the place for reporting that kind of problem - and there's multiple \"is it down\" sites that can tell you if this is a local or wider problem. E.g. http://www.isitdownrightnow.com)"]}, {"number": 24339, "title": "tf.contrib.copy_graph.copy_op_to_graph Error when copying a foldl operation", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: cuda 9.0, cudnn 7.1\r\n- GPU model and memory: GeForce GTX 1060 \r\n\r\n**Describe the current behavior**\r\ntf.contrib.copy_graph.copy_op_to_graph gives a \r\n```\r\nRecursionError: maximum recursion depth exceeded while calling a Python object\r\n```\r\nwhen called on a foldl operation.\r\ntf.contrib.copy_graph.copy_op_to_graph does not manages foldl operation. Maybe it is because of the recursive structure of the operation.\r\n\r\n**Describe the expected behavior**\r\ntf.contrib.copy_graph.copy_op_to_graph should copy the part of the graph containing the fold operation. Maybe it can manage in a particular way recursive blocks like fold.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nx = tf.ones((2,1))\r\nacc =  tf.ones((1,1))\r\n# definition of the fold operation\r\ny = tf.foldl(lambda a,y: return a + y, x, acc) \r\n# tf.Session().run(y) gives [[3.]], as expected\r\n\r\n# copy the fold to another graph\r\ng = tf.Graph()\r\nwith g.as_default():\r\n    tf.contrib.copy_graph.copy_op_to_graph(y, g, [])\r\n    # RecursionError: maximum recursion depth exceeded while calling a Python object\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nFull traceback attached.\r\n[tf_bug.zip](https://github.com/tensorflow/tensorflow/files/2675691/tf_bug.zip)\r\n\r\nBasically the issue is a recursion issue, tf does not understands the recursive nature of the foldl op.\r\nReduced traceback: \r\n```\r\nRecursionError: maximum recursion depth exceeded while calling a Python object\r\n```\r\n\r\n\r\n\r\n", "comments": ["@alextp Can you PTAL? Thanks!", "Contrib is deprecated and about to be deleted so I don't have the bandwidth to investigate this.\r\n\r\nI also suspect that with control flow v2 this won't be a problem."]}, {"number": 24338, "title": "tensorflow1.7 hangs at LocalMaster::RunStep with tf.train.MonitoredTrainingSession in sync mode", "body": "I'm training gan on wavenet model with tf.train.MonitoredTrainingSession, with 1 ps and 2 workers. It hangs when using tf.train.SyncReplicasOptimizer, but works well in async mode. And it works well with Vanilla GAN demo in sync mode.\r\n\r\nWith debug the chief worker, I found master  is waiting for worker to response. However, I don't know what the worker is doing, and which thread hangs?\r\n\r\nSo, how to debug this problem?\r\n\r\nMore infos: tenosrflow 1.7, M40, sorry can not paste the source code\r\n\r\nchief worker gdb info as follows:\r\n\r\n```\r\n(gdb) bt\r\n#0  0x00007f9f001796d5 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007f9e39a098dc in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/local/gcc-5.3.0/lib64/libstdc++.so.6\r\n#2  0x00007f9e4758566b in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007f9e47584f71 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007f9e47582402 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007f9e475828e3 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007f9e4472a04b in tensorflow::(anonymous namespace)::WaitForNotification(tensorflow::CallOptions*, long long, tensorflow::Notification*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007f9e4472a984 in tensorflow::LocalMaster::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007f9e447125ea in tensorflow::GrpcSession::RunProto(tensorflow::CallOptions*, tensorflow::MutableRunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007f9e447134a2 in tensorflow::GrpcSession::RunHelper(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007f9e44713c60 in tensorflow::GrpcSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007f9e449dd3d1 in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Tensor**, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Buffer*, TF_Status*) [clone .constprop.703]\r\n    () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#12 0x00007f9e449de69a in TF_Run () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#13 0x00007f9e446bb213 in tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#14 0x00007f9e446bb323 in tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#15 0x00007f9e4467b010 in _wrap_TF_Run () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#16 0x00000000004b8ef4 in PyEval_EvalFrameEx ()\r\n#17 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#18 0x00000000004b89cd in PyEval_EvalFrameEx ()\r\n#19 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#20 0x000000000052f2d8 in function_call ()\r\n#21 0x00000000004235da in PyObject_Call ()\r\n#22 0x00000000004b4ee2 in PyEval_EvalFrameEx ()\r\n#23 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#24 0x00000000004b89cd in PyEval_EvalFrameEx ()\r\n#25 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#26 0x00000000004b89cd in PyEval_EvalFrameEx ()\r\n#27 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#28 0x00000000004b89cd in PyEval_EvalFrameEx ()\r\n#29 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#30 0x000000000052f3cf in function_call ()\r\n#31 0x00000000004235da in PyObject_Call ()\r\n#32 0x00000000004b4ee2 in PyEval_EvalFrameEx ()\r\n#33 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#34 0x000000000052f3cf in function_call ()\r\n#35 0x00000000004235da in PyObject_Call ()\r\n#36 0x0000000000427bb5 in instancemethod_call ()\r\n#37 0x00000000004235da in PyObject_Call ()\r\n#38 0x00000000004b4396 in PyEval_EvalFrameEx ()\r\n#39 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#40 0x000000000052f3cf in function_call ()\r\n#41 0x00000000004235da in PyObject_Call ()\r\n#42 0x00000000004b4ee2 in PyEval_EvalFrameEx ()\r\n#43 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#44 0x00000000004b89cd in PyEval_EvalFrameEx ()\r\n#45 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#46 0x00000000004b89cd in PyEval_EvalFrameEx ()\r\n#47 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#48 0x00000000004b89cd in PyEval_EvalFrameEx ()\r\n---Type <return> to continue, or q <return> to quit---\r\n```\r\nwoker 1 gdb info:\r\n\r\n```\r\n(gdb) bt\r\n#0  0x00007ff6c5ace6d5 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007ff5ff35e8dc in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/local/gcc-5.3.0/lib64/libstdc++.so.6\r\n#2  0x00007ff60ceda66b in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007ff60ced9f71 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007ff60ced7402 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007ff60ced78e3 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007ff60a07f04b in tensorflow::(anonymous namespace)::WaitForNotification(tensorflow::CallOptions*, long long, tensorflow::Notification*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007ff60a07f984 in tensorflow::LocalMaster::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007ff60a0675ea in tensorflow::GrpcSession::RunProto(tensorflow::CallOptions*, tensorflow::MutableRunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007ff60a0684a2 in tensorflow::GrpcSession::RunHelper(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007ff60a068c60 in tensorflow::GrpcSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007ff60a3323d1 in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Tensor**, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Buffer*, TF_Status*) [clone .constprop.703]\r\n    () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#12 0x00007ff60a33369a in TF_Run () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#13 0x00007ff60a010213 in tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#14 0x00007ff60a010323 in tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#15 0x00007ff609fd0010 in _wrap_TF_Run () from /home/tops/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#16 0x00000000004b8ef4 in PyEval_EvalFrameEx ()\r\n#17 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#18 0x00000000004b89cd in PyEval_EvalFrameEx ()\r\n#19 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#20 0x000000000052f2d8 in function_call ()\r\n#21 0x00000000004235da in PyObject_Call ()\r\n#22 0x00000000004b4ee2 in PyEval_EvalFrameEx ()\r\n#23 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#24 0x00000000004b89cd in PyEval_EvalFrameEx ()\r\n#25 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#26 0x00000000004b89cd in PyEval_EvalFrameEx ()\r\n#27 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#28 0x00000000004b89cd in PyEval_EvalFrameEx ()\r\n#29 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#30 0x000000000052f3cf in function_call ()\r\n#31 0x00000000004235da in PyObject_Call ()\r\n#32 0x00000000004b4ee2 in PyEval_EvalFrameEx ()\r\n#33 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#34 0x000000000052f3cf in function_call ()\r\n#35 0x00000000004235da in PyObject_Call ()\r\n#36 0x0000000000427bb5 in instancemethod_call ()\r\n#37 0x00000000004235da in PyObject_Call ()\r\n#38 0x00000000004b4396 in PyEval_EvalFrameEx ()\r\n#39 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#40 0x000000000052f3cf in function_call ()\r\n#41 0x00000000004235da in PyObject_Call ()\r\n#42 0x00000000004b4ee2 in PyEval_EvalFrameEx ()\r\n#43 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#44 0x00000000004b89cd in PyEval_EvalFrameEx ()\r\n#45 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#46 0x00000000004b89cd in PyEval_EvalFrameEx ()\r\n#47 0x00000000004b9b98 in PyEval_EvalCodeEx ()\r\n#48 0x00000000004b89cd in PyEval_EvalFrameEx ()\r\n---Type <return> to continue, or q <return> to quit---\r\n```\r\n", "comments": ["Same issue on tf-1.12.0, and in our case tf hang at the last batch of dataset when the dataset epoch is 1.\r\nIf we use tf.train.StopAtStepHook() in MonitoredTrainingSession() to stop training, tf can exit successfully.\r\n```\r\n#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38\r\n#1  0x00007f475f56e1ac in nsync::futex (uaddr=0x555c3d471368, op=393, val=0, timeout=0x0, uaddr2=0x0, val3=-1) at external/nsync/platform/linux/src/nsync_semaphore_futex.c:21\r\n#2  0x00007f475f56e357 in nsync::nsync_mu_semaphore_p_with_deadline (s=0x555c3d471368, abs_deadline=...) at external/nsync/platform/linux/src/nsync_semaphore_futex.c:108\r\n#3  0x00007f475f56d1f6 in nsync::nsync_sem_wait_with_cancel_ (w=0x555c3d471360, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/sem_wait.c:36\r\n#4  0x00007f475f569277 in nsync::nsync_cv_wait_with_deadline_generic (pcv=0x7fff46c23e20, pmu=0x7fff46c23e10, lock=0x7f475f568fdf <nsync::void_mu_lock(void*)>, \r\n    unlock=0x7f475f568ffa <nsync::void_mu_unlock(void*)>, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/cv.c:246\r\n#5  0x00007f475f5699f5 in nsync::nsync_cv_wait_with_deadline (pcv=0x7fff46c23e20, pmu=0x7fff46c23e10, abs_deadline=..., cancel_note=0x0) at external/nsync/internal/cv.c:440\r\n#6  0x00007f475f569a32 in nsync::nsync_cv_wait (pcv=0x7fff46c23e20, pmu=0x7fff46c23e10) at external/nsync/internal/cv.c:450\r\n#7  0x00007f474de90bf5 in tensorflow::condition_variable::wait (this=0x7fff46c23e20, lock=...) at tensorflow/core/platform/default/mutex.cc:72\r\n#8  0x00007f4756864a5d in tensorflow::Notification::WaitForNotification (this=0x7fff46c23e10) at ./tensorflow/core/platform/default/notification.h:54\r\n#9  0x00007f4756c3dc34 in tensorflow::(anonymous namespace)::WaitForNotification (call_options=0x7fff46c24090, default_timeout_in_ms=0, n=0x7fff46c23e10)\r\n    at tensorflow/core/distributed_runtime/local_master.cc:39\r\n#10 0x00007f4756c3e439 in tensorflow::LocalMaster::RunStep (this=0x555c3ba54780, call_options=0x7fff46c24090, request=0x555c38dbae60, response=0x555c38dbb110)\r\n    at tensorflow/core/distributed_runtime/local_master.cc:105\r\n#11 0x00007f475686d03d in tensorflow::GrpcSession::RunProto (this=0x555c3b986280, call_options=0x7fff46c24090, req=0x555c38dbae60, resp=0x555c38dbb110)\r\n    at tensorflow/core/distributed_runtime/rpc/grpc_session.cc:289\r\n#12 0x00007f475686c7d6 in tensorflow::GrpcSession::RunHelper (this=0x555c3b986280, run_options=..., inputs=std::vector of length 0, capacity 0, \r\n    output_tensor_names=std::vector of length 4, capacity 4 = {...}, target_node_names=std::vector of length 1, capacity 1 = {...}, outputs=0x7fff46c242f0, run_metadata=0x7fff46c24350, \r\n    prun_handle=\"\") at tensorflow/core/distributed_runtime/rpc/grpc_session.cc:221\r\n#13 0x00007f475686ce35 in tensorflow::GrpcSession::Run (this=0x555c3b986280, run_options=..., inputs=std::vector of length 0, capacity 0, \r\n    output_tensor_names=std::vector of length 4, capacity 4 = {...}, target_node_names=std::vector of length 1, capacity 1 = {...}, outputs=0x7fff46c242f0, run_metadata=0x7fff46c24350)\r\n    at tensorflow/core/distributed_runtime/rpc/grpc_session.cc:270\r\n#14 0x00007f4756832f34 in tensorflow::SessionRef::Run (this=0x555c3ba38880, run_options=..., inputs=std::vector of length 0, capacity 0, \r\n    output_tensor_names=std::vector of length 4, capacity 4 = {...}, target_node_names=std::vector of length 1, capacity 1 = {...}, outputs=0x7fff46c242f0, run_metadata=0x7fff46c24350)\r\n    at tensorflow/python/client/session_ref.cc:427\r\n#15 0x00007f4756aeeabc in TF_Run_Helper (session=0x555c3ba38880, handle=0x0, run_options=0x555c38437160, input_pairs=std::vector of length 0, capacity 0, \r\n    output_tensor_names=std::vector of length 4, capacity 4 = {...}, c_outputs=0x7fff46c246d8, target_oper_names=std::vector of length 1, capacity 1 = {...}, run_metadata=0x555c38dd9b70, \r\n    status=0x555c38ddf6d0) at tensorflow/c/c_api.cc:783\r\n```", "@hgadig any advice?", "@lxn179208 Could you fill the issue template [here](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md). It will help us to find root-cause of the issue. Is it possible for you to use recent version and check whether the issue persists? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 24336, "title": "I  train the model use tf.estimator and use tonado for serve,when i request  the service  it got stuck,how to fix it?", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["Request you to provide a reproducible code which helps us to look into this issue. \r\nAlso this looks like an issue with the tornado webserver and you can post this issue in the respective github repository. If you think this issue is caused by tensorflow, please provide detailed description of the issue with a sample code snippet.", "> It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?\r\n\r\nWell i use tornado based on tensorserving and the escape the problem ", "> > It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?\r\n> \r\n> Well i use tornado based on tensorserving and the escape the problem\r\n\r\nDo you say that this issue is solved now ? Correct me if I'm wrong.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 24335, "title": "Update README.md", "body": "`chnage` to change", "comments": []}, {"number": 24334, "title": "Update README.md", "body": "some typo in readme", "comments": []}, {"number": 24333, "title": "How can i use inspect_checkpoint tool to print all the weight ?", "body": "I've try using inspect_checkpoint.py tool to see all weight in my checkpoint:\r\nmy checkpoint file contain:\r\nmodel.ckpt-10.data-00000-of-00001\r\nmodel.ckpt-10.index\r\nmodel.ckpt-10.meta\r\nmy command was:\r\npython inspect_checkpoint.py --file_name=model.ckpt-10 --all_tensors=True --tensor_name=''\r\nBut the weight missing it showed with \"...\"\r\nFor example:\r\ntensor_name: dnn/hiddenlayer_4/weights\r\n[[ 0.11010675 -0.00205228 0.06564941 ... 0.04844228 -0.06266475\r\n0.07596292]\r\n[ 0.06760433 0.12344839 0.12087007 ... -0.04885229 -0.15109406\r\n0.03623438]\r\n[-0.11817194 0.0303787 0.06215482 ... 0.02974839 0.09252764\r\n0.25440046]\r\n...\r\n[-0.03469304 -0.19408804 0.1232834 ... -0.101989 -0.11033994\r\n0.00296944]\r\n[-0.01577727 0.13655508 -0.07761313 ... -0.08856598 -0.13241452\r\n-0.00495515]\r\n[ 0.00119148 0.03261929 -0.096873 ... -0.04134168 0.02477826\r\n0.10813464]]\r\n\r\nthe shape of the tensor_name is: dnn/hiddenlayer_4/weights (DT_FLOAT) [128,128]\r\nHow can i really print all the weight ?\r\nThank you.\r\n", "comments": []}, {"number": 24332, "title": "Update output_init_files_test.py", "body": "documentation", "comments": ["Thank you for the typo fix!"]}, {"number": 24331, "title": " Malformed TF_STRING tensor; too short to hold number of elements", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS 7.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): Java API 1.12\r\n- Python version: \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI am using Java API for prediction and \r\nI tried to create a tensor of String type using the following Java API:\r\n```java\r\n                    String[] strs = {\"element1\", \"thisissecondelement\"};  \r\n                    ByteArrayOutputStream outputStream = new ByteArrayOutputStream( );   \r\n                    for (String s : strs) {  \r\n                        outputStream.write(s.getBytes());  \r\n                    }  \r\n                    long [] shape = new long[1];\r\n                    shape[0] = strs.length;\r\n                    Tensor<String> t = Tensor.create(String.class, shape,  ByteBuffer.wrap(outputStream.toByteArray());\r\n```\r\n\r\nHowever, when calling \"runner.fetch(\"predict/Sigmoid:0\").run().get(0)\" I got the following excetion:\r\n```\r\nException in thread \"main\" java.lang.IllegalArgumentException: Malformed TF_STRING tensor; too short to hold number of elements.\r\n```\r\n\r\nI looked into the code:\r\n![g1m018793wkgcqlwrvg-abv9paaa0cknmbeo475](https://user-images.githubusercontent.com/1213386/49913535-d5e0e480-fec8-11e8-8d94-4260800120a6.PNG)\r\n\r\nIt uses sizeof(tensorflow::uint64) as the size of the element in the byte array. However, the  size of the string element is not fixed. \r\n\r\n**Describe the expected behavior**\r\nThe API  `runner.fetch(\"predict/Sigmoid:0\").run().get(0)`  should run without error.\r\n\r\n**Code to reproduce the issue**\r\nCreate a String Tensor using `public static <T> Tensor<T> create(Class<T> type, long[] shape, ByteBuffer data) ` in org.tensorflow.Tensor.\r\n\r\n", "comments": ["@asimshankar this seems related to the C API code you wrote. Do you know who should handle this?", "\r\nI think the following will get what you want:\r\n\r\n```java\r\nString[] strs = {\"element1\", \"thisissecondelement\"};\r\nbyte[][] data = new byte[strs.length][];\r\nfor (int i = 0; i < data.length; ++i) {\r\n  data[i] = strs[i].getBytes();\r\n}\r\nTensor<String> t = Tensors.create(data);\r\n// Alternatively: Tensor<String> t = Tensor.create(data, String.class);\r\n```\r\n\r\nSome explanation:\r\n\r\n- The sample code provided was using [`Tensor.create(Class<T> type, long[] shape, ByteBuffer data)`](http://static.javadoc.io/org.tensorflow/libtensorflow/1.12.0/org/tensorflow/Tensor.html#create-java.lang.Class-long:A-java.nio.ByteBuffer-) which (though easy to miss) requires that the `ByteBuffer` contain the bytes encoded as required by the TensorFlow C API. (The Javadoc links to the [format described in `c_api.h`](https://github.com/tensorflow/tensorflow/blob/98c0deb828c2f98f0d6d77a12d32f3b33ed92887/tensorflow/c/c_api.h#L206)). So, the string tensors needed to be encoded with `uint64` offset information in addition to the raw bytes, which is why you were getting the error.\r\n\r\n- The snippet I provided above would probably be what you want instead - using [`Tensors.create(byte[][][])`](http://static.javadoc.io/org.tensorflow/libtensorflow/1.12.0/org/tensorflow/Tensors.html#create-byte:A:A-) which is a convenience wrapper over [`Tensor.create(Object, Class<T>)`](http://static.javadoc.io/org.tensorflow/libtensorflow/1.12.0/org/tensorflow/Tensor.html#create-java.lang.Object-java.lang.Class-), whose documentation provides some more examples.\r\n\r\nI'm tempted to close this since this doesn't appear to be a bug, just a slight misunderstanding in how to use the API. If I'm mistaken, or if you have suggestions on what would have helped (or how the documentation could be improved) so that you didn't run into this - feel free to re-open."]}, {"number": 24330, "title": "Add the eager coverage for some dataset experimental kernels", "body": "This PR adds the eager coverage for `NumElementsTest`, `CounterTest`, `DenseToSparseBatchTest`, and `DirectedInterleaveDatasetTest`.", "comments": ["Nagging Reviewer @jaingaurav: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 119 days with no activity and the `awaiting review` label has been applied.", "@feihugis is this PR still relevant? if so, could you please merge HEAD and resolve conflicts? thanks", "@jsimsa Sorry for the late response! Double checked this PR. The current tests have covered both the graph and eager mode (cached_session() provides [`FakeEagerSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L1896) for the eager mode). Please feel free to close this PR. "]}, {"number": 24329, "title": "Missing Docker Images referenced in Documentation", "body": "**System information**\r\n- TensorFlow version: Docker version\r\n- Doc Link: https://www.tensorflow.org/install/docker\r\n\r\n\r\n**Describe the documentation issue**\r\nIn the install with Docker documentation, there are examples of how to pull images by adding multiple parameters together, there is am implicit reference to latest-gpu-jupyter. On [docker-hub](https://hub.docker.com/r/tensorflow/tensorflow/tags/), it appears that tag is no longer valid or is no longer being built. This can cause a confusing error relating to a missing manifest when trying to pull that image.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nOf course, I just need to know if this is a permanent deprecation of latest-gpu-py3-jupyter and latest-gpu-jupyter, or if it's just a mistake, the nightly builds do contain those tags (nightly-gpu-py3-jupyter) for example\r\n", "comments": ["I am going through this exact issue. I am docker & tf newbie and was looking forward to the docker image to just work. However, latest-*-jupyter does not show up, neither does docker hub shows that. How is one suppose to get this image? Is that image discontinued? If so could someone please update the documentation?", "The default installation [page](https://www.tensorflow.org/install/) however seems to say that one can get to Jupyter notebooks with normal installation as well `docker run -it -p 8888:8888 tensorflow/tensorflow  # Start a Jupyter notebook server.` I am guessing the notebooks have become default on all images and which would make the need for tag with latest label / tag unnecessary. But the documentation still needs to be consistent so as to not confuse the newbies like me.", "This has been fixed. The tag for jupyter notebook is: `docker pull tensorflow/tensorflow:latest-gpu-jupyter`.\r\n\r\nPlease reopen if you hit an error."]}, {"number": 24328, "title": "tensorflow.keras printing control characters in progress bars", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04, docker hub tag: 1.12.0-gpu-py3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): docker pull\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): not source\r\n- GCC/Compiler version (if compiling from source): not source\r\n- CUDA/cuDNN version: 7.1.4\r\n- GPU model and memory: P80 12GB\r\n\r\n\r\n**Describe the current behavior**\r\nI'm running tensorflow.keras through a docker container and displaying the output in a Jupyter notebook that is outside of the container (using Amazon SageMaker notebooks). Regular Keras printed progress bars correctly, but switching to tensorflow.Keras now spams my screen with control characters when trying to print progress bars. For example when Keras is downloading ImageNet weights for a model it outputs this:\r\n\r\n```\r\nDownloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\r\n#015    8192/83683744 [..............................] - ETA: 12:27#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015   40960/83683744 [..............................] - ETA: 5:00 #010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015   90112/83683744 [..............................] - ETA: 3:24#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015  212992/83683744 [..............................] - ETA: 1:55#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015  442368/83683744 [..............................] - ETA: 1:09#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015  892928/83683744 [..............................] - ETA: 40s #010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 1810432/83683744 [..............................] - ETA: 23s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010\r\n```\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nIt shouldn't print control characters\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nReproducing this exactly may require running through SageMaker. The example I modified is this: https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning/keras_bring_your_own \r\nSpecifically, I changed all of the Keras imports in this file: https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/keras_bring_your_own/trainer/start.py to be tensorflow.keras imports. \r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Can you provide more specific details about your output terminal (shell)? Is it just through SSH on a terminal window or a web interface provided by SageMaker? \r\nCan you also confirm you are indeed running both versions of your code in Python3.5? Whats your default python version? `python --version`\r\nYou are running both versions of the code on the same system setup correct?\r\n\r\nMore information can help us understand your environment / issues... IPython + Ubuntu16 could be the real source of the issue..\r\n\r\nMy observations:\r\n\r\nBoth `keras` and `tensorflow.keras` are based on the same `/keras/utils/generic_utils.py` which calls `sys.stdout.write(info)` from its `ProgBar` class. Tensorflow's version of keras is slightly modified to export function call names etc.. Possibly for integrating function calls with C/C++, not sure exactly.\r\nAdditionally both versions use the backspace character (`\\b` or octo `#010` <- what you see) to create the desired progress bar look and feel. TF also added some additional features for tensorflows use cases to the keras code (not a submodule of keras).\r\n\r\nThe python spec says when running Python2 `sys.stdout.write` will print default ASCII and Python3 will print default UTF-8. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/bbb81ea42831b244289a059de9e4be0203dc4c35/tensorflow/python/keras/utils/generic_utils.py\r\n\r\nEDIT:\r\nIf you choose to fix this in your own code, you can override the default encoding of sys output in Python.. No clue how that might overridden after the fact in TF.\r\n\r\nHope to help out more ", "Hi, thanks for your response. I'm running Python 3.5.x with both. I've put the model code in a tensorflow docker image and uploaded it to the AWS ECR. Then when I run the SageMaker.estimator.Estimator.fit command while inside a SageMaker notebook instance (passing it the ECR path, an instance type, and an S3 train/valid data path) it spins up a separate EC2 instance, pulls the ECR image to it, pulls the data from S3, trains the model on there (and saves model artifacts to S3), while displaying the training print outputs in the SageMaker notebook instance cell from which I originally called the fit command. \r\n\r\nAll-in-all a difficult process for me to track, but what I do know is that this issue began when I switched from keras to tensorflow.keras.", "> Hi, thanks for your response. I'm running Python 3.5.x with both. I've put the model code in a tensorflow docker image and uploaded it to the AWS ECR. Then when I run the SageMaker.estimator.Estimator.fit command while inside a SageMaker notebook instance (passing it the ECR path, an instance type, and an S3 train/valid data path) it spins up a separate EC2 instance, pulls the ECR image to it, pulls the data from S3, trains the model on there (and saves model artifacts to S3), while displaying the training print outputs in the SageMaker notebook instance cell from which I originally called the fit command.\r\n> \r\n> All-in-all a difficult process for me to track, but what I do know is that this issue began when I switched from keras to tensorflow.keras.\r\n\r\nI'm having a same problem, did you find the fix for this?", "> > Hi, thanks for your response. I'm running Python 3.5.x with both. I've put the model code in a tensorflow docker image and uploaded it to the AWS ECR. Then when I run the SageMaker.estimator.Estimator.fit command while inside a SageMaker notebook instance (passing it the ECR path, an instance type, and an S3 train/valid data path) it spins up a separate EC2 instance, pulls the ECR image to it, pulls the data from S3, trains the model on there (and saves model artifacts to S3), while displaying the training print outputs in the SageMaker notebook instance cell from which I originally called the fit command.\r\n> > All-in-all a difficult process for me to track, but what I do know is that this issue began when I switched from keras to tensorflow.keras.\r\n> \r\n> I'm having a same problem, did you find the fix for this?\r\n\r\nNope.", "Just started encountering this issue when using TensorFlow 2.0.0 and 2.1.0 in SageMaker. Is there still no fix for this?", "I'm also facing this issue. Has anyone figured out a solution yet?", "I think the issue is here:\r\nhttps://github.com/tensorflow/tensorflow/blob/bbb81ea42831b244289a059de9e4be0203dc4c35/tensorflow/python/keras/utils/generic_utils.py#L337-L340\r\n\r\nThe code running inside SageMaker neither has a tty nor is running inside jupyter, yet the code above enables `_dynamic_display` because of the `'posix' in sys.modules` check. I don't understand why that check exists.\r\n\r\nEnabling the flag causes the backspace chars to be emitted in a platform that doesn't actually have an interactive console that supports it:\r\nhttps://github.com/tensorflow/tensorflow/blob/bbb81ea42831b244289a059de9e4be0203dc4c35/tensorflow/python/keras/utils/generic_utils.py#L387-L389", "Not really a solution but a compromise: if you don't have to see the progress within each epoch you can use `model.fit(..., verbose=2)` . No annoying control characters this way so it's easier to navigate the output. From the documentation (11.05.2020):\r\n\r\n> verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment).\r\n\r\nThat \"not running interactively\" part seems to be the key here", "There is a relevant discussion about the handling of backspace characters in Jupyter Notebook and JupyterLab [here](https://github.com/jupyter/notebook/issues/5381).", "same issue in AWS Sagemaker with tensorflow version 2.3.0.. any news?", "I'm facing the same issue. Can someone please shed some light here. ", "> I'm facing the same issue. Can someone please shed some light here.\r\n\r\nI was able to resolve this by using `verbose=2` in the fit method. https://keras.io/api/models/model_training_apis/", "@austinmw,\r\nCan you please confirm if [above comment](https://github.com/tensorflow/tensorflow/issues/24328#issuecomment-750793778) helped you to resolve your issue? Thanks!", "Can't speak for @austinmw but I can say that, while setting `verbose=2` certainly solves the problem in SageMaker specifically but also jupyter consoles in general, as @sonebu mentioned above, it's only a compromise as it solves by simply not emitting progress bars.\r\n\r\nThe real question here is why the code is treating a non-tty-capable output as a tty-capable output as mentioned in my comment above: https://github.com/tensorflow/tensorflow/issues/24328#issuecomment-616952828\r\n", "This behaves as intended. The default output is intended for typical terminals and notebook environments. For other types of environments and logging please use verbose=0 or verbose=2.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24328\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24328\">No</a>\n"]}, {"number": 24327, "title": "TFTRT: Lru cache", "body": "This PR contains following changes:\r\n- lru_cache logic in trt_engine_op\r\n- using input shapes instead of batch size \r\n- refactor tftrt tests (enable to run network multiple times)\r\n\r\nThis changes should affect only running TRT with dynamic mode.\r\n\r\nMotivation for this PR:\r\n\r\nTF-TRT supports a caching mechanism for the dynamic mode (when is_dynamic_op=True). Currently, the cache has a fixed size specified by the user. The cache size determines how many TRT engines can be saved in the cache for each TRTEngineOp in the graph. If during execution, the number of built TRT engines for a particular TRTEngineOp goes above the cache size, TF-TRT errors out. This would happen\r\nwhen users feed inputs with many different batch sizes during execution. Users have to increase the cache size during the conversion to avoid later failures. This PR adds a new functionality where we can reuse the cache by using LRU mechanism, so, TF-TR will never run out of cache.\r\n\r\nThe current cache uses batch size as the key, and uses a TRT engine/context as the value. It turns out batch size is not enough to distinguish between different TRT engines. Some times other dimensions of inputs can change while the batch size remains the same (such as in translation models where the sequence length changes). Currently, when a dimension other than the batch size changes during the execution, TF-TRT errors out because it downloads a TRT engine from the cache that supports the right batch size but not the right input shapes, and thus fails due to shape mismatch. This PR extends the key of the cache to be the shape of all the inputs of a TRTEngineOp.", "comments": ["We should change the LRU class so that it uses the resource manager.", "Thanks for sending out the PR. I will start reviewing this next week after you respond to @pooyadavoodi's suggestion.\r\n\r\nMean while, could you also update the PR description by adding more details about the motivation possibly with an example use-case of yours benefiting from this. I see there is a TODO to introduce LRU cache but given that this is a large PR, it would be good to document the motivation with details.", "Motivation for this PR:\r\n\r\n- TF-TRT supports a caching mechanism for the dynamic mode (when `is_dynamic_op=True`). Currently, the cache has a fixed size specified by the user. The cache size determines how many TRT engines can be saved in the cache for each TRTEngineOp in the graph. If during execution, the number of built TRT engines for a particular TRTEngineOp goes above the cache size, TF-TRT errors out. This would happen \r\n when users feed inputs with many different batch sizes during execution. Users have to increase the cache size during the conversion to avoid later failures. This PR adds a new functionality where we can reuse the cache by using LRU mechanism, so, TF-TR will never run out of cache.\r\n\r\n- The current cache uses batch size as the key, and uses a TRT engine/context as the value. It turns out batch size is not enough to distinguish between different TRT engines. Some times other dimensions of inputs can change while the batch size remains the same (such as in translation models where the sequence length changes). Currently, when a dimension other than the batch size changes during the execution, TF-TRT errors out because it downloads a TRT engine from the cache that supports the right batch size but not the right input shapes, and thus fails due to shape mismatch. This PR extends the key of the cache to be the shape of all the inputs of a TRTEngineOp.\r\n\r\n", "@smit-hinsu @aaroey Could you please check to make sure we are using ResouceBase/ResourceManager properly? Do we need to add a ScopedUnref?\r\n\r\nDoes using ResourceBase help with thread safety at all?", "If the resource manager doesn't help with race conditions, then what's the benefit of using it?", "Current issues:\r\n* I'm not sure how to implement your suggestions regarding refactors to the LRUCache. I can't see a clean way to incorporate Status into emplace().\r\n* I now see that the other input dimensions aside from batch were not being checked for compatibility. They are implicitly checked during cache lookups - need to think about `cached_engine_batch_sizes` behavior. Not sure how much effort should be put into this with the offline mode coming soon.", "@aaroey \r\nNow that the cache is a resource, it is not destroyed in the correct order (can't be destroyed before allocator). I would like to destroy the cache in `~TRTEngineOp`, however it seems like we do not have access to the OpKernelContext there? How can I ensure the cache is destroyed first?\r\n\r\nShould allocator_ be a resource as well?", "@trevor-m yes please put allocator_ into TRTEngineCacheResource, this is fine since resource manager is actually a member of the device, so as long as the resource manager is valid the device is valid, meaning that the allocator is valid.\r\n\r\nFor my comments about LRUCache, I'll take a look later and maybe add some commits.\r\n\r\nFor the non-batch dimensions yes we can delay until offline mode is in place.", "@aaroey Thanks again for all of your help and feedback. Moving allocator to the TRTEngineCacheResource did the trick :). Let me know what I can do to help with LRUCache - perhaps if you could list your ideas for the function signatures I could implement the rest.", "Hi @aaroey. I have fixed the merge conflicts from https://github.com/tensorflow/tensorflow/commit/e51fa30400b3a469b03111b25344bc47bdff96bd#diff-6047a6db6e49473fb94f1f3b59b5630e\r\nCould you double check to make sure I got everything? \r\nAside from that this should be good, tests are passing.", "@trevor-m Thanks! Now after #24675 is merged there are more conflicts, would you help to fix them as well?", "@aaroey Thanks for merging! Just resolved them. ", "@aaroey Thanks for the commits! Sanity test should pass now"]}, {"number": 24326, "title": "with tf.device('/cpu:0') is ignored when using MirroredStrategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n    Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n    Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):\r\n    pip / binary\r\n- TensorFlow version (use command below):\r\n    tf-nightly-gpu==1.13.0.dev20181022\r\n- Python version:\r\n   3.6\r\n- Bazel version (if compiling from source):\r\n    N/A\r\n- GCC/Compiler version (if compiling from source):\r\n    N/A\r\n- CUDA/cuDNN version:\r\n   7.2.1.38\r\n- GPU model and memory:\r\n    8x v100 (16 gb per)\r\n\r\n**Describe the current behavior**\r\nIf I call with tf.device while my distribution strategy is set to **MirroredStrategy** then my embeddings are forced onto gpu.\r\n```\r\n...\r\nwith tf.device('/cpu:0'):\r\n    with tf.variable_scope(\"testvar\"):\r\n        embeddings = tf.get_variable(\r\n            name='weights',\r\n            shape=[10, 5],\r\n            trainable=True)\r\nprint(embeddings)\r\n...\r\n```\r\n>>>\r\nMirroredVariable:{'/replica:0/task:0/device:**GPU**:0': <tf.Variable 'fh_universal_transformer_encoder/word_symbol_modality_1917597_300/input_emb/embeddings/weights:0' shape=(1917597, 300)\r\n\r\n**Describe the expected behavior**\r\n```\r\n...\r\nwith tf.device('/cpu:0'):\r\n    with tf.variable_scope(\"testvar\"):\r\n        embeddings = tf.get_variable(\r\n            name='weights',\r\n            shape=[10, 5],\r\n            trainable=True)\r\nprint(embeddings)\r\n...\r\n```\r\n>>>\r\nMirroredVariable:{'/replica:0/task:0/device:**CPU**:0': <tf.Variable 'fh_universal_transformer_encoder/word_symbol_modality_1917597_300/input_emb/embeddings/weights:0' shape=(1917597, 300)\r\n\r\n\r\n", "comments": ["@etragas-fathom -  pointing the ops to cpu:0 may not prevent tensorflow from initializing the GPU device. \r\n \r\nTo remove GPU from consideration completely, run   \r\n\r\n  export CUDA_VISIBLE_DEVICES=\r\n\r\n  config = tf.ConfigProto(log_device_placement=True)\r\n  config.gpu_options.per_process_gpu_memory_fraction=0.3 # don't hog all vRAM\r\n  config.operation_timeout_in_ms=50000   # terminate on long hangs\r\n  sess = tf.InteractiveSession(\"\", config=config)", "Hey @harshini-gadige I appreciate the response. However, my goal isn't to put all my variables on CPU, but instead have a mix of variables on gpu and cpu while using MirroredStrategy. Is that supported by MirrorStrategy?", "@josh11b  Any inputs on this ?", "MirroredStrategy controls variable placement when you use it, and there is no currently supported API for overriding this. It might work to use strategy.extended.colocate_vars_with(...) to override var placement, but this is not a current API promise.", "@yuefengz is thinking about adding API support for embeddings that would put them on the host CPU, assigning this to him.", "Thank you @josh11b for clarifying things and @yuefengz for picking up the embedding support. Looking forward to the solution! ", "We have a [RFC for embeddings](https://github.com/yuefengz/community/blob/master/rfcs/20190116-embedding-partitioned-variable.md) and will start the implementation soon. So closing this issue.", "Hi, @yuefengz \r\nI checked your document and latest code. I'm trying to implement a distributed trainer using MirroredStrategy, which replicates network in each devices. But I would like to force the embedding layers in CPU since it's too big to store in device memory in my task. \r\n\r\nIs this supported now? If this is not supported now, could you please guild me how can I implement such mechanism with Estimator and DistributedStrategy? It seems like that I need to implement a new Strategy. I find you're planing to implement a HybridStrategy. \r\n\r\nThanks!", "@yuefengz Any comments?"]}, {"number": 24325, "title": "Add: MKL switch to generate CPU model with intel MKL-DNN optimizations", "body": "This PR adds `mkl` switch to generate CPU `half_plus_two` model with intel MKL-DNN optimizations.", "comments": ["@gautamvasudevan @netfs please review this PR, when you get time. Added this PR on behalf of  @tonyreina "]}, {"number": 24324, "title": "Missing CPU AVX2 FMA and registered OpKernels", "body": "I'm trying to run a known working model but my tensorflow configuration is not setup right.\r\n\r\nI'm using Ubuntu 18, CUDA Version 9.0.176\r\n\r\nI'm trying to figure out what I should do to get these error messages to resolve.\r\nI figure I have to rebuild tensorflow with these supporting features.\r\n\r\n`No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU,XLA_CPU], Registered kernels:\r\n  <no registered kernels>`\r\n\r\n`Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA`\r\n\r\n\r\nIs this the right track?\r\n\r\nSo far I've been trying to build Tensorflow v1.19 with GPU support as thats what the code instructs me to use. I also ready I need bazel v15 as that is the most tested and used version of bazel in the tensorflow community.\r\n\r\nCan anybody provide insight into these error messages and guidance on how to get the right version of tensorflow setup so I may proceed with my work?\r\n\r\n\r\n**Full-Trace:**\r\n~~~\r\nTensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _run_fn\r\n    self._extend_graph()\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1352, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU,XLA_CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[{{node cu_dnnlstm_1/CudnnRNN}} = CudnnRNN[T=DT_FLOAT, direction=\"unidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode=\"lstm\", seed=87654321, seed2=0](cu_dnnlstm_1/transpose, cu_dnnlstm_1/ExpandDims_1, cu_dnnlstm_1/ExpandDims_2, cu_dnnlstm_1/concat_1)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n~~~\r\n\r\nComment: This error is triggered when I load a pretrained `.h5` model... The model likely calls for OpKernels that are not setup with tensorflow or Cuda/Cudnn ... not sure though\r\n\r\n~~~\r\nTraceback (most recent call last):\r\n  File \"script.py\", line 105, in <module>\r\n    model_best = load_model('/home/ubuntu/git/androidmodel/mod.h5')\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py\", line 419, in load_model\r\n    model = _deserialize_model(f, custom_objects, compile)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py\", line 287, in _deserialize_model\r\n    K.batch_set_value(weight_value_tuples)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2470, in batch_set_value\r\n    get_session().run(assign_ops, feed_dict=feed_dict)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 199, in get_session\r\n    [tf.is_variable_initialized(v) for v in candidate_vars])\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU,XLA_CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[node cu_dnnlstm_1/CudnnRNN (defined at /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:922)  = CudnnRNN[T=DT_FLOAT, direction=\"unidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode=\"lstm\", seed=87654321, seed2=0](cu_dnnlstm_1/transpose, cu_dnnlstm_1/ExpandDims_1, cu_dnnlstm_1/ExpandDims_2, cu_dnnlstm_1/concat_1)]]\r\n\r\nCaused by op 'cu_dnnlstm_1/CudnnRNN', defined at:\r\n  File \"script.py\", line 82, in <module>\r\n    model.add(CuDNNLSTM(40, return_sequences=True, stateful=False, input_shape=(None, data_dim)))\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py\", line 165, in add\r\n    layer(x)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 532, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 457, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/layers/cudnn_recurrent.py\", line 90, in call\r\n    output, states = self._process_batch(inputs, initial_state)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/layers/cudnn_recurrent.py\", line 517, in _process_batch\r\n    is_training=True)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 1544, in __call__\r\n    input_data, input_h, input_c, params, is_training=is_training)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 1435, in __call__\r\n    seed=self._seed)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 922, in _cudnn_rnn\r\n    outputs, output_h, output_c, _ = gen_cudnn_rnn_ops.cudnn_rnn(**args)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py\", line 116, in cudnn_rnn\r\n    is_training=is_training, name=name)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU,XLA_CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[node cu_dnnlstm_1/CudnnRNN (defined at /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:922)  = CudnnRNN[T=DT_FLOAT, direction=\"unidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode=\"lstm\", seed=87654321, seed2=0](cu_dnnlstm_1/transpose, cu_dnnlstm_1/ExpandDims_1, cu_dnnlstm_1/ExpandDims_2, cu_dnnlstm_1/concat_1)]]\r\n~~~", "comments": ["Bump", "Apologies for the delay in response. \r\nCompiling TF from sources with AVX flags will help you to make TF faster and solve the warning message related to CPU instructions but its unlikely to solve the InvalidArgumentError message. Is this still an issue for you? If yes can you please provide following info. Thanks!\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24323, "title": "undefined symbol: _ZN6tflite12tensor_utils27NeonSymmetricQuantizeFloatsEPKfiPaPfS4_S4_\\n", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspberry Pi 3B+ Linux\r\n- TensorFlow installed from (source or binary): installed in Python using pip3\r\n- TensorFlow version (use command below):  1.11.0\r\n- Python version: 3.5.3\r\n\r\n**Describe the current behavior**\r\nTrying to convert a frozen graph file (.pb) to a TensorFlow Lite FlatBuffer file (.tflite),  Executing this command from the Linux command line:\r\ncd ~/.local/bin\r\n./tflite_convert --output_file=/home/pi/sols/demo/src/image_classification/network/fruit_models/fruit_model.tflite --graph_def_file=/home/pi/sols/demo/src/image_classification/network/fruit_models/frozen_graph.pb --input_arrays=X --output_arrays=softmax\r\n\r\nIt gives me this error:\r\nRuntimeError: TOCO failed see console for info.\r\nb'/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module \\'tensorflow.python.framework.fast_tensor_util\\' does not match runtime version 3.5\\n  return f(*args, **kwds)\\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\\n  return f(*args, **kwds)\\nTraceback (most recent call last):\\n  File \"/bin/toco_from_protos\", line 7, in <module>\\n    from tensorflow.contrib.lite.toco.python.toco_from_protos import main\\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/toco/python/toco_from_protos.py\", line 22, in <module>\\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/toco/python/tensorflow_wrap_toco.py\", line 28, in <module>\\n    _tensorflow_wrap_toco = swig_import_helper()\\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/toco/python/tensorflow_wrap_toco.py\", line 24, in swig_import_helper\\n    _mod = imp.load_module(\\'_tensorflow_wrap_toco\\', fp, pathname, description)\\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\\n    return load_dynamic(name, filename, file)\\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\\n    return _load(spec)\\nImportError: /home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/toco/python/_tensorflow_wrap_toco.so: undefined symbol: _ZN6tflite12tensor_utils27NeonSymmetricQuantizeFloatsEPKfiPaPfS4_S4_\\n'\r\n\r\nPlease help!\r\n\r\n", "comments": ["Can use the latest tf-nightly build and test again? Thanks!", "How to install tf-nightly?  \r\n\r\nWhen I try to install, here is what I am getting:\r\n\r\npi@raspberrypi:~ $ pip3 install --user --upgrade tf-nightly\r\nCollecting tf-nightly\r\n  Could not find a version that satisfies the requirement tf-nightly (from versions: )\r\nNo matching distribution found for tf-nightly\r\n\r\nAny idea?", "You can try setting virtual env and test :\r\n> virtualenv -p python3 venv-tf-nightly\r\nsource venv-tf-nightly/bin/activate\r\npip3 install tf-nightly", "Is there a tf-nightly wheel file (.whl) that I could just download and install in my Raspberry Pi?", "Installing tf-nightly in virtual environment in Raspberry Pi (as you suggested) fails as well:  \r\n\r\n(py_env) pi@raspberrypi: $ pip3 install tf-nightly\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\nCollecting tf-nightly\r\n  Could not find a version that satisfies the requirement tf-nightly (from versions: )\r\nNo matching distribution found for tf-nightly\r\n", "Does anybody have any idea about the possible reasons for the undefined symbol error _ZN6tflite12tensor_utils27NeonSymmetricQuantizeFloatsEPKfiPaPfS4_S4_?", "@akotlarsky probably the same problem as #21574. That is, NEON stuff was not linked in.", "Since the related #21574 has been closed, closing this one as well. Please reopen if you're still hitting this problem."]}]