[{"number": 12062, "title": "Update monitors_test.py", "body": "Move from contrib to training", "comments": ["Can one of the admins verify this patch?", "@alanyee, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ispirmustafa, @tensorflower-gardener and @jart to be potential reviewers.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 12061, "title": "the efficiency of StagingArea on gpu", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:v1.2.0-2809-g7add4e6 1.2.1-rc2\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**:0.5.2\r\n- **CUDA/cuDNN version**:8.0/6.0\r\n- **GPU model and memory**:8 1080ti cards, each 11172MB\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\nI test the efficiency of StagingArea(as described in https://www.tensorflow.org/performance/performance_models) on GPU compared to direct feed_dict to session.run input method.\r\nThe script shows these two methods have almost same speed, all about 34 seconds. But if I put all inputs to StagingArea first, the script just need 23 seconds to run. StagingArea method can't hide data copy time.\r\n\r\n### Source code / logs\r\nhttps://gist.github.com/suiyuan2009/99fb21567e7ce3716ae25772754c7543\r\n", "comments": ["After a quick glance at your code, one thing is that your code doesn't use latency hiding.\r\nYou still use feed_dict in the critical loop with `sess.run([put_op, loss], feed_dict=xxx)`. Even though it is fed into staging area, Python->TF copy will still have to finish before any op really runs.\r\n\r\nIn tensorpack we did observe good performance improvement by using StagingArea for input in multi-gpu training (although not in every case, but sometimes it helps).", "@ppwwyyxx , at present, for some reason, we can't use tfrecord input pipeline...", "so there is no hack method if we insist on using `feed_dict`.", "Feed latency should be hidden if you use `feed_dict` in a different thread.", "I'll try thread method again. I use a thread to enqueue inputs in my first version code, but it didn't help. I'll use a fixed size queue on cpu and a staging area on gpu to see if I can beat GIL.", "excited! direct feed_dict method `30 seconds` vs no fixed size queue before staging area method `23 seconds` vs fixed size queue before staging area method `22.5 seconds` vs put all inputs on gpu before run `21 seconds`. GIL is not the problem...", "@suiyuan2009 have you compared with using queues only (no stagingarea)? Recently I seem to observe that StagingArea doesn't help much on top of queues."]}, {"number": 12060, "title": "Tensorflow installation error", "body": "Hi I installed tensorflow for cpu and while trying to run command to check the version of tensor i am prompted error message.  Please help me to get this resolved\r\n\r\nPlease see below the log:\r\n\r\nMicrosoft Windows [Version 6.2.9200]\r\n(c) 2012 Microsoft Corporation. All rights reserved.\r\n\r\nC:\\WINDOWS\\system32>python\r\nPython 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AM\r\nD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in impor\r\nt_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.p\r\ny\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in impor\r\nt_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", lin\r\ne 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.p\r\ny\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in impor\r\nt_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.p\r\ny\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in impor\r\nt_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_st\r\narted/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>", "comments": ["I see you are trying to install on Windows, @mrry has a great installation troubleshooter script that can help pinpoint issues available here: [mrry's Self-check Script](https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c)", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "Installing the [Microsoft Visual C++ 2015 Redistributable Update 3](https://www.microsoft.com/en-us/download/details.aspx?id=53587) has solved my issue"]}, {"number": 12059, "title": "tfdbg does not work with sparse tensors", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.2.0-5-g435cdfc', '1.2.1')\r\n- **Python version**:  Python 2.7.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0.61 / 5.1.10\r\n- **GPU model and memory**: Nvidia  TITAN X (Pascal) 12G\r\n- **Exact command to reproduce**: python sparse_debug.py --debug\r\n\r\n### Describe the problem\r\nThere seems to be a bug using tensorflow debugger with sparse tensors. Below is just a simple example  it fails when run with or without the ` --debug` option. It works when LocalCLIDebugWrapperSession line is removed. This prevents the use of the debugger while using sparse_placeholders, unless I'm missing something.\r\n[This issue](https://github.com/tensorflow/tensorflow/issues/6110) also reports the same error, but isn't related to tfdbg.\r\n\r\n\r\n### Source code / logs\r\nsparse_debug.py\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python import debug as tf_debug\r\na=tf.sparse_placeholder(tf.float32,shape=(None,5,5),name='tensor1')\r\nb=tf.sparse_placeholder(tf.float32,shape=(None,5,5),name='tensor2')\r\nadd=tf.sparse_add(a,b)\r\n\r\nsess = tf.Session()\r\nsess = tf_debug.LocalCLIDebugWrapperSession(sess)\r\n\r\na_val=([[0,0,1],[0,0,2]],[1,2],(1,5,5))\r\nb_val=([[0,0,1],[0,0,2]],[1,2],(1,5,5))\r\nres=sess.run(add,feed_dict={a:a_val,b:b_val})\r\n```\r\nTraceback\r\n\r\n```\r\n  File \"sparse_debug.py\", line 12, in <module>\r\n    res=sess.run(add,feed_dict={a:a_val,b:b_val})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/wrappers/framework.py\", line 411, in run\r\n    self._run_call_count))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py\", line 189, in on_run_start\r\n    request.feed_dict)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py\", line 487, in _update_run_calls_state\r\n    self._tensor_filters)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/cli_shared.py\", line 273, in get_run_start_intro\r\n    feed_dict_lines.append(feed_key.name)\r\nAttributeError: 'SparseTensor' object has no attribute 'name'\r\n```\r\n", "comments": ["Thanks for reporting this issue. I can reproduce it at HEAD. Will work on it.", "Why is it closed?\r\n\r\nThe above example gives me the same error. \r\n\r\nI am using Python 3.5.2.\r\n\r\nBelow is what shows after running `tf_env_collect.sh`\r\n\r\n== cat /etc/issue ===============================================\r\nLinux test-machine 4.4.0-79-generic #100-Ubuntu SMP Wed May 17 19:58:14 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux test-machine 4.4.0-79-generic #100-Ubuntu SMP Wed May 17 19:58:14 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.6)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n\r\n", "@xiaohan2012 I can confirm that. Sorry about the inconvenience. Will push a fix soon.", "This is a regression related to https://github.com/tensorflow/tensorflow/commit/73a8755cdfd7bd74d626eee35cc58a3ddb2198e6#diff-d04e2893c412aa1a1765a1c8b53158a6", "Sorry, I don't understand. Is that a fix? ", "No. That was just a note about why this bug occurred again. Stay tuned for a fix.", "is it fixed?", "@lnisre yes. The referred commit above should have fixed this issue. This fix is available in the latest python pip release of TensorFlow."]}, {"number": 12058, "title": "Couldn't restore attention_ocr checkpoint via saver", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMacOS Sierra 10.12.5\r\n- **TensorFlow installed from (source or binary)**:\r\ncreated environment in conda, then installed tf via pip\r\n- **TensorFlow version (use command below)**:\r\n('v1.2.0-rc2-21-g12f033d', '1.2.0')\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\nNot installed\r\n- **CUDA/cuDNN version**:\r\nNo GPU supported\r\n- **GPU model and memory**:\r\nNo\r\n- **Exact command to reproduce**:\r\npython test.py\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nHi, I am trying to use attention_ocr in my own data, a simple test is firstly implemented.\r\naccording to the instructions from [How to use a pre-trained model](https://github.com/tensorflow/models/tree/master/attention_ocr#how-to-use-a-pre-trained-model) but somehow failed in restoring the checkpoints without explicit error info.\r\n\r\nThe following condition has been checked:\r\n1. checkpoint files are complete\r\n2. right path to the checkpoint\r\n3. graphs have been imported from .meta\r\n4. Nothing changes after run saver.train.restore() (predictions remained the same)\r\n5. No error or hints provided\r\n\r\nThe checkpoint was downloaded as suggested:\r\n```\r\nwget http://download.tensorflow.org/models/attention_ocr_2017_05_17.tar.gz\r\ntar xf attention_ocr_2017_05_17.tar.gz\r\ncd attention_ocr_2017_05_17\r\nls -lh\r\n```\r\n```\r\ntotal 64216\r\n-rw-r-----  1 liuhuichuan  staff    14M  5 18 04:07 model.ckpt-399731.data-00000-of-00001\r\n-rw-r-----  1 liuhuichuan  staff   8.2K  5 18 04:07 model.ckpt-399731.index\r\n-rw-r-----  1 liuhuichuan  staff    17M  5 18 04:07 model.ckpt-399731.meta\r\n```\r\n\r\nThe graphs were successfully imported from .meta, but somehow saver couldn't recognize .index and .data files: \r\n```\r\nprint os.path.exists('../attention_ocr_2017_05_17/model.ckpt-399731.data-00000-of-00001')\r\nprint os.path.exists('../attention_ocr_2017_05_17/model.ckpt-399731.index')\r\nprint tf.train.get_checkpoint_state('../attention_ocr_2017_05_17/model.ckpt-399731')\r\n```\r\nreturns:\r\n```\r\nTure\r\nTure\r\nNone\r\n```\r\nA very simple test is attempted:\r\n```\r\nsaver = tf.train.import_meta_graph('../attention_ocr_2017_05_17/model.ckpt-399731.meta')\r\nwith tf.Session() as sess:\r\n    print os.path.exists('./attention_ocr_2017_05_17/model.ckpt-399731.meta')\r\n    print tf.train.get_checkpoint_state('../attention_ocr_2017_05_17/model.ckpt-399731')\r\n    saver.restore(sess,'../attention_ocr_2017_05_17/model.ckpt-399731')\r\n```\r\nreturns no error, but still not restored:\r\n```\r\n2017-08-06 16:24:41.346086: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nTrue\r\n2017-08-06 16:24:41.346124: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-06 16:24:41.346129: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-06 16:24:41.346133: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nNone\r\nINFO:tensorflow:Restoring parameters from ../attention_ocr_2017_05_17/model.ckpt-399731\r\nINFO 2017-08-06 16:24:41.000354: tf_logging.py: 82 Restoring parameters from ../attention_ocr_2017_05_17/model.ckpt-399731\r\n\r\nProcess finished with exit code 0\r\n```\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nfrom tensorflow.python.platform import flags\r\nimport matplotlib.image as mpimg\r\nimport common_flags\r\n\r\nFLAGS = flags.FLAGS\r\ncommon_flags.define()\r\n\r\n# yapf: disable\r\nflags.DEFINE_integer('num_batches', 100,\r\n                     'Number of batches to run eval for.')\r\n\r\nflags.DEFINE_string('eval_log_dir', '/tmp/attention_ocr/eval',\r\n                    'Directory where the evaluation results are saved to.')\r\n\r\nflags.DEFINE_integer('eval_interval_secs', 60,\r\n                     'Frequency in seconds to run evaluations.')\r\n\r\nflags.DEFINE_integer('number_of_steps', None,\r\n                     'Number of times to run evaluation.')\r\n\r\n\r\n# fake a simple test image\r\n\r\nraw_image_data = mpimg.imread('A4A8A5910A355-cvt.jpg').reshape((1,150,600,3))\r\nimages_placeholder = tf.placeholder(tf.float32,shape = (1,150, 600, 3),name='img_data')\r\n\r\nif not tf.gfile.Exists(FLAGS.eval_log_dir):\r\n    tf.gfile.MakeDirs(FLAGS.eval_log_dir)\r\ndataset = common_flags.create_dataset(split_name=FLAGS.split_name)\r\nmodel = common_flags.create_model(dataset.num_char_classes,\r\n                                    dataset.max_sequence_length,\r\n                                    dataset.num_of_views, dataset.null_code)\r\nendpoints = model.create_base(images_placeholder, labels_one_hot=None)\r\n\r\n# start loading attention_ocr model\r\n\r\nsaver = tf.train.import_meta_graph('../attention_ocr_2017_05_17/model.ckpt-399731.meta')\r\n\r\nwith tf.Session() as sess:\r\n    # init without checkpoint variables and predict\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n    predictions = sess.run(endpoints.predicted_chars, feed_dict={images_placeholder: raw_image_data})\r\n    print predictions\r\n\r\n    # restore from checkpoint then predict\r\n    print os.path.exists('./attention_ocr_2017_05_17/model.ckpt-399731.meta')\r\n    print tf.train.get_checkpoint_state('../attention_ocr_2017_05_17/model.ckpt-399731')\r\n    saver.restore(sess,'../attention_ocr_2017_05_17/model.ckpt-399731')\r\n    predictions = sess.run(endpoints.predicted_chars, feed_dict={images_placeholder: raw_image_data})\r\n    print predictions\r\n```\r\n\r\n```\r\nINFO 2017-08-06 16:48:44.000554: fsns.py: 130 Using FSNS dataset split_name=train dataset_dir=/Users/liuhuichuan/PycharmProjects/models/attention_ocr/python/datasets/data/fsns\r\nDEBUG 2017-08-06 16:48:44.000556: model.py: 343 images: Tensor(\"img_data:0\", shape=(1, 150, 600, 3), dtype=float32)\r\nDEBUG 2017-08-06 16:48:44.000561: model.py: 348 Views=4 single view: Tensor(\"AttentionOcr_v1/split:0\", shape=(1, 150, 150, 3), dtype=float32)\r\nDEBUG 2017-08-06 16:48:44.000561: model.py: 191 Using final_endpoint=Mixed_5d\r\nDEBUG 2017-08-06 16:48:46.000492: model.py: 191 Using final_endpoint=Mixed_5d\r\nDEBUG 2017-08-06 16:48:47.000546: model.py: 191 Using final_endpoint=Mixed_5d\r\nDEBUG 2017-08-06 16:48:48.000684: model.py: 191 Using final_endpoint=Mixed_5d\r\nDEBUG 2017-08-06 16:48:49.000862: model.py: 354 Conv tower: Tensor(\"AttentionOcr_v1/conv_tower_fn/INCE/InceptionV3/Mixed_5d/concat:0\", shape=(1, 16, 16, 288), dtype=float32)\r\nDEBUG 2017-08-06 16:48:49.000862: model.py: 357 Conv tower w/ encoded coordinates: Tensor(\"AttentionOcr_v1/conv_tower_fn/INCE/InceptionV3/Mixed_5d/concat:0\", shape=(1, 16, 16, 288), dtype=float32)\r\nDEBUG 2017-08-06 16:48:49.000869: model.py: 360 Pooled views: Tensor(\"AttentionOcr_v1/pool_views_fn/STCK/Reshape:0\", shape=(1, 1024, 288), dtype=float32)\r\nDEBUG 2017-08-06 16:48:49.000869: sequence_layers.py: 421 Use AttentionWithAutoregression as a layer class\r\nDEBUG 2017-08-06 16:48:53.000099: model.py: 363 chars_logit: Tensor(\"AttentionOcr_v1/sequence_logit_fn/SQLR/concat:0\", shape=(1, 37, 134), dtype=float32)\r\n2017-08-06 16:50:05.943512: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-06 16:50:05.943528: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-06 16:50:05.943532: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-06 16:50:05.943537: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nINFO:tensorflow:Restoring parameters from ../attention_ocr_2017_05_17/model.ckpt-399731\r\nINFO 2017-08-06 16:50:29.000024: tf_logging.py: 82 Restoring parameters from ../attention_ocr_2017_05_17/model.ckpt-399731\r\n[[  0   0   0 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123\r\n  123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123\r\n  123]]\r\nTrue\r\nNone\r\n[[  0   0   0 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123\r\n  123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123\r\n  123]]\r\n\r\nProcess finished with exit code 0\r\n```\r\n", "comments": []}, {"number": 12057, "title": "tf.contrib.distributions - Additional Distributions", "body": "Are there plans to add additional distributions in the tf.contrib.distributions module like Scaled Inverse Chi-Sq, LKJ, Gumbel etc?  Could mirror the distribution list supported by STAN\r\n", "comments": ["@jvdillon @langmore @rsepassi Can you comment on this?  Thanks!", "Sorry for our delayed response. We're working on adding a batch of new distributions. Ill make sure these (and Stan's) are on our list.", "Thanks"]}, {"number": 12056, "title": "Implement CRF decode (Viterbi decode) for tensor", "body": "Currently, Tensorflow doesn't support CRF decoding (Viterbi decoding) for tensor. Although `tf.contrib.crf.viterbi_decode` function can do CRF decoding, it can only be used at test time, since it accepts Numpy arrays as inputs.\r\n\r\nImplementing CRF decoding for tensor benefits us a lot, as this makes our model more portable (e.g. we can freeze model, save it to a `.pb` file and then load it in `Golang` at test time).\r\n\r\nThis PR implements CRF decoding for tensor and adds test code for it. \r\n\r\nThis PR's change is compatible with current API.\r\n", "comments": ["@betterenvi, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @jart and @drpngx to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@drpngx do you feel like reviewing this?", "Let me know when it's ready for review again. Thanks!", "@drpngx \r\n\r\nI have removed the non-deterministic example and added a deterministic one. In my test, `crf_decode` has passed this test.\r\n\r\nBy the way, `crf_decode` also has passed the previous non-deterministic example (comparing with `viterbi_decode`).", "Jenkins, test this please.", "Cool! Thanks for getting this reviewed so quickly @drpngx !"]}, {"number": 12055, "title": "Tensorboard Error on windows ", "body": "I want to use tensorboard on windows but when I am going to start it, its shows this error\r\n```\r\n(C:\\Users\\anura\\Anaconda3) C:\\Users\\anura>tensorboard --logdir foo:C:\\Users\\anura\\Desktop\\Python\\Tensorboard\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\anura\\Anaconda3\\Scripts\\tensorboard-script.py\", line 3, in <module>\r\n    import tensorflow.tensorboard.tensorboard\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\tensorflow\\tensorboard\\tensorboard.py\", line 33, in <module>\r\n    from tensorflow.tensorboard.backend import application\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\tensorflow\\tensorboard\\backend\\application.py\", line 47, in <module>\r\n    from tensorflow.tensorboard.plugins.projector import projector_plugin\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\tensorflow\\tensorboard\\plugins\\projector\\projector_plugin.py\", line 28, in <module>\r\n    from tensorflow.contrib.tensorboard.plugins.projector import PROJECTOR_FILENAME\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\", line 30, in <module>\r\n    from tensorflow.contrib import factorization\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\factorization\\__init__.py\", line 24, in <module>\r\n    from tensorflow.contrib.factorization.python.ops.gmm import *\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\factorization\\python\\ops\\gmm.py\", line 27, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators import estimator\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\__init__.py\", line 87, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\__init__.py\", line 23, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\__init__.py\", line 25, in <module>\r\n    from tensorflow.contrib.learn.python.learn import estimators\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\__init__.py\", line 297, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn.py\", line 29, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn_linear_combined.py\", line 31, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators import estimator\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 49, in <module>\r\n    from tensorflow.contrib.learn.python.learn.learn_io import data_feeder\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_io\\__init__.py\", line 21, in <module>\r\n    from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_io\\dask_io.py\", line 26, in <module>\r\n    import dask.dataframe as dd\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\__init__.py\", line 3, in <module>\r\n    from .core import (DataFrame, Series, Index, _Frame, map_partitions,\r\n  File \"C:\\Users\\anura\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\core.py\", line 36, in <module>\r\n    pd.computation.expressions.set_use_numexpr(False)\r\nAttributeError: module 'pandas' has no attribute 'computation'\r\n\r\n(C:\\Users\\anura\\Anaconda3) C:\\Users\\anura>\r\n```\r\nwhen is run localhost:6006 in my chrome browser \r\n\r\n![screenshot 140](https://user-images.githubusercontent.com/15853647/28995526-f9aa1742-7a08-11e7-92d6-f87038c9ea7a.png)\r\n", "comments": ["Please resubmit and pay attention to the issue template, including the version of TensorFlow and how you installed it (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "In addition to filling out the issue template, please file tensorboard-related issues under the tensorboard repository:\r\nhttps://github.com/tensorflow/tensorboard/issues"]}, {"number": 12054, "title": "Feature request: non-blocking enqueue operations", "body": "As for version 1.2, queue operations involving enqueuing can block if a queue is full or empty. This is very useful for designing input threads feeding the queue from static datasets as it suspends the thread until it can proceed doing its work. However, this behavior might not be desirable or acceptable if instead the data being fed comes from an asynchronous and continuously changing live source at a best effort basis, because blocking the thread implies missing the continuity of the source.\r\n\r\nFor example, let's say we have a distributed reinforcement learning environment where an external game plays asynchronously and a thread tries its best to read the most recent state of the game and input the actions to take. As a result, this thread keeps producing training batches that are enqueued in a PaddingFIFOQueue, which are consumed by a separate trainer.\r\n\r\nIn this case, if for any reason the trainer takes too long or has an un expected peak in the number of training batches it receives, the queue will fill and the producer thread(s) will block. Since the source of data (the external game) keeps playing, this means that the next time the producer threads unblock they will probably have lost temporal coherency on the game. In this case, dropping a training batch would be more desirable than blocking the thread until such batch can be enqueued.\r\n\r\nFor this reason, I'd like to propose the following feature: a new optional argument to the enqueuing methods (enqueue and enqueue_many) that allows to immediately return when the queue is full instead of blocking the thread. This argument would default to the existing behavior (e.g., 'non_blocking=False'), so no code updates are required.\r\n\r\nSignaling if/how many elements were successfully enqueued or failed to do so is likely a desirable output of a non-blocking enqueue operation. However, implementing it might not be trivial without altering the function return signature. If we find this is something we should provide, it might be better to instead of adding a new argument simply add new non-blocking versions of the enqueue ops that return a (op, num_elements_discarded) tuple.\r\n\r\nAny comments or suggestions are most welcome.", "comments": ["From implementation standpoint, behavior like this could be implemented by having enqueue kernel return bad status, so that entire session.run call terminates immediately with exception.\r\n\r\nHowever I wonder if this functionality could be implemented at the client level. IE, a client can check how much capacity is left in the queue, and if it's too small, skip the enqueue call altogether. You can issue enqueue calls in their own Python thread so that these calls are asynchronous. You can include timestamps in your batches and drop batches which are too old. You can use timeout feature to drop any enqueue call that blocks for more than x milliseconds", "Thanks for your suggestions!\r\n\r\nI've considered them one by one, but as far as I can tell they don't fully solve the problem. Some can offer a partial workaround, but with limitations.\r\n\r\n- **Issuing enqueue calls in their own Python thread**: that would be a straightforward solution to the problem, but unfortunately it's not available in this case because the problem enforces us to not block the thread generating the data. We can create yet a new thread to run the enqueue ops, but then we have the very same problem: we need to feed data to the thread doing the enqueue without blocking the thread generating it.\r\n\r\n- **Checking how much capacity is left, then skipping enqueuing conditionally**: this should work for as long as we have a single thread feeding the queue. If we happen to have multiple threads, however, we would need some kind of synchronization mechanism to ensure no other threads enqueues between our check and our enqueue op. Is there anything like that already I can use?  (BTW, a minor note: I found myself missing a queue capacity property when giving this a try.)\r\n\r\n- **Including timestamp in the batches and dropping the ones that are too old**: I'm not sure if I understood this suggestion correctly. In the enqueuing thread all data is recent, and in the dequeuing side the thread is busy running training steps. The only way I can think to do this would be to have some intermediate dequeuing thread that goes over the queue contents and filters them out. That would require having a secondary queue. Still, that won't help if there's a peak of finished short episodes since the queue would fill but all data would be very recent. I wouldn't ensure blocking cannot happen either.\r\n\r\n- **Using the timeout feature to drop enqueue calls that block for more than x milliseconds**: I actually considered doing this before writing the feature request. While this might work, it transforms the problem into finding a machine-dependent time threshold that is as low as possible for not blocking the thread (we're talking about producing data at multiple frames per second, blocking for milliseconds does matter) and high enough to ensure we can still compute the training batches we enqueue when there's enough space in the queue. Since I expect this to be distributed and run across different kinds of machines with possibly different kinds of work loads, having a machine-dependent threshold is something I'd rather avoid as much as possible.\r\n\r\nIf there's something I'm missing or something I misunderstood please do let me know. Any suggestions or ideas are most welcome.", "For bullet 2, there's a practical upper limit on the number of items that could be enqueued by other threads, say k, so the trick would be to have a spare buffer of k+1, and skip the enqueue when there's a risk of freezing. For OpenAI universe project we had to deal with realtime data and managed to deal with stale data using combination of tricks like above\r\n\r\nI guess the issue is -- how much work is would add it to implement/maintain such op vs. how much work it would remove to need to do workarounds like above. \r\n\r\nPerhaps one way to proceed would be for the motivated user to implement a user_op that implements a queue that throws exception instead of blocking, since that would give a sense how complicated adding such op is\r\n\r\ncc @mrry in case he has any more comments", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I'm fine with closing this, seems vague or hard to implement", "Catching up on old issues: I'm inclined to freeze the queue API as-is, with a view to deprecating it. Most queue-based use cases can be satisfied by the new `tf.data` API, and more advanced use cases (such as this one) would be more naturally implemented using standard Python objects and Eager mode.", "While I'm not against closing this, I disagree with the idea that the new tf.data API should deprecate the queue API. While tf.data does a pretty good job to simplify input pipelines, queues can also be used to perform more complex tasks like synchronizing distributed jobs within the TF graph and creating message pipelines across them.\r\n\r\nFor example, a tf.FIFOQueue can also be used as a distributed semaphore that locks in two ways: when acquiring with value (size) zero and when releasing with value of N (capacity). This can be used to perform mutually exclusive operations in different jobs if needed, creating distributed barriers and so on.\r\n\r\nSimilarly tf.FIFOQueue and tf.PaddingFIFOQueue can be used to implement a tensor-based messaging system across jobs.\r\n\r\nNote that using python synchronization mechanisms is not always an alternative. For example, in my use case all I can use are TF operations restored from a graph def that I call using the C API in Windows. In that case being able to do all the above within the TF graph is a must.\r\n\r\nSo, please, consider not deprecating queues even if their API is frozen. They do have more uses than just setting up an input pipeline.", "Came here to request non-blocking enqueue as well. Currently check the size of the queue as a proxy which doesn't seem ideal.\r\n\r\nBut have to add yet another request to not deprecate the queue API. We have some applications where it's really useful. Beyond what @leandro-gracia-gil already mentioned some of the simplest implementations of fast-wavenet rely on the `tf.FIFOQueue`. Also seen a use for it in experience-replay like use-cases. However, the QueueRunner etc. I haven't had any use for."]}, {"number": 12053, "title": "Update monitors.py", "body": "Replace deprecated SummaryWriter and SummaryWriterCache with FileWriter and FileWriterCache", "comments": ["Can one of the admins verify this patch?", "@alanyee, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @ilblackdragon and @ispirmustafa to be potential reviewers.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@alanyee please rebase to resolve merge conflicts.", "@tensorflow-jenkins test this please"]}, {"number": 12052, "title": "Upgrade to CuDNN 7 and CUDA 9", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows Server 2012\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0-rc1\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA V8.0.44, CuDNN 6.0\r\n- **GPU model and memory**: Nvidia GeForce GTX 1080 Ti, 11 GB\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nPlease upgrade TensorFlow to support CUDA 9 and CuDNN 7. Nvidia claims this will provide a 2x performance boost on Pascal GPUs.", "comments": ["@tfboyd do you have any comments on this?", "cuDNN 7 is still in preview mode and is being worked on.  We just moved to cuDNN 6.0 with 1.3, which should go final in a couple weeks.  You can download cuDNN 1.3.0rc2 if you are interested in that.  I have not compiled with cuDNN 7 or CUDA 9 yet.  I have heard CUDA 9 is not easy to install on all platforms and only select install packages are available.  When the libraries are final we will start the final evaluation.  NVIDIA has also just started sending patches to the major ML platforms to support aspects of these new libraries and I suspect there will be additional work.\r\n\r\nEdit: I meant to say CUDA 9 is not easy to install on all platforms and instead said cuDNN.  I also changed sure there will be work to I suspect there will be additional work.  The rest of my silly statement I left, e.g. I did not realize cuDNN 7 went live yesterday.    ", "Not saying how you should read the website.  But the 2x faster on pascal looks to be part of the CUDA 8 release.  I suppose it depends on how you read the site.  NVIDIA has not mentioned to us that CUDA 9 is going to speed up Pascal by 2x (on everything) and while anything is possible, I would not expect that to happen.  \r\n\r\nhttps://developer.nvidia.com/cuda-toolkit/whatsnew\r\n\r\nThe site is a little confusing but I think the section you are quoting is nested under the CUDA 8.  I only mention this so you do not have unrealistic expectations for their release.  For Volta there should be some great gains from what I understand and I think (I do not now for sure) people are just getting engineering samples of Volta to start high level work to get ready for the full release.  ", "@tfboyd cuDNN 7 is no longer in preview mode as of yesterday. It has been officially released for both CUDA 8.0 and CUDA 9.0 RC.", "Ahh I missed that.  Thanks @sclarkson and sorry for the wrong info.", "I will certainly try it because finally gcc 6 is supported by CUDA 9 and Ubuntu 17.04 comes with it.", "If you have luck let the thread know.  I am personally just starting to\nfully test cuDNN 6 (Internally it has been tested a lot but I have not been\nusing it personally).  I am often slow to upgrade to the latest stuff.  My\nguess is you may not see any real change with cuDNN 7 until everything gets\npatched to use the latest APIs.  I want to stress again that I am wrong all\nof the time.  What I have seen as an outsider is the new cuDNN versions add\nnew methods/APIs.  Some are interesting and some are not immediately\nuseful.  Then those APIs get exposed via the TensorFlow API or just used\nbehind the scenes to make existing methods faster.  My very high level\nunderstanding is cuDNN 7 + CUDA 9 will enhance FP16 support with a focus on\nVolta.  I think one of the main focuses is how to get models (many not just\na few) to converge with FP16 without having to endlessly guess the right\nconfig/hyperparameters to use.  I want to stress that this is how I\nunderstood the conversation and I may be incorrect or half correct.\n\nSTRESS:  If there are methods you think need to be added (or leverage for\nperformance) to TensorFlow from cuDNN we are always interested in a list.\nInternally, this happened with cuDNN 6 and we focused on implementing the\nfeatures teams said they wanted that would help their projects.\n\nOn Sat, Aug 5, 2017 at 8:46 AM, Courtial Florian <notifications@github.com>\nwrote:\n\n> I will certainly try it because finally gcc 6 is supported by CUDA 9 and\n> Ubuntu 17.04 comes with it.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-320450756>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZessKqj_nPY1br9SD9L9SX-8Kf5Dbtks5sVI5TgaJpZM4OuRL7>\n> .\n>\n", "Speaking of methods to be added, group convolution from cudnn7 would be a important feature for vision community.", "Cool I will add it to the list I am starting.  I may forget but feel free\nto remind me to publish some kind of list where I can provide some guidance\non what is likely being worked on.  It cannot be a promise but we want\nfeedback so we can prioritize what people want and need.  Thank you Yuxin.\n\nOn Sat, Aug 5, 2017 at 12:26 PM, Yuxin Wu <notifications@github.com> wrote:\n\n> Speaking of methods to be added, group convolution from cudnn7 would be a\n> important feature for vision community.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-320465264>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZesv9udRRxy9WvsK2eUEZCj7LAGM8bks5sVMHVgaJpZM4OuRL7>\n> .\n>\n", "I just tried to compile with cuDNN 7 with CUDA 8 and it failed which I kind\nof expected.  There is a patch incoming from NVIDIA that should help line\nthings up.  Just a heads up if anyone is trying.\n\nOn Sat, Aug 5, 2017 at 1:47 PM, Toby Boyd <tobyboyd@google.com> wrote:\n\n> Cool I will add it to the list I am starting.  I may forget but feel free\n> to remind me to publish some kind of list where I can provide some guidance\n> on what is likely being worked on.  It cannot be a promise but we want\n> feedback so we can prioritize what people want and need.  Thank you Yuxin.\n>\n> On Sat, Aug 5, 2017 at 12:26 PM, Yuxin Wu <notifications@github.com>\n> wrote:\n>\n>> Speaking of methods to be added, group convolution from cudnn7 would be a\n>> important feature for vision community.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-320465264>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AWZesv9udRRxy9WvsK2eUEZCj7LAGM8bks5sVMHVgaJpZM4OuRL7>\n>> .\n>>\n>\n>\n", "I am trying to get cuDNN 7 with CUDA 8/9 running. CUDA 8 is not supported by the GTX 1080 Ti - at least the installer says so ^^\r\n\r\nI am having a big time trouble getting it running together. I want to point out this great article that sums up what i already have tried: https://nitishmutha.github.io/tensorflow/2017/01/22/TensorFlow-with-gpu-for-windows.html\r\n\r\nThe CUDA examples are working via Visual-Studio in both setup combinations.\r\nHere the output of the deviceQuery.exe which was compiled using Visual-Studio:\r\n```\r\nPS C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v9.0\\bin\\win64\\Release> deviceQuery.exe\r\nC:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v9.0\\bin\\win64\\Release\\deviceQuery.exe Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nDetected 1 CUDA Capable device(s)\r\n\r\nDevice 0: \"GeForce GTX 1080 Ti\"\r\n  CUDA Driver Version / Runtime Version          9.0 / 9.0\r\n  CUDA Capability Major/Minor version number:    6.1\r\n  Total amount of global memory:                 11264 MBytes (11811160064 bytes)\r\n  (28) Multiprocessors, (128) CUDA Cores/MP:     3584 CUDA Cores\r\n  GPU Max Clock rate:                            1683 MHz (1.68 GHz)\r\n  Memory Clock rate:                             5505 Mhz\r\n  Memory Bus Width:                              352-bit\r\n  L2 Cache Size:                                 2883584 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  2048\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Supports Cooperative Kernel Launch:            No\r\n  Supports MultiDevice Co-op Kernel Launch:      No\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.0, CUDA Runtime Version = 9.0, NumDevs = 1, Device0 = GeForce GTX 1080 Ti\r\nResult = PASS\r\n```\r\n\r\n@tfboyd do you have any link confirming the cuDNN update from Nvidea?", "@4F2E4A2E 1080 Ti definitely supports CUDA 8.0. That's what I've been using with TensorFlow for the past several months.", "Hi all, so i have gtx 1080 ti with cuda 8.0. I am trying to install tensorflow-gpu, do i go for cuDNN 5.1, 6.0 or 7.0?", "I suggest sticking with 5.1 for the moment.  I am running some deeper perf\ntests on 6 and getting mixed results that need more testing to figure out.\n\nOn Aug 6, 2017 9:30 PM, \"colmantse\" <notifications@github.com> wrote:\n\n> Hi all, so i have gtx 1080 ti with cuda 8.0. I am trying to install\n> tensorflow-gpu, do i go for cuDNN 5.1, 6.0 or 7.0?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-320566071>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZeshvEFsdeWz-1uyzl_L6HE15E0BzSks5sVpLlgaJpZM4OuRL7>\n> .\n>\n", "thanks, i tried with cudnn 6.0 but doesn't work, i guess because of my dummy tf-gpu installation. cudnn 5.1 works for me with python 3.6 ", "@tpankaj Thank you! I've got it running with CUDA 8 and cuDNN 5.1", "Here are the full set of features in cuDNN 7:\r\n>Key Features and Enhancements\r\nThis cuDNN release includes the following key features and enhancements.\r\nTensor Cores\r\nVersion 7.0.1 of cuDNN is the first to support the Tensor Core operations in its\r\nimplementation. Tensor Cores provide highly optimized matrix multiplication\r\nbuilding blocks that do not have an equivalent numerical behavior in the traditional\r\ninstructions, therefore, its numerical behavior is slightly different.\r\ncudnnSetConvolutionMathType, cudnnSetRNNMatrixMathType, and\r\ncudnnMathType_t\r\nThe cudnnSetConvolutionMathType and cudnnSetRNNMatrixMathType\r\nfunctions enable you to choose whether or not to use Tensor Core operations in\r\nthe convolution and RNN layers respectively by setting the math mode to either\r\nCUDNN_TENSOR_OP_MATH or CUDNN_DEFAULT_MATH.\r\nTensor Core operations perform parallel floating point accumulation of multiple\r\nfloating point products.\r\nSetting the math mode to CUDNN_TENSOR_OP_MATH indicates that the library will use\r\nTensor Core operations.\r\nThe default is CUDNN_DEFAULT_MATH. This default indicates that the Tensor Core\r\noperations will be avoided by the library. The default mode is a serialized operation\r\nwhereas, the Tensor Core is a parallelized operation, therefore, the two might result\r\nin slightly different numerical results due to the different sequencing of operations.\r\nThe library falls back to the default math mode when Tensor Core operations are\r\nnot supported or not permitted.\r\ncudnnSetConvolutionGroupCount\r\nA new interface that allows applications to perform convolution groups in the\r\nconvolution layers in a single API call.\r\ncudnnCTCLoss\r\ncudnnCTCLoss provides a GPU implementation of the Connectionist Temporal\r\nClassification (CTC) loss function for RNNs. The CTC loss function is used for\r\nphoneme recognition in speech and handwriting recognition.\r\nCUDNN_BATCHNORM_SPATIAL_PERSISTENT\r\nThe CUDNN_BATCHNORM_SPATIAL_PERSISTENT function is a new batch\r\nnormalization mode for cudnnBatchNormalizationForwardTraining\r\nand cudnnBatchNormalizationBackward. This mode is similar to\r\nCUDNN_BATCHNORM_SPATIAL, however, it can be faster for some tasks.\r\ncudnnQueryRuntimeError\r\nThe cudnnQueryRuntimeError function reports error codes written by GPU\r\nkernels when executing cudnnBatchNormalizationForwardTraining\r\nand cudnnBatchNormalizationBackward with the\r\nCUDNN_BATCHNORM_SPATIAL_PERSISTENT mode.\r\ncudnnGetConvolutionForwardAlgorithm_v7\r\nThis new API returns all algorithms sorted by expected performance\r\n(using internal heuristics). These algorithms are output similarly to\r\ncudnnFindConvolutionForwardAlgorithm.\r\ncudnnGetConvolutionBackwardDataAlgorithm_v7\r\nThis new API returns all algorithms sorted by expected performance\r\n(using internal heuristics). These algorithms are output similarly to\r\ncudnnFindConvolutionBackwardAlgorithm.\r\ncudnnGetConvolutionBackwardFilterAlgorithm_v7\r\nThis new API returns all algorithms sorted by expected performance\r\n(using internal heuristics). These algorithms are output similarly to\r\ncudnnFindConvolutionBackwardFilterAlgorithm.\r\nCUDNN_REDUCE_TENSOR_MUL_NO_ZEROS\r\nThe MUL_NO_ZEROS function is a multiplication reduction that ignores zeros in the\r\ndata.\r\nCUDNN_OP_TENSOR_NOT\r\nThe OP_TENSOR_NOT function is a unary operation that takes the negative of\r\n(alpha*A).\r\ncudnnGetDropoutDescriptor\r\nThe cudnnGetDropoutDescriptor function allows applications to get dropout\r\nvalues.", "Alright I am thinking about starting a new issue that is more of a \"blog\" of CUDA 9 RC + cuDNN 7.0.  I have a TF build \"in my hand\" that is patched together but is CUDA 9RC and cuDNN 7.0 and I want to see if anyone is interesting in trying it.  I also need to make sure there is not some weird reason why I cannot share it.  There are changes that need to be made to some upstream libraries that TensorFlow uses but you will start to see PRs coming in from NVIDIA in the near future.  I and the team were able to test CUDA 8 + cuDNN 6 on Volta and then CUDA 9RC + cuDNN 7 on Volta (V100) with FP32 code.  I only do Linux builds and Python 2.7 but if all/any of you are interested I would like to try and involve the community more than we did with cuDNN 6.0.  It might not be super fun but I want to offer as well as try to make this feel more like we are in this together vs. I am feeing information.  I also still want to build out lists of what features we are working on but not promising for cuDNN 7 (and 6.0).  @cancan101  thank you for the full list.  ", "@tfboyd: I would be grateful for descriptions on doing CUDA 9.0RC+cuDNN 7.0. I am using a weird system myself (ubuntu 17.10 beta with TF1.3, CUDA 8.0 and cuDNN 6.0 gcc-4.8), and upgrading to cuda 9 and cudnn 7 would actually be nice compilerwise. ", "I will see what I can do on getting what you need to build yourself and a\nbinary.  The performance team lead indicated I can try and make this happen\nso we can be more transparent and I hope have more fun as a community.\nGetting you the patch and how to build it not super hard but is a little\nharder.  It will also be very informal as I do not have time to manage a\nbranch and the patch could bit rot (not apply cleanly) very quickly.  The\npatch was used to make sure everyone involved was ok with the changes in\ngeneral and I expect individual PRs will start coming in.\n\nOn Fri, Aug 11, 2017 at 5:22 AM, Erlend Aune <notifications@github.com>\nwrote:\n\n> @tfboyd <https://github.com/tfboyd>: I would be grateful for descriptions\n> on doing CUDA 9.0RC+cuDNN 7.0. I am using a weird system myself (ubuntu\n> 17.10 beta with TF1.3, CUDA 8.0 and cuDNN 6.0 gcc-4.8), and upgrading to\n> cuda 9 and cudnn 7 would actually be nice compilerwise.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-321798364>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZesj4WRkFKNX-Nt2oKtvp0oyQVBtM5ks5sXEdqgaJpZM4OuRL7>\n> .\n>\n", "@tfboyd: I am interested, how will you share it? A branch?", "@tfboyd I'd definitely be very interested as well. Thanks!", "Trying to figure it out this week.  Logistics are often harder than I\nthink.\n\nOn Aug 12, 2017 10:18 AM, \"Tanmay Bakshi\" <notifications@github.com> wrote:\n\n> @tfboyd <https://github.com/tfboyd> I'd definitely be very interested as\n> well. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-321994065>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZesjO42Rl1WCyW0KR22KgbydKh1O4Zks5sXd6AgaJpZM4OuRL7>\n> .\n>\n", "Instructions and a binary to play with if you like Python 2.7.  I am going to close this as I will update the issue I create to track the effort.  @tanmayb123 @Froskekongen \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/12474", "I just tried installing pre-compiled tensorflow-gpu-1.3.0 for Python 3.6 on Windows x64 and provided cuDNN library version 7.0 with Cuda 8.0 and at least for me, everything seems to work. I'm not seeing any exception or issues.\r\nIs this to be expected? Is cuDNN 7.0 backwards-compatible to cuDNN 6.0? May this lead to any issues?", "@apacha I am a little surprised it worked.  I have seen the error before in my testing where the TensorFlow binary cannot find cuDNN because it looks for it by name and the *.so files include 6.0/7.0 in the names.  Remotely possible you have cuDNN 6 still in your path.  I do not like making guesses about your setup but if I was making a bet I would saying it is still using cuDNN 6.  \r\n\r\nIn regards to backwards compatible minus TensorFlow being compiled to look for a specific version.  I do not know.\r\n\r\nFinally, it is not a big deal. cuDNN 7 PRs are almost approved/merged and the pre compiled binary will likely move to cuDNN 7 as of 1.5.\r\n\r\nUPDATE on progress to CUDA 9RC and cuDNN 7\r\n- PRs from NVIDIA are nearly approved\r\n- EIGEN change has been approved and merged\r\n- FP16 testing has started in earnest on V100 (Volta)", "@tfboyd just for the sake of completeness: I was using cuDNN 5 previously and since I had to update for tensorflow 1.3, I was just hopping to cuDNN version 7 to give it a shot. I've explicitly deleted `cudnn64_5.dll` and there is no `cudnn64_6.dll` in my CUDA installation path. Maybe it's Windows magic. :-P\r\n\r\nThough notice one thing: I am still using CUDA 8.0, not 9.0.", "@apacha It might be windows magic.  I did not want to sound judgmental as I had no idea.  I think windows magic is possible as the cuDNN calls should not have changed and thus backwards compatible seems likely.  For the linux builds TensorFlow is looking for specific files (or that is what it looks like when I get errors) and is very unhappy if it does not find cudnnblahblah.6.so.  Thanks for the update and specifics.   ", "Is there a branch / tag whatever we can checkout and try it out?\r\nStarted a brand new installation, Ubuntu 17... then new gcc impose CUDA 9, I see that CuDNN who fit with is 7... you see where I'm heading.\r\nI can for sure hack  my setup in many places (and start it from scratch again with Ubuntu 16) just I'm so close, the fix is said to be close... why make a big jump in the past if I can make a small jump in the future!", "The PRs are almost approved.  They are in review.  I suspect a couple more\nweeks at most, but these reviews can take time.  I think these are all of\nthem.  There could be a straggler or a change to get the EIGEN change for\nCUDA 9.  I have not ready them personally. They get closer each day.\n\nhttps://github.com/tensorflow/tensorflow/pull/12504\nhttps://github.com/tensorflow/tensorflow/pull/12503\nhttps://github.com/tensorflow/tensorflow/pull/12502\n\nOn Tue, Sep 12, 2017 at 7:49 PM, Remi Morin <notifications@github.com>\nwrote:\n\n> Is there a branch / tag whatever we can checkout and try it out?\n> Started a brand new installation, Ubuntu 17... then new gcc impose CUDA 9,\n> I see that CuDNN who fit with is 7... you see where I'm heading.\n> I can for sure hack my setup in many places (and start it from scratch\n> again with Ubuntu 16) just I'm so close, the fix is said to be close... why\n> make a big jump in the past if I can make a small jump in the future!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-329041739>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZesrpX6TSN6fVESEsql3QNtjgo-LM9ks5sh0KqgaJpZM4OuRL7>\n> .\n>\n", "@tfboyd Is this still an issue? I realized that cuda 9.0 has been released just today.", "cuda 9.0 has been released,I can't found cuda 8.0 install file....\r\npls upgrade tensorflow...", "> cuda 9.0 has been released,I can't found cuda 8.0 install file....\r\n\r\n@zjjott https://developer.nvidia.com/cuda-toolkit-archive", "@thomasjo thanks\uff01", "So following @tfboyd approving the PRs he mentioned, will tensorflow 1.3 now be compatible with CUDA 9 and cuDNN 7? Has anyone anywhere actually successfully installed this?", "@voxmenthe I just tried to install tf1.3 with CUDA 9.0 and cuDNN7. I got the error related in this issue #12489\r\n\r\nEDIT: Basically, I don't know how. But moving to the master branch (which in the beginning wasn't installing for me) without any further change, I was able to install it. Although, now when I try to import tensorflow it says it's missing platform module.", "Any indication of how close the associated PRs are to going in? I installed cuDNN 7 then realised it was causing issues - I can downgrade to v6 but figured I might wait if it's close to being resolved...", "The PRs seem to be approved.  I have not run the build myself in a few days.  Keep in mind 1.3 would not have these changes as that was a while ago.  1.4 would have the changes.  Hopefully this week I can download the latest versions and do a fresh build.  I suspect someone will do it well before I have time.  ", "Any chance any of you smart people might be making a tutorial for tf 1.3 or 1.4 with CUDA v9.0, cudnn 7.0 for Win 10 x64? I've tried installing (Anaconda) but keep getting '_pywrap_tensorflow_internal' error and I have already checked msvcp140.dll is added to my path..", "@devilsnare007: I guess the best chances are by following https://github.com/philferriere/dlwin. Simply replace the listed versions with the current versions. Note, that TF 1.4 has not even been released. But TF 1.3 should work fine with the instructions provided. Once everything has been upgraded and TF 1.4 has been released, we will update that tutorial.", "Will cuDNN 7.0 be supported when TF 1.4 is released?", "@soloice \r\nAt head (as of a couple days ago) I was able to compile CUDA 9 (release version) with cuDNN 7.0 with no special changes and ran a few tf_cnn_benchmarks.py tests on a GTX 1080.  Everything looks fine.  TF 1.4 which should RC this week will have CUDA 8 and cuDNN 6 binaries but will also compile just fine with CUDA 9 and cuDNN 7.  The goal is for TF 1.5 have CUDA 9 and cuDNN 7 in the binary.  This gives people time to upgrade their systems libraries and more time for testing.   If you are running Voltas feel free to start another thread and I will update it will progress on FP16 in real time. ", "@tfboyd Great to hear that TF 1.4 compiles with cuDNN 7! If at some point you feel up to creating an install guide it would be a great public service for the DL community.", "No problem, that should be easy enough and I am happy to try and fill that\ngap.\n\nOn Mon, Oct 9, 2017 at 9:42 AM, Jeff <notifications@github.com> wrote:\n\n> @tfboyd <https://github.com/tfboyd> Great to hear that TF 1.4 compiles\n> with cuDNN 7! If at some point you feel up to creating an install guide it\n> would be a great public service for the DL community.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-335212652>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZesp7d2aT8gsGOWb6YjHH_CnpKXUIPks5sqkztgaJpZM4OuRL7>\n> .\n>\n", "Is there a possibility to have a whl that works with CUDA 9 and cuDNN 7.0?\r\n\r\nThanks!", "I will publish mine (which will likely not not be 1.4 but some near match\nand I include the hash in the name) when I build it for testing but it will\nbe ubuntu 16.04 (I forget what gcc version), linux, python 2.7 just FYI.\nAnd I am not really suppose to share those builds because it can be\nconfusing for people and I will stress for all you know I included some\ncrazy back door.  Although adding some secret code feels like too much work\nto me.\n\nI think nightly-gpu builds are almost live in pip (I am pretty sure they\nhave always happened you just had to find them) which means after 1.4 the\nnightly builds will move to CUDA 9 + cuDNN 7 very quickly.\n\nOn Mon, Oct 9, 2017 at 11:41 AM, alexirae <notifications@github.com> wrote:\n\n> Is there a possibility to have a whl that works with CUDA 9 and cuDNN 7.0?\n>\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-335249979>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZeskJ5LXJwFcRpm-sKZ9QORoltrHOEks5sqmj1gaJpZM4OuRL7>\n> .\n>\n", "@tfboyd Thanks for your reply. Finally I managed to build the latest TF from sources with CUDA 8 + cuDNN 7 support on Ubuntu 16.04 and everything works fine on GTX 1080 Ti.", "@tfboyd Does 14.rc TF support CuDNN 7 and CUDA 9?", "It is included if you build from source.  I want to change the default\nbinaries which requires me to run some regression tests on K80s on AWS to\nmake sure everything looks good as well as get builds created.  We\nimmediately had a problem as the NVIDIA driver needed decreased performance\non Kokoro running in Google Cloud by 30%.  Nothing is every straight\nforward, but CUDA 9 and cuDNN 7 are in 1.4 source and have as expected in\nvery limited tests on Pascal for me.\n\nOn Thu, Oct 12, 2017 at 2:40 AM, Konstantin <notifications@github.com>\nwrote:\n\n> @tfboyd <https://github.com/tfboyd> Does 14.rc TF support CuDNN 7 and\n> CUDA 9?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-336075883>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZesq03dvnXWd0GLXODBTNCWExlZGPnks5srd6pgaJpZM4OuRL7>\n> .\n>\n", "Is there a possibility to have a whl that works with CUDA 9, cuDNN 7.0 and python 3.5?\r\n", "After TF 1.4 is finalized the nightly builds will move to the CUDA 9 +\ncuDNN 7 assuming there are no issues.  Builds I made and share for fun are\nalways python 2.7 because that is the default on my test systsems.\n\nOn Mon, Oct 16, 2017 at 6:59 AM, Diego Stalder <notifications@github.com>\nwrote:\n\n> Is there a possibility to have a whl that works with CUDA 9, cuDNN 7.0 and\n> python 3.5?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-336894073>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZesqgclWqET4OufQHV3FAD4XxgnZiKks5ss2E6gaJpZM4OuRL7>\n> .\n>\n", "Any expected release date?", "All those chomping at the bit, just build #master from source. It's not too hard (just time consuming), you get the latest CUDA/cuDNN, _and_ additional optimizations over a pip/whl install (eg, see the CPU optimizations in [this tut](https://gist.github.com/Brainiarc7/6d6c3f23ea057775b72c52817759b25c)). Plus next time CUDA/cuDNN upgrade, you can build again w/o having to wait.", "\r\nbuilding from the sources, TF1.4 is working with cuda 9.0, cuDNN v7.0.3 and python3.5\r\n", "Can I build from source on win10 platform? \r\nWould like TF to work on cuda 9.0, cuDNN v7, python3.**6**, and win10", "@tonyyuandao https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md", "Building from the sources, TF1.4 is working with cuda 9.0, cuDNN v7.0.3 and python2.7 as well. ", "@affromero Did you have any issues with jsoncpp by chance?", "I did a test of tf_cnn_benchmarks on AWS building from TF 1.4RC0 branch with CUDA 9 / cuDNN 7 and the results were equal or slightly faster than CUDA 8 + cuDNN 6.  \r\n\r\nEdit: remove mention I was not addressing elipeters comment.  :-)", "@elipeters \r\n\r\nWhen we say building we mean building from source not installing a wheel file.  A wheel is already compiled and 1.4 binaries support CUDA 8 + cuDNN 6.  To get CUDA 9 you will need to build from source.  I have never done the windows build.  Once 1.4 ships the team will switch the nightly builds over to CUDA 9.", "There is the 2nd release candidate (rc1) for 1.4 out as precompiled wheel ( https://pypi.python.org/pypi/tensorflow ). Has anyone tested that with CUDA 9 yet ?", "Tried, but not with working cuda 9.0.", "I will try again.\r\n\r\n1.4 is CUDA 8 + cuDNN 6  this will not work with CUDA 9 you will have to compile from source\r\nonce 1.4 is released we will work to switch the nightly builds to CUDA 9 and then 1.5 will most likely be CUDA 9.  \r\n\r\nI know CUDA 9 works fine when building 1.4 from source (ubuntu 16.04/python 2) because I was doing benchmarks on AWS last weekend.  ", "I have a recent recipe on building from source [here](https://github.com/yaroslavvb/tensorflow-community-wheels) (please post a link to your CUDA 9.0 wheel there as well once you build it)", "You are the best Yaroslav.\n\nOn Wed, Oct 25, 2017 at 8:11 AM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> I have a recent recipe on building from source here\n> <https://github.com/yaroslavvb/tensorflow-community-wheels> (please post\n> a link to your CUDA 9.0 wheel there as well once you build it)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-339361959>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZesmRwb0UmLWxzANWCq5RrT6teYtr5ks5sv0-IgaJpZM4OuRL7>\n> .\n>\n", "hello\uff0ci try to build tensorflow gpu by win10 env\uff0cthen i also met like this issue\uff0c can anyone help me\uff0cthanks first.\r\nmy environment\uff1a\r\nwin10 + gtx 1080ti + cuda 9.0 + cuDNN 7 + visual studio profession 2015 + cmake 3.6.3 + python 3.5.4\r\n\r\nwhen i switch to tensorflow r1.4\uff0cand build by cmake at win10 environment\uff0cissue accur that\uff1a\r\n\r\n`CUSTOMBUILD : Internal error : assertion failed at: \"C:/dvs/p4/build/sw/rel/gpu_drv/r384/r384_00/drivers/compiler/edg/EDG_4.12/src/lookup.c\", line 2652 [C:\\TF\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n\r\n  1 catastrophic error detected in the compilation of \"C:/Users/ADMINI~1/AppData/Local/Temp/tmpxft_00000c94_00000000-8_adjust_contrast_op_gpu.cu.cpp4.ii\".\r\n\r\n  Compilation aborted.\r\n\r\n  adjust_contrast_op_gpu.cu.cc\r\n\r\nCUSTOMBUILD : nvcc error : 'cudafe++' died with status 0xC0000409 [C:\\TF\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n\r\n  CMake Error at tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.Release.cmake:267 (message):\r\n    Error generating file\r\n    C:/TF/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/Release/tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj`\r\n![image](https://user-images.githubusercontent.com/32933617/32207793-3c4342f8-be39-11e7-8881-9779f84807f5.png)\r\n\r\nabove issue look like cuda compolie itself problem\uff0cbut when i switch tensorflow version to r1.3\uff0canother issue accur\uff1a\r\n\r\n`c:\\tf\\test\\tensorflow\\tensorflow\\contrib\\cmake\\build\\external\\eigen_archive\\eigen\\src/Core/util/Macros.h(416): fatal error C1017:\r\n \u65e0\u6548\u7684\u6574\u6570\u5e38\u91cf\u8868\u8fbe\u5f0f [C:\\TF\\test\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n\r\n  CMake Error at tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.Release.cmake:267 (message):\r\n    Error generating file\r\n    C:/TF/test/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/Release/tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj`\r\n![image](https://user-images.githubusercontent.com/32933617/32207864-c4f0cb98-be39-11e7-8968-12eaea9bdbb7.png)\r\n\r\nit look like the file adjust_contrast_op_gpu.cu.cc have some problem\uff0cbut i can't find any error from it. \r\n\r\nsuch above issues trouble me few days\uff0cwish to someone help me going this try and success\uff0cand strong expect google upgrade tensorflow support cuda 9.0 and cudnn 7 at win10 environment. \r\n", "Has anyone released a whl for TensorFlow with CUDA 9 and cuDNN 7.0?", "@vellamike  I know your question is general, but the TF team will have CUDA 9 in the binaries with 1.5 that should land in Q4.  For now, you have to build from source.", "I am trying to build 1.4 with CUDA 9 and cuDNN 7 in mac 10.13 high sierra. I keep getting this error \r\n\r\n```\r\nERROR: /Users/smitshilu/tensorflow/tensorflow/core/kernels/BUILD:2948:1: output 'tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.pic.o' was not created.\r\nERROR: /Users/smitshilu/tensorflow/tensorflow/core/kernels/BUILD:2948:1: not all outputs were created or valid.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nAny solution for this?", "@smitshilu possibly related https://github.com/tensorflow/tensorflow/issues/2143", "Why 1.4 still doesn't have CUDA 9 in binaries? This version was released quit a long ago and for using with V100 building from source is required which is not so smooth and fast following to a number of issues reported.", "@ViktorM  What issues did you have compiling from source? It was a bit tricky but not that hard.", "26-SEP-2017 was the GA for CUDA 9.  If we release CUDA 9 + cuDNN 7 binaries in Q4 I think this will be the fastest we have upgraded cuDNN.  I was not here for 8.5 to 9 so I have no idea.   I would like us to go a little faster but this also means anyone with a CUDA 8 setup has to upgrade not just to CUDA 9 but they also need to upgrade their device driver to 384.x, which I can say is not something production people take lightly.  \r\n\r\nIdeally we would have infinite (or just a few more but the matrix explodes fast) builds, but that is another problem that would take a long time to explain and I doubt many people care. \r\n    ", "BTW, I [observed](https://medium.com/@yaroslavvb/peak-performance-of-amazon-p3-instances-f2bc48f9ef71) 85 T ops/second with CUDA 9 on float16 matmul/V100 using Nvidia's [NGC](https://ngc.nvidia.com/signin/email\r\n) TensorFlow container (as opposed to 8.8 T ops/s on my GTX 1080 at home). Really looking forward to having these improvements in the officially supported version!", "@yaroslavvb  Being very honest we are working through some FP16 issues.  There is a path in the tf_cnn_benchmarks for FP16 and the focus is on ResNet50 first and we are working on auto scaling for FP16 as well.  You can give it a try if you are interested but we are actively working through some problems.  People are on it and it is just taking time.  We finally have DGX-1s in house so we can also play with the same containers and try to keep track of performance on that exact platform moving forward.  ", "Ok, So I am going to install Ubuntu 17.10 and I just wanted to try all the latest stuffs for fun. \r\nBefore I do I just wanted to know did any one try the below stacks building from source and got any luck?\r\n\r\n-> Ubuntu 17.10, CUDA 9.0, cuDNN 7.0, TF master\r\n-> Ubuntu 17.10, CUDA 8.0, cnDNN 6.1, TF 1.4", "I am encountering the same issue as @xsr-ai, specifically using Python 3.6.3, VS 2017, CUDA 9, cuDNN 7.", "@aluo-x You mean you tried on Windows 10? Assuming because you said VS 2017.", "Yes, that is correct. Here is the specific error:\r\n\r\n```\r\nCustomBuild:\r\n  Building NVCC (Device) object CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/Release/tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj\r\n  CMake Error at tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.Release.cmake:222 (message):\r\n    Error generating\r\n    C:/optimae/tensorflow-1.4.0/tensorflow/contrib/cmake/build/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/Release/tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj\r\n\r\n\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\BuildTools\\Common7\\IDE\\VC\\VCTargets\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1. [C:\\optimae\\tensorflow-1.4.0\\\r\ntensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n```", "@aluo-x  Did you use the latest c-make? i.e Release candidate or the stable release?", "Using cmake 3.9.5, swig 3.0.12, CUDA 9.0.176, cuDNN 7.0.3. VS 2017 19.11.25547. ", "@aluo-x  Even I didnt have much luck with c-make. But can you try building with Bazel?", "@smitshilu If I am not mistaken, you are getting an error regarding alignment, right? Similar to the one described here for pytorch: https://github.com/pytorch/pytorch/issues/2692\r\n\r\nI tried applying the same solution, that is removing all ___align__(sizeof(T))_ from the problematic files: \r\n_tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc_\r\n_tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc_\r\n_tensorflow/core/kernels/split_lib_gpu.cu.cc_\r\n\r\nI am not sure if this causes any issues, but it seems to be working fine so far. And from what I understand, the runtime will always use a fixed alignment of 16 for the shared memory.", "For folks interested we have CUDA 9 wheels uploaded. No need to build yourself! https://github.com/mind/wheels/releases/tag/tf1.4-gpu-cuda9", "Ubuntu 17.10, CUDA 9, CuDNN 7, Python 3.6, bazel 0.7.0 + TF from source (master).\r\n\r\nFollow instructions as in this answer to get CUDA up and running:\r\nhttps://askubuntu.com/questions/967332/how-can-i-install-cuda-9-on-ubuntu-17-10\r\n\r\nNote, you might want to use these commands instead for 64 bit version: \r\n```\r\nsudo ln -s /usr/bin/gcc-6 /usr/local/cuda-9.0/bin/gcc \r\nsudo ln -s /usr/bin/g++-6 /usr/local/cuda-9.0/bin/g++ \r\nsudo ./cuda_9.0.176_384.81_linux-run --override\r\n```\r\n\r\nTo install Tensorflow you will need\r\n1. Before compiling TF: correctly configure path variables (the paths from NVIDIA page did not work for me):\r\n```\r\nexport PATH=/usr/local/cuda-9.0/bin:${PATH}\r\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda-9.0/lib64\r\n```\r\n2. Before compiling: Configure bazel to use same gcc version as during CUDA installation:\r\n```\r\nsudo update-alternatives --remove-all g++\r\nsudo update-alternatives --remove-all gcc\r\nsudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-6 10\r\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-6 10\r\n```\r\n3. While following TF instructions once you get to  bazel build step note, that youll need an additional flag to compile with gcc version higher than 4.*:\r\n`bazel build --config=opt --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package`\r\n", "@alexbrad I ran into the same issue building for Mac GPU with CUDA 9, cuDNN 7. This solution also worked for me and I haven't run into any issues using TF so far.\r\nSource changes and wheel: https://github.com/nathanielatom/tensorflow/releases/tag/v1.4.0-mac\r\n", "Ubuntu 16.04, TensorFlow 1.4 with CUDA 9.0 and cuDNN 7.0.3 already installed and tested:\r\n\r\nInstall Tensorflow 1.4 from Source\r\n```\r\ncd ~/Downloads\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\ngit checkout r1.4\r\n```\r\n\r\n- Configure for CUDA version: 9.0\r\n- Configure for cuDNN version: 7.0.3\r\n- Get your compute capability from https://developer.nvidia.com/cuda-gpus\r\n- I set this to 6.1 as I have a GeForce GTX 1070\r\n- Configure other options as appropriate\r\n\r\n`./configure`\r\n\r\nInstalling Bazel\r\n```\r\nsudo add-apt-repository ppa:webupd8team/java\r\nsudo apt-get update && sudo apt-get install oracle-java8-installer\r\necho \"deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list\r\ncurl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add -\r\nsudo apt-get update && sudo apt-get install bazel\r\nsudo /sbin/ldconfig -v\r\n```\r\n\r\nBuilding TensorFlow\r\n```\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\npip install /tmp/tensorflow_pkg/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl\r\n```\r\n\r\nThe name of the tensorflow wheel file above may be different\r\nJust `ls /tmp/tensorflow_pkg` to check\r\n", "Installation steps for Mac 10.13, CUDA 9 and tensorflow 1.4 https://gist.github.com/smitshilu/53cf9ff0fd6cdb64cca69a7e2827ed0f", "Can someone tell me, how can i build tensorflow whl package from sources for windows in linux (Ubuntu 16.04) with bazel? It was possible for version 1.2 if i right. Thanks.", "@ValeryPiashchynski you can follow these steps https://www.tensorflow.org/install/install_sources", "@smitshilu Thanks for you answer. I can build wheel package in Ubuntu following this steps and all works good in Ubuntu. But i can't install that whl package in Windows OS (there is an error: isn't supportet wheel). So my question is how can i build package in Ubuntu which i can then install in Windows?", "~@ValeryPiashchynski I don't think that's possible.~\r\n\r\n(taking it out as comments below suggest otherwise)", "Cross-building on Ubuntu for Windows should be possible one day through clang. It will probably require lots of fixes, since Windows binaries are currently built with MSVC. I asked basically the same question when talking in person to @gunan last Monday. Should this be forked into a GH issue of its own, since it has little to do directly with CUDA?", "Cross compile is possible with bazel but not sure how to do it on tensorflow. For reference \r\n[https://github.com/bazelbuild/bazel/wiki/Building-with-a-custom-toolchain](https://github.com/bazelbuild/bazel/wiki/Building-with-a-custom-toolchain)\r\n[https://github.com/bazelbuild/bazel/issues/1353](https://github.com/bazelbuild/bazel/issues/1353)", "Does anyone know if the tensorflow 1.5 nightly builds posted here(win10 builds) have CUDA9+CuDNN7 support?\r\nhttps://pypi.python.org/pypi/tf-nightly-gpu/1.5.0.dev20171115\r\n\r\nOn a side note, it is totally irresponsible to close this ticket as well as #14126 just because you say \"It is going to be released with TF 1.5\".  MXNET 0.12 already has CUDA9 FP16 in production.  Tensorflow and CNTK need to hurry up.  It is not just beneficial for Volta.\r\n\r\n", "Not yet, we are working on upgrading our build infra for CUDA 9.\r\nWe are aiming to have pip packages with CUDA 9 before the end of this week.", "i have two computer, and yesterday i install 1080ti and install everything(cuda8 and cudnn6) new graphic driver, visual studio 2015\r\ni compare the time of epoch 1080ti vs 980ti \r\nand i see 1080ti run each epoch in 22 min but 980ti run in 13 min !!!(batch=60 for 1080 vs batch=20for 980ti) \r\n**why 1080ti is work slower than 980ti !!!!** and how i can check what is wrong ?!\r\n", "What is the run time if you use 20 batch for 1080Ti? ", "@gunan, just wondering if there's a new ETA for this? ", "https://github.com/tensorflow/tensorflow/pull/14773", "@smitshilu \r\nin 1080ti with 20 batch =26 min\r\nand with 60batch = 19min\r\nin 980gtx with batch 20 = 14 min !!!!\r\ni use windows, install last version of driver with cuda 8 and cudnn 6\r\nhow i can find out why it's run slower than 980 ?", "@nasergh Are you having GTX 1080ti and 980ti in SLI?", "@vickylance \r\nNo\r\ntwo different computer !\r\nboth are cori7 and 1TB hard disk and i load data image from 1TB sata HDD\r\nbut in 980 i have windows on SSD hard disk\r\ni try different version of driver last thing i check 388.13 downloaded from asus website with CUDA 8 and cudnn 6\r\ni don't know which one of this are the reason \r\n1- windows! maybe it's work better on linux\r\n2- HDD speed\r\n3- fake 1080TI\r\n4- CUDA and cudnn not compatible with 1080ti \r\n5- CPU (CPU on 1080TI computer is more powerful than 980)\r\nwhat do you suggest ?\r\n", "@nasergh \r\n1) Is the RAM same? if so. I am not sure if it will affect so much but check if the MHz of the RAM are also same in both the systems.\r\n2) Check the GPU utilization % when running on 980ti and 1080ti. Use this tool if you want to check the GPU utilization. https://docs.microsoft.com/en-us/sysinternals/downloads/process-explorer There are better ones out there, but this is what came on top of my head.\r\n3) If you want to eek out the best performance I would suggest installing Ubuntu16.04 as a dual boot on your 1080ti system and using CUDA 9.0 and cuDNN 7.0\r\n4) Also windows takes up a lot of system resources in itself, so running it on a SSD definitely gives it an edge, but not of that magnitude as seen in your test scenario.", "Maybe it is the selected board architecture.\nTF is configured by default for 3.0, 3.5 and 5.2; while 1080TI is 6.1 (Pascal) while 980 is 5.2 (Maxwell) according to https://en.wikipedia.org/wiki/CUDA#GPUs_supported.\nMaybe the downgrade to 3.0 or 5.2 is not efficient on the 1080TI, while it is native for the 980?\nTry computing with both capabilities 5.2 and 6.1 (see CMakeLists.txt l.232 and l.246)\n \nDe : nasergh [mailto:notifications@github.com] \nEnvoy\u00e9 : mercredi 22 novembre 2017 17:17\n\u00c0 : tensorflow/tensorflow\nCc : sylvain-bougnoux; Manual\nObjet : Re: [tensorflow/tensorflow] Upgrade to CuDNN 7 and CUDA 9 (#12052)\n \n@vickylance\nNo\ntwo different computer !\nboth are cori7 and 1TB hard disk and i load data image from 1TB sata HDD\nbut in 980 i have windows on SSD hard disk\ni try different version of driver last thing i check 388.13 downloaded from asus website with CUDA 8 and cudnn 6\ni don't know which one of this are the reason\n1- windows! maybe it's work better on linux\n2- HDD speed\n3- fake 1080TI\n4- CUDA and cudnn not compatible with 1080ti\nwhat do you suggest ?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.", "environment: cuda9.0+cudnn7.0+tf1.4\uff0cand I meet an error when I run the \"ptb\" example , TypeError: __init__() got an unexpected keyword argument 'input_size',   'input_size' is the parameter of the CudnnLSTM", "after watching this thread for months I am going to give it a try on Gentoo linux", "i have asus strix 1080TI \r\n1- in ubunto i can use driver in nvidia website or i must download from asus(because i don't see driver for linux in asus website)\r\n2- last version is ok or i must intall 378.13 because i see in most comment they say use 378.13?\r\nthanks\r\n", "For those on Windows, I have just uploaded TF 1.4.0 built against CUDA 8.0.61.2, cuDNN 7.0.4, Python 3.6.3, with AVX support [onto my repo](https://github.com/aluo-x/tensorflow_windows). Hopefully this is enough until CUDA 9 gets sorted out on Windows.", "i am trying to install CUDA9 and cudnn 7 on ubuntu 16.04 and python 3.6\r\nbut i am fail :(\r\ni try everything, search everywhere but still give same error \"importError: libcublas.so.8.0 can not open shared object file: no such file or directory \r\ni think tensor want to run CUDA8 \r\nhow i can tell him to use cuda9 ?!!!! if answer is run from source how exactly? i did not see very clear website about build from source\r\nthanks\r\n", "You should install tf from source@nasergh", "@withme6696 \r\nhow i can install it from source ?\r\n\r\ni know i can download one of this\r\nhttps://github.com/mind/wheels/releases\r\nbut i don't know download which one and how install it !?\r\n", "@nasergh check out our [README](https://github.com/mind/wheels) for how to install. If you don't mind installing MKL, you can do\r\n\r\n```\r\npip --no-cache-dir install https://github.com/mind/wheels/releases/download/tf1.4-gpu-cuda9-37/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl\r\n```\r\n\r\nIf you don't want to install MKL, you can do \r\n\r\n```\r\npip --no-cache-dir install https://github.com/mind/wheels/releases/download/tf1.4-gpu-cuda9-nomkl/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl\r\n```", "I will use this issue as the tracking issue for CUDA 9 support.\r\nCurrently, there are two blockers:\r\n1 - https://github.com/tensorflow/tensorflow/pull/14770\r\n2 - On windows, it looks like we have a bug with NVCC. Building TF with CUDA9 seems to be failing with a compiler crash. NVIDIA is helping investigate this, and once we have an update we will proceed.", "@danqing \r\nThanks\r\n1- MKL how much improve the speed ?\r\n2- in version without MKL i need to install MKL ?!", "for 1, see [this](https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture) - note that computations done on GPU won't have the speed up obviously.\r\n\r\nfor 2, you don't. make sure you install the right version.\r\n\r\nbtw, this is a thread with many subscribers. if you have future issues with our wheels, please open an issue in our repo instead of commenting below, so we don't spam a ton of folks.", "@Tweakmind : I cannot pass this part:\r\n\r\n**Building TensorFlow**\r\n\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\npip install /tmp/tensorflow_pkg/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl\r\n\r\nThe first line seems to be incomplete (missing double quotes)? Are these three lines or two lines?\r\n\r\n", "**@goodmangu** the correct code, I think, is:\r\n\r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"`\r\n\r\nDouble quotation marks are missing from the command.", "I'm getting past this part using script [here](https://github.com/yaroslavvb/tf_build), but getting blocked by some cuda compiler errors in https://github.com/tensorflow/tensorflow/issues/15108", "Thanks guys. Appreciated this. I got it working the same day by using the nightly build Linux binary instead. See: https://github.com/tensorflow/tensorflow\r\nNow running 3 GTX 1080 Tis with Keras. Cool! ", "Spent last two days trying to build Tensorflow from source (r1.4) for my MacBook Pro with an eGPU. Driver working, with Cuda 8.0, cuDNN 6.0, Mac OSX Sierra 10.12. Very close to end, but blocked by some build errors after 20 minutes. Anybody has had luck so far? Any successfully built package that you can share? Thanks in advance. ", "@goodmangu Could you please specify which \"the nightly build Linux binary\" do you use?", "Sure, this one: tf_nightly_gpu-1.head-cp27-none-linux_x86_64.whl", "Still no Windows 10 support for Cuda 9.0 + cuDNN 7.0? Just verifying.\r\nTensorflow GPU 1.4.0", "@goodmangu I tried with 1.4 but OSX 10.13 and CUDA 9 cuDNN 7. You can find steps [here](https://gist.github.com/smitshilu/53cf9ff0fd6cdb64cca69a7e2827ed0f)", "@eeilon79 there is an nvcc bug on windows that prevent us frombuilding binaries. We are getting help from nvidia to fix those issues.", "Is there any update for CUDA 9 in the Tensorflow Nightly Version (1.5-dev) under the `tf-nightly-gpu` pip package? Need to use this 1.5 for the CuDNNLSTM in Keras", "OK, the PR is just merged.\r\nIn about 10-12 hours our new nightlies should be built with cuda9, except for windows.\r\nOn windows, we are still  blocked by an NVCC bug.", "I finished general CUDA 9 and CUDANN 7 package for Gentoo system and tried dummy test and looks like working by dummy import tensorflow as tf in python, but I need to do additional tests:\r\n\r\nI am using commit: c9568f1ee51a265db4c5f017baf722b9ea5ecfbb", "> On windows, we are still blocked by an NVCC bug.\n\nWould you mind posting the link in here to that issue? Thanks in advance!\n", "@smitshilu Your article helped me.\r\nAnd I wrote an article with some elements added.\r\nhttps://github.com/masasys/MacTF1.4GPU", "@arbynacosta\r\ni run\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\nbut i get this error\r\nError : the build command is only supported from within a workspace.\r\n\r\ni also try tensor nightly but it give error\r\nattributeerror : module 'tensorflow' has no attribute ....\r\noutput of dir(tf)\r\n['__doc__', '__loarder__', '__name__', __package__' '__path__ __spec__]", "Sorry @goodmangu, I was away. Did you get it working? I did miss the closing double quotes as @arbynacosta pointed out. I have this running under Ubuntu 17.10 now with CUDA 9.0 and cuDNN 7.0.4. I can work on a MacOS build if needed. I bailed on both Win10 and MacOS but can work on them this weekend if folks need it.\r\n\r\n@nasergh, are you running that command from inside the cloned tensorflow directory. Make sure WORKSPACE exists in the directory.\r\n\r\nFor example:\r\n```\r\n~/Downloads/tensorflow$ ls\r\nACKNOWLEDGMENTS     bazel-bin         bazel-testlogs      configure          LICENSE       tensorflow   WORKSPACE\r\nADOPTERS.md         bazel-genfiles    BUILD               configure.py       models.BUILD  third_party\r\narm_compiler.BUILD  bazel-out         CODE_OF_CONDUCT.md  CONTRIBUTING.md    README.md     tools\r\nAUTHORS             bazel-tensorflow  CODEOWNERS          ISSUE_TEMPLATE.md  RELEASE.md    util\r\n```\r\n", "@Tweakmind \r\ni run command \r\nsudo su\r\nand then goto tensorflow folder(there was workspace file in there)\r\nbut i get this errors\r\n```\r\nroot@pc:/home/pc2/tensorflow# bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\n..........\r\nWARNING: Config values are not defined in any .rc file: opt\r\nERROR: /root/.cache/bazel/_bazel_root/cccfa03cbaf937d443248403ec70306e/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):\r\n\tFile \"/root/.cache/bazel/_bazel_root/cccfa03cbaf937d443248403ec70306e/external/local_config_cuda/crosstool/BUILD\", line 4\r\n\t\terror_gpu_disabled()\r\n\tFile \"/root/.cache/bazel/_bazel_root/cccfa03cbaf937d443248403ec70306e/external/local_config_cuda/crosstool/error_gpu_disabled.bzl\", line 3, in error_gpu_disabled\r\n\t\tfail(\"ERROR: Building with --config=c...\")\r\nERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\r\nERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /root/.cache/bazel/_bazel_root/cccfa03cbaf937d443248403ec70306e/external/local_config_cuda/crosstool/BUILD\r\nINFO: Elapsed time: 6.830s\r\nFAILED: Build did NOT complete successfully (2 packages loaded)\r\n    currently loading: @bazel_tools//tools/jdk\r\n\r\n```", "@nasergh Please make sure you follow all of the instructions here:\r\nhttps://www.tensorflow.org/install/install_sources", "If you are building with GPU support, make sure you are configuring appropriately. \r\n\r\n#### Install Tensorflow 1.4 from Source.\r\n* As of this writing, this is the only way it will work with CUDA 9.0 and cuDNN 7.0\r\n* Instructions: https://www.tensorflow.org/install/install_sources\r\n* Some of the instructions may not make sense, here is how I did it:\r\n```\r\ncd $HOME/Downloads\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\ngit checkout r1.4\r\n./configure\r\n```\r\nThe sample output and options will differ from that in the instructions\r\n* Make sure you configure for CUDA version: 9.0\r\n* Make sure you configure for cuDNN version: 7.0.4\r\n* Make sure you know your compute capability from https://developer.nvidia.com/cuda-gpus\r\n* I set this to 6.1 as I have a GeForce GTX 1070\r\n\r\nInstalling Bazel\r\n```\r\nsudo add-apt-repository ppa:webupd8team/java\r\nsudo apt-get update && sudo apt-get install oracle-java8-installer\r\necho \"deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list\r\ncurl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add -\r\nsudo apt-get update && sudo apt-get install bazel\r\nsudo /sbin/ldconfig -v\r\n```\r\nBuilding TensorFlow\r\n```\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\npip install /tmp/tensorflow_pkg/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl\r\n```\r\n\r\n\r\n\r\n", "@Tweakmind : thanks for getting back to me on this. Yes, I got it working for Ubuntu, and still have no luck with Mac OSX (10.12.6) with an eGPU (1080 Ti). For all the sources I followed,  build failed after some 10-15 minutes. It would be great if we have a reproducible success. Thanks in advance.", "@Tweakmind \r\ni do following and also configure the file but it say \r\n```\r\npc2@pc:~/Downloads/tensorflow$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/pc2/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1042\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/pc2/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 905, in _create_local_cuda_repository\r\n\t\t_get_cuda_config(repository_ctx)\r\n\tFile \"/home/pc2/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 662, in _get_cuda_config\r\n\t\t_cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n\tFile \"/home/pc2/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 360, in _cudnn_version\r\n\t\t_find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\r\n\tFile \"/home/pc2/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 612, in _find_cudnn_header_dir\r\n\t\tauto_configure_fail((\"Cannot find cudnn.h under %s\" ...))\r\n\tFile \"/home/pc2/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 129, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find cudnn.h under /usr/lib/x86_64-linux-gnu\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/pc2/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1042\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/pc2/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 905, in _create_local_cuda_repository\r\n\t\t_get_cuda_config(repository_ctx)\r\n\tFile \"/home/pc2/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 662, in _get_cuda_config\r\n\t\t_cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n\tFile \"/home/pc2/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 360, in _cudnn_version\r\n\t\t_find_cudnn_header_dir(repository_ctx, cudnn_install_base...)\r\n\tFile \"/home/pc2/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 612, in _find_cudnn_header_dir\r\n\t\tauto_configure_fail((\"Cannot find cudnn.h under %s\" ...))\r\n\tFile \"/home/pc2/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 129, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find cudnn.h under /usr/lib/x86_64-linux-gnu\r\nINFO: Elapsed time: 0.082s\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n\r\n```\r\ni think i install cuda and cudnn correctly\r\n ```\r\nfind /usr | grep libcudnn\r\n/usr/share/doc/libcudnn7\r\n/usr/share/doc/libcudnn7/copyright\r\n/usr/share/doc/libcudnn7/NVIDIA_SLA_cuDNN_Support.txt\r\n/usr/share/doc/libcudnn7/changelog.Debian.gz\r\n/usr/share/lintian/overrides/libcudnn7\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.4\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7\r\n\r\n```", "@goodmangu, I will work on the MacOS build over the weekend.\r\n\r\n@nasergh, Did you install cuDNN?\r\n\r\nHere is my guide for cuDNN including source and docs to test the install:\r\n#### Download cuDNN 7.0.4 files\r\nYou must log into your Nvidia developer account in your browser\r\n* https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.0.4/prod/9.0_20171031/cudnn-9.0-linux-x64-v7\r\n* https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.0.4/prod/9.0_20171031/Ubuntu16_04-x64/libcudnn7_7.0.4.31-1+cuda9.0_amd64\r\n* https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.0.4/prod/9.0_20171031/Ubuntu16_04-x64/libcudnn7-dev_7.0.4.31-1+cuda9.0_amd64\r\n* https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.0.4/prod/9.0_20171031/Ubuntu16_04-x64/libcudnn7-doc_7.0.4.31-1+cuda9.0_amd64\r\n\r\nCheck Each Hash\r\n```\r\ncd $HOME/Downloads\r\nmd5sum cudnn-9.0-linux-x64-v7.tgz && \\\r\nmd5sum libcudnn7_7.0.4.31-1+cuda9.0_amd64.deb && \\\r\nmd5sum libcudnn7-dev_7.0.4.31-1+cuda9.0_amd64.deb && \\\r\nmd5sum libcudnn7-doc_7.0.4.31-1+cuda9.0_amd64.deb\r\n```\r\nOutput should be:\r\n\r\n> fc8a03ac9380d582e949444c7a18fb8d  cudnn-9.0-linux-x64-v7.tgz\r\n> e986f9a85fd199ab8934b8e4835496e2  libcudnn7_7.0.4.31-1+cuda9.0_amd64.deb\r\n> 4bd528115e3dc578ce8fca0d32ab82b8  libcudnn7-dev_7.0.4.31-1+cuda9.0_amd64.deb\r\n> 04ad839c937362a551eb2170afb88320  libcudnn7-doc_7.0.4.31-1+cuda9.0_amd64.deb\r\n\r\n#### Install cuDNN 7.0.4 and libraries\r\n```\r\ntar -xzvf cudnn-9.0-linux-x64-v7.tgz\r\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include\r\nsudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\r\nsudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\r\nsudo dpkg -i libcudnn7_7.0.4.31-1+cuda9.0_amd64.deb\r\nsudo dpkg -i libcudnn7-dev_7.0.4.31-1+cuda9.0_amd64.deb\r\nsudo dpkg -i libcudnn7-doc_7.0.4.31-1+cuda9.0_amd64.deb\r\n```\r\n#### Verifying cuDNN\r\nUbuntu 17.10 includes version 7+ of the GNU compilers\r\nCUDA is not compatible with higher than version 6\r\nThe error returned is:\r\n\r\n> #error -- unsupported GNU version! gcc versions later than 6 are not supported!\r\n\r\nFix - Install Version 6 and create symbolic links in CUDA bin directory:\r\n```\r\nsudo apt-get install gcc-6 g++-6\r\nsudo ln -sf /usr/bin/gcc-6 /usr/local/cuda/bin/gcc\r\nsudo ln -sf /usr/bin/g++-6 /usr/local/cuda/bin/g++\r\n```\r\nNow build mnistCUDNN to test cuDNN\r\n```\r\ncp -r /usr/src/cudnn_samples_v7/ $HOME\r\ncd $HOME/cudnn_samples_v7/mnistCUDNN\r\nmake clean && make\r\n./mnistCUDNN\r\n```\r\nIf cuDNN is properly installed, you will see:\r\n\r\n> Test passed!\r\n\r\n", "Dear @Tweakmind \r\nyour way works thanks for your help(i was trying to install tensor for more than 3 weeks!!!)\r\nproblem is i install it on python3.6 and now i have a problem with PIL package\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/pc2/venv/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 551, in get\r\n    inputs = self.queue.get(block=True).get()\r\n  File \"/home/pc2/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 644, in get\r\n    raise self._value\r\n  File \"/home/pc2/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home/pc2/venv/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 391, in get_index\r\n    return _SHARED_SEQUENCES[uid][i]\r\n  File \"/home/pc2/venv/lib/python3.6/site-packages/keras/preprocessing/image.py\", line 761, in __getitem__\r\n    return self._get_batches_of_transformed_samples(index_array)\r\n  File \"/home/pc2/venv/lib/python3.6/site-packages/keras/preprocessing/image.py\", line 1106, in _get_batches_of_transformed_samples\r\n    interpolation=self.interpolation)\r\n  File \"/home/pc2/venv/lib/python3.6/site-packages/keras/preprocessing/image.py\", line 345, in load_img\r\n    raise ImportError('Could not import PIL.Image. '\r\nImportError: Could not import PIL.Image. The use of `array_to_img` requires PIL.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 7, in <module>\r\n  File \"/home/pc2/venv/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/pc2/venv/lib/python3.6/site-packages/keras/models.py\", line 1227, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File \"/home/pc2/venv/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/pc2/venv/lib/python3.6/site-packages/keras/engine/training.py\", line 2115, in fit_generator\r\n    generator_output = next(output_generator)\r\n  File \"/home/pc2/venv/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 557, in get\r\n    six.raise_from(StopIteration(e), e)\r\n  File \"<string>\", line 3, in raise_from\r\nStopIteration: Could not import PIL.Image. The use of `array_to_img` requires PIL.\r\n```\r\n\r\ni try to install pillow but it's not help\r\ni also try to install PIL but \r\n```\r\nUnsatisfiableError: The following specifications were found to be in conflict:\r\n  - pil -> python 2.6*\r\n  - python 3.6*\r\n\r\n```", "@nasergh What do you get with:\r\n```\r\npip install pillow\r\n```\r\nMine looks like:\r\n```\r\n~$ pip install pillow\r\nRequirement already satisfied: pillow in ./anaconda3/lib/python3.6/site-packages\r\n```", "@nasergh, I need to crash but I'll check in when I get up.", "@goodmangu, I won't be able to do the Mac build over the weekend as I don't have access to my 2012 Mac Pro. Hopefully, you're good with Ubuntu for now. I know it works well for me. I should have it back next weekend.", "@Tweakmind - Thanks! , have you seen any performance boost with CUDA 9 and cuDNN 7?\r\n\r\nAlso I think some steps mentioned by @Tweakmind  below are redundant, you either need:\r\n\r\n```\r\ntar -xzvf cudnn-9.0-linux-x64-v7.tgz\r\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include\r\nsudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\r\nsudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\r\n```  \r\n\r\nor \r\n\r\n```\r\nsudo dpkg -i libcudnn7_7.0.4.31-1+cuda9.0_amd64.deb\r\nsudo dpkg -i libcudnn7-dev_7.0.4.31-1+cuda9.0_amd64.deb\r\nsudo dpkg -i libcudnn7-doc_7.0.4.31-1+cuda9.0_amd64.deb\r\n\r\n```\r\n", "@gunan \r\nCUDA 9.1.85 was just released moments ago with CuDNN 7.0.5, with nvcc compiler bug fixes.  I wonder if it lets win10 users compile Tensorflow 1.4.1?  It is about time.", "From our correspondences with NVIDIA, I dont think 9.1 fixed this issue.\r\nHowever, we have workarounds. First, we need this PR to be merged into eigen:\r\nhttps://bitbucket.org/eigen/eigen/pull-requests/351/win-nvcc/diff\r\n\r\nThen we will update our eigen dependency, which should fix all our builds for CUDA9", "The pr is declined but it seams to be merged manually. Do we have to wait for a eigen release or is it getting built by the sources?", "Cool, then it will be on Nightly pip?", "@Tweakmind \r\ni try to rebuild tensor with using python 2.7\r\nbut in bazel build i get this  error\r\ni also install numpy but no change.\r\n\r\n```\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\nERROR: /home/gh2/Downloads/tensorflow/util/python/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n\tFile \"/home/gh2/Downloads/tensorflow/third_party/py/python_configure.bzl\", line 310\r\n\t\t_create_local_python_repository(repository_ctx)\r\n\tFile \"/home/gh2/Downloads/tensorflow/third_party/py/python_configure.bzl\", line 274, in _create_local_python_repository\r\n\t\t_get_numpy_include(repository_ctx, python_bin)\r\n\tFile \"/home/gh2/Downloads/tensorflow/third_party/py/python_configure.bzl\", line 257, in _get_numpy_include\r\n\t\t_execute(repository_ctx, [python_bin, \"-c\",...\"], <2 more arguments>)\r\n\tFile \"/home/gh2/Downloads/tensorflow/third_party/py/python_configure.bzl\", line 76, in _execute\r\n\t\t_python_configure_fail(\"\\n\".join([error_msg.strip() if ... \"\"]))\r\n\tFile \"/home/gh2/Downloads/tensorflow/third_party/py/python_configure.bzl\", line 37, in _python_configure_fail\r\n\t\tfail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: Problem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n**ImportError: No module named numpy**\r\nIs numpy installed?\r\n and referenced by '//util/python:python_headers'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed\r\nINFO: Elapsed time: 10.826s\r\nFAILED: Build did NOT complete successfully (26 packages loaded)\r\n    currently loading: tensorflow/core ... (3 packages)\r\n    Fetching http://mirror.bazel.build/.../~ooura/fft.tgz; 20,338b 5s\r\n    Fetching http://mirror.bazel.build/zlib.net/zlib-1.2.8.tar.gz; 19,924b 5s\r\n    Fetching http://mirror.bazel.build/.../giflib-5.1.4.tar.gz; 18,883b 5s\r\n```", "It seems that OSX is excluded in version 7.0.5 of cuDNN. Does anyone know a detailed thing?", "I still can't get tensorflow-gpu to work in Windows 10 (with CUDA 9.0.176 and cudnn 7.0).\r\nI've uninstalled both tensorflow and tensorflow-gpu and reinstalled them (with the --no-cache-dir to ensure downloading of most recent version with the eigen workaround). When I install both, my GPU is not recognized:\r\n\r\n> InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'random_uniform_1/sub': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\r\n\r\nWhen I install just tensorflow-gpu it complains about a missing dll:\r\n\r\n> ImportError: Could not find 'cudart64_80.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 8.0 from this URL: https://developer.nvidia.com/cuda-toolkit\r\n\r\nWhich is weird because my CUDA version is 9.0, not 8.0, and is recognized (deviceQuery test passed).\r\nMy python version is 3.6.3. I'm trying to run [this](https://medium.com/@erikhallstrm/hello-world-tensorflow-649b15aed18c) code in Spyder (3.2.4) in order to test tensorflow-gpu.\r\nWhat did I miss?", "I'm trying to build from source by bazel on win 7, get error\r\n\r\n> No toolcahin for cpu 'x64_windows'\r\n\r\nCan anyone build whl?", "@hadaev8, I need a lot more information to help. I can work on a whl but it will have heavy dependencies and not Win7, once I solve MacOS, I will solve Win10. In any case, post your details.\r\n\r\n@eeilon79, I need to recreate this under Win10. I'm currently focused on MacOS now that Ubuntu is solved. I will come back to Win 10. ", "@nasergh, is there a requirement for python 2.7?", "With CUDA 8.0 and cuDNN 6.0, this is how I installed TensorFlow from source for Cuda GPU and AVX2 support in Win10::\r\n\r\nRequirements:\r\n\r\n\t* Windows 10 64-Bit\r\n\t* Visual Studio 15 C++ Tools\r\n\t* NVIDIA CUDA\u00ae Toolkit 8.0\r\n\t* NVIDIA cuDNN 6.0 for CUDA 8.0\r\n\t* Cmake\r\n\t* Swig\r\n\r\nInstall Visual Studio Community Edition Update 3 w/Windows Kit 10.0.10240.0\r\nFollow instructions at: https://github.com/philferriere/dlwin  (Thank you Phil)\r\n\r\nCreate a Virtual Drive N: for clarity\r\nI suggest creating a directory off C: or your drive of choice and creating N: based on these instructions (2GB min):\r\nhttps://technet.microsoft.com/en-us/library/gg318052(v=ws.10).aspx\r\n\r\nInstall Cuda 8.0 64-bit\r\nhttps://developer.nvidia.com/cuda-downloads  (Scroll down to Legacy)\r\n\r\nInstall cuDNN 6.0 for Cuda 8.0\r\nhttps://developer.nvidia.com/rdp/cudnn-download\r\nPut cuda folder from zip on N:\\and rename cuDNN-6\r\n\r\nInstall CMake\r\nhttps://cmake.org/files/v3.10/cmake-3.10.0-rc5-win64-x64.msi\r\n\r\nInstall Swig (swigwin-3.0.12)\r\nhttps://sourceforge.net/projects/swig/files/swigwin/swigwin-3.0.12/swigwin-3.0.12.zip\r\n\r\n#### cntk-py36\r\n```conda create --name cntk-py36 python=3.6 numpy scipy h5py jupyter\r\nactivate cntk-py36\r\npip install https://cntk.ai/PythonWheel/GPU/cntk-2.2-cp36-cp36m-win_amd64.whl\r\npython -c \"import cntk; print(cntk.__version__)\"\r\nconda install pygpu\r\npip install keras\r\n```\r\n#### Remove old tensorflow in Tools if it exists\r\n```cd C:\\Users\\%USERNAME%\\Tools\\\r\nmove tensorflow tensorflow.not\r\ngit clone --recursive https://github.com/tensorflow/tensorflow.git\r\ncd C:\\Users\\%USERNAME%\\Tools\\tensorflow\\tensorflow\\contrib\\cmake\r\nEdit CMakeLists.txt\r\n```\r\nComment out these:\r\n```\r\n# if (tensorflow_OPTIMIZE_FOR_NATIVE_ARCH)\r\n#   include(CheckCXXCompilerFlag)\r\n#   CHECK_CXX_COMPILER_FLAG(\"-march=native\" COMPILER_OPT_ARCH_NATIVE_SUPPORTED)\r\n#   if (COMPILER_OPT_ARCH_NATIVE_SUPPORTED)\r\n#     set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -march=native\")\r\n#   endif()\r\n# endif()\r\n```\r\nAdd these:\r\n```\r\nif (tensorflow_OPTIMIZE_FOR_NATIVE_ARCH)\r\n  include(CheckCXXCompilerFlag)\r\n  CHECK_CXX_COMPILER_FLAG(\"-march=native\" COMPILER_OPT_ARCH_NATIVE_SUPPORTED)\r\n  if (COMPILER_OPT_ARCH_NATIVE_SUPPORTED)\r\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -march=native\")\r\n  else()\r\n    CHECK_CXX_COMPILER_FLAG(\"/arch:AVX2\" COMPILER_OPT_ARCH_AVX_SUPPORTED)\r\n    if(COMPILER_OPT_ARCH_AVX_SUPPORTED)\r\n      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /arch:AVX2\")\r\n    endif()\r\n  endif()\r\nendif()\r\n```\r\nmkdir build & cd build\r\n```\r\n\"C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\amd64\\vcvars64.bat\"\r\n\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^\r\n-DSWIG_EXECUTABLE=N:/swigwin-3.0.12/swig.exe ^\r\n-DPYTHON_EXECUTABLE=N:/Anaconda3/python.exe ^\r\n-DPYTHON_LIBRARIES=N:/Anaconda3/libs/python36.lib ^\r\n-Dtensorflow_ENABLE_GPU=ON ^\r\n-DCUDNN_HOME=\"n:\\cuDNN-6\" ^\r\n-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2\r\n```\r\n-- Building for: Visual Studio 14 2015\r\n-- Selecting Windows SDK version 10.0.14393.0 to target Windows 10.0.16299.\r\n-- The C compiler identification is MSVC 19.0.24225.1\r\n-- The CXX compiler identification is MSVC 19.0.24225.1\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed\r\n-- Performing Test COMPILER_OPT_ARCH_AVX_SUPPORTED\r\n-- Performing Test COMPILER_OPT_ARCH_AVX_SUPPORTED - Success\r\n-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED\r\n-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED - Success\r\n-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0 (found suitable version \"8.0\", minimum required is \"8.0\")\r\n-- Found PythonInterp: C:/Users/%USERNAME%/Anaconda3/python.exe (found version \"3.6.3\")\r\n-- Found PythonLibs: C:/Users/%USERNAME%/Anaconda3/libs/python36.lib (found version \"3.6.3\")\r\n-- Found SWIG: C:/Users/%USERNAME%/Tools/swigwin-3.0.12/swig.exe (found version \"3.0.12\")\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: C:/Users/%USERNAME%/Tools/tensorflow/tensorflow/contrib/cmake/build\r\n```\r\nMSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj\r\n```", "@Tweakmind\r\npython 3.6, tensorflow last from master, cuda 9.0, cudnn 7.0.5 for cuda 9.0, basel and swig loaded today.", "@Tweakmind do you build with master or ?", "@Tweakmind \r\nMay you build on windows with cuda 9 cudnn 7 and share .whl?", "@Tweakmind\r\n\r\nDon't you try to build on win 10 with cuda 9 cudnn 7 ? \r\n\r\nThanks for your expertise !\r\n", "@hadaev8 @alc5978 \r\npip install -U tf-nightly-gpu now gives a win10 build dated 20171221, which is based on TF 1.5 beta with CUDA 9.0 and CuDNN 7.0.5.  I ran it last night, it is ok.  Now we should move onto CUDA 9.1 for the 12x CUDA kernel launch speed.  Tensorflow windows support is pretty slow and anemic.  Stable official builds should be offered ASAP.  I am actually for Tensorflow 1.5 stable to be released with CUDA 9.1, by the end of January please?", "Go to http://www.python36.com/install-tensorflow141-gpu/ for step by step installation of tensorflow with cuda 9.1 and cudnn7.05 on ubuntu. And go to http://www.python36.com/install-tensorflow-gpu-windows for step by step installation of tensorflow with cuda 9.1 and cudnn 7.0.5 on Windows.", "It's 2018, almost end of January and installation of TF with CUDA9.1 and CuDNN7 on Windows 10 is still not done?", "[1.5 is RC](https://pypi.python.org/pypi/tensorflow) with CUDA 9 + cuDNN 7 and should go GA in the next few days.   (CUDA 9.1 was GA in December and requires another device driver upgrade that is disruptive to many users.  The current plan is to keep the default build on CUDA 9.0.x and keep upgrading to newer cuDNN versions).  \r\n\r\nI opened an [issue](https://github.com/tensorflow/tensorflow/issues/15140) to discuss CUDA 9.1.\r\n\r\nThe 12x kernel launch speed improvement is more nuanced than the 12x number.  The top end of 12x is for `ops` with a lot of arguments and the disruption to users is high due to the device driver upgrade.  I hope to have a \"channel\" testing 9.1 in the near future and figure out how to deal with this paradigm.  ", "I hope it will be finally CUDA 9.1, not 9.0.", "I hope it will be finally CUDA 9.1, not 9.0 too.", "I 'm sur it will be finally CUDA 9.1, not 9.0 too, isn't it ? :)", "@ViktorM @Magicfeng007 @alc5978 \r\nThe 9.1 [thread  is here](https://github.com/tensorflow/tensorflow/issues/15140) if you want to follow along although it is basically closed.  If you could list why you want 9.1 that would be useful, and what is your setup/configuration.  A benchmark that you ran showing the perf boost would also be useful in understanding the immediate need.  In meetings with NVIDIA, we both agreed there was not an immediate need to make 9.1 the default. which would then force people to upgrade their drivers again.  ", "If anybody are still facing problems like Keras with TensorFlow backend not using GPU.... just follow the instructions in this page. It is updated and works 100% correctly.\r\nhttps://research.wmz.ninja/articles/2017/01/configuring-gpu-accelerated-keras-in-windows-10.html", "Hi All\r\nI today install tensorflow-gpu 1.6.0rc1 on win10  with CUDA 9.0 and cuDNN 7.0.5 library with http://www.python36.com/install-tensorflow-using-official-pip-pacakage/\r\n\r\nEverything seems ok \r\n", "I created one script for NVIDIA GPU prerequisites (CUDA-9.0 and cuDNN-7.0) for the latest TensorFlow (v1.5+), [here is the link](https://gist.github.com/ashokpant/5c4e9481615f54af4025ab2085f85869)."]}, {"number": 12051, "title": "an API to tell TF ABI", "body": "This is a feature request.\r\nTF pip packages might be built with different C++ ABI. The released binaries are built with old ABI. If a user manually compile it with gcc>=5, the default is to use new CXX11 ABI (unless explicitly changed).\r\n\r\nAs someone who wrote custom ops, this could cause trouble: the op has to be compiled with the same ABI, otherwise there will be issues like #10714 #9137. Therefore the user of my ops would need to be aware of what ABI he's using, and change the flags manually.\r\n\r\nI hope there is an API simply tells what ABI should be used when compiling user ops, similar to `tf.sysconfig.get_include()` which tells what path to include.", "comments": ["cc @keveman ", "@asimshankar Do you have any suggestions here?  C++ ABI compatibility is a can of worms.  :)", "Uber has a complicated way to test TF ABI:\r\nhttps://github.com/uber/horovod/blob/10835d25eccf4b198a23a0795edddf0896f6563d/setup.py#L72-L100\r\n\r\nwhich again shows the importance of this feature.", "Fixed by https://github.com/tensorflow/tensorflow/pull/13496"]}, {"number": 12050, "title": "Fix debugger logic in r1.3", "body": "In tensorflow/tensorflow#11952, I had set made some logic within debug_grpc_testlib return too early, breaking some debugger-related behavior. This PR fixes that.", "comments": ["@tensorflow-jenkins test this please", "The Linux GPU build is failing in an obscure way. Testing again\r\n\r\n@tensorflow-jenkins test this please.", "Ah - the GPU build failure can be ignored. It is due to an unrelated test tag change.", "@chihuahua, does this change need to be commit to the master branch as well?", "@caisq, tensorflow/tensorflow#12012 already makes this change in master. Thanks for starting tests. Let me try again.\r\n\r\n@tensorflow-jenkins test this please."]}, {"number": 12049, "title": "Branch 164309367", "body": "", "comments": []}, {"number": 12048, "title": "Fix cmake builds", "body": "https://github.com/tensorflow/tensorflow/commit/8bf3f88f72a3671f888ca463deeabdf2e6658fc2\r\nremoved tpu_sendrecv_ops.cc ... reflect this in cmake.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please!", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (26/25 (104%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from o4.sgmail.github.com ([192.254.112.99]:2199)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES128-GCM-SHA256:128)\n\t(Exim 4.89)\n\t(envelope-from <bounces+848413-e6d9-mazecreator=mazecreator.com@sgmail.github.com>)\n\tid 1dem5e-0007au-AQ\n\tfor mazecreator@mazecreator.com; Mon, 07 Aug 2017 12:45:48 -0500\nDKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=github.com; \n\th=from:reply-to:to:cc:in-reply-to:references:subject:mime-version:content-type:content-transfer-encoding:list-id:list-archive:list-post:list-unsubscribe; \n\ts=s20150108; bh=e1nnIGLwm2ejDJ2//bF9aNjwJPw=; b=n0JU3WSpWF0GO1HS\n\toQjv7XEOowp6fXBNdiilqX9iU3FBY1o1XCK0V5ImtbLOGxpDfR/2pwKaD0efc6jC\n\tVYw5pCUgqvYnrUix4DQn3S5SEUnaHrGNEhudTQ/OVjeDr/+I3X0C/4Zeeo7waTuq\n\tAb123Tu91XTcIFmNj40rZXbG6GA=\nReceived: by filter1106p1mdw1.sendgrid.net with SMTP id filter1106p1mdw1-31815-5988A9BA-83\n        2017-08-07 17:56:10.826239408 +0000 UTC\nReceived: from github-smtp2a-ext-cp1-prd.iad.github.net (github-smtp2a-ext-cp1-prd.iad.github.net [192.30.253.16])\n\tby ismtpd0002p1iad1.sendgrid.net (SG) with ESMTP id qTF_PzGPRoiLShMsSkBZLw\n\tfor <mazecreator@mazecreator.com>; Mon, 07 Aug 2017 17:56:10.808 +0000 (UTC)\nDate: Mon, 07 Aug 2017 17:56:10 +0000 (UTC)\nFrom: Derek Murray <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/pull/12048/c320733771@github.com>\nIn-Reply-To: <tensorflow/tensorflow/pull/12048@github.com>\nReferences: <tensorflow/tensorflow/pull/12048@github.com>\nSubject: Re: [tensorflow/tensorflow] Fix cmake builds (#12048)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5988a9ba807e8_4c9b3f9177bebc3c338239\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: mrry\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a04d648b01473b82eeee3de365400032ef38b4023692cf0000000115a06bba92a169ce0ec9c0c2@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoM2e44y_NS1r8E0HpUJm3CrEcSJOks5sV0-6gaJpZM4OuIR_>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-SG-EID: BJ4qYjf5a3yL0lCrdDNghY4YYR+k1a+cluU6wEX1JqxIJZKhvxOYS9eVjJmxjq251sO37a+a74EU/U\n yZmAiF4Rrfev9FmHHoPn/OtyLaXU5bbYWofGsPkPNnc4YtpT4q7KSvboeWm2Ua87DJyIubiMCNBgBD\n giMxaCDCZr6iqn9fN8GTOrK6nSIgdC1cr7kgffuBo88u+RYILS0HG9viS3XhW0zHbpUI7n6ij7JQPX\n w=\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n----==_mimepart_5988a9ba807e8_4c9b3f9177bebc3c338239\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n@tensorflow-jenkins test this please!\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/pull/12048#issuecomment-320733771\n----==_mimepart_5988a9ba807e8_4c9b3f9177bebc3c338239\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p><a href=\"https://github.com/tensorflow-jenkins\" class=\"user-mention\">@tensorflow-jenkins</a> test this please!</p>\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/pull/12048#issuecomment-320733771\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoBbDqAznD8AWaL3CKJUh1mk2K79Jks5sV0-6gaJpZM4OuIR_\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoO7WkSdDig8sd2GmOgVlBucmb6twks5sV0-6gaJpZM4OuIR_.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/pull/12048#issuecomment-320733771\"></link>\n  <meta itemprop=\"name\" content=\"View Pull Request\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Pull Request on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@mrry in #12048: @tensorflow-jenkins test this please!\"}],\"action\":{\"name\":\"View Pull Request\",\"url\":\"https://github.com/tensorflow/tensorflow/pull/12048#issuecomment-320733771\"}}}</script>\n----==_mimepart_5988a9ba807e8_4c9b3f9177bebc3c338239--\n", "@guschmue Thanks for the fix!"]}, {"number": 12047, "title": "Could you please provide a TensorFlow.dll 32 bits ? many thanks", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Compiling custom tensorflow binaries is generally left to users rather than TensorFlow team. There are some binaries uploaded [here](https://github.com/yaroslavvb/tensorflow-community-wheels), maybe you could ask an active uploader to build one for you"]}, {"number": 12046, "title": "tf.scatter_update to variable pinned on GPU fails", "body": "### System information\r\n- Linux Ubuntu 16.04\r\n- tensorflow-gpu v1.2.0 binary installed from pip\r\n- Python 3.5\r\n- CUDA 8.0, cuDNN v5.1 \r\n- GeForce GTX 1080 Ti, 11GB\r\n- A simple example I came up with which reproduces the error follows below:\r\n**************************************************************************************************\r\n```python\r\nimport tensorflow as tf\r\nwith tf.device(\"/gpu:0\"):\r\n    a = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\r\n    c = tf.Variable([0, 0, 0, 0, 0, 0, 0, 0])\r\n    step_sos = tf.Variable([False, False, True, True, False, False, True, True])\r\n    write_ops = []\r\n    for b in range(8):\r\n        write_ops.append(tf.cond(step_sos[b], lambda: tf.scatter_update(a, b, 0), lambda: a))\r\n\r\n    with tf.control_dependencies(write_ops):\r\n       d = tf.assign(c, a)\r\n\r\n\r\nsession_config = tf.ConfigProto(allow_soft_placement=False, log_device_placement=True)\r\n\r\nwith tf.Session(config=session_config) as sess:\r\n    init_op = tf.global_variables_initializer()\r\n    sess.run(init_op)\r\n    sess.run(d)\r\n    print(sess.run(c))\r\n```\r\n********************************************************************************************************              \r\n\r\n### \r\nWhen setting allow_soft_placement=True the error is solved but the variable and some operations which are often used are placed in the CPU, which leads to fluctuating GPU utilization.\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1139, in _do_call\r\n    return fn(*args)\r\n  File \"/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1117, in _run_fn\r\n    self._extend_graph()\r\n  File \"/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1166, in _extend_graph\r\n    self._session, graph_def.SerializeToString(), status)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Variable_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nAssign: CPU \r\nIdentity: CPU \r\nVariableV2: CPU \r\n\t [[Node: Variable_2 = VariableV2[container=\"\", dtype=DT_BOOL, shape=[8], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"scatter_test.py\", line 20, in <module>\r\n    sess.run(init_op)\r\n  File \"/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 997, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Variable_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nAssign: CPU \r\nIdentity: CPU \r\nVariableV2: CPU \r\n\t [[Node: Variable_2 = VariableV2[container=\"\", dtype=DT_BOOL, shape=[8], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\r\n\r\nCaused by op 'Variable_2', defined at:\r\n  File \"scatter_test.py\", line 7, in <module>\r\n    step_sos = tf.Variable([False, False, True, True, False, False, True, True])\r\n  File \"/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 200, in __init__\r\n    expected_shape=expected_shape)\r\n  File \"/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 297, in _init_from_args\r\n    name=name)\r\n  File \"/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/ops/state_ops.py\", line 128, in variable_op_v2\r\n    shared_name=shared_name)\r\n  File \"/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 684, in _variable_v2\r\n    name=name)\r\n  File \"/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/ylli/neuralattention/code/venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Variable_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nAssign: CPU \r\nIdentity: CPU \r\nVariableV2: CPU \r\n\t [[Node: Variable_2 = VariableV2[container=\"\", dtype=DT_BOOL, shape=[8], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\r\n```\r\n", "comments": ["I'm able to reproduce with this simplified case:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n  tf.Variable([False, False, True, True, False, False, True, True])\r\n\r\nsession_config = tf.ConfigProto(allow_soft_placement=False, log_device_placement=True)\r\n\r\nwith tf.Session(config=session_config) as sess:\r\n  init_op = tf.global_variables_initializer()\r\n  sess.run(init_op)\r\n```\r\n\r\nSo it seems boolean Variables cannot be placed (or at least initialized) on the GPU. \r\n\r\n@mrry, can you take a look?", "It looks like the kernel backing `tf.Variable` isn't registered for `tf.bool` on GPU devices. See the use of `TF_CALL_GPU_NUMBER_TYPES()` to register the kernel for different types [here](https://github.com/tensorflow/tensorflow/blob/b93fd37e143bcdd6339f8e6081c948384a262e0b/tensorflow/core/kernels/variable_ops.cc#L85), and note that the macro's definition [here](https://github.com/tensorflow/tensorflow/blob/b93fd37e143bcdd6339f8e6081c948384a262e0b/tensorflow/core/framework/register_types.h#L171) doesn't include `bool`.", "Marking as contributions welcome if someone wants to add GPU boolean variable support.", "Have sent a PR for this issue.", "We had to revert the fix. A minimal reproducible breakage is here:\r\n```\r\ndef main(unused_argv):\r\n  dtype = tf.bool\r\n  bool_var = tf.get_variable(\"bool_var\", dtype=dtype,\r\n                             initializer=tf.cast(1, dtype),\r\n                             trainable=False)\r\n  def set_bool_var_false():\r\n    return tf.group(tf.assign(bool_var, tf.cast(0, dtype)))\r\n\r\n  conditional_on_bool_var = tf.cond(pred=tf.cast(bool_var, tf.bool),\r\n                                    true_fn=set_bool_var_false,\r\n                                    false_fn=tf.no_op)\r\n\r\n  with tf.train.MonitoredSession() as sess:\r\n    print(sess.run(bool_var))\r\n    print(sess.run(conditional_on_bool_var))\r\n    print(sess.run(bool_var))\r\n```", "I'm confused where the attribute type will become the ref-type.", "Hi @muhadriy ! \r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more.  I could not find issue in [2.7 ](https://colab.sandbox.google.com/gist/mohantym/c0cdb3e63396b18b9d2aeec9b4035171/github_12046.ipynb)though. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 12045, "title": "Merge Rc2 back to master.", "body": "", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @alanyee and @keveman to be potential reviewers."]}, {"number": 12044, "title": "MKL: Removing visited_node hash table - fixing multinode shape mismatch issue", "body": "This PR fixes an issue that arises in multinode setup. MklLayoutRewritePass\r\nmaintains a hash table for visited nodes, and the hash table is part of the\r\npass and used for every graph being rewritten. But it looks like in multinode\r\nsetup, multiple graphs may be processed simulteneously leading to incorrect\r\nmodifications to the hash table. So removing the hash table as we do not\r\nreally need it.", "comments": ["Can one of the admins verify this patch?", "@nhasabni, thanks for your PR! By analyzing the history of the files in this pull request, we identified @caisq, @tensorflower-gardener and @martinwicke to be potential reviewers.", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (29/25 (115%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from o11.sgmail.github.com ([167.89.101.202]:2958)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES128-GCM-SHA256:128)\n\t(Exim 4.89)\n\t(envelope-from <bounces+848413-e6d9-mazecreator=mazecreator.com@sgmail.github.com>)\n\tid 1ddgsG-0003QU-I7\n\tfor mazecreator@mazecreator.com; Fri, 04 Aug 2017 12:59:30 -0500\nDKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=github.com; \n\th=from:reply-to:to:cc:subject:mime-version:content-type:content-transfer-encoding:list-id:list-archive:list-post:list-unsubscribe; \n\ts=s20150108; bh=NvO5B3vmfojaNDd6C0u+AIQthRc=; b=aXJn+ZuGpg1OuPNN\n\tMx89X13fUEwaltVuCh+YjBGW3tn3J7/K24ZAOrjAZh95IHOpzccZPj+8xlaRmM85\n\tI2QWbhMR8dZev3cVcBH0swyxnGgNf15lVYB/Prl4N+VZmjJcznXjirFqrTb9VZ++\n\t3cx+MSW97rkJRUx0eTt5nfObVXk=\nReceived: by filter0553p1mdw1.sendgrid.net with SMTP id filter0553p1mdw1-3629-5984B867-4F\n        2017-08-04 18:09:43.709876163 +0000 UTC\nReceived: from github-smtp2a-ext-cp1-prd.iad.github.net (github-smtp2a-ext-cp1-prd.iad.github.net [192.30.253.16])\n\tby ismtpd0026p1mdw1.sendgrid.net (SG) with ESMTP id h2CLpfkzRRmoRQ6eG1yeaA\n\tfor <mazecreator@mazecreator.com>; Fri, 04 Aug 2017 18:09:43.653 +0000 (UTC)\nDate: Fri, 04 Aug 2017 18:09:43 +0000 (UTC)\nFrom: Niranjan Hasabnis <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/pull/12044@github.com>\nSubject: [tensorflow/tensorflow] Removing visited_node hash table - fixing\n multinode shape mismatch issue (#12044)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5984b8675f8cc_5f933fb873449c2c30804d\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: nhasabni\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a01da1ab456eb0bf67c67bc4a63f8a4acf2ef3bf6492cf00000001159c7a6792a169ce0ec95e35@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoKfhfsfeNTZVCM31sEd-FijKm9z2ks5sU15ngaJpZM4OuBZn>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-SG-EID: BJ4qYjf5a3yL0lCrdDNghY4YYR+k1a+cluU6wEX1JqxAs5RoPBq771Vp5mLYI+BT7Eb8cdCYl8yOuw\n LEip2+5He0jxemwyUj29hbNqPf6VY1TgjAYLHWDMRdSqP1tMJMDgvD0ASLg+lEq7vcVv4jWEdtxQfT\n lGl15ehK4L7OKPEfsF4vU/7PaXE/DwcePVGo4eZ0pwpNZ4pWScNPvHRvwuldz1kMifjZuRN9GlFZnO\n RcucdT9kRjbncps95ECKKY\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n----==_mimepart_5984b8675f8cc_5f933fb873449c2c30804d\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\nThis PR fixes an issue that arises in multinode setup. MklLayoutRewritePass\nmaintains a hash table for visited nodes, and the hash table is part of the\npass and used for every graph being rewritten. But it looks like in multinode\nsetup, multiple graphs may be processed simulteneously leading to incorrect\nmodifications to the hash table. So removing the hash table as we do not\nreally need it.\nYou can view, comment on, or merge this pull request online at:\n\n  https://github.com/tensorflow/tensorflow/pull/12044\n\n-- Commit Summary --\n\n  * Removing visited_node hash table - fixing multinode shape mismatch issue\n\n-- File Changes --\n\n    M tensorflow/core/graph/mkl_layout_pass.cc (41)\n\n-- Patch Links --\n\nhttps://github.com/tensorflow/tensorflow/pull/12044.patch\nhttps://github.com/tensorflow/tensorflow/pull/12044.diff\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/pull/12044\n\n----==_mimepart_5984b8675f8cc_5f933fb873449c2c30804d\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p>This PR fixes an issue that arises in multinode setup. MklLayoutRewritePass<br>\nmaintains a hash table for visited nodes, and the hash table is part of the<br>\npass and used for every graph being rewritten. But it looks like in multinode<br>\nsetup, multiple graphs may be processed simulteneously leading to incorrect<br>\nmodifications to the hash table. So removing the hash table as we do not<br>\nreally need it.</p>\n\n<hr>\n\n<h4>You can view, comment on, or merge this pull request online at:</h4>\n<p>&nbsp;&nbsp;<a href='https://github.com/tensorflow/tensorflow/pull/12044'>https://github.com/tensorflow/tensorflow/pull/12044</a></p>\n\n<h4>Commit Summary</h4>\n<ul>\n  <li>Removing visited_node hash table - fixing multinode shape mismatch issue</li>\n</ul>\n\n<h4>File Changes</h4>\n<ul>\n  <li>\n    <strong>M</strong>\n    <a href=\"https://github.com/tensorflow/tensorflow/pull/12044/files#diff-0\">tensorflow/core/graph/mkl_layout_pass.cc</a>\n    (41)\n  </li>\n</ul>\n\n<h4>Patch Links:</h4>\n<ul>\n  <li><a href='https://github.com/tensorflow/tensorflow/pull/12044.patch'>https://github.com/tensorflow/tensorflow/pull/12044.patch</a></li>\n  <li><a href='https://github.com/tensorflow/tensorflow/pull/12044.diff'>https://github.com/tensorflow/tensorflow/pull/12044.diff</a></li>\n</ul>\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/pull/12044\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoAojCzqfYlGLwSgf9MHXS9rDWNuVks5sU15ngaJpZM4OuBZn\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoOtzpCCLRFZcBc5U1eyxYRtV6xBIks5sU15ngaJpZM4OuBZn.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/pull/12044\"></link>\n  <meta itemprop=\"name\" content=\"View Pull Request\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Pull Request on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"DESCRIPTION\",\"message\":\"Removing visited_node hash table - fixing multinode shape mismatch issue (#12044)\"}],\"action\":{\"name\":\"View Pull Request\",\"url\":\"https://github.com/tensorflow/tensorflow/pull/12044\"}}}</script>\n\n----==_mimepart_5984b8675f8cc_5f933fb873449c2c30804d--\n", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (30/25 (120%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from o10.sgmail.github.com ([167.89.101.201]:60883)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES128-GCM-SHA256:128)\n\t(Exim 4.89)\n\t(envelope-from <bounces+848413-e6d9-mazecreator=mazecreator.com@sgmail.github.com>)\n\tid 1ddgsK-0003Qg-03\n\tfor mazecreator@mazecreator.com; Fri, 04 Aug 2017 12:59:34 -0500\nDKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=github.com; \n\th=from:reply-to:to:cc:in-reply-to:references:subject:mime-version:content-type:content-transfer-encoding:list-id:list-archive:list-post:list-unsubscribe; \n\ts=s20150108; bh=JojCJuC50pBQjtqpufyp3oTzkgo=; b=G1qaRLoz7mT9Zfy0\n\tbGkvcKKbz9xVz88fGPBvIdp9s5GcEEx1f08Jw11NdxZNFSQvqMoUREWQSmmhqmCs\n\t2F5kV4ogg2RkgskNNPYZkqA/ilIjennM1o4VhRR/B/7fSOYL/qGgq5vv8pgyc9lR\n\tiIy5hpU49wS9PdhhpWYRF3Ay7kc=\nReceived: by filter0648p1mdw1.sendgrid.net with SMTP id filter0648p1mdw1-13987-5984B86C-58\n        2017-08-04 18:09:48.49781257 +0000 UTC\nReceived: from github-smtp2b-ext-cp1-prd.iad.github.net (github-smtp2b-ext-cp1-prd.iad.github.net [192.30.253.17])\n\tby ismtpd0036p1mdw1.sendgrid.net (SG) with ESMTP id Iy8FxhLcT9y55P-CEsQ8iQ\n\tfor <mazecreator@mazecreator.com>; Fri, 04 Aug 2017 18:09:48.539 +0000 (UTC)\nDate: Fri, 04 Aug 2017 18:09:48 +0000 (UTC)\nFrom: Tensorflow Jenkins <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/pull/12044/c320316290@github.com>\nIn-Reply-To: <tensorflow/tensorflow/pull/12044@github.com>\nReferences: <tensorflow/tensorflow/pull/12044@github.com>\nSubject: Re: [tensorflow/tensorflow] Removing visited_node hash table - fixing\n multinode shape mismatch issue (#12044)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5984b86be1a84_53483f90f3797c3c2869d2\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: tensorflow-jenkins\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a06e02a2286240dd61371234e053757cd82147d18692cf00000001159c7a6b92a169ce0ec95e35@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoG0gfCr2l68lnEPJbxFsuqD9TlWoks5sU15rgaJpZM4OuBZn>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-SG-EID: BJ4qYjf5a3yL0lCrdDNghY4YYR+k1a+cluU6wEX1JqxK6MqiR/bwOt9RUbRXK82kW4jE4CQCvOF2bK\n shs41jvsRml592In2clGnzpPIy5c//5dsktR3jAD5IeFwgNVyAzjDf0dK5SrpbqMoZwEWMoy+lSkRo\n vAO6cgPkg/ZunVLUicBrMO7dspJRzUwJVEh4BU8sfONaB0ryeqX3ODsGsGL7CODGA03kYIDAfQmVJR\n +MK0zORxQlbp6DhepnFv8P\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n----==_mimepart_5984b86be1a84_53483f90f3797c3c2869d2\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\nCan one of the admins verify this patch?\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/pull/12044#issuecomment-320316290\n----==_mimepart_5984b86be1a84_53483f90f3797c3c2869d2\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p>Can one of the admins verify this patch?</p>\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/pull/12044#issuecomment-320316290\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoJ8fUe3XdKUyFhacQcNOVG09fEdkks5sU15rgaJpZM4OuBZn\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoOcNPMfEXdMBsx8hJjGGqyilvdUoks5sU15rgaJpZM4OuBZn.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/pull/12044#issuecomment-320316290\"></link>\n  <meta itemprop=\"name\" content=\"View Pull Request\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Pull Request on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@tensorflow-jenkins in #12044: Can one of the admins verify this patch?\"}],\"action\":{\"name\":\"View Pull Request\",\"url\":\"https://github.com/tensorflow/tensorflow/pull/12044#issuecomment-320316290\"}}}</script>\n----==_mimepart_5984b86be1a84_53483f90f3797c3c2869d2--\n", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 12043, "title": "tf.contrib.util.make_ndarray is slow the first time it is run", "body": "### System information\r\n\r\n**Output of tf_env_collect.sh:** [tf_env.txt](https://github.com/tensorflow/tensorflow/files/1201133/tf_env.txt)\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc1-479-g82456f9 1.2.1-rc1\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**:\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/lib/python3.5/dist-packages/torch/lib/libcudart.so.8.0\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.7\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-7.5/lib/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/lib/libcudart_static.a\r\n/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/lib64/libcudart_static.a\r\n\r\n- **GPU model and memory**:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 381.22                 Driver Version: 381.22                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1060    Off  | 0000:01:00.0      On |                  N/A |\r\n| N/A   48C    P0    28W /  N/A |    591MiB /  6064MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n```\r\n- **Exact command to reproduce**:\r\nDownload  [frame.summary.zip](https://github.com/tensorflow/tensorflow/files/1201101/frame.summary.zip) and extract it to `frame.summary`. Some info about it:\r\n\r\n```\r\ntensor_proto dtype: DT_UINT8\r\ntensor_shape {\r\n  dim {\r\n    size: 1059\r\n  }\r\n  dim {\r\n    size: 768\r\n  }\r\n}\r\n```\r\n\r\nRun the following:\r\n\r\n```python\r\nimport time\r\nfrom google.protobuf import message\r\nimport tensorflow as tf\r\n\r\nfor i in range(2):\r\n  path = 'frame.summary'\r\n  with open(path, 'rb') as summary_file:\r\n    summary_string = summary_file.read()\r\n\r\n  summary_proto = tf.Summary()\r\n  summary_proto.ParseFromString(summary_string)\r\n  tensor_proto = summary_proto.value[0].tensor\r\n  a = time.time()\r\n  array = tf.contrib.util.make_ndarray(tensor_proto)\r\n  b = time.time()\r\n  print('b-a', b-a)\r\n```\r\n\r\n### Describe the problem\r\nThe first time `make_ndarray` is run, it takes a long time. For this isolated example, the output is this on my machine:\r\n```\r\nb-a 2.810185194015503\r\nb-a 0.00030040740966796875\r\n```\r\nIn my project, it took 105.96585726737976 for the first run, and 0.0005743503570556641 the second time (and closer to 0.0003 afterwards).\r\n\r\n### Source code / logs\r\nHere is the branch I was using : https://github.com/chrisranderson/beholder/tree/bug-report\r\nAnd the line that took ~105 seconds: https://github.com/chrisranderson/beholder/blob/b54d4203d803492af9852b7c8453e8a1e5342f46/beholder/file_system_tools.py#L34\r\n\r\n\r\n", "comments": ["I don't have the same problem with `tf.make_ndarray` (thanks @wchargin!).\r\n```\r\nb-a 0.0008018016815185547\r\nb-a 0.0006482601165771484\r\n```", "I can reproduce your issue, and fix it as follows:\r\n```diff\r\n--- sumbench.py\t2017-08-04 10:50:21.550342085 -0700\r\n+++ sumbench2.py\t2017-08-04 10:50:26.258294226 -0700\r\n@@ -10,6 +10,7 @@\r\n   summary_proto = tf.Summary()\r\n   summary_proto.ParseFromString(summary_string)\r\n   tensor_proto = summary_proto.value[0].tensor\r\n+  tf.contrib.util\r\n   a = time.time()\r\n   array = tf.contrib.util.make_ndarray(tensor_proto)\r\n   b = time.time()\r\n```\r\n(My results aren't quite as bad as yours\u20140.2448s vs. 0.0004s\u2014but I assume that we're hitting the same thing.)\r\n\r\nIt's just a benchmarking error: you're loading all of `tf.contrib` during your benchmark.\r\n\r\nYou can similarly fix this without the above change by using `tf.make_ndarray` instead of `tf.contrib.util.make_ndarray`, for the same reason.", "(Relevant: [one](https://github.com/tensorflow/tensorflow/blob/0542739b9a88bebb2f74b8e9a6db15c5a845d480/tensorflow/__init__.py#L28), [two](https://github.com/tensorflow/tensorflow/blob/0542739b9a88bebb2f74b8e9a6db15c5a845d480/tensorflow/python/util/lazy_loader.py).)", "As @wchargin points out, it's due to loading of tf.contrib. Here's a self-contained benchmark which doesn't show the difference\r\n\r\n\r\n```\r\nimport time\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\na = [1]*10000\r\ntf.contrib.util.make_ndarray\r\nfor i in range(5):\r\n  from tensorflow.python.framework import tensor_util\r\n  tensor_proto = tensor_util.make_tensor_proto(a)\r\n  a = time.time()\r\n  array = tf.contrib.util.make_ndarray(tensor_proto)\r\n  b = time.time()\r\n  print('b-a', b-a)\r\n```", "Thank you both! Any idea why it took 105 seconds in the context of my application? Feel free to close if you'd like.", "I have no idea."]}, {"number": 12042, "title": "[docs] Broken link on Tool Developer's page", "body": "Source page: https://www.tensorflow.org/extend/tool_developers/\r\n\r\nBad link (`graph_run_run2.pbtxt`): https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/components/tf_tensorboard/test/data/graph_run_run2.pbtxt", "comments": ["@wolffg could you please update it.", "@MatthewScholefield @shivaniag The bad link is already fixed here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/extend/tool_developers/index.md\r\n\r\nYou have to wait until they update the website, so please close this issue for now.", "This commit (https://github.com/tensorflow/tensorflow/commit/d30efdbaa2b80bf4eb8db9e0b94501fdbd4107d8) fixes the link.  However, the root version of tensorflow.org docs represents what's in the current public binary branch; in this case, it reflects the docs as they were when we cut r1.2.  Unless that's cherrypicked back into r1.2, documentation won't get updated.  This is a known limitation/feature of having versioned documentation.\r\n\r\nHowever, as r1.3 will be coming soon to root, I've submitted that CL to be cherrypicked back into r1.3, so when r1.3 is up this link will be fixed.\r\n\r\nI'm closing this, then, as there's no further action needed."]}, {"number": 12041, "title": "[OpenCL] Fix for //tensorflow/python/kernel_tests:image_ops_test (#111)", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "```docker: Error response from daemon: devmapper: Thin Pool has 977274 free data blocks which is less than minimum required 983040 free data blocks. Create more free space in thin pool or use dm.min_free_space option to change behavior.``` - seems like GPU box is full"]}, {"number": 12040, "title": "NaNs only on GPU with large convolution kernel", "body": "Consider the following (silly) autoencoder-style network, which performs a strided convolution followed by a transposed convolution:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nn = 100\r\nm = 24\r\nk = 14\r\n\r\nW1 = tf.Variable(tf.truncated_normal([1, k, 32, 64], stddev=1e-1,\r\n                                     dtype=tf.float32))\r\nb1 = tf.Variable(tf.truncated_normal([64], stddev=1e-1,\r\n                                     dtype=tf.float32))\r\nW2 = tf.Variable(tf.truncated_normal([1, k, 32, 64], stddev=1e-1,\r\n                                     dtype=tf.float32))\r\nb2 = tf.Variable(tf.truncated_normal([1], stddev=1e-1,\r\n                                     dtype=tf.float32))\r\n\r\nx = tf.placeholder(tf.float32, [n, 1, m, 32])\r\nx_1 = tf.nn.tanh(tf.nn.conv2d(x, W1,\r\n                              [1, 1, 8, 1], 'SAME')) + b1\r\nx_2 = tf.nn.conv2d_transpose(x_1, W2, [n, 1, m, 32],\r\n                             [1, 1, 8, 1], 'SAME') + b2\r\n\r\nloss = tf.nn.l2_loss(x - x_2)\r\n\r\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\r\ncheck_step = tf.add_check_numerics_ops()\r\n\r\nsess = tf.InteractiveSession()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nfor i in range(10000):\r\n    _, _, loss_val = \\\r\n        sess.run([train_step, check_step, loss],\r\n                 feed_dict={x: np.random.randn(n, 1, m, 32)})\r\n\r\n    print(\"Iteration: {}; loss: {}\".format(i, loss_val))\r\n```\r\nOn the CPU, this runs fine.  Using CUDA with k >= 14, though, I reproducibly get the following error:\r\n```\r\nname: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.582\r\npciBusID 0000:82:00.0\r\nTotal memory: 10.91GiB\r\nFree memory: 10.76GiB\r\n2017-08-04 17:09:01.683907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0\r\n2017-08-04 17:09:01.683916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y\r\n2017-08-04 17:09:01.683927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0)\r\n2017-08-04 17:09:03.022214: E tensorflow/core/kernels/check_numerics_op.cc:157] abnormal_detected_host @0x1020d800b00 = {1, 0} gradients/Conv2D_grad/Conv2DBackpropFilter:0\r\nTraceback (most recent call last):\r\n  File \"mintrans.py\", line 35, in <module>\r\n    feed_dict={x: np.random.randn(n, 1, m, 32)})\r\n  File \"/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 997, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: gradients/Conv2D_grad/Conv2DBackpropFilter:0 : Tensor had NaN values\r\n         [[Node: CheckNumerics_60 = CheckNumerics[T=DT_FLOAT, message=\"gradients/Conv2D_grad/Conv2DBackpropFilter:0\", _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/Conv2D_grad/Conv2DBackpropFilter, ^CheckNumerics_59)]]\r\n  \r\nCaused by op u'CheckNumerics_60', defined at:\r\n  File \"mintrans.py\", line 27, in <module>\r\n    check_step = tf.add_check_numerics_ops()\r\n  File \"/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/ops/numerics.py\", line 68, in add_check_numerics_ops\r\n    check_op = [array_ops.check_numerics(output, message=message)]\r\n  File \"/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 415, in check_numerics\r\n    message=message, name=name)\r\n  File \"/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/sschulze/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n  \r\nInvalidArgumentError (see above for traceback): gradients/Conv2D_grad/Conv2DBackpropFilter:0 : Tensor had NaN values\r\n         [[Node: CheckNumerics_60 = CheckNumerics[T=DT_FLOAT, message=\"gradients/Conv2D_grad/Conv2DBackpropFilter:0\", _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/Conv2D_grad/Conv2DBackpropFilter, ^CheckNumerics_59)]]\r\n```\r\n\r\nThis code is the minimal example that I could reproduce the error with.  I am running tensorflow-gpu 1.2.1, installed via pip on Ubuntu 16.04.2, with Python version 2.7.12.  CUDA is 8.0.61-1 from the nVidia repo, and cuDNN is 5.1.10.  My GPU is an nVidia GTX 1080 Ti with 11172 MiB of memory.\r\n\r\nTo reproduce the error, run the above code with CUDA enabled.\r\n\r\nMy suspicion is that when the kernel is too large with respect to the input or the output of the strided convolution, a bug is triggered.  I am not entirely sure how the size of the kernel must relate to the other convolution parameters, but I could produce errors both in the conv2d and the conv2d_transpose op.", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "As stated above,  I am running tensorflow-gpu 1.2.1, installed via pip on Ubuntu 16.04.2 x86_64 (following https://www.tensorflow.org/install/install_linux), with Python version 2.7.12. CUDA is 8.0.61-1 from the nVidia repo, and cuDNN is 5.1.10. My GPU is an nVidia GTX 1080 Ti with 11172 MiB of memory.\r\n\r\nThe exact command I entered is:\r\n`CUDA_VISIBLE_DEVICES=1 python mintrans.py`", "The problem persists with Tensorflow 1.3.0.  Has anyone tried to reproduce it?", "The numerics on a GPU are different than on a CPU, sometimes compromising numeric stability for performance. Does the problem go away if you reduce the learning rate (the usual fix to NaNs)?  If it does, i'd expect it is unlikely a bug.", "No, reducing the Adam step size to, e.g., 1e-6 does not help.  It just makes the error occur a little later.  Neither an excessive gradient nor an excessive variable norm was observed prior to the NaN.  Did you manage to reproduce the error?", "You are feeding random data. I would expect this never to converge and possibly diverge. Could you try this with multiple epochs through a data set instead? Also, you should try asking on StackOverflow, since I believe this is more of a model building quetsion. \r\n\r\n@zheng-xq, have you seen problems with large kernel sizes before?", "I originally encountered this problem using real data; I just changed the example to random data to make it simpler and more portable, so other people can check if they can reproduce it.\r\n\r\nWhether or not large kernel sizes are useful is arguable, but they certainly should not trigger an NaN error.  I am pretty certain that this is a bug, as the error occurs spontaneously after very few iterations, and neither the variables nor the gradients explode, and I cannot think of a sensible reason why setting k < 14 fixes it completely.  Even more suspiciously, setting use_locking=True in the AdamOptimizer seems to make the error appear a little later.  I am just not sure if it is a bug in Tensorflow itself or perhaps some CUDA library, so I would like to know if anyone else can reproduce it.", "Well I believe we delegate all conv operations to cudnn, so it may be a cudnn bug. @reedwm, could you take a look?", "I cannot reproduce on Ubuntu 14.04, with a GTX 1080, Cuda 8, cuDNN 6, on TensorFlow 1.3. @sdschulze, what version of Cuda and cuDNN did you use on TensorFlow 1.3? cuDNN 5.1 isn't supported on TensorFlow 1.3.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Closing due to a lack of a response."]}, {"number": 12039, "title": "Model callbacks don't work in TF r1.2 and Keras functional API (Python 3.6 + windows 10)", "body": "Two test cases were constructed in order to show better the problem with \"Model callbacks, which don't work in TF r1.2 and Keras functional API\".\r\n\r\n1/ Callbackes were used in the following way:\r\n#------------ checkpoints -------------------------------\r\nimport tensorflow.contrib.keras as K\r\nfiledest=\"./models/weights_model011a.best.hdf5\"\r\ncheckpoint = K.callbacks.ModelCheckpoint(filedest, monitor='val_acc', verbose=1, \r\n                             save_best_only=True, mode='max')\r\n\r\ncheckRLR = K.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1, \r\n                                         patience=7,min_lr=1.0e-10)\r\n\r\nearlyStop = K.callbacks.EarlyStopping(monitor='val_acc', patience=30)\r\ntbgraph = K.callbacks.TensorBoard(log_dir='./graphs', histogram_freq=1, \r\n                      write_graph=True, write_images=False, \r\n                      embeddings_freq=1)\r\n\r\ncallbacks_list = [checkpoint,earlyStop,checkRLR,tbgraph]\r\n\r\nand then:\r\nhistory = model.fit(X_train, y_train[:11200], validation_data=(X_test, y_tst_cat[:7200]),\r\n          epochs=epochs, batch_size=40,shuffle=True,\r\n          callbacks=callbacks_list, verbose=0)\r\n\r\nThat resulted in successful run through the training session, but with the following info:\r\nINFO:tensorflow:Summary name embedding/embeddings:0 is illegal; using embedding/embeddings_0 instead.\r\nINFO:tensorflow:Summary name conv1d/kernel:0 is illegal; using conv1d/kernel_0 instead.\r\nINFO:tensorflow:Summary name conv1d/bias:0 is illegal; using conv1d/bias_0 instead.\r\nINFO:tensorflow:Summary name conv1d_1/kernel:0 is illegal; using conv1d_1/kernel_0 instead.\r\nINFO:tensorflow:Summary name conv1d_1/bias:0 is illegal; using conv1d_1/bias_0 instead.\r\nINFO:tensorflow:Summary name conv1d_2/kernel:0 is illegal; using conv1d_2/kernel_0 instead.\r\nINFO:tensorflow:Summary name conv1d_2/bias:0 is illegal; using conv1d_2/bias_0 instead.\r\nINFO:tensorflow:Summary name conv1d_3/kernel:0 is illegal; using conv1d_3/kernel_0 instead.\r\nINFO:tensorflow:Summary name conv1d_3/bias:0 is illegal; using conv1d_3/bias_0 instead.\r\nINFO:tensorflow:Summary name conv1d_4/kernel:0 is illegal; using conv1d_4/kernel_0 instead.\r\nINFO:tensorflow:Summary name conv1d_4/bias:0 is illegal; using conv1d_4/bias_0 instead.\r\nINFO:tensorflow:Summary name dense/kernel:0 is illegal; using dense/kernel_0 instead.\r\nINFO:tensorflow:Summary name dense/bias:0 is illegal; using dense/bias_0 instead.\r\n\r\n2/ Callbacks were used with Keras functional API in similar way, described above. The run resulted in similar info message PLUS the following:\r\nC:\\path\\AppData\\Local\\Continuum\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\callbacks.py:411: RuntimeWarning: Can save best model only with val_acc available, skipping.\r\n  'skipping.' % (self.monitor), RuntimeWarning)\r\nC:\\path\\AppData\\Local\\Continuum\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\callbacks.py:499: RuntimeWarning: Early stopping requires val_acc available!\r\n  RuntimeWarning)\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-70e23aa02b0a>\", line 1, in <module>\r\n    runfile('C:/path_to_the files/Test_bugs/NonTagTFmodel_local_v011.py', wdir='C:/path_to_the files/Test_bugs')\r\n\r\n  File \"C:\\path\\AppData\\Local\\Continuum\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 880, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\path\\AppData\\Local\\Continuum\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 102, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/path/Documents/Python Scripts/Recognition/Run09recipes/Test_bugs/NonTagTFmodel_local_v011.py\", line 149, in <module>\r\n    callbacks=callbacks_list, verbose=0)\r\n\r\n  File \"C:\\path\\AppData\\Local\\Continuum\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\engine\\training.py\", line 1495, in fit\r\n    initial_epoch=initial_epoch)\r\n\r\n  File \"C:\\path\\AppData\\Local\\Continuum\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\engine\\training.py\", line 1158, in _fit_loop\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n\r\n  File \"C:\\path\\AppData\\Local\\Continuum\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\callbacks.py\", line 96, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n\r\n  File \"C:\\path\\AppData\\Local\\Continuum\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\callbacks.py\", line 501, in on_epoch_end\r\n    if self.monitor_op(current - self.min_delta, self.best):\r\n\r\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\r\n\r\nThe same code (TF r1.2 and Keras functional API) runs perfectly alright if 'callbacks=None':\r\nhistory = model.fit(X_train, [Y_tr_hot,y_tr_cat[:11200]], validation_data=(X_test, [Y_ts_hot,y_ts_cat[:7200]]),\r\n          epochs=epochs, batch_size=40, shuffle=True,\r\n          callbacks=None, verbose=1)\r\n\r\nWhat can be the reason for Model callbacks not working???", "comments": ["It's not clear to me whether you have evidence for a bug in TensorFlow, or are looking for help in understanding how to use Keras with TensorFlow.  Unless this is a bug report, please ask your question on stackoverflow, where there is a larger and more active community addressing usage questions.\r\n\r\nIf you believe there is a bug, please provide all of the information requested in the issue template, along with a minimal reproducible example.   Note that the windows port of TF is not fully functional (see [notes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md)).", "Sorry, I found mistake in my code. Everything is working now as it should."]}, {"number": 12038, "title": "R1.2", "body": "miss", "comments": ["Can one of the admins verify this patch?", "@missdown, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @alanyee to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 12037, "title": "Missing tf_python_protos_cc library dependency in tf_tutorials.cmake", "body": "### System information\r\nWindows10\r\nVisualStudio 2017\r\nTensorFlow 1.3.0\r\nPython 3.5.3\r\nCMake 3.8.1\r\n\r\n### Describe the problem\r\nI can't build **tf_tutorials_example_trainer** due missing library dependency to tf_python_protos_cc.lib. A lot of linking errors occurred. If the bold line is added into tf_tutorials.cmake then the compilation works:\r\n\r\ntarget_link_libraries(tf_tutorials_example_trainer PUBLIC\r\n    tf_protos_cc\r\n    **tf_python_protos_cc**\r\n    ${tf_core_gpu_kernels_lib}\r\n    ${tensorflow_EXTERNAL_LIBRARIES}\r\n)\r\n\r\nCMake is used build the project files for VisualStudio. \r\n\r\n![image](https://user-images.githubusercontent.com/30691148/28971246-95f12174-792b-11e7-8d9f-5bb5542a5ce0.png)\r\n\r\n### Source code / logs\r\n`1>------ Build started: Project: tf_tutorials_example_trainer, Configuration: Release x64 ------\r\n1>   Creating library C:/Development/dev/test_projects/deeplearning/tensorflow_build64/Release/tf_tutorials_example_trainer.lib and object C:/Development/dev/test_projects/deeplearning/tensorflow_build64/Release/tf_tutorials_example_trainer.exp\r\n1>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::DecisionTreeConfig(void)\" (??0DecisionTreeConfig@trees@boosted_trees@tensorflow@@QEAA@XZ) referenced in function \"public: static class tensorflow::boosted_trees::trees::DecisionTreeConfig * __cdecl google::protobuf::Arena::CreateMessage<class tensorflow::boosted_trees::trees::DecisionTreeConfig>(class google::protobuf::Arena *)\" (??$CreateMessage@VDecisionTreeConfig@trees@boosted_trees@tensorflow@@@Arena@protobuf@google@@SAPEAVDecisionTreeConfig@trees@boosted_trees@tensorflow@@PEAV012@@Z)\r\n1>prediction_ops.cc.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::DecisionTreeConfig(void)\" (??0DecisionTreeConfig@trees@boosted_trees@tensorflow@@QEAA@XZ)\r\n1>training_ops.cc.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::DecisionTreeConfig(void)\" (??0DecisionTreeConfig@trees@boosted_trees@tensorflow@@QEAA@XZ)\r\n1>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::Swap(class tensorflow::boosted_trees::trees::DecisionTreeConfig *)\" (?Swap@DecisionTreeConfig@trees@boosted_trees@tensorflow@@QEAAXPEAV1234@@Z) referenced in function \"public: virtual void __cdecl tensorflow::AddTreesToEnsembleOp::Compute(class tensorflow::OpKernelContext * const)\" (?Compute@AddTreesToEnsembleOp@tensorflow@@UEAAXQEAVOpKernelContext@2@@Z)\r\n1>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol \"protected: __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::DecisionTreeConfig(class google::protobuf::Arena *)\" (??0DecisionTreeConfig@trees@boosted_trees@tensorflow@@IEAA@PEAVArena@protobuf@google@@@Z) referenced in function \"public: static class tensorflow::boosted_trees::trees::DecisionTreeConfig * __cdecl google::protobuf::Arena::CreateMessage<class tensorflow::boosted_trees::trees::DecisionTreeConfig>(class google::protobuf::Arena *)\" (??$CreateMessage@VDecisionTreeConfig@trees@boosted_trees@tensorflow@@@Arena@protobuf@google@@SAPEAVDecisionTreeConfig@trees@boosted_trees@tensorflow@@PEAV012@@Z)\r\n1>training_ops.cc.obj : error LNK2001: unresolved external symbol \"protected: __cdecl tensorflow::boosted_trees::trees::DecisionTreeConfig::DecisionTreeConfig(class google::protobuf::Arena *)\" (??0DecisionTreeConfig@trees@boosted_trees@tensorflow@@IEAA@PEAVArena@protobuf@google@@@Z)\r\n1>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeMetadata::DecisionTreeMetadata(void)\" (??0DecisionTreeMetadata@trees@boosted_trees@tensorflow@@QEAA@XZ) referenced in function \"public: static class tensorflow::boosted_trees::trees::DecisionTreeMetadata * __cdecl google::protobuf::Arena::CreateMessage<class tensorflow::boosted_trees::trees::DecisionTreeMetadata>(class google::protobuf::Arena *)\" (??$CreateMessage@VDecisionTreeMetadata@trees@boosted_trees@tensorflow@@@Arena@protobuf@google@@SAPEAVDecisionTreeMetadata@trees@boosted_trees@tensorflow@@PEAV012@@Z)\r\n1>training_ops.cc.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::boosted_trees::trees::DecisionTreeMetadata::DecisionTreeMetadata(void)\" (??0DecisionTreeMetadata@trees@boosted_trees@tensorflow@@QEAA@XZ)\r\n1>ensemble_optimizer_ops.cc.obj : error LNK2019: unresolved external symbol \"protected: __cdecl tensorflow::boosted_trees::trees::DecisionTreeMetadata::DecisionTreeMetadata(class google::protobuf::Arena *)\" (??0DecisionTreeMetadata@trees@boosted_trees@tensorflow@@IEAA@PEAVArena@protobuf@google@@@Z) referenced in function \"public: static class tensorflow::boosted_trees::trees::DecisionTreeMetadata * __cdecl google::protobuf::Arena::CreateMessage<class tensorflow::boosted_trees::trees::DecisionTreeMetadata>(class google::protobuf::Arena *)\" (??$CreateMessage@VDecisionTreeMetadata@trees@boosted_trees@tensorflow@@@Arena@protobuf@google@@SAPEAVDecisionTreeMetadata@trees@boosted_trees@tensorflow@@PEAV012@@Z)\r\n1>training_ops.cc.obj : error LNK2001: unresolved external symbol \"protected: __cdecl tensorflow::boosted_trees::trees::DecisionTreeMetadata::DecisionTreeMetadata(class google::protobuf::Arena *)\" (??0DecisionTreeMetadata@trees@boosted_trees@tensorflow@@IEAA@PEAVArena@protobuf@google@@@Z)\r\n\r\n**...**\r\n\r\n1>C:\\Development\\dev\\test_projects\\deeplearning\\tensorflow_build64\\Release\\tf_tutorials_example_trainer.exe : fatal error LNK1120: 85 unresolved externals\r\n1>Done building project \"tf_tutorials_example_trainer.vcxproj\" -- FAILED.\r\n========== Build: 0 succeeded, 1 failed, 0 up-to-date, 0 skipped ==========\r\n`\r\n", "comments": ["I'm puzzled that if this is broken, we don't have a failing test.  Could it be something in your environment?  Meanwhile, pinging our very overcommitted @mrry.", "In the tf_tutorials_example_trainer project they are dependency to tensorflow::boosted_trees. The declarations and definitions for boosted_trees module is in the tf_python_protos_cc project what I have found.", "Which branch are you building? (It looks like b4056fcc2c1a7003c190e1345b8b534d8353e21a affects how `boosted_trees` is built in CMake, but I can't tell if it's included in your change or not.)\r\n\r\nI'm puzzled about why adding `tf_python_protos_cc` fixes things, since (i) the example binary is a pure C++ binary, and (ii) it doesn't appear to depend on any of the `boosted_trees` code....\r\n\r\n/cc @ThomasColthurst ", "PS. @poxvoculi to answer your puzzlement, we don't build this as part of any of the CI builds, because it isn't a binary that we ship, and we're constrained on testing resources :(.", "The code in the original comment wasn't formatted as code so all the local variables are interpreted as usernames.", "The same problems happened to me.\r\nMy system information:\r\nWindows 7\r\nVisual Studio 2015\r\nTensorflow v1.3.0-rc2\r\nPython 3.5.2\r\ncmake version 3.6.2\r\nAnd my cmake setting\r\n![image](https://user-images.githubusercontent.com/4746227/29161448-dfa05d78-7de8-11e7-9ef8-46e823105891.png)\r\n\r\n**In addition the issue occurs in 6 other projects to me**:\r\ncompare_graph \r\nsummarize_graph \r\ntransform_graph \r\nbenchmark_model\r\ntf_label_image_example\r\ngrpc_tensorflow_server \r\n\r\n**So i apply to same modification** to below three cmake files **as what rasitsimsek have done to tf_tutorials.cmake**\r\n- tf_tools.cmake \r\n- tf_core_distributed_runtime.cmake \r\n- tf_label_image_example.cmake \r\n", "j30206868 is right. There are more lack of dependencies in other projects. The tensorflow project is mainly focused on Python and not other languages like C++ only projects. I have managed to build a C++ CMake project without the tensorflow environment. Under C++ it is not running out of the box. There is a lot research needed to put all library and include dependencies together. \r\n\r\nThe answer for [mrry ](https://github.com/mrry): I'm using tensorflow V1.3.0-rc2."]}, {"number": 12036, "title": "refresh model for spark streaming with sv = tf.train.Supervisor() under sv.managed_session()", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, I made custom distributed mnist code with spark streaming, based on [TensorFlowOnSpark example](https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/streaming/mnist_dist.py).\r\nOS Platform and Distribution: CentOS 7\r\nTensorFlow installed from (source or binary): Unmodified source with HDFS enabled\r\nTensorFlow version (use command below): 1.2.1\r\nPython version: Python 3.5.2 :: Anaconda 4.2.0 (64-bit)\r\nBazel version (if compiling from source): bazel release 0.5.2\r\nCUDA/cuDNN version: 8.0/5.1.10\r\nGPU model and memory: NVIDIA Geforce 1070 8GB (shared by two yarn containers, each has two excuetors)\r\n\r\nThe code works fine, but with --mode inference a yarn container will restore the model from logdir at the very beginning and use the same model during the whole prediction periode(based on spark logging info for restoring event).\r\n\r\nMy question is, is there a way that I can reload some new trained model(same architecture) under sv.managed_session() periodically or after new model is generated? Because at the same time, the other yarn container with --mode train produced new model.\r\n\r\nThanks for helping!", "comments": ["Hello, \r\nI got the solution, simply add\r\n```\r\nsaver.restore(sess, tf.train.latest_checkpoint(path/to/model/livetrain))\r\n```\r\nunder `sv.managed_session()` for --mode inference,\r\nplease close this issue. Thanks and\r\n\r\nCheers"]}, {"number": 12035, "title": "RDMA+verbs stuck in some nodes", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\nYes, I made custom distributed inception code\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04\r\nTensorFlow installed from (source or binary): Unmodified source with RDMA Verbs enabled\r\nTensorFlow version (use command below): 1.3.0-rc1\r\nPython version: 2.7.12\r\nBazel version (if compiling from source): 0.5.1\r\nCUDA/cuDNN version: 8.0/5.1.5\r\nGPU model and memory: NVIDIA TITAN Xp PCIe 12GB (4 per node)\r\n\r\nThe code works for normal grpc, but stuck between some nodes, not between all nodes.\r\nI've tested all nodes with ib_write_bw and ibv_rc_pingpong, communication between all of the nodes works fine.", "comments": ["Maybe it is related to #11416 and #11725.\r\n\r\nCould you provide a minimal reproducing script? Some logs help, too.", "There are no explicit logs, but just hanging. \r\nIt's the hardware information for the failed node. \r\n\r\nibv_devinfo | grep board_id\r\n\tboard_id:\t\t\tDEL2180110032\r\n\tboard_id:\t\t\tMT_2180110032\r\n\r\nThe hardware information is different in successful nodes. Can it be related?\r\nibv_devinfo | grep board_id\r\n       board_id:\t\t\tDEL2180110032\r\n       board_id:\t\t\tDEL2180110032", "@sj6077 , how many GPUs per node ? how many nodes ?\r\nAre u training Inception in Sync data parallelism ? \r\nAs @byronyi mentioned, sounds like #11725 , which is still under investigation.", "There are 4 GPUs per node, and I tested with 3 nodes, 6 workers(2 workers per node and 2 GPUs per worker). Yes, I trained with inception in sync data parallelism, however, it has the same issue with Resnet or simple Cifar10 model, too. Does it only happen with inception model?", "@sj6077 - Didn't tried Cifar10 but it happens with ResNet and trivial models as well when running [tf_cnn_benchmark](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py). Which GPUs are you using ? One thing we suspect is that it relates to Pascal GPUs only.\r\nAlso what is the Parameter Servers configuration (1 dedicated, in each node, etc..) ?", "I used dedicated and colocated parameter server with worker, but the result was same. I used same GPUs for all of the machines, which is TITAN Xp. ", "Hi @sj6077, \r\n\r\nPer progress in #11725, can please configure the Session with high values of  inter/intra_op_parallelism_threads (I tried 500) and recheck ?\r\nIt's just a workaround, to see if we indeed experiencing the same issue.", "I tried with inter/intra_op_parallelism_threads=500, 3000, but it doesn't work. In my case, sessions are not created for some nodes. The hanged nodes are not random, same nodes are always hanged.\r\n\r\nIt's my backtrace, even thought I'm not sure it's helpful.\r\n#0  0x00007ffff7bcbe8c in __libc_waitpid (pid=61571, stat_loc=0x7fffffffdcc0, options=0) at ../sysdeps/unix/sysv/linux/waitpid.c:31\r\n#1  0x0000000000420b45 in ?? ()\r\n#2  0x000000000052714b in PyEval_EvalFrameEx ()\r\n#3  0x0000000000555551 in PyEval_EvalCodeEx ()\r\n#4  0x0000000000525560 in PyEval_EvalFrameEx ()\r\n#5  0x00000000005247ea in PyEval_EvalFrameEx ()\r\n#6  0x0000000000555551 in PyEval_EvalCodeEx ()\r\n#7  0x0000000000524338 in PyEval_EvalFrameEx ()\r\n#8  0x00000000005247ea in PyEval_EvalFrameEx ()\r\n#9  0x00000000005247ea in PyEval_EvalFrameEx ()\r\n#10 0x0000000000567d14 in ?? ()\r\n#11 0x0000000000465bf4 in PyRun_FileExFlags ()\r\n#12 0x000000000046612d in PyRun_SimpleFileExFlags ()\r\n#13 0x0000000000466d92 in Py_Main ()\r\n#14 0x00007ffff7814f45 in __libc_start_main (main=0x466e50 <main>, argc=6, argv=0x7fffffffe8d8, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, \r\n    stack_end=0x7fffffffe8c8) at libc-start.c:287\r\n#15 0x0000000000577c2e in _start ()\r\n", "Thanks @sj6077, maybe it's too much threads for your system.\r\nAnyway @yanivbl6 found has a real fix (not workaround) for the problem.\r\nOnce he will PR it, I'll ping you to try it to verify it works.", "@sj6077 , The fix in #12361 , feel free to try it out.", "@sj6077, can we close this one? ", "Sure! Thank you for the fix."]}, {"number": 12034, "title": "Update eigen and gemmlowp dependencies", "body": "This PR updates some dependencies which are required to make Tensorflow work on the ARM platform. \r\nThe first dependency, `Eigen` has been updated to version 3.4 containing [this fix](https://bitbucket.org/eigen/eigen/commits/d781c1de98342c5ca29c2fe719d8d3c96a35dcd4).\r\nThe second one updates `Gemmlowp` with [this fix](https://github.com/google/gemmlowp/commit/9941cad948313bffd44b8cafe759aa6d8d6a9d75) with the issue reported on [this thread](https://github.com/tensorflow/tensorflow/issues/5303#issuecomment-319718323) .\r\n\r\n\u26a0\ufe0f The PR doesn't work as-is simply because I wasn't able to upload the archives to `http://mirror.bazel.build`. Could someone points me to the right direction on how to do it? Or do it in my place if I don't have the rights to do so? Thank you.", "comments": ["Can one of the admins verify this patch?", "@EKami, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @tensorflower-gardener and @frankchn to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Closing this PR. The dependencies updates seems to make some conflicts with the current Tensorflow code on some architectures.", "I'll eventually come back with another PR and the appropriate changes in TF code to make it work everywhere if I'm able to. Still, regarding my comment on `http://mirror.bazel.build` @jart could you clarify this point for me? How am I supposed to upload the archive there? Thanks"]}, {"number": 12033, "title": "read batch file using filename queue when fit wide and deep model", "body": "### Describe the problem\r\nI read records from files using typical pipeline:\r\n```Python\r\nfilename_queue = tf.train.string_input_producer(file_list, num_epochs=num_epochs)\r\nreader = tf.TextLineReader()\r\n _, value = reader.read_up_to(filename_queue, num_records=num_records)\r\ndata_batch = tf.train.batch([value]...)\r\nrecord_defaults = [['string'] for _ in range(len(COLUMNS))]\r\ncol = tf.decode_csv(records=data_batch,\r\n                    record_defaults=record_defaults,\r\n                    field_delim=',')\r\n```\r\n\r\nThen, creates a dictionary mapping from each feature column name (k) to the values of that column stored in a Tensor.\r\n\r\n```Python\r\nfeature_dict = dict()\r\nlabel = None\r\nfor i, cname in enumerate(COLUMNS):\r\n    c = tf.slice(col, begin=[i, 0], size=[1, -1])\r\n    if cname in CONTINUOUS_COLUMNS:\r\n        c = tf.string_to_number(c, tf.float64)\r\n        c = tf.transpose(c)\r\n        feature_dict[cname] = c\r\n    elif cname in CATEGORICAL_COLUMNS:\r\n        c = dense_to_sparse(c[0])\r\n        feature_dict[cname] = c\r\n    elif cname == 'click':\r\n        label = tf.string_to_number(c[0], tf.int64)\r\n```\r\n\r\nCreating threads to prefetch using QueueRunner objects\r\n\r\n```Python\r\nmodel = DNNLinearCombinedClassifier(...)\r\nconfig = tf.ConfigProto(allow_soft_placement=True)\r\nconfig.gpu_options.allow_growth = True\r\nwith tf.Session(config=config) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(tf.local_variables_initializer())\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(coord=coord)\r\n    try:\r\n        while not coord.should_stop():\r\n            model.fit(input_fn=lambda: (feature_dict, label), steps=FLAGS.train_step)\r\n        except tf.errors.OutOfRangeError:\r\n            print('Done training, epoch reached')\r\n        finally:\r\n            coord.request_stop()\r\n        coord.join(threads)\r\n```\r\n\r\nThe error is:\r\n\r\n```log\r\nTraceback (most recent call last):\r\nFile \"/Users/houxue/workspace/Python/dnn-tf/v2/dnntf.py\", line 262, in <module>\r\n    main(FLAGS)\r\nFile \"/Users/houxue/workspace/Python/dnn-tf/v2/dnntf.py\", line 145, in main\r\n    train_v2(FLAGS)\r\nFile \"/Users/houxue/workspace/Python/dnn-tf/v2/dnntf.py\", line 128, in train_v2\r\n    model.fit(input_fn=lambda: (feature_dict, label), steps=FLAGS.train_step)\r\nFile \"/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 289, in new_func\r\n    return func(*args, **kwargs)\r\nFile \"/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 455, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\nFile \"/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1003, in _train_model\r\n    config=self._session_config\r\nFile \"/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 352, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\nFile \"/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 648, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\nFile \"/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 470, in __init__\r\n    h.begin()\r\nFile \"/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 163, in begin\r\n    for (tag, tensor) in self._tensors.items()}\r\nFile \"/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 163, in <dictcomp>\r\n    for (tag, tensor) in self._tensors.items()}\r\nFile \"/usr/local/devtools/anaconda3/envs/tf/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 685, in _as_graph_element\r\n    \"to current graph %s.\" % (obj, graph))\r\nValueError: Passed Tensor(\"binary_logistic_head/log_loss_with_two_classes/loss:0\", shape=(), dtype=float32) should have graph attribute that is equal to current graph <tensorflow.python.framework.ops.Graph object at 0x1131f3610>.\r\n```\r\nGoogleing the error, I found that [ValueError: Passed Tensor(...) should have graph attribute that is equal to current graph](https://stackoverflow.com/questions/42799041/valueerror-passed-tensor-should-have-graph-attribute-that-is-equal-to-curr)\r\n>  Returning the features or labels from a closure fails because a new tf.Graph is created when you call model.fit, so any modifications to the graph (e.g. tf.contrib calls) need to be made from within the input_fn (and therefore after the new graph has been instantiated).                                    -[alcorn](https://stackoverflow.com/users/6536722/alcorn)\r\n\r\nHow to read batch data using filename queue when fit DNNLinearCombinedClassifier model? Thanks!\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}]