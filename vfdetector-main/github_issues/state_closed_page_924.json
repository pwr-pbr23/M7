[{"number": 25734, "title": "TPU has XLA compilation issue on TF 1.13rc1", "body": "I am getting an issue with using XLA on the cloud TPU on tensorflow version 1.13 (but not 1.12).\r\n\r\ntag:comp:tpus\r\ncomp:tpus\r\n\r\n**System information**\r\n- Using Google's cloud TPU with Tensorflow 1.12\r\n\r\nSystem info:\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux aportnoy 4.9.0-8-amd64 #1 SMP Debian 4.9.130-2 (2018-10-27) x86_64 GNU/Linux\r\nVERSION_ID=\"9\"\r\nVERSION=\"9 (stretch)\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux aportnoy 4.9.0-8-amd64 #1 SMP Debian 4.9.130-2 (2018-10-27) x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.16.1)\r\nprotobuf (3.6.1)\r\ntensorflow (1.12.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.12.0\r\ntf.GIT_VERSION = v1.12.0-0-ga6d8ffae09\r\ntf.COMPILER_VERSION = v1.12.0-0-ga6d8ffae09\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I uninstall tensorflow 1.12 and install tensorflow 1.13 via:\r\n`pip3 uninstall tensorflow`\r\n`pip3 install tensorflow==1.13.0rc1`\r\n\r\nThen run my model, I get a very strange error which looks like this:\r\n\r\n```\r\naportnoy@aportnoy:~$ python3 script.py\r\n2019-02-13 17:59:43.159373: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-02-13 17:59:43.165347: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-02-13 17:59:43.165597: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5642dd857ad0 executing computations on platform Host. Devices:\r\n2019-02-13 17:59:43.165641: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nWARNING:tensorflow:From /home/aportnoy/.local/lib/python3.5/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-02-13 17:59:43.305988: I tensorflow/compiler/jit/encapsulate_xla_computations_pass.cc:179] Subgraph fingerprint:7855156155640739707\r\n2019-02-13 17:59:43.401974: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at xla_ops.cc:310 : Invalid argument: Expected LHS feature dimension (value 19) to be a multiple of feature_group_count (value 728), and LHS feature \r\ndimension / feature_group_count = RHS feature dimension (value 1); got <conv>(f32[1,19,19,728,1], f32[1,19,19,728,1])\r\nDimension numbers: {kernel_input_feature_dimension: 4\r\nkernel_output_feature_dimension: 1\r\nkernel_spatial_dimensions: 0\r\nkernel_spatial_dimensions: 2\r\nkernel_spatial_dimensions: 3\r\ninput_batch_dimension: 4\r\ninput_feature_dimension: 1\r\noutput_batch_dimension: 3\r\noutput_feature_dimension: 4\r\ninput_spatial_dimensions: 0\r\ninput_spatial_dimensions: 2\r\ninput_spatial_dimensions: 3\r\noutput_spatial_dimensions: 0\r\noutput_spatial_dimensions: 1\r\noutput_spatial_dimensions: 2\r\n}.\r\n         [[{{node gradients/sep0/separable_conv2d/depthwise_grad/DepthwiseConv2dNativeBackpropFilter}}]]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nWhen using tensorflow version 1.12 there isn't an error:\r\n```\r\naportnoy@aportnoy:~$ python3 script.py\r\n2019-02-13 18:19:58.250311: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-02-13 18:19:58.404755: I tensorflow/compiler/jit/encapsulate_xla_computations_pass.cc:179] Subgraph fingerprint:1939207595948626618\r\n2019-02-13 18:19:58.499394: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-02-13 18:19:58.499806: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x7f39fc00b5b0 executing computations on platform Host. Devices:\r\n2019-02-13 18:19:58.499871: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): <undefined>, <undefined>\r\n```\r\n\r\n**Code to reproduce the issue**\r\nMy `script.py`, reproduce with `python3 script.py`\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.compiler import xla\r\n\r\ndata_format = 'channels_first'\r\n\r\ndef simple(features, labels):\r\n  net = features\r\n\r\n  net = tf.keras.layers.SeparableConv2D(728, [3, 3],\r\n    strides=1,\r\n    padding='same',\r\n    data_format=data_format,\r\n    depth_multiplier=1,\r\n    activation=None,\r\n    use_bias=True,\r\n    name='sep0')(net)\r\n  \r\n  net = tf.keras.layers.Flatten(data_format=data_format)(net)\r\n  net = tf.keras.layers.Dense(10, activation=None, use_bias=True, name='fully0')(net)\r\n\r\n  labels = tf.stop_gradient(labels)\r\n  loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=net) )\r\n\r\n  train_step = tf.train.GradientDescentOptimizer(1e-6).minimize(loss)\r\n\r\n  return net, train_step\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\nsess = tf.Session(config=config)\r\n\r\nimages = tf.constant(1.2, shape=[1, 728, 19, 19], dtype=tf.float32)\r\nlabels = tf.constant(1, shape=[1, 10], dtype=tf.float32)\r\n\r\n[y] = xla.compile(simple, inputs=[images, labels])\r\n\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(y)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThe issue only exists on the new 1.13.\r\nI would use 1.12 but my situation requires me to use 'channels_first' for my model, XLA enabled, and 1.13.\r\n\r\nHere are also images of me running the script ( First on 1.12 which is working then on 1.13 which doesn't work ) :\r\n![screen shot 2019-02-13 at 10 20 31 am](https://user-images.githubusercontent.com/44978436/52759838-37edc100-2fc2-11e9-8cfc-955a15a098c8.png)\r\n\r\n![screen shot 2019-02-13 at 10 00 27 am](https://user-images.githubusercontent.com/44978436/52759837-37edc100-2fc2-11e9-85cf-45b599252e27.png)\r\n\r\n\r\n", "comments": ["Can you try on master?", "FYI internal tracking b/124448884", "Alright,\r\n\r\nUsing a separate machine I use for building, I just built tensorflow from scratch using the latest `master` branch and created the whl file (using `-c opt`). This produced a whl file with the name of `tensorflow-1.12.0-cp35-cp35m-linux_x86_64.whl` which I installed after uninstalling my current tensorflow using pip.\r\n\r\nI then ran the `script.py` that reproduces the issue, and the issue *does NOT* happen. Output here:\r\n```\r\nLimited tf.compat.v2.summary API due to missing TensorBoard installation\r\n2019-02-14 18:34:03.829934: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\n2019-02-14 18:34:03.845549: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2693675000 Hz\r\n2019-02-14 18:34:03.856202: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x4013f20 executing computations on platform Host. Devices:\r\n2019-02-14 18:34:03.856244: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0214 18:34:03.872129 140737353991936 deprecation.py:506] From /home/elyas/.local/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:1253: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\n```\r\n\r\nHowever keep in mind this doesn't resolve my issue, I need this issue to be resolved on version 1.13  \r\n\r\nThanks", "I've reproduced this on docker in 1.13.0rc0 and 1.13.0rc1 but it seems to not a problem on nightly or 1.12. So yes it is definitely a problem unique to 1.13 RC as you say. We're taking a look at this and will keep you posted.", "Closing this issue since the PR has been merged. Thanks!", "@ymodak No, thank you!"]}, {"number": 25733, "title": "Scalar input shape added for floor and ceil testing", "body": "", "comments": []}, {"number": 25732, "title": "ModuleNotFoundError: No module named 'numpy'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v1.12.0\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip without virtualenv\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): I'm not sure what version I'm using but, I'm using VS2015 and _MSC_VER is 1900\r\n- CUDA/cuDNN version: CUDA v9.0.176 / cuDNN v7.4.2.24\r\n- GPU model and memory: GTX960M (2GB)\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm trying to install TF on my laptop from the source code.\r\nI installed GPU driver, CUDA, cuDNN, python, bazel, MSYS2 and Visual Studio, following the official guide (https://www.tensorflow.org/install/source_windows).\r\nOf course, I checked the system variables such as CUDA_PATH and BAZEL_SH.\r\nThen, I cloned the git repository and attempted to build.\r\nAfter I had launched the build, I saw the build working well.\r\nSo that, I just let it go and slept.\r\nHowever, I found out the build failed in the morning.\r\nI cannot understand why the build failed because I can see 'numpy' package installed when I type \"pip3 list\" as follows:\r\n\r\n> C:\\tensorflow>pip3 list\r\n> Package             Version\r\n> ------------------- -------\r\n> absl-py             0.7.0\r\n> astor               0.7.1\r\n> gast                0.2.2\r\n> grpcio              1.18.0\r\n> h5py                2.9.0\r\n> Keras-Applications  1.0.6\r\n> Keras-Preprocessing 1.0.5\r\n> Markdown            3.0.1\r\n> numpy               1.16.1\r\n> pip                 18.1\r\n> protobuf            3.6.1\r\n> setuptools          40.6.2\r\n> six                 1.12.0\r\n> tensorboard         1.12.2\r\n> tensorflow-gpu      1.12.0\r\n> termcolor           1.1.0\r\n> Werkzeug            0.14.1\r\n> wheel               0.32.3\r\n\r\nAnd I have also searched similar issue by googling but I could not found proper solution for me.\r\nPlease give me any suggestion.\r\n\r\nThanks!\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n> C:\\tensorflow>python configure.py\r\n> WARNING: Running Bazel server needs to be killed, because the startup options are different.\r\n> WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\n> You have bazel 0.15.0 installed.\r\n> Please specify the location of python. [Default is C:\\Users\\nickeys\\AppData\\Local\\Programs\\Python\\Python36\\python.exe]:\r\n> \r\n> \r\n> Found possible Python library paths:\r\n>   C:\\Users\\nickeys\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\r\n> Please input the desired Python library path to use.  Default is [C:\\Users\\nickeys\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages]\r\n> \r\n> Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: N\r\n> No Apache Ignite support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with XLA JIT support? [y/N]: N\r\n> No XLA JIT support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with ROCm support? [y/N]:\r\n> No ROCm support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with CUDA support? [y/N]: Y\r\n> CUDA support will be enabled for TensorFlow.\r\n> \r\n> Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]:\r\n> \r\n> \r\n> Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0]:\r\n> \r\n> \r\n> Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:\r\n> \r\n> \r\n> Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0]:\r\n> \r\n> \r\n> Please specify a list of comma-separated Cuda compute capabilities you want to build with.\r\n> You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\n> Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 5.0\r\n> \r\n> \r\n> Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n> \r\n> Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: N\r\n> Not overriding eigen strong inline, some compilations could take more than 20 mins.\r\n\r\n> C:\\tensorflow>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nINFO: From Linking tensorflow/contrib/resampler/python/ops/_resampler_ops.so:\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/contrib/resampler/python/ops/lib_resampler_ops.so.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/contrib/resampler/python/ops/lib_resampler_ops.so.exp\r\nERROR: C:/tensorflow/tensorflow/BUILD:533:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/nickeys/_bazel_nickeys/xv6zejqw/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\libnvvp;C:\\Users\\nickeys\\AppData\\Local\\Programs\\Python\\Python36\\Scripts;C:\\Users\\nickeys\\AppData\\Local\\Programs\\Python\\Python36;C:\\Users\\nickeys\\apps;C:\\msys64\\usr\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\extras\\CUPTI\\libx64;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\ProgramData\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Intel\\iCLS Client\\;C:\\Program Files\\Intel\\iCLS Client\\;C:\\Program Files\\Dell\\DW WLAN Card;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn\\;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\dotnet\\;;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Users\\nickeys\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\;C:\\Users\\nickeys\\AppData\\Local\\Programs\\Python\\Python36\\;C:\\Users\\nickeys\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Program Files\\Bandizip\\\r\n    SET PYTHON_BIN_PATH=C:/Users/nickeys/AppData/Local/Programs/Python/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/nickeys/AppData/Local/Programs/Python/Python36/lib/site-packages\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0\r\n    SET TF_CUDA_VERSION=9.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/create_tensorflow.python_api_1.exe --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/x64_windows-opt/genfiles/tensorflow_api/v1/ --apiname=tensorflow --apiversion=1 --package=tensorflow.python --output_package=tensorflow._api.v1 bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/app/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/bitwise/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/compat/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/data/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/data/experimental/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/debugging/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/distributions/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/dtypes/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/errors/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/feature_column/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/gfile/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/graph_util/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/image/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/io/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/initializers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/activations/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/densenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_resnet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_v3/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/nasnet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/resnet50/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg16/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg19/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/xception/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/backend/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/callbacks/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/constraints/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/boston_housing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar10/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar100/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/fashion_mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/imdb/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/reuters/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/estimator/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/initializers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/layers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/losses/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/metrics/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/models/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/optimizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/image/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/sequence/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/text/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/regularizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/utils/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/wrappers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/wrappers/scikit_learn/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/layers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/linalg/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/logging/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/losses/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/manip/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/math/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/metrics/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/nn/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/nn/rnn_cell/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/profiler/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/python_io/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/quantization/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/random/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/resource_loader/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/strings/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/builder/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/constants/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/loader/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/main_op/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/signature_constants/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/signature_def_utils/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/tag_constants/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/utils/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/sets/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/sparse/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/spectral/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/summary/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/sysconfig/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/test/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/train/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/train/queue_runner/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/user_ops/__init__.py\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\nickeys\\AppData\\Local\\Temp\\Bazel.runfiles_s3knfdzs\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\C:\\Users\\nickeys\\AppData\\Local\\Temp\\Bazel.runfiles_s3knfdzs\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 47, in <module>\r\n    import numpy as np\r\nModuleNotFoundError: No module named 'numpy'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 3196.198s, Critical Path: 2857.14s\r\nINFO: 992 processes: 992 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["@nickeys This looks to be an error in numpy.\r\nCould you check your numpy version?\r\nYou may try uninstalling numpy, and installing 1.15.4.\r\n```\r\npip uninstall numpy\r\npip install numpy==1.15.4\r\n```\r\nPlease let us know whether it resolved or not. Thanks!", "@jvishnuvardhan OK~ Thanks. I'm about to initiate the build after changing the version of 'numpy' to 1.15.4. So, I'm going to tell you the result later because it will cost a lot of time, I suppose :-).", "Oh... the build failed again! BTW, the result seems to be changed a little bit.\r\nIn the below log, the path which cp cannot stat ('C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/__multiarray_api.h') does not exist.\r\nIn the 'sitepackages' directory, there is not 'numpy' sub directory there.\r\nAt this point, I cannot understand why the build tool (bazel) refers different python path (C:/Users/nickeys/AppData/Roaming/Python/Python36), instead of the original path (C:/Users/nickeys/AppData/Local/Programs/Python/Python36).\r\nIs this result influenced by the version change ?\r\nIf it is, to build TF successfully, do I have to create the path and copy the file ('__multiarray_api.h') into the directory manually ?\r\n\r\n```\r\nINFO: From Linking external/protobuf_archive/libprotobuf.a:\r\nerror_listener.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nINFO: From Linking external/protobuf_archive/libprotobuf_lite.a:\r\narenastring.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nERROR: C:/users/nickeys/_bazel_nickeys/xv6zejqw/external/local_config_python/BUILD:148:1: Executing genrule @local_config_python//:numpy_include failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/nickeys/_bazel_nickeys/xv6zejqw/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\libnvvp;C:\\Users\\nickeys\\AppData\\Local\\Programs\\Python\\Python36\\Scripts;C:\\Users\\nickeys\\AppData\\Local\\Programs\\Python\\Python36;C:\\Users\\nickeys\\apps;C:\\msys64\\usr\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\extras\\CUPTI\\libx64;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\ProgramData\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Intel\\iCLS Client\\;C:\\Program Files\\Intel\\iCLS Client\\;C:\\Program Files\\Dell\\DW WLAN Card;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn\\;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\dotnet\\;;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Users\\nickeys\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\;C:\\Users\\nickeys\\AppData\\Local\\Programs\\Python\\Python36\\;C:\\Users\\nickeys\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Program Files\\Bandizip\\\r\n    SET PYTHON_BIN_PATH=C:/Users/nickeys/AppData/Local/Programs/Python/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/nickeys/AppData/Local/Programs/Python/Python36/lib/site-packages\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0\r\n    SET TF_CUDA_VERSION=9.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh;\r\ncp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/__multiarray_api.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/__multiarray_api.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/__ufunc_api.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/__ufunc_api.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/_neighborhood_iterator_imp.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/_neighborhood_iterator_imp.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/_numpyconfig.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/_numpyconfig.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/arrayobject.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/arrayobject.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/arrayscalars.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/arrayscalars.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/halffloat.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/halffloat.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/multiarray_api.txt\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/multiarray_api.txt\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/ndarrayobject.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/ndarrayobject.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/ndarraytypes.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/ndarraytypes.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/noprefix.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/noprefix.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/npy_1_7_deprecated_api.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/npy_3kcompat.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/npy_3kcompat.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/npy_common.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/npy_common.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/npy_cpu.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/npy_cpu.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/npy_endian.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/npy_endian.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/npy_interrupt.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/npy_interrupt.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/npy_math.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/npy_math.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/npy_no_deprecated_api.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/npy_no_deprecated_api.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/npy_os.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/npy_os.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/numpyconfig.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/numpyconfig.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/old_defines.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/old_defines.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/oldnumeric.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/oldnumeric.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/ufunc_api.txt\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/ufunc_api.txt\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/ufuncobject.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/ufuncobject.h\" && cp -f \"C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/utils.h\" \"bazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include/numpy/utils.h\"\r\n\r\ncp: cannot stat 'C:/Users/nickeys/AppData/Roaming/Python/Python36/site-packages/numpy/core/include/numpy/__multiarray_api.h': No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 244.986s, Critical Path: 18.11s\r\nINFO: 1346 processes: 1346 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "At last, I could resolve the problem! Thanks to @jvishnuvardhan . I solved the other problem caused by the wrong reference to numpy package, by copying the numpy package from the original python library path.", "I am closing this as it was resolved. Thanks!", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25732)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25732)\r\n"]}, {"number": 25731, "title": "AttributeError: 'Tensor' object has no attribute 'numpy' in image_captioning_with_attention.ipynb", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): **2.0**\r\n- Python version:3.x\r\n\r\n**Describe the current behavior**\r\nWhen running the code in \r\n\r\n> Caching the features extracted from InceptionV3\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nfor img, path in tqdm(image_dataset):\r\n  batch_features = image_features_extract_model(img)\r\n  batch_features = tf.reshape(batch_features, \r\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\r\n\r\n  for bf, p in zip(batch_features, path):\r\n    path_of_feature = p.numpy().decode(\"utf-8\")\r\n    np.save(path_of_feature, bf.numpy())\r\n**Other info / logs**\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-15-9b3dc8e2ecd4> in <module>()\r\n     15   for bf, p in zip(batch_features, path):\r\n     16     path_of_feature = p.numpy().decode(\"utf-8\")\r\n---> 17     np.save(path_of_feature, bf.numpy())\r\n\r\nAttributeError: 'Tensor' object has no attribute 'numpy'", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet/weblink of the tutorial to reproduce the issue reported here. Thanks!\r\n", "https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/image_captioning.ipynb#scrollTo=Dx_fvbVgRPGQ", "Thanks for the link and creating this issue.", "Thanks for dealing with this!\n\nOn Thu, Feb 14, 2019, 5:50 PM ymodak <notifications@github.com wrote:\n\n> Thanks for the link and creating this issue.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25731#issuecomment-463833648>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEZlcMANlqeX7KDZBKrNtFveT4EbYpYmks5vNeg7gaJpZM4a6egl>\n> .\n>\n", "This is resolved in the alpha, closing", "I have installed the 2.0 alpha version but still get the `'Tensor' object has no attribute 'numpy'` error during runtime. Why? It works fines in ipython terminal. \r\n\r\nI also tried calling `import tensorflow.compat.v1 as tf\r\ntf.enable_eager_execution()` at the beginning of the code, but still the error.\r\n\r\n> Traceback (most recent call last):\r\n>   File \"cnn.py\", line 192, in <module>\r\n>     tf.app.run()\r\n>   File \"/home/bowang/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n>     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n>   File \"/home/aaa/.local/lib/python3.5/site-packages/absl/app.py\", line 300, in run\r\n>     _run_main(main, args)\r\n>   File \"/home/aaa/.local/lib/python3.5/site-packages/absl/app.py\", line 251, in _run_main\r\n>     sys.exit(main(argv))\r\n>   File \"cnn.py\", line 161, in main\r\n>     tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\r\n>   File \"/home/aaa/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\r\n>     return executor.run()\r\n>   File \"/home/aaa/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\r\n>     return self.run_local()\r\n>   File \"/home/aaa/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\r\n>     saving_listeners=saving_listeners)\r\n>   File \"/home/bowang/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 359, in train\r\n>     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n>   File \"/home/aaa/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1139, in _train_model\r\n>     return self._train_model_default(input_fn, hooks, saving_listeners)\r\n>   File \"/home/aaa/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1169, in _train_model_default\r\n>     features, labels, ModeKeys.TRAIN, self.config)\r\n>   File \"/home/aaa/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1127, in _call_model_fn\r\n>     model_fn_results = self._model_fn(features=features, **kwargs)\r\n>   File \"/data/home/BASELINES/tf_v2/network.py\", line 140, in model_fn2\r\n>     logits_train = conv_net(features, params, reuse=False, is_training=True)\r\n>   File \"/data/home/BASELINES/tf_v2/network.py\", line 40, in conv_net\r\n>     lens = np.array([len(xi) for xi in x.numpy()])\r\n> AttributeError: 'Tensor' object has no attribute 'numpy'", "This seems like the code is running with estimators. Can you share the code? ", "@yashk2810 Yes I am. So I just call `model_fn`\r\n\r\n```\r\nmodel = tf.estimator.Estimator(model_fn, logdir, cfg, params)\r\n\r\ndef model_fn(features, labels, mode, params):\r\n\r\n    logits_train = conv_net(features, params, reuse=False, is_training=True)\r\n    logits_test = conv_net(features, params, reuse=True, is_training=False)\r\n    pred_classes = tf.argmax(logits_test, axis=1)\r\n    ... ...\r\n    ... ...\r\n\r\n```\r\n\r\nconv_net function:\r\n\r\n```\r\ndef conv_net(x_dict, params, reuse, is_training):\r\n    # Define a scope for reusing the variables\r\n    with tf1.variable_scope('ConvNet', reuse=reuse):\r\n        x = x_dict['features']\r\n\r\n        lens = np.array([len(xi) for xi in x.numpy()])\r\n        mask = np.arange(lens.max()) < lens[:,None]\r\n        padded = np.zeros(mask.shape)\r\n        padded[mask] = np.hstack((x[:]))\r\n\r\n        x = tf.reshape(padded, shape=[-1, padded.shape[1], 1])\r\n        filter_sizes = [8, 16, 32, 64]\r\n\r\n        pooled_outputs = []\r\n        for filter_size in filter_sizes:\r\n            with tf.name_scope('conv_maxpool-%s' % filter_size):\r\n                conv1 = tf.layers.conv1d(x, 40, filter_size, strides=30, # 40 filters\r\n                                        padding='same', activation=tf.nn.relu, \r\n                                        kernel_initializer=tf.keras.initializers.VarianceScaling()\r\n                                        )\r\n\r\n                pool1 = tf.reduce_max(input_tensor=conv1, axis=1)\r\n                pooled_outputs.append(pool1)\r\n\r\n        # Concatenation and flattening\r\n        concat = tf.concat(pooled_outputs, axis=-1)\r\n        flat = tf.keras.layers.Flatten()(concat)\r\n\r\n        # add dropout\r\n\r\n        with tf.name_scope(\"dense_layers\"):\r\n            fc1 = tf.layers.dense(flat, params['n_hidden_1'], activation=tf.nn.relu,\r\n                                kernel_initializer=tf.keras.initializers.VarianceScaling()\r\n                                )\r\n            # add dropout (if is_training is False, dropout is not applied)\r\n            dropout_1 = tf.layers.dropout(fc1, rate=params['dropout_rate'], \r\n                                    training=is_training)\r\n                                \r\n            out = tf.layers.dense(dropout_1, 4, kernel_initializer=tf.keras.initializers.VarianceScaling())\r\n\r\n    return out\r\n```", "https://stackoverflow.com/questions/55147097/tensor-conversion-function-numpy-doesnt-work-within-tf-estimator-model-functi/55147364#55147364\r\n\r\nOk, so \r\n\r\n> Calling methods of Estimator will work while eager execution is enabled. However, the model_fn and input_fn is not executed eagerly"]}, {"number": 25730, "title": "TFTRT: Fix bugs with Concat and clean up", "body": "Previously, the Concat layer had a bug where we only checked if the first input was a tensor, and then assumed all N inputs were tensors as well. This could lead to segfaults. Now we also perform the necessary checks.", "comments": ["> Thanks @trevor-m for the fix. I think `ops::Concat()` will result in a ConcatV2 op, so please help to add unit test using that.\r\n\r\nThanks Lambda. I will add some unit tests.", "@aaroey I have added the unit tests.", "@aaroey Could you review this when you have a chance? Thanks"]}, {"number": 25729, "title": "TF_SessionRun_wrapper: expected all values in input dict to be ndarray  (GPU nightly build)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n       I have used the basic example from: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/1_Introduction/basic_operations.py along with my own code. I have also made a SO question on the matter (https://stackoverflow.com/questions/54678961/tf-sessionrun-wrapper-expected-all-values-in-input-dict-to-be-ndarray) \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.13.0-dev20190208  (GPU nightly build)\r\n- Python version: Python 3.7.1\r\n- CUDA/cuDNN version: V10.0.130 / \r\n- GPU model and memory: RTX 2070, 8GB\r\n\r\n**Describe the current behavior**\r\n\r\nI am running into this error \"TF_SessionRun_wrapper: expected all values in input dict to be ndarray\" for even very basic examples. I am aware that v1.13 nightly is unstable, but building from source on Windows is very inconvenient (I have tried many times).  What should I do/is there anything I can do? \r\n\r\n\r\n--- I will try Python 3.6.x when back home ---\r\n", "comments": ["I have since fixed the issue.", "Was it an issue with your code or was it a bug in the TF codebase ?", "@roymiles How did you fix the issue?", "Sorry for the late reply. Turns out there were some incompatible versions with some dependencies. This is not a bug in the TF codebase.", "@roymiles Any chance you recall which dependencies? I'm having this issue. ", "I did a complete upgrade on my anaconda installation and that fixed the problem.", "\"pip uninstall numpy\" for two times\r\nthen \"pip install numpy\"\r\nyou may have two numpy version in your computer"]}, {"number": 25728, "title": "Floating point addition is not commutative - Unexpected behavior - leads to NaN issues", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows and Linux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0\r\n- Python version: Python 3.6.5\r\n- CUDA/cuDNN version: v9.0 / 7.2.1\r\n- GPU model and memory: GTX 1080 and also on GTX 1080 TI\r\n\r\n**Describe the current behavior**\r\n\r\nSwapping the position of operands gives different results:\r\n\r\n- `1e-8 + 1.0 - tf.identity(1.0)` results in `0.0`\r\n- `1.0 - tf.identity(1.0) + 1e-8` results in `1e-08`\r\n\r\n**Describe the expected behavior**\r\n\r\nOperations like addition are commutative. Swapping the position of operands doesn't change the result:\r\n\r\n- `1e-8 + 1.0 - tf.identity(1.0)` results in `1e-08`\r\n- `1.0 - tf.identity(1.0) + 1e-8` results in `1e-08`\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\n\r\nprint(sess.run(1e-8 + 1.0 - tf.identity(1.0)))\r\nprint(sess.run(1.0 - tf.identity(1.0) + 1e-8))\r\n```\r\n\r\nOutput:\r\n\r\n> 0.0\r\n> 1e-08\r\n\r\nExpected output:\r\n\r\n> 1e-08\r\n> 1e-08\r\n\r\n**Additional comment**\r\n\r\nIn combination with `tf.log` / `tf.divide` this results in `-inf` / `inf` instead of `-18.420681` / `100000000.0`\r\nWorking with further calculations, for example log loss,  this results in a sudden `NaN` error without any exploding or other indicators. This is unexpected and leads to confusion. Many people on the internet who have inexplicable `NaN` issues try a multitude of proposed solutions (use other weight init, lower learning rate, clipping, strange `tf.where` `NaN` treatment, ....) to work around this unexpected behavior. I don't think any of the before mentioned solutions is appropriate to handle this unexpected behavior and I wish for it to work as expected.", "comments": ["I don't think floating point addition is commutative.", "@Spenhouet,\r\nSorry for the delayed response. This [Stack Overflow Answer](https://stackoverflow.com/questions/24442725/is-floating-point-addition-commutative-in-c#:~:text=The%20mathematical%20operation%20%22sum%22%20is,addition%20is%20definitely%20not%20associative.) explains that **`Floating Point Addition`** in **`C++`** is not **`Commutative`**.  \r\n\r\nSo, the behavior can be considered as expected. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25728\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25728\">No</a>\n"]}, {"number": 25727, "title": "Update clip_ops.py", "body": "Interestingly, the deprecation warning uses a deprecated function (`tf.to_float`).\r\nThis changes it to `tf.cast`.", "comments": ["Sorry, the first version exceeds the linter line length. Please test again!", "@rthadur  Please help proceeding with the next steps as I've some access issues(I'm trying to resolve)."]}, {"number": 25726, "title": "[Intel MKL] Fix incorrect way to dump optimized graph", "body": "This PR fixes #25674 . It simply adds a check to ensure that\r\na graph is valid before dumping it.", "comments": ["@penpornk Thanks for review. Will this fix be included in 1.13?", "@nhasabni I don't think so. But I can try asking @aselle about it."]}, {"number": 25725, "title": "Prepare for rc2 by updating version & fixing tolerance in contrib cudnn test", "body": "", "comments": []}, {"number": 25724, "title": "GPU is idle even when there are operations ready to be executed", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: Python 2.7.15rc1\r\n- Bazel version (if compiling from source): 0.21\r\n- GCC/Compiler version (if compiling from source): (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: 10.0/7.4.2\r\n- GPU model and memory: NVIDIA Corporation GP100GL [Tesla P100 PCIe 12GB] (2 GPUs)\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI am running a toy matrix multiplication example ([code here ](https://gist.github.com/xilenteyex/f569b0e0984808e1fb3a284c03b99984)and [timeline can be found here](https://gist.github.com/xilenteyex/c9565c20f32ea73383de96a484d6eae9)). If u look at the timeline, operations named **MatMul_1** and **MatMul_2** are placed on GPU1 while all other matrix multiplications are placed on GPU0. Operation named **MatMul_2** takes input from operations named **MatMul** and **MatMul_1**. **MatMul** and **MatMul_1** are completed almost at the same time while there is a large gap after completion of these operations before the execution of **MatMul_2** starts. I am not sure why is this gap there ? \r\n\r\n**Describe the expected behavior**\r\nExpected behavior according to my understanding is that as soon as **MEMCPYPtoP** has completed (Op named **MatMul** is placed on GPU0), excution of MatMul_2 should be started. Is this a performance issue or am I missing something here ? \r\n\r\n**Code to reproduce the issue**\r\n[toy_matmul code](https://gist.github.com/xilenteyex/f569b0e0984808e1fb3a284c03b99984)\r\n\r\n**Other info / logs**\r\n[timeline for toy_matmul](https://gist.github.com/xilenteyex/c9565c20f32ea73383de96a484d6eae9)\r\nFollowing is the snapshot of the timeline using chrome-tracing visualizer.\r\n![screen shot 2019-02-13 at 1 02 32 pm](https://user-images.githubusercontent.com/10864603/52733046-b4b37780-2f8f-11e9-9ab3-1ea51c7e2e14.png)\r\n\r\n\r\n", "comments": ["Un-assigning myself as this does not seem related to distribution strategy. Perhaps someone on the GPU team can look into it. ", "Can it be reproduced? I tried your script on GCE and it is executed as you expected\r\n![image](https://user-images.githubusercontent.com/1269299/53456952-391ce600-39e5-11e9-9d33-b11c35e87585.png)\r\n", "Hi @qqfish ,\r\nThanks for looking into this.\r\nIn this execution, it looks like [this control dependency ](https://gist.github.com/xilenteyex/f569b0e0984808e1fb3a284c03b99984#file-toy_matmul_cdep-py-L37)is ignored. Not sure why. Can you please explain this behavior ?\r\nThanks!", "Sorry for delay.\r\n\r\nFirst for the control dependency, I think it is expected. The control dependency is created on placeholder. When you call session.run, you already fulfilled the placeholder. So the ops depends on it is ready to run as well. \r\n\r\nTo fix it, you should move line 38 and 41 out from the control_dependency context.\r\n\r\nAnd back to the original issue, it is actually caused by the profiler. In order to record the runtime of each kernel, our profiler calls cuCtxSynchronize to synchronize CPU and GPU. Unfortunately, right now we call this on the main thread, which blocks scheduling. This issue should go away if you turn off profiling.\r\n\r\nWe have an experimental feature that create private threads for scheduling, which can fix this issue as well. To turn it on, you need to set two environment variable before executing tf:\r\n`\r\nexport TF_GPU_THREAD_MODE=gpu_private\r\nexport TF_GPU_THREAD_COUNT=2\r\n`\r\n\r\nThis is the timeline I generated after setting these two variables:\r\n![image](https://user-images.githubusercontent.com/1269299/53850261-73e7c680-3f6f-11e9-91db-4bcbb887ff1c.png)", "I am still investigating the gap between MatMul and memcpy.", "Hi @qqfish thanks for your input. I moved the lines outside the control_dependency context and set the environment variables as you suggested. It appears to be working fine for me now (Not sure why are you still seeing a gap in your execution) as shown in the timeline below:\r\n[timeline JSON file can be found here](https://gist.github.com/xilenteyex/d6606ddcb7a6bc3edba95d1e531cf403)\r\n[Updated toy_matmul_code can be found here](https://gist.github.com/xilenteyex/47c0810ec447d87f26cf3ce419f6efb8)\r\n\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/10864603/53915020-069a6b00-402d-11e9-9ab4-bd59a952e6c2.png)\r\n\r\n \r\nNext step for my use-case is to try similar settings for larger real models e.g. [RRNLM (Recurrent Neural Network for Language Modeling)](https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb), [Inception, ResNet etc.](https://github.com/tensorflow/models/tree/master/research/slim) Do you think these environment variable settings are safe to use for those bigger and more complicated control flow graphs as well ?\r\n\r\nAlso, if you can tell what these environment variables are doing, that will help as well.\r\nThanks! ", "Yes, it should be safe. We may turn this feature on by default recently.\r\n\r\nPrevious we use one thread on GPU to compute CPU ops and schedule kernels on GPU. These variables create a private scheduling thread for each GPU. Then we don't need to do everything in one thread.\r\n\r\nThere are two gap in the original timeline: (1) between MatMul and MemCPYPtoP (2) between MemCPYPtoP and MatMul_2. The environment variables should address gap (2). However, (1) is non-deterministic\r\n\r\nI am working on a fix to address (1). ", "Hi @qqfish \r\nfor variable `TF_GPU_THREAD_COUNT`. Value should be equal to number of GPUs or it should always be 2 ?\r\nAlso, after setting both the environment variables, for execution on my local machine 'both the gaps' [(1) and (2) in your last comment] are gone, but not for your case ?\r\nDo you think for me GAP has disappeared by chance ? Like it may appear if I try to run the same thing again  or try to run a different example ?", "If you set `TF_GPU_THREAD_MODE=gpu_private` then `TF_GPU_THREAD_COUNT` should always be 2. It means each GPU will have 2 private threads to launch ops. \r\n\r\nYes, I think gap (1) disappear in your case by change. If you rerunning the same thing, it will not appear. But the gap may appear again if running a different example. ", "@qqfish Thanks for the explanation. I will wait for your fix then before I try the other larger models. If I can be helpful in anyway, let me know.\r\nThanks!", "Fixing this issue requires a much bigger change than I originally thought. I cannot find a easy workaround works for all cases. This may require to re-design op scheduling of TF to fully support this kind of fine-grained dependency.\r\n\r\nThe good news is, someone in our team has already started working on fine-grained dependency support. But it may take a while.", "@qqfish Thanks for the update. Do we know approximately how much time will it take ?", "I am not sure as well. It is part of a larger project that is still in its early stage. So please stay tuned.", "any updates ?", "Sorry, not much yet. We are still working on it.", "Just checking if there are any updates yet?", "Can someone explain why TF_GPU_THREAD_COUNT should be 2 when gpu_thread_mode is private? Why not larger numbers?\r\n\r\nAlso, how about when thread_mode is shared? Shall I have larger TF_GPU_THREAD_COUNT be larger? \r\n\r\nThanks!", "> Can someone explain why TF_GPU_THREAD_COUNT should be 2 when gpu_thread_mode is private? Why not larger numbers?\r\n\r\nWhen `TF_GPU_THREAD_MODE` is set to `gpu_private`, then `TF_GPU_THREAD_COUNT` is the number of threads **per GPU** for kernel launching. For some reference / benchmark models we find using 1 or 2 threads per GPU perform well. You can customize it to larger numbers, but be aware of contention if the total number of GPU private threads, data processing threads, and other CPU computation threads.\r\n\r\n> Also, how about when thread_mode is shared? Shall I have larger TF_GPU_THREAD_COUNT be larger?\r\n\r\nWhen set to shared, it will be the total number of threads for kernel launching on all GPUs. It's supposed to be a larger number (for example, 2 * num_gpus).", "Hi @Haoyuz, \r\n\r\nThanks. Is this TF_GPU_THREAD_COUNT thread pool for computation? I'm not very familiar with tensorflow runtime, when you said \"kernel launching\", do you mean a single GPU can only use 2 threads for computation?\r\n", "> Is this TF_GPU_THREAD_COUNT thread pool for computation?\r\n\r\nThread pool is for CPU, and this number is controlling how many CPU threads you want to schedule GPU kernels. You do not have to worry about the number of \"threads\" on GPU (technically GPU has different concepts controlling different levels of parallelism, and a single GPU stream running ops with a large enough batch size can usually saturate the GPU).", "@qqfish, just checking, any updates yet ?", "Sorry, there is still not much yet. I think you shouldn't be blocked by this issue.", "Hi @qqfish,\r\nI am trying to create a framework for speeding up TensorFlow programs using model parallelism. When one of the GPUs stays idle for sometime even when there are operations ready to be executed, performance improvement is not as expected and is sometimes worse than the CPU, negatively impacting the performance of my framework.\r\n\r\nBecause of this, I am unable to tell if performance is getting worse because of bad partitioning by my framework or because of bad behavior of TensorFlow scheduler.\r\nIf you can share a temporary workaround for now. That will be helpful as well.\r\nThanks!", "First, I think `TF_GPU_THREAD_MODE` is good enough. In our multi-gpu benchmark, we don't see any performance issues causing by this.\r\n\r\nIf you find any gap causing by this issue in timeline, I think you can use control dependency to work it around. You can add a tf.Identity with control dependency to make sure the Memcpy is scheduled before other heavy gpu ops.", "@qqfish, Can you please elaborate this? I am not an expert in TensorFlow yet, sorry :( .\r\n(If you can give a small toy example that will help a lot!)\r\n\r\n> If you find any gap causing by this issue in timeline, I think you can use control dependency to work it around. You can add a tf.Identity with control dependency to make sure the Memcpy is scheduled before other heavy gpu ops.\r\n\r\nThanks! ", "Let say you have a graph like this:\r\n\r\n```\r\nwith tf.device('cpu:0'):\r\n  X0 = tf.random_uniform([dim, dim], 0, 10)\r\n\r\nwith tf.device('gpu:0'):\r\n  Y0 = tf.matmul(X0, X0)\r\n\r\nwith tf.device('gpu:1'):\r\n  X1 = tf.random_uniform([dim, dim], 0, 10)\r\n  with tf.control_dependencies([X0]):\r\n    Y1 = tf.matmul(X1, X1)\r\n```\r\n\r\nIf you notice memory copy is execute after `Y1` on gpu, you can work around by adding following control dependency:\r\n\r\n```\r\nwith tf.device('cpu:0'):\r\n  X0 = tf.random_uniform([dim, dim], 0, 10)\r\n  X0_ = tf.identity(X0)\r\n\r\nwith tf.device('gpu:0'):\r\n  Y0 = tf.matmul(X0, X0)\r\n\r\nwith tf.device('gpu:1'):\r\n  X1 = tf.random_uniform([dim, dim], 0, 10)\r\n  with tf.control_dependencies([X0_]):\r\n    Y1 = tf.matmul(X1, X1)\r\n```\r\n\r\nor:\r\n```\r\nwith tf.device('cpu:0'):\r\n  X0 = tf.random_uniform([dim, dim], 0, 10)\r\n\r\nwith tf.device('gpu:0'):\r\n  Y0 = tf.matmul(X0, X0)\r\n\r\nwith tf.device('gpu:1'):\r\n  with tf.control_dependencies([X0]):\r\n    X1 = tf.random_uniform([dim, dim], 0, 10)\r\n    Y1 = tf.matmul(X1, X1)\r\n```\r\nBut like I said, it is a very rare issue, you don't need to worry about it too much.\r\n\r\n", "Closing as per https://github.com/tensorflow/tensorflow/issues/25724#issuecomment-522719673: this is a rare issue that has a workaround.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25724\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25724\">No</a>\n"]}, {"number": 25723, "title": "TF 2.0: tf.estimator import issue.", "body": "The TF 2.0 nightly build cannot import `tf.estimator`.\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> import tensorflow_estimator.python.estimator.api._v2.estimator\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\r\n    import tensorflow_estimator.python.estimator.estimator_lib\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 54, in <module>\r\n    from tensorflow_estimator.python.estimator.mode_keys import ModeKeysV2\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/mode_keys.py\", line 22, in <module>\r\n    from tensorflow.python.training.mode_keys import ModeKeys\r\nModuleNotFoundError: No module named 'tensorflow.python.training.mode_keys'\r\n>>> tf.__version__\r\n'2.0.0-dev20190213'\r\n>>> \r\n>>> exit()\r\n```", "comments": []}, {"number": 25722, "title": "[feature request] Alternating Multi-Head for Estimator-API", "body": "Hello everyone,\r\n\r\nI really like the Estimator-API! Its clean, works well (for the most models) and it does a lot of work for you.\r\nHowever, if you want to train custom models with alternating losses, its almost impossible to use the Estimator-API - unfortunate. (With tf-2.0, you definitely could use eager-mode + tfe.defun(), but I like the idea of a static graph for better performance.)\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIt would be great to have a `MultiHeadSpec`, which allows to define multiple losses and train_ops as well an interval, how often the train_ops should be run.\r\n\r\nIn case of a GAN: \r\n- Discriminator, head1: \r\n  - discriminator_loss, distriminator_train_op, run train_op 2 times\r\n- Generator, head2: \r\n  - generator_loss, generator_train_op, run train_op once\r\n\r\n```python\r\nwhile not mon_sess.should_stop():\r\n   for _estimator_spec in estimator_specs:\r\n      for _ in range(0, _estimator_spec.iterations):\r\n         _, loss = mon_sess.run([_estimator_spec.train_op, estimator_spec.loss])\r\n```\r\n\r\n**Will this change the current api? How?**\r\nNo. \r\n\r\n**Who will benefit with this feature?**\r\nPeople, who train models with alternating loss-functions.\r\n\r\n**Any Other info.**\r\n- ", "comments": ["@lhlmgr Thanks for FR! We do have [multi_head](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/contrib/estimator/python/estimator/multi_head.py) in tf.contrib, but don't think it's suitable for GAN. If you are interested in GAN, you can take [tf-gan](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/gan) as a reference.", "@yhliang2018 thanks for your reply and the references. I'm familiar with both of these tools (I used tf-gan some times, and I quickly skimmed through the `multi_head`-classes). While `multi_head` is a nice idea, it doesn't allow to alternate through the different headers. It [adds them up to a total loss](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/contrib/estimator/python/estimator/multi_head.py#L212).\r\nThe GAN-Estimator is in my opinion a bit to inflexible (fixed learning schedule, etc.).\r\nBut, GAN was just an easy example, why a alternating `multi_head` could be useful. There are also papers like:\r\n- [Autoencoding beyond pixels using a learned similarity metric](https://arxiv.org/abs/1512.09300)\r\n- [Adversarial Autoencoders](https://arxiv.org/abs/1511.05644)\r\n- .. \r\nwhich couldn't be properly trained with estimators.\r\n\r\nGiven these two mentioned libraries, I still think it would be a good idea to have such an `multi_head` that gives a bit freedom to chosen loss function / optimization schedule.", "@lhlmgr thanks for the detailed comments. Yes, I agree, the current multi_head is mainly for multi-objective learning, and not designed for GAN or other similar models. Currently this FR may not be prioritized as we mainly focus on TF 2.0 recently. Contributions on this feature are welcome to our [Addons](https://github.com/tensorflow/addons)."]}, {"number": 25721, "title": "convert_variables_to_constants raise issue of while/ReadVariableOp/Enter while dealing with LSTM and GRU", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy]\r\n\r\n**System information**\r\n\r\n- TensorFlow version:1.12\r\n- Python version:3.6\r\n- NO GPU, and with GPU(GTX 1080Ti)\r\n\r\n**Describe the current behavior**\r\nconvert_variables_to_constants first changes computed variables to constants, then converts ReadVariableOps to Identity node. However, in RNN models such as GRU and LSTM,  some variables are attached to Enter op which are incompatible with the generated constants.\r\n**Describe the expected behavior**\r\nThe model can be successfully frozen and loaded in CNN, however, failed to work in RNN such as LSTM and GRU \r\n**Code to reproduce the issue**\r\n```\r\n\r\nfrom tensorflow.python.keras.models import Sequential\r\nfrom tensorflow.python.keras.layers import Dense, LSTM\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras import backend as K\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\ndef freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\r\n    \"\"\"\r\n    Freezes the state of a session into a pruned computation graph.\r\n    Creates a new computation graph where variable nodes are replaced by\r\n    constants taking their current value in the session. The new graph will be\r\n    pruned so subgraphs that are not necessary to compute the requested\r\n    outputs are removed.\r\n    @param session The TensorFlow session to be frozen.\r\n    @param keep_var_names A list of variable names that should not be frozen,\r\n                          or None to freeze all the variables in the graph.\r\n    @param output_names Names of the relevant graph outputs.\r\n    @param clear_devices Remove the device directives from the graph for better portability.\r\n    @return The frozen graph definition.\r\n    \"\"\"\r\n    from tensorflow.python.framework.graph_util import convert_variables_to_constants\r\n    graph = session.graph\r\n    with graph.as_default():\r\n        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\r\n        output_names = output_names or []\r\n        output_names += [v.op.name for v in tf.global_variables()]\r\n        input_graph_def = graph.as_graph_def()\r\n        if clear_devices:\r\n            for node in input_graph_def.node:\r\n                node.device = \"\"\r\n        frozen_graph = convert_variables_to_constants(session, input_graph_def,\r\n                                                      output_names, freeze_var_names)\r\n        return frozen_graph\r\n\r\ndef load_graph(frozen_graph_filename):\r\n    # We load the protobuf file from the disk and parse it to retrieve the\r\n    # unserialized graph_def\r\n    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n\r\n    # Then, we import the graph_def into a new Graph and returns it\r\n    with tf.Graph().as_default() as graph:\r\n        # The name var will prefix every op/nodes in your graph\r\n        # Since we load everything in a new graph, this is not needed\r\n        tf.import_graph_def(graph_def, name=\"prefix\")\r\n    return graph\r\n\r\n\r\ndef lstm_model(input_shape):\r\n    model = Sequential()\r\n    model.add(LSTM(256, name='LSTM', return_sequences=False, use_bias=False,  input_shape=(256,1)))\r\n    model.add(Dense(100,activation='relu'))\r\n    model.add(Dense(1, activation='linear', name='output'))\r\n    return model\r\n\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    #load lstm model\r\n    input_shape = (None, 256)\r\n    model = lstm_model(input_shape)\r\n\r\n    x=np.random.rand(1000, 256, 1)\r\n    y = np.random.rand(1000, 1)\r\n    print(x.shape)\r\n\r\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\r\n\r\n    model.fit(x=x, y=y, epochs=2, validation_split=0.2)\r\n\r\n    #freeze the session and save it to a pd.file\r\n    frozen_graph = freeze_session(K.get_session(),\r\n                                  output_names=[out.op.name for out in model.outputs])\r\n    tf.train.write_graph(frozen_graph, './', './model.pb', as_text=False)\r\n\r\n    #load the pb file into graph\r\n    frozen_model_filename = './model.pb'\r\n    graph = load_graph(frozen_model_filename)\r\n\r\n```\r\n\r\n**Other info / logs**\r\nThe model can be frozen but could not be load. If put LSTM/kernel to black list, the pd file can be load later on, however, cannot be used in c++ and c #.\r\n\r\n\r\nValueError: Input 0 of node prefix/LSTM/while/ReadVariableOp/Enter was passed float from prefix/LSTM/kernel:0 incompatible with expected resource.\r\n", "comments": ["I think I'm running into the same issue, when loading my frozen graph from C++:  session->Create(graph_def) fails with: Invalid argument: Input 0 of node gru/while/ReadVariableOp/Enter was passed float from gru/kernel:0 incompatible with expected resource. \r\nDid you find any solution?", "> I think I'm running into the same issue, when loading my frozen graph from C++: session->Create(graph_def) fails with: Invalid argument: Input 0 of node gru/while/ReadVariableOp/Enter was passed float from gru/kernel:0 incompatible with expected resource.\r\n> Did you find any solution?\r\n\r\nNot yet. It seems this tf function could not freeze any rnn keras models. If your keras models is a sequential model, you might find some other api. Mine keras model has to be a functional model. Have to wait tensorflow or keras to fix this issue. ", "Hi, I am facing the same issue, did any one find a workaround?", "@RuilongMachineLearning -- can you clarify why you are using graph freezing here? It might be that there's a better approach.", "@karmel , I want to freeze my model (keras model containing LSTM and GRU) so that I can directly load the model using Java API without having to setup python environment on the server.\r\n\r\nI have tried freezing small practice models too. It seems this issue comes up everytime LSTM/GRU is used in tf.keras model.\r\n\r\nPython = 3.6.0\r\nTensorflow = 1.13.1\r\nwindows 7\r\n\r\n\r\n\r\n", "@disha3 Maybe you can try our version of convert_variables_to_constants (https://github.com/intel-analytics/analytics-zoo/blob/master/pyzoo/zoo/util/tf_graph_util.py#L226).  This issue should have been fixed there. \r\n\r\n(And by the way, if you are using java, you may also want to checkout our tfnet example, which support running tensorflow graph on spark (https://github.com/intel-analytics/analytics-zoo/tree/master/zoo/src/main/scala/com/intel/analytics/zoo/examples/tfnet) and our java model serving api which also supports loading tensorflow graph (https://analytics-zoo.github.io/master/#ProgrammingGuide/inference/))", "@yangw1234, thanks a lot for the solution, it solved the issue. :D\r\n\r\nAlso, thank you for informing about the java apis for tensorflow.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25721\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25721\">No</a>\n", "> @yangw1234, thanks a lot for the solution, it solved the issue. :D\r\n> \r\n> Also, thank you for informing about the java apis for tensorflow.\r\n\r\nhi, guy. how you resolved this problem?", "@yangw1234 thanks for the solution. I am able to freeze a graph consisting with LSTM node but unable to load the frozen graph using \" tf.import_graph_def\".  I am trying to load the graph in order to profile it. Do you have any suggestions on how can I profile a TF graph with LSTM loads since freezing and loading doesnt seem to be working.\r\n", "@GaoQ1 replace your /usr/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py  file with /pyzoo/zoo/util/tf_graph_util.py", "**EDIT* This change made it possible to export a ProtoBuf file, but that file was not useful for either the Android inference API or converting to Tensorflow Lite.\r\n\r\nThis ended up working better for me:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/TensorFlowLite_LSTM_Keras_Tutorial.ipynb\r\n\r\nHere's hoping all of the different versions and data types get consolidated better soon.", "Does anyone have an LSTM running on Android?"]}, {"number": 25720, "title": "Fixed the warning in slide.cc file", "body": "Fixed the warning from the slide.cc file", "comments": ["@rthadur  Please help proceeding with the next steps as I've some access issues(I'm trying to resolve).", "@rthadur, i have checked the failures, it has nothing to do with this changes, can you pls check and merge this PR."]}, {"number": 25719, "title": "Removed Warning in the lstm.cc file", "body": "Removed 3 warning from the lstm.cc file", "comments": ["Nagging Reviewer @miaout17, @suharshs: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied."]}, {"number": 25718, "title": "Add Python 3.7 to classifiers (PyPI)", "body": "Python 3.7 compatibility seems fixed in 1.13, see eg. tensorflow/tensorflow#20517 and tensorflow/tensorflow#17022.\r\n\r\nNot sure if this is also true for the profiler and should be added there too: https://github.com/tensorflow/tensorflow/blob/8a3407d7ec8d2a79534cc1b4c587eae0fd8dd924/tensorflow/contrib/tpu/profiler/pip_package/setup.py#L53-L60", "comments": []}, {"number": 25717, "title": "Removed the warning in optimized_ops.h file", "body": "Removed compilation warning caused by the file", "comments": ["@jaingaurav ,can you pls review the PR."]}, {"number": 25716, "title": " KeyError in Serve-DualProtoBuf.py", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-4-da7bde60a553> in <module>()\r\n----> 1 features = encoder_forward_pass(sess, \"Images/gen_plane.png\")\r\n\r\n<ipython-input-2-e5cc47d4db69> in encoder_forward_pass(sess, image_path)\r\n     14 \r\n     15 def encoder_forward_pass(sess, image_path):\r\n---> 16     in1 = graph.get_tensor_by_name(\"encoder/InputImage:0\")\r\n     17     out1 = graph.get_tensor_by_name(\"encoder/Preprocessed_JPG:0\")\r\n     18     feed_dict = {in1: image_path}\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in get_tensor_by_name(self, name)\r\n   3662       raise TypeError(\"Tensor names are strings (or similar), not %s.\" %\r\n   3663                       type(name).__name__)\r\n-> 3664     return self.as_graph_element(name, allow_tensor=True, allow_operation=False)\r\n   3665 \r\n   3666   def _get_tensor_by_tf_output(self, tf_output):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in as_graph_element(self, obj, allow_tensor, allow_operation)\r\n   3486 \r\n   3487     with self._lock:\r\n-> 3488       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n   3489 \r\n   3490   def _as_graph_element_locked(self, obj, allow_tensor, allow_operation):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)\r\n   3528           raise KeyError(\"The name %s refers to a Tensor which does not \"\r\n   3529                          \"exist. The operation, %s, does not exist in the \"\r\n-> 3530                          \"graph.\" % (repr(name), repr(op_name)))\r\n   3531         try:\r\n   3532           return op.outputs[out_n]\r\n\r\nKeyError: \"The name 'encoder/InputImage:0' refers to a Tensor which does not exist. The operation, 'encoder/InputImage', does not exist in the graph.\"\r\n", "plz help me ", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25715, "title": "Added 8-bit Quantization support for one_hot op.", "body": "This was also a TODO item in the code.", "comments": ["@alanchiao ,can you pls review the PR.", "Nagging Reviewer @alanchiao: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied.", "Looks like you've created https://github.com/tensorflow/tensorflow/pull/28279. Should we close this Pull Request then?", "@alanchiao , thanks for pointing this out, i am closing this PR and will update the changes in #28279\r\n\r\nRegards\r\nAmit "]}, {"number": 25714, "title": "TF Remove the limitation of int max for dim_size in SparseSoftmax ops", "body": "This fix tries to address the simmiliat issue raised in #25701 where the dim_size in SparseSoftmax ops was set to . The restriction is likely unnecessary, due to historical reasons. This fix remove this limitation.\r\n\r\nRefer PR #25702.", "comments": ["Nagging Reviewer @timshen91: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied."]}, {"number": 25713, "title": "Removed redundant code from verifier.cc", "body": "Constant Tensor check is done with IsConstantTensor(). The redundant source code is removed", "comments": ["LGTM"]}, {"number": 25712, "title": "TF Lite Exp Complex64 datatypes supported", "body": "TFLite Exp currently supports only Float32.\r\nThis PR add support for Complex64 datatypes as well.", "comments": ["Nagging Reviewer @alanchiao: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 47 days with no activity and the `awaiting review` label has been applied.", "Is there a specific model which requires complex support for this kernel? In general, for complex and/or other less frequently used types, we only add kernel support when there is a clear need.", "Let's re-open if there is a model which requires this."]}, {"number": 25711, "title": "Version/Build supporting python3.7 on macOS 10.11.x?", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.11.6\r\n- TensorFlow installed from (source or binary): `pip` (unclear)\r\n- TensorFlow version: 1.9-1.13\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: `pip`\r\n\r\n\r\n**Describe the problem**\r\n\r\nI've tried `pip` installing versions 1.9 all the way up to 1.13 on the machine described above. All failed either because the `.so` was built for macOS 10.12 or with the `async` failures mentioned in another issue (I don't have the reference handy). After fixing those, I ran into other issues: sometimes similar, one time a 3.6 (compile-time) != 3.7 (runtime) error.\r\n\r\nEffectively, no matter which of the versions I tried, `python -c 'import tensorflow'` failed.\r\n\r\nI've spent the better part of 2 hours hoping to find a (relatively painless) way to install tensorflow on my setup: I find it hard to believe python 3.7 supports macOS 10.11.6 but tensorflow somehow doesn't.\r\n\r\nI would really appreciate either\r\n1) an explanation of why this setup is not supported; or\r\n2) an explanation of what version to use/what steps to take in order to install tensorflow and have it work.\r\n\r\nN.B. upgrading to macOS 10.12 is not really a suitable option for me right now, otherwise I'd have done it.", "comments": ["Hi @benknoble \r\n\r\n1. For most of the python binaries on MacOS we build for pypi, we only support 10.12+ for MacOS. For Python3.7 I believe it's actually 10.13+. \r\n\r\n2. You can always try [building from source](https://www.tensorflow.org/install/source#setup_for_linux_and_macos). \r\n\r\nHope this helps!", "Closing this issue for now due to lack of activity."]}, {"number": 25710, "title": "Tensorflow Build fail on rockchip arm64", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 64 bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.8.0\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.12.0\r\n- GCC/Compiler version (if compiling from source):  gcc version 7.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nTensorflow build ERROR:\r\n\r\n**Build command** \r\n**bazel build -c opt --copt=\"-funsafe-math-optimizations\" --copt=\"-ftree-vectorize\" --copt=\"-fomit-frame-pointer\" --verbose_failures tensorflow/tools/pip_package:build_pip_package**\r\n\r\n\r\n**ERROR: /home/rock64/tensorflow/tensorflow/core/kernels/BUILD:2529:1: C++ compilation of rule '//tensorflow/core/kernels:matrix_exponential_op' failed (Exit 4): gcc failed: error executing command** \r\n  (cd /home/rock64/.cache/bazel/_bazel_rock64/c534027a01b43b19fa0469dd5f97fd9f/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/rock64/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -g0 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-std=c++0x' -g0 -MD -MF bazel-out/host/bin/tensorflow/core/kernels/_objs/matrix_exponential_op/tensorflow/core/kernels/matrix_exponential_op.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/kernels/_objs/matrix_exponential_op/tensorflow/core/kernels/matrix_exponential_op.pic.o' -fPIC -DEIGEN_MPL2_ONLY -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -iquote . -iquote bazel-out/host/genfiles -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/host/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' -pthread -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/kernels/matrix_exponential_op.cc -o bazel-out/host/bin/tensorflow/core/kernels/_objs/matrix_exponential_op/tensorflow/core/kernels/matrix_exponential_op.pic.o)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1788.353s, Critical Path: 102.71s\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\nHere GCC version info:\r\n**rock64@rockpro64:/usr/share/doc/gcc-7$ gcc -v**\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/aarch64-linux-gnu/7/lto-wrapper\r\n**Target: aarch64-linux-gnu**\r\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro 7.3.0-27ubuntu1~18.04' --with-bugurl=file:///usr/share/doc/gcc-7/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-7 --program-prefix=aarch64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libquadmath --disable-libquadmath-support --enable-plugin --enable-default-pie --with-system-zlib --enable-multiarch --enable-fix-cortex-a53-843419 --disable-werror --enable-checking=release --build=aarch64-linux-gnu --host=aarch64-linux-gnu --target=aarch64-linux-gnu\r\nThread model: posix\r\n**gcc version 7.3.0 (Ubuntu/Linaro 7.3.0-27ubuntu1~18.04)**\r\n\r\n\r\n", "comments": ["any update on this ? I'm running this on RK3399 pine board and getting following error", "FYI, with large enough swap space and patience, I was able to build recent master branch on an arm64 platform running Debian rootfs. What I used\r\n\r\n1. my patches, https://github.com/tensorflow/tensorflow/pull/16175\r\n2. bazel 0.22.0\r\n3. gcc version 8.2.0 (Debian 8.2.0-20)\r\n4. \r\n```\r\nbazel build --config opt \\\r\n--config=noaws --config=nogcp --config=nohdfs \\\r\n--config=noignite --config=nokafka --config=nonccl \\\r\n--local_resources 1024,2,2 \\\r\n--linkopt=-Wl,--no-keep-memory \\\r\n--define tensorflow_mkldnn_contraction_kernel=0 \\\r\n//tensorflow/tools/pip_package:build_pip_package\r\n```", "@freedomtan Still getting the error. with latest tensorflow and bazel 0.22.0\r\n\r\nTensroflow version : Master branch  (with your changes #16175 )\r\nBazel version : 0.22.0\r\ngcc -v \r\ngcc version 8.2.0 (Ubuntu 8.2.0-1ubuntu2~18.04)\r\n\r\n**ERROR: /home/rock64/Documents/tensorflow/tensorflow/core/kernels/BUILD:3012:1: C++ compilation of rule '//tensorflow/core/kernels:matrix_square_root_op' failed (Exit 1)**\r\ngcc: fatal error: Killed signal terminated program cc1plus\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 11834.180s, Critical Path: 4097.51s\r\nINFO: 1171 processes: 1171 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n**And here is the build command** \r\nbazel build --jobs 1 --config opt --config=noaws --config=nogcp --config=nohdfs --config=noignite --config=nokafka --config=nonccl --local_resources 1024,2,2 --linkopt=-Wl,--no-keep-memory --define tensorflow_mkldnn_contraction_kernel=0 //tensorflow/tools/pip_package:build_pip_package\r\n", "@TropSpirit Do you have large enough swap space? Did you check system issues, such as memory pressure and thermal problem?\r\n\r\nCheck https://github.com/tensorflow/tensorflow/issues/25688", "Thanks @freedomtan \r\nfinally today i was able to build completely. I guess it was swap space or memory going out of range. Every time it failed i was rebooting the machine and ran the same command to continue the build from where it failed. \r\n\r\n**Target //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 1626.568s, Critical Path: 47.03s\r\nINFO: 260 processes: 260 local.\r\nINFO: Build completed successfully, 302 total actions\r\nrock64@rockpro64:~/Documents/tensorflow$ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg**\r\n\r\n\r\n", "I have similar problem described here:\r\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/developers/GEV7Z76Cws4\r\n\r\nUsing Debian 10 Buster, Python v2.7.\r\nI try to build specific tensorflow v1.14.0 without avx2 support, because I get following error:\r\nNov  4 17:12:32 moodle37 kernel: [9773297.574293] traps: python2.7[4570] trap invalid opcode ip:7fb9b74bca59 sp:7ffdb7605e10 error:0 in libtensorflow_framework.so.1[7fb9b6d97000+18f8000]\r\n\r\nGot following error on build:\r\n**ERROR: /root/inst/tensorflow_src/tensorflow/core/kernels/BUILD:3255:1: C++ compilation of rule '//tensorflow/core/kernels:matrix_square_root_op' failed (Exit 1): gcc failed: error executing command**\r\n\r\nI will try to make more space on drive root.\r\n\r\n"]}, {"number": 25709, "title": "Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Windows 10 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no \r\n- TensorFlow installed from (source or binary):source \r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: 9.0 / 7.1\r\n- GPU model and memory:  GTX 1050 Ti\r\n\r\n\r\n**Describe the current behavior**\r\ni tried to train the model by using image datasets\r\n\r\n**Code to reproduce the issue**\r\n    \r\n    from keras.callbacks import ModelCheckpoint, EarlyStopping\r\n    from model import load_model\r\n    import numpy as np \r\n    import argparse\r\n    parser=argparse.ArgumentParser()\r\n    parser.add_argument('n_epochs',type=int)\r\n\r\n    args=parser.parse_args()\r\n    X_train=np.load('training.npy')\r\n    frames=X_train.shape[2]\r\n    #Need to make number of frames divisible by 10\r\n\r\n    frames=frames-frames%10\r\n    X_train=X_train[:,:,:frames]\r\n    X_train=X_train.reshape(-1,227,227,10)\r\n    X_train=np.expand_dims(X_train,axis=4)\r\n    Y_train=X_train.copy()\r\n\r\n    epochs=args.n_epochs\r\n    batch_size=1\r\n\r\n    if __name__==\"__main__\":\r\n\r\n\t    model=load_model()\r\n\r\n\t    callback_save = ModelCheckpoint(\"model.h5\",\r\n\t\t\t\t\t\t\t\t\tmonitor=\"mean_squared_error\", save_best_only=True)\r\n\r\n\t    callback_early_stopping = EarlyStopping(monitor='val_loss', patience=3)\r\n\r\n\t    print('Model has been loaded')\r\n\r\n\t    model.fit(X_train,Y_train,\r\n\t\t\t      batch_size=batch_size,\r\n\t\t\t      epochs=epochs,\r\n\t\t\t      callbacks = [callback_save,callback_early_stopping]\r\n\t\t\t  )\r\n\r\n**Output**\r\n2019-02-13 03:45:14.939208: E tensorflow/stream_executor/cuda/cuda_driver.cc:981] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n\r\n2019-02-13 03:45:14.943572: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 000001F501169B70: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n\r\n2019-02-13 03:45:14.948104: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 000001F501169B70: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n\r\n2019-02-13 03:45:14.952242: F tensorflow/stream_executor/cuda/cuda_dnn.cc:231] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 25708, "title": "DLL load failed with error code -1073741795", "body": "---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\Anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\Anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-2-64156d691fe5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 try:\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 from tensorflow.python.tools import component_api_helper\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\believe\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\believe\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\believe\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\believe\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\believe\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["From the error message, I take this as a TensorFlow Build/Installation Issue (correct me if wrong)\r\nCan you please provide following information requested by the template?\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I'm having the same issue:\r\nOS: Windows 7\r\ntensorflow installed with pip\r\nversion: 1.13.1\r\npython version: 3.6.7\r\ninstalled with pip no virtual environment\r\ndid not use Bazel\r\nGCC/Compiler: 2015-2019 (installed with visual studio 2019)\r\nI have cuda 9 installed but using tensorflow, not tensorflow-gpu \r\nGraphics card: NVIDIA GeForce 9800 GT\r\n\r\nStack trace:\r\nImportError                               Traceback (most recent call last)\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\nc:\\program files\\python36\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\nc:\\program files\\python36\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-4-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 from tensorflow._api.v1 import app\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Rodrigo\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Rodrigo\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Rodrigo\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"c:\\program files\\python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"c:\\program files\\python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n"]}, {"number": 25707, "title": "summary v2 create_file_writer() behavior leads to incorrect resource deletion", "body": "This is a tracking bug for the known issue that the V2 summary API's `create_file_writer()` logic behaves unexpectedly in regards to the underlying resource lifecycle management.  This typically manifests as users seeing errors like `tensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/logdir:./log/N10tensorflow22SummaryWriterInterfaceE does not exist. [Op:WriteScalarSummary] name: epoch_loss/`\r\n\r\nUsers can encounter this in 1.x when using `tf.contrib.summary.create_file_writer()` in eager mode, or in 2.0 as `tf.summary.create_file_writer()`, or via wrapper APIs like the Keras TensorBoard callback.\r\n\r\nExample issues: #24632, #25524\r\n\r\nThe root cause is that `create_file_writer(logdir)` returns a SummaryWriter python object backed by a C++ SummaryWriterInterface resource, but the mapping is not 1:1, which violates other assumptions about resource management under eager mode.  In particular, two SummaryWriter instances created for the same logdir by default will both attempt to reference the same named resource.  (Sharing by logdir was intended to be a mechanism for using one eventfile per logdir akin to `tf.summary.FileWriterCache` in 1.x, which makes TensorBoard better behaved.)\r\n\r\nThis is a problem when one of the SummaryWriters is deleted (aka loses its last remaining reference), because it will unconditionally attempt to delete the named resource it was created with.  If a resource with that same name is referenced by any other SummaryWriters, those ones will now emit the above `NotFoundError` when attempting to use them.  Note that this happens even if the deleted SummaryWriter was properly `close()`'d before the new one was created, because the logdir name sharing means that it new writer's resource has the same name as the old one.\r\n\r\nThe correct fix is changing the resource names to be unique so that the mapping from SummaryWriter object <-> SummaryWriterInterface resource is 1:1 as designed.  This will likely require other workarounds for the one-eventfile-per-logdir problem.\r\n\r\nMinimal repro:\r\n```python\r\nimport tempfile\r\nimport tensorflow as tf\r\n\r\nif tf.__version__.startswith('2.'):\r\n  create_file_writer = tf.summary.create_file_writer\r\nelse:\r\n  tf.enable_eager_execution()\r\n  create_file_writer = tf.contrib.summary.create_file_writer\r\n\r\ndir = tempfile.mkdtemp('shared-writer-bug')\r\nw = create_file_writer(dir)\r\nw.close()  # succeeds, deletes old resource\r\n# 1) new SummaryWriter created w/ new resource (but same name)\r\n# 2) reassigning w deletes old SummaryWriter along w/ new resource (BUG)\r\nw = create_file_writer(dir)\r\nw.close()  # raises NotFoundError; new resource has been incorrectly deleted\r\n```", "comments": ["Is there a workaround while the bug is being resolved?\r\nUPDATE: opened a new bug #25976 as I had a issue with the same error, but different reason", "Reopening to track propagating this fix into the TF 2.0 Keras TensorBoard callback, which will fix the user issues reported in the original description above.", "Closed by 059ea3ba68db861e40d750eba688281011d2735f.\r\n"]}, {"number": 25706, "title": "[ROCm] Add no_rocm tags for unit tests that currently fail the ROCm build", "body": "Added `no_rocm` tags to the failing unit tests that had been previously excluded via `rocm/run_py3_core.sh`.  This was suggested by @gunan as a follow-up PR for the now merged PR https://github.com/tensorflow/tensorflow/pull/25627", "comments": ["I'll add that my local ci_build run with `run_py3_core.sh` shows this result:  \r\n```\r\nExecuted 1062 out of 1062 tests: 1062 tests pass.\r\n```", "Apologies @gunan.  After running all of the unit tests for a few more rounds, I found a flaky unit test and applied the \"no_rocm\" tag to it.  Should be all set now.  "]}, {"number": 25705, "title": "Fix infinite loop when ignore_errors is used with TFRecordDataset", "body": "This fix tries to address the issue raised in #25700 where\r\nignore_errors combined with TFRecordDataset will have a infinite loop.\r\nThe issue was that when ignore errors happen, the index of the file names\r\nare not moved forward with error returned. So the same file is repeated.\r\n\r\nThis fix is a short solution to fix it.\r\n\r\nThis fix fixes #25700.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Not very familiar with this part of the code, maybe there are some better ways of fixing it? /cc @mrry ", "Thanks for the contribution! I'm reassigning this to @jsimsa to make a call on what the right semantics should be here."]}]