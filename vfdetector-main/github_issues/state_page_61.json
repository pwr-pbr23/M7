[{"number": 36465, "title": "How can I clear GPU memory in tensorflow 2?", "body": "\r\n\r\n### System information\r\n- Custom code; nothing exotic though.\r\n- Ubuntu 18.04\r\n- installed from source (with pip)\r\n- tensorflow version v2.1.0-rc2-17-ge5bf8de\r\n- 3.6\r\n- CUDA 10.1\r\n- Tesla V100, 32GB RAM\r\n\r\nI created a model, nothing especially fancy in it. When I create the model, when using nvidia-smi, I can see that tensorflow takes up nearly all of the memory. When I try to fit the model with a small batch size, it successfully runs. When I fit with a larger batch size, it runs out of memory. Nothing unexpected so far. \r\n\r\nHowever, the only way I can then release the GPU memory is to restart my computer. When I run nvidia-smi I can see the memory is still used, but there is no process using a GPU. Also, If I try to run another model, it fails much sooner. \r\n\r\nNothing in the first five pages of google results works. (and most solutions are for TF1)\r\n\r\nIs there any way to release GPU memory in tensorflow 2?", "comments": ["@HristoBuyukliev,\r\nCould you please check [this](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) Tensorflow documentation and let us know if it helps. Thanks!", "@amahendrakar Hi, this is not what I am looking for. Not using up all the memory at once sounds like a useful feature, however I am looking to clear the memory tf has already taken. \r\n\r\nI just tried it out, it doesn't help. I am iteratively increasing batch size, trying to find the biggest one I can use. Once the jupyter kernel crashes, the memory stays taken up.\r\n\r\nAdditionally, even the advertised functionality does not work. I made a model that had two times fewer parameters, tensorflow still took up 31 out of 32 gigabytes. \r\n\r\n", "Hello @HristoBuyukliev, I had a similar problem when I was iterating over model.predict(), if you are iteratively increasing batch size, try after each batch_size training do `tf.keras.backend.clear_session()`.\r\nThat seems to be a case of memory leak in each training.", "You may try [limiting gpu memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) in this case.\r\nPut following snippet on top of your code;\r\n```python\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0], True)\r\n# your code\r\n```", "Hi @HristoBuyukliev , this is a very old issue that everyone is facing in TF 1.x as well as TF 2.x, it seems to be a design flaw and the TF team doesn't seem to care about fixing (I have been facing this issue for more than 2 years now).\r\n\r\nWhat worked well for me was just to run my train/eval in a separate process and wait for it to finish. So when the process finishes the system kills it and releases the GPU resources automatically.\r\nYou can achieve this by doing something like:\r\n\r\n```\r\nimport multiprocessing\r\n\r\nprocess_eval = multiprocessing.Process(target=evaluate, args=(...))\r\nprocess_eval.start()\r\nprocess_eval.join()\r\n```\r\n", "@ymodak \r\nAs I also said to amahendrakar:\r\n1. This seems like a nice feature, but not relevant to my problem.\r\n2. Tried it anyway, did not work. \r\n\r\n@taborda11 Thank you for your suggestion, unfortunately it did not work.\r\n@EKami Yes, I figured by now there is no solution. Thank you for your suggestion, I will try it out.\r\n\r\n@taborda11 @EKami a teammate of mine found a hacky solution that kind of works:\r\n\r\n    for i in $(sudo lsof /dev/nvidia2 | grep python | awk '{print $2}' | sort -u); do sudo kill -9 $i; done\r\n\r\nThis gets all the python processes that are using GPU2 in my case, and kills them. It works, but is very very ugly and I was hoping for a better way. ", "> However, the only way I can then release the GPU memory is to restart my computer. \r\n\r\nHow do you exit the TF processes?\r\n\r\nThis looks like an issue with `nvidia-smi` based on your [last comment](https://github.com/tensorflow/tensorflow/issues/36465#issuecomment-582790407).  If `lsof /dev/nvidia2` can find the processes using the GPU then `nvidia-smi` should find them as well.", "@sanjoy I think that nvidia-smi does not list GPU processes when used within Docker (as in my case)", "> @sanjoy I think that nvidia-smi does not list GPU processes when used within Docker (as in my case)\r\n\r\nI see, thanks!\r\n\r\nHow do you exit the TF processes?  Based on what you've said so far, it looks like the TF processes are not being dying and the workaround is to find them via `lsof /dev/nvidia2` and to `kill -9` them manually.  So there may be something wrong with how they are being stopped normally.", "I have also been battling with the issue of releasing GPU memory for quite some time...\r\n\r\nMy use case is a machine in a production environment with a single Python process that has to serve different types of clients and I need to switch models depending on the service to be provided. Thus, purging previous models from memory is mandatory in this case, otherwise resource exhausted errors appear sooner than later.\r\nWith TF 1.x and Keras (when it was separate from TF) I managed to make this work with `keras.backend.clear_session()`. At some point, I decided to finally move to TF 2.x with Keras integrated into TF (`tf.keras`) and then clearing GPU memory apparently became an impossible thing to do! I got the impression that something broke in TF memory management when Keras was integrated into TF.\r\n\r\nI tried all combinations of `tf.keras.backend.clear_session()`, `tf.compat.v1.reset_default_graph()`, `gc.collect()`, `close()` the session, `tf.compat.v1.disable_eager_execution()`, and other solutions that I found online, but none of these really solved the issue.\r\n\r\nAs a last resort, I will try the solution proposed by @EKami to spawn a subprocess every time I need to switch models and I will report on how it goes.\r\nIn any case, this introduces inter-process communication and complicates things unnecessarily, so I really hope the TF team will improve GPU memory management and offer a function to really clear the session!", "Replying to my own comment...\r\n\r\nI implemented the solution based on spawning a subprocess to run Tensorflow code and (as expected) it actually works, because all resources (particularly GPU memory) are released once the subprocess is destroyed.\r\n\r\nOf course, there are some drawbacks in terms of implementation complexity, since one has to deal with multi-processing related stuff that otherwise would not be needed, such as inter-process communication or logging from multiple processes.\r\nPerformance is also significantly affected, since every subprocess will need to import TF and other modules and load models on the GPU. So, this is definitely not suited for time-critical operations.", "@EKami @mminervini \r\nI have been struggling with this issue for an amount of time that is way beyond reasonable at this point as well... PyTorch did have working solutions for this already two years ago, but I am stuck with TF for now... if you can make the switch, I can warmly recommend it. \r\n\r\nAnyhow, could you point me to a good example / tutorial for the subprocess approach if you know of any? ", "@phiwei Yep I came to the exact same conclusion but it's hard to move big projects which rely so much on TF/Keras to Pytorch. For my future projects, I won't do the same mistake tho and I can clearly see from the papers trends that it's where everyone is heading to. Even the argument of \"TF is better suited for production\" doesn't hold anymore, in fact we are shooting ourselves in the foot with bugs like this one which even after many years, are still not fixed.\r\n\r\nThe future is JAX/Pytorch, TF is doomed to be a relic of the past at this rate.\r\n\r\nAs for the subprocess tutorial, I don't have any to share but the small example I gave here: https://github.com/tensorflow/tensorflow/issues/36465#issuecomment-582749350\r\n\r\nThe bad news is: It seems that this solution doesn't work with TF 2.2 on RTX cards (yet another problem). It works well with RTX cards on TF 1.15.x and non-RTX cards on TF 2.2 (like nvidia T4). It seems to be driver related so maybe with the next driver release for RTX the issue will go away... no idea, we'll see, but at this point, I don't expect much.", "@EKami Thanks for the warning, I am in fact using TF 2.2 with an RTX card... I worked with PyTorch a lot two years ago and in my opinion, it was already a very mature tool that actually behaves the way you would want Python code to behave. Something I found very neat was that they by default use dicts for batches, loved that for customising models / handing information through models. \r\n\r\nEdit:\r\nOn that note though, Keras-Tuner works for me with RTX and TF 2.2, so there must be some way to accomplish this. ", "@phiwei I tested the \"TF-in-subprocess\" approach on a GTX with TF v2.2, but I don't have experience with the RTX series.\r\n\r\nAs for the how-to, it largely depends on your use case (e.g., grid search training, inference with multiple models, etc.) and the specific features that you want to move inside the subprocesses.\r\n\r\nIn general, I mainly relied on the Python documentation: [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) for spawning and communicating with the subprocess wrapping the TF code, and [logging cookbook](https://docs.python.org/3/howto/logging-cookbook.html) to implement logging from main process and subprocesses without conflicts.\r\n\r\nI hope this helps!", "> @EKami @mminervini\r\n> I have been struggling with this issue for an amount of time that is way beyond reasonable at this point as well... PyTorch did have working solutions for this already two years ago, but I am stuck with TF for now... if you can make the switch, I can warmly recommend it.\r\n> \r\n> Anyhow, could you point me to a good example / tutorial for the subprocess approach if you know of any?\r\n\r\nso maybe like this,hope to help you!\r\n`import multiprocessing`\r\n`p = multiprocessing.Process(target=your_train_task,args=(a,)`\r\n`p.start()`\r\n`p.join()`", "I am wondering if there has been any progress. I try to increase my batch size to see how much my model can handle, but then when I reach that point, any batch size gives an OOM error. ", "I'd be interested in that, too. \r\nEspecially since the provided methods (set_memory_growth and per_process_gpu_memory_fraction) are not sufficient enough to really release GPU Memory completely.\r\n\r\nIs there a way in the C-API to release everything completely?", "Same here, wondering if they are even working on that..", "Interesting - @EKami and @phiwei are right. I think I am going to get my lab to make the switch to Pytorch. One PhD student made the switch, but I will advocate for the rest of to do the same. These sort of issues, among others, are really problematic for tensorflow, and if it is true that Pytorch is as mature as I've been hearing, then I won't look back", "@andyrevell I have made the switch now and have not regretted it one minute, my productivity has increased a lot. I can especially recommend the combination with ray tune: https://docs.ray.io/en/latest/tune/\r\nit is a bit tricky to figure out how to best integrate this but is a great tool for HP optimisation once it is running. ", "I use this to release memory. I run it at the start and end of the process. It's important to run this at the end of your program even if the program failed. If you start a new session without having released the memory you will not get the memory back when you run this again. The only way I have found to get memory that has already been allocated back in that situation is to reboot.\r\n\r\nfrom numba import cuda\r\ncuda.select_device(0)\r\ncuda.close()\r\nprint('CUDA memory released: GPU0')\r\n", "I usually run into GPU memory issue when I train a model in one notebook  and move to train a new model in another notebook. \r\n\r\n@robotoil I placed your code snippet before training a tensorflow tf2.0) model in jupyter notebook moving from another jupyter notebook. The following code snippet didnt help :(\r\n\r\n```\r\nfrom numba import cuda\r\ncuda.select_device(0)\r\ncuda.close()\r\nprint('CUDA memory released: GPU0')\r\n```", "Any progress?", "> I use this to release memory. I run it at the start and end of the process. It's important to run this at the end of your program even if the program failed. If you start a new session without having released the memory you will not get the memory back when you run this again. The only way I have found to get memory that has already been allocated back in that situation is to reboot.\r\n> \r\n> from numba import cuda\r\n> cuda.select_device(0)\r\n> cuda.close()\r\n> print('CUDA memory released: GPU0')\r\n\r\nOnce run this, you can not `fit` again, it stucks.", "This did the trick for me: [https://stackoverflow.com/questions/15197286/how-can-i-flush-gpu-memory-using-cuda-physical-reset-is-unavailable/46597252#46597252](https://stackoverflow.com/questions/15197286/how-can-i-flush-gpu-memory-using-cuda-physical-reset-is-unavailable/46597252#46597252)", "`nvidia-smi`\r\nWed Nov 18 15:12:04 2020      \r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 455.28       Driver Version: 455.28       CUDA Version: 11.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 206...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   53C    P0    31W /  N/A |   2815MiB /  5934MiB |      4%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      2975      G   /usr/lib/xorg/Xorg                295MiB |\r\n|    0   N/A  N/A      3160      G   /usr/bin/gnome-shell               89MiB |\r\n|    0   N/A  N/A     17302      G   /usr/lib/firefox/firefox            2MiB |\r\n|    0   N/A  N/A     17309      G   /usr/lib/firefox/firefox            2MiB |\r\n|    0   N/A  N/A     17327      G   /usr/lib/firefox/firefox           18MiB |\r\n|    0   N/A  N/A     17354      G   /usr/lib/firefox/firefox            2MiB |\r\n|    0   N/A  N/A     17381      G   /usr/lib/firefox/firefox            2MiB |\r\n|    0   N/A  N/A     17399      G   /usr/lib/firefox/firefox            2MiB |\r\n|    0   N/A  N/A     47887      G   ...gAAAAAAAAA --shared-files       27MiB |\r\n|    0   N/A  N/A     97563      G   ...AAAAAAAA== --shared-files       25MiB |\r\n|    0   N/A  N/A    125022      C   python                           2339MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nand next\r\n\r\n`kill -9 125022`\r\n", "> \r\n> `kill -9 125022`\r\n\r\n@smougel No, I think the main point of this issue is that we want a way to clear the tensorflow gpu memory usage while keeping the main python process.\r\n", "CC @imintz ", "The inability to clear the GPU RAM after clearing the session in production is a huge problem. Unfortunately, I am unable to apply the Process trick because I am running the model in Celery worker which is a daemonic process and it is not allowed to have children. and freeing the RAM using numba.cuda will put the GPU in an unhealthy state.  There should be a way to cleanly clear the RAM from Tensorflow", "I'm feeling pain from this issue 2 years later again. For the love of god we need a fix for this", "I have a similar issue when running the inference model with Celery worker, the allocated GPU remains there in the memory even after program has completed successfully.  \r\nWaiting for the solution from tensorflow  :(", "Any progress ? It's not amusing any more ", "Using the numba libraries helped me aswell, normally tensorflow 2 automatically closes the sessions, unlike tensorflow1.\r\nBut while working with celery worker nodes, the sessions are not closed, as the PID holds onto celery somehow, manually including cuda.close() from numba might help. It did for me :)\r\n", "Hello, is there any plan to add functionality for clearing GPU memory to Tensorflow? At best, this is a really embarrassing design oversight.", "Same problem and I have come to the same conclusions. Additionally I am using the C API (inference) which is not as capable as feature rich as the Python one. I ended up patching and building TensorFlow myself, where I have exposed two functions that are meant to be used for testing only. The functions are `GPUProcessState::TestOnlyReset` and `ProcessState::TestOnlyReset`. This resets the singleton allocator objects that control the memory allocations on the CPU and GPU (CUDA) - a `Session` needs to be recreated after this, which I don't mind as it fits my use case.\r\n\r\nThis is far from a clean solution but it works for me and honestly issues/tickets on this platform for this project are next to worthless.", "```\r\nimport gc\r\ngc.collect()\r\n```\r\nWorking nice with AMD.\r\n", "> \r\n> \r\n> ```\r\n> import gc\r\n> gc.collect()\r\n> ```\r\n> \r\n> Working nice with AMD.\r\n\r\nDidnt work for me. RTX Titan, ThreadRipper 3960", "We are using 6 pytorch models in production. We have an easy solution for this issue in pytorch. Recently we tried tf2 for a use case. Now we want to deploy it into GPU and this issue is severely affecting us.\r\n\r\nTF models clearly take more memory than what is needed in inference. This gives a headache in maintaining our models available in production. I wish I should have stuck with pytorch. \"TF is better suited for production\" looks like a joke to me.\r\nWe are using pytorch in production for almost 2 years, Never faced a problem in memory management. ", "Not sure if this will help but this is what I've noticed.\r\n\r\nI'm working on time-series problems where I train a model, roll forward one month, train a model... I was initially using RTX 2060 and TF 2.1 in Windows and everything worked fine. I didn't notice any memory leak/OOM issues. Built my new PC with a RTX 3070, Ubuntu 20.04, TF 2.5, CUDA 11.2, cuDNN 8.1 (I followed the TF compatibility table so there shouldn't be an issue from a version POV). I noticed TF seems to be leaking 3-5mb (memory usage was creeping up in `nvidia-smi`) at each iteration. This really adds up after hundreds of iterations. Then I tried various settings and I noticed that if I turn JIT/XLA off, the memory leak seems to go away. Note that I wasn't using XLA in Windows with the RTX 2060 when I could train thousands of iterations without issues. So if you're having memory issues in production, do you have XLA on?\r\n\r\nThis is how I'm clearing sessions after each iteration:\r\n```\r\ndel model # tried model = None, it doesn't have a difference\r\ntf.keras.backend.clear_session()\r\ngc.collect()\r\n```\r\n", "Any update?  It's really painful to have done all the legwork training in Tensorflow only to see that so much as loading a model into GPU memory basically removes the ability to swap off of that GPU during a long-running process.\r\n\r\nThis affects anyone who is running a server that:\r\n- Also does GPU rendering/processing.\r\n- Runs multiple models (ie Pytorch models) and would like to swap them.\r\n- Anything else using the GPU.\r\n\r\nI'd argue that these are critical use cases in production.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Still having this issue. Just trying to move some 'leaky' code inside a function to hope this hints to the gc that artefacts created during the function should be cleaned up... AND IT WORKED!\r\n\r\ncan confirm this works\r\n```py\r\ndef leaky_function():\r\n    . . . leaky stuff . . .\r\n    return something\r\nthing = leaky_function()\r\n\r\ntf.keras.backend.clear_session()\r\ngc.collect()\r\n```\r\nBut... this shouldn't be required. Clearly there is some issue in tensorflow making gc.collect() mandatory as well.", "@Avelina9X \r\n\r\nI am not seeing similar results with an RTX 3060 on Manjaro (Nvidia 465.31 and CUDA 11.3). I've wrapped all code in a main function, and have not been able to get TF to release any vram after the function runs, even with:\r\n\r\n```\r\nif __name__ == '__main__':\r\n    main()\r\n    tf.keras.backend.clear_session()\r\n    del tf\r\n    gc.collect()\r\n    \r\n    # try to do something else with GPU\r\n```\r\n\r\nSubsequent uses (in the same Python process) fail to allocate GPU memory, and I can confirm that TF is still holding onto some 10GB of vram via nvidia-smi.\r\n\r\nIf your version is in fact working for you, the manual call to `gc.collect()` isn't particularly surprising. Python's GC runs only periodically, and does not automatically remove items, even if they're marked for deletion. For instance `a = 10; del a;` only guarantees that no subsequent lines of code have access to the variable `a`, but makes absolutely no promises about if/when the memory allocated for `a` will be released. For a short-running process or one that uses little memory, it may never be released until the process terminates.\r\n\r\nAll that said, I think the simplest fix available is to run *everything* as a subprocess, such that no large amounts of data like a training set are passed to the subprocess, which is what causes the performance degradation others have mentioned.\r\n\r\n```\r\nif __name__ == '__main__':\r\n\r\n    p = multiprocessing.Process(target=main)\r\n    p.start()\r\n    p.join()\r\n\r\n    # use the GPU for something else now (works for me on Manjaro w/ an RTX 3060)\r\n```\r\n\r\nThe above is taken from: https://stackoverflow.com/questions/55479221/how-to-clearing-tensorflow-keras-gpu-memory", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36465\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36465\">No</a>\n", "I walked through the whole page, tried mostly every solution, and it still doesn't work.", "I'm facing this same issue. I load a model and want to continually do predictions with it, so clearing the entire memory is out of the question (it would also clear the model, which I want to load only once at the start of the application and then have it running predictions indefinitely).\r\n\r\nWhat amuses me is the fact that the memory usage grows after some batches pass through the model. From my understanding, if I have only one model loaded, the memory should stay constant no matter how many predictions I run on it, but it seems TensorFlow somehow just keeps increasing the memory usage as predictions happen.\r\n\r\nUpdate: can confirm that using both `gc.collect()` and `tf.keras.backend.clear_session()` doesn't work, GPU will still run out of memory after enough predictions.\r\n", "Have you guys tried my suggestion of turning XLA off? `tf.keras.backend.clear_session()` works for me after turning XLA off and I can repeatedly train models (in fact, I've been training hundreds of new models) without restarting python to clear memory.", "So I found out that my issue was that I was using [decord](https://github.com/dmlc/decord) for loading videos and setting `decord.bridge.set_bridge('tensorflow')` (which allowed frames to be loaded in tensorflow tensors). Removing this and then transforming the values with `tf.constant()` fixed the leak.", "Hi everyone,\r\n\r\nIt looks like there are couple of different issues here, with different root causes & fixes.\r\n\r\n1. In some cases the TensorFlow process does not exit (\"the only way I can then release the GPU memory is to restart my computer\").  I suspect this behavior is environmental and is outside TensorFlow so we don't have a fix or a workaround.  If you're able to share a reproducer for this that we can run (e.g. on a cloud machine or locally) then I'm happy to take a look.\r\n2. TensorFlow allocates almost all the memory on the GPU by design, which does not play well with other processes trying to use the same chip (https://github.com/tensorflow/tensorflow/issues/36465#issuecomment-871755065).  This is a deliberate choice, but we have a workaround for now, and are working towards a longer term solution.  More on this below.\r\n3. Something specific to how XLA GPU integrates with TensorFlow (https://github.com/tensorflow/tensorflow/issues/36465#issuecomment-903189335).  **I'd suggest filing a separate issue for this with a reproducer**, since I believe this is distinct from (1) and (2).\r\n\r\n\r\nRegarding (2), there are a couple of options:\r\n\r\n1. Enable `allow_growth` (e.g. by adding `TF_FORCE_GPU_ALLOW_GROWTH=true` to the environment).  This will prevent TF from allocating all of the GPU memory on first use, and instead \"grow\" its memory footprint over time.  However, there are a few caveats: a) this does not cause TF to release memory (i.e. we only grow, we don't shrink) so it may not help in some use cases, and b) it increases fragmentation so models that used to fit can now run OOM.\r\n2. Enable the new CUDA malloc async allocator by adding `TF_GPU_ALLOCATOR=cuda_malloc_async` to the environment.  This leverages the new async allocation APIs, which allows us to have BFC-like behavior without preallocating all of the memory on the GPU.  This is experimental currently (and the implementation was contributed by @nouiz from NVIDIA), but we're working towards making it non-experimental.", "How about an example of how to properly release memory?\n\n> On Aug 23, 2021, at 2:17 PM, Sanjoy Das ***@***.***> wrote:\n> \n> \ufeff\n> Hi everyone,\n> \n> It looks like there are couple of different issues here, with different root causes & fixes.\n> \n> In some cases the TensorFlow process does not exit (\"the only way I can then release the GPU memory is to restart my computer\"). I suspect this behavior is environmental and is outside TensorFlow so we don't have a fix or a workaround. If you're able to share a reproducer for this that we can run (e.g. on a cloud machine or locally) then I'm happy to take a look.\n> TensorFlow allocates almost all the memory on the GPU by design, which does not play well with other processes trying to use the same chip (#36465 (comment)). This is a deliberate choice, but we have a workaround for now, and are working towards a longer term solution. More on this below.\n> Something specific to how XLA GPU integrates with TensorFlow (#36465 (comment)). I'd suggest filing a separate issue for this with a reproducer, since I believe this is distinct from (1) and (2).\n> Regarding (2), there are a couple of options:\n> \n> Enable allow_growth (e.g. by adding TF_FORCE_GPU_ALLOW_GROWTH=true to the environment). This will prevent TF from allocating all of the GPU memory on first use, and instead \"grow\" its memory footprint over time. However, there are a few caveats: a) this does not cause TF to release memory (i.e. we only grow, we don't shrink) so it may not help in some use cases, and b) it increases fragmentation so models that used to fit can now run OOM.\n> Enable the new CUDA malloc async allocator by adding TF_GPU_ALLOCATOR=cuda_malloc_async to the environment. This leverages the new async allocation APIs, which allows us to have BFC-like behavior without preallocating all of the memory on the GPU. This is experimental currently (and the implementation was contributed by @nouiz from NVIDIA), but we're working towards making it non-experimental.\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "About cudaMallocAsync. The verison in TF isn't working.\r\nThis PR fix it: https://github.com/tensorflow/tensorflow/pull/50961 So watch this PR and when it get fixed, wait a day and you can try TF nighly.\r\n\r\n@robotoil it is currently not possible to ask TF to release memory.\r\n\r\n@sanjoy About allow_growth, you are right that in theory it can still increase memory fragmentation. But recently it was changed to use the cuMemMap interface and grow an allocation inside the same virtual address. It doesn't create distinct poll as before, only 1 pool that grow.\r\nSo the fragmentation now is much lower and to my knowledge only come from the ptr being at different position, so it trigger different fragmentation issue. Do you know a real case where the current allow_growth implementation trigger a new OOM, while not using allow_growth works?", "The fix for cudaMallocAsync was merged a few hours ago.\r\nCan you wait 24h and try TF nightly build to be sure that it also works for you?\r\nIf you have any comments on that new features, please share with us.\r\n\r\nTo enable it, use the environment variable `TF_GPU_ALLOCATOR=cuda_malloc_async`.\r\n\r\nPlease share your result on this new feature.\r\n\r\nNote, with this new allocator, the cuda driver will release automatically reserved, but not used memory when other library in the same process make a cudaMalloc calls that miss memory. So you do not need to trigger a trim command yourself.", "This problem is radicurous, I saw this problem in TF1.x era (2016), and when it comes TF2 in 2021, this problem still makes no progress. People struggle to solve this problem, but all the solutions look ugly.  I am sure I would still be a Pytorcher, and I would never use Tensorflow. (Meet trouble just for copying a single model...)", "The current proposed solution will hopefully become the default in the future. It is a new memory allocator.\r\nWhy do you find that solution ugly? I'm very interested to know why you find it ugly as I do not see it like this.\r\n\r\nIf what you find ugly is the use of the environment variable, we are good. It should go away at some point.\r\nThe environment variable is something temporary to help us test the new memory allocator before doing changing the default. Changing a memory allocator must be done with caution as it have many implication. This is why it is available as an option first. ", "Setup:\r\n\r\nTF 2.5\r\nCuda 11.4\r\nNvidia RTX 2080 Super.\r\n\r\nTrying to run a bunch of tests that involve building models, running a training and evaluation step. Each model is built in series in its own subprocess which terminates, after calling \r\n`del models\r\ntf.keras.backend.clear_session()\r\n gc.collect()\r\n `\r\n The memory of the GPU is not being released after the subprocess exits, so the next model cannot be built with a slew of tensorflow/core/common_runtime/bfc_allocator.cc:1054] errors appearing when the new subprocess is started.\r\n \r\nAfter setting TF_GPU_ALLOCATOR=cuda_malloc_async I get the following error:\r\n\r\n```\r\nin setup_gpus\r\n    logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\r\nvenv/lib/python3.6/site-packages/tensorflow/python/framework/config.py:452: in list_logical_devices\r\n    return context.context().list_logical_devices(device_type=device_type)\r\nvenv/lib/python3.6/site-packages/tensorflow/python/eager/context.py:1395: in list_logical_devices\r\n    self.ensure_initialized()\r\nvenv/lib/python3.6/site-packages/tensorflow/python/eager/context.py:525: in ensure_initialized\r\n    context_handle = pywrap_tfe.TFE_NewContext(opts)\r\nE   tensorflow.python.framework.errors_impl.InternalError: No allocator statistics\r\n```\r\n\r\nPlease just fix all of the memory leaks and memory allocation problems. I just want to be able to use your API without it constantly falling down.\r\n ", "Hi, I understand that memory problems aren't great. Just a few notes:\r\n- cudaMallocAsync isn't made to help the problem you encounter. It helps only for some other cases.\r\n- You need TF nightly build to use cudaMallocAsync for now. The working version will be available in the next TF release.\r\n\r\nAbout your issues. I tried what you describe years ago with another software then TF. It was something hard to do due to python. The only reliable way that I found is to have bash script that launch all the jobs or to launch sub process one after the other. You said that you used subprocess, but are you sure it wasn't multiple threads? They aren't the same thing and in your case could make a big difference. Also make sure that you read the subprocess return value.", "The processes are sequential, the next model is not built until after the previous one has finished the training and evaluation step. I call subprocess.run(my_training_script.py) which is a blocking call, i.e. the next call cannot occur until the subprocess has finished. \r\nTensorflow is just not deallocating memory, even after processes finish. In order to clear the gpu memory I have to kill the process that is spawning subprocesses. ", "  from numba import cuda\r\n   cuda.select_device(0)\r\n   cuda.close()\r\nThis works on kaggle kernel, when I first run tf2 keras predict then release gpu and do torch inference.", "I'm using TensorFlow in C++ and I am observing the same. \r\n\r\nIn Python, I have successfully created background processes for performing inference and that works well, as the session is created and deleted in the background process, keeping the main process free of this memory leakage issue.\r\n\r\nHowever, in C++ this solution does not seem to work, as creating processes in C++ is not viable. It really needs to be a proper reset method of sorts for this.\r\n\r\nWould love if TensorFlow could fix this really annoying issue... It is making it challenging to use TensorFlow in production!", "> from numba import cuda cuda.select_device(0) cuda.close() This works on kaggle kernel, when I first run tf2 keras predict then release gpu and do torch inference.\r\n\r\nThis worked for me:)\r\nTF version 2.7.0, docker image", "> from numba import cuda cuda.select_device(0) cuda.close() This works on kaggle kernel, when I first run tf2 keras predict then release gpu and do torch inference.\r\n\r\nThis worked for me !\r\nTF version 1.15.0==gpu in Colab\r\n\r\nthank you so much :-)", "Looks like the issue got resolved by the resolution provided by @chenghuige . Can we close this issue? Thanks!\r\n\r\nEDIT: Not closing it. Thanks for the update", "No, this does not resolve the issue. What we are asking for is a clear and clean TensorFlow API function (some of us need it in the C API as well) to clear the memory TF allocated.", "I agree with @3a2l. This has **not** been resolved. Need a proper, stable solution for freeing GPU memory through the TF API. This is relevant for all programming languages that use TF. Strange that this does not exist in 2022.", "what a 180 degrees turn. first time I picked up tensorflow a few years ago it seemed like a badass, state of the art library, but it's quite shocking that an issue like this is still not elegantly fixed. makes me lose hope. \r\nLately, it starts to be increasingly clear that basing future projects on it is a painful path, except if we see a breakthrough (highly doubt it).\r\nfunnily, I really liked how 'tensorflow' sounds compared to alternatives", "I'm going to add my 2 cents here, hoping that this will be useful to some, but also knowing that this goes against the currently popular opinion.\r\n\r\nJust because `nvidia-smi` says that TF occupies (almost) all available GPU memory, doesn't actually mean that all of this memory is actively in use. TensorFlow manages device memory by itself and what `nvidia-smi` reports is the amount of memory that is currently under TensorFlow's management rather than the amount of memory that is occupied by tensors. \r\n\r\nIf you want to see how much memory is currently occupied by tensors use: [tf.config.experimental.get_memory_info](https://www.tensorflow.org/api_docs/python/tf/config/experimental/get_memory_info). This is because\r\n\r\n> For GPUs, TensorFlow will allocate all the memory by default, unless changed with [tf.config.experimental.set_memory_growth](https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth). The dict specifies only the current and peak memory that TensorFlow is actually using, not the memory that TensorFlow has allocated on the GPU.\r\n\r\nIf you are running into OOM exceptions, chances are high that you (unknowingly) kept a reference to your model around which prevents that memory from being freed. Here is a list of a few pitfalls that are easy to fall into (and that I _definitely_ didn't learn about the hard way):\r\n\r\n- In Jupyter (IPython), if your training loop (or call to `model.fit`) uses objects from different cells, make sure those objects don't keep references to your model or any tensors. If they do, those tensors will stick around.\r\n- The `history` object returned by `keras.model.fit` holds a reference to both the model and its callbacks. If you keep the history object around the model will stick around, too.\r\n- Callbacks (`tf.keras.callbacks.Callback`) hold a reference to the model (`self.model`), so your model will live as long as the callback lives. The same applies if you are using custom callbacks that hold references to sub-models (or other things that reference the Graph).\r\n- `tf.data.Dataset` may be (partly) allocated in the GPU and will become part of the graph during training. As such, it will be alive for as long as your model is alive.\r\n\r\n---\r\n\r\nMost of the time your main process will be the only thing that uses the GPU, so it is okay for TF to use all available vRAM. Sometimes, however you may want tensorflow to manage less memory (because you have multiple processes that need parallel access to the GPU). IN this case use a [logical GPU](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and limit its memory to the peak amount that the current process needs.\r\n\r\n---\r\n\r\n**TL:DR**: Intuition on why allocating all the memory is actually desirable.\r\n\r\nTF allocating (and hogging) _all_ available GPU memory from the start is actually a good thing and exactly what you would expect from a managed solution like this. Conceptually, this is one of the big differences between TF and torch. Torch asks you to manage your own memory (`.to(device)` and consorts), which gives you greater power, but - as it goes - with greater power comes greater foot shooting potential. You have to worry about things like memory fragmentation (in long-running processes), the performance impact of placing batches (or the entire dataset, if you are okay with small models) onto the GPU or the host, how many batches to place into the GPU, how to distribute if you are using multiple GPUs, etc. TF will worry about all of this for you, but that comes at the cost of giving TF an aligned chunk of memory that it can work with. In a nutshell, that's what you are buying when you go down the managed route and why it is not a bad thing that TF holds on to the memory until you terminate the process (giving memory back incrementally is a potential source of aforementioned memory fragmentation).", "@FirefoxMetzger I feel that you misunderstand what is actually the issue here. If you were to load models (and clear the graph and delete the relevant objects) multiple times sequentially, you will run into OOM using TF. That demonstrates the core problem. I'm not talking about allocating some or all memory. OOM will occur eventually regardless as some memory will remain for each iteration.\r\n\r\nI believe this happens because TF creates the session globally and fails to have a proper clear method, which removes all necessary objects. Might be that we are just unable to remove them through the API. We have the exact same problem in C++ using TF, and I have found no way of fixing that. I believe it is a fundamental issue with the way sessions work in TF.\r\n\r\nIn production this is a **big** problem. The only solution I have used is to use multiprocessing and wrap the entire TF process inside independent processes, which is not ideal and not possible for all applications.\r\n\r\nI can understand why there might be some \"leak\" due to any number of reasons, but the leak I am observing is quite severe. I also don't buy the \"that is what you get when you use TF\" argument. If you work with torch you will come to realize that it is actually not that low-level and challenging to use. Torch has come a long way. The main reason why I put my energy on TF was with the introduction of Keras, but if you compare TF today with PyTorch-Lightning (for instance), there really is not that much difference. However, we have several projects which depends on TF and numerous developers that use it in production. \r\n\r\nThis is indeed a **big** issue and we cannot hide it under the rug!", "@FirefoxMetzger This is probably the worst instance in the steady parade of \u201cTF apologists\u201d over the past two years, of various individuals who do not understand the issue claiming it is not a problem. As explained above, it is a real issue in production. At this point, it has become amusing to receive a new email from another apologist every few months, but what is not amusing is that this repeatedly derails the conversation and the problem never gets solved.", "Seriously, try doing hyperparameter optimization using nested cross validation. You WILL run out of GPU memory in TF eventually. No amount of `delete model` or `tf.config.experimental.set_memory_growth(gpus[0], True)` or `tf.keras.backend.clear_session()` or `gc.collect()` has ever worked for me. Even in cases when I don't keep the `history` object, when I don't use callbacks, and when I don't use `Dataset`s (I typically just use Python lists), this is still an issue for me.\r\n\r\nThis has been a problem for two years now. Every once in a while, I'll come back to this thread to see if anything's been done, but nope. This is never getting fixed. Just use PyTorch.", "@andreped You're right, I think I have indeed misunderstood the main issue. Let's see if I get it right the second time around: It is not so much that tensorflow allocates a big block of memory from the start, but rather that tensorflow's leaks GPU memory?\r\n\r\nI actually never ran into that particular issue, but it does indeed seem to be the case. Here is a snippet that - I think - visualizes the problem:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.ticker as ticker\r\n\r\n@ticker.FuncFormatter\r\ndef sizeof_fmt(num, pos, *, suffix=\"B\"):\r\n    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\r\n        if abs(num) < 1024.0:\r\n            return f\"{num:3.1f}{unit}{suffix}\"\r\n        num /= 1024.0\r\n    return f\"{num:.1f}Yi{suffix}\"\r\n\r\n\r\ndef log_usage(skip=False):\r\n    foo = tf.keras.Input((1000,), dtype=tf.uint8)\r\n    if not skip:\r\n        x = tf.keras.layers.Dense(1)(foo)\r\n    return tf.config.experimental.get_memory_info('GPU:0')[\"current\"]\r\n\r\nfig, ax = plt.subplots()\r\nax.plot(\r\n    [tf.config.experimental.get_memory_info('GPU:0')[\"current\"]]\r\n    +[log_usage() for _ in range(250)]\r\n    +[log_usage(skip=True) for _ in range(250)]\r\n)\r\nax.yaxis.set_major_formatter(sizeof_fmt)\r\nax.set_xlabel(\"Num Re-Instantiations\")\r\nax.set_ylabel(\"Used vRAM\")\r\nfig.tight_layout()\r\nfig.savefig(\"memtest.png\", dpi=150)\r\n```\r\n\r\n![memtest](https://user-images.githubusercontent.com/4402489/159140148-f6348a32-4c24-4cf3-86a5-583273ad3ccd.png)\r\n\r\nWhat I would have expected to see is that after the GC ran the usage is reset back to zero (or at least to some small value that is independent of the number of re-instantiations). Also - to my surprise - it looks like tensorflow doesn't run GC once the current scope exits (CPython's policy), but instead runs at what appears to be regular intervals. Further, if I change the layer size, memory usage (and the amount of leakage) increases:\r\n![memtest](https://user-images.githubusercontent.com/4402489/159140368-a01feed7-4c19-4f23-bc4a-99619b7662ca.png)\r\n(I increased the number of input units 100x and I end up leaking 100x more memory. If I keep increasing the number of parameters (or increase the number of re-instantiations), I will eventually see an OOM exception.)\r\n\r\nI ran this on TF 2.8.0 on both Windows and Linux, on 3 different GPUs (with different CUDA versions), and with both the BFC memory manager and the experimental async allocator (`TF_GPU_ALLOCATOR=cuda_malloc_async`); results are consistent across all variations I've tried. Assuming this does capture the core issue here, perhaps it can be a starting point to track down where it comes from."]}, {"number": 36453, "title": "Keras - Supporting load/save models and weights to Google Storage", "body": "I want to save / load my keras model to / from Google storage bucket.\r\nmy environment : docker image - tensorflow/tensorflow-latest-py3 (tensorflow 2.1.0, python 3.6.9)\r\n\r\n```python\r\nfrom tensorflow.keras.models import Sequential\r\n\r\nmodel = Sequential([\r\n...\r\n...\r\n])\r\n\r\nmodel.compile(...)\r\nmodel.fit(...)\r\n\r\nmodel.save('gs://path/to/my/bucket/model.h5')\r\n```\r\n\r\nand I got this error message\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 118, in <module>\r\n    model.save('gs://MY-BUCKET/model.h5')\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\", line 1008, in save\r\n    signatures, options)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\", line 112, in save_model\r\n    model, filepath, overwrite, include_optimizer)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 92, in save_model_to_hdf5\r\n    f = h5py.File(filepath, mode='w')\r\n  File \"/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\", line 408, in __init__\r\n    swmr=swmr)\r\n  File \"/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\", line 179, in make_fid\r\n    fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl)\r\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/h5f.pyx\", line 108, in h5py.h5f.create\r\nOSError: Unable to create file (unable to open file: name = 'gs://MY-BUCKET/model.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)\r\n```\r\n\r\nI found [this PR](https://github.com/keras-team/keras/pull/11636) in keras repository, supporting above behavior. but It seems like that it's not implemented in `tensorflow.keras`.\r\n\r\nDo you have plans to support it? or, are there any alternatives in tensorflow?\r\n", "comments": ["@kim-sardine Please check [this resource](https://stackoverflow.com/questions/45585104/save-keras-modelcheckpoints-in-google-cloud-bucket) which explains clearly how to save the model in the Google Storage. Thanks!", "I just wondered if I can save model in Google Storage in one line.\r\nso for now, I have to do this by overriding ModelCheckpoint callback function and making new model saving function.\r\nI'll refer to the link above, thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36453\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36453\">No</a>\n", "BTW, It won't be easy because `MoodelCheckpoint` is using not only `save()`, but also `load_weights`, `save_weights`. ", "This feature as been enabled in regular keras. Shouldn't be hard to move it here.", "Having a one liner to load from GCS buckets would be highly desirable. It seems like a regression especially given that `saver.restore(sess,  gs://path_to_checkpoint)` worked in the past, and using `model.load_weights()` works in regular keras, but this behavior is not maintained for tf.keras. Any plans to support this moving forward? ", "This feature would be extremely helpful.", "@gadagashwini @jvishnuvardhan I think this issue should be reopend, as the stack overflow post you linked only worksaround the fact that \r\n`model.save('gs://bucket/model.h5')` isn't natively supported, whereas the TF file format `model.save('gs://bucket/model')` works as expected.\r\nIn my opinion, inconsistencies like this severly hurt to usability of TensorFlow especially in combination with GCS so it would be great if this issue could be reopened.", "@lgeiger Sure. Reopening this issue considering the comments above. Thanks!", "@k-w-w This issue has been tagged as 2.1, has there been any progress on it so far?", "What is the status of this one so far? Would love to jump in.", "I have been able to serialize `SavedModel`s to GCS Bucket folders directly from AI Platform Notebooks in the following manner:\r\n\r\n```python\r\ntf.saved_model.save(model, export_module_dir)\r\n```\r\n\r\nwhere `export_module_dir` is either a GCS Bucket or a `SavedModel` name inside a GCs Bucket. Of course, you would need to have write access to the bucket to be able to do this. I don't think that is problematic to configure. \r\n\r\nOne can load back the model similarly like - \r\n\r\n```python\r\nmodel = tf.saved_model.load(export_module_dir)\r\n```\r\n\r\n[Here's an example](https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Magenta_arbitrary_style_transfer_model_conversion.ipynb) that benefits from this. \r\n\r\nThis is doable from a Colab Notebook as well provided you have performed the authorization steps. This should look something like the following:\r\n\r\n```python\r\nfrom google.colab import auth\r\nauth.authenticate_user()\r\n```\r\nI hope this helps. ", "@sayakpaul That is correct, the `SavedModel` format correctly supports GCS, however when saving a `.h5` file GCS is still not supported.", "True that. On a personal level, I think serializing your model as a `SavedModel` is more uniform with respect to the TensorFlow ecosystem. ", "@sayakpaul Could you tell me how to save model in ```SavedModel``` format at each checkpoint?"]}, {"number": 36327, "title": "Ability to calculate projected memory usage for a given model", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): Yes, but I would likely need assistance/guidance\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe closest feature I can find to this is `tf.keras.Model.summary()`. This feature calculate the trainable and non-trainable parameters per-layer but doesn't attempt to calculate any memory usage statistics.\r\n\r\nThe proposed feature would extend `tf.keras.Model.summary()` by also calculating memory requirements per-layer and for the complete model. If multiple compute devices are used in the model then a per-device memory breakdown would also be useful.\r\n\r\n**Will this change the current api? How?**\r\nThis could either just add functionality to `tf.keras.Model.summary()` without changing the interface, or it could add extra parameters to `tf.keras.Model.summary()`. Alternatively, a new function could be added, whatever is deemed most appropriate.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone with a desire to get an idea about how much memory their model will need for either training or inference. It should be possible for someone to determine whether the model they wish to train/evaluate will fit into RAM (either system of GPU) that they have available.\r\n\r\n**Any Other info.**\r\nMy focus is on the `tf.keras.Model.summary()` API, but this feature should probably be extended to other means of model creation.\r\n", "comments": ["The answers to [this StackOverflow question](https://stackoverflow.com/questions/43137288/how-to-determine-needed-memory-of-keras-model/) have some (approximate?) functions for estimating the GPU memory usage of a Keras model.\r\n\r\nI contributed an answer to the question, which is:\r\n```python\r\ndef keras_model_memory_usage_in_bytes(model, *, batch_size: int):\r\n    \"\"\"\r\n    Return the estimated memory usage of a given Keras model in bytes.\r\n    This includes the model weights and layers, but excludes the dataset.\r\n\r\n    The model shapes are multipled by the batch size, but the weights are not.\r\n\r\n    Args:\r\n        model: A Keras model.\r\n        batch_size: The batch size you intend to run the model with. If you\r\n            have already specified the batch size in the model itself, then\r\n            pass `1` as the argument here.\r\n    Returns:\r\n        An estimate of the Keras model's memory usage in bytes.\r\n\r\n    \"\"\"\r\n    default_dtype = tf.keras.backend.floatx()\r\n    shapes_mem_count = 0\r\n    internal_model_mem_count = 0\r\n    for layer in model.layers:\r\n        if isinstance(layer, tf.keras.Model):\r\n            internal_model_mem_count += keras_model_memory_usage_in_bytes(\r\n                layer, batch_size=batch_size\r\n            )\r\n        single_layer_mem = tf.as_dtype(layer.dtype or default_dtype).size\r\n        out_shape = layer.output_shape\r\n        if isinstance(out_shape, list):\r\n            out_shape = out_shape[0]\r\n        for s in out_shape:\r\n            if s is None:\r\n                continue\r\n            single_layer_mem *= s\r\n        shapes_mem_count += single_layer_mem\r\n\r\n    trainable_count = sum(\r\n        [tf.keras.backend.count_params(p) for p in model.trainable_weights]\r\n    )\r\n    non_trainable_count = sum(\r\n        [tf.keras.backend.count_params(p) for p in model.non_trainable_weights]\r\n    )\r\n\r\n    total_memory = (\r\n        batch_size * shapes_mem_count\r\n        + internal_model_mem_count\r\n        + trainable_count\r\n        + non_trainable_count\r\n    )\r\n    return total_memory\r\n\r\n```", "@jamesmishra I have found that this method vastly underestimates the amount of GPU memory needed by my models (this method reports ~5GB, however my models with a batch size of 1 are unable to train on a 32GB V100).", "@Bidski I've noticed around a 5% - 10% discrepancy on my own models, but they are comparatively smaller models. If you're willing to send me your `model.summary()` and the `input_shape`, I'ld to see if I can replicate the issue you're seeing. My email is j@jamesmishra.com\r\n\r\nI don't want to clutter up the TensorFlow issue tracker, so I'll post further discussion and updates [on this GitHub Gist](https://gist.github.com/jamesmishra/34bac09176bc07b1f0c33886e4b19dc7).", "Hi, I've been interested in this as well and still haven't found an adequate solution. I tried using the method suggested by @jamesmishra, but I can't get it to work (the output_shape is always `None` even if I compile and call the model on dummy data).\r\n\r\nIn any case, I don't believe your method is likely to be robust across model architectures, although this is not my area of expertise at all. [Microsoft has a recent paper](https://www.microsoft.com/en-us/research/publication/estimating-gpu-memory-consumption-of-deep-learning-models/) on trying to estimate the required memory for a model and it is significantly more complex. Unfortunately, their code is not published (at least anywhere I can find). It would be really nice to have something along the lines of what they describe built into the toolkit.", "I'm always looking for feedback for my code snippet. If you have anything, feel free to comment on [this GitHub gist](https://gist.github.com/jamesmishra/34bac09176bc07b1f0c33886e4b19dc7).\r\n\r\nIt would be great to get Microsoft to open-source their code, but the good news is that most of us would be happier with a much simpler function that only covers the memory usage of the core Keras layers. That can't be that hard.", "@jamesmishra I agree it would still be great to have something that works for the basic Keras layers and 80% of use cases and I'm very appreciative of yours (and others) work on the problem. \r\n\r\nIt's still frustrating though that for the models I'm working on (GPT like transformers), and apparently other common models, the simpler approaches can wildly underestimate the memory needed depending on the hyper-parameters of the model. If I gain any insights why or when I'll be sure to comment on your gist with my thoughts and discoveries.", "A work around for subclassed models (instead of Sequential) could to use for example \r\n```\r\nmodel_object = SomeSubclassedModel()\r\ninputs = tf.keras.layers.Input(shape=(256, 512, 3))\r\nsequential = tf.keras.Model(inputs=inputs, output=model_object(inputs))\r\nsequential.summary() # i think now the memory can be better estimated because all shapes are computed.\r\n```\r\n\r\nTo incorporate this in a class one could do for example\r\n\r\n```\r\nclass SomeSubclassedModel(keras.Model): \r\n   ...\r\n   def call(self, x): \r\n       ...\r\n   def get_seq_model(self):\r\n      inputs = tf.keras.layers.Input(shape=(256, 512, 3))\r\n      tf.keras.Model(inputs=inputs, output=self.call(inputs))\r\n      return tf.keras.Model(input=inputs, output=self.call(inputs))\r\n\r\n```\r\n\r\nTo include batch_size estimation too, we could instantiate the model with random normal distribution for a trail run.\r\n\r\n```\r\nclass SomeSubclassedModel(keras.Model): \r\n   ...\r\n   def call(self, x): \r\n       ...\r\n   def get_seq_model(self, shape=()):\r\n      inputs = tf.random.normal(full_shape)\r\n      model = tf.keras.Model(input=inputs, output=self.call(inputs))\r\n      # monkey patching \r\n      def keras_model_memory_usage_in_bytes():\r\n           ....\r\n      model.memory_usage = memory_usage\r\n      return \r\n\r\n```\r\n\r\nLater use it \r\n\r\n```\r\nmodel = SomeSubclassedModel()\r\nmodel.get_seq_model(shape=(batch_size, H, W, C)).summary()\r\nmodel.get_seq_model(). keras_model_memory_usage_in_bytes()\r\n```\r\n\r\nOnce we use `call` on a subclasses object, all shapes are computed. This could be a source of information for computing the memory. What else would be required to compute the complete memory profile? I am not sure.", "> Once we use call on a subclasses object, all shapes are computed. \r\n\r\nIf I am not mistaken, using `call` also causes all needed memory to be allocated. While I may not have been too explicit on this point originally, my intention for this feature is to calculate the memory requirements without actually allocating the memory. In this way you would know exactly how much memory you would need for a single batch and it would allow you to determine the GPU specs needed in order to train your model with a batch size of `N`"]}, {"number": 36276, "title": "Support nested structure as return value in tf.py_function.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tensorflow==2.1.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nSupport for return values in nested structures as in `tf.data.Dataset.from_generator`.\r\nI cant return any nested structure as, e.g., a dict or namedtuple.\r\n\r\nSee example:\r\n \r\n````python\r\narg1_const = tf.constant([0, 1], dtype=tf.int32)\r\ndef eager_py_dict(arg1):\r\n    return {0: arg1}\r\n\r\ntf.py_function(eager_py_dict, [arg1_const], {0: tf.int32})\r\n# Returns:\r\n# tensorflow.python.eager.core._FallbackException: Expecting a DType.dtype for attr Tout, got dict\r\n````\r\n\r\n**Will this change the current api? How?**\r\nSupporting nesting structures.\r\n\r\n**Who will benefit with this feature?**\r\nPeople building their `tf.data` pipeline with `tf.pyfunction`.\r\n\r\n**Any Other info.**\r\nI will create more `tf.py_function` related stories. Since this are distinct features from my POV, there multiple feature requests. I hope this is ok.\r\n", "comments": []}, {"number": 36274, "title": "Support setting shapes directly in tf.py_function", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tensorflow==2.1.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIn comparison to `tf.data.Dataset.from_generator`, `tf.py_function` does not take an (optional) shape argument. Therefore all returned tensors have unknown shape. Looking up the docu I realized you can solve this by setting `tf.Tensor.set_shape`. It would be more convienent if  `tf.py_function` would support this directly.\r\nTherefore I wrote a wrapper for myself:\r\n\r\n```\r\ndef py_function2(\r\n    func: Callable,\r\n    inp: tf.Tensor,\r\n    tOut: Iterable[tf.dtypes.DType],\r\n    shapes: Iterable[Optional[Shape]] = None,\r\n    name: str = None,\r\n) -> Union[tf.Tensor, Iterable[tf.Tensor]]:\r\n    result = tf.py_function(func, inp, tOut, name=None)\r\n\r\n    if shapes is not None:\r\n        # Set shape of the output tensors.\r\n        for tensor, shape in zip(result, shapes):\r\n            if shape is not None:\r\n                tensor.set_shape(tf.TensorShape(shape))\r\n\r\n    return result\r\n````\r\n\r\nNote: This is not working for nested structures, which are on the other hand currently not supported by  `tf.py_function` anyway. But I will create another FR for this.\r\n\r\n\r\n**Will this change the current api? How?**\r\nYes, `tf.py_function` would get another optional argument.\r\n\r\n**Who will benefit with this feature?**\r\nPeople building their `tf.data` pipeline with `tf.pyfunction`.\r\n**Any Other info.**\r\nI will create more `tf.py_function` related stories. Since this are distinct features from my POV, there multiple feature requests. I hope this is ok.", "comments": ["Re-assigning to @rohan100jain for triage."]}, {"number": 36181, "title": "AttributeError: 'Tensor' object has no attribute 'log_prob'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina (Version: 10.15.2 (19C57))\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.7.5\r\n- GPU model and memory: Intel Iris Pro 1536 MB\r\n\r\n**Describe the current behavior**\r\n\r\nI get the error\r\n\r\n> AttributeError: 'Tensor' object has no attribute 'log_prob'\r\n\r\nwith TensorFlow Probability 0.9 (and TF 2.1).\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThe following code\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\nfrom tensorflow_probability import distributions as tfd\r\n\r\n\r\ndef get_mnist_data(normalize=True):\r\n    img_rows, img_cols = 28, 28\r\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n\r\n    if tf.keras.backend.image_data_format() == 'channels_first':\r\n        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n        input_shape = (1, img_rows, img_cols)\r\n    else:\r\n        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n        input_shape = (img_rows, img_cols, 1)\r\n\r\n    x_train = x_train.astype('float32')\r\n    x_test = x_test.astype('float32')\r\n\r\n    if normalize:\r\n        x_train /= 255\r\n        x_test /= 255\r\n\r\n    return x_train, y_train, x_test, y_test, input_shape\r\n\r\n\r\ndef get_bayesian_cnn(input_shape, num_classes=10):\r\n    model_input = tf.keras.layers.Input(shape=input_shape)\r\n\r\n    # kernel_divergence_fn=None to solve a symbolic exception.\r\n    x = tfp.layers.Convolution2DFlipout(6, kernel_size=(5, 5), padding=\"SAME\", activation=tf.nn.relu,\r\n                                        kernel_divergence_fn=None)(model_input)\r\n    x = tf.keras.layers.Flatten()(x)\r\n    x = tfp.layers.DenseFlipout(84, activation=tf.nn.relu)(x)\r\n    x = tfp.layers.DenseFlipout(num_classes)(x)\r\n\r\n    model_output = tfp.layers.DistributionLambda(lambda t: tfd.Categorical(logits=t, validate_args=True))(x)\r\n\r\n    model = tf.keras.Model(model_input, model_output)\r\n\r\n    return model\r\n\r\n\r\ndef neg_log_likelihood(y_true, y_pred):\r\n    return -tf.reduce_mean(y_pred.log_prob(tf.cast(tf.argmax(y_true, axis=-1), tf.int32)))\r\n\r\n\r\ndef train():\r\n    x_train, y_train, x_test, y_test, input_shape = get_mnist_data()\r\n\r\n    model = get_bayesian_cnn(input_shape=input_shape)\r\n\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(), loss=neg_log_likelihood,\r\n                  metrics=[neg_log_likelihood])\r\n\r\n    model.fit(x_train, y_train, batch_size=128, epochs=1, verbose=1)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    train()\r\n```\r\n\r\n**Comments**\r\n\r\nThis error seems to be due to the fact that `y_pred` is a tensor when the loss is called, while it should be a distribution. Meanwhile, I found a [question on Stack Overflow related to the third issue I mentioned above](https://stackoverflow.com/q/59743872/3924118). \r\n\r\n(_This is a duplicate issue of https://github.com/tensorflow/probability/issues/742, but, for completeness, I decided to open it here too._)", "comments": ["Could able to reproduce the issue with Tf 2.1.\r\nPlease take a look at the gist [here](https://colab.research.google.com/gist/gadagashwini/a64c7d6f31212ac2b3b0653989d45b32/untitled359.ipynb). Thanks!", "See https://github.com/keras-team/keras/issues/4506.", "@nbro I think the error is coming from the line `model_output` while creating the model. \r\n\r\n`model_output = tfp.layers.DistributionLambda(lambda t: tfd.Categorical(logits=t[0], validate_args=True))(x)\r\n`\r\n\r\nThe shape of output layer is ((), ()) while the metric tries to access the `index -1` and throws the following error \r\n\r\n`InvalidArgumentError: slice index -1 of dimension 0 out of bounds. for 'metrics_9/neg_log_likelihood/strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <-1>, input[2] = <0>, input[3] = <1>.`\r\n\r\nPlease check the model summary below.\r\n\r\n```\r\nModel: \"model_12\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_13 (InputLayer)        [(None, 28, 28, 1)]       0         \r\n_________________________________________________________________\r\nconv2d_flipout_12 (Conv2DFli (None, 28, 28, 6)         306       \r\n_________________________________________________________________\r\nflatten_12 (Flatten)         (None, 4704)              0         \r\n_________________________________________________________________\r\ndense_flipout_24 (DenseFlipo (None, 84)                790356    \r\n_________________________________________________________________\r\ndense_flipout_25 (DenseFlipo (None, 10)                1690      \r\n_________________________________________________________________\r\ndistribution_lambda_11 (Dist ((), ())                  0         \r\n=================================================================\r\nTotal params: 792,352\r\nTrainable params: 792,352\r\nNon-trainable params: 0\r\n```", "@jvishnuvardhan Yes, you're right. The problem is `t[0]`. If you pass `t` to logits, the output of the NN will now be `distribution_lambda (Distrib ((None,), (None,))` and we don't get the `InvalidArgumentError` error anymore. However, we still get the other error that I describe in the duplicate issue in the TFP issue tracker\r\n\r\n> AttributeError: 'Tensor' object has no attribute 'log_prob'\r\n\r\nI have updated the original issues (above and in the other issue tracker), so I suggest you remove the other comments above regarding the other issue (which was just a programming mistake).\r\n\r\nAnother user on Stack Overflow also reported this issue: https://stackoverflow.com/q/59743872/3924118.", "@nbro It is clearly showing the root-cause of the error. When I added `print(help(y_pred))` in the loss function, it  showed `Tensor in module tensorflow.python.framework.ops object`. As a tensor object, it doesn't have any `log_prob` attribute. So, it is throwing `AttributeError`. Thanks!", "I think this was resolved. I am closing the issue. Please feel free to open if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36181\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36181\">No</a>\n", "@jvishnuvardhan The issue https://github.com/tensorflow/probability/issues/742 (which is now the title of this issue) was not resolved yet, so you should not have closed this issue.", "@nbro I think this is more related to `tensorflow/probability` and you already opened an issue there. Anyway I am opening this here. Thanks!", "Do we have any solution to this issue?", "Need it too", "I am able to replicate this issue on tf-nightly (2.4.0-dev20200929), please find the [gist here](https://colab.research.google.com/gist/Saduf2019/2a7de31fddf7d8b5d942bfd7aefd9513/untitled418.ipynb).", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/a32575d4f526d8508e2820bd804ab485/35650.ipynb). Thanks!", "@nbro Was able to reproduce your issue in TF  v[2.6.0](https://colab.research.google.com/gist/sushreebarsa/eb332284c6bce9be712c5bc84edf891e/35650.ipynb#scrollTo=WTpV7wjrTFLT) and  Nightly[ 2.8.0-dev20211022](https://colab.research.google.com/gist/sushreebarsa/3ba2b921c3003df99bacb79d11faebfb/35650.ipynb#scrollTo=kGg_oY4k3pqV), please find the attached gists. Thank you!"]}, {"number": 36164, "title": "Memory leaks when doing a random tf.op on a previously converted to NumPy Tensor in TF2.1 ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nope\r\n- TensorFlow installed from (source or binary): binary (docker image)\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce GTX TITAN X (12Go)\r\n\r\n**Describe the current behavior**\r\n\r\nWhen converting a Tensor to NumPy before doing a random operation, there is a quite intense memory leak. Intriguingly, if we do not set a general seed or if we don't convert to numpy before doing the random operation, the leak is not happening. I really don't see why this is happening. I'm maybe doing something wrong, but I'm sceptical about it, the example code below being very very simple.\r\n\r\n**Describe the expected behavior**\r\n\r\nThere should not be a leak.\r\n\r\n**Code to reproduce the issue**\r\n\r\nHere is a script that uses [memory-profiler](https://pypi.org/project/memory-profiler/) to profile memory in each functions. \r\n\r\nTo generate the profiling data: \r\n\r\n`mprof run --python <script-name>`\r\n\r\nTo generate the figure below:\r\n\r\n`mprof plot -o <figure-name>` \r\n\r\n```python\r\n\"\"\"This script aims to show the memory leak that exists when tf has a\r\ngeneral seed and a conversion to numpy is done.\r\n\"\"\"\r\n\r\nimport os\r\nimport random\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tqdm\r\n\r\n\r\ndef transform_with_numpy():\r\n    def transform(x):\r\n        x = x.numpy() # Get a numpy version of x\r\n        # Some NumPy operations here\r\n        x = tf.cast(x, tf.float32) # Repass to tf version of x\r\n        # A random TF function\r\n        x = tf.image.random_contrast(x, 0.1, 0.8)\r\n        return x\r\n    return transform\r\n\r\n\r\ndef transform_with_tf():\r\n    def transform(x):\r\n        # Some TF operations here\r\n        x = tf.cast(x, tf.float32) # Recast, just to isolate cause\r\n        x = tf.image.random_contrast(x, 0.1, 0.8)\r\n        return x\r\n    return transform\r\n\r\n\r\ndef get_example(T):\r\n    example = tf.cast(np.random.rand(224, 224), tf.float32)[tf.newaxis]\r\n    return T(example)\r\n\r\n\r\n#-----------------------------------------------------------------------\r\n\r\n\r\n@profile\r\ndef function_with_conversion():\r\n    T = transform_with_numpy()\r\n    print(f'function_with_conversion')\r\n    for i in tqdm.tqdm(range(1000)):\r\n        examples = [get_example(T) for i in range(32)]\r\n\r\n\r\n@profile\r\ndef function_with_seed():\r\n    np.random.seed(100)\r\n    random.seed(100)\r\n    tf.random.set_seed(100)\r\n    T = transform_with_tf()\r\n    print(f'function_with_seed')\r\n    for i in tqdm.tqdm(range(1000)):\r\n        examples = [get_example(T) for i in range(32)]\r\n\r\n\r\n@profile\r\ndef function_with_seed_and_conversion():\r\n    np.random.seed(100)\r\n    random.seed(100)\r\n    tf.random.set_seed(100)\r\n    T = transform_with_numpy()\r\n    print(f'function_with_seed_and_conversion')\r\n    for i in tqdm.tqdm(range(1000)):\r\n        examples = [get_example(T) for i in range(32)]\r\n\r\n\r\nif __name__ == '__main__':\r\n    # Disable INFO and WARNING tf messages\r\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\n    # Sets the memory growth, experimental...\r\n    physical_devices = tf.config.experimental.list_physical_devices('GPU')\r\n    if physical_devices:\r\n        tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\n    function_with_conversion()\r\n    function_with_seed()\r\n    function_with_seed_and_conversion()\r\n```\r\n\r\n**Other info / logs**\r\n\r\n![test_mem_leak_2](https://user-images.githubusercontent.com/59838807/73010435-6aae5500-3de0-11ea-8615-1887894d7594.png)\r\n\r\n**Maybe related to**\r\n- https://github.com/tensorflow/tensorflow/issues/36034\r\n- https://github.com/tensorflow/tensorflow/issues/30873\r\n\r\n**EDIT:** \r\n\r\nAfter running the script a few more times, the output of the memory profiling data is more random than I thought. The second time I run the script, the leak disappears from the third function, but there seem to be a problem with the second function:\r\n\r\n![test_mem_leak_all_seed2](https://user-images.githubusercontent.com/59838807/73025706-0ac6a700-3dfe-11ea-9031-592d829af162.png)\r\n\r\nThen, I change the size of the examples to be (112, 112) and the leak reappears in the third function:\r\n\r\n![test_mem_leak_7](https://user-images.githubusercontent.com/59838807/73025808-41042680-3dfe-11ea-8730-f70024673baa.png)\r\n\r\nThen I rerun the code with (224, 224) sized examples and the leak reappears here as well:\r\n\r\n![test_mem_leak_all_seed1](https://user-images.githubusercontent.com/59838807/73025871-5bd69b00-3dfe-11ea-974a-5de5f24fc897.png)\r\n\r\n**EDIT 2:**\r\n\r\nThe memory leak is actually also there where the seed is set but no conversion is made. The conversion only makes it worst. For more details see [this repo](https://github.com/lerobitaille/tf-issue-36164-workaround).   ", "comments": ["Do you have any workaround meanwhile?", "I've added more details about the source of the bug and a temporary workaround at https://github.com/lerobitaille/tf-issue-36164-workaround. The memory leaks comes from the sampling of the `op_seed` in the function `tensorflow/python/framework/random_seed.py:get_seed` [here](https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/framework/random_seed.py#L39).", "I believe this may be caused by caching which `convert_to_eager_tensor` performs. Notably look at https://github.com/tensorflow/tensorflow/blob/7072568ed6b735e347fb87bc84a2b83daf806e3f/tensorflow/python/framework/constant_op.py#L68-L72 which mentions that some values can be cached.\r\n\r\nIn the C++ code, https://github.com/tensorflow/tensorflow/blob/7072568ed6b735e347fb87bc84a2b83daf806e3f/tensorflow/python/eager/pywrap_tensor.cc#L307-L313 performs the caching -- currently for Python scalars only.", "@lerobitaille \r\nCould you please let us know if this is still an issue in tf 2.7", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I've just tested with a production model and it is still an issue in tf 2.7. It seems to be related with the tf.data.TFRecordDataset, as the memory load increase seems to happen while shuffling the dataset. The LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 hack still does the work. Tested in linux kernel 5.4.0-91-generic with tf 2.7.0"]}, {"number": 36128, "title": "Tensorflow.summary.histogram produces wrong output", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n**Tested on my machine with Linux Ubuntu 16.04 and in Google Colab**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): \r\n**Installed with pip on my machine or using the one preinstalled in Colab**\r\n- TensorFlow version (use command below): \r\n**v2.0.0-rc2-26-g64c3d38 2.0.0 on my machine or v2.1.0-rc1-0-g064e1535a7 2.1.0-rc1 in Colab**\r\n- Python version: \r\n**3.5.2 on my machine or 3.6.9 in Colab**\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: \r\n**run on CPU on my machine or run on CPU in Colab or run on GPU in 10.0, V10.0.130 in Colab (there seems to be no problem when running on GPU)**\r\n- GPU model and memory: \r\n**no GPU on my machine or Colab GPU Tesla P100-PCIE-16GB**\r\n\r\n**Describe the current behavior**\r\n;tldr\r\n`tensorboard.plugins.histogram.summary_v2._buckets` produces wrong result when run with CPU on arrays with more than approx. `7e7` elements.\r\n\r\nLonger description:\r\nI wanted to add a histogram of my model outputs (not only weights) to tensorboard using `tf.summary.histogram()` in a custom keras callback. All works, histogram is shown, but the values displayed are not correct. After I have inspected further, I found out that the function `tensorboard.plugins.histogram.summary_v2._buckets`, which is internally used in the `tf.summary.histogram()` produces wrong results for larger inputs. Namely it returns incorrect counts for some of the buckets. It is very simple to check this, because the size of the data should equal the sum of counts, which are stored in the first column of the `_buckets` output. For larger arrays, this is not true.\r\n\r\nWierdly, the discrepancy seems to occur only when the code is run on CPU. On the GPU, the histogram is correct (up to some tiny difference - missing one or two elements). But I can not check larger arrays than `100e6`, because running in Colab on GPU gives an error when calling `_buckets()`: \r\n`InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:GPU:0 to /job:localhost/replica:0/task:0/device:CPU:0 in order to run LinSpace: GPU sync failed [Op:LinSpace]`\r\n\r\nDescription of each step is in Gist along with the code.\r\n\r\n**Describe the expected behavior**\r\nExpected behavior of `tensorboard.plugins.histogram.summary_v2._buckets` would be to produce the correct counts of elements in a specific bucket for all sizes of data.\r\n\r\n**Code to reproduce the issue**\r\n\r\nSimplified code:\r\n```python\r\nimport numpy as np\r\nfrom tensorboard.plugins.histogram.summary_v2 import _buckets\r\n# Create random data\r\ndata = np.random.standard_normal(int(100e6)).astype(np.float16)\r\n# Compute the histogram with tf\r\ntfhist = _buckets(data, bucket_count=20)\r\ncounts_tf = tfhist[:,2] # Counts are in the last column\r\nedges_tf = tfhist[:,0] # Edges are in the first column\r\nnp.equal(np.sum(counts_tf), data.size)  # If false, there is a bug\r\n```\r\n\r\nFull example in Gist (can be run in Colab): https://gist.github.com/paloha/d079aa9afb832b996657fb97a2763a47#file-tensorflow-produces-wrong-histogram-ipynb\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI think that the error I get when running the gist on GPU in Colab (mentioned above) should be addressed as well.", "comments": ["Was able to reproduce the issue in TF 2.6.0-dev20210530,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/91f7f61b529333de15400cc86a38f6ea/untitled125.ipynb#scrollTo=hkhqnkaBBnJm)..Thanks !", "Thank you very much for filing this issue with such a detailed and comprehensive reproduction!  I apologize for the long delay in actually investigating this further.\r\n\r\nI've done more of a deep dive in this colab of my own: https://gist.github.com/nfelt/0ae51ba3eb901b3438d9eab9a7ca4d16\r\n\r\nTo summarize my conclusions:\r\n\r\n- A) The main problem you identified, with a large number of entirely missing counts, is due to bin counts effectively being truncated at 2^24 (= 16,777,216).  This is due to the way the bin counts are constructed as a sum of 1's, where the summation inadvertently was using float32, and simple accumulation in float32 results in numerical instability because integer values past 2^24 are not exactly representable: https://github.com/tensorflow/tensorflow/issues/51419\r\n    - The reason you would only have seen this for larger arrays is because they needed to be large enough to have over 2^24 values falling into a single bin in order to manifest the effect.\r\n    - My operating assumption is that the GPU kernel for summation doesn't have the same numerical instability issues due to a different implementation, and thus didn't result in truncation (or perhaps only did so at much higher counts)\r\n    \r\n- As part of digging into this, I also noticed:\r\n   - B) some much smaller correctness issues around the handling of bin edge boundaries when floating point values in the input are very close to the bin edge. This shouldn't affect the overall count, but could mean individual bin counts are still not entirely correct in certain relatively rare cases.\r\n   - C) The actual bin edges computed by TF are different than Numpy for float16 data because of different linspace computations (TF uses float64 arithmetic, Numpy uses float64 and then rounds back to float16).  I don't think this is really a correctness issue since either choice is defensible, but it's easy to conflate with the issue in B so worth noting for that reason.\r\n\r\nWe should be able to fix at least issue (A) relatively easily.  Unfortunately until we do that fix, there is no particularly good workaround other than avoiding generating bin counts that exceed 2^24 (e.g. perhaps use more bins to keep the count in each bin smaller).", "Thank you @nfelt for getting back to this issue, I was wondering if somebody picks it up eventually. Good job in the [gist](https://gist.github.com/nfelt/0ae51ba3eb901b3438d9eab9a7ca4d16) and thx for the conclusions. :clap: ", "Hi @paloha, the fix for this (`A` in Nick's [comment](https://github.com/tensorflow/tensorflow/issues/36128#issuecomment-912115542) above)  (https://github.com/tensorflow/tensorboard/pull/5337) is merged and now in tb-nightly. ", "@paloha, Could you please let us know if we can close the issue as the [PR](https://github.com/tensorflow/tensorboard/pull/5348) is merged ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Let's leave this issue open since only `A` was resolved - at least `B` is probably also worth still addressing. If need be, we can always transfer this issue to the TensorBoard component."]}, {"number": 35960, "title": "Add python dev (alpha version) for testing and building in CI", "body": "Add python dev in CI can efficiently avoid the problem in next release. it it can make TensorFlow always support latest python stable release.", "comments": ["Can you expand please? Preferably also suggest a PR?", "@mihaimaruseac , sorry, I am currently not the staff of Google. I have no access to edit the CI configuration script. ", "Our CI is configured at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build (modulo some settings that pertain to internal tooling).\r\n\r\nIn any case, can you please expand on what python dev can bring? How should it be implemented? Examples of other repos having it? Anything can help.", "@mihaimaruseac , For example Tensorflow doesn't support python3.8 as soon as Python3.8 stable released. This is because there is still some trouble in python3.8. But before python 3.8, there is 3.8 rc release. Why don't use those release candidate for a reference to pretest the compatibility of python 3.8 stable.\r\n\r\nFor example for next 3.9 stable. Before that python release, there will be release candidate, Why don't we use those release candidate to avoid the situation like this (latest stable python not yet support).\r\n\r\n\r\nWe can name this CI case as python-next. And we don't have to let CI pass all test case in this script. It is just a reminder.\r\n\r\nWhat is your opinion?", "TF 2.1 doesn't support py3.8 because downstream dependencies didn't provide support for py3.8 before `r2.1` branch was cut.\r\n\r\nTF nightly will support py3.8 in the very near future, as now all downstream dependencies support py3.8.\r\n\r\nAdding a new CI build to test release candidates of our dependencies is neither useful nor feasible (we have around 100 downstream dependencies, compilers and libraries; adding CI build to test each one of them would spend too many CPU/GPU hours and heat the planet by too much).\r\n\r\nIn the end, remember that this is a community project. Anyone from the open source community can attempt to build against a future py3.9 release candidate and fill in bugs and send PRs to make TF work with py3.9 faster than Google engineers can do it by themselves alone.", "@mihaimaruseac  Oh, by mention downstream dependencies, it make sense now.\r\n\r\nThanks.", "Is anything happening on this?  TensorFlow fails to install on ___Python 3.9 final candidate 1___ and TensorFlow CI system has become so convoluted and nonstandard that it would take hours if not days for an outsider to figure out how to create a pull request to test this installation and discover where fixes are required.", "As long as TF's dependencies fail to build on newer Python versions we cannot upgrade.\r\n\r\nAlso, we don't create releases on release candidates of Python. Once Python 3.9 is released we will start working on releasing TF supporting that version of Python.\r\n\r\nWe do not have cycles to test alpha python releases.", "> We do not have cycles to test alpha python releases.\r\n\r\nYes, but final candidate != alpha.\r\n\r\nThe message in the last paragraph of https://github.com/tensorflow/tensorflow/issues/35960#issuecomment-577760808 was that the community should do the work required to ensure that TF is compatible with new Python releases so that Google does not need to.  That requires that the community has a CI test run that operates in _allow_failures_ mode so it knows where fixes are still needed.", "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/release contains all the release scripts.\r\n\r\nUsing the TF docker containers and using those scripts, members of the community could be able to build TF themselves. Alternatively, https://www.tensorflow.org/install/source\r\n\r\nIf any of these steps work in py3.8 (say) but don't in py3.9, please send fixing PRs.\r\n\r\nMy comment was that we don't have cycles to support a full CI build, though it might be possible via SIG Build (https://github.com/tensorflow/build/)"]}, {"number": 35944, "title": "tf.data.Dataset.from_generator converts input argument types implicitly instead of just forwarding", "body": "**System information**\r\n- custom code\r\n- Ubuntu 18.04.1 LTS\r\n- Thinkpad X240\r\n- TensorFlow installed via pip3\r\n- TensorFlow v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python 3.6.8\r\n- no CUDA/cuDNN\r\n\r\n**Describe the current behavior**\r\n\r\nThe code below generates the output `(1, 0)` for the first print when using the generator directly and the exception below when wrapping the generator using `tf.data.Dataset.from_generator`.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-35-36b15431cf1a> in <module>()\r\n     17 \r\n     18 print( next( movingWindow( data, window_size ) ) )\r\n---> 19 print( next( iter( dataset ) ) )\r\n\r\n/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in __next__(self)\r\n    620 \r\n    621   def __next__(self):  # For Python 3 compatibility\r\n--> 622     return self.next()\r\n    623 \r\n    624   def _next_internal(self):\r\n\r\n/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in next(self)\r\n    664     \"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\r\n    665     try:\r\n--> 666       return self._next_internal()\r\n    667     except errors.OutOfRangeError:\r\n    668       raise StopIteration\r\n\r\n/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    649             self._iterator_resource,\r\n    650             output_types=self._flat_output_types,\r\n--> 651             output_shapes=self._flat_output_shapes)\r\n    652 \r\n    653       try:\r\n\r\n/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py in iterator_get_next_sync(iterator, output_types, output_shapes, name)\r\n   2671       else:\r\n   2672         message = e.message\r\n-> 2673       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n   2674   # Add nodes to the TensorFlow graph.\r\n   2675   if not isinstance(output_types, (list, tuple)):\r\n\r\n/usr/lib/python3/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: TypeError: an integer is required\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/script_ops.py\", line 221, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 585, in generator_py_func\r\n    values = next(generator_state.get_iterator(iterator_id))\r\n\r\n  File \"<ipython-input-35-36b15431cf1a>\", line 3, in movingWindow\r\n    buffer = collections.deque( data[:window_size-1], maxlen = window_size )\r\n\r\nTypeError: an integer is required\r\n\r\n\r\n\t [[{{node PyFunc}}]] [Op:IteratorGetNextSync]\r\n```\r\n\r\nAs it turns out this is because the `window_size` given in the `args` argument is unexpectedly converted from `int` to `np.int32`, which can't be used interchangeably for slicing or the deque maxlen parameter.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe example should work without exception. Which means, from_generator should not change the types of any arguments given via the `args` parameter.\r\n\r\n**Code to reproduce the issue**\r\n\r\n\r\n```Python3\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport collections\r\n\r\ndef movingWindow( data, window_size ):\r\n    #window_size = int( window_size )\r\n    buffer = collections.deque( data[:window_size-1], maxlen = window_size )\r\n    for i, datum in enumerate( data[window_size-1:] ):\r\n        buffer.append( datum )\r\n        for b in buffer:\r\n            yield datum, b\r\n\r\nwindow_size = 2\r\ndata = np.arange( 10 )\r\n\r\ndataset = tf.data.Dataset.from_generator( \r\n    movingWindow,\r\n    args = ( data, window_size ),\r\n    output_types = ( np.int32, np.int32 )\r\n)\r\n\r\nprint( next( movingWindow( data, window_size ) ) )\r\nprint( next( iter( dataset ) ) )\r\n```", "comments": ["Issue replicating for given code in 2.0 and [tf-nightly](https://colab.sandbox.google.com/gist/oanush/1e681b438b022288058eca121014b041/35944.ipynb).TThanks!", "@mxmlnkn would you be able to check whether the same issue can be reproduced using `py_function` without tf.data. `tf.data.Dataset.from_generator` relies on `py_function` internally and I believe that this issue is inherent to `py_function`.", "> @mxmlnkn would you be able to check whether the same issue can be reproduced using `py_function` without tf.data. `tf.data.Dataset.from_generator` relies on `py_function` internally and I believe that this issue is inherent to `py_function`.\r\n\r\nI'm not able to check that as I don't know how it should work. `py_function` expects the function to return tf datatypes, I think. However, if I want to test the conversion of arguments to the generator function, then the return type would be a generator object: `tf.py_function( movingWindow, ( data, window_size ), ( np.int32, np.int32 ) )` is what I tried naively and gives a long backtrace.\r\n\r\nAlso, from what I see `from_generator` uses [`numpy_function`](https://github.com/tensorflow/tensorflow/blob/41a09aa33dece4d8c051983f0af6428e9d1d593c/tensorflow/python/data/ops/dataset_ops.py#L754) not `py_function`.\r\n\r\nLooking at the source code description and again at the documentation, the behavior actually seems to be at least known and maybe even wanted although I don't understand why:\r\n> args: (Optional.) A tuple of tf.Tensor objects that will be evaluated and passed to generator as NumPy-array arguments.", "Issue persists in [TF v2.2](https://colab.research.google.com/gist/amahendrakar/5774003c375fe71b6a690cb00bd18b47/35944.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/0f979ce994bea24bcc30c1718bebe7b3/35944-tf-nightly.ipynb#scrollTo=4O4xzQAJQ9WD) i.e. 2.3.0-dev20200527. Please find the attached gist. Thanks! ", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/8e7162698d0b194c6a4dc85934c134c8/35650.ipynb)."]}, {"number": 35889, "title": "STM32F7 Hello World example fails for arm_cmplx_mag_squared_q10p6.c", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \"Ubuntu\"\r\nVERSION=\"18.04.3 LTS (Bionic Beaver)\"\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): current\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm Mbed OS - STM32F7\r\n\r\n**Describe the problem**\r\nWhen running make for \"generate_hello_world_mbed_project\", a failure is seen and it stops building. \r\nRef: [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world](url)\r\nEx:\r\n`$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project`\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n```\r\n$ git clone --recursive https://github.com/tensorflow/tensorflow.git\r\n\r\n$ cd tensorflow/\r\n\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project\r\n```\r\n\r\nError seen:\r\n\r\n```\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853d75de2af87622ad293ba\" tensorflow/lite/micro/tools/make/downloads/gemmlowp  \r\ndownloading https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz\" \"02c64880acb89dbd57eebacfd67200d8\" tensorflow/lite/micro/tools/make/downloads/flatbuffers  \r\ndownloading https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/ARM-software/CMSIS_5/archive/d76d5e3acb87cf089daf50b31f991026149ecb6c.zip\" \"866f79cfb86f7aee29a320aeda530aca\" tensorflow/lite/micro/tools/make/downloads/cmsis  \r\ndownloading https://github.com/ARM-software/CMSIS_5/archive/d76d5e3acb87cf089daf50b31f991026149ecb6c.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://developer.arm.com/-/media/Files/downloads/gnu-rm/7-2018q2/gcc-arm-none-eabi-7-2018-q2-update-linux.tar.bz2\" \"299ebd3f1c2c90930d28ab82e5d8d6c0\" tensorflow/lite/micro/tools/make/downloads/gcc_embedded  \r\ndownloading https://developer.arm.com/-/media/Files/downloads/gnu-rm/7-2018q2/gcc-arm-none-eabi-7-2018-q2-update-linux.tar.bz2\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_01_13.zip\" \"8a7d2c70325f53136faea6dde517b8cc\" tensorflow/lite/micro/tools/make/downloads/person_model_int8  \r\ndownloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_01_13.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/mborgerding/kissfft/archive/v130.zip\" \"438ba1fef5783cc5f5f201395cc477ca\" tensorflow/lite/micro/tools/make/downloads/kissfft patch_kissfft \r\ndownloading https://github.com/mborgerding/kissfft/archive/v130.zip\r\nFinished patching kissfft\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2019_11_21.zip\" \"fe2934bd0788f1dcc7af3f0a954542ab\" tensorflow/lite/micro/tools/make/downloads/person_model_grayscale  \r\ndownloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2019_11_21.zip\r\nmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/tensorflow/lite/micro/tools/make/downloads/CMSIS_ext/arm_cmplx_mag_squared_q10p6.c', needed by 'generate_hello_world_mbed_project'.  Stop.\r\n\r\n```", "comments": ["For anyone seeing similar error messages, try using `cmsis-nn` instead of `CMSIS`. This may not be the underlying cause of #35889, but it's the correct way to specify the CMSIS-NN tag.\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"cmsis-nn disco_f746ng\" generate_hello_world_mbed_project\r\n```", "I am also facing same issue", "Also ran into this issue, using cmsis-nn did not work for me, as it appears to create multiple definition errors with the demo. \r\n\r\nHowever using changes in [36444](https://github.com/tensorflow/tensorflow/pull/36444) with the additional step of removing [line 20](https://github.com/tensorflow/tensorflow/pull/36444/files#diff-061bd90e5d8a39bb7cf39065bb5144e6)\r\n`  third_party/CMSIS_ext/README.md \\`\r\n\r\nsolved the issue", "@jomoengineer  It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.4.1 or 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I attempted with Tensorflow 2.5.0 and I get the following error:\r\n```\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=disco_f746ng OPTIMIZED_KERNEL_DIR=cmsis_nn generate_hello_world_mbed_project\r\n\r\n\r\nKERNEL_DIR=cmsis_nn generate_hello_world_mbed_project\r\n--2021-08-08 15:53:17--  http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip\r\nResolving mirror.tensorflow.org (mirror.tensorflow.org)... 216.58.194.208, 2607:f8b0:4005:805::2010\r\nConnecting to mirror.tensorflow.org (mirror.tensorflow.org)|216.58.194.208|:80... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 1760478 (1.7M) [application/zip]\r\nSaving to: \u2018/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip\u2019\r\n\r\n/tmp/dca12522a9f9e3 100%[===================>]   1.68M  4.26MB/s    in 0.4s    \r\n\r\n2021-08-08 15:53:18 (4.26 MB/s) - \u2018/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip\u2019 saved [1760478/1760478]\r\n\r\nCloning into 'tensorflow/lite/micro/tools/make/downloads/pigweed'...\r\nremote: Sending approximately 16.66 MiB ...\r\nremote: Counting objects: 23, done\r\nremote: Finding sources: 100% (23/23)\r\nremote: Total 26938 (delta 12530), reused 26929 (delta 12530)\r\nReceiving objects: 100% (26938/26938), 16.53 MiB | 6.14 MiB/s, done.\r\nResolving deltas: 100% (12530/12530), done.\r\nNote: checking out '47268dff45019863e20438ca3746c6c62df6ef09'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\n\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n\r\n  git checkout -b <new-branch-name>\r\n\r\nHEAD is now at 47268dff pw_hdlc_lite: Client I/O improvements\r\ntensorflow/lite/micro/tools/make/Makefile:600: tensorflow/lite/micro/tools/make/targets/disco_f746ng_makefile.inc: No such file or directory\r\nURL transformed to HTTPS due to an HSTS policy\r\n--2021-08-08 15:53:25--  https://github.com/ARM-software/CMSIS_5/archive/0d7e4fa7131241a17e23dfae18140e0b2e77728f.zip\r\nResolving github.com (github.com)... 192.30.255.113\r\nConnecting to github.com (github.com)|192.30.255.113|:443... connected.\r\nHTTP request sent, awaiting response... 302 Found\r\nLocation: https://codeload.github.com/ARM-software/CMSIS_5/zip/0d7e4fa7131241a17e23dfae18140e0b2e77728f [following]\r\n--2021-08-08 15:53:25--  https://codeload.github.com/ARM-software/CMSIS_5/zip/0d7e4fa7131241a17e23dfae18140e0b2e77728f\r\nResolving codeload.github.com (codeload.github.com)... 192.30.255.120\r\nConnecting to codeload.github.com (codeload.github.com)|192.30.255.120|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: unspecified [application/zip]\r\nSaving to: \u2018/tmp/0d7e4fa7131241a17e23dfae18140e0b2e77728f.zip\u2019\r\n\r\n/tmp/0d7e4fa7131241     [ <=>                ]  48.04M  6.95MB/s    in 7.2s    \r\n\r\n2021-08-08 15:53:32 (6.69 MB/s) - \u2018/tmp/0d7e4fa7131241a17e23dfae18140e0b2e77728f.zip\u2019 saved [50371309]\r\n\r\nmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/targets/disco_f746ng_makefile.inc'.  Stop.\r\n```\r\n\r\nThis is similar to an older issue I filed:\r\n[https://github.com/tensorflow/tensorflow/issues/34219](url)"]}, {"number": 35816, "title": "Batching images first or formatting them first?", "body": "https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_lite/tflite_c04_exercise_convert_model_to_tflite_solution.ipynb\r\n\r\nUnder \"Create a Dataset from Images and Labels\", we have this code\r\n\r\n`train_batches=train_examples.cache().shuffle(num_examples//4).batch(BATCH_SIZE).map(format_example).prefetch(1)`\r\n\r\nand similar for the validation and test examples...\r\n\r\nIs this a right practise?? \r\nShouldn't we first format the raw images and then batch them together rather than opposite way?", "comments": ["@ManishAradwad , I don't think that it is a problem to batch together first and pre-processing later. These two things are independent and if the data is well shuffled then we can batch them before. Please state any disadvantages of this implementation?", "Hey @ashutosh1919 , sorry for the late reply...\r\n\r\nI actually came across this issue while doing the exercise in the dl course. I've tried replicating the problem in following colab: https://github.com/ManishAradwad/ML_Stuff/blob/master/Courses/Udacity:%20Intro%20to%20Tensorflow%20for%20DeepLearning/Lesson5:%20TransferLearning/FlowersWithTransferLearning.ipynb\r\n\r\nIf we batch the flower images first and then map them, then we get following error while training-\r\n\r\n`InvalidArgumentError: 2 root error(s) found.\r\n(0) Invalid argument:  Cannot batch tensors with different shapes in component 0. First element had shape [213,320,3] and element 1 had shape [240,240,3].\r\n\t [[node IteratorGetNext (defined at <ipython-input-40-b1d077d96724>:9) ]]\r\n(1) Invalid argument:  Cannot batch tensors with different shapes in component 0. First element had shape [213,320,3] and element 1 had shape [240,240,3].\r\n\t [[node IteratorGetNext (defined at <ipython-input-40-b1d077d96724>:9) ]]\r\n\t [[IteratorGetNext/_2]]`\r\n\r\n`0 successful operations.\r\n0 derived errors ignored. [Op:__inference_distributed_function_147782]`\r\n\r\n`Function call stack:\r\ndistributed_function -> distributed_function`\r\n\r\nI think it has something to do with the number of color channels since the code for MNIST works flawlessly."]}, {"number": 35689, "title": "TFLite C/C++ library header file installation", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n\r\n**Describe the problem**\r\nI need just a TFLite header file without the whole TensorFlow source code. Is there any method to install the TFLite header?\r\n\r\nI found a similar issue at #3536. But, `//tensorflow:install_headers` Bazel target does not install a TFLite header file.\r\n", "comments": ["I have the same issue here. Could tf maintainers team nicely provides a way to export tflite headers?", "@jdduke any idea on this?", "Thanks for flagging this issue. We've been working on something like this, but still need to remove the flatbuffer inclusion from our public C++ APIs. We'll try to prioritize this for early Q2.", "@jdduke any update on this?", "Sadly, no immediate update. At the moment, we're steering developers toward our C API if they want both a prebuilt library and a compact set of headers. The C API and headers are actually included in the latest release artifacts that we publish to Bintray: https://bintray.com/google/tensorflow/tensorflow-lite. We've added some guidance on this approach @ https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/android.md#build-android-app-using-c.\r\n\r\nIn Q3, we'll be investing in full CMake support so that we can better integrate into developer toolchains. This will also include some cleanup for the C++ API to minimize external dependencies. Thanks for your patience on this. ", "Thanks for the update, @jdduke!", "@jdduke the C API appears insufficient for running inference on the edge tpu, will that also be a priority?", "> @jdduke the C API appears insufficient for running inference on the edge tpu, will that also be a priority?\r\n\r\nThanks for flagging the issue. You should be able to inject custom ops (`edgetpu::kCustomOp`) in the C API from the `c_api_experimental.h` header, `TfLiteInterpreterOptionsAddCustomOp`.", "@Namburger, can you offer some help with the suggestion above? Can you offer an example?\r\n\r\nThere is no documentation for:\r\n```\r\nvoid TfLiteInterpreterOptionsAddCustomOp(TfLiteInterpreterOptions* options,        \r\n                                         const char* name,                      \r\n                                         const TfLiteRegistration* registration,\r\n                                         int min_version, int max_version) {       \r\n  options->op_resolver.AddCustom(name, registration, min_version, max_version); \r\n}\r\n```\r\nCan I use `edgetpu_c.h`? It looks like I must use `edgetpu.h`. What do `min_version` and `max_version` correspond to? What to set these to?\r\n\r\nWhen I try:\r\n```\r\n  //                                                                            \r\n  // Create model and interpreter options:                                      \r\n  //                                                                            \r\n  model_ = TfLiteModelCreateFromFile(params_.model.c_str());                    \r\n  options_ = TfLiteInterpreterOptionsCreate();                                  \r\n                                                                                \r\n  //                                                                            \r\n  // Create the interpreter:                                                    \r\n  //                                                                            \r\n  interpreter_ = TfLiteInterpreterCreate(model_, options_);                     \r\n                                                                                \r\n  //                                                                            \r\n  // Modify interpreter with the delegate:                                      \r\n  //                                                                            \r\n  LOG(INFO) << \"Using type \" << (device.type ? \"USB\" : \"PCI\") << \" at \" << device.path;                                                     \r\n                                                                                \r\n  delegate_ = edgetpu_create_delegate(device.type, device.path, nullptr, 0);    \r\n  TfLiteInterpreterOptionsAddDelegate(options_, delegate_);                     \r\n  TfLiteInterpreterOptionsAddCustomOp(options_,                                 \r\n                                      edgetpu::kCustomOp,                       \r\n                                      edgetpu::RegisterCustomOp(), 10, 13);\r\n```\r\nI get the following:\r\n```\r\nERROR: Encountered unresolved custom op: edgetpu-custom-op.\r\nERROR: Node number 0 (edgetpu-custom-op) failed to prepare.\r\n```", "I got it working, thanks for the pointers. My problem was I had to switch to 2.1 and change the order of the arguments. I still have no idea what min and max version are.\r\n\r\nWorking code with TF 2.1:\r\n```\r\n  //                                                                            \r\n  // Create model and interpreter options:                                      \r\n  //                                                                            \r\n  model_ = TfLiteModelCreateFromFile(params_.model.c_str());                    \r\n  options_ = TfLiteInterpreterOptionsCreate();                                  \r\n                                                                                \r\n  //                                                                            \r\n  // Register Edge TPU custom ops:                                              \r\n  //                                                                            \r\n  TfLiteInterpreterOptionsAddCustomOp(options_,                                 \r\n                                      edgetpu::kCustomOp,                       \r\n                                      edgetpu::RegisterCustomOp(), 10, 13);     \r\n  //                                                                            \r\n  // Create the delegate:                                                       \r\n  //                                                                            \r\n  delegate_ = edgetpu_create_delegate(device.type, device.path, nullptr, 0);    \r\n  TfLiteInterpreterOptionsAddDelegate(options_, delegate_);                     \r\n                                                                                \r\n  //                                                                            \r\n  // Create the interpreter:                                                    \r\n  //                                                                            \r\n  interpreter_ = TfLiteInterpreterCreate(model_, options_);\r\n```\r\nI high-jacked this issue, feel free to delete my comments.", ">  I still have no idea what min and max version are.\r\n\r\nThanks for flagging, will add some improved comments to the header. They correspond to the C++ MutableOpResolver API, but for custom ops you can generally safely just set the min/max to \"1\".\r\n\r\n>  delegate_ = edgetpu_create_delegate(device.type, device.path, nullptr, 0);    \r\n\r\nDo you need both the delegate and the CustomOp? Does it work to add just the delegate?", "@jdduke turns out you are right, only the delegate is needed. I must have had the call order wrong before and missed that. Thanks for the help and the clarification on min and max.", "@llschloesser @jdduke do you have any advice on how I can register the RandomStandardNormal custom operator prototype in the C api? It was implemented in 2.4.0-rc0 \r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/11823c6179a5e2ca6ac4f8480aa00c42c772885e#\r\n\r\nI'm not sure how to use it because the registration is defined in a C++ namespace and I don't know how to use `tflite::ops::custom::Register_RANDOM_STANDARD_NORMAL` in C.\r\n\r\nApologies for hijacking the issue", "We're hoping to make the RANDOM_STANDARD_NORML op a builtin op for the next 2.5 release.", "@jdduke would you mind checking this thread on the TFLite mailing list? https://groups.google.com/a/tensorflow.org/g/tflite/c/1zfZtRtVfpY\r\n\r\nI feel super close to getting the Register_RANDOM_STANDARD_NORMAL working from the C API, but I'm having trouble exporting the symbol to call it.", "@jdduke Hi~ Can I export the header file of libTensorFlowLite.so now?\r\nThe version I am using is v2.5.0.", "@lilinxiong we don't have the feature of exporting C++ headers yet."]}, {"number": 35685, "title": "Define variable shape at restore/load, allow direct restoring of variables prior to calling __build__ (non-lazy variable loading from checkpoint)", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes (if necessary)\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nTypically for RL, I use tensorflow in combination with Ray, where a remote agent collects data using a policy model, and the episodes are fed to a tf-agents EpisodicReplayBuffer in a second train-loop process. A tf.dataset is used to feed the main model training on this thread, which is then serialized via get_weights and sent back to the remote actor via a ray remote call to set_weights.\r\n\r\nCurrently I use tf.Checkpoint.restore.expect_partial() to restore my subclassed tf.keras.Model. In practice, this is extremely convoluted on initialization:\r\n* I make a forward pass through my remote Agent on random policy (since I have not initialized the weights), retrieve this forward pass, and send it to the train loop\r\n* I make a second forward pass through my train loop (again, randomly initialized network still), this has the side effect of initializing the network, allowing my checkpoint to restore the variables in this model\r\n* I call get_weights on the train_loop model, and pass the weights to my remote model via set_weights.\r\n* I can now collect a full replay buffer with my Agent on policy\r\n* I can now run the train loop\r\n\r\nMaybe I'm missing something, but most of my labmates are also confused by what the best practice for this currently is.\r\n\r\n**Will this change the current api? How?**\r\nI'm not sure what the best approach is. Some ideas for discussion? \r\n\r\n* Allow set_weights to define the shapes and names of tf.Variables, such that calls to __build__ in the tf.keras.Model on first run will align with the initialized tf.Variables.\r\n* Allow tf.keras.model.load_weights() to set the weights immediately rather than waiting for first call to build()\r\n\r\n**Who will benefit with this feature?**\r\nRL community \r\n**Any Other info.**\r\n", "comments": ["This is partly a question for the TF-Agents repo, and partly for Keras (CC @fchollet).  On the TF-Agents side, it's true you need to do a forward pass to create the appropriate Variables.  The easiest way to do this is to take your TF-Agents network objects, and call net.create_variables() on each of them (you may want to pass training=True to this call in case you have batch norm or dropout).\r\n\r\nThat takes care of feeding appropriate random inputs.  If that doesn't work, you should file a bug in tensorflow/agents instead of this repo.\r\n\r\nRegarding Keras models, we rely on the lower level Keras Network class to represent computation in TF-Agents, so I can't help much there.  I hope @fchollet can discuss your ideas on variable initialization.", "@ebrevdo Thanks for the comments! Just to clarify, the *only* thing I'm using from tf-agents is the EpisodicReplayBuffer class, I'm constructing my own tf.keras model using the low-level apis."]}, {"number": 35677, "title": "Explanation regarding `seed` parameter", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/examples/blob/d631c0545dac90c6390da76ed8df7c4f6a2a25bc/courses/udacity_intro_to_tensorflow_for_deep_learning/l08c01_common_patterns.ipynb#L307\r\n\r\n`def white_noise(time, noise_level=1, seed=None):`\r\n\r\n## Description of issue (what needs changing):\r\n\r\nI think, we should add explanation of `seed` parameter here since it's quite an important one.\r\n\r\n### Clear description\r\n\r\nSome explanation about how `seed` affects generation of random numbers every time along with links for reference can be added.\r\n\r\n### Submit a pull request?\r\n\r\nYes", "comments": []}, {"number": 35632, "title": "TFLite GPU execution failed", "body": "I trying to run one model on TFite mobile GPU using opencl and getting below error:\r\n\r\n> INFO: Initialized TensorFlow Lite runtime.                                                                                           \r\n> INFO: Created TensorFlow Lite delegate for GPU.                                                                                      \r\n> ERROR: Next operations are not supported by GPU delegate:                                                                            \r\n> SPLIT_V: Operation is not supported.                                                                                                 \r\n> First 294 operations will run on the GPU, and the remaining 40 on the CPU.                                                           \r\n> INFO: Initialized OpenCL-based API.                                                                                                  \r\n> Applied GPU delegate.                                                                                                                \r\n> Initialized session in 107237ms.                                                                                                     \r\n> Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.                         \r\n> ERROR: TfLiteGpuDelegate Invoke: Failed to read data from GPU (clEnqueueReadBuffer) - Execution status error for events in wait list \r\n> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.                                                                       \r\n>                                                                                                                                      \r\n> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list\r\n> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.                                                                                                            \r\n>                                                                                                                                      \r\n> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list\r\n> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n> \r\n> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list\r\n> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n> \r\n> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list\r\n> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n> \r\n> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list\r\n> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n> \r\n> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list\r\n> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n> \r\n> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list\r\n> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n> \r\n> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list\r\n> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n> \r\n> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list\r\n> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list\r\n> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n> \r\n> count=138 first=73906 curr=1840 min=1414 max=73906 avg=3142.52 std=6057\r\n> \r\n\r\nPlease help me to resolve this issue\r\n", "comments": ["Can you specify more details about what device you are trying this on? are you using a standard model?", "Is it possible to share the model?", "@aselle @impjdi I am using a standard model. My model contain Conv2D=>InstanceNorm=>relu. The problem is coming from instancenorm implementation.\r\n\r\nInstanceNorm is implemented with combination of operators since Tensorflow doesn't have a single operator for this.\r\nI have used Matmul to implement the reduce_mean to calculate Mean and Variance part of InstanceNorm. \r\n\r\nI am running on mobile phone using tflite GPU. Tflite CPU its working fine", "I am also sharing the part of the model with I am facing the problem. Please help me to find the reason for the issue and any alternative solution?\r\n\r\n[conv2d-instancenorm-relu.zip](https://github.com/tensorflow/tensorflow/files/4034657/conv2d-instancenorm-relu.zip)\r\n\r\n\r\ncommand tried is:\r\n`adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/conv2d-instancenorm-relu.tflite --num_threads=9 --num_runs=5 --use_gpu=true`", "Thank you for the network.  I forwarded the issue to our OpenCL specialist.", "He has confirmed that https://github.com/tensorflow/tensorflow/files/4034657/conv2d-instancenorm-relu.zip works fine on his device.  What kind of device are you using?", "@impjdi Thanks for your reply.\r\nI am trying to run this on mobile device Honor20. Could you also please let me know in which environment it works fine?", "Same on LG Nexus 5X\r\n\r\nRunning posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite downloaded from the tensorflow lite webpage just days ago.\r\nWith CPU it works pefectly fine\r\n\r\n```\r\n java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: OpenCL library not loaded - dlopen failed: library \"libOpenCL-pixel.so\" not found\r\n    Falling back to OpenGL\r\n    TfLiteGpuDelegate Invoke: ToTensorConverter: input data size does not match expected size.\r\n    Node number 31 (TfLiteGpuDelegateV2) failed to invoke.\r\n    \r\n    TfLiteGpuDelegate Invoke: ToTensorConverter: input data size does not match expected size.\r\n    Node number 31 (TfLiteGpuDelegateV2) failed to invoke.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:171)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n```", "@joyalbin Apologies, forgot to follow up on this.  He usually tests it on a variety of phones.  I don't think we have Honor20 in particular but definitely have Kirin devices.  Is this now resolved or shall I ping him again?", "@nerdaliciousCH the way I read the message is that you are supplying an external SSBO for inference, but it has wrong dimensions, i.e. not 257x257x3.", "> @nerdaliciousCH the way I read the message is that you are supplying an external SSBO for inference, but it has wrong dimensions, i.e. not 257x257x3.\r\n\r\nSo I use Android with Java and used the GpuDelegate provided by the tensorflow2 packages. When I don't add the delegate to the Intepreter and run on CPU alone (i.e. nlt providing any executor delegates), it works well. I have no code that is adapted to the device type besides when I create the tensorflow interpreter.\r\n\r\nDo I have to create the input buffers differently when running on GPU? What struck me as odd is the OpenCL related part of the Exception. Could the size error be a consequence of that somewhere downstream the call stack?", "@nerdaliciousCH \r\n\r\n> Do I have to create the input buffers differently when running on GPU?\r\n\r\nNo, you don't have to create the input buffers differently.  In fact, it should be just the same.\r\n\r\n> What struck me as odd is the OpenCL related part of the Exception. \r\n\r\nDon't worry about OpenCL.  It's just a \"FYI\" message.\r\n\r\n> Could the size error be a consequence of that somewhere downstream the call stack?\r\n\r\nMaybe, but it's hard to tell without seeing the code.  Are you calling interpreter->ResizeInputTensors at one point?", "Hi. I also have a basic CNN model that is crashing with a similar error only when using GPU delegate (Runs fine on CPU and with NNAPI Delegate). \r\n\r\nStrangely it only crashes on the Samsung S9 (Android 10) with a Exynos 9810. It doesn't crash on any other device that I have tested (Kirin and Snapdragon). I am using the Java TFLite API.\r\n\r\nHere's a link to the model: [link](https://drive.google.com/file/d/1HnclW3Qh_LOkIjMrd5gd9VfShITMbhwL/view?usp=sharing)\r\n\r\nHere's the error message:\r\n```\r\n2020-03-04 09:34:06.950 16795-16795/com.test.app E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.test.app, PID: 16795\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: TfLiteGpuDelegate Invoke: Failed to read data from GPU (clEnqueueReadBuffer) - Execution status error for events in wait list\r\n    Node number 8 (TfLiteGpuDelegateV2) failed to invoke.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:154)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:314)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:275)\r\n```", "@impjdi \r\nNo resizing tensors in my code :(", "@bilalsoomro Please sync to the latest git commit and try it out.  A fix should have been submitted.", "Hi @impjdi, the latest build has fixed the issue.", "Hello, have the same issue. All works fine on different devices except of Samsung. Model S730FM.\r\nExynos 7 Octa (Exynos 7870) \r\nGPU - Mali-T830\r\n\r\nTried on different gpus: Adreno 615, Adreno 508, Adreno 509, Mali - G72, Adreno 540. Works well.\r\n\r\nTensorflow 2.3.0 \r\n\r\n` something went wrong: Internal error: Failed to run on the given Interpreter: Following operations are not supported by GPU delegate:\r\n    SUM: Operation is not supported.\r\n    226 operations will run on the GPU, and the remaining 103 operations will run on the CPU.\r\n    TfLiteGpuDelegate Invoke: Failed to clEnqueueNDRangeKernel - Out of resources\r\n    Node number 329 (TfLiteGpuDelegateV2) failed to invoke.`\r\n\r\nWhen i run on cpu without gpu delegate all code works well.\r\n\r\nIf anyone have idea how to handle it? Maybe there is some trick how to understand that gpu not support operations without running inference? If there a possibility check gpu compatability ? \r\n\r\nAccording to this articles, Mali-T830 does'nt have support of opencl 2.0. Looks like possible reason of this behaviour.\r\nhttps://en.wikipedia.org/wiki/Mali_(GPU)\r\nhttps://en.wikipedia.org/wiki/Adreno\r\n\r\n", "\r\n\r\n> Same on LG Nexus 5X\r\n> \r\n> Running posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite downloaded from the tensorflow lite webpage just days ago.\r\n> With CPU it works pefectly fine\r\n> \r\n> ```\r\n>  java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: OpenCL library not loaded - dlopen failed: library \"libOpenCL-pixel.so\" not found\r\n>     Falling back to OpenGL\r\n>     TfLiteGpuDelegate Invoke: ToTensorConverter: input data size does not match expected size.\r\n>     Node number 31 (TfLiteGpuDelegateV2) failed to invoke.\r\n>     \r\n>     TfLiteGpuDelegate Invoke: ToTensorConverter: input data size does not match expected size.\r\n>     Node number 31 (TfLiteGpuDelegateV2) failed to invoke.\r\n>     \r\n>         at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n>         at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:171)\r\n>         at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n> ```\r\n\r\njava.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: OpenCL library not loaded - dlopen failed: library \"libOpenCL-pixel.so\" not found Falling back to OpenGL TfLiteGpuDelegate Invoke: ToTensorConverter: input data size does not match expected size. Node number 237 (TfLiteGpuDelegateV2) failed to invoke. \r\n\r\nI'm facing the same error on Pixel 2 XL (Android 11).\r\n\r\nActually, it was working fine with Android 10 on the same device but getting the same mentioned issue when I updated it to Android 11.\r\n\r\nI'm using the following libraries for tensoreflow:\r\norg.tensorflow:tensorflow-lite:2.0.0\r\norg.tensorflow:tensorflow-lite-gpu:2.0.0\r\n\r\nIs anyone got success to get rid of this issue? @nerdaliciousCH  @impjdi ", "Hi guys, I'm getting the same error in a new model that I'm testing in [Huawei P10 Lite LX1A](https://www.gsmarena.com/huawei_p10_lite-8598.php). In Samsung S9 and Xiaomi Redmi Note 8 it worked fine.\r\n\r\nThe P10 Lite has a **Mali-T830** GPU, like @OleksandrGrument mentioned above. I'm also running with `Tensorflow 2.3.0`", "This thread is diluted with multiple reports.  Can I get a TFLite model that reproduces this?  We'll see whether we have a Mali-T830 device at hand.  (FYI Due to COVID-19 closed offices, we don't have access to the devices anymore and may not be able to try out that particular (or a similar) device.)", "@impjdi , [here it is](https://we.tl/t-GOEGyHTmqS). Thank you in advance", "@tgpsantos I've verified that your model works fine on a Pixel 4 OpenGL and OpenCL.  I'll ping the team whether anyone can reproduce this on any of the devices they have.", "@tgpsantos A teammate ran your model on a Huawei P9 (Mali T880MP4) and it worked fine, matching the CPU output.  Can you sync to the latest master branch and double-check?", "Updated to from 2.3.0 to 2.4.0 and problem was solved. :) Thank you @impjdi and the team.\r\n\r\nPS: I had no idea there was a new tflite release. Are they in-sync with python TF releases? If not, where can I learn about new releases?", "TFLite GPU team is not really familiar with the different TF releases really; we work with the master branch's latest code only.", "> > Same on LG Nexus 5X\r\n> > Running posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite downloaded from the tensorflow lite webpage just days ago.\r\n> > With CPU it works pefectly fine\r\n> > ```\r\n> >  java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: OpenCL library not loaded - dlopen failed: library \"libOpenCL-pixel.so\" not found\r\n> >     Falling back to OpenGL\r\n> >     TfLiteGpuDelegate Invoke: ToTensorConverter: input data size does not match expected size.\r\n> >     Node number 31 (TfLiteGpuDelegateV2) failed to invoke.\r\n> >     \r\n> >     TfLiteGpuDelegate Invoke: ToTensorConverter: input data size does not match expected size.\r\n> >     Node number 31 (TfLiteGpuDelegateV2) failed to invoke.\r\n> >     \r\n> >         at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n> >         at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:171)\r\n> >         at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n> > ```\r\n> \r\n> java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: OpenCL library not loaded - dlopen failed: library \"libOpenCL-pixel.so\" not found Falling back to OpenGL TfLiteGpuDelegate Invoke: ToTensorConverter: input data size does not match expected size. Node number 237 (TfLiteGpuDelegateV2) failed to invoke.\r\n> \r\n> I'm facing the same error on Pixel 2 XL (Android 11).\r\n> \r\n> Actually, it was working fine with Android 10 on the same device but getting the same mentioned issue when I updated it to Android 11.\r\n> \r\n> I'm using the following libraries for tensoreflow:\r\n> org.tensorflow:tensorflow-lite:2.0.0\r\n> org.tensorflow:tensorflow-lite-gpu:2.0.0\r\n> \r\n> Is anyone got success to get rid of this issue? @nerdaliciousCH @impjdi\r\n\r\n\r\nHi there,\r\n\r\nI was experiencing the same issue with the following configuration:\r\n\r\nOnePlus One\r\nAdreno 330\r\nAndroid 10\r\norg.tensorflow:tensorflow-lite:2.4.0\r\norg.tensorflow:tensorflow-lite-gpu:2.4.0\r\n\r\nI downgrade to 2.1.0 and worked", "I have the same error, but cannot update to newer TF version as I've made modifications and aren't compatible to the newer branch.\r\n\r\nCan you share the difference between the two branches (2.3.0 & 2.4.0) that fix this error, maybe I could incorporate them in my version?\r\n\r\nthnx"]}, {"number": 35489, "title": "`files_io.get_matching_files` fails for valid filenames that contain globs", "body": "the function `files_io.get_matching_files` says it takes a \"filepath\", but actually it takes a glob\r\n\r\nthis means that if you save your checkpoints to a folder like `x=[abc]`, then you can't load the previous checkpoint using something like:\r\n\r\n```\r\ndef load_checkpoint(sess, checkpoint_path):\r\n  saver = tf.train.Saver(tf.global_variables())\r\n  ckpt = tf.train.get_checkpoint_state(checkpoint_path)\r\n  tf.logging.info('Loading model %s.', ckpt.model_checkpoint_path)\r\n  saver.restore(sess, ckpt.model_checkpoint_path)\r\n```\r\n\r\nwhere `checkpoint_path=\"./logs/x=[abc]\"`.", "comments": ["@silky,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!", "@silky,\r\nAny updates regarding this issue? Thanks!", "@amahendrakar \r\n\r\ncopying the example from here: https://www.tensorflow.org/tutorials/keras/save_and_load and just changing the checkpoint folder path\r\n\r\n```\r\n# glob_doom.py\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nprint(f\"version = {tf.version.VERSION}\")\r\n\r\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\r\n\r\ntrain_labels = train_labels[:1000]\r\ntest_labels = test_labels[:1000]\r\n\r\ntrain_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\r\ntest_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0\r\n\r\n# Define a simple sequential model\r\ndef create_model():\r\n  model = tf.keras.models.Sequential([\r\n    keras.layers.Dense(512, activation='relu', input_shape=(784,)),\r\n    keras.layers.Dropout(0.2),\r\n    keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n\r\n  model.compile(optimizer='adam',\r\n                loss='sparse_categorical_crossentropy',\r\n                metrics=['accuracy'])\r\n\r\n  return model\r\n\r\n# Create a basic model instance\r\nmodel = create_model()\r\n\r\n# Display the model's architecture\r\nmodel.summary()\r\n\r\ncheckpoint_path = \"logs/x=[1,2]/cp.ckpt\"\r\ncheckpoint_dir = os.path.dirname(checkpoint_path)\r\n\r\n# Create a callback that saves the model's weights\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\r\n                                                 save_weights_only=True,\r\n                                                 verbose=1)\r\n\r\n# Train the model with the new callback\r\nmodel.fit(train_images, \r\n          train_labels,  \r\n          epochs=1,\r\n          validation_data=(test_images,test_labels),\r\n          callbacks=[cp_callback])  # Pass callback to training\r\n\r\n\r\n# Loads the weights\r\nmodel.load_weights(checkpoint_path)\r\n\r\n# Re-evaluate the model\r\nloss,acc = model.evaluate(test_images,  test_labels, verbose=2)\r\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\r\n```\r\n\r\nresulting error (note: hiding some tensorflow info messages that aren't relevant):\r\n\r\n```\r\n(tf-issue) 09:18 AM noon \u2208 tf-issue TF_CPP_MIN_LOG_LEVEL=3 python glob_doom.py\r\nversion = 2.1.0\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense (Dense)                (None, 512)               401920    \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 512)               0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 10)                5130      \r\n=================================================================\r\nTotal params: 407,050\r\nTrainable params: 407,050\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nTrain on 1000 samples, validate on 1000 samples\r\n 928/1000 [==========================>...] - ETA: 0s - loss: 1.1697 - accuracy: 0.6681\r\nEpoch 00001: saving model to logs/x=[1,2]/cp.ckpt\r\n1000/1000 [==============================] - 1s 614us/sample - loss: 1.1252 - accuracy: 0.6810 - val_loss: 0.6657 - val_accuracy: 0.8100\r\nTraceback (most recent call last):\r\n  File \"/home/noon/tools/miniconda3/envs/tf-issue/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py\", line 95, in NewCheckpointReader\r\n    return CheckpointReader(compat.as_bytes(filepattern))\r\nRuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for logs/x=[1,2]/cp.ckpt\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"glob_doom.py\", line 55, in <module>\r\n    model.load_weights(checkpoint_path)\r\n  File \"/home/noon/tools/miniconda3/envs/tf-issue/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 234, in load_weights\r\n    return super(Model, self).load_weights(filepath, by_name, skip_mismatch)\r\n  File \"/home/noon/tools/miniconda3/envs/tf-issue/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1187, in load_weights\r\n    py_checkpoint_reader.NewCheckpointReader(filepath)\r\n  File \"/home/noon/tools/miniconda3/envs/tf-issue/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py\", line 99, in NewCheckpointReader\r\n    error_translator(e)\r\n  File \"/home/noon/tools/miniconda3/envs/tf-issue/lib/python3.6/site-packages/tensorflow_core/python/training/py_checkpoint_reader.py\", line 35, in error_translator\r\n    raise errors_impl.NotFoundError(None, None, error_message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for logs/x=[1,2]/cp.ckpt\r\n```", "Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/b1d0c9a5e313790ec442199aed32e8a3/35489.ipynb). Thanks!", "Do you have a fix in mind? I doubt I'll get to this soon, but if you want to make changes I'm happy to discuss/review.", "@allenlavoie only fix i had in mind was to not use globs, and just match the path exactly, which may break some compat with people who are using the (as i see it, unintended) glob behaviour.", "But `get_matching_files` is supposed to allow globs.", "Is the documentation for the C++ `GetMatchingPaths` (which Python's `files_io.get_matching_files` ends up calling in the end) wrong?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8ffef7b09555a65174632513069bd87d834c116c/tensorflow/core/platform/file_system.h#L123-L146", "@mihaimaruseac there's a documentation mismatch\r\n\r\nmaybe down that far in the call chain globs are okay, but the main api for checkpoints says _path_ *not* \"glob\".\r\n\r\nsee - https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\r\n\r\n-- edit: and actually, as i say in the original post, the same is true of `get_matching_files` in python-land. so i guess python-land docs disagree with the c++.", "Seems to be a bug in the checkpoint reading code: it assumes end of path is a glob but doesn't check that no globbing characters are used in the path before that.\r\n\r\nI have a roadmap to fix all of these filesystem issues but they are pending on [modularizing the filesystem support](https://github.com/tensorflow/community/pull/101) so it will be a while until I get here.\r\n\r\nIf you have a PR that fixes the behavior at the beginning of this comment, I'd be happy to review ahead of roadmap.\r\n\r\nSorry it takes too long to get to a fix on my side.", "@mihaimaruseac no prob; it's not urgent, glad it's on the _path_ to getting fixed :)", "Was able to replicate the issue in TF v2.5 ,please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/a52944d6a7bc0f0e79e34f39b9337083/untitled122.ipynb)..Thanks !"]}, {"number": 35433, "title": "Flag --incompatible_no_implicit_file_export will break TensorFlow in a future Bazel release", "body": "Incompatible flag --incompatible_no_implicit_file_export will be enabled by default in a future Bazel release [1], thus breaking TensorFlow.\n\nThe flag is documented here: https://github.com/bazelbuild/bazel/issues/10225\n\nPlease check the following CI builds for build and test results:\n\n* <a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/361#3e0dba20-d84a-4394-bbe8-4155855f8aa5\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/buildkite/emojis/master/img-buildkite-64/mac.png\" height=\"16\"/>macOS, OpenJDK 8</a>\n* <a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/361#8067be5e-7d21-4333-a4e1-6e2817d456d4\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/buildkite/emojis/master/img-buildkite-64/ubuntu.png\" height=\"16\"/>Ubuntu 18.04, OpenJDK 11</a>\n* <a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/361#5664296d-5125-45bc-a16b-80fb8b5b94f1\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/buildkite/emojis/master/img-buildkite-64/windows.png\" height=\"16\"/>Windows, OpenJDK 8</a>\n\nNever heard of incompatible flags before? We have [documentation](https://docs.bazel.build/versions/master/backward-compatibility.html) that explains everything.\n\nIf you don't want to receive any future issues for TensorFlow or if you have any questions,\nplease file an issue in https://github.com/bazelbuild/continuous-integration\n\n**Important**: Please do NOT modify the issue title since that might break our tools.\n\n[1] The target release hasn't been determined yet. Our tool will update the issue title once the flag flip has been scheduled.\n", "comments": []}, {"number": 35383, "title": "Enable SO_REUSEPORT option in tensorflow training server", "body": "**System information**\r\n- TensorFlow version (you are using): 1.15 and >=2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAdd SO_REUSEPORT option when starting tensorflow training server. It will enable to scan ports to build TF_CONFIG env variable. It is necessary to use distributed tensorflow with resource managers that do not reserve ports (such as Yarn).\r\n\r\nIt already has been discussed in ticket https://github.com/tensorflow/tensorflow/issues/21492. It is unclear why the option has been disabled in https://github.com/tensorflow/tensorflow/commit/8cf38e81e638db173238a8f95d6ea613c24d3d9f\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\n\r\nProjects like https://github.com/criteo/tf-yarn (tensorflow on yarn) will use it to implement the recommended way to create the cluster configuration. (from https://www.tensorflow.org/guide/distributed_training): The procedure will be: \r\n* Launch on all executors a process that will scan ports and reserve a free one\r\n* A master gathers ports numbers. \r\n* Master creates configuration and broadcasts TF_CONFIG variable to all executors\r\n* Launch tensorflow servers\r\n\r\n**Any Other info.**\r\n", "comments": ["The problem is more important now with the introduction of new distribution strategies like MultiWorkerMirroredStrategy.\r\n\r\nBefore with PS strategy we were able to start the tf server on our own:\r\n```\r\n server = tf.train.Server(\r\n            tf.train.ClusterSpec(cluster_spec),\r\n            job_name=task_type,\r\n            task_index=task_id,\r\n            config=session_config,\r\n            start=True)\r\n```\r\n\r\nand then just injecting 'google' env in here to prevent the server from starting again:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/distribute_coordinator.py#L432\r\n\r\nThis reduced the race condition of port reservation and therefore we never really saw the problem.\r\n\r\nNow with MultiWorkerMirroredStrategy we can't start the server upfront anymore but need to start the whole strategy which then starts the server which leaves a longer delay between port reservation and the port really taken.\r\n(See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/collective_all_reduce_strategy.py#L278)\r\n\r\n@superbobry \r\n\r\n", "I am curious why you want to start the servers upfront? Why not allow distribution strategies create std TF servers?", "Not being able to reserve a port is really a Yarn issue, not a TF one. \r\n\r\nCurrently TF server does not support clean shutdown, and as a workaround I'd suggest you to use a dummy server to probe available ports instead of creating and destroying TF servers repeatedly.", "We currently use the solution described by @byronyi. But we encounter some race conditions with it. Because of the option SO_REUSE_PORT is not set, we have to shutdown the dummy server before to start the TF sever. As yarn cannot reserve ports, some other rogue processes can take the port before the tensorflow server. For this reason, we would like to activate this option to be sure the port will be reserved to tensorflow.", "@jdlesage I see what you mean by the race condition. Unfortunately, using SO_REUSEPORT does not solve this issue. Suppose two independent TF job bind to the same port with SO_REUSEPORT on a same machine. They thought they could reserve a port, but since both of them are using SO_REUSEPORT, neither of them will think they fail.\r\n\r\nYou have to start the servers upfront and propagate the cluster spec after the server initialization.", "I was not aware that it is possible to propagate the cluster spec after the servers initialization. Definitively, that's the best solution. Do you have some pointers that describe how to propagate the cluster spec ?", "Take a look at this: https://github.com/tensorflow/tensorflow/issues/11081\r\n\r\nNot sure how this will work with dist-strat though; I will leave the question to @yuefengz.", "Passing a ClusterResolver instead of TF_CONFIG could be helpful as well.", "Thanks for the replies.\r\n\r\n@byronyi \r\nI had a look at using ClusterResolver instead of TF_CONFIG but it seems that this doesn't solve this problem. From what I understood all it does is giving the spec in some form to dist strategies which then start the server again.  So I suppose the race condition would be the same.\r\n\r\n[The code](https://github.com/tensorflow/tensorflow/issues/11081) you linked to is nice and we actually use [this](https://github.com/criteo/tf-yarn/blob/master/examples/session_run_example.py) for low level tensorflow in tf 1.15 but it doesn't seem to work anymore with tf 2 (at least all methods display as deprecated)\r\n\r\n> I see what you mean by the race condition. Unfortunately, using SO_REUSEPORT does not solve this issue. Suppose two independent TF job bind to the same port with SO_REUSEPORT on a same machine. They thought they could reserve a port, but since both of them are using SO_REUSEPORT, neither of them will think they fail.\r\n\r\nI don't necessarily agree on this. We can have two tf jobs starting at the same time, yes, but the initial port assignment is random, so the probability that this collides is really small. Also in this case we could even code a port assignment service on our own and only reserve free ports.\r\nWith the current tf behavior  we are blocked because we need to free the ports all the time (before  starting the tf server) and then we don't even know when tf assigns them again. \r\nWith PS strategy this timeframe is really small, because we can start the server upfront, so we have this race condition but it is not a real issue.\r\nWith CollectiveAllReduce strategy (aka MultiWorkerMirroredStrategy) it is an issue because it seems not possible to start the server upfront anymore (@yuefengz can you confirm this statement is true ?)\r\n\r\nSo, imo, we would need two fixes:\r\n- re-activate SO_REUSEPORT \r\n- permit in every case to start the tf server upfront.\r\n", "This is not supported for `MultiWorkerMirroredStrategy` to skip starting std servers. The server is started by the context object. @haoyuz probably can share a workaround if any."]}, {"number": 35346, "title": "AutoGraph is compiled 5x slower in TF2.x Multi-GPU Distributed Mirrored Strategy", "body": "**System information**\r\n* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n* TensorFlow installed from (source or binary): binary\r\n* TensorFlow version (use command below): v2.1.0-rc0-47-g064e153 2.1.0-rc1 (`python3 -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`)\r\n* Python version: Python 3.6.8\r\n* CUDA/cuDNN version: Driver Version: 440.33.01, CUDA Version: 10.2, cuDNN 7.6.2\r\n* GPU model and memory: Tesla V100-SXM2-16GB\r\n\r\n**Describe the current behavior**\r\nCompiling autograph function is 4-5x slower in Distributed Mirrored Strategy:\r\n* Single-GPU Distributed Mirrored Strategy: under 5 minutes\r\n* Multi-GPU Distributed Mirrored Strategy: about 30 minutes\r\nThe tensorflow code is identical in both setups.\r\n\r\n**Describe the expected behavior**\r\nAutograph compilation should take roughly the same time in single and multi-GPU Distributed Mode with Mirrored Strategy.\r\n\r\n**Code to reproduce the issue**\r\nTraining loop (hierarchical VAE in the current configuration):\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1151\r\n\r\nThe code is adapted from TF1.x repository:\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/8866a0b1457cbd4be5d6f549f9bf4075d49b2486/SPADE.py#L1045\r\nand is compiled using TF2.x @tf.function annotation.\r\n\r\nIt uses a dry-run of the model to pre-create variables using tf.compat.v1.variable_scope(scope, reuse=tf.compat.v1.AUTO_REUSE):\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1174\r\n(it takes about 10 minutes to compile and run it in multi-gpu mode)\r\n\r\nThen it runs the actual training step(s)\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1257\r\n(it takes about 20 minutes to compile and run it for the first time in multi-gpu mode)\r\n\r\nIt looks that just disabling lines with 'optim.apply_gradients'\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1229\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1231\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1233\r\nslashes about 10 out 20 minutes needed for initial run in multi-gpu mode. Which is essentially compiling back-propagation?\r\n\r\nAfter the first training step that takes 20 minutes the following training steps run at normal speed.\r\n\r\nThe total number of mirrored parameters is around 500MB.\r\n\r\nWith 4 V100 GPUs training step is around 5x slower than with single V100 GPU.\r\n\r\nCommand:\r\nNCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=ALL nohup python3 main.py --dataset CelebAMask-HQ --img_height 256 --img_width 256 --ch 16 --img_ch 3 --phase train --save_freq 10000 --batch_size 12 --gan_type hinge --code_gan_type gan --n_critic 1 --code_num_layers=4 --code_dist_num_layers=0 --augment_flag false --sn=False --train_main=true --train_nondet=true --lr 0.0002 --epoch=50 --decay_epoch=25 --print_freq 100 &> train.CelebAMask-HQ.log&\r\n\r\n**Other info / logs**\r\n[train.CelebAMask-HQ.slow_compile.log](https://github.com/tensorflow/tensorflow/files/3993026/train.CelebAMask-HQ.slow_compile.log)\r\n", "comments": ["I've managed to create a minimalistic script to demonstrate that in multi-gpu autograph distributed mirrored mode compiling a function is much slower than in single-gpu mode. The total size of variables in the script is 2x512x512x512x4=1GB.\r\n\r\nIn my real code this means that I have to wait 15 minutes for my network to build in mutli-gpu mode.\r\n\r\nSingle-gpu:\r\n```\r\nCUDA_VISIBLE_DEVICES=0 time python3 test.py\r\n```\r\nResult:\r\n```\r\n...\r\nstep: 0\r\nresult tf.Tensor(0.0, shape=(), dtype=float32)\r\nduration: 35.61067724227905\r\nstep: 1\r\nresult tf.Tensor(0.0, shape=(), dtype=float32)\r\nduration: 0.036466360092163086\r\nstep: 2\r\nresult tf.Tensor(0.0, shape=(), dtype=float32)\r\nduration: 0.034343719482421875\r\n...\r\nstep: 99\r\nresult tf.Tensor(0.0, shape=(), dtype=float32)\r\nduration: 0.030506134033203125\r\n40.67user 7.41system 0:46.16elapsed 104%CPU (0avgtext+0avgdata 2235208maxresident)k\r\n0inputs+96outputs (0major+469010minor)pagefaults 0swaps\r\n```\r\n\r\nMulti-gpu:\r\n```\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 time python3 test.py\r\n```\r\nResult:\r\n```\r\n...\r\nstep: 0\r\nresult tf.Tensor(0.0, shape=(), dtype=float32)\r\nduration: 311.546569108963\r\nstep: 1\r\nresult tf.Tensor(0.0, shape=(), dtype=float32)\r\nduration: 27.299265146255493\r\nstep: 2\r\nresult tf.Tensor(0.0, shape=(), dtype=float32)\r\nduration: 0.06128430366516113\r\nstep: 3\r\nresult tf.Tensor(0.0, shape=(), dtype=float32)\r\nduration: 0.04851055145263672\r\n...\r\nstep: 99\r\nresult tf.Tensor(0.0, shape=(), dtype=float32)\r\nduration: 0.04903459548950195\r\n355.49user 13.74system 5:56.17elapsed 103%CPU (0avgtext+0avgdata 9388168maxresident)k\r\n0inputs+120outputs (0major+2382567minor)pagefaults 0swaps\r\n```\r\n\r\nThe script:\r\n```\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndistribute_strategy = tf.distribute.MirroredStrategy()\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(([np.zeros((512,),np.float32)]))\r\ndataset = dataset.repeat(None).batch(64)\r\ndataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\ndataset = distribute_strategy.experimental_distribute_dataset(dataset)\r\ndataset = iter(dataset)\r\n\r\nwith distribute_strategy.scope():\r\n  optim = tf.keras.optimizers.Adam(0.0002)\r\n\r\n  def layer(x, scope):\r\n    with tf.compat.v1.variable_scope(scope, reuse=tf.compat.v1.AUTO_REUSE):\r\n        w = tf.compat.v1.get_variable(\"w\", shape=(512,512), initializer=tf.compat.v1.random_normal_initializer())\r\n        return tf.linalg.matvec(w,x)\r\n\r\n  def compute(x):\r\n    r = 0\r\n    for i in range(0,512):\r\n      r = r + layer(x, \"layer_\" + str(i))\r\n    r = tf.reduce_mean(r)\r\n    return r\r\n\r\n  @tf.function \r\n  def train(x):\r\n    def train_fn(x):\r\n      with tf.GradientTape(persistent=True) as tape:\r\n        r = compute(x)\r\n      v = tf.compat.v1.trainable_variables()\r\n      g = tape.gradient(r,v)\r\n      optim.apply_gradients(zip(g, v))\r\n      return r\r\n    result = distribute_strategy.experimental_run_v2(train_fn, args=(x,))\r\n    result = tf.reduce_mean(distribute_strategy.experimental_local_results(result))\r\n    return result\r\n\r\n  for i in range(0,100):\r\n      start = time.time()\r\n      print(\"step:\", i)\r\n      x = next(dataset)\r\n      result = train(x)\r\n      print(\"result\", result)\r\n      print(\"duration:\", time.time()-start)\r\n```", "Also see this issue:\r\nhttps://github.com/tensorflow/tensorflow/issues/35415#issuecomment-570075254", "olegmyrk@ this issue is a known one and due to the fact that we trace the train function on each of the GPUs sequentially. This is known to result in slow start up times when using MirroredStrategy. \r\n", "@anj-s  Is there any way to speed up the compilation?\r\n\r\nI am currently training 7 models in tf.function, and it takes more than 20 minutes to start training !!! Very bad. \r\n\r\nI observed that the CPU can only use one core to compile, is there any way to start multiple threads to compile at the same time?", "@rohan100jain this is an example of a painfully slow tf.function tracing + compilation", "This is a known issue with tf.function and multiple devices right now. @allenlavoie is working on something that might help with this", "Any solution now?", "Discovered this issue as well when scaling up to use multiple GPUs for a densenet. With multi GPU servers tending to have a ton of cores and be bursting with RAM, and the startup being something that's inherently parallelisable and relatively slow, it's a shame not to exploit available resources to accelerate startup as much as possible. Would be good to know if this is realistically likely to be an issue indefinitely, or whether it's on the cards to be improved soon.   I'm also surprised that the result of compilation isn't cached to be reused on a subsequent run, either with automatic detection of a modified model or reliant on a user supplied id and the caveat that if you change the model and don't refresh the id, then don't expect things to work as you expect. I'd take that every time over twiddling my thumbs for some minutes waiting for something to start happening and adding 10% or more to the training time."]}, {"number": 35333, "title": "Optimizer within Estimator computes incorrectly small gradient when used with MirroredStrategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: Anaconda, clean install: `conda create -n tf2 tensorflow-gpu=2.0.0`\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: 10.0.130, 7.6.4\r\n- GPU model and memory: Tesla V100, 16GB\r\n\r\n**Describe the current behavior**\r\nA gradient is reported that is only half of the expected gradient when using distributed training with Estimator. See example code below.\r\n**Describe the expected behavior**\r\nThe gradient should be twice as large. If somehow this behavior is actually intended, then this needs to be much more loudly documented since it is quite unexpected - right now I cannot find any place where it is documented at all. For example, https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_estimator_limited_support and https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Optimizer#compute_gradients both have no mention of it.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport sys\r\nimport os\r\nimport tensorflow as tf\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\r\n\r\ndef model_fn(features,labels,mode,params):\r\n  if mode == tf.estimator.ModeKeys.PREDICT:\r\n    raise NotImplementedError()\r\n  if mode == tf.estimator.ModeKeys.EVAL:\r\n    raise NotImplementedError()\r\n  if mode == tf.estimator.ModeKeys.TRAIN:\r\n    w = tf.compat.v1.get_variable(initializer=tf.zeros([2]),name=\"w\")\r\n    losses = tf.square(w - labels)\r\n    loss = tf.reduce_sum(losses)\r\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)\r\n    gradients = optimizer.compute_gradients(tf.reduce_sum(loss))\r\n    global_step=tf.compat.v1.train.get_global_step()\r\n    train_op = optimizer.apply_gradients(gradients,global_step=global_step)\r\n    print_op = tf.print(\r\n      w,\r\n      losses,\r\n      [(grad,var.name) for (grad,var) in gradients],\r\n    )\r\n    return tf.estimator.EstimatorSpec(\r\n      mode,\r\n      loss=loss,\r\n      train_op=tf.group(train_op,print_op)\r\n    )\r\n\r\ndef input_fn():\r\n  dataset = tf.data.Dataset.from_tensors(({}, [1.,2.]))\r\n  return dataset.repeat(1)\r\n\r\nprint(\"SINGLE GPU\")\r\nestimator = tf.estimator.Estimator(\r\n  model_fn=model_fn,\r\n  model_dir=\"test_single_gpu/\",\r\n  params={}\r\n)\r\nestimator.train(input_fn)\r\n\r\ndef input_fn():\r\n  dataset = tf.data.Dataset.from_tensors(({}, [1.,2.]))\r\n  return dataset.repeat(2)\r\n\r\nprint(\"MULTI GPU\")\r\nstrategy = tf.distribute.MirroredStrategy(devices=[\"/GPU:0\",\"/GPU:1\"])\r\nestimator = tf.estimator.Estimator(\r\n  model_fn=model_fn,\r\n  model_dir=\"test_multi_gpu/\",\r\n  params={},\r\n  config=tf.estimator.RunConfig(train_distribute = strategy)\r\n)\r\nestimator.train(input_fn)\r\n```\r\n\r\n**Other info / logs**\r\nThe relevant portion of the output of the above test case is:\r\n```\r\nSINGLE GPU\r\n[0 0] [1 4] [([-2 -4], 'w:0')]\r\nMULTI GPU\r\n[0 0] [1 4] [([-1 -2], 'w:0')]\r\n[0 0] [1 4] [([-1 -2], 'w:0')]\r\n```\r\nThe model is attempting to fit two weights to be equal to 1 and 2, respectively, penalizing them by the squared error. The weights are initialized to 0.\r\n* Single GPU case: We can see it reads one data element from the dataset, and correctly computes the loss as as \"1\" and \"4\", which are the squared differences. The gradients are correctly computed as \"-2\" and \"-4\", which indeed the mathematical derivatives of (x-1)^2 and (x-2)^2 at x = 0, respectively. So this is all correct.\r\n* Multi GPU case: We can see each GPU independently and in parallel reads an element from the dataset, reading two elements in total. Each one correctly computes the loss as \"1\" and \"4\" as before on its own element. However, each one separately only computes \"-1\" and \"-2\" as the gradient. This is wrong, each one is a factor of 2 too small.\r\n\r\n(edit: corrected linux version, some grammar edits)", "comments": ["@lightvector - yes this is expected - for multi GPU, the loss is scaled by number of replicas[1], and later on in the optimizer, the gradients are summed over the replicas[2]. Overall this gives the correct averaged gradients. Another alternative would be to not scale the loss, but average the gradients. This would be equivalent but we found this to be slower in performance. \r\n\r\nYou're right that we don't seem to document the behavior with estimator & old optimizers anywhere. We do document the behavior in new optimizers[3]. Happy to accept a PR to add this documentation in optimizer v1 if you want to send one.\r\n\r\n[1] https://github.com/tensorflow/tensorflow/blob/aa8a34071d1170b6361d11676f3256c3311af2d1/tensorflow/python/training/optimizer.py#L468\r\n[2] https://github.com/tensorflow/tensorflow/blob/aa8a34071d1170b6361d11676f3256c3311af2d1/tensorflow/python/training/optimizer.py#L668\r\n[3] https://github.com/tensorflow/tensorflow/blob/aa8a34071d1170b6361d11676f3256c3311af2d1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L151\r\n\r\n", "This seems still incorrect though - how can this actually be intended?\r\n\r\nIt is natural to expect that the numeric output of \"compute the gradient\" is the gradient. At the point of computing the gradient, there is no inter-GPU communication that should be happening, right? So there is no question of taking the mean versus the sum. Each GPU independently computes the gradient on its own batch, and if the user then asks what the gradient on that batch, they should get the gradient on that batch. \r\n\r\nThis matters in some cases. For example, if using tf.clip_by_global_norm, one might expect to specify a bound on the global norm of the per-replica batch - expecting that each GPU independently will perform the clipping on its batch, then jointly apply the results as usual. If the per-replica batch was obtained by splitting a global batch size (e.g. using dataset.unbatch()) into two, then one might naturally lower the norm clipping threshold by a factor of sqrt(2), since a batch of uncorrelated data samples split into two replica-batches will result in gradients sqrt(2) smaller on each replica-batch on average, when summing across the samples in the batch.\r\n\r\nInstead, one has to specify a bound that is sqrt(8) times smaller, because not only are the gradients on each per-replica batch naturally smaller, but Tensorflow additionally reports them as a further factor of two smaller than they truly are on that per-replica batch.\r\n\r\nIf scaling should happen, it should be entirely contained within the apply_gradients function, right? I.e. the scaling should happen at the time of applying the gradient, rather than contaminating the value that compute_gradients returns and making it no longer the gradient? Because only at the point of actually _applying_ the gradient do you now actually get cross-replica communication and only then does the question of averaging versus summing come into play. Along a note in the documentation, this would make everything far more intuitive. Otherwise you end up with the bizarre situation where \"optimizer.compute_gradients\" actually does not return the gradient, it returns the gradient scaled by something depending on the existence of other GPUs, even though the gradient computation should be entirely GPU-local?\r\n\r\n\r\n", "Was able to replicate the issue in TF v2.5 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/3f41744c174a9bed0d9b15ea08ac7cb4/untitled120.ipynb)..Thanks ! "]}, {"number": 35292, "title": "unbounded memory leak in tf.io.gfile.isdir()", "body": "This was discovered in debugging of https://github.com/tensorflow/tensorboard/issues/766 by a combination of @psybuzz, @wchargin, and myself.  From empirical evidence from TensorBoard users it appears that this grows without bound, so in practical usage it only takes a day or so to consume dozens of GB of memory.\r\n\r\nCalling `tf.io.gfile.isdir()` leaks memory at a rate of approximately 1 MB per 20,000 calls, and this is reproducible at TF 2.0.0 and latest tf-nightly (`tf-nightly-2.1.0.dev20191219`), on macOS, Ubuntu 16.04, and Linux Debian (a Google workstation), and with python 2.7, 3.5, and 3.7.\r\n\r\nHere's our repro script:\r\n```python\r\nimport gc\r\nimport os\r\nimport resource\r\nimport time\r\n\r\nimport tensorflow as tf\r\n\r\nprint(\"PID: %d\\n\" % (os.getpid(),))\r\nprev = 0\r\nwhile True:\r\n  peak = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\r\n  print(\"peak memory = %d (+%d) in kb (Linux) or b (macOS)\" % (peak, peak-prev))\r\n  prev = peak\r\n  for _ in range(20000):\r\n    tf.io.gfile.isdir(b\"/tmp/nonexistent-file-for-tf-memory-leak\")\r\n  gc.collect()\r\n  time.sleep(1.0)\r\n```\r\n\r\nSample output of `python repro.py`:\r\n```\r\nPID: 153611\r\n\r\npeak memory = 226796 (+226796) in kb (Linux) or b (macOS)\r\npeak memory = 228108 (+1312) in kb (Linux) or b (macOS)\r\npeak memory = 229132 (+1024) in kb (Linux) or b (macOS)\r\npeak memory = 229900 (+768) in kb (Linux) or b (macOS)\r\npeak memory = 230924 (+1024) in kb (Linux) or b (macOS)\r\npeak memory = 231948 (+1024) in kb (Linux) or b (macOS)\r\npeak memory = 232716 (+768) in kb (Linux) or b (macOS)\r\npeak memory = 233740 (+1024) in kb (Linux) or b (macOS)\r\npeak memory = 234764 (+1024) in kb (Linux) or b (macOS)\r\npeak memory = 235788 (+1024) in kb (Linux) or b (macOS)\r\npeak memory = 236556 (+768) in kb (Linux) or b (macOS)\r\npeak memory = 237580 (+1024) in kb (Linux) or b (macOS)\r\n...\r\n```\r\n\r\nOur initial attempt to find a root cause led us to suspect the fact that `is_directory_v2` uses ScopedTFStatus while the rest of the `gfile` API does not (we spot-checked a few other APIs, including `tf.io.gfile.stat()`, and did not see the same issue).\r\n\r\nHere's the code from v2.0.0 (file_io.py was just converted to PyBind11 today so it's possible this actually fixes the issue, but there is not yet a nightly with the change to check):\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/lib/io/file_io.py#L585-L596\r\n\r\nWe attempted to debug further by deconstructing the calls to `isdir()` into the two lines, one that creates `ScopedTFStatus` and one that calls `pywrap_tensorflow.IsDirectory()`, and it seemed to be the case that the memory leak is proportional to the number of times `IsDirectory()` is called with a *distinct* `ScopedTFStatus` pointer (calling it over and over with the same status doesn't seem to leak at a proportional rate; reusing the status here seemed fine for testing this because `IsDirectory()` does not actually touch the status in the codepath for a nonexistent file).  So we suspect maybe there's a weird interaction at the SWIG boundary that results in the leak.\r\n\r\nFurthermore, it also seems to leak when the argument is an existing filename; the repro uses a nonexistent one for simplicity and because that makes the codepath slightly simpler (since then `IsDirectory()` exits early on file nonexistence via the `access()` syscall and never even calls `stat()`).  Also, the leak still occurs when the `gc.collect()` is omitted; it's also just there to isolate possible causes of the leak.", "comments": ["NB: I can confirm that your provided `repro.py` works for me on TF\r\nnightly from 2019-12-19 but not from 2019-12-20. Furthermore, hammering\r\n`list(tf.io.gfile.walk(\"bigdir\"))` used to quickly consume unbounded\r\nmemory; now, it consumes memory cyclically with a time series that looks\r\nlike standard GC behavior, never exceeding around 400 MB. So it\u2019s\r\npossible that the pybind11 change really did fix it, and we caught this\r\nbug on the last day of its existence. :-)\r\n\r\nFurther investigation may still be warranted.\r\n"]}, {"number": 35283, "title": "Android TFlite inconsistent performance when app is not in focus.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 9.0 and 10.0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 3a (Android 10), tested on Android 9 as well with same bad behaviour\r\n- TensorFlow installed from (source or binary):  tflite .so built from source\r\n- TensorFlow version (use command below): 1.13, 1.14, 1.15, 2.0, 2.1\r\n- Bazel version (if compiling from source): various; 0.21 to 0.29\r\n\r\nProblem:\r\nWe have an audio processing application that runs in real time, including when the phone's screen is off. Using the built shared objects (tensorflowlite.so) from r1.13 and r1.14 branches, our processing time stays consistent. Locking the phone or minimizing the app does not affect the tflite inference times. Our app also uses a foreground service to make Android give us optimal process scheduling.\r\n\r\nHowever, when using newer tflite shared objects (.so) built from r1.15, r2.0, and r2.1 the inference performance drops when the app is not in focus (either screen was locked, or our app minimized). The behaviour is especially bad when using more threads, e.g: `interpreter->SetNumThreads(2);`\r\n\r\n**Describe the current behavior**\r\nAndroid tflite model inference times are lower when app is not in focus with tflite versions r1.15, r2.0, r2.1. \r\n**Describe the expected behavior**\r\nExactly the same inference processing performance when the app is in focus or not.\r\n\r\nIs there something in the API of newer tflite versions (r1.15 and newer) that I could play around with to fix this? Any help is greatly appreciated, thank you!\r\n", "comments": ["We adopted a new matrix multiplication library in 1.15, which generally has offered significantly improved multi-threaded performance. Prior to 1.15, unless you explicitly pin your process to big cores, multi-threaded performance was extremely variable. In 1.15+, the performance is generally far more deterministic. However, we haven't done as much testing against scenarios where the screen is locked or the app loses focus.\r\n\r\nIf you're building from source, could you try adding `--define=tflite_with_ruy=false` to your bazel build command, and get back to us with the results? Thanks!", "This has resolved it, thank you for the tip! For our application we're only using 1-2 cores, so multi-threaded performance is not as critical as consistent inference times.", "> This has resolved it, thank you for the tip!\r\n\r\nFor completeness, how was it resolved? Using the `--define=tflite_with_ruy=false` flag? If so, would you mind sharing more about your model so we can investigate ruy-based performance? Thanks!", "Yes using the ``--define=tflite_with_ruy=false`` made the performance the same with screen off. We're using a LSTM based model that uses the `tf.nn.rnn_cell.LSTMCell` and `tf.nn.dynamic_rnn` classes.", "I'm glad this has resolved the issue! We are interested in reproducing on our side. Is it possible for you to upload the model?\r\n\r\nThe new matrix multiply library, ruy, shouldn't be invoked on the LSTM/RNN ops as of yet, so I'm interested in understanding why this fixed your issue.\r\n\r\nAlso, to clarify, is the following correct:\r\n\r\nOld behavior: same inference latency whether or not the app is in focus\r\n\r\nNew (bad) behavior: inference time is *HIGHER* when the app is not in focus (i.e. the performance is not as good when the app is not in focus)\r\n\r\nThanks!", "Could you please try the following (With ruy. So in this experiment, do NOT build with `--define=tflite_with_ruy=false`):\r\n\r\nEdit this line:\r\nhttps://github.com/tensorflow/tensorflow/blob/e6983d538f140552d18fff6377a80c45236f1e36/tensorflow/lite/experimental/ruy/wait.cc#L66\r\n\r\nincrease the number of milliseconds here, change the current value 2 to something like 20 instead. does this help?", "@talumbau The behavior is indeed as you described; higher inference times when app is not in focus. I've gotten a model that I can share, could you email me @ artashes333@hotmail.com so I can share it?\r\n\r\n@bjacob Changing the `DurationFromMilliseconds(N)` number has improved the performance when the app is not in focus by a bit. On Pixel 3a:\r\n`DurationFromMilliseconds(2)`: ~3.3x increase in inference times\r\n`DurationFromMilliseconds(20)`: ~1.6x increase in inference times", "Thanks for running this experiment. ~1.6x is sufficiently close to ~1x that we can draw the following conclusion as to what is the main phenomenon happening here --- there may well be additional things at play, but this should be the main thing, especially if you find that further increasing this duration beyond 20ms further reduces the inference times.\r\n\r\nOn a mobile system like Android, the OS aggressively throttles the computing power available to applications in order to preserve battery charge. The throttling is even more aggressive for applications that aren't foreground. So as a first approximation you could say that everything here is \"working as intended\" from the perspective of the OS.\r\n\r\nIn multithreaded inference, TensorFlow Lite uses internal a pool of \"worker\" threads. After a thread is done with its current task, it has to wait for a next task to come up. The standard way of doing that, \"condition-variable waiting\", informs the OS about this thread being waiting. That's good because that allows to conserve battery charge. But Android is so aggressive at reducing CPU clock speeds and even turning CPU cores off altogether, and then so conservative at ramping CPU cores up again, that if we enter that waiting state then there is a very large penalty for the next workload. It may take 100ms or more to reach back high clock speeds. That is why the above-linked wait.cc code uses some \"spin-waiting\", i.e. just running a loop until a certain number of milliseconds have elapsed, to avoid the worst of that issue.\r\n\r\nThe problem of course is that there is no ideal choice for this \"spin duration\", as it's a compromise between wasting battery charge and suffering very high latency penalties. The current 2ms value was empirically determined from a few benchmarks, but this depends not only on TFLite models (governing time gaps between matrix multiplications within the inference) but also on application-level usage patterns (governing time gaps between inferences). 2ms was a conservative choice to avoid wasting too much battery charge, but the gap between this 2ms on the one hand, and the 100ms order of magnitude of the penalty to ramp back up to high clock speed is so large, there will be lots of room for variation among applications as to what's the best choice here.\r\n\r\nThat is why I believe the best course of action here would be to make this time duration runtime-configurable:\r\n1) in ruy, make this configurable, presumably by exposing a control on ruy::Context.\r\n2) in TFLite, expose this through CpuBackendContext, then on the user-facing TFLite API (TfLiteInterpreter?)\r\n\r\nI had filed an internal issue (b/135595069) - @jdduke @talumbau please consider staffing.", "I have some updates here. I ran our standard `benchmark_model` tool on the model provided by Artaches (thanks!) and I am now confused by the results you report. The model is all in float and the performance is dominated by the FullyConnected op (it spends 99% of its time in FC). When run with standard options (i.e. with Ruy on) on a small core of a Pixel 4, you get:\r\n\r\n... a snippet from the tool output:\r\n\r\n```\r\nAverage inference timings in us: Warmup: 14022, Init: 1183, Inference: 13681.3\r\n\r\nProfile (1 threads):\r\n\r\nThread 0 (1410 samples)\r\n\r\n* 99.29% FullyConnected\r\n  * 99.29% cpu_backend_gemm::Gemm\r\n    * 99.29% cpu_backend_gemm::Gemm: CustomGemv\r\n\r\n```\r\n\r\nThe profiling indicates that the model does not actually execute any code inside Ruy. To simplify things, I will just say that the CustomGemv code exists to handle GEMV operations because Ruy, being a general matrix multiplication library, does not do well at the GEMV case (this is changing, but not germane to this conversation).\r\n\r\nWhen I rebuild the `benchmark_model` tool with `--define=tflite_with_ruy=false` and re-run, I get:\r\n\r\n```\r\nAverage inference timings in us: Warmup: 66187.9, Init: 1295, Inference: 65619.1\r\n                                                                                                                                                     \r\nProfile (1 threads):\r\n                                                                                                                                                     \r\nThread 0 (3577 samples)\r\n                                                                                                                                                    \r\n* 99.89% FullyConnected\r\n  * 99.89% cpu_backend_gemm::Gemm\r\n    * 99.89% cpu_backend_gemm::Gemm: general GEMM\r\n```\r\n\r\nso the inferemce time is approximately 5x worse.\r\n\r\nSo the things I don't understand would be:\r\n1. How does changing a parameter of Ruy multi-threading impact the latency you see in your model, given that Ruy code is not invoked to run your model at this time?\r\n\r\n2. How does adding threads make the inference time worse, since the benchmarking tool shows that, in the default case, the CustomGemV path takes advantage of threading and reduces latency?\r\n\r\nI'm not sure how to proceed here. One thing: can you run your model on Pixel3a with the benchmark tool and verify some of the findings I am seeing? At least we would have some level of agreement on raw performance numbers (I have access to a Pixel 3 and Pixel 4, but not Pixel 3a)", "For completeness, I will mention that the output `cpu_backend_gemm::Gemm: general GEMM` in the second profiling indicates that another GEMM implementation was used. In this case, the GEMM operation used `eigen` since that is our fallback path is Ruy is not available.", "@talumbau There are two FC layers that the ``benchmark_model`` tool is showing. The first is actually an RNN LSTM cell (`tf.nn.rnn_cell.LSTMCell` + `tf.nn.static_rnn`; 5.5m params), the second is a small actual FC layer (`tf.keras.layers.Dense`; 327k params). The computation time of the FC layers make sense to me. \r\n\r\nHere are the numbers I get on the ``pixel 3a`` with the standard ``benchmark_model``:\r\n1 thread: ``adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/performance_issue_model.tflite --num_threads=1 --num_runs=5000``:\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\ncount=102 first=13137 curr=4872 min=4682 max=13137 avg=4913.32 std=828\r\n\r\nRunning benchmark for at least 5000 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=5000 first=4883 curr=4832 min=4660 max=5049 avg=4833.36 std=53\r\n\r\n[Overall] - Memory usage: max resident set size = 23.0469 MB, total malloc-ed size = 0.104813 MB\r\nAverage inference timings in us: Warmup: 4913.32, Init: 885, no stats: 4833.36\r\n```\r\n\r\n4 threads: ``adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/performance_issue_model.tflite --num_threads=4 --num_runs=5000``:\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\ncount=140 first=15603 curr=3159 min=3056 max=15603 avg=3556.35 std=1387\r\n\r\nRunning benchmark for at least 5000 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=5000 first=3340 curr=3175 min=3042 max=8959 avg=3199.51 std=277\r\n\r\n[Overall] - Memory usage: max resident set size = 22.4844 MB, total malloc-ed size = 0.0189743 MB\r\nAverage inference timings in us: Warmup: 3556.35, Init: 849, no stats: 3199.51\r\n```\r\n   \r\n` `\r\n  \r\nNow with a ``benchmark_model`` binary that was built with ``--define=tflite_with_ruy=false``:\r\n1 thread: ``adb shell /data/local/tmp/benchmark_model_v21_ruyFalse --graph=/data/local/tmp/performance_issue_model.tflite --num_threads=1 --num_runs=5000``:\r\n```\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=162 first=10464 curr=3053 min=2900 max=10464 avg=3074.59 std=583\r\n\r\nRunning benchmark for at least 5000 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=5000 first=3141 curr=3121 min=2893 max=3406 avg=3039.76 std=42\r\n\r\n[Overall] - Memory usage: max resident set size = 23.0625 MB, total malloc-ed size = 0.104752 MB\r\nAverage inference timings in us: Warmup: 3074.59, Init: 1282, no stats: 3039.76\r\n```\r\n4 threads: ``adb shell /data/local/tmp/benchmark_model_v21_ruyFalse --graph=/data/local/tmp/performance_issue_model.tflite --num_threads=4 --num_runs=5000``:\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\ncount=161 first=10793 curr=3071 min=2949 max=10793 avg=3091.92 std=610\r\n\r\nRunning benchmark for at least 5000 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=5000 first=3098 curr=3013 min=2898 max=3383 avg=3045.69 std=41\r\n\r\n[Overall] - Memory usage: max resident set size = 23.0586 MB, total malloc-ed size = 0.104813 MB\r\nAverage inference timings in us: Warmup: 3091.92, Init: 997, no stats: 3045.6\r\n```\r\nI hope this helps!", "Regarding the previous 3 comments, the explanation is that tflite uses the ruy thread-pool directly in place like this CustomGemv, via the cpu_backend_threadpool facility:\r\nhttps://github.com/tensorflow/tensorflow/blob/9c7d78bce12a6374748ce84dd9a4fc0a37606775/tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h#L179\r\n\r\nWhen tflite_with_ruy=true (the default on ARM), cpu_backend_threadpool maps onto the ruy thread-pool,\r\nhttps://github.com/tensorflow/tensorflow/blob/9c7d78bce12a6374748ce84dd9a4fc0a37606775/tensorflow/lite/kernels/cpu_backend_threadpool.h#L32-L41\r\n\r\nWhich ends up in the wait.cc code that I linked above.\r\n\r\nSo as far as I can see, everything here is still perfectly consistent with the theory proposed above, and it would be very useful to invest in the above described project to expose direct control over the spin duration to the application, with a clear explanation of the compromises here, and some degree of acknowledgement that there is a fundamental conflict here between a mobile OS conserving battery charge, and allowing background activities to have as low latency as foreground activities.", "How to make the foreground service?..\r\nAfter make the notification and sservice\r\nWhat class need to be called to the foregroundservice.java class?", "We've ran into another weird performance issue with tflite (this time using tflite r2.4 runtime) on the Pixel 3a. We've discovered that when the app is out of focus (minimized or screen turned off), the Pixel 3a forces all the tflite threads to run on the very last CPU core. This is particularly bad when using more than 1 thread for tflite (`interpreter->SetNumThreads(2 or greater)`) since all the tflite worker threads compete for the same core, and the scheduling causes worse performance than just using 1 thread. This can be clearly seen using the Android Studio profiler (with NDK profiling setup, since we're using C++ tflite APIs). I've attached a screenshot that demonstrates this scheduling behavior. The \"App Start\" mark is where tflite processing begins. Our application runs tflite processing about every 8ms, so it's crucial to get consistent performance.\r\n![image](https://user-images.githubusercontent.com/3443179/115820143-1adc8600-a3b5-11eb-9fac-b64f184fc084.png)\r\n\r\n\r\nWe've only seen this type of scheduling behavior on the Google Pixel 3a, many much worse phones (such as the Samsung Galaxy J3) don't do this. Can anything be done on developer's end? Is there any way in Android to encourage the device to not schedule all the tflite worker threads on the same core? Any help is greatly appreciated!\r\n\r\nAlso, shall I make a separate issue for this?", "> We've ran into another weird performance issue with tflite (this time using tflite r2.4 runtime) on the Pixel 3a. We've discovered that when the app is out of focus (minimized or screen turned off), the Pixel 3a forces all the tflite threads to run on the very last CPU core. This is particularly bad when using more than 1 thread for tflite (`interpreter->SetNumThreads(2 or greater)`) since all the tflite worker threads compete for the same core, and the scheduling causes worse performance than just using 1 thread. This can be clearly seen using the Android Studio profiler (with NDK profiling setup, since we're using C++ tflite APIs). I've attached a screenshot that demonstrates this scheduling behavior. The \"App Start\" mark is where tflite processing begins. Our application runs tflite processing about every 8ms, so it's crucial to get consistent performance.\r\n> ![image](https://user-images.githubusercontent.com/3443179/115820143-1adc8600-a3b5-11eb-9fac-b64f184fc084.png)\r\n> \r\n\r\nMany thanks for such a detailed perf. investigation! I think such an OS scheduling strategy on Pixel3a in this case isn't that surprising if considering the goal is to save power consumption, and the screen being off implies no user interaction w/ the phone?\r\n\r\n> We've only seen this type of scheduling behavior on the Google Pixel 3a, many much worse phones (such as the Samsung Galaxy J3) don't do this. Can anything be done on developer's end? Is there any way in Android to encourage the device to not schedule all the tflite worker threads on the same core? Any help is greatly appreciated!\r\n\r\nThese are great questions! I don't think Android has official APIs to set cpu affinity for threads as it's not encouraged to do so. Even if there's sth. that could be done on Pixel 3a to mitigate this issue, considering each Android phone manufacturer may adopt different OS scheduling strategies for each phone, there might be no universally unified approach to fix this. I could be totally wrong here as I don't have much Android experience. Anyway, I'll keep this thread updated once I learned more about such a scheduling strategy on Pixel 3a.\r\n\r\n> \r\n> Also, shall I make a separate issue for this?\r\n\r\n\r\n", "> These are great questions! I don't think Android has official APIs to set cpu affinity for threads as it's not encouraged to do so. \r\n\r\nSee a related discussion: https://github.com/tensorflow/tensorflow/issues/29910. Yes, it is possible to choose the fast cores (by parsing **/proc/cpuinfo**), and set the process's CPU affinity mask accordingly. The question whether this improves the overall performance of your application, can only be answered by experimenting."]}, {"number": 35248, "title": "Flag --incompatible_restrict_string_escapes will break TensorFlow in Bazel 1.2.1", "body": "Incompatible flag --incompatible_restrict_string_escapes will break TensorFlow once Bazel 1.2.1 is released.\n\nPlease see the following CI builds for more information:\n\n* [:darwin: (OpenJDK 8)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7\" target=\"_blank\">:darwin: (OpenJDK 8)</a>)\n* [:windows: (OpenJDK 8)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa\" target=\"_blank\">:windows: (OpenJDK 8)</a>)\n* [:ubuntu: 18.04 (OpenJDK 11)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1\" target=\"_blank\">:ubuntu: 18.04 (OpenJDK 11)</a>)\n\nQuestions? Please file an issue in https://github.com/bazelbuild/continuous-integration\n\n**Important**: Please do NOT modify the issue title since that might break our tools.\n", "comments": []}, {"number": 35247, "title": "Flag --incompatible_use_platforms_repo_for_constraints will break TensorFlow in Bazel 1.2.1", "body": "Incompatible flag --incompatible_use_platforms_repo_for_constraints will break TensorFlow once Bazel 1.2.1 is released.\n\nPlease see the following CI builds for more information:\n\n* [:darwin: (OpenJDK 8)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7\" target=\"_blank\">:darwin: (OpenJDK 8)</a>)\n* [:windows: (OpenJDK 8)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa\" target=\"_blank\">:windows: (OpenJDK 8)</a>)\n* [:ubuntu: 18.04 (OpenJDK 11)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1\" target=\"_blank\">:ubuntu: 18.04 (OpenJDK 11)</a>)\n\nQuestions? Please file an issue in https://github.com/bazelbuild/continuous-integration\n\n**Important**: Please do NOT modify the issue title since that might break our tools.\n", "comments": []}, {"number": 35246, "title": "Flag --incompatible_load_cc_rules_from_bzl will break TensorFlow in Bazel 1.2.1", "body": "Incompatible flag --incompatible_load_cc_rules_from_bzl will break TensorFlow once Bazel 1.2.1 is released.\n\nPlease see the following CI builds for more information:\n\n* [:darwin: (OpenJDK 8)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7\" target=\"_blank\">:darwin: (OpenJDK 8)</a>)\n* [:windows: (OpenJDK 8)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa\" target=\"_blank\">:windows: (OpenJDK 8)</a>)\n* [:ubuntu: 18.04 (OpenJDK 11)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1\" target=\"_blank\">:ubuntu: 18.04 (OpenJDK 11)</a>)\n\nQuestions? Please file an issue in https://github.com/bazelbuild/continuous-integration\n\n**Important**: Please do NOT modify the issue title since that might break our tools.\n", "comments": []}, {"number": 35245, "title": "Flag --incompatible_disable_target_provider_fields will break TensorFlow in Bazel 1.2.1", "body": "Incompatible flag --incompatible_disable_target_provider_fields will break TensorFlow once Bazel 1.2.1 is released.\n\nPlease see the following CI builds for more information:\n\n* [:darwin: (OpenJDK 8)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7\" target=\"_blank\">:darwin: (OpenJDK 8)</a>)\n* [:windows: (OpenJDK 8)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa\" target=\"_blank\">:windows: (OpenJDK 8)</a>)\n* [:ubuntu: 18.04 (OpenJDK 11)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1\" target=\"_blank\">:ubuntu: 18.04 (OpenJDK 11)</a>)\n\nQuestions? Please file an issue in https://github.com/bazelbuild/continuous-integration\n\n**Important**: Please do NOT modify the issue title since that might break our tools.\n", "comments": []}, {"number": 35244, "title": "Flag --incompatible_no_implicit_file_export will break TensorFlow in Bazel 1.2.1", "body": "Incompatible flag --incompatible_no_implicit_file_export will break TensorFlow once Bazel 1.2.1 is released.\n\nPlease see the following CI builds for more information:\n\n* [:darwin: (OpenJDK 8)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7\" target=\"_blank\">:darwin: (OpenJDK 8)</a>)\n* [:windows: (OpenJDK 8)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa\" target=\"_blank\">:windows: (OpenJDK 8)</a>)\n* [:ubuntu: 18.04 (OpenJDK 11)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1\" target=\"_blank\">:ubuntu: 18.04 (OpenJDK 11)</a>)\n\nQuestions? Please file an issue in https://github.com/bazelbuild/continuous-integration\n\n**Important**: Please do NOT modify the issue title since that might break our tools.\n", "comments": []}, {"number": 35243, "title": "Flag --incompatible_load_python_rules_from_bzl will break TensorFlow in Bazel 1.2.1", "body": "Incompatible flag --incompatible_load_python_rules_from_bzl will break TensorFlow once Bazel 1.2.1 is released.\n\nPlease see the following CI builds for more information:\n\n* [:darwin: (OpenJDK 8)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7\" target=\"_blank\">:darwin: (OpenJDK 8)</a>)\n* [:windows: (OpenJDK 8)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa\" target=\"_blank\">:windows: (OpenJDK 8)</a>)\n* [:ubuntu: 18.04 (OpenJDK 11)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1\" target=\"_blank\">:ubuntu: 18.04 (OpenJDK 11)</a>)\n\nQuestions? Please file an issue in https://github.com/bazelbuild/continuous-integration\n\n**Important**: Please do NOT modify the issue title since that might break our tools.\n", "comments": []}, {"number": 35242, "title": "Flag --incompatible_disallow_empty_glob will break TensorFlow in Bazel 1.2.1", "body": "Incompatible flag --incompatible_disallow_empty_glob will break TensorFlow once Bazel 1.2.1 is released.\n\nPlease see the following CI builds for more information:\n\n* [:darwin: (OpenJDK 8)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7\" target=\"_blank\">:darwin: (OpenJDK 8)</a>)\n* [:windows: (OpenJDK 8)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa\" target=\"_blank\">:windows: (OpenJDK 8)</a>)\n* [:ubuntu: 18.04 (OpenJDK 11)](<a href=\"https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1\" target=\"_blank\">:ubuntu: 18.04 (OpenJDK 11)</a>)\n\nQuestions? Please file an issue in https://github.com/bazelbuild/continuous-integration\n\n**Important**: Please do NOT modify the issue title since that might break our tools.\n", "comments": []}]