[{"number": 29723, "title": "Cannot import tensor_util in TF 1.14.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 1.14.0rc1\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: TITAN V\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nCannot import tensor_util.\r\n\r\n```\r\n$ python\r\nPython 3.6.8 (default, Jan 14 2019, 11:02:34) \r\n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from tensorflow.python.framework import tensor_util\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: cannot import name 'tensor_util'\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nWe can import tensor_util in TF 1.13.\r\n\r\n**Code to reproduce the issue**\r\n\r\nSee above.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Cannot reproduce\r\n\r\n```sh\r\nmihaimaruseac@ankh:/tmp/5$ virtualenv . -p python3.5\r\nRunning virtualenv with interpreter /usr/bin/python3.5\r\nUsing base prefix '/usr'\r\nNew python executable in /tmp/5/bin/python3.5\r\nAlso creating executable in /tmp/5/bin/python\r\nInstalling setuptools, pkg_resources, pip, wheel...source bin/activate\r\npipdone.\r\n\r\nmihaimaruseac@ankh:/tmp/5$ source bin/activate\r\n\r\n(5) mihaimaruseac@ankh:/tmp/5$ pip install tensorflow==1.14.0rc1\r\nCollecting tensorflow==1.14.0rc1\r\n  Downloading https://files.pythonhosted.org/packages/d3/29/cd1f608f260addce161745be44e5696c3d4ef04a9974940bce32787b2711/tensorflow-1.14.0rc1-cp35-cp35m-manylinux1_x86_64.whl (109.2MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 109.2MB 45.0MB/s \r\nCollecting gast>=0.2.0 (from tensorflow==1.14.0rc1)\r\nCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow==1.14.0rc1)\r\n  Using cached https://files.pythonhosted.org/packages/35/3c/c2ac7a2f56045adf1c94869fe012362b51bc8c94ac9e330823d207864904/tensorflow_estimator-1.14.0rc1-py2.py3-none-any.whl\r\nCollecting protobuf>=3.6.1 (from tensorflow==1.14.0rc1)\r\n  Using cached https://files.pythonhosted.org/packages/7c/d2/581ebc3c41879aca2c4fce5c37cdb8d779c4ea79109b6da7f640735ea0a2/protobuf-3.8.0-cp35-cp35m-manylinux1_x86_64.whl\r\nCollecting keras-applications>=1.0.6 (from tensorflow==1.14.0rc1)\r\n  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\r\nCollecting keras-preprocessing>=1.0.5 (from tensorflow==1.14.0rc1)\r\n  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\r\nCollecting astor>=0.6.0 (from tensorflow==1.14.0rc1)\r\n  Using cached https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\r\nCollecting absl-py>=0.7.0 (from tensorflow==1.14.0rc1)\r\nCollecting six>=1.10.0 (from tensorflow==1.14.0rc1)\r\n  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\r\nCollecting wrapt>=1.11.1 (from tensorflow==1.14.0rc1)\r\nCollecting google-pasta>=0.1.6 (from tensorflow==1.14.0rc1)\r\n  Using cached https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl\r\nCollecting termcolor>=1.1.0 (from tensorflow==1.14.0rc1)\r\nCollecting numpy<2.0,>=1.14.5 (from tensorflow==1.14.0rc1)\r\n  Using cached https://files.pythonhosted.org/packages/bb/ef/d5a21cbc094d3f4d5b5336494dbcc9550b70c766a8345513c7c24ed18418/numpy-1.16.4-cp35-cp35m-manylinux1_x86_64.whl\r\nCollecting tensorboard<1.14.0,>=1.13.0 (from tensorflow==1.14.0rc1)\r\n  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.2MB 33.4MB/s \r\nCollecting grpcio>=1.8.6 (from tensorflow==1.14.0rc1)\r\n  Using cached https://files.pythonhosted.org/packages/14/19/f1858ed60786ff681a5f8681448b56bffaaa87a81e9a7ca5cd075a873b35/grpcio-1.21.1-cp35-cp35m-manylinux1_x86_64.whl\r\nRequirement already satisfied: wheel>=0.26 in ./lib/python3.5/site-packages (from tensorflow==1.14.0rc1) (0.33.4)\r\nRequirement already satisfied: setuptools in ./lib/python3.5/site-packages (from protobuf>=3.6.1->tensorflow==1.14.0rc1) (41.0.1)\r\nCollecting h5py (from keras-applications>=1.0.6->tensorflow==1.14.0rc1)\r\n  Using cached https://files.pythonhosted.org/packages/4c/77/c4933e12dca0f61bcdafc207c7532e1250b8d12719459fd85132f3daa9fd/h5py-2.9.0-cp35-cp35m-manylinux1_x86_64.whl\r\nCollecting werkzeug>=0.11.15 (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.14.0rc1)\r\n  Using cached https://files.pythonhosted.org/packages/9f/57/92a497e38161ce40606c27a86759c6b92dd34fcdb33f64171ec559257c02/Werkzeug-0.15.4-py2.py3-none-any.whl\r\nCollecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.14.0rc1)\r\n  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\r\nInstalling collected packages: gast, tensorflow-estimator, six, protobuf, numpy, h5py, keras-applications, keras-preprocessing, astor, absl-py, wrapt, google-pasta, termcolor, werkzeug, grpcio, markdown, tensorboard, tensorflow\r\nSuccessfully installed absl-py-0.7.1 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.21.1 h5py-2.9.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 numpy-1.16.4 protobuf-3.8.0 six-1.12.0 tensorboard-1.13.1 tensorflow-1.14.0rc1 tensorflow-estimator-1.14.0rc1 termcolor-1.1.0 werkzeug-0.15.4 wrapt-1.11.1\r\n\r\n(5) mihaimaruseac@ankh:/tmp/5$ python\r\nPython 3.5.4 (default, Sep  5 2017, 18:32:10) \r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from tensorflow.python.framework import tensor_util\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.14.0-rc1'\r\n>>> tensor_util\r\n<module 'tensorflow.python.framework.tensor_util' from '/tmp/5/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py'>\r\n>>> \r\n```\r\n\r\nMost likely there's some issue on your system?", "@leimao Did you get chance to look on @mihaimaruseac comment. Let us know if that resolves your issue. Thanks!", "Looks like the installation has some problems.\r\n```\r\n$ python3\r\nPython 3.6.8 (default, Jan 14 2019, 11:02:34) \r\n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\r\n>>> \r\n```\r\nI will try to install again.", "Can also try\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf)\r\n```\r\n\r\nAnd that will give you the install location (something like `/tmp/1/lib/python3.6/site-packages/tensorflow/__init__.py`). Then, in the same directory you'll have a `tensorflow-${version}`/`tf-nightly-${version}` directory which gives you the version (for example `/tmp/1/lib/python3.6/site-package/tf-nightly...`)", "> Can also try\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> print(tf)\r\n> ```\r\n> \r\n> And that will give you the install location (something like `/tmp/1/lib/python3.6/site-packages/tensorflow/__init__.py`). Then, in the same directory you'll have a `tensorflow-${version}`/`tf-nightly-${version}` directory which gives you the version (for example `/tmp/1/lib/python3.6/site-package/tf-nightly...`)\r\n\r\nMy output looks funny.\r\n\r\n```\r\n$ python3\r\nPython 3.6.8 (default, Jan 14 2019, 11:02:34) \r\n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\r\n>>> print(tf)\r\n<module 'tensorflow' (namespace)>\r\n```", "what if you print `tf.VERSION` instead?", "> what if you print `tf.VERSION` instead?\r\n\r\n```\r\n$ python3\r\nPython 3.6.8 (default, Jan 14 2019, 11:02:34) \r\n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\r\n>>> print(tf)\r\n<module 'tensorflow' (namespace)>\r\n>>> print(tf.VERSION)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'VERSION'\r\n```\r\n\r\nI will reinstall TensorFlow 1.14 anyway.", "Were you able to resolve this issue? Thanks!", "> Were you able to resolve this issue? Thanks!\r\n\r\nI have not got time to reinstall recently. I will try to make an install tomorrow. Which branch of the repository do you recommend me to use by the way? I am using Ubuntu 18.04 and CUDA 10.1. Thanks.", "For 1.14 should use the `r1.14` branch if you want to install from source.\r\n\r\nRecommend doing a `pip install` if possible, though. Less chances for environment differences to cause issues", "> For 1.14 should use the `r1.14` branch if you want to install from source.\r\n> \r\n> Recommend doing a `pip install` if possible, though. Less chances for environment differences to cause issues\r\n\r\nLooks like you guys have just uploaded a new wheel file for installation via pip. I just installed TensorFlow using pip, and it seems to work for me now. Thanks.\r\n\r\n```\r\n$ python\r\nPython 3.6.8 (default, Jan 14 2019, 11:02:34) \r\n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.14.0'\r\n```", "While running the object detection using tensorflow , i'm getting the below error. I uninstalled and installed the tensorflow and have the latest version. Any help in this regard is appreciated. \r\nThanks much in advance.\r\n\r\n\"AttributeError: module 'wrapt' has no attribute 'ObjectProxy' \"", "Please open a new issue, @periraviteja Please fill in all the details in the issue template and also provide a proper minified reproducer.", "@periraviteja have you been able to solve this error: \"AttributeError: module 'wrapt' has no attribute 'ObjectProxy' \"", "> @periraviteja\u60a8\u662f\u5426\u80fd\u591f\u89e3\u51b3\u6b64\u9519\u8bef\uff1a\u201c AttributeError\uff1a\u6a21\u5757'wrapt'\u6ca1\u6709\u5c5e\u6027'ObjectProxy'\u201c\r\n\r\n\u6211\u4e5f\u9047\u5230\u4e86\u8fd9\u4e2a\u95ee\u9898", "@DS-ejike , @periraviteja I have solved the problem, it due to wrapt using wrong version. Please install `wrapt==1.11.1`\r\n\r\n>`pip install wrapt==1.11.1`"]}, {"number": 29722, "title": "Cherrypicks for Keras SavedModel", "body": "", "comments": []}, {"number": 29721, "title": "R1.11", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29721) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 29720, "title": "Make check_load_test.py and pip_smoke_test.py run on Python 3", "body": "The scripts `check_load_py_test.py` and `pip_smoke_test.py` in `tensorflow/tools/pip_package` currently fail under Python 3 because they call `split()` on a `bytes` object. This PR adds some code to convert the bytes to a UTF-8 string. After the change, the scripts run on Python 2 and 3.", "comments": []}, {"number": 29719, "title": "TF problem when run with lingvo", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Ubuntu 16.04:\r\n- TensorFlow installed from pip:\r\n- TensorFlow 1.13:\r\n- 3.5:\r\n- conda and pip:\r\n- Bazel 0.26.1:\r\n- 4.8:\r\n- CUDA/cuDNN 10.0/ 7.5:\r\n- RTX 2080 TI:\r\n\r\nThis may be not the proper area to ask. But I cannot find any other place to ask. I am running lingvo based on tensorflow. I follow their mnist demo and run:\r\n\r\nbazel build -c opt //lingvo:trainer\r\n\r\nIt gives me an error:\r\n\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nDEBUG: Rule 'subpar' indicated that a canonical reproducible form can be obtained by modifying arguments commit = \"07ff5feb7c7b113eea593eb6ec50b51099cf0261\", shallow_since = \"1524766240 -0700\" and dropping [\"tag\"]\r\nINFO: Analyzed target //lingvo:trainer (37 packages loaded, 4470 targets configured).\r\nINFO: Found 1 target...\r\nERROR: missing input file '@tensorflow_solib//:tensorflow_solib/libtensorflow_framework.so.1'\r\nERROR: /home/feit/.cache/bazel/_bazel_feit/965fc5b73beb9f5ea41273ac0e3737bf/external/tensorflow_solib/BUILD:2:1: @tensorflow_solib//:framework_lib: missing input file '@tensorflow_solib//:tensorflow_solib/libtensorflow_framework.so.1'\r\nTarget //lingvo:trainer failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/feit/.cache/bazel/_bazel_feit/965fc5b73beb9f5ea41273ac0e3737bf/external/tensorflow_solib/BUILD:2:1 1 input file(s) do not exist\r\nINFO: Elapsed time: 9.563s, Critical Path: 0.02s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\nWhat is this about? How can I fix it?", "comments": ["@sun-peach Just to verify, Did you follow this [link](https://github.com/tensorflow/lingvo#quick-start) to install lingvo. Let us know if that works for you. Thanks!", "Yes. But I do the \"installing directly\". I can manage to do the first steps:\r\n\r\nmkdir -p /tmp/mnist\r\nbazel run -c opt //lingvo/tools:keras2ckpt -- --dataset=mnist --out=/tmp/mnist/mnist\r\n\r\nBut after that, the problem pops out.\r\n", "@gadagashwini BTW, I also tried with tensorflow 1.14 (also tf-nightly). The same problem.", "The tensorflow lingvo gihub repo can be a good platform to raise this issue. Please post it on [tensorflow/lingvo](https://github.com/tensorflow/lingvo/issues) . Thanks!", "finaly,how do you solve this problem?"]}, {"number": 29718, "title": "How to generate graph.pbtxt file from the frozen_graph.pb file?", "body": "Hello,\r\nI am trying to import a custom tensor flow model in opencv using the dnn module. Below is the example command:\r\ncvNet = cv2.dnn.readNetFromTensorflow('frozen_inference_graph.pb','graph.pbtxt')\r\nI have the frozen_inference_graph.pb' file? Please tell me how can I generate the 'graph.pbtxt' file?\r\n\r\nBelow is the complete code:\r\n#!/usr/bin/env python\r\nimport cv2\r\n\r\n# Load a model imported from Tensorflowload using OpenCV dnn module\r\ncvNet = cv2.dnn.readNetFromTensorflow('frozen_inference_graph.pb','graph.pbtxt')\r\n\r\n# Input image\r\nimg = cv2.imread(\"image1.jpg\")\r\nrows = img.shape[0]\r\ncols = img.shape[1]\r\n\r\n# Use the given image as input, which needs to be blob(s).\r\ncvNet.setInput(cv2.dnn.blobFromImage(img, size=(300, 300), swapRB=True, crop=False))\r\n\r\n# Runs a forward pass to compute the net output\r\ncvOut = cvNet.forward()\r\n\r\n# Loop on the outputs\r\nfor detection in cvOut[0,0,:,:]:\r\n    score = float(detection[2])\r\n    if score > 0.3:\r\n        left = detection[3] * cols\r\n        top = detection[4] * rows\r\n        right = detection[5] * cols\r\n        bottom = detection[6] * rows\r\n        cv2.rectangle(img, (int(left), int(top)), (int(right), int(bottom)), (23, 230, 210), thickness=2)\r\n\r\ncv2.imshow('img', img)\r\n#save the image\r\ncv2.imwrite( \"processedImg4.jpg\", img );\r\ncv2.waitKey()\r\ncv2.destroyAllWindows()", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there and provide better help to such type of issues. Also for reference you can have a look on this [link](https://jeanvitor.com/tensorflow-object-detecion-opencv/).Thanks!\r\n"]}, {"number": 29717, "title": "Allow ci_build.sh to use CUDA images on non-GPU machines", "body": "The script `tensorflow/tools/ci_build/ci_build.sh` assumes that the `nvidia-docker` utility is present when running with a GPU-enabled Docker image. This assumption makes it difficult to check whether your code will configure, build, and link against CUDA when the machine you're running on doesn't have NVidia's development toolchain installed.\r\n\r\nThis PR modifies `ci_build.sh` slightly so that the script will fall back on `docker` if the program `nvidia-docker` is not available on the host machine. With this version of the script, I'm able to perform a full build against CUDA on a machine with neither CUDA nor a GPU installed by running the command:\r\n```\r\ntensorflow/tools/ci_build/ci_build.sh GPU bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nI'm also able to run the configuration checks in `tensorflow/tools/ci_build/ci_sanity.sh` on the same machine without errors by running:\r\n```\r\ntensorflow/tools/ci_build/ci_build.sh GPU tensorflow/tools/ci_build/ci_sanity.sh\r\n```", "comments": []}, {"number": 29716, "title": "[ROCm] Fix for the broken `--config=rocm` build", "body": "The following commit broken the `--config=rocm` build when it was merged\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/754ac36f54db34d303a60eb08d34199a7945e576\r\n\r\nThe above commit uses the Cuda* version of GPU utility APIs. The Cuda* names are not visible in the ROCm build and hence the breakage.  Moving forward it is required for all new code to use the Gpu* names when using the GPU utility routines.\r\n\r\n----------------------------------\r\n\r\n@tatianashp @whchung ", "comments": ["/cc @chsigg - FYI."]}, {"number": 29715, "title": "Session crashed when I use TFLiteConverter with tf.lite.OpsSet.TFLITE_BUILTINS_INT8 option.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colaboratory (Ubuntu 18.04.2 LTS)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): pre-installed\r\n- TensorFlow version (use command below): 2.0.0-beta0\r\n- Python version: Python 3.6.7\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source) NA:\r\n- CUDA/cuDNN version: CUDA v10.0.130, cuDNN 7.6.0 \r\n- GPU model and memory: K80\r\n\r\n**Describe the current behavior**\r\nI tried to build fully quantized AutoEncoder model using TF2.0 beta and keras on [Colaboratory](https://colab.research.google.com/gist/ohtaman/09c66e27f240d151f6f17ac4a9f62e54/-post-training-integer-quantization-error.ipynb)\r\nBut when I run TFLiteConverter.convert method, the Jupyter kernel crashes and the session is restarted.\r\n\r\n**Describe the expected behavior**\r\n\r\nFinish conversion without any errors.\r\n\r\n**Code to reproduce the issue**\r\n\r\nPlease check [Colaboratory](https://colab.research.google.com/gist/ohtaman/09c66e27f240d151f6f17ac4a9f62e54/-post-training-integer-quantization-error.ipynb)\r\n\r\n**Other info / logs**\r\n\r\n```\r\nJun 13, 2019, 6:47:41 AM | WARNING | WARNING:root:kernel dd551b22-18b1-4c36-917e-0dc4b698c41e restarted\r\n-- | -- | --\r\nJun 13, 2019, 6:47:41 AM | INFO | KernelRestarter: restarting kernel (1/5), keep random ports\r\nJun 13, 2019, 6:47:38 AM | WARNING | what(): _Map_base::at\r\nJun 13, 2019, 6:47:38 AM | WARNING | terminate called after throwing an instance of 'std::out_of_range'\r\nJun 13, 2019, 6:47:38 AM | WARNING | INFO: Initialized TensorFlow Lite runtime.\r\n```", "comments": ["Tried executing the attached Colab code snippet with TF GPU version 2.0.0-beta  and was able to replicate the issue. ", "Hmm, when I run the colab just now and it works fine for me, am i reproducing the error incorrectly? \r\n\r\n![githubcolab](https://user-images.githubusercontent.com/1450614/59544225-01b74400-8ec5-11e9-9e8c-2b5452b9e5d0.png)\r\n\r\nCan you verify that this is still an issue?", "Yes, session crashes in my environment. The lower left Japanese message means it.\r\nI also checked that same error appears with tensorflow-gpu-2.0-beta1 .\r\n\r\n![image](https://user-images.githubusercontent.com/329750/59547868-b2364e00-8f80-11e9-8e7f-b3386613e1db.png)\r\n\r\nI found that the cause of this error is the upsampling implemented by myself.\r\nIf I use `keras.layers.UpSampling2D(2, interpolation='bilinear')`, it works well.\r\nIn the colab notebook, I implemented upsampling by myself because UpSampling2D with interpolation='nearest' is not supported by the quantization tool yet.\r\n\r\n```python\r\n    def upsampling(x):\r\n        x = keras.backend.repeat_elements(x, scale[0], axis=1)\r\n        x = keras.backend.repeat_elements(x, scale[1], axis=2)\r\n        return x\r\n```", "@ohtaman I am closing the issue as it was resolved in `tf-nightly`. Please feel free to open it if the issue persists again. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/701764218387356183515f772bbef27f/-post-training-integer-quantization-error.ipynb). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29715\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29715\">No</a>\n"]}, {"number": 29714, "title": "Polynomial Decay Document Page question", "body": "\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/train/polynomial_decay\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/train/polynomial_decay\r\n## Description of issue (what needs changing):\r\nA clarification of the example and why it actually works.\r\n\r\n### Clear description\r\nIn the example mentioned, if the global step is 0, it is unclear how the learning rate will actually change.\r\n\r\nThe formula can be rewritten as follows:\r\n\r\nd = (l - e) * (1 - g/s)^p + e\r\n\r\nIn the example, g = 0, which means the formula becomes (l - e) * 1 + e = l - e + e = l\r\n\r\nSo, I'm very unsure of why/how the learning rate in this example is actually going to decrease.\r\n\r\n", "comments": ["```global_step``` is an iteration. Thus when we set it to zero there should be no change since we didn't iterate over the batch. The formula to compute ```polynomial_decay``` does not accept negative values for ```global_step``` which makes sense since we cannot have negative iterations.", "If your explanation is correct, I am not sure how the documentation example reflects this:\r\n\r\nExample: decay from 0.1 to 0.01 in 10000 steps using sqrt (i.e. power=0.5):\r\n\r\nHowever, when you inspect the code segment, it is very clear that the global step is 0 and remains 0, which implies that you're not actually changing the learning rate. Then how is the example correct?"]}, {"number": 29713, "title": "Can't import tensorflow", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (Windows 7):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6\r\n- Installed using virtualenv? installed using pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: Geforce GTX 550 Ti 2811mb\r\n\r\n\r\n\r\n**Hello everyone. I just started to learn machine learning with python, and when I try to import tensorflow, I keep getting the same error:**\r\n\r\nPython 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)] on win32\r\nrunfile('D:/Users/Lucas/PycharmProjects/TensorENV/test.py', wdir='D:/Users/Lucas/PycharmProjects/TensorENV')\r\nTraceback (most recent call last):\r\n  File \"D:\\Users\\Lucas\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.1.3\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"D:\\Users\\Lucas\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Users\\Lucas\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Users\\Lucas\\Anaconda3\\envs\\tensor\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Users\\Lucas\\Anaconda3\\envs\\tensor\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.1.3\\helpers\\pydev\\_pydev_bundle\\pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.1.3\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"D:/Users/Lucas/PycharmProjects/TensorENV/test.py\", line 1, in <module>\r\n    import tensorflow\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.1.3\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"D:\\Users\\Lucas\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.1.3\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"D:\\Users\\Lucas\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.1.3\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"D:\\Users\\Lucas\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Users\\Lucas\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.1.3\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"D:\\Users\\Lucas\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Users\\Lucas\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Users\\Lucas\\Anaconda3\\envs\\tensor\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Users\\Lucas\\Anaconda3\\envs\\tensor\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\nFailed to load the native TensorFlow runtime.\r\n\r\n\r\n", "comments": ["@lmdrezende Just to verify, Did you follow the steps mentioned in official website of [Tensorflow](https://www.tensorflow.org/install/pip). Let us know if that helps. Thanks!", "Yeah. I installed the cpu-only version in a virtual environment for python 3.6 through pip install. But I just unistalled everything and I will try once more and get back here with the results. Thank you.", "Nit: When posting code, or errors, please wrap multiple lines of that with triple backquotes, so that formatting is nice and output is easy to read.", "I managed to get it to work. Instead of using pip to install tensorflow, I tried with conda install and somehow it worked. Thanks everyone.", "What if I don't want to use conda"]}, {"number": 29712, "title": "Delay the tf.gather using the batch_dims enabled C++ kernel.", "body": "PiperOrigin-RevId: 252687731", "comments": ["> Wait I don't think this change will fix it since the release branch has the forward compat date in november: https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/compat/compat.py\r\n> \r\n> Can we change this to after november?\r\n\r\nGood catch - moved it to Nov 10. Thanks!"]}, {"number": 29711, "title": "Make N_CPUS check in ci_sanity.sh work on MacOS", "body": "The pylint sections of `tensorflow/tools/ci_build/ci_sanity.sh` do not run on MacOS because they contain code that assumes the existence of `/proc/cpuinfo`.\r\n\r\nThis is a problem for Mac users because pylint is extremely slow on Docker on Mac -- over 12 hours to to run a single pass of linting on my machine. The same pylint check runs in 15 minutes outside of Docker.\r\n\r\nThe changes in this PR allow the linting portions of `ci_sanity.sh` to run on MacOS with Docker. After my modifications, `ci_sanity.sh` uses the POSIX standard command `getconf` to query the number of cores if `/proc/cpuinfo` does not exist.\r\n\r\nTo get reliable results when running pylint locally, users will need a Python environment that is close to that of the TensorFlow Docker images. I have written a script that will set up such an environment. My script is currently checked into my TensorFlow config files repository at [https://github.com/frreiss/tf-dev-conf/blob/master/lintenv.sh](https://github.com/frreiss/tf-dev-conf/blob/master/lintenv.sh). I would be happy to contribute this script if there's interest.\r\n", "comments": []}, {"number": 29710, "title": "Fix resource variable leak after Lookup", "body": "PiperOrigin-RevId: 252682806", "comments": []}, {"number": 29709, "title": "Export FeatureColumn and subclasses", "body": "**System information**\r\n\r\n- TensorFlow version (you are using): 2.0.0b0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nTensorFlow should export the `FeatureColumn` class and all classes that are subclasses of it. Currently, only the functions used to construct objects of these types, such as `numeric_column`, are exported.\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes, it will change the current api by extending it.\r\n\r\n**Who will benefit with this feature?**\r\n\r\n- Developers who want to extend the provided `FeatureColumn` functionality, such as by subclassing `FeatureColumn` or by creating new functions to instantiate members of its existing subclasses\r\n- Developers who document and/or verify their code using type annotations\r\n\r\n**Any Other info.**\r\n", "comments": ["@novog,\r\nSorry for the delayed response. In **`Tensorflow Version 2.x`**, as we recommend [TF Keras](https://www.tensorflow.org/api_docs/python/tf/keras) over [Estimators](https://www.tensorflow.org/guide/estimator) and the [Saved Model](https://www.tensorflow.org/guide/saved_model) over [Export Functionality](https://www.tensorflow.org/api_docs/python/tf/compat/v1/keras/experimental/export_saved_model), can you please let us know if this **`functionality`** is still relevant? Thanks!", "To be clear, by \"export\", I meant in the sense of making a Python class available in user code, not in the sense of saving models to disk. However, based on your comment about Estimators, I am guessing that Keras [Preprocessing Layers](https://keras.io/guides/preprocessing_layers/) are now preferred over `FeatureColumn`. Closing."]}, {"number": 29708, "title": "Fix docstring for tf.nn.bias_add()", "body": "", "comments": []}, {"number": 29707, "title": "Make pImpl class private in ThreadPool", "body": "", "comments": []}, {"number": 29706, "title": "Prevent spurious wake up of cond var", "body": "", "comments": []}, {"number": 29705, "title": "Prevent spurious wakeup of condition variable", "body": "", "comments": []}, {"number": 29704, "title": "Tensorflow 2.0 for raspberry pi installation", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian GNU/Linux 9 (stretch)\r\n- TensorFlow version: Tried tensorflow 2.0 alpha and tensorflow 2.0 beta\r\n- Python version: 3.5.3\r\n- Installed using virtualenv? pip? conda?: virtual environmet and pip\r\n\r\n**Describe the problem**\r\n\r\nCan not install tensorflow 2.0 with pip install. The pip install command doesn't find the distribution package\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nOn a raspberry py with raspbian 9:\r\n\r\n- Create virtualenv\r\n- Activate virtualenv\r\n- Use command: pip install tensorflow==2.0.0-beta0 or pip install tensorflow==2.0.0-a0\r\n\r\n**Any other info / logs**\r\n\r\n ERROR: Could not find a version that satisfies the requirement  tensorflow==2.0.0-beta0 (from versions: 0.11.0, 1.8.0, 1.9.0, 1.10.1, 1.11.0)\r\nERROR: No matching distribution found for  tensorflow==2.0.0-beta0\r\n", "comments": ["I have the same issue when trying to install TensorFlow 2.0.0-rc0 via pip3, Raspberry Pi 3 B+ with raspbian 10 (buster):\r\n\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\nCollecting tensorflow==2.0.0-rc0\r\n  Could not find a version that satisfies the requirement tensorflow==2.0.0-rc0 (from versions: 0.11.0, 1.12.0, 1.13.1)\r\nNo matching distribution found for tensorflow==2.0.0-rc0", "I've tried this method:\r\n\r\nhttps://www.tensorflow.org/install/source_rpi\r\n\r\n```\r\ngit checkout r2.0\r\n```\r\n\r\nUsing python3 for the target.\r\n\r\nBut it fails:\r\n\r\n```\r\nSuccessfully built 798896a6c5b6\r\nSuccessfully tagged tf_ci.pi-python3:latest\r\nRunning 'tensorflow/tools/ci_build/pi/build_raspberry_pi.sh' inside tf_ci.pi-python3...\r\nReading package lists...\r\nBuilding dependency tree...\r\nReading state information...\r\nsudo is already the newest version (1.8.16-0ubuntu1.7).\r\n0 upgraded, 0 newly installed, 0 to remove and 9 not upgraded.\r\naddgroup: Please enter a username matching the regular expression configured\r\nvia the NAME_REGEX[_SYSTEM] configuration variable.  Use the `--force-badname'\r\noption to relax this check or reconfigure NAME_REGEX.\r\n```\r\n\r\nmacOS 10.14.6\r\ndocker desktop 2.1.0.2", "> I've tried this method:\r\n> \r\n> https://www.tensorflow.org/install/source_rpi\r\n> \r\n> ```\r\n> git checkout r2.0\r\n> ```\r\n> \r\n> Using python3 for the target.\r\n> \r\n> But it fails:\r\n> \r\n> ```\r\n> Successfully built 798896a6c5b6\r\n> Successfully tagged tf_ci.pi-python3:latest\r\n> Running 'tensorflow/tools/ci_build/pi/build_raspberry_pi.sh' inside tf_ci.pi-python3...\r\n> Reading package lists...\r\n> Building dependency tree...\r\n> Reading state information...\r\n> sudo is already the newest version (1.8.16-0ubuntu1.7).\r\n> 0 upgraded, 0 newly installed, 0 to remove and 9 not upgraded.\r\n> addgroup: Please enter a username matching the regular expression configured\r\n> via the NAME_REGEX[_SYSTEM] configuration variable.  Use the `--force-badname'\r\n> option to relax this check or reconfigure NAME_REGEX.\r\n> ```\r\n> \r\n> macOS 10.14.6\r\n> docker desktop 2.1.0.2\r\n\r\nI get similar results as well on mac os \r\n`git checkout v2.0.0-rc2`\r\nUsing the python3 target outputs \r\n\r\n![Screenshot 2019-10-02 at 10 58 04](https://user-images.githubusercontent.com/6953881/66035719-9476ff80-e503-11e9-85ca-402c62d65a59.png)\r\n\r\nmacOS 10.14.5\r\nDocker desktop 2.1.0.3\r\n\r\nOn windows it dosn't even get to that point (I'm on my mac right now. I will post the windows results later).\r\n\r\n\r\n", "A bit more digging and I found this guy \r\nhttps://github.com/PINTO0309/Tensorflow-bin\r\nthat was able to compile the arm version of TensorFlow 2 but I haven't checked it out yet.", "I have never been able to generate a healthy binary with the official procedure. So I always do native builds with RaspberryPi. RaspberryPi4 (Buster) can be built in about 12 hours. glibc2.29 and openjdk-8-jdk are required to use my Tensorflow binary, so please read the usage section carefully.\r\n**https://github.com/PINTO0309/Tensorflow-bin#usage**\r\nPlease note that the wheel file I created is unofficial.\r\nThe build parameters are as follows.\r\n**https://github.com/PINTO0309/Tensorflow-bin#build-parameter**", "Is there any installation of TensorFlow 2.0 for Raspberry Pi?\r\n\r\nBuilding does not work.\r\n\r\nWhen will there be a pip wheel?", "@gkvoelkl \r\nI updated Wheel today. Import confirmed in Raspbian Buster.\r\n**https://github.com/PINTO0309/Tensorflow-bin/raw/master/tensorflow-2.0.0-cp37-cp37m-linux_armv7l.whl**", "Is there an estimate to when this might be fixed? I'm at a point where I'm needing to make a use/don't use TensorFlow decision with respect to the Raspberry Pi 4. The oldness of this report, combined with the incorrect official documentation that indicates 2.0 is supported through \"pip install tensorflow\", leaves me in a tight spot with regard to moving forward with my project.", "@dougparkarosa The PINTO0309 repo seems to have up to date packages. But, of course, it's not the official distribution, if that's what you're looking for.\r\n\r\nhttps://github.com/PINTO0309/Tensorflow-bin/", "@FlorinAndrei @dougparkarosa\r\nSince the official wheel will not be released indefinitely, it was generated on its own. I also created it to speed up Tensorflow Lite. If you need to use the official one, you have to wait a lot.", "Tried PINTO0309 package on my Pi 4. Ran the Fashion tutorial and it seemed to work. Can't say how solid it is at this point, but looks promising. And probably enough to keep me moving forward, for now.", "Tried PINTO0309 package On my pi B+. Same success there.", "PINTO0309 package worked for me as well, thank you! Pi B+ Raspbian 10 Buster. At first, when importing TensorFlow, I saw the error \r\n\"libf77blas.so.3: cannot open shared object file: No such file or directory\"\r\nbut `sudo apt-get install libatlas-base-dev` solved the problem - that's probably unrelated to the issue at hand, just mentioning this in case somebody else has the same problem.", "I'll let you know just in case. My repository README already mentions installing libatlas-base-dev for usage.\r\n**https://github.com/PINTO0309/Tensorflow-bin/#usage**", "Oh thanks! I overlooked that, I just pip-installed the .whl file you linked to above, which actually caused a problem because I hadn't installed h5py. But by reading your README I was able to resolve that problem! Thanks again!", "@PINTO0309 \r\nI've tried as the usage you had suggested but found errors in the final step installing .whl file. It looks I have problem building scipy package. Anyone knows how to fix the problem? Followings are some error messages:\r\n\r\nBuilding wheels for collected packages: scipy\r\n  Building wheel for scipy (PEP 517) ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /usr/local/bin/python3.7 /usr/local/lib/python3.7/site-packages/pip/_vendor/pep517/_in_process.py build_wheel /tmp/tmpwtronura\r\n       cwd: /tmp/pip-install-r2qva1ao/scipy\r\n  Complete output (4511 lines):\r\n  lapack_opt_info:\r\n  lapack_mkl_info:\r\n  customize UnixCCompiler\r\n    libraries mkl_rt not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/arm-linux-gnueabihf']\r\n    NOT AVAILABLE\r\n\r\n  openblas_lapack_info:\r\n  customize UnixCCompiler\r\n  customize UnixCCompiler\r\n    libraries openblas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/arm-linux-gnueabihf']\r\n    NOT AVAILABLE\r\n\r\n  openblas_clapack_info:\r\n  customize UnixCCompiler\r\n  customize UnixCCompiler\r\n    libraries openblas,lapack not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/arm-linux-gnueabihf']\r\n    NOT AVAILABLE\r\n", "@aeropia\r\nPlease refer to the following issue. Solves the same problem as you. Fixed repository README.\r\n**[Can't install 2.1.0 on Python 3.7 Raspberry Pi 4 #19](https://github.com/PINTO0309/Tensorflow-bin/issues/19)**", "@PINTO0309 \r\nIt looks the issue has been resolved and now being installed. Thanks! ", "@PINTO0309 \r\nI have another error message : ERROR: tensorboard 2.1.0 has requirement setuptools>=41.0.0, but you'll have setuptools 40.6.2 which is incomp. \r\nIs this an error expected?", "@aeropia\r\nTry the following:\r\n```\r\n$ sudo pip3 install setuptools --upgrade\r\n```\r\n\r\n**https://pypi.org/project/setuptools/**", "@PINTO0309 \r\nGot it. Thanks again!", "Still not fixed!\r\nwhen I run `pip3 install tensorflow` it installs v1.14.0!\r\nWhen I try to run `pip3 install tensorflow==2.1.0` I get the described error.\r\n\r\nSystem:\r\nRaspbian 10 buster,\r\npython version: 3.7.3,\r\npip version: 20.0.2", "Plus, the installation guide on official tensorflow website for the version 2 says it supports 64-bit system of raspbian 9+. What's that mean? I don't think raspbian as of now is 64-bit yet.", "Are you attempting to train TF 2.x models on a Raspberry Pi, or just execute them? If the latter, [TF Lite supports TF 2.x SavedModels](https://www.tensorflow.org/lite/guide/build_rpi).", "@Risslock \r\nIs this still an issue.", "Tensorflow 2.x is still not available as pre-built package for the Raspberry Pi. I'd say it's still an issue.", "> Tensorflow 2.x is still not available as pre-built package for the Raspberry Pi. I'd say it's still an issue.\r\n\r\n\r\n\r\n> @Risslock\r\n> Is this still an issue.\r\n\r\nAs KopfKrieg said. Even though you can compile it, it is still not available as pre-built package for the Raspberry Pi\r\n", "When will it be available?", "Update: There *are* pre-built packages for the Raspberry Pi available, the urls (not even links you can just click!) are at the bottom of this page: https://www.tensorflow.org/install/pip\r\n\r\nThose packages aren't available via pip/piwheels, though (and that's the real issue for me)\r\n\r\n**Edit:** By the way, today I tried compiling the package according to the linked page but it didn't work. And I didn't have time to take a closer look at the issue :/", "Why is no official `.whl` package for the Raspberry Pi 4 available? => https://www.tensorflow.org/install/pip", "Update to my previous comment: Building the Tensorflow 2.x package is not possible as described in the official docs, it will only result in an error. Installing the provided package is also not possible on a current Raspbian 10 OS because it's not compatible with Python 3.7.\r\n\r\nTl;dr: Still no official way of installing Tensorflow 2.x.\r\n\r\n> Why is no official `.whl` package for the Raspberry Pi 4 available? => https://www.tensorflow.org/install/pip\r\n\r\nThe package of the Raspberry Pi 3 should work on the RPi 4 too (unfortunately it currently doesn't because it's not for Python 3.7)", "@KopfKrieg What error are you seeing when trying to build? I'm experiencing an error when trying to build as well, and I'm wondering if it's the same one.\r\n\r\nI'm getting  the following (also described in issue #44028)\r\n```\r\naddgroup: Please enter a username matching the regular expression configured\r\nvia the NAME_REGEX[_SYSTEM] configuration variable.  Use the `--force-badname'\r\noption to relax this check or reconfigure NAME_REGEX.\r\n```", "> What error are you seeing when trying to build? I'm experiencing an error when trying to build as well, and I'm wondering if it's the same one.\r\n\r\nNo idea, sorry. I currently don't have time to look further into that issue, all I know is building/installing Tensorflow 2.x didn't work for me. \r\n\r\n**EDIT:** Not sure if it should be part of this issue, but I tried to installed [the official file from here](https://storage.googleapis.com/tensorflow/raspberrypi/tensorflow-2.3.0-cp35-none-linux_armv7l.whl) on Debian 9/Buster (Python 3.5), but couldn't get it working because it also needs SciPy 1.4.x, which couldn't be built on my RPi 3 (first running out of RAM, and after adding SWAP another issue occurred. Unfortunately I can't look into that further).", "Update: I was able to build Tensorflow 2.4 for Python 3.5 for the Raspberry Pi 3 using the docker method (and it only took 8 hours...). Unfortunately, Tensorflow 2.4 requires Numpy 1.19.2 or higher, but Numpy dropped Python 3.5 support in the 1.19.x release. Python 3.5 is also now unsupported.", "### tldr: get your tf2 wheel from https://github.com/lhelontra/tensorflow-on-arm/releases\r\n\r\nFor future reference, this setup does not need to build any package from source, by pointing pip to arm wheels:\r\n\r\n```Dockerfile\r\n# no need to add the 'arm32v7/' prefix when using something like docker buildx --platform=\"linux/arm/v7\"\r\n# because the plain debian tags are multi-arch https://hub.docker.com/_/debian?tab=tags\r\nFROM arm32v7/debian:buster-slim\r\n\r\n# install python3.7 and expose latest pip\r\nRUN apt-get update && \\\r\n    apt-get install -y --no-install-recommends python3-pip && \\\r\n    pip3 install -U --no-cache-dir pip && \\\r\n    # cleanup\r\n    rm -rf /var/lib/apt/lists/*\r\n\r\n# avoid source distributions as much as possible by looking for wheels at piwheels.org\r\nENV PIP_EXTRA_INDEX_URL=https://piwheels.org/simple\r\n\r\n# neither pypi.org/project/tensorflow nor piwheels.org/project/tensorflow has tf2 wheels for armv7l\r\n# but luckily piwheels.org has wheels for numpy, scipy etc and github.com/lhelontra/tensorflow-on-arm has a tf2 wheel\r\nRUN pip install --no-cache-dir https://github.com/lhelontra/tensorflow-on-arm/releases/download/v2.4.0/tensorflow-2.4.0-cp37-none-linux_armv7l.whl\r\n```", "> ### tldr: get your tf2 wheel from https://github.com/lhelontra/tensorflow-on-arm/releases\r\n> For future reference, this setup does not need to build any package from source, by pointing pip to arm wheels:\r\n> \r\n> ```dockerfile\r\n> # no need to add the 'arm32v7/' prefix when using something like docker buildx --platform=\"linux/arm/v7\"\r\n> # because the plain debian tags are multi-arch https://hub.docker.com/_/debian?tab=tags\r\n> FROM arm32v7/debian:buster-slim\r\n> \r\n> # install python3.7 and expose latest pip\r\n> RUN apt-get update && \\\r\n>     apt-get install -y --no-install-recommends python3-pip && \\\r\n>     pip3 install -U --no-cache-dir pip && \\\r\n>     # cleanup\r\n>     rm -rf /var/lib/apt/lists/*\r\n> \r\n> # avoid source distributions as much as possible by looking for wheels at piwheels.org\r\n> ENV PIP_EXTRA_INDEX_URL=https://piwheels.org/simple\r\n> \r\n> # neither pypi.org/project/tensorflow nor piwheels.org/project/tensorflow has tf2 wheels for armv7l\r\n> # but luckily piwheels.org has wheels for numpy, scipy etc and github.com/lhelontra/tensorflow-on-arm has a tf2 wheel\r\n> RUN pip install --no-cache-dir https://github.com/lhelontra/tensorflow-on-arm/releases/download/v2.4.0/tensorflow-2.4.0-cp37-none-linux_armv7l.whl\r\n> ```\r\n\r\nHello, I've had the same problem happen to me and I've seen your comment.\r\nI'm not very experienced with managing packages and using wheels so I am saying sorry in advanced for the noob question.\r\nwhat does the first line mean?\r\n`FROM arm32v7/debian:buster-slim` isn't a line I can execute in the terminal \r\nthe only line which is executable for me is `pip install --no-cache-dir https://github.com/lhelontra/tensorflow-on-arm/releases/download/v2.4.0/tensorflow-2.4.0-cp37-none-linux_armv7l.whl`\r\nin addition when I do run this line I get back in response `tensorflow-2.4.0-cp37-none-linux_armv7l.whl is not a supported wheel on this platform.`\r\n\r\nmy system information is as follows:\r\nraspberry pi 3 model B\r\npython 3.7.3\r\npip 21.0.1\r\ngcc version 8.3.0 (Raspbian 8.3.0-6+rpi1) \r\n\r\nthe python already being 3.7 and pip being up to date mean I can skip the second phase?\r\n\r\nThanks in advance for the response.", "Hi @COfek, the above snippet is a Dockerfile, so not intended to be interpreted by a shell. For you indeed only the last RUN statement is relevant. The ENV statement with piwheels.org is set by default on Raspbian, so only needed within docker: you can ignore it. Maybe you're running an ARM v6 kernel. Try changing the wheel link from v7 to v6, ~maybe that helps~.\r\n\r\nEdit: this is unlikely. However, might you be [running](https://raspberrypi.stackexchange.com/a/77707/105000) a v8 kernel? ", "@ddelange uname -r returns 5.10.14-v7+", "@ddelange The problem is those are not official packages. I prefer/require packages that are either available from piwheels.org or from tensorflow.org, but not from a random repository. I could also build the package myself (as explained in a previous comment, using the docker method), but it'd be way easier if the tensorflow team simply provides official whl-files.\r\n\r\n**EDIT:** Building Tensorflow with the current build scripts also creates incompatible dependencies. When built for Python 3.7 it uses NumPy 20.x, but if I want to install the package on my Raspberry Pi it wants 19.x. However, if I then try to run my code Tensorflow quits because it was built with a different NumPy version.", "Providing official wheels means someone has to do maintenance for those. We currently don't have headcount for this position.", "Hi @Risslock!\r\nWe are checking to see if you still need help in this issue , Have checked these [thread ](https://projectszonepk.blogspot.com/2021/10/object-detection-using-raspberrypi.html)yet ? Thanks!", "> Hi @Risslock! We are checking to see if you still need help in this issue , Have checked these [thread ](https://projectszonepk.blogspot.com/2021/10/object-detection-using-raspberrypi.html)yet ? Thanks!\r\n\r\nHi @mohantym!\r\nThanks for the follow-up. We were able to use TFLite on raspberry using the balenaOS for raspberry.\r\n\r\nBest!", "Ok @Risslock! Thanks for confirming the same.Could you please close this issue then? ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29704\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29704\">No</a>\n"]}, {"number": 29703, "title": "Move training_op_helpers to framework", "body": "This PR moves training_op_helpers from [`tensorflow/core/kernels`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/) to [`tensorflow/core/framework`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/), to support people writing new C++ optimizers. \r\n\r\nSolves issue: https://github.com/tensorflow/tensorflow/issues/27899", "comments": ["Looks fine to me based on comments in that issue, adding @alextp as FYI.", "This might get tricky to do, because the dependencies on framework -> core/kernels/{dense_update_functor,variable_ops} -> framework causes a circular dependency...", "> This might get tricky to do, because the dependencies on framework -> core/kernels/{dense_update_functor,variable_ops} -> framework causes a circular dependency...\r\n\r\nCould moving training_op_helpers to util be a viable solution?", "Putting the helpers in their own separate library might work, yes. This doesn't really care whether they are in the framework directory or the util directory. I prefer the framework directory.", "The latest commit should finally remove the circular dependency.", "Thanks, I think I also fixed the problem with the AdroidDemoApp.", "@vcarpani can you please check build failures", "> @vcarpani can you please check build failures\r\n\r\nI will do it later this week ;)", "Can one of the admins verify this patch?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@vcarpani Why was this closed? Seems pretty useful, imo. ", "I closed it because because I did not have time for continuing it; I could pick it up in the near future, especially if someone is interested in helping", "@vcarpani We need it for TensorFlow Addons. I'll check out your code and make the needed changes. Just wanted to know if there was any other reason for closing this PR. Thanks for the clarification. ", "@Squadrick ok, I'll reopen this PR and push back the changes to my fork, so we can use it as base for the changes ;)", "@vcarpani Thanks so much. ", "Closing since it has been moved to:\r\nhttps://github.com/tensorflow/tensorflow/pull/37873"]}, {"number": 29702, "title": "small typo", "body": "", "comments": ["@mihaimaruseac Thank you. Closing this PR. Thanks!"]}, {"number": 29701, "title": "ERROR: Using multiple TPUs in a single session is not yet implemented", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9\r\n- TensorFlow installed from (source or binary): preinstalled\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.5\r\n\r\n**Describe the current behavior**\r\n\r\nError when trying to use 2 x TPUs v3 in TPUClusterResolver\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error\r\n\r\nModel\r\n```\r\nTPU_ADDRESS1 = 'grpc://10.240.1.2:8470'\r\nTPU_ADDRESS2 = 'grpc://10.240.2.2:8470'\r\n   \r\nopt = tf.train.AdamOptimizer(0.01)\r\n\r\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\r\n\r\ntpu_model = tf.contrib.tpu.keras_to_tpu_model(model, \r\n        strategy=tf.contrib.tpu.TPUDistributionStrategy(\r\n            tf.contrib.cluster_resolver.TPUClusterResolver(tpu = [TPU_ADDRESS1,TPU_ADDRESS2])))\r\n```\r\nError\r\n```\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu_cluster_resolver.py in __init__(self, tpu, zone, project, job_name, coordinator_name, coordinator_address, credentials, service, discovery_url)\r\n    215       if len(tpu) != 1:\r\n    216         raise NotImplementedError(\r\n--> 217             'Using multiple TPUs in a single session is not yet implemented')\r\n    218       tpu = tpu[0]\r\n    219 \r\n\r\nNotImplementedError: Using multiple TPUs in a single session is not yet implemented\r\n```\r\n", "comments": ["Sorry to resurrect a year old issue, but I'm running into the same limitations. \r\n\r\nAre there any plans to support multiple TPUs not in a pod configuration?", "> Are there any plans to support multiple TPUs not in a pod configuration?\r\n\r\ndoes TPU pod make a difference? could you please elaborate\r\n\r\n", "Please check https://cloud.google.com/tpu/docs/training-on-tpu-pods", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29701\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29701\">No</a>\n"]}, {"number": 29700, "title": "[TF 2.0] tf.linalg.inv", "body": "**System information**\r\n- TensorFlow version: '2.0.0-dev20190612'\r\n- Python version: 3.6.7\r\n\r\n**Describe the current behavior**\r\nI am taking the inverse of a matrix with a large condition number, see the code for the matrix example, when I try tf.linalg.inv(A) I get the error: InvalidArgumentError: Input is not invertible. [Op:MatrixInverse]\r\n\r\n**Describe the expected behavior**\r\nI don't think that is the correct error, because the Matrix can be inverted because the determinant is not exactly zero and TF gets it as zero due to the way tf.linalg.det is implemented. But if you try the same in numpy or Matlab, we can get the inv, in numpy you get the inv and in Matlab, you get the inv with a warning. Again, the determinant is very small (2.1934511e-08), for all purposes it can be assumed to be zero, so in the least, the error can be more of a warning rather than a not invertible error.\r\n\r\nAlso, I am curious to know how precise TF calculations are or after how many significant figure something is rounded to be zero. If you have any resources or links so I can check it out, would really appreciate it.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nA = tf.constant([[4.1, 2.8], [9.676, 6.608]], dtype=tf.float32)\r\nAinvnp = np.linalg.inv(A)\r\nprint(A2inv)\r\n\r\nAinvtf = tf.linalg.inv(A)\r\nprint(Ainvtf)\r\n```\r\n", "comments": ["I have tried on Colab with TensorFlow version 2.0.0-dev20190612 and was able to get inverse of matrix when trying with numpy but got the mentioned error when trying with \"tf.linalg.inv\"", "any update?\r\n", "If the condition number of your matrix approaches ```1/std::numeric_limits<T>::epsilon()``` then it is \"numerically singular\", or \"numerically indistinguishable\" from a singular matrix. \r\n\r\nAs you mention, Matlab warns you if it determines that you are in this regime, since this means that the results returned by numpy.linalg.inv or inv in Matlab are essentially garbage (well, it turns out they are not entirely random, and you _can_ solve ill-conditioned systems to your advantage, if you know what you are doing, e.g. when computing eigenvectors of a matrix using inverse iteration. But that's beyond the scope of this issue, I think.)\r\n\r\nIn TensorFlow we chose to fail with an error in this case, instead of returning garbage. The TF error only occurs when zeros appear in the diagonal of the underlying LU decomposition, which is not nearly as reliable as the condition number estimation approach used by Matlab. I contributed a condition estimator to the Eigen library used by TF on CPU, and we should probably consider using it.\r\n\r\nMy advice would be: \r\n\r\n1. Don't compute the inverse of a matrix unless you absolutely have to. If you are really solving a linear system of equations, consider using tf.linalg.solve. Use tf.linalg.solve_ls, if you are solving a least-squares problem or an under-determined linear system.\r\n2. If you think you need the inverse for an ill-conditioned matrix, you most likely want the pseudo-inverse provided by tf.linalg.pinv.\r\n3. If you really want to work with an ill-conditioned matrix, use tf.linalg.svd.\r\n\r\nI would recommend picking up a textbook on numerical linear algebra and read about (backwards) error analysis to better understand what the limitations of these algorithms are. Personally I really like Jim Demmel's book: https://epubs.siam.org/doi/book/10.1137/1.9781611971446?mobileUi=0\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29700\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29700\">No</a>\n", "Hello guys,\r\nMy simple question is that why the Matlab running on my laptop (Intel, core i5, 8G ram) computes the inverse faster as compared to the Google TPU. Code attached...\r\nMatlab code:\r\nA = rand(3000,3000);\r\ntic\r\ninv(A);\r\nt = toc;\r\ndisp(t)\r\nOutput:\r\n1.1297\r\n\r\nPython code:\r\nimport numpy as np\r\nimport scipy as sp\r\nimport timeit\r\nimport tensorflow as tf\r\n\r\ndim = 3000\r\nmat = tf.random.uniform(shape=(dim,dim))\r\n\r\nst = timeit.default_timer()\r\ntf.linalg.inv(mat)\r\nprint(\"time elapsed tensorflow:\", timeit.default_timer() - st)\r\n\r\nx = np.random.rand(dim,dim)\r\nst = timeit.default_timer()\r\nnp.linalg.inv(x)\r\nprint(\"time elapsed scipy: \", timeit.default_timer() - st)\r\n\r\nOutput:\r\ntime elapsed tensorflow: 1.3009283419999065\r\ntime elapsed scipy: 2.618305011000075", "@umairbinmansoor Please open a new issue with a simple standalone code to reproduce the issue. Your code uses only cpu as you didn't enable any TPU. Thanks!", "> @umairbinmansoor Please open a new issue with a simple standalone code to reproduce the issue. Your code uses only cpu as you didn't enable any TPU. Thanks!\r\n\r\n@jvishnuvardhan, I have the earlier output with the TPU selected but the funny thing is, with GPU, I am getting 0.1153 s"]}, {"number": 29699, "title": "Fix deprecated args to buildifier in ci_sanity.sh", "body": "The call to the `buildifier` utility in `tensorflow/tools/ci_build/ci_sanity.sh` uses a deprecated method of asking `builidifier` for a diff. As a result, the script generates output like this when run in an environment with a recent version of `builidifier`:\r\n```\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\nbuildifier: selecting diff program with the BUILDIFIER_DIFF, BUILDIFIER_MULTIDIFF, and DISPLAY environment variables is deprecated, use flags -diff_command and -multi_diff instead\r\ntensorflow/java/src/main/native/BUILD:\r\ntensorflow/contrib/mpi_collectives/BUILD:\r\ntensorflow/contrib/kafka/BUILD:\r\ntensorflow/compiler/xla/BUILD:\r\ntensorflow/compiler/jit/BUILD:\r\n--: tkdiff: command not found\r\nexit status 127\r\n```\r\nThis PR modifies `tensorflow/tools/ci_build/ci_sanity.sh` to set the `-diff_command` argument when asking `buildifer` for a diff. After the change, the script's output looks like this:\r\n```\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\ntensorflow/java/src/main/native/BUILD:\r\n17c17\r\n< load(\"//tensorflow:tensorflow.bzl\", \"tf_cuda_library\", \"tf_copts\")\r\n---\r\n> load(\"//tensorflow:tensorflow.bzl\", \"tf_copts\", \"tf_cuda_library\")\r\nexit status 1\r\n[...]\r\n```", "comments": []}, {"number": 29698, "title": "Fix indentation and add note for env param change", "body": "Thanks to @wdirons for noticing the parameter difference.", "comments": []}, {"number": 29697, "title": "tensorflow 2.0 tf.data cache", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Binary pip3\r\n- TensorFlow version (use command below):  2.0.0.dev20190523 \r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 10/7\r\n- GPU model and memory: Titan V 12GB\r\n\r\n**Describe the current behavior**\r\nWhenever using dataset.cache(\"tmp_disk_file\"), memory keeps increasing even though I save them into a disk file.  \r\n\r\n**Describe the expected behavior**\r\nNo memory usage difference with and without cache to disk.\r\n\r\n**Code to reproduce the issue**\r\nI don't have a code to regenerate the exact issue, but here is a pipeline sample.\r\n```\r\nlist_of_tfrecords = [filename1, filename2, filename3]\r\n# construct_dataset will form a pipeline to process data, \r\n# at the end of the pipeline, cache that pipeline for file\r\nlist_of_datasets_to_cache = [construct_dataset(filename) for filename in list_of_tfrecords]\r\ntrain_dataset = tf.data.experimental.sample_from_datasets(list_of_datasets_to_cache)\r\n```\r\nNote: the reason I'm doing this is I have a very expensive pipeline, I want to speed up the second epoch. There's better way doing this, but I want to do it this way because I'm doing cross-validation, and if I cache by forming a pipeline with all files, then I can't reuse the cache (I'm going to change to different folds next epoch - i.e. new dataset).\r\n", "comments": ["Will it be possible for you to present a minimal code snippet that can replicate the issue. This will help not only to understand the issue more clearly but also to proceed faster. Thanks! ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29696, "title": "MemoryError in the middle of training after x epochs: _XlaCompile error at beginning", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- custom code\r\n- Windows 10 1803\r\n- TensorFlow installed using pip with conda as environment manager. \r\n    `pip install --ignore-installed --upgrade tensorflow-gpu==2.0.0-beta0`\r\n- TensorFlow version unknown 2.0.0-beta0\r\n- Python version: 3.6.8\r\n- CUDA version: 10.0\r\n- GPU model and memory: Nvidia GeForce 1060 3GB\r\n\r\n**Describe the current behavior**\r\nWhile training the model after a number of epochs, a MemoryError suddenly occurs with top Error as `Operation 'simple_rnn_1/while' has no attr named '_XlaCompile'`. This occured while extending the training epochs by executing `model.fit()` subsequently. However, restarting the computer resolves the problem. \r\n\r\n_The problem occurred only once so far._ \r\n\r\n**Describe the expected behavior**\r\nThe training should finish after the set number of epochs.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nuniform_regularizer=0\r\nmodel=tf.keras.Sequential([\r\n    tf.keras.layers.SimpleRNN(\r\n    units=5,\r\n    kernel_initializer='he_normal',\r\n    recurrent_initializer='he_normal',\r\n    kernel_regularizer=tf.keras.regularizers.l1(l=uniform_regularizer),\r\n    use_bias=False,\r\n    stateful=True,\r\n    batch_input_shape=(3, 20, 3)),\r\n    tf.keras.layers.Dense(3,\r\n        kernel_initializer='he_normal',\r\n        kernel_regularizer=tf.keras.regularizers.l1(l=uniform_regularizer),\r\n        use_bias=False)\r\n])\r\ndef loss(explanvar, targetvar):\r\n  return tf.keras.metrics.mean_absolute_error(\r\n    explanvar,\r\n    targetvar)\r\nmodel.compile(\r\n    optimizer='adam',\r\n    loss = loss)\r\ncheckpoint_path = \"training_1/cp.ckpt\"\r\ncheckpoint_dir = os.path.dirname(checkpoint_path)\r\n\r\n# Create checkpoint callback\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\r\n                                                 save_weights_only=True)\r\nhistory = model.fit(train, \r\n                    epochs=55, \r\n                    steps_per_epoch=10,\r\n                    callbacks=[cp_callback],\r\n                    class_weight={0:0.4,\r\n                                  1:0.2,\r\n                                  2:0.4},                   \r\n                    validation_data=test)\r\n```\r\n**Other info / logs**\r\n_See attached file_\r\n[stacktrace.txt](https://github.com/tensorflow/tensorflow/files/3281953/stacktrace.txt)\r\n", "comments": ["@arjay55 Please provide us complete code to reproduce the issue.Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29695, "title": "Scripts halts with make_initializable_iterator with non empty shared_name", "body": "**System information**\r\n- Custom code\r\n- Linux Ubuntu 16.04\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 2.7.12\r\n- Bazel version: 0.25.1\r\n- CUDA/cuDNN version: 9.0.176 / 7.0\r\n- GPU model and memory: GeForce GTX 1080Ti\r\n\r\n**Describe the current behavior**\r\n\r\nThe script doesn't finish. I discovered that it halts after TF_CloseSession and even KeyboardInterrupt can't stop the script. I also discovered that it exits normally if I pass `shared_name=\"\"`. So it looks like that this place https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/iterator_ops.cc#L465\r\ncontains the bug. Possible deadlock or session can wait for iterator to free resources.\r\n\r\nI also can reproduce the problem with MacOS without Cuda.\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect this script to finish normally.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = \"\"\r\n\r\n\r\ndef generator():\r\n    for i in range(10):\r\n        yield [20.]\r\n\r\n\r\ndef main():\r\n    config = tf.ConfigProto(intra_op_parallelism_threads=1,\r\n                            inter_op_parallelism_threads=1)\r\n\r\n    dataset = tf.data.Dataset.from_generator(\r\n        generator,\r\n        output_types=(tf.float32), output_shapes=(tf.TensorShape([1])))\r\n    train_iterator = dataset.make_initializable_iterator(shared_name='g')\r\n\r\n    with tf.Session(config=config) as session:\r\n        session.run(train_iterator.initializer)\r\n\r\n        data_producer = train_iterator.get_next()\r\n        session.run(data_producer)\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n**Other info / logs**\r\nTraceback from gdb:\r\n```\r\n#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38\r\n#1  0x00007fffeafd62b1 in nsync::nsync_mu_semaphore_p(nsync::nsync_semaphore_s_*) () from /usr/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fffeafd4028 in nsync::nsync_mu_lock_slow_(nsync::nsync_mu_s_*, nsync::waiter*, unsigned int, nsync::lock_type_s*) () from /usr/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fffeafd411d in nsync::nsync_mu_lock(nsync::nsync_mu_s_*) () from /usr/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fffe5618998 in tensorflow::ResourceMgr::Cleanup(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) ()\r\n   from /usr/lib/python2.7/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fffe8a96fc0 in ?? () from /usr/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fffe8a99ec5 in tensorflow::data::CapturedFunction::RunInstantiated(std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) ()\r\n   from /usr/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fffe894b13b in tensorflow::data::GeneratorDatasetOp::Dataset::Iterator::~Iterator() () from /usr/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007fffe894b211 in tensorflow::data::GeneratorDatasetOp::Dataset::Iterator::~Iterator() () from /usr/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007fffe8946ac5 in ?? () from /usr/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007fffe8966571 in tensorflow::data::IteratorResource::~IteratorResource() () from /usr/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007fffe5615d1c in tensorflow::ResourceMgr::Clear() () from /usr/lib/python2.7/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#12 0x00007fffea97826b in tensorflow::DirectSession::~DirectSession() () from /usr/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#13 0x00007fffea9787a1 in tensorflow::DirectSession::~DirectSession() () from /usr/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#14 0x00007fffe7903a07 in tensorflow::SessionRef::Close() () from /usr/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#15 0x00007fffe7ae2f0b in TF_CloseSession () from /usr/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#16 0x00007fffe789fae6 in ?? () from /usr/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#17 0x00000000004bc4aa in call_function (oparg=<optimized out>, pp_stack=0x7fffffffd260) at ../Python/ceval.c:4350\r\n#18 PyEval_EvalFrameEx () at ../Python/ceval.c:2987\r\n#19 0x00000000004b9b66 in PyEval_EvalCodeEx () at ../Python/ceval.c:3582\r\n#20 0x00000000004c1f56 in fast_function (nk=<optimized out>, na=<optimized out>, n=1, pp_stack=0x7fffffffd460, func=<function at remote 0x7fffb659b578>) at ../Python/ceval.c:4445\r\n#21 call_function (oparg=<optimized out>, pp_stack=0x7fffffffd460) at ../Python/ceval.c:4370\r\n#22 PyEval_EvalFrameEx () at ../Python/ceval.c:2987\r\n#23 0x00000000004b9b66 in PyEval_EvalCodeEx () at ../Python/ceval.c:3582\r\n#24 0x00000000004d5669 in function_call.lto_priv () at ../Objects/funcobject.c:523\r\n#25 0x00000000004eef5e in PyObject_Call (kw=0x0,\r\n    arg=(<Session(_config=<ConfigProto at remote 0x7fffa22d57d0>, _graph=<Graph(_default_original_op=None, _handle_feeders={}, _stack_state_is_thread_local=False, _collections={'iterators': [<Tensor(_op=<Operation(_graph=<...>, _device_code_locations=[], _id_value=2, _control_flow_context=None, _outputs=[<...>], _original_op=None, _traceback=[('minimal_example.py', 30, '<module>', {'generator': <function at remote 0x7fffa22d56e0>, '__builtins__': <module at remote 0x7ffff7fb7b08>, '__file__': 'minimal_example.py', '__package__': None, 'tf': <module at remote 0x7ffff7ead2f0>, '__name__': '__main__', 'main': <function at remote 0x7fffa22d55f0>, 'os': <module at remote 0x7ffff7f8cd00>, '__doc__': None}, 3, None), ('minimal_example.py', 20, 'main', {...}, 13, None), ('/usr/lib/python2.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py', 140, 'make_initializable_iterator', {'CacheDataset': <ABCMeta(__module__='tensorflow.python.data.ops.dataset_ops', _abc_negative_cache=<WeakSet(_remove=<function at remote 0x7fffb5...(truncated), func=<function at remote 0x7fffb659f140>) at ../Objects/abstract.c:2546\r\n#26 instancemethod_call.lto_priv () at ../Objects/classobject.c:2602\r\n#27 0x00000000004ae043 in PyObject_Call (kw=0x0, arg=(None, None, None), func=<instancemethod at remote 0x7fffa3329820>) at ../Objects/abstract.c:2546\r\n#28 PyObject_CallFunctionObjArgs () at ../Objects/abstract.c:2773\r\n#29 0x00000000004bed46 in PyEval_EvalFrameEx () at ../Python/ceval.c:2948\r\n#30 0x00000000004c141f in fast_function (nk=<optimized out>, na=<optimized out>, n=0, pp_stack=0x7fffffffdb40, func=<function at remote 0x7fffa22d55f0>) at ../Python/ceval.c:4435\r\n#31 call_function (oparg=<optimized out>, pp_stack=0x7fffffffdb40) at ../Python/ceval.c:4370\r\n#32 PyEval_EvalFrameEx () at ../Python/ceval.c:2987\r\n#33 0x00000000004b9b66 in PyEval_EvalCodeEx () at ../Python/ceval.c:3582\r\n#34 0x00000000004eb69f in PyEval_EvalCode (\r\n    locals={'generator': <function at remote 0x7fffa22d56e0>, '__builtins__': <module at remote 0x7ffff7fb7b08>, '__file__': 'minimal_example.py', '__package__': None, 'tf': <module at remote 0x7ffff7ead2f0>, '__name__': '__main__', 'main': <function at remote 0x7fffa22d55f0>, 'os': <module at remote 0x7ffff7f8cd00>, '__doc__': None},\r\n    globals={'generator': <function at remote 0x7fffa22d56e0>, '__builtins__': <module at remote 0x7ffff7fb7b08>, '__file__': 'minimal_example.py', '__package__': None, 'tf': <module at remote 0x7ffff7ead2f0>, '__name__': '__main__', 'main': <function at remote 0x7fffa22d55f0>, 'os': <module at remote 0x7ffff7f8cd00>, '__doc__': None}, co=0x7ffff7eecd30) at ../Python/ceval.c:669\r\n#35 run_mod.lto_priv () at ../Python/pythonrun.c:1376\r\n#36 0x00000000004e58f2 in PyRun_FileExFlags () at ../Python/pythonrun.c:1362\r\n#37 0x00000000004e41a6 in PyRun_SimpleFileExFlags () at ../Python/pythonrun.c:948\r\n#38 0x00000000004938ce in Py_Main () at ../Modules/main.c:640\r\n#39 0x00007ffff760b830 in __libc_start_main (main=0x493370 <main>, argc=2, argv=0x7fffffffdf88, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffdf78) at ../csu/libc-start.c:291\r\n#40 0x0000000000493299 in _start ()\r\n```\r\n\r\nLooking at the `info threads`, we can see that all the threads are waiting for something:\r\n\r\n```\r\n* 1    Thread 0x7ffff7faf840 (LWP 27148) \"python\" syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38\r\n  2    Thread 0x7fffa2256700 (LWP 27153) \"python\" pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\n  3    Thread 0x7fffa1a55700 (LWP 27154) \"python\" pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\n  6    Thread 0x7fff93fff700 (LWP 27157) \"python\" pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\n  7    Thread 0x7fffa0a53700 (LWP 27158) \"python\" pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\n  8    Thread 0x7fffa1254700 (LWP 27159) \"python\" pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\n```", "comments": ["I am able to reproduce the reported issue with Tensorflow 1.12.0 and 1.13.1. Thanks!", "I can reproduce this issue on the latest master branch. PR #30102 is submitted to try to fix this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29695\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29695\">No</a>\n"]}, {"number": 29694, "title": "Get Per-sample Gradient for a Batch", "body": "**System information**\r\n- TensorFlow version (you are using): --\r\n- Are you willing to contribute it (Yes/No): Yes, but not sure if I am able to.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nNow when I do SGD on a model, the gradients for a batch is returned but I want an array of gradients such that each element corresponds to a sample in this batch.\r\n\r\n**Will this change the current api? How?**\r\nI assume we can achieve this by changing `optimizer.compute_gradients` or `tf.gradients`\r\n\r\n**Who will benefit with this feature?**\r\nThere are other ways to use the gradients including the one I am trying to implement.\r\n\r\n**Any Other info.**", "comments": ["Thanks for reaching out to us. Can you please let us know to which TensorFlow version this feature will be intended to. ", "> Thanks for reaching out to us. Can you please let us know to which TensorFlow version this feature will be intended to.\r\n\r\nto version 1.13 would be nice.", "I think you can already achieve this using [tf.vectorized_map](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/vectorized_map) and `tf.gradients`.", "> I think you can already achieve this using [tf.vectorized_map](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/vectorized_map) and `tf.gradients`.\r\n\r\nThanks! I'll look into that~"]}]